paper_id,venue,focused_review,point,id,chatgpt_discard
ACL_2017_503_review,ACL_2017,"Reranking use is not mentioned in the introduction.
It would be a great news in NLP context if an Earley parser would run in linear time for NLP grammars (unlike special kinds of formal language grammars).
Unfortunately, this result involves deep assumptions about the grammar and the kind of input. Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
To me, the paper should be more clear in this as a random reader may miss the difference between semantic parsing (from strings) and parsing of semantic parses (the current work).
There does not seem to be any control of the linear order of 0-arity edges. It might be useful to mention that if the parser is extended to string inputs with the aim to find the (best?) hypergraph for a given external nodes, then the item representations of the subgraphs must also keep track of the covered 0-arity edges. This makes the string-parser variant exponential. - Easily correctable typos or textual problems: 1) Lines 102-106 is misleading. While intersection and probs are true, ""such distribution"" cannot refer to the discussion in the above.
2) line 173: I think you should rather talk about validation or recognition algorithms than parsing algorithms as ""parsing"" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.
3) lines 195-196 are unclear: what are the elements of att_G; in what sense they are pairwise distinct. Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.
4) l.206. Move *rank* definition earlier and remove redundancy.
5) l. 267: rather ""immediately derives"", perhaps.
6) 279: add ""be"" 7) l. 352: give an example of a nontrivial internal path.
8) l. 472: define a subgraph of a hypergraph 9) l. 417, l.418: since there are two propositions, you may want to tell how they contribute to what is quoted.
10) l. 458: add ""for"" Table: Axiom: this is only place where this is introduced as an axiom. Link to the text that says it is a trigger.
- General Discussion: It might be useful to tell about MSOL graph languages and their yields, which are context-free string languages. What happens if the grammar is ambiguous and not top-down deterministic?
What if there are exponential number of parses even for the input graph due to lexical ambiguity or some other reasons. How would the parser behave then?
Wouldn't the given Earley recogniser actually be strictly polynomial to m or k ?
Even a synchronous derivation of semantic graphs can miss some linguistic phenomena where a semantic distinction is expressed by different linguistic means. E.g. one language may add an affix to a verb when another language may express the same distinction by changing the object. I am suggesting that although AMR increases language independence in parses it may have such cross-lingual challenges.
I did not fully understand the role of the marker in subgraphs. It was elided later and not really used.
l. 509-510: I already started to miss the remark of lines 644-647 at this point.
It seems that the normal order is not unique. Can you confirm this?
It is nice that def 7, cond 1 introduces lexical anchors to predictions.
Compare the anchors in lexicalized grammars.
l. 760. Are you sure that non-crossing links do not occur when parsing linearized sentences to semantic graphs?
- Significant questions to the Authors: Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
What would you say about parsing complexity in the case the RGG is a non-deterministic, possibly ambiguous regular tree grammar, but one is interested to use it to assign trees to frontier strings like a context-free grammar? Can one adapt the given Earley algorithm to this purpose (by guessing internal nodes and their edges)?
Although this question might seem like a confusion, it is relevant in the NLP context.
What prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are then linearised? What principle determines how they are linearised? Is the linear order determined by the Earley paths (and normal order used in productions) or can one consider an actual word order in strings of a natural language? There is no clear connection to (non)context-free string languages or sets of (non)projective dependency graphs used in semantic parsing. What is written on lines 757-758 is just misleading: Lines 757-758 mention that HRGs can be used to generate non-context-free languages. Are these graph languages or string languages? How an NLP expert should interpret the (implicit) fact that RGGs generate only context-free languages? Does this mean that the graphs are noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?","6) 279: add ""be""7) l. 352: give an example of a nontrivial internal path.",1,0
ACL_2017_792_review,ACL_2017,"1. Unfortunately, the results are rather inconsistent and one is not left entirely convinced that the proposed models are better than the alternatives, especially given the added complexity. Negative results are fine, but there is insufficient analysis to learn from them. Moreover, no results are reported on the word analogy task, besides being told that the proposed models were not competitive - this could have been interesting and analyzed further.
2. Some aspects of the experimental setup were unclear or poorly motivated, for instance w.r.t. to corpora and datasets (see details below).
3. Unfortunately, the quality of the paper deteriorates towards the end and the reader is left a little disappointed, not only w.r.t. to the results but with the quality of the presentation and the argumentation.
- General Discussion: 1. The authors aim ""to learn representations for both words and senses in a shared emerging space"". This is only done in the LSTMEmbed_SW version, which rather consisently performs worse than the alternatives. In any case, what is the motivation for learning representations for words and senses in a shared semantic space? This is not entirely clear and never really discussed in the paper.
2. The motivation for, or intuition behind, predicting pre-trained embeddings is not explicitly stated. Also, are the pre-trained embeddings in the LSTMEmbed_SW model representations for words or senses, or is a sum of these used again? If different alternatives are possible, which setup is used in the experiments?
3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context.
4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated.
5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel, which makes comparisons problematic, in this case giving three ""wins"" as opposed to one.
6. The proposed models are said to be faster to train by using pre-trained embeddings in the output layer. However, no evidence to support this claim is provided. This would strengthen the paper.
7. Table 4: why not use the same dimensionality for a fair(er) comparison?
8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached.
9. A reference to Table 2 is missing.
10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset.","4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated.",2,0
ACL_2017_148_review,ACL_2017,"- The goal of your paper is not entirely clear. I had to read the paper 4 times and I still do not understand what you are talking about!
- The article is highly ambiguous what it talks about - machine comprehension or text readability for humans - you miss important work in the readability field - Section 2.2. has completely unrelated discussion of theoretical topics.
- I have the feeling that this paper is trying to answer too many questions in the same time, by this making itself quite weak. Questions such as “does text readability have impact on RC datasets” should be analyzed separately from all these prerequisite skills.
- General Discussion: - The title is a bit ambiguous, it would be good to clarify that you are referring to machine comprehension of text, and not human reading comprehension, because “reading comprehension” and “readability” usually mean that.
- You say that your “dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty”, but this depends on the method/features used for answer detection, e.g. if you use POS/dependency parse features.
- You need to proofread the English of your paper, there are some important omissions, like “the question is easy to solve simply look..” on page 1.
- How do you annotate datasets with “metrics”??
- Here you are mixing machine reading comprehension of texts and human reading comprehension of texts, which, although somewhat similar, are also quite different, and also large areas.
- “readability of text” is not “difficulty of reading contents”. Check this: DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact information. - it would be good if you put more pointers distinguishing your work from readability of questions for humans, because this article is highly ambiguous.
E.g. on page 1 “These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions” you should add “for machine comprehension” - Section 3.1. - Again: are you referring to such skills for humans or for machines? If for machines, why are you citing papers for humans, and how sure are you they are referring to machines too?
- How many questions the annotators had to annotate? Were the annotators clear they annotate the questions keeping in mind machines and not people?","- “readability of text” is not “difficulty of reading contents”. Check this: DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact information.",3,0
ACL_2017_494_review,ACL_2017,"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors. - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding baselines. General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.
Minor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.
- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.
- Line 223: x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.
- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.
- I have read the author response",- Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.,4,0
ACL_2017_37_review,ACL_2017,"Weak results/summary of ""side-by-side human"" comparison in Section 5. Some disfluency/agrammaticality.
- General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether ""second"", ""third"", and ""last"" imply a side-specific or global enumeration.
2. Some reader confusion may be eliminated by explicitly defining what ""segment"" means in ""segment level"", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as ""a sequence-sequence [similarity matrix]"". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean ""word subsequence"" and ""word subsequence to word subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.
3. Currently, the variable symbol ""n"" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.
4. The statement ""This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice."" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases.
The authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than ""better"") than the VHRED baseline.
5. The authors may choose to insert into Figure 1 the explicit ""first layer"", ""second layer"" and ""third layer"" labels they use in the accompanying text.
6. Their is a pervasive use of ""to meet"" as in ""a response candidate can meet each utterace"" on line 280 which is difficult to understand.
7. Spelling: ""gated recurrent unites""; ""respectively"" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; ""baseline model over"" -> ""baseline model by""; ""one cannot neglects"".","2. Some reader confusion may be eliminated by explicitly defining what ""segment"" means in ""segment level"", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as ""a sequence-sequence [similarity matrix]"". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean ""word subsequence"" and ""word subsequence to word subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.",5,0
ACL_2017_108_review,ACL_2017,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?","- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?",6,0
ACL_2017_484_review,ACL_2017,"The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.
- General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5. The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5.","5. The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq.",7,0
ACL_2017_333_review,ACL_2017,"There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained. - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both ""represent the meaning"". Are both indeed necessary? Did you trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?
- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.
- page 1, lines 93-96: please provide a reference for this passage: ""This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.""
- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks.""
- Figure 2: What is ""MLP""? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).
- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).
- Table 2: what does ""#(ref)"" mean?
- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove ""the"" word in this line? "" SGD as our optimizing algorithms"" instead of ""SGD as our the optimizing algorithms.""
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "" council of europe again slams french prison conditions"" (again or against?)
- typo ""supper script"" -> ""superscript"" (4 times)","- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?",8,0
ACL_2017_433_review,ACL_2017,"- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement. UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.
- General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.
- Questions for the authors: - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.
- Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?
- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?
- Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.
- Low-level comments: - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.
- You use the word ""grammar"" in a slightly strange way. I think replacing ""grammar"" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90) - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.
- Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with ""very high performing model"" or something like that.
- It would be helpful if you provided glosses in Figure 2.","-General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.",9,0
ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?",10,0
ACL_2017_775_review,ACL_2017,"weakness section.
- Weaknesses: Regarding writing =============== No doubt the paper is well-written. But the major issue with the paper is its lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in scientific writing is very much needed.
I hope you will agree with most of the stuff being articulated here: https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/ Let me put my objections on writing here: - ""while providing a method which is effectively zero-shot""..left readers in the blank. The notion of zero-shot has not been introduced yet!
- Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?
- Talk more about data. Otherwise, the method is less intuitive.
- I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques?
Putting some examples would be better, I believe.
Technicality ============ ""A strength of this model is its simplicity"" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.
- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.
- General Discussion:","- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.",11,0
ACL_2017_270_review,ACL_2017,"Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.
Other minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.
2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.
While the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems.
Given these strengths, I am changing my recommendation score to 3. I have read the authors' responses.","2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis. While the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. Given these strengths, I am changing my recommendation score to 3. I have read the authors' responses.",12,1
ACL_2017_433_review,ACL_2017,"There are three main issues I see with this paper: - There is insufficient comparison to the UD annotation of non-English languages. Many of the constructions they bring up as specific to Singlish are also present in other UD languages, and the annotations should ideally be consistent between Singlish and these languages.
- I'd like to see an analysis on the impact of training data size. A central claim of this paper is that using English data can improve performance on a low-resource language like Singlish. How much more Singlish data would be needed before the English data became unnecessary?
- What happens if you train a single POS/dep parsing model on the concatenated UD Web and Singlish datasets? This is much simpler than incorporating neural stacking. The case for neural stacking is stronger if it can outperform this baseline.
- General Discussion: Line 073: “POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations” - be more clear that you will quantify this later. As such, it seems a bit hand-wavy.
Line 169: Comparison to neural network models for multi-lingual parsing. As far as I can tell, you don't directly try the approach of mapping Singlish and English word embeddings into the same embedding space.
Line 212: Introduction of UD Eng. At this point, it is appropriate to point out that the Singlish data is also web data, so the domain matches UD Eng.
Line 245: “All borrowed words are annotated according to their original meanings”. Does this mean they have the same POS as in the language from which they were borrowed? Or the POS of their usage in Singlish?
Figure 2: Standard English glosses would be very useful in understanding the constructions and checking the correctness of the UD relations used.
Line 280: Topic prominence: You should compare with the “dislocated” label in UD. From the UD paper: “The dislocated relation captures preposed (topics) and postposed elements”. The syntax you are describing sounds similar to a topic-comment-style syntax; if it is different, then you should make it clear how.
Line 294: “Second, noun phrases used to modify the predicate with the presence of a preposition is regarded as a “nsubj” (nominal subject).”
Here, I need a gloss to determine if this analysis makes sense. If the phrase is really being used to modify the predicate, then this should not be nsubj. UD makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If this is a case of modification, then you should use one of the modification relations, not a core argument relation. Should clarify the language here.
Line 308: “In UD-Eng standards, predicative “be” is the only verb used as a copula, which often depends on its complement to avoid copular head.” This is an explicit decision made in UD, to increase parallelism with non-copular languages (e.g., Singlish). You should call this out. I think the rest of the discussion of copula handling is not necessary.
Line 322: “NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects.” This is common in other languages (zero-anaphora in e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out, and also point to how this is dealt with in UD in those languages (I believe the same way you handle it).
Ling 330: Subj/verb inversion is common in interrogatives in other languages (“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag questions are present in English (though perhaps are not as frequent). You should make sure that your analysis is consistent with these languages.
Sec 3.3 Data Selection and Annotation: The way you chose the Singlish sentences, of course an English parser will do poorly (they are chosen to be disimilar to sentences an English parser has seen before). But do you have a sense of how a standard English parser does overall on Singlish, if it is not filtered this way? How common are sentences with out-of-vocabulary terms or the constructions you discussed in 3.2?
A language will not necessarily capture unusual sentence structure, particularly around long-distance dependencies. Did you investigate whether this method did a good job of capturing sentences with the grammatical differences to English you discussed in Section 3.2?
Line 415: “the inter-annotator agreement has an unlabeled attachment score (UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”
- What’s the agreement on POS tags? Is this integrated with LAS?
- Note that in Silveira et al 2014, which produced UD-Eng, they measured 94% inter-annotator agreement on a per-token basis. Why the discrepancy?
POS tagging and dep parsing sections: For both POS-tagging and dep parsing, I’d like to see some analysis on the effect of training set size. E.g., how much more Singlish data would be needed to train a POS tagger/dep parser entirely on Singlish and get the same accuracy as the stacked model?
What happens if you just concatenate the datasets? E.g., train a model on a hybrid dataset of EN and Singlish, and see what the result is?
Line 681: typo: “pre-rained” should be “pre-trained” 742 “The neural stacking model leads to the biggest improvement over nearly all categories except for a slightly lower yet competitive performance on “NP Deletion” cases” --- seems that the English data strongly biases the parser to expect an explicit subj/obj. you could try deleting subj/obj from some English sentences to improve performance on this construction.",- I'd like to see an analysis on the impact of training data size. A central claim of this paper is that using English data can improve performance on a low-resource language like Singlish. How much more Singlish data would be needed before the English data became unnecessary?,13,0
ACL_2017_338_review,ACL_2017,"- In Section 3 it is not clear what is exactly the dataset that you used for training the SVM and your own model. Furthermore, you only give the starting date for collecting the testing data, but there is no other information related to the size of the dataset or the time frame when the data was collected. This might also give some insight for the results and statistics given in Section 3.2.
- In Table 3 we can see that the number of reviewers is only slightly lower than the number of reviews posted (at least for hotels), which means that only a few reviewers posted more than one review, in the labeled dataset. How does this compare with the full dataset in Table 2? What is the exact number of reviewers in Table 2 (to know what is the percentage of labeled reviewers)? It is also interesting to know how many reviews are made by one person on average.
If there are only a few reviewers that post more than one review (i.e., not that much info to learn from), the results would benefit from a thorough discussion. - General Discussion: This paper focuses on identifying spam reviews under the assumption that we deal with a cold-start problem, i.e., we do not have enough information to draw a conclusion. The paper proposes a neural network model that learns how to represent new reviews by jointly using embedded textual information and behaviour information. Overall, the paper is very well written and the results are compelling.
- Typos and/or grammar: - The new reviewer only provide us - Jindal and Liu (2008) make the first step -> the work is quite old, you could use past tense to refer to it - Usage of short form “can’t”, “couldn’t”, “what’s” instead of the prefered long form - The following sentence is not clear and should be rephrased: “The new reviewer just posted one review and we have to filter it out immediately, there is not any historical reviews provided to us.“","-General Discussion: This paper focuses on identifying spam reviews under the assumption that we deal with a cold-start problem, i.e., we do not have enough information to draw a conclusion. The paper proposes a neural network model that learns how to represent new reviews by jointly using embedded textual information and behaviour information. Overall, the paper is very well written and the results are compelling.",14,1
ACL_2017_516_review,ACL_2017,"Missing related work on anchor words Evaluation on 20 Newsgroups is not ideal Theoretical contribution itself is small - General Discussion: The authors propose a new method of interactive user specification of topics called Tandem Anchors. The approach leverages the anchor words algorithm, a matrix-factorization approach to learning topic models, by replacing the individual anchors inferred from the Gram-Schmidt algorithm with constructed anchor pseudowords created by combining the sparse vector representations of multiple words that for a topic facet. The authors determine that the use of a harmonic mean function to construct pseudowords is optimal by demonstrating that classification accuracy of document-topic distribution vectors using these anchors produces the most improvement over Gram-Schmidt. They also demonstrate that their work is faster than existing interactive methods, allowing interactive iteration, and show in a user study that the multiword anchors are easier and more effective for users.
Generally, I like this contribution a lot: it is a straightforward modification of an existing algorithm that actually produces a sizable benefit in an interactive setting. I appreciated the authors’ efforts to evaluate their method on a variety of scales. While I think the technical contribution in itself is relatively small (a strategy to assemble pseudowords based on topic facets) the thoroughness of the evaluation merited having it be a full paper instead of a short paper. It would have been nice to see more ideas as to how to build these facets in the absence of convenient sources like category titles in 20 Newsgroups or when initializing a topic model for interactive learning.
One frustration I had with this paper is that I find evaluation on 20 Newsgroups to not be great for topic modeling: the documents are widely different lengths, preprocessing matters a lot, users have trouble making sense of many of the messages, and naive bag-of-words models beat topic models by a substantial margin. Classification tasks are useful shorthand for how well a topic model corresponds to meaningful distinctions in the text by topic; a task like classifying news articles by section or reviews by the class of the subject of the review might be more appropriate. It would also have been nice to see a use case that better appealed to a common expressed application of topic models, which is the exploration of a corpus.
There were a number of comparisons I think were missing, as the paper contains little reference to work since the original proposal of the anchor word model.
In addition to comparing against standard Gram-Schmidt, it would have been good to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference”. I also would have liked to have seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models”, both of which provide useful insights into the anchor selection process.
I had some smaller notes: - 164: …entire dataset - 164-166: I’m not quite sure what you mean here. I think you are claiming that it takes too long to do one pass? My assumption would have been you would use only a subset of the data to retrain the model instead of a full sweep, so it would be good to clarify what you mean.
- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.
- 337: Usenet should be capitalized - 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also, did you remove headers, footers, and/or quotes from the messages?
- 436-440: I would have liked to see a bit more explanation of what this tells us about confusion.
- 692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora.
POST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different.","- 692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora. POST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different.",15,1
ACL_2017_104_review,ACL_2017,"- Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion:","- It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.",16,0
ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".",17,0
ACL_2017_371_review,ACL_2017,"- The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section",- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?,18,0
ACL_2017_433_review,ACL_2017,"There are three main issues I see with this paper: - There is insufficient comparison to the UD annotation of non-English languages. Many of the constructions they bring up as specific to Singlish are also present in other UD languages, and the annotations should ideally be consistent between Singlish and these languages.
- I'd like to see an analysis on the impact of training data size. A central claim of this paper is that using English data can improve performance on a low-resource language like Singlish. How much more Singlish data would be needed before the English data became unnecessary?
- What happens if you train a single POS/dep parsing model on the concatenated UD Web and Singlish datasets? This is much simpler than incorporating neural stacking. The case for neural stacking is stronger if it can outperform this baseline.
- General Discussion: Line 073: “POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations” - be more clear that you will quantify this later. As such, it seems a bit hand-wavy.
Line 169: Comparison to neural network models for multi-lingual parsing. As far as I can tell, you don't directly try the approach of mapping Singlish and English word embeddings into the same embedding space.
Line 212: Introduction of UD Eng. At this point, it is appropriate to point out that the Singlish data is also web data, so the domain matches UD Eng.
Line 245: “All borrowed words are annotated according to their original meanings”. Does this mean they have the same POS as in the language from which they were borrowed? Or the POS of their usage in Singlish?
Figure 2: Standard English glosses would be very useful in understanding the constructions and checking the correctness of the UD relations used.
Line 280: Topic prominence: You should compare with the “dislocated” label in UD. From the UD paper: “The dislocated relation captures preposed (topics) and postposed elements”. The syntax you are describing sounds similar to a topic-comment-style syntax; if it is different, then you should make it clear how.
Line 294: “Second, noun phrases used to modify the predicate with the presence of a preposition is regarded as a “nsubj” (nominal subject).”
Here, I need a gloss to determine if this analysis makes sense. If the phrase is really being used to modify the predicate, then this should not be nsubj. UD makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If this is a case of modification, then you should use one of the modification relations, not a core argument relation. Should clarify the language here.
Line 308: “In UD-Eng standards, predicative “be” is the only verb used as a copula, which often depends on its complement to avoid copular head.” This is an explicit decision made in UD, to increase parallelism with non-copular languages (e.g., Singlish). You should call this out. I think the rest of the discussion of copula handling is not necessary.
Line 322: “NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects.” This is common in other languages (zero-anaphora in e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out, and also point to how this is dealt with in UD in those languages (I believe the same way you handle it).
Ling 330: Subj/verb inversion is common in interrogatives in other languages (“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag questions are present in English (though perhaps are not as frequent). You should make sure that your analysis is consistent with these languages.
Sec 3.3 Data Selection and Annotation: The way you chose the Singlish sentences, of course an English parser will do poorly (they are chosen to be disimilar to sentences an English parser has seen before). But do you have a sense of how a standard English parser does overall on Singlish, if it is not filtered this way? How common are sentences with out-of-vocabulary terms or the constructions you discussed in 3.2?
A language will not necessarily capture unusual sentence structure, particularly around long-distance dependencies. Did you investigate whether this method did a good job of capturing sentences with the grammatical differences to English you discussed in Section 3.2?
Line 415: “the inter-annotator agreement has an unlabeled attachment score (UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”
- What’s the agreement on POS tags? Is this integrated with LAS?
- Note that in Silveira et al 2014, which produced UD-Eng, they measured 94% inter-annotator agreement on a per-token basis. Why the discrepancy?
POS tagging and dep parsing sections: For both POS-tagging and dep parsing, I’d like to see some analysis on the effect of training set size. E.g., how much more Singlish data would be needed to train a POS tagger/dep parser entirely on Singlish and get the same accuracy as the stacked model?
What happens if you just concatenate the datasets? E.g., train a model on a hybrid dataset of EN and Singlish, and see what the result is?
Line 681: typo: “pre-rained” should be “pre-trained” 742 “The neural stacking model leads to the biggest improvement over nearly all categories except for a slightly lower yet competitive performance on “NP Deletion” cases” --- seems that the English data strongly biases the parser to expect an explicit subj/obj. you could try deleting subj/obj from some English sentences to improve performance on this construction.",- What happens if you train a single POS/dep parsing model on the concatenated UD Web and Singlish datasets? This is much simpler than incorporating neural stacking. The case for neural stacking is stronger if it can outperform this baseline.,19,0
ACL_2017_726_review,ACL_2017,"- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it. General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of “near-state-of-the-art accuracies” on ATIS and GeoQuery, which doesn’t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.
- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.
- Table 6: what is the breakdown of the score by correctness and incompleteness?
What % of incompleteness do these queries exhibit?
- What is expertise required from crowd-workers who produce the correct SQL queries? - It would be helpful to see some analysis of the 48% of user questions which could not be generated.
- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages. - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?
I thank the authors for their response.",- Table 6: what is the breakdown of the score by correctness and incompleteness? What % of incompleteness do these queries exhibit?,20,0
ACL_2017_150_review,ACL_2017,"I have some doubts about the interpretation of the results. In addition, I think that some of the claims regarding the capability of the method proposed to learn morphology are not propperly backed by scientific evidence.
- General Discussion: This paper explores a complex architecture for character-level neural machine translation (NMT). The proposed architecture extends a classical encoder-decoder architecture by adding a new deep word-encoding layer capable of encoding the character-level input into sub-word representations of the source-language sentence. In the same way, a deep word-decoding layer is added to the output to transform the target-language sub-word representations into a character sequence as the final output of the NMT system. The objective of such architecture is to take advantage of the benefits of character-level NMT (reduction of the size of the vocabulary and flexibility to deal with unseen words) and, at the same time, improving the performance of the whole system by using an intermediate representation of sub-words to reduce the size of the input sequence of characters. In addition, the authors claim that their deep word-encoding model is able to learn morphology better than other state-of-the-art approaches.
I have some concerns regarding the evaluation. The authors compare their approach to other state-of-the-art systems taking into account two parameters: training time and BLEU score. However, I do not clearly see the advantage of the model proposed (DCNMT) in front of other approaches such as bpe2char. The difference between both approaches as regards BLEU score is very small (0.04 in Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming the other one without statistical significance information: has statistical significance been evaluated? As regards the training time, it is worth mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs training time is not provided (why not?) and for En-Fr bpe2char is not evaluated. I think that a more complete comparison with this system should be carried out to prove the advantages of the model proposed.
My second concern is on the 5.2 Section, where authors start claiming that they investigated about the ability of their system to learn morphology. However, the section only contains a examples and some comments on them. Even though these examples are very well chosen and explained in a very didactic way, it is worth noting that no experiments or formal evaluation seem to have been carried out to support the claims of the authors. I would definitely encourage authors to extend this very interesting part of the paper that could even become a different paper itself. On the other hand, this Section does not seem to be a critical point of the paper, so for the current work I may suggest just to move this section to an appendix and soften some of the claims done regarding the capabilities of the system to learn morphology.
Other comments, doubts and suggestions: - There are many acronyms that are used but are not defined (such as LSTM, HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN or BPE). Even though some of these acronyms are well known in the field of deep learning, I would encourage the authors to defined them to improve clearness.
- The concept of energy is mentioned for the first time in Section 3.1. Even though the explanation provided is enough at that point, it would be nice to refresh the idea of energy in Section 5.2 (where it is used several times) and providing some hints about how to interpret it: a high energy on a character would be indicating that the current morpheme should be split at that point? In addition, the concept of peak (in Figure 5) is not described.
- When the acronym BPE is defined, capital letters are used, but then, for the rest of mentions it is lower cased; is there a reason for this?
- I am not sure if it is necessary to say that no monolingual corpus is used in Section 4.1.
- It seems that there is something wrong with Figure 4a since the colours for the energy values are not shown for every character.
- In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not taken from the papers, since they are not reported. If the authors computed these results by themselves (as it seems) they should mention it.
- I would not say that French is morphologically poor, but rather that it is not that rich as Slavic languages such as Czech.
- Why a link is provided for WMT'15 training corpora but not for WMT'14?
- Several references are incomplete Typos: - ""..is the bilingual, parallel corpora provided..."" -> ""..are the bilingual, parallel corpora provided..."" - ""Luong and Manning (2016) uses"" -> ""Luong and Manning (2016) use"" - ""HGRU (It is"" -> ""HGRU (it is"" - ""coveres"" -> ""covers"" - ""both consists of two-layer RNN, each has 1024"" -> ""both consist of two-layer RNN, each have 1024"" - ""the only difference between CNMT and DCNMT is CNMT"" -> ""the only difference between CNMT and DCNMT is that CNMT""",- Why a link is provided for WMT'15 training corpora but not for WMT'14?,21,0
ACL_2017_37_review,ACL_2017,"Weak results/summary of ""side-by-side human"" comparison in Section 5. Some disfluency/agrammaticality.
- General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether ""second"", ""third"", and ""last"" imply a side-specific or global enumeration.
2. Some reader confusion may be eliminated by explicitly defining what ""segment"" means in ""segment level"", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as ""a sequence-sequence [similarity matrix]"". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean ""word subsequence"" and ""word subsequence to word subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.
3. Currently, the variable symbol ""n"" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.
4. The statement ""This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice."" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases.
The authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than ""better"") than the VHRED baseline.
5. The authors may choose to insert into Figure 1 the explicit ""first layer"", ""second layer"" and ""third layer"" labels they use in the accompanying text.
6. Their is a pervasive use of ""to meet"" as in ""a response candidate can meet each utterace"" on line 280 which is difficult to understand.
7. Spelling: ""gated recurrent unites""; ""respectively"" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; ""baseline model over"" -> ""baseline model by""; ""one cannot neglects"".","3. Currently, the variable symbol ""n"" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.",22,0
ACL_2017_384_review,ACL_2017,"The experiments are encouraging. However, it would be nice to see ROC curves for the new approach alone, not in an ensemble with Hearst patterns. Table 5 tells us that Mods_I increases coverage at the cost of precision and Figure 2 tells us that Mods_I matches Hearst pattern precision for the high precision region of the data. However, neither of these tell us whether the model can distinguish between the high and low precision regions, and the ROC curves (which would tell us this) are only available for ensembled models.
I believe that Eqn. 7 has an unnecessary $w$ since it is already the case that $w=D(\rangle e, p, o \langle)$. - discussion Overall, this is a nice idea that is well described and evaluated. I think this paper would be a good addition to ACL.","- discussion Overall, this is a nice idea that is well described and evaluated. I think this paper would be a good addition to ACL.",23,1
ACL_2017_614_review,ACL_2017,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?","- The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).",24,0
ACL_2017_699_review,ACL_2017,"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc.
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).","3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset.",25,1
ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","- Detailed comments:026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies? Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature? Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.",26,0
ACL_2017_503_review,ACL_2017,"Reranking use is not mentioned in the introduction.
It would be a great news in NLP context if an Earley parser would run in linear time for NLP grammars (unlike special kinds of formal language grammars).
Unfortunately, this result involves deep assumptions about the grammar and the kind of input. Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
To me, the paper should be more clear in this as a random reader may miss the difference between semantic parsing (from strings) and parsing of semantic parses (the current work).
There does not seem to be any control of the linear order of 0-arity edges. It might be useful to mention that if the parser is extended to string inputs with the aim to find the (best?) hypergraph for a given external nodes, then the item representations of the subgraphs must also keep track of the covered 0-arity edges. This makes the string-parser variant exponential. - Easily correctable typos or textual problems: 1) Lines 102-106 is misleading. While intersection and probs are true, ""such distribution"" cannot refer to the discussion in the above.
2) line 173: I think you should rather talk about validation or recognition algorithms than parsing algorithms as ""parsing"" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.
3) lines 195-196 are unclear: what are the elements of att_G; in what sense they are pairwise distinct. Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.
4) l.206. Move *rank* definition earlier and remove redundancy.
5) l. 267: rather ""immediately derives"", perhaps.
6) 279: add ""be"" 7) l. 352: give an example of a nontrivial internal path.
8) l. 472: define a subgraph of a hypergraph 9) l. 417, l.418: since there are two propositions, you may want to tell how they contribute to what is quoted.
10) l. 458: add ""for"" Table: Axiom: this is only place where this is introduced as an axiom. Link to the text that says it is a trigger.
- General Discussion: It might be useful to tell about MSOL graph languages and their yields, which are context-free string languages. What happens if the grammar is ambiguous and not top-down deterministic?
What if there are exponential number of parses even for the input graph due to lexical ambiguity or some other reasons. How would the parser behave then?
Wouldn't the given Earley recogniser actually be strictly polynomial to m or k ?
Even a synchronous derivation of semantic graphs can miss some linguistic phenomena where a semantic distinction is expressed by different linguistic means. E.g. one language may add an affix to a verb when another language may express the same distinction by changing the object. I am suggesting that although AMR increases language independence in parses it may have such cross-lingual challenges.
I did not fully understand the role of the marker in subgraphs. It was elided later and not really used.
l. 509-510: I already started to miss the remark of lines 644-647 at this point.
It seems that the normal order is not unique. Can you confirm this?
It is nice that def 7, cond 1 introduces lexical anchors to predictions.
Compare the anchors in lexicalized grammars.
l. 760. Are you sure that non-crossing links do not occur when parsing linearized sentences to semantic graphs?
- Significant questions to the Authors: Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
What would you say about parsing complexity in the case the RGG is a non-deterministic, possibly ambiguous regular tree grammar, but one is interested to use it to assign trees to frontier strings like a context-free grammar? Can one adapt the given Earley algorithm to this purpose (by guessing internal nodes and their edges)?
Although this question might seem like a confusion, it is relevant in the NLP context.
What prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are then linearised? What principle determines how they are linearised? Is the linear order determined by the Earley paths (and normal order used in productions) or can one consider an actual word order in strings of a natural language? There is no clear connection to (non)context-free string languages or sets of (non)projective dependency graphs used in semantic parsing. What is written on lines 757-758 is just misleading: Lines 757-758 mention that HRGs can be used to generate non-context-free languages. Are these graph languages or string languages? How an NLP expert should interpret the (implicit) fact that RGGs generate only context-free languages? Does this mean that the graphs are noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?",3) lines 195-196 are unclear: what are the elements of att_G; in what sense they are pairwise distinct. Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.,27,0
ACL_2017_56_review,ACL_2017,"- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.
- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.
- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.
- General Discussion: This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.
I have read the author response. I look forward to seeing the revised version of the paper.","- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.",28,0
ACL_2017_489_review,ACL_2017,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.","-General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.",29,1
ACL_2017_503_review,ACL_2017,"Reranking use is not mentioned in the introduction.
It would be a great news in NLP context if an Earley parser would run in linear time for NLP grammars (unlike special kinds of formal language grammars).
Unfortunately, this result involves deep assumptions about the grammar and the kind of input. Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
To me, the paper should be more clear in this as a random reader may miss the difference between semantic parsing (from strings) and parsing of semantic parses (the current work).
There does not seem to be any control of the linear order of 0-arity edges. It might be useful to mention that if the parser is extended to string inputs with the aim to find the (best?) hypergraph for a given external nodes, then the item representations of the subgraphs must also keep track of the covered 0-arity edges. This makes the string-parser variant exponential. - Easily correctable typos or textual problems: 1) Lines 102-106 is misleading. While intersection and probs are true, ""such distribution"" cannot refer to the discussion in the above.
2) line 173: I think you should rather talk about validation or recognition algorithms than parsing algorithms as ""parsing"" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.
3) lines 195-196 are unclear: what are the elements of att_G; in what sense they are pairwise distinct. Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.
4) l.206. Move *rank* definition earlier and remove redundancy.
5) l. 267: rather ""immediately derives"", perhaps.
6) 279: add ""be"" 7) l. 352: give an example of a nontrivial internal path.
8) l. 472: define a subgraph of a hypergraph 9) l. 417, l.418: since there are two propositions, you may want to tell how they contribute to what is quoted.
10) l. 458: add ""for"" Table: Axiom: this is only place where this is introduced as an axiom. Link to the text that says it is a trigger.
- General Discussion: It might be useful to tell about MSOL graph languages and their yields, which are context-free string languages. What happens if the grammar is ambiguous and not top-down deterministic?
What if there are exponential number of parses even for the input graph due to lexical ambiguity or some other reasons. How would the parser behave then?
Wouldn't the given Earley recogniser actually be strictly polynomial to m or k ?
Even a synchronous derivation of semantic graphs can miss some linguistic phenomena where a semantic distinction is expressed by different linguistic means. E.g. one language may add an affix to a verb when another language may express the same distinction by changing the object. I am suggesting that although AMR increases language independence in parses it may have such cross-lingual challenges.
I did not fully understand the role of the marker in subgraphs. It was elided later and not really used.
l. 509-510: I already started to miss the remark of lines 644-647 at this point.
It seems that the normal order is not unique. Can you confirm this?
It is nice that def 7, cond 1 introduces lexical anchors to predictions.
Compare the anchors in lexicalized grammars.
l. 760. Are you sure that non-crossing links do not occur when parsing linearized sentences to semantic graphs?
- Significant questions to the Authors: Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
What would you say about parsing complexity in the case the RGG is a non-deterministic, possibly ambiguous regular tree grammar, but one is interested to use it to assign trees to frontier strings like a context-free grammar? Can one adapt the given Earley algorithm to this purpose (by guessing internal nodes and their edges)?
Although this question might seem like a confusion, it is relevant in the NLP context.
What prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are then linearised? What principle determines how they are linearised? Is the linear order determined by the Earley paths (and normal order used in productions) or can one consider an actual word order in strings of a natural language? There is no clear connection to (non)context-free string languages or sets of (non)projective dependency graphs used in semantic parsing. What is written on lines 757-758 is just misleading: Lines 757-758 mention that HRGs can be used to generate non-context-free languages. Are these graph languages or string languages? How an NLP expert should interpret the (implicit) fact that RGGs generate only context-free languages? Does this mean that the graphs are noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?","2) line 173: I think you should rather talk about validation or recognition algorithms than parsing algorithms as ""parsing"" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.",30,0
ACL_2017_67_review,ACL_2017,"The main weaknesses for me are evaluation and overall presentation/writing.
- The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).
- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.
- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.
- The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.
- The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)... - General Discussion: The approach itself is not really original or novel, but it is applied to a problem that has not been addressed with deep learning yet. For this reason, I think this paper is interesting, but there are two main flaws. The first and easiest to fix is the presentation. There are many errors/typos that need to be corrected. I started listing them to help, but there are just too many of them.
The second issue is the evaluation, in my opinion. Technically, the performances are better, but it does not feel convincing as explained above.
What is Memb, is it the method from Shwartz et al 2016, maybe? If not, what performance did this recent approach have? I think the authors need to reorganize the evaluation section, in order to properly list the baseline systems, clearly show the benefit of their approach and where the others fail.
Significance tests also seem necessary given the slight improvement on one dataset.","- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.",31,0
ACL_2017_792_review,ACL_2017,"1. Unfortunately, the results are rather inconsistent and one is not left entirely convinced that the proposed models are better than the alternatives, especially given the added complexity. Negative results are fine, but there is insufficient analysis to learn from them. Moreover, no results are reported on the word analogy task, besides being told that the proposed models were not competitive - this could have been interesting and analyzed further.
2. Some aspects of the experimental setup were unclear or poorly motivated, for instance w.r.t. to corpora and datasets (see details below).
3. Unfortunately, the quality of the paper deteriorates towards the end and the reader is left a little disappointed, not only w.r.t. to the results but with the quality of the presentation and the argumentation.
- General Discussion: 1. The authors aim ""to learn representations for both words and senses in a shared emerging space"". This is only done in the LSTMEmbed_SW version, which rather consisently performs worse than the alternatives. In any case, what is the motivation for learning representations for words and senses in a shared semantic space? This is not entirely clear and never really discussed in the paper.
2. The motivation for, or intuition behind, predicting pre-trained embeddings is not explicitly stated. Also, are the pre-trained embeddings in the LSTMEmbed_SW model representations for words or senses, or is a sum of these used again? If different alternatives are possible, which setup is used in the experiments?
3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context.
4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated.
5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel, which makes comparisons problematic, in this case giving three ""wins"" as opposed to one.
6. The proposed models are said to be faster to train by using pre-trained embeddings in the output layer. However, no evidence to support this claim is provided. This would strengthen the paper.
7. Table 4: why not use the same dimensionality for a fair(er) comparison?
8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached.
9. A reference to Table 2 is missing.
10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset.","10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset.",32,0
ACL_2017_483_review,ACL_2017,"- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was performed on.
General Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with ""sequence-to-sequence models"". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not enough well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be appropriate.
- 590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.
- 745: This could alternatively show that structural cues are less important for this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).
[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection."" EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016.",- 617: Specify what the length of an AC is measured in (words?).,33,0
ACL_2017_318_review,ACL_2017,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?","1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.",34,0
ACL_2017_130_review,ACL_2017,"The paper suffers from several drawbacks 1. The paper is hard to read due to incorrect usage of English. The current manuscript would benefit a lot from a review grammar and spellings.
2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100?
3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence?
4. The datsets are from a biomedical domain. No domain specific tools have been leveraged.
5. Since dataset class distribution is unclear, it is unclear to determine if accuracy is a good measure for evaluation. In either case, since it is a binary classification task, F1 would have been a desirable metric.
6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant.","3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence?",35,0
ACL_2017_266_review,ACL_2017,"The outcome of the experiments is very predictable. The methods that are employed are very simple and ad-hoc. I found hardly any new idea in that paper. Neither are there any significant lessons that the reader learns about embeddings or sentiment analysis. The main idea (i.e. focusing on more task-specific data for training more accurate embeddings) was already published in the context of named-entity recognition by Joshi et al. (2015). The additions made in this paper are very incremental in nature.
I find some of the experiments inconclusive as (apparently) no statistical signficance testing between different classifiers has been carried out. In Tables 2, 3 and 6, various classifier configurations produce very similar scores. In such cases, only statistical signficance testing can really give a proper indication whether these difference are meaningful. For instance, in Table 3 on the left half reporting results on RT, one may wonder whether there is a significant difference between ""Wikipedia Baseline"" and any of the combinations. Furthermore, one doubts whether there is any signficant difference between the different combinations (i.e. either using ""subj-Wiki"", ""subj-Multiun"" or ""subj-Europarl"") in that table.
The improvement by focusing on subjective subsets is plausible in general.
However, I wonder whether in real life, in particular, a situation in which resources are sparse this is very helpful. Doing a pre-selection with OpinionFinder is some pre-processing step which will not be possible in most languages other than English. There are no equivalent tools or fine-grained datasets on which such functionality could be learnt. The fact that in the experiments for Catalan, this information is not considered proves that. Minor details: - lines 329-334: The discussion of this dataset is confusing. I thought the task is plain polarity classification but the authors here also refer to ""opinion holder"" and ""opinion targets"". If these information are not relevant to the experiments carried out in this paper, then they should not be mentioned here.
- lines 431-437: The variation of ""splicing"" that the authors explain is not very well motivated. First, why do we need this? In how far should this be more effective than simple ""appending""?
- lines 521-522: How is the subjective information isolated for these configurations? I assume the authors here again employ OpinionFinder? However, there is no explicit mention of this here.
- lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.
- lines 689-695: Similar to lines 329-334 it is unclear what precise task is carried out. Do the authors take opinion holders and targets in consideration?
***AFTER AUTHORS' RESPONSE*** Thank you very much for these clarifying remarks.
I do not follow your explanations regarding the incorporation of opinion holders and targets, though.
Overall, I will not change my scores since I think that this work lacks sufficient novelty (the things the authors raised in their response are just insufficient to me). This submission is too incremental in nature.",- lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.,36,0
ACL_2017_557_review,ACL_2017,"- The approach is incremental and seems like just a combination of existing methods. - The improvements on the performance (1.2 percent points on dev) are relatively small, and no significance test results are provided.
- General Discussion: - Major comments: - The model employed a recent parser and glove word embeddings. How did they affect the relation extraction performance?
- In prediction, how did the authors deal with illegal predictions?
- Minor comments: - Local optimization is not completely ""local"". It ""considers structural correspondences between incremental decisions,"" so this explanation in the introduction is misleading.
- Points in Figures 6 and 7 should be connected with straight lines, not curves.
- How are entities represented in ""-segment""?
- Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR, and Li et al. (2014) misses pages.","- Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR, and Li et al. (2014) misses pages.",37,0
ACL_2017_494_review,ACL_2017,"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors. - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding baselines. General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.
Minor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.
- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.
- Line 223: x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.
- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.
- I have read the author response","- Line 327: (create, creates) seems like a wrong example for that rule.",38,0
ACL_2017_606_review,ACL_2017,"- [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not using the most popular WebQuestions (Berant et al., 2013) benchmark set? Since NSM only requires weak supervision, using WebQuestions would be more intuitive and straightforward, plus it could facilitate direct comparison with main-stream QA research.
- [Analysis of Compositionality] One of the contribution of this work is the usage of symbolic intermediate execution results to facilitate modeling language compositionality. One interesting question is how well questions with various compositional depth are handled. Simple one-hop questions are the easiest to solve, while complex multi-hop ones that require filtering and superlative operations (argmax/min) would be highly non-trivial. The authors should present detailed analysis regarding the performance on question sets with different compositional depth.
- [Missing References] I find some relevant papers in this field missing. For example, the authors should cite previous RL-based methods for knowledge-based semantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE training method of (Ranzato et al., 2016) which is closely related to augmented REINFORCE, and the neural enquirer work (Yin et al., 2016) which uses continuous differentiable memories for modeling neural execution.
- Misc.
- Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of using parameters pre-trained with iterative ML?
- What is KG server in Figure 5?","- [Analysis of Compositionality] One of the contribution of this work is the usage of symbolic intermediate execution results to facilitate modeling language compositionality. One interesting question is how well questions with various compositional depth are handled. Simple one-hop questions are the easiest to solve, while complex multi-hop ones that require filtering and superlative operations (argmax/min) would be highly non-trivial. The authors should present detailed analysis regarding the performance on question sets with different compositional depth.",39,0
ACL_2017_49_review,ACL_2017,"There are some minor points, listed as follows: 1) Figure 1: I am a bit surprised that the function words dominate the content ones in a Japanese sentence. Sorry I may not understand Japanese. 2) In all equations, sequences/vectors (like matrices) should be represented as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ... 3) Equation 12: s_j-1 instead of s_j.
4) Line 244: all encoder states should be referred to bidirectional RNN states.
5) Line 285: a bit confused about the phrase ""non-sequential information such as chunks"". Is chunk still sequential information???
6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k) to indicate the word in a chunk. 7) Some questions for the experiments: Table 1: source language statistics? For the baselines, why not running a baseline (without using any chunk information) instead of using (Li et al., 2016) baseline (|V_src| is different)? It would be easy to see the effect of chunk-based models. Did (Li et al., 2016) and other baselines use the same pre-processing and post-processing steps? Other baselines are not very comparable. After authors's response, I still think that (Li et al., 2016) baseline can be a reference but the baseline from the existing model should be shown. Figure 5: baseline result will be useful for comparison? chunks in the translated examples are generated *automatically* by the model or manually by the authors? Is it possible to compare the no. of chunks generated by the model and by the bunsetsu-chunking toolkit? In that case, the chunk information for Dev and Test in Table 1 will be required. BTW, the authors's response did not address my point here. 8) I am bit surprised about the beam size 20 used in the decoding process. I suppose large beam size is likely to make the model prefer shorter generated sentences. 9) Past tenses should be used in the experiments, e.g., Line 558: We *use* (used) ... Line 579-584: we *perform* (performed) ... *use* (used) ... ... - General Discussion: Overall, this is a solid work - the first one tackling the chunk-based NMT; and it well deserves a slot at ACL.","9) Past tenses should be used in the experiments, e.g., Line 558: We *use* (used) ... Line 579-584: we *perform* (performed) ... *use* (used) ... ...",40,0
ACL_2017_768_review,ACL_2017,". First, the classification model used in this paper (concat + linear classifier) was shown to be inherently unable to learn relations in ""Do Supervised Distributional Methods Really Learn Lexical Inference Relations?"" ( Levy et al., 2015). Second, the paper makes superiority claims in the text that are simply not substantiated in the quantitative results. In addition, there are several clarity and experiment setup issues that give an overall feeling that the paper is still half-baked.
= Classification Model = Concatenating two word vectors as input for a linear classifier was mathematically proven to be incapable of learning a relation between words (Levy et al., 2015). What is the motivation behind using this model in the contextual setting?
While this handicap might be somewhat mitigated by adding similarity features, all these features are symmetric (including the Euclidean distance, since |L-R| = |R-L|). Why do we expect these features to detect entailment?
I am not convinced that this is a reasonable classification model for the task.
= Superiority Claims = The authors claim that their contextual representation is superior to context2vec. This is not evident from the paper, because: 1) The best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.
2) This experiment uses ready-made embeddings (GloVe) and parameters (context2vec) that were tuned on completely different datasets with very different sizes. Comparing the two is empirically flawed, and probably biased towards the method using GloVe (which was a trained on a much larger corpus).
In addition, it seems that the biggest boost in performance comes from adding similarity features and not from the proposed context representation. This is not discussed.
= Miscellaneous Comments = - I liked the WordNet dataset - using the example sentences is a nice trick.
- I don’t quite understand why the task of cross-lingual lexical entailment is interesting or even reasonable.
- Some basic baselines are really missing. Instead of the ""random"" baseline, how well does the ""all true"" baseline perform? What about the context-agnostic symmetric cosine similarity of the two target words?
- In general, the tables are very difficult to read. The caption should make the tables self-explanatory. Also, it is unclear what each variant means; perhaps a more precise description (in text) of each variant could help the reader understand?
- What are the PPDB-specific features? This is really unclear.
- I could not understand 8.1.
- Table 4 is overfull.
- In table 4, the F1 of ""random"" should be 0.25.
- Typo in line 462: should be ""Table 3"" = Author Response = Thank you for addressing my comments. Unfortunately, there are still some standing issues that prevent me from accepting this paper: - The problem I see with the base model is not that it is learning prototypical hypernyms, but that it's mathematically not able to learn a relation.
- It appears that we have a different reading of tables 3 and 4. Maybe this is a clarity issue, but it prevents me from understanding how the claim that contextual representations substantially improve performance is supported.
Furthermore, it seems like other factors (e.g. similarity features) have a greater effect.","- It appears that we have a different reading of tables 3 and 4. Maybe this is a clarity issue, but it prevents me from understanding how the claim that contextual representations substantially improve performance is supported. Furthermore, it seems like other factors (e.g. similarity features) have a greater effect.",41,0
ACL_2017_96_review,ACL_2017,"(1) Do not provide detailed statistics of constructed dataset.
(2) Integrating sentiment word clustering with machine translation techniques only is simple and straightforward, novelty may be a challenging issue. - General Discussion: Overall, this paper is well written. The experiments are conducted carefully and the analysis is reasonable. I offer some comments as follows.
(1) According to data collection process, each tweet should be annotated five times. How to determine which one is regarded as gold standard for measure performance?
(2) The MT technique (Moses) is well known, but it may not be a good baseline. Another MT technique (RNN) should be put together for comparison. (3) Differ from most work focuses on sarcasm detection. The research topic is interesting. It attempts to interpret sarcasm for reflecting semantics.","-General Discussion: Overall, this paper is well written. The experiments are conducted carefully and the analysis is reasonable. I offer some comments as follows. (1) According to data collection process, each tweet should be annotated five times. How to determine which one is regarded as gold standard for measure performance? (2) The MT technique (Moses) is well known, but it may not be a good baseline. Another MT technique (RNN) should be put together for comparison. (3) Differ from most work focuses on sarcasm detection. The research topic is interesting. It attempts to interpret sarcasm for reflecting semantics.",42,0
ACL_2017_726_review,ACL_2017,"/ clarifications: 1\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice of the length of span for querying the search engine. Why and how is it progressively reduced? ( line 333).
2\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback loop (algorithm 1) is *not* used for these experiments. If this is indeed the case, I'm not sure when does data augmentation occur. Is all the annotated training data augmented with paraphrases? When is the ""initial data"" from templates added? Is it also added to the gold training set? If so, I think it's not surprising that it doesn't help much, as the gold queries may be more diverse. In any case, I think this should be stated more clearly. In addition, I think it's interesting to see what's the performance of the ""vanilla"" model, without any augmentation, I think that this is not reported in the paper.
3\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear.
Does the accuracy measure the correctness of the execution of the query (i.e., the retrieved answer) as the text seem to indicate? ( Line 471 mentions *executing* the query). Alternatively, are the queries themselves compared? ( as seems to be the case for Dong and Lapata in Table 2). If this is done differently for different systems (I.e., Dong and Lapata), how are these numbers comparable? In addition, the text mentions the SQL model has ""slightly lower accuracy than the best non-SQL results"" (Line 515), yet in table 2 the difference is almost 9 points in accuracy. What is the observation based upon?
Was some significance test performed? If not, I think the results are still impressive for direct to SQL parsing, but that the wording should be changed, as the difference in performance does seem significant.
4\ Line 519 - Regarding the data recombination technique used in Jia and Liang (2016): Since this technique is applicable in this scenario, why not try it as well? Currently it's an open question whether this will actually improve performance. Is this left as future work, or is there something prohibiting the use of this technique?
5\ Section 6.2 (Three-stage online experiment) - several details are missing / unclear: - What was the technical background of the recruited users?
- Who were the crowd workers, how were they recruited and trained?
- The text says ""we recruited 10 new users and asked them to issue at least 10 utterances"". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in total (1 for each).
- What was the size of the initial (synthesized) training set? - Report statistics of the queries - some measure of their lexical variability / length / complexity of the generated SQL? This seems especially important for the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR uses SQL and NL, it would have been nice if it were attached to this submission, to allow its review during this period.
6\ Section 6.3 (SCHOLAR dataset) - The dataset seems pretty small in modern standards (816 utterances in total), while one of the main advantages of this process is its scalability. What hindered the creation of a much larger dataset?
- Comparing performance - is it possible to run another baseline on this newly created dataset to compare against the reported 67% accuracy obtained in this paper (line 730).
7\ Evaluation of interactive learning experiments (Section 6): I find the experiments to be somewhat hard to replicate as they involve manual queries of specific annotators. For example, who's to say if the annotators in the last phase just asked simpler questions? I realise that this is always problematic for online learning scenarios, but I think that an effort should be made towards an objective comparison. For starters, the statistics of the queries (as I mentioned earlier) is a readily available means to assess whether this happens. Second, maybe there can be some objective held out test set? This is problematic as the model relies on the seen queries, but scaling up the experiment (as I suggested above) might mitigate this risk. Third, is it possible to assess a different baseline using this online technique? I'm not sure whether this is applicable given that previous methods were not devised as online learning methods.
- Minor comments: 1\ Line 48 - ""requires"" -> ""require"" 2\ Footnote 1 seems too long to me. Consider moving some of its content to the body of the text.
3\ Algorithm 1: I'm not sure what ""new utterances"" refers to (I guess it's new queries from users?). I think that an accompanying caption to the algorithm would make the reading easier.
4\ Line 218 - ""Is is"" -> ""It is"" 5\ Line 278 mentions an ""anonymized"" utterance. This confused me at the first reading, and if I understand correctly it refers to the anonymization described later in 4.2. I think it would be better to forward reference this. - General Discussion: Overall, I like the paper, and given answers to the questions I raised above, would like to see it appear in the conference.
- Author Response: I appreciate the detailed response made by the authors, please include these details in a final version of the paper.","- Report statistics of the queries - some measure of their lexical variability / length / complexity of the generated SQL? This seems especially important for the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR uses SQL and NL, it would have been nice if it were attached to this submission, to allow its review during this period. 6\ Section 6.3 (SCHOLAR dataset) - The dataset seems pretty small in modern standards (816 utterances in total), while one of the main advantages of this process is its scalability. What hindered the creation of a much larger dataset?",43,0
ACL_2017_699_review,ACL_2017,"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc.
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).","4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.",44,0
ACL_2017_12_review,ACL_2017,"There are some underspecification in the paper that makes it difficult to reproduce the results. See below for details.
- General Discussion: - Section 4.1: what are there 5 seasons? What about things such as Ramadan month or Holiday Season?
- Section 5.1: ""two benchmark datasets"" => ""three datasets""?
- Section 5.2: an example without time token will be helpful.
- Section 5.2: given this approach is close to the ceiling of performance since 93% expressions contain time token, and the system has achieved 92% recall, how do you plan to improve further?
- Is there any plan to release the full set of rules/software used?",- Section 5.2: an example without time token will be helpful.,45,0
ACL_2017_130_review,ACL_2017,"The paper suffers from several drawbacks 1. The paper is hard to read due to incorrect usage of English. The current manuscript would benefit a lot from a review grammar and spellings.
2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100?
3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence?
4. The datsets are from a biomedical domain. No domain specific tools have been leveraged.
5. Since dataset class distribution is unclear, it is unclear to determine if accuracy is a good measure for evaluation. In either case, since it is a binary classification task, F1 would have been a desirable metric.
6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant.","6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant.",46,0
ACL_2017_727_review,ACL_2017,"Quantitative results are given only for the author's PSL model and not compared against any traditional baseline classification algorithms, making it unclear to what degree their model is necessary. Poor comparison with alternative approaches makes it difficult to know what to take away from the paper.
The qualitative investigation is interesting, but the chosen visualizations are difficult to make sense of and add little to the discussion. Perhaps it would make sense to collapse across individual politicians to create a clearer visual.
- General Discussion: The submission is well written and covers a topic which may be of interest to the ACL community. At the same time, it lacks proper quantitative baselines for comparison. Minor comments: - line 82: A year should be provided for the Boydstun et al. citation - It’s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.
- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).
- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if they used pre-trained embeddings.
- The authors give no intuition behind why unigrams are used to predict frames, while bigrams/trigrams are used to predict party.
- The authors note that temporal similarity worked best with one hour chunks, but make no mention of how important this assumption is to their results. If the authors are unable to provide full results for this work, it would still be worthwhile to give the reader a sense of what performance would look like if the time window were widened.
- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the “evaluation metrics” section on page 6.","- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).",47,0
ACL_2017_178_review,ACL_2017,"- The evaluation reported in this paper includes only intrinsic tasks, mainly on similarity/relatedness datasets. As the authors note, such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks. Accordingly, it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models.
- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.
- The medical concept evaluation dataset, ‘mini MayoSRS’ is extremely small (29 pairs), and its larger superset ‘MayoSRS’ is only a little larger (101 pairs) and was reported to have a relatively low human annotator agreement. The other medical concept evaluation dataset, ‘UMNSRS’, is more reasonable in size, but is based only on concepts that can be represented as single words, and were represented as such to the human annotators. This should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and general concepts. - As the authors themselves note, they (quite extensively) fine tune their hyperparameters on the very same datasets for which they report their results and compare them with prior work. This makes all the reported results and analyses questionable.
- The authors suggest that their method is superb to prior work, as it achieved comparable results while prior work required much more manual annotation. I don't think this argument is very strong because the authors also use large manually-constructed ontologies, and also because the manually annotated dataset used in prior work comes from existing clinical records that did not require dedicated annotations.
- In general, I was missing more useful insights into what is going on behind the reported numbers. The authors try to treat the relation between a phrase and its component words on one hand, and a concept and its alternative phrases on the other, as similar types of a compositional relation. However, they are different in nature and in my mind each deserves a dedicated analysis. For example, around line 588, I would expect an NLP analysis specific to the relation between phrases and their component words. Perhaps the reason for the reported behavior is dominant phrase headwords, etc. Another aspect that was absent but could strengthen the work, is an investigation of the effect of the hyperparameters that control the tradeoff between the atomic and compositional views of phrases and concepts.
General Discussion: Due to the above mentioned weaknesses, I recommend to reject this submission. I encourage the authors to consider improving their evaluation datasets and methodology before re-submitting this paper.
Minor comments: - Line 069: contexts -> concepts - Line 202: how are phrase overlaps handled?
- Line 220: I believe the dimensions should be |W| x d. Also, the terminology ‘negative sampling matrix’ is confusing as the model uses these embeddings to represent contexts in positive instances as well.
- Line 250: regarding ‘the observed phrase just completed’, it not clear to me how words are trained in the joint model. The text may imply that only the last words of a phrase are considered as target words, but that doesn’t make sense. - Notation in Equation 1 is confusing (using c instead of o) - Line 361: Pedersen et al 2007 is missing in the reference section.
- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) for human annotations.
- Line 430: The newly introduced term ‘strings’ here is confusing. I suggest to keep using ‘phrases’ instead.
- Line 496: Which task exactly was used for the hyper-parameter tuning?
That’s important. I couldn’t find that even in the appendix.
- Table 3: It’s hard to see trends here, for instance PM+CL behaves rather differently than either PM or CL alone. It would be interesting to see development set trends with respect to these hyper-parameters.
- Line 535: missing reference to Table 5.",- Line 430: The newly introduced term ‘strings’ here is confusing. I suggest to keep using ‘phrases’ instead.,48,0
ACL_2017_489_review,ACL_2017,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.","558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?",49,0
ACL_2017_128_review,ACL_2017,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.","- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?",50,0
ACL_2017_376_review,ACL_2017,"Many points are not explained well in the paper. - General Discussion: This work tackles an important and interesting event extraction problem -- identifying positive and negative interactions between pairs of countries in the world (or rather, between actors affiliated with countries). The primary contribution is an application of supervised, structured neural network models for sentence-level event/relation extraction. While previous work has examined tasks in the overall area, to my knowledge there has not been any publicly availble sentence-level annotated data for the problem -- the authors here make a contribution as well by annotating some data included with the submission; if it is released, it could be useful for future researchers in this area.
The proposed models -- which seem to be an application of various tree-structured recursive neural network models -- demonstrate a nice performance increase compared to a fairly convincing, broad set of baselines (if we are able to trust them; see below). The paper also presents a manual evaluation of the inferred time series from a news corpus which is nice to see.
I'm torn about this paper. The problem is a terrific one and the application of the recursive models seems like a contribution to this problem.
Unfortunately, many aspects of the models, experimentation, and evaluation are not explained very well. The same work, with a more carefully written paper, could be really great.
Some notes: - Baselines need more explanation. For example, the sentiment lexicon is not explained for the SVM. The LSTM classifier is left highly unspecified (L407-409) -- there are multiple different architectures to use an LSTM for classification. How was it trained? Is there a reference for the approach?
Are the authors using off-the-shelf code (in which case, please refer and cite, which would also make it easier for the reader to understand and replicate if necessary)? It would be impossible to replicate based on the two-line explanation here. - (The supplied code does not seem to include the baselines, just the recursive NN models. It's great the authors supplied code for part of the system so I don't want to penalize them for missing it -- but this is relevant since the paper itself has so few details on the baselines that they could not really be replicated based on the explanation in the paper.)
- How were the recursive NN models trained?
- The visualization section is only a minor contribution; there isn't really any innovation or findings about what works or doesn't work here.
Line by line: L97-99: Unclear. Why is this problem difficult? Compared to what? ( also the sentence is somewhat ungrammatical...) L231 - the trees are binarized, but how?
Footnote 2 -- ""the tensor version"" - needs citation to explain what's being referred to.
L314: How are non-state verbs defined? Does the definition of ""event word""s here come from any particular previous work that motivates it? Please refer to something appropriate or related.
Footnote 4: of course the collapsed form doesn't work, because the authors aren't using dependency labels -- the point of stanford collapsed form is to remove prepositions from the dependeny path and instead incorporate them into the labels.
L414: How are the CAMEO/TABARI categories mapped to positive and negative entries? Is performance sensitive to this mapping? It seems like a hard task (there are hundreds of those CAMEO categories....) Did the authors consider using the Goldstein scaling, which has been used in political science, as well as the cited work by O'Connor et al.? Or is it bad to use for some reason?
L400-401: what is the sentiment lexicon and why is it appropriate for the task?
L439-440: Not clear. ""We failed at finding an alpha meeting the requirements for the FT model."" What does that mean? What are the requirements? What did the authors do in their attempt to find it?
L447,L470: ""precision and recall values are based on NEG and POS classes"".
What does this mean? So there's a 3x3 contingency table of gold and predicted (POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and recall are calculated from this information.
5.1 aggregations: this seems fine though fairly ad-hoc. Is this temporal smoothing function a standard one? There's not much justification for it, especially given something simpler like a fixed window average could have been used.
5.2 visualizations: this seems pretty ad-hoc without much justification for the choices. The graph visualization shown does not seem to illustrate much.
Should also discuss related work in 2d spatial visualization of country-country relationships by Peter Hoff and Michael Ward.
5.3 L638-639: ""unions of countries"" isn't a well defined concept. mMybe the authors mean ""international organizations""?
L646-648: how were these 5 strong and 5 weak peaks selected? In particular, how were they chosen if there were more than 5 such peaks?
L680-683: This needs more examples or explanation of what it means to judge the polarity of a peak. What does it look like if the algorithm is wrong? How hard was this to assess? What was agreement rate if that can be judged?
L738-740: The authors claim Gerrish and O'Connor et al. have a different ""purpose and outputs"" than the authors' work. That's not right. Both those works try to do both (1) extract time series or other statistical information about the polarity of the relationships between countries, and *also* (2) extract topical keywords to explain aspects of the relationships. The paper here is only concerned with #1 and less concerned with #2, but certainly the previous work addresses #1. It's fine to not address #2 but this last sentence seems like a pretty odd statement.
That raises the question -- Gerrish and O'Connor both conduct evaluations with an external database of country relations developed in political science (""MID"", military interstate disputes). Why don't the authors of this work do this evaluation as well? There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.","- (The supplied code does not seem to include the baselines, just the recursive NN models. It's great the authors supplied code for part of the system so I don't want to penalize them for missing it -- but this is relevant since the paper itself has so few details on the baselines that they could not really be replicated based on the explanation in the paper.) - How were the recursive NN models trained?",51,0
ACL_2017_494_review,ACL_2017,"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors. - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding baselines. General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.
Minor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.
- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.
- Line 223: x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.
- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.
- I have read the author response",- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.,52,1
ACL_2017_56_review,ACL_2017,"This paper would benefit from a check by a native speaker of English, especially regarding the use of articles. The description of the similarity and analogy tasks comes at a strange place in the paper (4.1 Datasets). - General Discussion: As is done at some point well into the paper, it could be clarified from the start that this is simply a generalization of the original word2vec idea, redefining the word as an ngram (unigram) and then also using bigrams. It would be good to give a rationale why larger ngrams have not been used.
(I have read the author response.)","-General Discussion: As is done at some point well into the paper, it could be clarified from the start that this is simply a generalization of the original word2vec idea, redefining the word as an ngram (unigram) and then also using bigrams. It would be good to give a rationale why larger ngrams have not been used. (I have read the author response.)",53,0
ACL_2017_365_review,ACL_2017,"1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.
2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.
3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.
References: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.
Multi-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.
- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.","2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.",54,0
ACL_2017_266_review,ACL_2017,"Some conclusions are not fully backed up by the numerical results. E.g., the authors claim that for Catalan, the improvements of using specific corpora for training word vectors is more pronounced than English. I am not sure why this conclusion is made based on the results. E.g., in Table 6, none of the combination methods outperform the baseline for the 300-dimension vectors.
- General Discussion: The paper presents a set of simple, yet interesting experiments that suggest word vectors (here trained using the skip-gram method) largely benefit from the use of relevant (in-domain) and subjective corpora.
The paper answers important questions that are of benefit to practitioners of natural language processing. The paper is also very well-written, and very clearly organized.","-General Discussion: The paper presents a set of simple, yet interesting experiments that suggest word vectors (here trained using the skip-gram method) largely benefit from the use of relevant (in-domain) and subjective corpora. The paper answers important questions that are of benefit to practitioners of natural language processing. The paper is also very well-written, and very clearly organized.",55,1
ACL_2017_557_review,ACL_2017,"- The approach is incremental and seems like just a combination of existing methods. - The improvements on the performance (1.2 percent points on dev) are relatively small, and no significance test results are provided.
- General Discussion: - Major comments: - The model employed a recent parser and glove word embeddings. How did they affect the relation extraction performance?
- In prediction, how did the authors deal with illegal predictions?
- Minor comments: - Local optimization is not completely ""local"". It ""considers structural correspondences between incremental decisions,"" so this explanation in the introduction is misleading.
- Points in Figures 6 and 7 should be connected with straight lines, not curves.
- How are entities represented in ""-segment""?
- Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR, and Li et al. (2014) misses pages.",- The approach is incremental and seems like just a combination of existing methods.,56,0
ACL_2017_483_review,ACL_2017,"- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was performed on.
General Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with ""sequence-to-sequence models"". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not enough well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be appropriate.
- 590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.
- 745: This could alternatively show that structural cues are less important for this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).
[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection."" EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016.","- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing? This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?",57,0
ACL_2017_182_review,ACL_2017,"- nothing ground-breaking, application of existing technologies - code not available - results are as could be expected - General Discussion: - why didn't you use established audio features such as MFCCs?
- Minor Details: - L155 and other places: a LSTM -> an LSTM - L160, L216 and other Places: why are there hyphens (-) after the text?
- L205: explanation of convolution is not clear - Table1 should appear earlier, on page 2 already cited - L263: is 3D-CNN a standard approach in video processing? alternatives?
- L375, 378: the ^ should probably positioned above the y - L380: ""to check overfitting"" -> did you mean ""to avoid""?
- L403, 408..: put names in "" "" or write them italic, to make it easier to recognize them - L420: a SVM -> an SVM - L448: Output ... are -> wrong numerus, either ""Outputs"", or use ""is"" - L489: superflous whitespace after ""layer"" - L516, 519: ""concatenation"" should not be in a new line - L567: why don't you know the exact number of persons?
- L626: remove comma after Since - L651: doesnt -> does not - L777: insert ""hand, the"" after other - References: need some cleanup: L823 superflous whitespace, L831 Munich, L860 what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines, L956 indent Linguistics properly","- nothing ground-breaking, application of existing technologies - code not available - results are as could be expected -",58,0
ACL_2017_49_review,ACL_2017,"There are some minor points, listed as follows: 1) Figure 1: I am a bit surprised that the function words dominate the content ones in a Japanese sentence. Sorry I may not understand Japanese. 2) In all equations, sequences/vectors (like matrices) should be represented as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ... 3) Equation 12: s_j-1 instead of s_j.
4) Line 244: all encoder states should be referred to bidirectional RNN states.
5) Line 285: a bit confused about the phrase ""non-sequential information such as chunks"". Is chunk still sequential information???
6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k) to indicate the word in a chunk. 7) Some questions for the experiments: Table 1: source language statistics? For the baselines, why not running a baseline (without using any chunk information) instead of using (Li et al., 2016) baseline (|V_src| is different)? It would be easy to see the effect of chunk-based models. Did (Li et al., 2016) and other baselines use the same pre-processing and post-processing steps? Other baselines are not very comparable. After authors's response, I still think that (Li et al., 2016) baseline can be a reference but the baseline from the existing model should be shown. Figure 5: baseline result will be useful for comparison? chunks in the translated examples are generated *automatically* by the model or manually by the authors? Is it possible to compare the no. of chunks generated by the model and by the bunsetsu-chunking toolkit? In that case, the chunk information for Dev and Test in Table 1 will be required. BTW, the authors's response did not address my point here. 8) I am bit surprised about the beam size 20 used in the decoding process. I suppose large beam size is likely to make the model prefer shorter generated sentences. 9) Past tenses should be used in the experiments, e.g., Line 558: We *use* (used) ... Line 579-584: we *perform* (performed) ... *use* (used) ... ... - General Discussion: Overall, this is a solid work - the first one tackling the chunk-based NMT; and it well deserves a slot at ACL.",4) Line 244: all encoder states should be referred to bidirectional RNN states.,59,0
ACL_2017_56_review,ACL_2017,"- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.
- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.
- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.
- General Discussion: This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.
I have read the author response. I look forward to seeing the revised version of the paper.","- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.",60,0
ACL_2017_636_review,ACL_2017,"- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly. - Section 4.1 could be clearer. For example, I presume there is padding to make sure the output resolution after each block is the same as the input resolution. Might be good to mention this. - I think an ablation study of number of layers vs perf might be interesting.
RESPONSE TO AUTHOR REBUTTAL: Thank you very much for a thoughtful response. Given that the authors have agreed to make the content be more specific to NER as opposed to sequence-tagging, I have revised my score upward.",- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly.,61,0
ACL_2017_128_review,ACL_2017,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.","- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters. Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.",62,0
ACL_2017_12_review,ACL_2017,"There are some underspecification in the paper that makes it difficult to reproduce the results. See below for details.
- General Discussion: - Section 4.1: what are there 5 seasons? What about things such as Ramadan month or Holiday Season?
- Section 5.1: ""two benchmark datasets"" => ""three datasets""?
- Section 5.2: an example without time token will be helpful.
- Section 5.2: given this approach is close to the ceiling of performance since 93% expressions contain time token, and the system has achieved 92% recall, how do you plan to improve further?
- Is there any plan to release the full set of rules/software used?",-General Discussion:- Section 4.1: what are there 5 seasons? What about things such as Ramadan month or Holiday Season?,63,0
ACL_2017_31_review,ACL_2017,"] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.
- No detailed error analysis is provided.
- A feature comparison with prior work is shallow, missing two relevant papers.
- The paper has several obscure descriptions, including typos.
[General Discussion:] The paper would be more impactful if it states novelties more explicitly. Is the paper presenting the first neural network based approach for event factuality identification? If this is the case, please state that.
The paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of Table 3 and 4. What are dominant sources of errors made by the best system BiLSTM+CNN(Att)? What impacts do errors in basic factor extraction (Table 3) have on the overall performance of factuality identification (Table 4)? The analysis presented in Section 5.4 is more like a feature ablation study to show how useful some additional features are.
The paper would be stronger if it compares with prior work in terms of features. Does the paper use any new features which have not been explored before? In other words, it is unclear whether main advantages of the proposed system come purely from deep learning, or from a combination of neural networks and some new unexplored features. As for feature comparison, the paper is missing two relevant papers: - Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.
- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.
Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.
The paper would be more understandable if more examples are given to illustrate the underspecified modality (U) and the underspecified polarity (u). There are two reasons for that. First, the definition of 'underspecified' is relatively unintuitive as compared to other classes such as 'probable' or 'positive'.
Second, the examples would be more helpful to understand the difficulties of Uu detection reported in line 690-697. Among the seven examples (S1-S7), only S7 corresponds to Uu, and its explanation is quite limited to illustrate the difficulties.
A minor comment is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by: - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*; - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.
- The title of Section 3 ('Baseline') is misleading. A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.
- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.
- Table 2 seems to show factuality statistics only for all sources. The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.
- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.
- Section 4.1 says, ""Aux_Words can describe the *syntactic* structures of sentences,"" whereas section 5.4 says, ""they (auxiliary words) can reflect the *pragmatic* structures of sentences."" These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.
- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6. What is the auxiliary word for 'ensure' in S7?
- Line 162: 'event go in S1' should be 'event go in S2'.
- Line 315: 'in details' should be 'in detail'.
- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.
- Line 771: 'recent researches' should be 'recent research' or 'recent studies'. 'Research' is an uncountable noun.
- Line 903: 'Factbank' should be 'FactBank'.","- Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer.",64,1
ACL_2017_792_review,ACL_2017,"1. Unfortunately, the results are rather inconsistent and one is not left entirely convinced that the proposed models are better than the alternatives, especially given the added complexity. Negative results are fine, but there is insufficient analysis to learn from them. Moreover, no results are reported on the word analogy task, besides being told that the proposed models were not competitive - this could have been interesting and analyzed further.
2. Some aspects of the experimental setup were unclear or poorly motivated, for instance w.r.t. to corpora and datasets (see details below).
3. Unfortunately, the quality of the paper deteriorates towards the end and the reader is left a little disappointed, not only w.r.t. to the results but with the quality of the presentation and the argumentation.
- General Discussion: 1. The authors aim ""to learn representations for both words and senses in a shared emerging space"". This is only done in the LSTMEmbed_SW version, which rather consisently performs worse than the alternatives. In any case, what is the motivation for learning representations for words and senses in a shared semantic space? This is not entirely clear and never really discussed in the paper.
2. The motivation for, or intuition behind, predicting pre-trained embeddings is not explicitly stated. Also, are the pre-trained embeddings in the LSTMEmbed_SW model representations for words or senses, or is a sum of these used again? If different alternatives are possible, which setup is used in the experiments?
3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context.
4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated.
5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel, which makes comparisons problematic, in this case giving three ""wins"" as opposed to one.
6. The proposed models are said to be faster to train by using pre-trained embeddings in the output layer. However, no evidence to support this claim is provided. This would strengthen the paper.
7. Table 4: why not use the same dimensionality for a fair(er) comparison?
8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached.
9. A reference to Table 2 is missing.
10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset.","3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context.",65,0
ACL_2017_31_review,ACL_2017,"] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.
- No detailed error analysis is provided.
- A feature comparison with prior work is shallow, missing two relevant papers.
- The paper has several obscure descriptions, including typos.
[General Discussion:] The paper would be more impactful if it states novelties more explicitly. Is the paper presenting the first neural network based approach for event factuality identification? If this is the case, please state that.
The paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of Table 3 and 4. What are dominant sources of errors made by the best system BiLSTM+CNN(Att)? What impacts do errors in basic factor extraction (Table 3) have on the overall performance of factuality identification (Table 4)? The analysis presented in Section 5.4 is more like a feature ablation study to show how useful some additional features are.
The paper would be stronger if it compares with prior work in terms of features. Does the paper use any new features which have not been explored before? In other words, it is unclear whether main advantages of the proposed system come purely from deep learning, or from a combination of neural networks and some new unexplored features. As for feature comparison, the paper is missing two relevant papers: - Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.
- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.
Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.
The paper would be more understandable if more examples are given to illustrate the underspecified modality (U) and the underspecified polarity (u). There are two reasons for that. First, the definition of 'underspecified' is relatively unintuitive as compared to other classes such as 'probable' or 'positive'.
Second, the examples would be more helpful to understand the difficulties of Uu detection reported in line 690-697. Among the seven examples (S1-S7), only S7 corresponds to Uu, and its explanation is quite limited to illustrate the difficulties.
A minor comment is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by: - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*; - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.
- The title of Section 3 ('Baseline') is misleading. A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.
- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.
- Table 2 seems to show factuality statistics only for all sources. The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.
- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.
- Section 4.1 says, ""Aux_Words can describe the *syntactic* structures of sentences,"" whereas section 5.4 says, ""they (auxiliary words) can reflect the *pragmatic* structures of sentences."" These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.
- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6. What is the auxiliary word for 'ensure' in S7?
- Line 162: 'event go in S1' should be 'event go in S2'.
- Line 315: 'in details' should be 'in detail'.
- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.
- Line 771: 'recent researches' should be 'recent research' or 'recent studies'. 'Research' is an uncountable noun.
- Line 903: 'Factbank' should be 'FactBank'.",- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.,66,0
ACL_2017_516_review,ACL_2017,"The paper is fairly limited in scope in terms of the interactive topic model approaches it compares against. I am willing to accept this, since they do make reference to most of them and explain that these other approaches are not necessarily fast enough for interactive experimentation or not conducive to the types of interaction being considered with an ""anchoring"" interface. Some level of empirical support for these claims would have been nice, though.
It would also have been nice to see experiments on more than one data set (20 newsgroups, which is now sort of beaten-to-death).
- General Discussion: In general, this is a strong paper that appears to offer an incremental but novel and practical contribution to interactive topic modeling. The authors made the effort to vet several variants of the approach in simulated experiments, and to conduct fairly exhaustive quantitative analyses of both simulated and user experiments using a variety of metrics that measure different facets of topic quality.","-General Discussion: In general, this is a strong paper that appears to offer an incremental but novel and practical contribution to interactive topic modeling. The authors made the effort to vet several variants of the approach in simulated experiments, and to conduct fairly exhaustive quantitative analyses of both simulated and user experiments using a variety of metrics that measure different facets of topic quality.",67,1
ACL_2017_318_review,ACL_2017,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?,68,0
ACL_2017_433_review,ACL_2017,"- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement. UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.
- General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.
- Questions for the authors: - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.
- Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?
- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?
- Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.
- Low-level comments: - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.
- You use the word ""grammar"" in a slightly strange way. I think replacing ""grammar"" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90) - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.
- Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with ""very high performing model"" or something like that.
- It would be helpful if you provided glosses in Figure 2.",- Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.,69,0
ACL_2017_501_review,ACL_2017,"The experiments are missing a key baseline: a state-of-the-art VQA model trained with only a yes/no label vocabulary. I would have liked more details on the human performance experiments. How many of the ~20% of incorrectly-predicted images are because the captions are genuinely ambiguous? Could the data be further cleaned up to yield an even higher human accuracy?
- General Discussion: My concern with this paper is that the data set may prove to be easy or gameable in some way. The authors can address this concern by running a suite of strong baselines on their data set and demonstrating their accuracies. I'm not convinced by the current set of experiments because the chosen neural network architectures appear quite different from the state-of-the-art architectures in similar tasks, which typically rely on attention mechanisms over the image.
Another nice addition to this paper would be an analysis of the data set. How many tokens does the correct caption share with distractors on average? What kind of understanding is necessary to distinguish between the correct and incorrect captions? I think this kind of analysis really helps the reader understand why this task is worthwhile relative to the many other similar tasks. The data generation technique is quite simple and wouldn't really qualify as a significant contribution, unless it worked surprisingly well.
- Notes I couldn't find a description of the FFNN architecture in either the paper or the supplementary material. It looks like some kind of convolutional network over the tokens, but the details are very unclear. I'm also confused about how the Veq2Seq+FFNN model is applied to both classification and caption generation. Is the loglikelihood of the caption combined with the FFNN prediction during classification? Is the FFNN score incorporated during caption generation?
The fact that the caption generation model performs (statistically significantly) *worse* than random chance needs some explanation. How is this possible?
528 - this description of the neural network is hard to understand. The final paragraph of the section makes it clear, however. Consider starting the section with it.","- Notes I couldn't find a description of the FFNN architecture in either the paper or the supplementary material. It looks like some kind of convolutional network over the tokens, but the details are very unclear. I'm also confused about how the Veq2Seq+FFNN model is applied to both classification and caption generation. Is the loglikelihood of the caption combined with the FFNN prediction during classification? Is the FFNN score incorporated during caption generation? The fact that the caption generation model performs (statistically significantly) *worse* than random chance needs some explanation. How is this possible?",70,0
ACL_2017_727_review,ACL_2017,"1) no method adapted from related work for a result comparison 2) some explanations about the uniqueness of the task and discussion on limitations of previous research for solving this problem can be added to emphasize the research contributions further. - General Discussion: The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models. 17 political frames are classified in tweets in a multi-label classification task. The experimental results demonstrate the benefit of the predicates created using the behavior-based signals. Please find my more specific comments below: The paper should have a discussion on how frame classification differs from stance classification. Are they both under the same umbrella but with different levels of granularity?
The paper will benefit from adding a brief discussion on how exactly the transition from long congressional speech to short tweets adds to the challenges of the task. For example, does past research rely on any specific cross-sentential features that do not apply to tweets? Consider adapting the method of a frame classification work on congressional speech (or a stance classification work on any text) to the extent possible due to its limitations on Twitter data, to compare with the results of this work.
It seems “weakly supervised” and “unsupervised” – these two terms have been interchangeably used in the paper (if this is not the case, please clarify in author response). I believe ""weakly supervised"" is the more technically correct terminology under the setup of this work that should be used consistently throughout. The initial unlabeled data may not have been labeled by human annotators, but the classification does use weak or noisy labels of some sort, and the keywords do come from experts. The presented method does not use completely unsupervised data as traditional unsupervised methods such as clustering, topic models or word embeddings would. The calculated Kappa may not be a straightforward reflection of the difficulty of frame classification for tweets (lines: 252-253), viewing it as a proof is a rather strong claim. The Kappa here merely represents the annotation difficulty/disagreement. Many factors can contribute to a low value such as poorly written annotation guidelines, selection of a biased annotator, lack of annotator training etc.
(on top of any difficulty of frame classification for tweets by human annotators, which the authors actually intend to relate to).
73.4% Cohen’s Kappa is strong enough for this task, in my opinion, to rely on the annotated labels. Eq (1) (lines: 375-377) will ignore any contextual information (such as negation or conditional/hypothetical statements impacting the contributing word) when calculating similarity of a frame and a tweet. Will this have any effect on the frame prediction model? Did the authors consider using models that can determine similarity with larger text units such as perhaps using skip thought vectors or vector compositionality methods? An ideal set up would exclude the annotated data from calculating statistics used to select the top N bi/tri-grams (line: 397 mentions entire tweets data set has been used), otherwise statistics from any test fold (or labeled data in the weakly supervised setup) still leaks into the selection process. I do not think this would have made any difference in the current selection of the bi/tri-grams or results as the size of the unlabeled data is much larger, but would have constituted a cleaner experimental setup. Please add precision and recall results in Table 4. Minor: please double check any rules for footnote placements concerning placement before or after the punctuation.","-General Discussion: The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models.",71,1
ARR_2022_285_review,ARR_2022,"The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below.
Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue.
The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)
- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much. - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?
- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.
- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting ""matched"" for ""not matched"" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions.","- It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?",72,0
ARR_2022_169_review,ARR_2022,"1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure ""unlabelled"" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels. 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1. 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples? 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT. 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that ""it would be beneficial for the LD to be trained with examples containing events"". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training.
1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043.
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method.
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT.
4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance?
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern.","6. It is claimed in lines 128-132 that ""it would be beneficial for the LD to be trained with examples containing events"". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.",73,0
ARR_2022_123_review,ARR_2022,"1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable.
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied.
1) It's better to adopt experiment settings consistent with previous work.
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved.
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021.","2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved.",74,0
ARR_2022_214_review,ARR_2022,"1.Adding the random selection is not a new idea. The randomness of the layer selection could cause the unstable of performance. It is better to show more results such as the convergence analysis. Random selection might not be the best option here.
2.The performance is not convincing. The improvements under some settings are not significant. 3.The new approach could not find the best subset (subset mapping) since the proposed model just selects m layers randomly. But it is not clear if we should ignore it or solve it later. From my opinion, the attention-based approaches with best mapping search may achieve better performance.
The whole approach lacks the evidences to prove the usefulness. Comparing to ALP-KD, the improvement is not significant.","3.The new approach could not find the best subset (subset mapping) since the proposed model just selects m layers randomly. But it is not clear if we should ignore it or solve it later. From my opinion, the attention-based approaches with best mapping search may achieve better performance. The whole approach lacks the evidences to prove the usefulness. Comparing to ALP-KD, the improvement is not significant.",75,0
ARR_2022_324_review,ARR_2022,"- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?
- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process).
- In lines 198-220, explanation of $\phi$, $\psi$ and $\pi$ is not clear. Can they be better explained or incorporated in Figure 2?
- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.
- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper.","- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper.",76,0
ARR_2022_331_review,ARR_2022,"Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?
2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity. 3) Briefly discussing what kinds of data augmentation techniques ( ♥ ) other works use in Table 4 would be great (and maybe why not used in this paper).
Line 17: add a space ""datasets. We"" --> ""datasets. We""",2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.,77,0
ARR_2022_142_review,ARR_2022,"My only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page.
- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as ""Description"" or ""Symbolism"". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?
- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer’s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!
- L275: this inclusion criteria -> these inclusion criteria - L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader.","- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer’s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!",78,1
ARR_2022_235_review,ARR_2022,"- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings.
- I would recommend to clarify notation when it comes to GPT-2. There's the GPT-2 architecture and the GPT-2 pre-trained language model. Saying that CLIP uses GPT-2 as it's language model (line 84) might lead readers to assume it uses the pre-trained GPT-2. Which is not the case. It will be helpful if the authors are more explicit and consistent when referring to the CLIP language encoder.
- In lines 507-514 authors talk about how CWEs lose mutual information with the the input word and cite Voita et al (2019). Additionally, they argue how this might not be the case for the CLIP [EOS] token. I'm not sure if the finding of Voita et al (2019) applies to the CLIP representations as no language modeling objective is used during CLIP training. So mutual information with the input might be higher for all tokens, not just the [EOS] token. This is something that could be checked.",- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings.,79,0
ARR_2022_329_review,ARR_2022,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case.
General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice. 2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others. 3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format) - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps. - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus? - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in",- Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.,80,0
ARR_2022_333_review,ARR_2022,"There some parts left unclear during the compilation process. Please see below.
Also the dataset submitted as supplementary material to the paper seems to be a small part of the whole dataset.
My comments are as follows: - How did you decide the number of classes (10 for coarse-grained and 104 for fine-grained) and how did you decide these classes?
- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.
- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.
- Another related issue is, if the dataset was annotated on a syllable basis, how useful will be such an annotation for NLP tasks? For instance, relation extraction, where we usually extract the NEs and the find relations between them. By using syllable annotations, how can we find word NEs? Using a segmenter or tokenizer?
- How is the annotation on Thai NER datasets, on a syllable-basis or word-basis, and why?
- It is stated that the dataset includes 4,894 documents. In NER datasets, it is better to state the size of the dataset with the number of sentences. How many sentences does the dataset include?
- Were these 4,894 documents selected randomly?",- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.,81,0
ARR_2022_198_review,ARR_2022,"(1) It is claimed that this paper “propose a task”, “reframe a research question”. It thus seems to suggest that this work addresses temporal grounding as a *new* task. However, information about its utility and scenarios of applicability is not really elaborated (just a generic sentence at l.60) and are rather left to the reader to imagine. Also, the related work is presented as more of a description of previous studies rather than a critical “mapping”. As a consequence, it is hard to situate the proposed methods and insights and how they differ from/ and contribute to the topic. I would suggest being more specific and explicit about its contributions and how it relates to previous work. (2) I see some issues with the proposed methods and evaluation: 2a) Figure 2 shows how different annotators (for the same language) referred to different time ranges for the same part of the day. In fact, Hindi annotations suggest that night can span till 9 am, thus overlapping with the time range for the morning, too. Then, at lines 195-99, the extractive approach is defined as to infer *non-overlapping* time ranges. Finally, in Figure 3, the gold standard annotations are represented as non-overlapping time spans among parts of the day. It is however unmentioned how a clear-cut distinction across parts of the day with definite time spans was defined to create such a final benchmark. 2b) Section 3.1: the extractive corpus-based approach relies on *English* Wikipedia data. No information about the number of time expressions needed is however provided. Also, I wonder whether by relying on English time expressions -- which are then automatically translated into other languages -- a strong western bias is induced as well as translation errors: was there someone with the needed language competence to verify the soundness of such translations? This part should be better documented, for instance in the supplementary material.
2c) The translation error issue presents itself also in section 3.2. Footnote number 5 mentions that BERT queries are based on the template “The <morning> starts at <9.00>"", and that such sentences are automatically translated into other languages. In a language with grammatical gender such as Italian, however, the translation of “the” would vary depending on the mentioned part of the day (e.g. the afternoon: IL pomeriggio; the morning: LA mattina). Is this something that has been adapted? If not could it have skewed BERT predictions?
2d) In section 4.2. the proposed approaches are compared against a baseline based on the “greeting method” by Vilares and Gόmez-Rόdriguez (2018). Their work however does not really seem to serve well as a baseline since it does not entail a *method*. Rather, they created a corpus to conduct an analysis on the semantics of greetings. Thus their goal is different from the one pursued in this paper, and their corpus here is simply employed to test the extractive method (3.1) on a different source of data. (3) Although the paper is reasonably well written and pleasant to read, some turns are taken for granted and not really justified. For instance, the analysis in section 5 seems to concern the 24 unlabeled languages, this is however left unstated. Also, for instance, in 5.1 the analysis is carried out for Italian only, but the choice for such a decision is not given.
As a general comment, I find that this paper seems to be a bit scattered, or better, it alternatively shifts from a culturally-situated focus to the strive for “language-agnostic” approaches. Besides the fact that I am having some doubt as to whether such approaches grant the definition of language-independent method (it does not seem to be very effective on all languages, it largely relies on *English* data and concepts, e.g. noon), I think that such shifts bring the paper out of focus at times and may affect its narrative. Note that this is more more of an opinionated point of view (which could be discarded and I am thus not putting in the weaknesses section), but I think the paper should enhance the former, linguistically and culturally grounded perspective, which in my opinion is also the main strength of this work. Few minor suggestions: - footnote1. move it at line 58 - footnote2: specify HIT acronym - the related work would be maybe put to better use after the introduction Finally, as per https://2021.aclweb.org/ethics/Ethics-review-questions/, information about the compensation of annotators via Amazon Mechanical Turk has to be provided.","- footnote1. move it at line 58 - footnote2: specify HIT acronym - the related work would be maybe put to better use after the introduction Finally, as per https://2021.aclweb.org/ethics/Ethics-review-questions/, information about the compensation of annotators via Amazon Mechanical Turk has to be provided.",82,0
ARR_2022_91_review,ARR_2022,"- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation). - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.
[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering
- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)?","- Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.",83,0
ARR_2022_347_review,ARR_2022,"1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the ""time"" dimension, it will be less favorable than ""FLOPs."" So I'd be happy if the authors could also provide these figures in the ""time"" dimension.
2. Some of the names seem not consistently referred to in the text. See ""comments, suggestions and typos.""
1. If I understand correctly, in Line 283, there seem to be typos on the operators in the equation -F(x;f) < t = log(...).
2. There is an inconsistency in the definition of ""logits"" between Line 251 (where f_y(x) could negative, and are regarded as logits) and Line 391 (where logits is the normalized conditional probability of the i-th class.)","1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the ""time"" dimension, it will be less favorable than ""FLOPs."" So I'd be happy if the authors could also provide these figures in the ""time"" dimension.",84,0
ARR_2022_319_review,ARR_2022,"1 The paper poses mostly subjective points which are themselves largely not novel or are flawed in themselves. It starts with the emphasized sentence on page 1 attributed to Jacovi et al which I will call ""Opinion A"": ""As an explanation method, the evaluation criteria of attribution methods should be how accurately it reflects the true reasoning process of the model (faithfulness), not how convincing it is to humans (plausibility)"" This point is debatable. Whether attributions should be purely faithful to model behaviour or offer human-interpretability is a decision to be made in the definition of an attribution method or its motivation. Neither option is wrong as long as the method is applied to its intended use case. Further, the noted choice of faithfulness has implications on some of the ""logic traps"" described subsequently. I elaborate on each in the comments section below.
Significant positioning fixes need to be implemented in this work starting with Opinion A. Either arguments for faithfulness need to be made or the paper needs to restrict itself to attributions whose primary concern is faithfulness. The latter option would have to be written from the perspective that faithfulness is an apriori goal.
The novelty of arguments being made needs to be addressed as well. I find that the cores of the ""logic traps"" are all known points and are often well considered in attribution, evaluation, and robustness work. Presently the paper does not demonstrate that (logic traps) ""in existing evaluation methods have been ignored for a long time"". If there is a gap in some segment of the community, a survey or systemization paper where applicable would be more appropriate.
2 Arguments and experiments with regard to ""model reasoning process"" are vague in key definitions and thus non-convincing. Specifically, no definition of ""model reasoning process"" is given.
Experiment outlined in Figure 7 argues that extractor's bits are equivalent to ""model reasoning process"" but this is arguable. The extractor can derive the same bits using different means or different bits using very similar means. While I do not disagree with the main points regarding model reasoning process and robustness, I do not think the experiments demonstrate them.
Detailed comments regarding Weakness 1): * Logic Trap 1: The point being made is obvious: ""The decision-making process of neural networks is not equal to the decision-making process of humans.""
The example Experiment 1 incorporates not a single attribution method. Is it presumed that all attributions will be wrong as there is no human explanation possible? Can we also say that any human explanation would be wrong for the same reason? If there is no ground truth, how can it be wrong for an attribution method to say anything?
Regardless, no-one *expects* models to fully replicate the reasoning of a human. Replicating human behaviour may be useful to gauge human-interpretability of explanations but this is precluded by Opinion A. Finally, it is well understood that what is correct to a human may not be correct in a model as per faithfulness vs. explainability discussion which is had alongside attribution evaluations in literature and in Opinion A (some examples from cited works below).
* Logic Trap 2: The second point made is a roundabout way of saying that ablation orders are varied: ""Using an attribution method as the ground truth to evaluate the target attribution method.""
The authors are rightly pointing out that numerous forms of attribution evaluation based on ablation or reconstruction inputs have been used to motivate attribution methods. Here the opposite conclusion to Opinion A is helpful. Expecting humans to interpret an attribution one way may lead to one ablation order whereas another form of interpretation may lead to another. There is no single correct metric because there is no single interpretation. This point has been made at least in [a].
* Logic Trap 3: The third point is known: ""The change in attribution scores maybe because the model reasoning process is really changed rather than the attribution method is unreliable.""
Ghorbani et al. (2019) note in their concluding discussion that fragility of attribution is reflective of fragility of the model (and that their attacks have not ""broken"" an attribution).
Likewise Alvarez-Melis et al. (2018) point out that the non-robust artifacts in attributions they discover are actually reasonable if faithfulness to model is the only goal of an attribution.
Thus Logic Trap 3 does not seem to add anything beyond Opinion A. * 3.1 -- ""Attacking attribution methods by replacing the target model.""
This section seems to be pointing out that attribution methods have a problem of domain faithfulness in that they often internally apply a model to inputs that they have not been trained on or don't expect to operate reliably on.
* 3.2 -- ""Should We Use Attributions Methods in a Black-Box Way?""
The paper argues that black-box is not a worthwhile goal. This is again highly subjective as there are several reasons, despite presented arguments, to prefer black-box methods, like 1) having explanations of the semantics of what a model is modeling instead of an explanation tainted by how the model is implemented, 2) black-box means model agnostic, hence can apply to any model structure, 3) some scenarios just do not have access to the model internals.
Other comments: - The term ""logic traps"" is not define or explained. Dictionaries and reference materials equate them to logical fallacies which I'm unsure is the intended meaning here.
- The term ""erasure-based"" is used in the intro but later the term ""ablation"" is used.
- Typo/word choice near ""with none word overlap"".
- Typo/grammar near ""there are works (...) disprove"" - Suggestion of 3.3, point 2 is unclear. Adversarial examples can be highly confident and I suspect means of incorporating confidence can themselves be subverted adversarially.
- Figure 6 is not useful. I presume the paths are supposed to be indicative of model behaviour but as mentioned in earlier comments, defining it is a crucial problem in explanation work. The Figure is suggestive of it being a triviality.
- Several points in the paper use phrases like ""A lot of works ..."" or ""most existing methods ..."".
I think it would be more appropriate to list the works or methods instead of noting the relative abundance.
- There are some inconsistencies in the notation for AOPC and the k parameter with potential related typos in its definition.
- Potential grammar issue near ""can be seen as an attribution"".
- Grammar issue near ""results are mainly depend"" - Grammar issue near ""achieve 86.4% accuracy"" - Where is footnote 1?
- In Figure 4, the resulting attributions for the target word ""good"" in LOO, Marg are not presented or indicated by color.
References from comments: [a] - Wang et al. ""Interpreting Interpretations: Organizing Attribution Methods by Criteria"".","2 Arguments and experiments with regard to ""model reasoning process"" are vague in key definitions and thus non-convincing. Specifically, no definition of ""model reasoning process"" is given. Experiment outlined in Figure 7 argues that extractor's bits are equivalent to ""model reasoning process"" but this is arguable. The extractor can derive the same bits using different means or different bits using very similar means. While I do not disagree with the main points regarding model reasoning process and robustness, I do not think the experiments demonstrate them. Detailed comments regarding Weakness",85,0
ARR_2022_97_review,ARR_2022,"- Several important aspects of the approach are underspecified: How are the meaning representations segmented into individual facts (Section 2.1)? What are the inputs/outputs to each stage of the system (Sections 2.2 to 2.4)? How are the lexicalization, aggregation and post-editing tasks performed (e.g., is this just a T5_small model)?
- The development and exact form of the loss function (Eq 1) is unclear.
- The label inference process seems spurious (Section 2.6). Some examples and statistics would be helpful.
The work seems to be presented as an engineering task and provides limited insight into the fundaments of how decomposing the task help language models to consume meaning representations to generate text. In particular, exploring the potential relationship between lexicalization and missing slot error would have made for a much stronger paper. Also the level of specificity in Section 2 obscured the exact nature of the approach.
The terms s_x and v_x are not clearly defined. Are these slots and value bindings, respectively? If so, how are is each derived?",- The development and exact form of the loss function (Eq 1) is unclear.,86,0
ARR_2022_88_review,ARR_2022,"- While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold. - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors.
Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline.
1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right?
2. It would have been useful if in Table 5, examples of the templates would have also been given.","2. It would have been useful if in Table 5, examples of the templates would have also been given.",87,0
ARR_2022_130_review,ARR_2022,"- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me.
* If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting.
* If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small.
* The sizes are Train: 6465, Val: 646, Test: 646.
* The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test.
* Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model.
#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world","* If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.",88,1
ARR_2022_81_review,ARR_2022,"1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements. This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines?
2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509.
3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning.
4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required.
1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}: 4 should be x1 x2 y1 y2, and this should be stated. Table 3: the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough.
2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work.
3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since ""DVD"" seems to distract readers from its real meaning.","3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since ""DVD"" seems to distract readers from its real meaning.",89,0
ARR_2022_334_review,ARR_2022,"- There are some technical aspects of the paper that weren't clear to me: * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
* L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer?
* It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
* Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
* Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow.
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?)","* It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?",90,0
ARR_2022_28_review,ARR_2022,"The main concerns with this paper is that it doesn't fully explain some choices in the model (see comments/questions section). Moreover, some parts of the paper are actually not fully clear. Finally, some details are missing, making the paper incomplete.
- Algorithm 1 is not really explained. For example, at each step (1, 2, 2a, 3, 3a) are you sampling a different batch from S and T? Is the notation L(X) meaning that you optimize only the parameters X of the architecture?
- Line 232: When you say you ""mine"", what do you exactly mean? Does this mean you sample P sentences from the set of sentences of S and T with similar constraints?
- Lines 237-238 and Line 262: Why would you want to use the representation from the critic last layer? - Line 239: ""Ci are a set of constraints for a sentence"" should be moved before.
- Table 1: It seems that the results for DRG and ARAE are not averaged over 5 runs (they're exactly the same of the previous paper version) - Table 1: How did you choose the p=0.6?
- Table 1: row=ARAE, column=POLITICAL-FL It seems this value should be the one in bold.
- Lines 349-353: It seems you're comparing results for ARAE + CONTRA, ARAE + CLF and ARAE + CONTRA + CL with respect to simple ARAE, while in the text you mention only ARAE + CONTRA and ARAE + CLF.
- Line 361: and SIM to -> and SIM with respect to - Figure 3: Please, rephrase the caption of the errors bars (or explain it in the text). It is not clear what do you mean.
- Line 389: You mention here you used different p values as in Table 1. This table doesn't report results with different values for p. - Lines 422-423: why using nucleous sampling when the best results were with greedy decoding? Where does 0.9 come from?
- In general, in the experiments, what are the source and target domains?
- Line 426-Table4: What do you want to demonstrate here? Could you add an explanation? What constraints/attributes are preserved? What is the source domain? What is the target domain?
- Lines 559-560: This is not entirely true. In Cycle Consistency loss you can iterate between two phases of the reconstructions (A-B-A and B-A-B) with two separate standard backpropagation processes.
- Line 573: works focuses -> works focus",- Line 389: You mention here you used different p values as in Table 1. This table doesn't report results with different values for p.,91,0
ARR_2022_19_review,ARR_2022,"- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only.
- Including an example created by BERT and comparing them with one created by the proposed model may be helpful.",- Including an example created by BERT and comparing them with one created by the proposed model may be helpful.,92,0
ARR_2022_24_review,ARR_2022,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10?
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights?
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer?
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2?
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes.
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"".
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders.
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.","- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?",93,0
ARR_2022_19_review,ARR_2022,"- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only.
- Including an example created by BERT and comparing them with one created by the proposed model may be helpful.","- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only.",94,0
ARR_2022_324_review,ARR_2022,"Despite its strengths, there are also several areas that offer opportunities for improvement in future revisions: - In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward). Revising this section could benefit the overall clarity of findings and subsequent takeaway points.
- Some aspects of the dataset are unclear. For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?
- Some of the presented results seem very close, particularly in the ablation study. It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings.
Two typos that could be addressed: - Line 278: The word *the* is repeated - Line 321-323: The text seems to indicate that $p_{\theta}(Z^{p})$ is shorthand for $p_{\theta}(Z^{p})$. Should the second instance of this be something else?","- In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward). Revising this section could benefit the overall clarity of findings and subsequent takeaway points.",95,0
ARR_2022_152_review,ARR_2022,"Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.
Besides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point.
There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.
- line 151: why the length of R is not equal to the one of U?
- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one",- line 080: incorrect use of quotation marks and repeated quotation marks.,96,0
ARR_2022_330_review,ARR_2022,"- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results.
- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained.",- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.,97,0
ARR_2022_39_review,ARR_2022,"- Result in Table 1 are substantially lower than SOTA results, casting doubt on usefulness of this approach.
- Approach appears to be very incremental to T5, leading to questions about novelty.
- A clearer ablation studying comparing the seven potential combinations of the three differences to T5 is missing.
- The array of GLM models evaluated (e.g., GLM_{Doc}, GLM_{Sent}, GLM_{410M}, etc.) were not well motivated and did not add much to the presentation.
I generally liked the paper, but I am not sure that the results (as presented) make a compelling argument to add another model to the transformer-based “pantheon of models.” It would be great to see how larger GLM models compare to SOTA results, given that hardware resources do not appear to be a constraint.
Typo: Should “BRET” in line 65 be “BERT”?","- Approach appears to be very incremental to T5, leading to questions about novelty.",98,0
ARR_2022_161_review,ARR_2022,"The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments. The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas).
### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.
- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,
- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity. - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.
- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?
--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).
- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.
- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’. - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already. - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.
--- ### Typos/Style: Very well written paper, no remarks here.","- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.",99,0
ARR_2022_276_review,ARR_2022,"- Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.
- [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought.
- I might have missed it but the results section stated ""When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.""; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder? - Would the code and experiments be open sourced like Susanto et al? - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?
- Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline. - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one",- Would the code and experiments be open sourced like Susanto et al?,100,0
ARR_2022_361_review,ARR_2022,"- It lacks novelty that I would expect from a long research paper. It is read more like a report, rather than a research paper. - The paper could be restructured and some parts could be better written. Please see comments and suggestions for more details. - Some contributions are not clear, e.g., which datasets were available, which are released (what kind of postprocessing is done)? hard to digest this from text. Some techniques that could be a contribution remain somehow hidden in lines 425-462, and some are not clear. - Most of the evaluation tasks are already well-known, but had been executed separately. Morpheme-based tasks are more relevant to multilingual probing literature (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) which are not mentioned throughout the paper.
- 091: I don't understand how this is relevant to the paper. It is mentioned a couple of times however I haven't seen such an effort, maybe I'm missing something? - 096: typo: MLR -> MRL - 121: Not clear if you propose these tasks or only combine them into a pipeline? You could write your contributions as bullet points. - 156: ""objective objectives"" sounds weird.
- Section 2 can be condensed quite a lot. 133-156 is already known, so too long. I'd be more interested to see a review on multilingual benchmarks and probing tasks, especially for morphologically rich languages. I'd rather see more details of the other Hebrew models. - 174-176: Again sounds like the authors take GLUE (or similar benchmark) and translate the tasks into Hebrew. - 178:181: But there is a rich literature on morphological and syntactic probing tasks (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) - Throughout the paper, the authors use the term NLU, however I'm not sure whether some of the lower level tasks count as NLU or syntactic tasks. - 254: Oscar is introduced here but there is no description until line 276.
- 313 - footnote 2: What's the postprocessing effort here, not clear. - 320-346: This could be a table.
- 330: Introduce SPMR, what does it stand for?
- 332: which UD version?
- Table 3: The english transcriptions are needed - footnote 4,5: what's the added value (if any)? why is 5 anonymous, didn't understand?
- 428-448: I didn't understand how you feed the subword or token representation to a char-LSTM? Doesn't your model need the ""char"" representation?
- 636-638: Language-agnostic is over claiming here. - Why Aleph?","- It lacks novelty that I would expect from a long research paper. It is read more like a report, rather than a research paper.",101,0
ARR_2022_95_review,ARR_2022,"- One major concern is that the contribution of this paper to the QA and NLP community seems mediocre. To me, the difference between FairytaleQA and other reading comprehension datasets on stories and narratives is not obvious. Although the additional annotations about reading skills could be counted as one highlight, this categorization is still coarse-grained to me since it seems nothing more than question types (I assume the categorization might be easily inferred from the question surface form, e.g., ""How did ... feel?"" belongs to the ""Feeling"" category). - Some details of the annotation process are still vague to me. In Line 342, it claims that the ""texts were broken down into small sections based on their semantic content by our annotators."" The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?
- Another worrying fact is that: from Table 5, we see the performance gap between the QA model and the human is not very big. The BART model fine-tuned on FairytaleQA is 0.533 Rouge and 0.536 F1, which is already around 82% of the human performance (0.651 / 0.644). This suggests that FairytaleQA might not be challenging enough. The possibility is that, with some additional engineering efforts on the QA model, FairytaleQA becomes yet another QA dataset that gets solved within a matter of weeks that does not truly evaluate the phenomena it sets out to.
Minor Suggestions: - I think explicit and implicit questions might not be a good naming. Factual detail questions and inductive questions might be more clear? Putting together all the strengths and weaknesses, I suggest this paper would benefit from another round of revision to address the above concerns before publishing. Otherwise, it might become yet another incremental reading comprehension dataset that might be addressed within a couple of months.","- Some details of the annotation process are still vague to me. In Line 342, it claims that the ""texts were broken down into small sections based on their semantic content by our annotators."" The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?",102,0
ARR_2022_188_review,ARR_2022,"1) The main issue in this paper is that you only show that you can successfully detect known sound changes, and that you don’t spuriously detect them when they’re absent, thanks to the control corpus. But you might detect a lot of changes in characters embeddings using your methods, that are not related to sound change. Using your system in an exploratory fashion, to detect all possible changes and categorize them, would greatly strengthen the paper. As it is, to my understanding, there is no way to be sure that the character embeddings do not lead to detecting a lot of changes independent from phonology, and this is a major issue. Moreover, the link between spelling and sound change is not straightforward to me and should be more clearly justified.
2) Semantic and phonological change across time should indeed take a logistic shape, as you describe in your description; your experiments also seem to show that the change in the Geo corpus is not linear. Why did you stick to a linear shape? Shoemark et al (2019) that you cite also tried a logarithmic shape, you could try it too.
3) There is a large amount of work on semantic change using contextualized embeddings and pre-trained language models, that you do not evoke in your related works (from Mario Giulianelli, Matej Martinc…). 4) The formalism you use, a --> b / c, should be introduced more clearly from the beginning. Especially since you use it as early as in the summary, where it can’t be comprehended without prior knowledge of this formalism. The explanation at line 190 might benefit from a scheme. Similarly, some words like “plosive” might benefit from a short definition (even in a footnote) for readers not familiar with the domain.
Nor clear how the Bin:Control effect is computed. Globally, all metrics described in Section 5 would benefit from equations.
Overall, the localization of Tables and Figures in the paper would be more optimal to ease the reading.
Tables 2 to 4 look strange to me without lines.
L.32: Choose only one between “since” and “however” L59-62: Add references here.
L. 362 and 364: add “the” distance, “the” main effect.
L. 390: in both corpora (no “the”) L. 480: repetition of “language”’.","2) Semantic and phonological change across time should indeed take a logistic shape, as you describe in your description; your experiments also seem to show that the change in the Geo corpus is not linear. Why did you stick to a linear shape? Shoemark et al (2019) that you cite also tried a logarithmic shape, you could try it too.",103,0
ARR_2022_339_review,ARR_2022,"1. The novelty of this method seems to be marginal. The only novelty component proposed in this paper is Length-Control inference.
1. It would be better to separate the description of the previous methods and the proposed method in section 2.
2. Some examples could help the audience to better understand the importance of length-control inference.",1. It would be better to separate the description of the previous methods and the proposed method in section 2.,104,0
ARR_2022_7_review,ARR_2022,"**What is contributing to the performance improvement?**
- Because the paper presents so many novelties, it is a bit hard to grasp what led to the improvement on BEIR, i.e. what are the main factors that contribute to the improvement?
- It appears (Table 3) the strongest improvement come from Lexicon-Enhanced Dense Retrieval, i.e. combining the dense sim score with a BM25 score. This hybrid approach has been shown effective in several previous works, e.g. https://arxiv.org/abs/2005.00181 or https://arxiv.org/pdf/2004.13969.pdf (and many more) - It would be interesting to get the results for other dense + BM25 combinations. The dense retriever appears to be not the strongest (cf. Table 3, col. w/o LEDR), i.e. it is weaker than TAS-B (0.396 vs 0.415). So what would be the results of TAS-B+LEDR?
**Pre-training approach** - A large contribution of the author is the proposal of a new pre-training approach that combines ICT + SimCSE with a large negative cache (ICoL) - Table 3 shows an improvement for the sole dense retrieval model if pre-training is used (e.g. w/o LEDR vs. w/O LEDR & PT) - However, we know that BERT is undertrained (see RoBERTa paper) and performing more pre-training steps yields an improvemed BERT model - A comparison against other pre-training approaches would have been interesting. What happens when I train e.g. with MLM for the same amount of compute on C4 and then do fine-tuning? What about other pre-training methods for dense retrievers (for an overview see https://arxiv.org/abs/2112.07577)? Is the proposed pre-training method actually better than other pre-training strategies?
I think the 3 additions (pre-training, ICoL, hybrid retrieval with BM25) could be better separated / evaluated more individually. It would help to see what are the contributing factors for the improved performance. So far it appears that switching to a hybrid approach with BM25 made the large difference in performance.
Having a more clear separation in their evaluation would help to assess what is really relevant for future methods.
- Line 231: DaPI was concurrently also proposed by Liu et al (which they call Mirror-BERT https://arxiv.org/abs/2104.08027, published April 16; SimCSE was published April 18) - Line 430: You say that the batch size for each GPU is 4,096, so 16*4k = 64k in total for your server. But how do you get a batch of 4,096 on a single 32GB GPU? A batch larger than 100 examples results usually in an out-of-memory error when using huggingface transformers with a distilbert model. Did you use some special off-loading?
- Maybe I might missed it in the paper: Did you specify your max sequence length for pre-training / fine-tuning? I.e. do you use 512 word pieces or do you pre-train on shorter paragraphs? - Table 4: The heading ""w/o ICT (SimCSE 2021c)"" looks confusing, you would think that ICT was proposed by SimCSE 2021c) - Table 4: Adding SimCSE to pre-training appears to bring little effect, performance improves from 43.4 -> 43.8. Could this improvement just be due to randomness? How does the performance change for the fine-tuning setting when pre-training was without SimCSE (i.e. just ICT pre-training followed by supervised MS MARCO training)? - Table 5: The three settings, I assume these are the hybrid approaches with BM25. How is the performance only for the dense retrievers without BM25?","- Table 5: The three settings, I assume these are the hybrid approaches with BM25. How is the performance only for the dense retrievers without BM25?",105,0
ARR_2022_187_review,ARR_2022,"- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.
- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right?
- Are ""slugs"" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.
- ""In this background"" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. ""With this background"", ""Given this background"", etc.
- 2 Related Work - ""result in"" --> ""results in"" - 4.4 Evaluation - ""w"" --> ""we""? It's unclear what is meant.
- Limitations of NPRM - ""while"" --> ""why"" (or just get rid of ""while"")","- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right?",106,0
ARR_2022_139_review,ARR_2022,"Minor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later.
Specific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA.
2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice.",1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA.,107,0
ARR_2022_351_review,ARR_2022,"1. In the backward query of Fig 4, why the span is ""must visit"" instead of ""a must visit"" given the probability of the word ""a"" is 86%, which is the same as the word ""must""?
2. What if the forward query and backward query give conflict prediction? For example in the example sentence of Fig 2, if the answer to the first query is only ""small"" (without ""good""), what will happen? The example in Fig 4 is too simple.
3. What are the differences between two versions of ASTE-Data datasets?
4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct?
The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper.
The variables in formula 1 and 2 should be explained, for example, what is pair_asp.","4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct? The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper. The variables in formula 1 and 2 should be explained, for example, what is pair_asp.",108,0
ARR_2022_186_review,ARR_2022,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient
My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1? - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used? - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence.","- Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?",109,0
ARR_2022_51_review,ARR_2022,"1. The choice of the word-alignment baseline seems odd. The abstract claims that “Word alignment has proven to benefit many-to-many neural machine translation (NMT).” which is supported by (Lin et al., 2020). However, the method proposed by Lin et al was used as baseline. Instead, the paper compared to an older baseline proposed by (Garg et al., 2019). Besides, this baseline by Garg et al (+align) seems to contradict the claim in the abstract since it always performs worse than the baseline without word-alignment (Table 2). If for some practical reason, the baseline of (Lin et al., 2020) can’t be used, it needs to be explained clearly.
2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.
3. If the claim that better word-alignment improves many-to-many translation is true, why does the proposed method have no impact on the MLSC setup (Table 3)? Section 4 touches on this point but provides no explanation.
1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ).
From the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?
2. For Table 3, are the non-highlighted cells not significant or not significantly better? If it’s the latter, please also highlight cells where the proposed approaches are significantly worse. For example, from Kk to En, +FA is significantly better than mBART (14.4 vs 14.1, difference of 0.3) and thus the cell is highlighted. However, from En to Kk, the difference between +FA and mBART is -0.5 (1.3 vs 1.8) but this cell is not highlighted.","1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ). From the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?",110,0
ARR_2022_314_review,ARR_2022,"1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa).
1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results?
1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well?
1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks.
- Title: ""Incorporating the whole encoder layer"" - l.08: ""incorporates all the components""
- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.
- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving.","- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving.",111,0
ARR_2022_342_review,ARR_2022,"W1: In the process of collecting the Redding messages, a pair (parent, target) is considered, but a better justification of this decision is needed to explain why authors have not considered more response messages.
W2: In the process of annotation analysis, five annotators label each message. An explanation of this number is necessary. W3: The analysis of the results is complete but didactic examples while explaining each case would help to interpret the discussion of the authors. W4: An analysis of the difficulty of the annotators in identifying each class is not presented. Would be important to see in which cases the annotators do not agree and why.
W5: I believe that the annotation guide may be too short for annotators who are not familiar with this type of task. Have examples been provided to the annotators?
- The word ""context"" in the title of the paper could be more specified. I suggest the authors replace it with ""Conversational context"", otherwise could be confused by the context of the sentence. - What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task. - How the authors have resolved the possible doubts of the annotators while the annotating process in MTurk? - Please, mention the language you focus on to retrieve the Reddit posts. - Please, move Table 1 to page 2. - The link of the code is not available but the authors in a footnote affirm ""Code and data available at anonymous_GitHub_link"" - A dot is missing after the ""Label distribution and linguistic insights"" paragraph title.
- In paragraph 481, please show didactic examples where the ""Parent"" helps to identify the class of the ""Target"". - Please, explain what do you mean by ""Intricate text"" in Table 8.",- What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task.,112,0
ARR_2022_307_review,ARR_2022,"- Training is performed on each dataset separately. This may result in the models being quite domain-specific, i.e., limiting their generalisation ability. - Method is limited to 2 sequences (minor point)
- PISP objective: I can see how patch-based swapping may help for learning a better intra-modal association in the case where the patches depict some objects the text refers to. But what is the assumption regarding the benefit of general patch-based (as in CLIP-ViL) swapping for the sequencing task (general intra-modal understanding is already the motivation for SMRM, isn't it?)?
- Sect 4.2.3: Maybe an illustration would help to explain this part, I found it hard to understand.
- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?
- Section 5: Why did you not also compare against a pre-trained ResNet version, as you did for CLIP (image-only)?
- Section 5: How would you explain the gap of the effectiveness in the Image-Only setting on WikiHow vs. RecipeQA? For the other settings/models/modalities, the difference in effectiveness is comparably marginal. **Typos, Grammar, Style, and Presentation Improvements** - l135: ""consists"" -> ""which consists"" or ""consisting"" - Section 3.2: The amount of footnotes could be reduced. - l318: select *the* same - l.466: *the* text-only","- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?",113,0
ARR_2022_329_review,ARR_2022,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case.
General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice. 2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others. 3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format) - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps. - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus? - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in","- Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).",114,0
ARR_2022_215_review,ARR_2022,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.","4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.",115,0
ARR_2022_13_review,ARR_2022,"1. Although the translators may be highly professional, different translators usually have different choices, so producing annotations by using 1 translator per language does not seem enough.
2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used.
3. It would have been good to have the number of items per language pair when providing the statistics of the annotated dataset.
The example on the first page sounds like figurative use of the word ""to march"". This is an important aspect of mismatched translations.","2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used.",116,0
ARR_2022_344_review,ARR_2022,"- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.
- There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.
- There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1.
- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.
- It would be valuable to investigate the reason behind the high impact of the LDND distance.
- I would recommend removing singletons from Figure 4.
- Add statistical significance tests in Section 5.
- Cite Pires et al., 2019 in the 2nd paragraph in the introduction.","- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.",117,0
ARR_2022_321_review,ARR_2022,"- Number of instances in a test set: Did the authors analyze if this affects the rankings?
- Section 3: There are no baselines presented for summarisation and image captioning.
- It does not become clear why automatic metrics tend to overrate machine-generated text?
Line 473: missing word - “depending on the downstream task of interest”",- Section 3: There are no baselines presented for summarisation and image captioning.,118,0
ARR_2022_348_review,ARR_2022,"- The contributions of the paper are not very clear.
- The explanation of the methods is not well explained.
- The performance improvement from audio-only to audio-visual is very small (WER: 2.7 to 2.6) and there is significance analysis. This raises a big question on the necessity to add the visual modality in this way and also justification on the extra computation for such a small gain.
Questions/Comments: - Why did the authors did not include experiments with external LM for their approach? This will make a better comparison with other models using external LM.
- Line 183: “we truncate the first convolutional layer in MoCo v2” -> why?
- In the Audio-only model, what are the differences from Watanabe et al., 2017? This is very important to note the contribution of this work.
- Line 250: “a canonical transformer decoder” -> what is it?
- Line 254: “is arguably a decoder” -> why arguably?
- Line 577: the authors discuss about limited ImageNet data for pretraining. For self-supervised pretraining, the authors are not limited to labeled images as in ImageNet. They can use any image for pretraining. So, I’m not sure why the authors discuss this as a problem.
Typos: - Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping",- Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping,119,1
ARR_2022_360_review,ARR_2022,"It wasn't obvious to me why attention-based architectures weren't used for the task. I can imagine the NLP community would be interested in that, but that doesn't make the paper's contribution less interesting. Also, it wasn't clear to me whether the data would can be released, as this is one of the important contributions of the paper.
- It took me a while to understand that ""sign"" and ""gloss"" refer to the same thing. Or am I still misunderstanding it?
- I couldn't understand why a multiclass multilabel approach couldn't be presented in parallel. Was it just left for future work? I assume it would be necessary for the tokenization task that the authors mention in the conclusion, would it not?","- It took me a while to understand that ""sign"" and ""gloss"" refer to the same thing. Or am I still misunderstanding it?",120,0
ARR_2022_14_review,ARR_2022,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved.
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.","- While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance. -The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.",121,0
ARR_2022_330_review,ARR_2022,"1) The paper doesn't present any clear hypothesis and experimental set-up to validate their hypothesis. Neither do the authors explore the issue of unanswerability in MRC in exhaustive detail. They fail to account for previous works on this task (https://ojs.aaai.org/index.php/AAAI/article/download/4619/4497) 2) The claims made in the abstract are unsupported in the results. The authors claim that ""it is shown that uncertainty outperforms a system explicitly built with an NOA option"". But, the results of the Implicit system are inferior to that of the Explicit system. The explicit: option A system is not a meaningful system in itself.
3) The results of the negative penalty scheme show that the proposed uncertainty measure is not that useful in common cases such as 3:1. The authors fail to explore reasons for this, neither do they attempt to experiment with other MRC datasets to see if this is ubiquitous. This makes the exploration incomplete.
4) The writing of the paper is quite convoluted. The paper is not divided into motivation, hypothesis, proposal, experimental design, results, discussion and related work clearly. Most of the sections are a mix of several of these componenets. This undermines the readability of the paper significantly.
1) Kindly include the details of the expected entropy calculations in the main paper to make the paper self-contained.
2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore.","2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore.",122,0
ARR_2022_10_review,ARR_2022,"- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.
- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?
- The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details.
It would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are.
If you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?
Typos: - Page 1: ""set of value"" -> ""set of values"" ""For instance, Orlanski and Gittens (2021) fine-tunes BART"" -> ""fine-tune"" - Page 2: ""Non determinism"" -> ""Non-Determinism""","- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?",123,0
ARR_2022_330_review,ARR_2022,"- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results.
- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained.",- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained.,124,0
ARR_2022_94_review,ARR_2022,"- It is not clear what effect the initial set of candidate constituents and distuents has on final performance. Multiple times the authors claim this form of supervision is “minimal”, and although it is certainly simple to use, perhaps different initial sets would yield different results. Based on Appendix A.1 it does seem like several ways of exploring initial set were explored, although no results reported. There are some results with this respect in section 5.3 showing that left-branching and random strategies to build initial seed set yield very low performance, and authors also state “the manner in which we perform the initial classification has a strong impact on the final tree structure” here. It would greatly help to be more consistent about the effect of the initial seed set throughout the text.
- It’s not clear how h_in(i, j) and h_out(i, j) are computed. For h_in, two likely possibilities are: a) RoBERTa is run over only the tokens i:j and the last vector is used to represent the phrase, or b) RoBERTa is run over an entire sentence x, and two or more output vectors are concatenated to get phrase vectors. If (a) is used, this seems clean although a bit expensive. If (b) is used, it seems like this would have trouble distinguishing between inside and outside. In either case, the text should be more clear about how this is done otherwise reproducibility would be quite difficult.
- Do you think that validation using performance on the dev set (using early stopping or hyperparam selection) would be helpful? There are many reasons why people are interested in methods that could benefit from validation (even for unsupervised learning) because it can help find a model with a good inductive bias that does not overfit to given labels. I suppose by reporting three settings (inside, inside w/ self-training, inside + outside) you are doing a type of validation over hyperparams. - Do you think your method of seed bootstrapping would be generally applicable to other types of unsupervised parsing models?
- What does it mean to re-normalize class probability scores in Fig 3?
- How much does the set of labels grow each epoch?
- How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents?","- How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents?",125,0
ARR_2022_158_review,ARR_2022,"This is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.
My main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels.
This is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion.
- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent.",- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent.,126,0
ARR_2022_330_review,ARR_2022,"I have identified some weaknesses in the paper: - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned.
- While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta).
What follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.
- What is ""Fraction unanswerable"" in Figure 5 and how is it obtained?
- I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25.","- The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.",127,0
ARR_2022_298_review,ARR_2022,"The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section).
- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.
- Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention ""no significant"" difference, which instead is referring to the statistical significance. I would suggest to revise this part.
- It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?
- On which data is fine-tuned the model for the ""Knowledge Distillation"" - Please, add an intro paragraph to section 4.
- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?
- In line 303 the authors mention ""sentence transformers"". Why are the authors mentioning this? Is it possible to add a citation?
- There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.
- I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? -","- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?",128,0
ARR_2022_101_review,ARR_2022,"1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.
2. Figure 2 does not show the time complexity of SimCSE_{CLS} method.
3. I am confused about the definition of ""\vec \mathbf{1}"" in Equation(1).
Missing citation: BERTScore: Evaluating Text Generation with BERT (Zhang et al. 2020) For other suggestions, please refer to the weakness section.","1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.",129,0
ARR_2022_153_review,ARR_2022,"Some evaluations results are mixing. Please see the detailed comments below.
Question: 1. In line 257, it's strange that including the collected LIV-EN parallel data for finetuning actually makes the NMT system perform worse. Could you provide more discussions/explanations on this?
2. Could you provide some insight why the multilingual model is ""noticeably weaker"" (line 236) on the ET→En and LV→EN evaluations?
3. It's interesting that the LV→EN model performs better than the ET→EN model, especially in section 2 authors mentioned that Livonian and Latvian languages are similar in many aspects. minor: 1. Maybe the information that ""the translation is done by hired experts (section 3)"" can be added to the footnote 2 on page 2 (section 1)? I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.
2. Line 194, maybe I'm not familiar with the context, could you explain what does ""implement the support of ..."" mean?
3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.
4. Line 210, on what ET, EN, LV data is the Sentencepiece tokenizer obtained on? Those in the 4-language parallel corpus?
5. Line 255: typo, ""perform performed""","1. Maybe the information that ""the translation is done by hired experts (section 3)"" can be added to the footnote 2 on page 2 (section 1)? I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.",130,0
ARR_2022_40_review,ARR_2022,"- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.
- Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset. - Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results - Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses. - It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?
- Line 216: How many paraphrases were created for each question, and what was their quality rate?
- Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?
- Line 254-257: How many templates were manually created? - Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear) - Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses - Line 324: how the repeated dialogues are detected? - Line 356: how and how many sentences are finally selected from the 120 generated sentences?
- Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones?
- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker)","- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker)",131,0
ARR_2022_331_review,ARR_2022,"Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?
2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity. 3) Briefly discussing what kinds of data augmentation techniques ( ♥ ) other works use in Table 4 would be great (and maybe why not used in this paper).
Line 17: add a space ""datasets. We"" --> ""datasets. We""","1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?",132,0
ARR_2022_48_review,ARR_2022,"1. The contribution of this paper is slightly less: the improvement on prediction effect of SWRM seems to mainly come from the capability of BERT in detecting the position of sentiment words and predicting correct sentiment words.
2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates.
3. Insufficient datasets and explanation of experimental settings: this paper only used one dataset (CMU-MOSI) for evaluation, which cannot fully compare the performance and robustness between SWRM and baseline models. Particularly, CMU-MOSI is one of the datasets employed by Dumpala et al. (2018), and there is a larger dataset named CMU-MOSEI in (Dumpala et al., 2018). It is suggested to add the CMU-MOSEI dataset for evaluation. Moreover, the experimental part doesn’t mention the setting of hyperparameters.
4. Lines 308-319 are quite confused to me. The definition of notations should be largely improved for clarity.
1. Line 220: is absent -> are absent; Line 259: in the training and testing phrases -> in the training and testing phases; Lines 310-311: k_i is defined as the number of sentiment words, but k_i is not used elsewhere; Line 352: Subsequently, We -> Subsequently, we.
2. As mentioned above, I think that supplementary datasets are important for better comparing SWRM and baselines. Moreover, the effect of sentiment word detection and correction of SWRM should better be shown.
3. From my opinion, instead of correcting sentiment word errors from the ASR models, improving the performance of the ASR models is a thorough solution to guarantee the effect of the MSA models in practice.","2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates.",133,0
ARR_2022_223_review,ARR_2022,"The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.
1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning.
2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation.
3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels?
4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation?
5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance. The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice.
6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is.
7. The majority of Model transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim.
8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like ""our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures""
1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks.
2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't.
3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts.
4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White.
5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained.
6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained.
7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy.
8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels.
9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt.
10. Line 074: This sentence is confusing. Perhaps something like ""Thus"" over ""Hence only""?
11. Line 165: Remove ""remedy,""",5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained.,134,1
ARR_2022_75_review,ARR_2022,"1. One problem I can notice is the scalability issue of the proposed method. As mentioned in section 4.3, step-01 of training required labeled parallel sentences, limiting the proposed approaches to the few high-resource languages.
2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only).
3. Many of the reported scores (particularly for mBERT) in Table-2,3 & 4 are close to baseline; the statistical significance test is recommended for reliability
1. It is hard to read numbers in Figure-3 and Figure-5 with a printed copy.
2. In Line 492, the referred table number is incorrect.
3. Author(s) can plan to investigate the unsupervised approach as a future extension of this work which will be more promising.","2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only).",135,0
ARR_2022_109_review,ARR_2022,"1. This paper seems to reduce the impact of semantic noise on text matching to ""length divergence bias"". From the first example in Table 1, it can be seen that what affects the model's prediction should be that T2 contains more other semantics (noise) such as ""Canadian"", ""University of Waterloo grads Kaheer Suleman and Sam Pasupalak"", which may lead to semantic representation drifting and then affects the TM models. In my opinion, the ""length divergence bias"" should only have a discrepancy in the length of the textual expressions rather than introducing additional semantics.
2. Lack of details and intuitive examples of the constructed adversarial datasets.
- Line 136-138: ""... we down-sample each category to align with the average PosRatio of the whole test set"". Could the authors provide more details about the adversarial data construction?
- Could the authors use some specific examples to analyze why the constructed adversarial training sets can improve the robustness of the TM models?","1. This paper seems to reduce the impact of semantic noise on text matching to ""length divergence bias"". From the first example in Table 1, it can be seen that what affects the model's prediction should be that T2 contains more other semantics (noise) such as ""Canadian"", ""University of Waterloo grads Kaheer Suleman and Sam Pasupalak"", which may lead to semantic representation drifting and then affects the TM models. In my opinion, the ""length divergence bias"" should only have a discrepancy in the length of the textual expressions rather than introducing additional semantics.",136,0
ARR_2022_128_review,ARR_2022,"1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper.
2. The results in the analysis with varying homogeneity of training samples (Section 4) are conflicting: they don't support the hypothesis that this method improves more when some language types are underrepresented in the training samples. Therefore, it's unclear why this method improves over baseline methods, i.e. does it help the model learn from skewed training samples or is it something else?
3. I'd expect a discussion of how this method relates to and differs from existing methods on zero-shot cross-lingua transfer. For example, meta-learning methods optimize the initial parameters [1] or optimization algorithms [2] during fine-tuning toward better performance on outlier/unseen languages.
[1] Zero-Shot Cross-Lingual Transfer with Meta Learning. Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein. 2020.
[2] Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. Weijia Xu, Batool Haider, Jason Krone, Saab Mansour. 2021.
For the analysis with varying homogeneity of training samples, would it help to look at performance on the romance and the outlier languages separately? Also, it might also be helpful to consider the typological distance between different language genera.","1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper.",137,0
ARR_2022_227_review,ARR_2022,"1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy - a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs. b) gets to the fundamental problem with automated evaluation raised in the paper, which is that ""when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage."" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case. In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion.
Questions to the authors (which also act as suggestions): Q1. - Line 151: ""four representative CQA models"" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense. Q2. Line 196: ""We noticed that the annotators are biased when evaluating the correctness of answers"" - are any statistics on this available? Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (""With Little Power Comes Great Responsibility."") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: ""The gap between HAM and ExCorD is significant in Auto-Gold"" - how is significance measured here?
Q5. Lines 360-364: ""We determine whether e∗_j = e_j by checking if F1(s∗_{j,1}, s_{j,1}) > 0 .... .... as long as their first mentions have word overlap."" Two questions here - 5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid?
5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten. Comments, suggestions, typos: - Line 031: ""has the promise to revolutionize"" - this should be substantiated further, seems quite vague. - Line 048: ""extremely competitive performance of"" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague. - The abstract is written well and invokes intrigue early - could potentially be made even better if, for ""evaluating with gold answers is inconsistent with human evaluation"" - an example of the inconsistency, such as models get ranked differently is also given there. - Line 033: ""With recent development of large-scale datasets"" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself. - Line 147: ""more modeling work has been done than in free-form question answering"" - potential typo, maybe it should be ""maybe more modeling work has been done 'in that'"" - where that refers to extractive QA?
- Line 222: ""In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs"" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could. - Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart. - Lines 250-252: ""the absolute numbers of human evaluation are much higher than those of automatic evaluations"" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not. - Line 348: ""background 4, S∗ - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - ""background,\footnote{} ..."" in latex.
- Footnote 4: 'empirically helpful' - should have a cite or something to back that there. - Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting.","- Lines 250-252: ""the absolute numbers of human evaluation are much higher than those of automatic evaluations"" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not.",138,0
ARR_2022_37_review,ARR_2022,"1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important. 2) The paper has the following phrase explaining the problem that it wishes to address: “entail preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” However, majority of the paper and indeed the method itself seems to then emphasis the following specific problem, i.e. “eliminating task-specific nuisances.” I think maybe the former line should then either be removed or somewhere it state whether it is a hypothesis of the authors that “eliminating task-specific nuisances” indeed means “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” Perhaps the authors would need another whole paper to analyze what “eliminating task-specific nuisances” w.r.t. the proposed approach precisely means. So I would not say this as a weakness of this paper. However, is the phrase “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information” is used, it would help the reader if a connection is made to the actual solution which is mostly referred to as “eliminating task-specific nuisances.”
3) I also wonder if the authors would be willing to rephrase “nuisances” in “eliminating task-specific nuisances?” Maybe as “noise?”
4) Table 4. Curious why the ablation results are not shown on the CONLL 2003 dataset even though the method seems to have a maximum impact on it? Further, the CONLL 2003 – OOV data in particular seems quite relevant in this experimental setting. Can the authors provide these results? Otherwise the empirical analysis would seem incomplete to me by missing a seeming very relevant detail. 5) Not a weakness of this work as such. But maybe as a suggestion for future work. Lin et al [1] described four categories of artificially simulating OOV scenarios in NER. An empirical analysis with the method proposed in this work over their dataset or at least a dataset using their OOV synthesis methodology would be very interesting to have particularly on this problem of OOV NER.
6) The authors did not mention MINER code will be publicly released if the paper were accepted.
---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
Table 1 in this paper corresponds exactly to Table 3 Vanilla Baseline in Lin et al. [1]. However, Lin et al. mention using the ACE 2005 dataset (paragraph 1 in section 2.1 in Lin et al. [1]) and this paper says it uses the CONLL 2003 dataset. Two concerns here: 1) the dataset used in this paper must be clarified – how do the authors get the exact same results?; 2) if indeed the experimental datasets are the same, I am not sure if it is okay to replicate the exact same table across papers.
-------------------------------- Comments on Structure -------------------------------- Line 194: Should it be section 3.4? The subsection organization seemed confusing.
--------- Typos --------- The paper has several typos and grammar errors more toward the latter half of the paper. I list a few of the typos here. But this paper if accepted needs better proofreading.
Line 83: “cus via eliminating” -> “cues via eliminating” Line 123: “shotcut learning problem” -> “shortcut learning problem” Line 217: “we can subdividing” -> “we can subdivide” Line 334: “static results” -> statistic?
Line 420: “the the span classification” ---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.",1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important.,139,0
ARR_2022_3_review,ARR_2022,"The writing could be further improved, including the structure organization and detailed concept clarification. It's also unclear why V&L models are able to store visual commonsense effectively (Is it really from the images or just from the captions?).
1. It's better to add more vision-and-language models including VisualBERT and ViLBERT, since they are also pre-trained with MLM objective.
2. It seems not reasonable to me for the formula in Adjective Projection part. If we want to estimate whether the object is large or small, we need to first calculate the similarity between ""large"" and the object, then ""small"" and the object, respectively. Then we should calculate the difference between the two similarity instead of using the similarity between the object embedding and the difference between ""large"" and ""small"" embeddings.
3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues.
4. Paper title is a bit overclaimed and also does not cover one of the main topics reporting bias.","3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues.",140,0
ARR_2022_266_review,ARR_2022,"1. One of the main drawbacks of this approach is that presumably the different component black-box experts of the controlled text generation have to be manually selected and the weighted linear combination has to be fine-tuned for each task. It is also not discussed if the inference time is significantly affected by this approach.
2. For the sentiment transfer task, the model with the higher Hamming distance coefficient is considered to be the best model based on the BertScore with respect to the source, which essentially measures how much deviation has been introduced. It appears however that the model with the higher Discriminator coefficient is better, in terms of perplexity and the internal/external classifiers. Given that the Hamming distance in the reference is much higher, it may not be necessary to absolutely reduce the number of changes made, if it serves the overall purpose of the text generation to make more changes. This is somewhat true for the formality transfer task as well.
3. In Table 3, for the formality transfer task, the method sees a decline in performance for the ->Informal task. While the improvement in the ->Formal task is probably a decent tradeoff, this issue is not addressed at all.
4. Percentage preference through majority voting is reported for the human evaluation. More robust correlation/agreement metrics such as Cohen's Kappa should be reported for reliability.
- BertScore and BLEURT are inconsistently typeset through the paper (alternatively as Bertscore or Bleurt). It would be better to maintain consistency.
- Line 244 in Section 2.3 refers to $E_{gen}$ and $E_{rev}$ which have not been previously introduced. It is not easy to deduce what they mean since they are not explained until the next section. Some re-writing for clarity might help here. - Line 182: discirminate: discriminate - Line 203: This penalization token -> This penalizes token - Line 254: describe -> described - Line 376: Dathathri et al. (2020) -> (Dathathri et al, 2020) - Line 434: Ma et al citation missing year - Line 449: describedd -> described - Line 449: in the text -> in a text - Line 520: prodduct -> product - Table 3 BertScore(sc) -> BertScore (src) - Line 573: which use for -> which are used for - Line 631: similar, approaches -> similar approaches",4. Percentage preference through majority voting is reported for the human evaluation. More robust correlation/agreement metrics such as Cohen's Kappa should be reported for reliability.,141,0
ARR_2022_333_review,ARR_2022,"There some parts left unclear during the compilation process. Please see below.
Also the dataset submitted as supplementary material to the paper seems to be a small part of the whole dataset.
My comments are as follows: - How did you decide the number of classes (10 for coarse-grained and 104 for fine-grained) and how did you decide these classes?
- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.
- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.
- Another related issue is, if the dataset was annotated on a syllable basis, how useful will be such an annotation for NLP tasks? For instance, relation extraction, where we usually extract the NEs and the find relations between them. By using syllable annotations, how can we find word NEs? Using a segmenter or tokenizer?
- How is the annotation on Thai NER datasets, on a syllable-basis or word-basis, and why?
- It is stated that the dataset includes 4,894 documents. In NER datasets, it is better to state the size of the dataset with the number of sentences. How many sentences does the dataset include?
- Were these 4,894 documents selected randomly?","- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.",142,0
T8ABT8q3FS,EMNLP_2023,"1. The method has some limitations under several scenarios. For example, this method relies on the MT model, and the translation quality of MT trained with limited data is not good. Thus, the alignment effect will not be ideal enough. Furthermore, if the given speeches are not based on a document or have been shuffled, this method will not work well.
2. The comparison is not sufficient and not fair compared to other methods. The methods compared in Table 2 are weak, and your method is not based on a strong baseline. The model you compared in Table 3 contains about 150M parameters, while your model contains about 800M parameters. Additionally, MBART and w2v-large use much more unlabeled data.","2. The comparison is not sufficient and not fair compared to other methods. The methods compared in Table 2 are weak, and your method is not based on a strong baseline. The model you compared in Table 3 contains about 150M parameters, while your model contains about 800M parameters. Additionally, MBART and w2v-large use much more unlabeled data.",143,0
JIrP8CIvx6,EMNLP_2023,"- The datasets used are somewhat artificial. More analysis on real-world mistakes could better highlight the benefits.
- There could be more ablation or analysis to understand exactly where the gains come from - is it mainly the retrieval or the fact representations?
- The efficiency gains flatten out after thousands of edits, suggesting there may be limits to scaling further. More discussion of limitations would be helpful.
- The approach relies on access to related factual knowledge, which may not always be available in practice. How critical is this knowledge?
- The gains over prior SME methods, while substantial, are a bit incremental. The novelty may not seem as high.","- The gains over prior SME methods, while substantial, are a bit incremental. The novelty may not seem as high.",144,0
YivRtscaFW,EMNLP_2023,"Some potential weaknesses of this paper:
- The approach relies on unlabeled data which may not always be readily available.
- More analysis could be provided on computational overhead of USP.
- Additional ablation studies validating design choices would strengthen the method.
The main risks if this paper was accepted:
- Reviewers may want to see experiments on a wider variety of languages and tasks.
- More work may be needed to demonstrate computational efficiency.
- The techniques used for scoring pseudo-demos could be critiqued as ad-hoc or lacking in sophistication.",- The approach relies on unlabeled data which may not always be readily available.,145,0
9cALtYoAEy,EMNLP_2023,"## Weaknesses
1. **Complex Presentation & Writing Style:** Although the approach seems to be executed well, I feel the writing style could be more well structured and a better insight as to the motivation of the approach could have been given. I personally did not have much experience with vector-quantization methods and complex nuances such as the index collapse issue and more such things were initially difficult to follow. A well-rounded understanding for this should have been presented.
2. **Efficiency wrt current generative LLMs:** I also would have been interested in observing the performance of this approach when applied to modern generative LLMs, given their current superiority in paraphrasing tasks. It would have been valuable to have comparisons made between VQPrompt and both closed LLMs like the GPT models and the open LLMs. Even if the VQPrompt framework didn't directly outperform them, analyzing the trade-off between performance and inference costs on the Pareto front would have been insightful.
3. **Vector Quantization may Backfire?:** The paper evaluates the proposed approach on a limited set of benchmark datasets, which may not fully represent the diversity and complexity of real-world paraphrasing tasks. The lack of evaluation on more diverse and challenging datasets raises concerns about the scalability and generalization of the VQPrompt model to different domains and languages. End-to-end text-to-text paraphrasing might be more generalizable on a myriad of text types, and I have concerns related to domain adaptability and concept drift in the future.
- It appears that the solution space for this task may be large. Better Auto-Encoder based models which are more robust might outshine this approach, changing the generative decoding methods such as using beam-search trees or top sampling might generate the necessary valid perturbations needed (although I'm not sure how the outputs would compare). Beam search can help improve the diversity of generated paraphrases by considering multiple candidate sequences during decoding; this could enhance the model's ability to produce a wider range of paraphrase variations, modifying the decoding sampling technique, such as using nucleus sampling or temperature scaling, can provide more control over the level of diversity in the generated paraphrases.
4. **Not Enough Discussion of Negative Scenarios:** The paper mainly highlights the positive results and state-of-the-art performance achieved by VQPrompt. However, the absence of a discussion on scenarios where the proposed approach might fail or produce suboptimal results limits the paper's comprehensive analysis and practical guidance for potential users.
5. **Codebook Maintenance and Trade-offs:** I'm not sure how the codebook would be updated with time and concept drift scenarios. A more detailed analysis of the trade-offs and computational costs associated with codebook maintenance is needed. Would like to see a more critical assessment of statements made.
6. **Minimal Discussion of Limitations**: The limitations section is negligible.",5. **Codebook Maintenance and Trade-offs:** I'm not sure how the codebook would be updated with time and concept drift scenarios. A more detailed analysis of the trade-offs and computational costs associated with codebook maintenance is needed. Would like to see a more critical assessment of statements made.,146,0
NW09xt3kvH,EMNLP_2023,"* The paper does not perform convincingly in empirical evaluations. Notably, it does not outperform the baseline MCMIPL on the LastFM and MovieLens datasets. This raises questions about the practicality and efficacy of the proposed method.
* It appears that the improvements seem to be more from allowing multiple choice feedback than the proposed hierarchical tree and policy learning contributions. More evidence is needed to demonstrate their effectiveness.
* The necessity of the current and global graphs in the ablation study is not well justified. There is a lack of evidence that these components significantly contribute to the system's performance.
* The connection between the proposed method and the challenges mentioned in the introduction is unclear. For example, the paper does not explain how the hierarchical tree framework handles situations where ""users might not necessarily like all attributes of the target item, nor dislike attributes that are not included in the target item.""","* The paper does not perform convincingly in empirical evaluations. Notably, it does not outperform the baseline MCMIPL on the LastFM and MovieLens datasets. This raises questions about the practicality and efficacy of the proposed method.",147,0
kyHwalUpPu,EMNLP_2023,"1. This paper is a case study that utilize LLMs to generate texts in the field of mental health. The main contribution is proposing a prompt based on CBT, which is good but not sufficiently novel to appear in such a competitive conference.
2. ""Diagnosis"" is kind of over-claim. There is no clear evidence that LLMs can really diagnose.","2. ""Diagnosis"" is kind of over-claim. There is no clear evidence that LLMs can really diagnose.",148,0
NYlL3oACU2,EMNLP_2023,"- The MCM, VBCM, MBCM measures are not well explained and make it very hard to interpret the tables and charts using the information in the paper.
- I found it hard to understand some of the experiment descriptions/interpretations, particularly Phase 3.","- I found it hard to understand some of the experiment descriptions/interpretations, particularly Phase 3.",149,0
b7ZJcAkjC3,EMNLP_2023,"1. I may not fully understand the motivation behind the proposed method. What's the limitation of the existing method and how you address the issues? What's the advantages of your method and why? The paper can be improved if authors provide more insights in the introduction part.
2. From Table3, we can see that the proposed method underperforms the baseline methods on E-F1. Some explanations should be provided.
3. No code was provided. No code sharing promised.","2. From Table3, we can see that the proposed method underperforms the baseline methods on E-F1. Some explanations should be provided.",150,0
9Ax0pyaLgh,EMNLP_2023,"1. The two introduced techniques are well-studied in speech translation field. Leveraging mixup and JD loss to bridge the modality gap is proposed by [1]. Leveraging gloss-to-text model to translate gloss from video-gloss data into language text is very similar to leveraging MT model to translate the source text from ASR data into target text [2].
2. My main concern is the soundness of leveraging CTC as a sign-gloss forced aligner (Section 2.3). MFA (a HMM-GMM model) is widely used as speech-transcription forced aligner, there are two reasons why CTC classifier is discard.
* Firstly, the CTC classifier is more likely to produce <blank> than repeat. Suppose the gloss is ""i love cat"", the CTC classifier is more likely to produce ""_ i _ _ love _ _ cat _"" than ""i i love love love love cat cat cat"". How to deal with the blanks to segment sign video?
* Secondly, the CTC classifier may produce wrong output. Suppose the gloss is ""i love cat"", the CTC classifier may generate ""i do like cat"". How to deal with this session to perform force-aligned, especially the performance of CTC model is not good (according to table 8, the WER is about 20).
If this paper makes extra efforts, they should describe in detail. Otherwise, the soundness of choosing CTC classifier as forced aligner should be discussed.
3. The analysis are not convincing.
* To prove the effectiveness of mixup mechanism, there are only qualitative analysis without any quantitative analysis. Word-level similarity or sequence-level similarity are more convincing. Also, visulization of the representation produced by translation encoder will be more convincing than on the word embedding level.
* In Table 3 and Table 4, this paper shows that the introduced two techniques work on different examples (based on word frequency and sequence length). The phenomenon is interesting, but there is no analysis.
[1] Fang Q, Ye R, Li L, et al. Stemm: Self-learning with speech-text manifold mixup for speech translation. ACL 2021.
[2] Jia Y, Johnson M, Macherey W, et al. Leveraging weakly supervised data to improve end-to-end speech-to-text translation. ICASSP 2019.",1. The two introduced techniques are well-studied in speech translation field. Leveraging mixup and JD loss to bridge the modality gap is proposed by [1]. Leveraging gloss-to-text model to translate gloss from video-gloss data into language text is very similar to leveraging MT model to translate the source text from ASR data into target text [2].,151,1
HzecOxOGAS,EMNLP_2023,"1. Injecting knowledge into a pre-trained language model is similar to ERNIE（Enhanced Language Representation with Informative Entities ）.
2. The Conditional Time Series Prediction module（bottom(b) of figure 1） seems a little complicated.",1. Injecting knowledge into a pre-trained language model is similar to ERNIE（Enhanced Language Representation with Informative Entities ）.,152,1
N58BZj5JB7,EMNLP_2023,"1) Limited reproducibility - I request the authors to both, open source their code, and release their datasets, including the modified versions. Typically, an anonymized code/data repository helps reviewers verify the claims of the paper.
2) Assumes the availability of diverse knowledge in the LLM - Typically an LLM can only self-critique if it is exposed to alternative forms of responses in the training dataset. For smaller LLMs trained with smaller volumes of data, they may have limited exposure to diverse content and hence, incapable of self-critique.
3) This method is somewhat computationally expensive - For a typical response, the authors engage in iterative improvement via multiple rounds of self critiquing. This may prove expensive for large LLM models, computationally and in terms of user response time in real time scenarios such as chat bots.","1) Limited reproducibility - I request the authors to both, open source their code, and release their datasets, including the modified versions. Typically, an anonymized code/data repository helps reviewers verify the claims of the paper.",153,0
0LXEvcD3dB,EMNLP_2023,"1. The role of each stage in the training is unclear. The authors should add more motivational detail in the method section.
2. The component name in the method section (discrete unit extractor, large language modal and unit vocode) should be unified to that (discrete unit extractor, SpeechGPT and unit vocode) in the FIgure 2.
3. The authors can consider using automated metrics and human evaluation of test model performance on more tasks.",1. The role of each stage in the training is unclear. The authors should add more motivational detail in the method section.,154,0
My6Rgv7xXV,EMNLP_2023,"There are some specific issues with the task of argument quality detection using LLMs -
1. The author/s take the average of three runs for this task. Considering the variance inherent in LLMs and the relatively small size of the context, it may be possible that three runs are not enough. It would be good to see the impact of more runs and whether that causes a change in the results reported.
2. More details need to be provided with regards to the context provided for the LLMs to be able to assign a quality score to an argument - are they arguments from IBM30k? If so, are they from the same topic or from different ones and does that impact the score assigned?
3. The author/s provide a maximum of 10 arguments in the context for LLMs to score arguments. This seems a bit random, especially considering that IBM-30K has nearly 30,000 scored arguments. It may be interesting to see if the scoring is improved on provision of more context and whether there is a drop off in terms of improvement after a certain threshold.","3. The author/s provide a maximum of 10 arguments in the context for LLMs to score arguments. This seems a bit random, especially considering that IBM-30K has nearly 30,000 scored arguments. It may be interesting to see if the scoring is improved on provision of more context and whether there is a drop off in terms of improvement after a certain threshold.",155,0
4MjZNeTCqZ,EMNLP_2023,"1. Lack of Model Comparison Details: The authors compare their model, UniChart, with ChatGPT in Table 2 without providing sufficient details about how the comparison was conducted. Given that ChatGPT is not designed for visual input, it raises concerns about the fairness and validity of this comparison.
2. Incomplete Presentation of Models: The authors did not include important information, such as the number of parameters for each model, in their comparison. This omission limits the reader's understanding of the complexity and computational demands of the proposed model compared to others.
3. Ambiguity in Statistical Interpretation: The authors used p-values in their analysis without properly explaining their significance or interpretation, causing confusion for readers unfamiliar with statistical testing.
4. Insufficient Information on Data Preprocessing: The paper lacks a clear description of the data preprocessing steps and whether all data in Table 1 was used for pre-training. This omission hinders the reproducibility of the results.
5. Unclear Training Details: The authors did not specify whether the Chart image encoder and text decoder were trained from scratch or used pre-trained weights from other datasets. This lack of information further limits the replicability of the research and the full understanding of the model's capabilities.",5. Unclear Training Details: The authors did not specify whether the Chart image encoder and text decoder were trained from scratch or used pre-trained weights from other datasets. This lack of information further limits the replicability of the research and the full understanding of the model's capabilities.,156,0
ivSJdhcuTi,EMNLP_2023,"- It's unclear whether the authors have proposed any concrete solutions or strategies to address the problem of out-of-distribution generalization in NLP.
- The paper might be too focused on the problem itself, without providing sufficient practical insights or guidelines for dealing with it","- The paper might be too focused on the problem itself, without providing sufficient practical insights or guidelines for dealing with it",157,0
I13VHLJjLO,EMNLP_2023,"* The description of the method is not clear. Specifically, the term $\hat{r}$ in Section 2.2 is not clearly explained. The source of this label is questionable.
* The contribution of the paper is thin. Essentially, this is a combination of the well-known re-ranking technique and the now popular reward models. Therefore, it does not provide interesting technical insights.","* The description of the method is not clear. Specifically, the term $\hat{r}$ in Section 2.2 is not clearly explained. The source of this label is questionable.",158,0
RkqyZj5QNN,EMNLP_2023,"1. The author mentions that large models perform poorly due to inadequate reasoning capability and performance constraints caused by window length limitations. However, the experimental results show that large models perform well (with an average accuracy of 90% under zero-shot) which makes it confusing.
2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets.
3. There is a lack of novelty, and clues reasoning can be considered a natural extension of cot. Knn sampling is also something that has already been proposed in existing work.
4. The method proposed by the author has limited improvement in performance.","2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets.",159,0
RwzFNbJ3Ez,EMNLP_2023,"1. The method presented relies on extracting multiple responses from the LLM. For the variant with optimal performance, LLM prompting, 20 samples are needed to achieve the best reported results. Assuming a response contains 5 sentences, this requires 100 API calls to obtain a passage-level score (if I understand correctly), which is cost heavy and ineffective.
2. It remains unclear whether the proposed approach is suitable for detecting hallucinations in responses from other LLMs and across various application scenarios beyond WikiBio. This uncertainty arises because the experiment dataset exclusively encompasses WikiBio responses drawn from text-davinci-003.
3. The proposed method might struggle to detect hallucinations in open-ended responses, for example, the prompt ""introduce a sports celebrity to me"". In this case, the sampled responses could pertain to different individuals, making it challenging to identify shared information for consistency checking.",2. It remains unclear whether the proposed approach is suitable for detecting hallucinations in responses from other LLMs and across various application scenarios beyond WikiBio. This uncertainty arises because the experiment dataset exclusively encompasses WikiBio responses drawn from text-davinci-003.,160,0
SViJgzox1z,EMNLP_2023,"1. This paper is similar to previous prompt tuning paper, with the added element of retrieval for prompt initialization. While the approach is straightforward, it doesn't offer significant innovation compared to earlier studies.
2. In this paper, the authors primarily utilized the T5-small and T5-large models, which seem somewhat outdated. It might be beneficial for the authors to explore more recent decoder-only models, like the LLaMa model, for a more current analysis.
3. This paper concentrates solely on classification tasks, including GLUE and SuperGLUE, which can be considered somewhat dated. Generation tasks, by contrast, present more utility and challenges than classification tasks.
In general, this article feels like it's from a few years ago and isn't particularly captivating.","3. This paper concentrates solely on classification tasks, including GLUE and SuperGLUE, which can be considered somewhat dated. Generation tasks, by contrast, present more utility and challenges than classification tasks. In general, this article feels like it's from a few years ago and isn't particularly captivating.",161,0
PSlrVYPTAX,EMNLP_2023,"1. Since there are smaller LMs behind UniMIND (e.g., BART) and larger LMs representing LLMs, the experimental set up makes it hard to understand if the gains are due to the proposed methods or simply due to increased model capacity. Comparing the combination of LLMs + CRSs to the LLMs alone shows small gains (e.g., on Tables 1 and 4), so one conclusion can be that these marginal gains are due to the additional parameters introduced by the CRS --- in this case, would a slightly larger LLM be enough to achieve similar gains?
2. Methodology aside, I am not sure how novel and practically helpful the findings are. Essentially, the main takeaway is that it depends on the nature of the sub-task how to combine LLMs and CRSs. This takeaway has already been contributed by various previous works, for example:
a) ""What does BERT know about books, movies and music? Probing BERT for conversational recommendation"" (Penha et al., 2020) shows how LLMs can be helpful in recommendation-making;
b) ""'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems"" (Bursztyn et al., 2021) shows how LLMs can be helpful in preference interpretation;
c) ""ReXPlug: Explainable Recommendation using Plug-and-Play Language Model"" (Hada and Shevade, 2021) shows how LLMs can be helpful in recommendation explanation.
However, these previous contributions are not properly acknowledged in the paper.
3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above).
4. The majority of the Appendices is copied *verbatim* from ""U-NEED: A Fine-grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation"" (Liu et al., 2023), even duplicated in lines 979-994.","3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above).",162,0
Srxf1V2jPa,EMNLP_2023,"- **Ethics**: The paper demonstrates a thorough consideration of ethics while creating the new benchmark. However, there is a notable concern regarding copyright. TEM-8 is a language test organized by an official Chinese institution, but the authors claim that it was collected from ""educational websites,"" without providing clarity on whether these websites are officially authorized sources. Besides, it is also unclear how the authors deal with the privacy problems of test takers of TEM-8.
- **Analysis**: Some arguments in this paper may not be clear. I list them below:
- 1) Line 417: “Given that numerous individuals rely on LLMs as personal grammar checkers today…”. Running LLMs can indeed be expensive for individuals, and many might prefer using commercial products due to their availability and cost-effectiveness. However, the paper suggests the potential use of LLMs for GEC systems without concrete evidence of tech giants developing such systems. If the authors wish to maintain this point, they need to provide further evidence or references to support their claim.
- 2) Line 470: “...count the number of occurrences of the target word (i.e., B) in the training data…”. According to Appendix, the reproducible GEC systems are trained/fine-tuned on the CLang8 data. But the training data of ChatGPT is unknown for now, how can we ensure the divided types of word occurrences are reasonable for ChatGPT?","-1) Line 417: “Given that numerous individuals rely on LLMs as personal grammar checkers today…”. Running LLMs can indeed be expensive for individuals, and many might prefer using commercial products due to their availability and cost-effectiveness. However, the paper suggests the potential use of LLMs for GEC systems without concrete evidence of tech giants developing such systems. If the authors wish to maintain this point, they need to provide further evidence or references to support their claim.",163,0
I5hTganf3z,EMNLP_2023,"Although I am inclined towards accepting the paper, I have a few concerns:
1. Some of the dataset statistics reported in Section 4 and appendix I do not seem to correlate.
2. The dataset size is small with close to 50% of the documents not labelled with any vulnerability label. Additionally, only 2 labels dominate the overall label distribution. This raises questions on the dataset quality as well the scope for future models to perform well on this dataset.
3. At least one generative model could have been tried out leveraging the descriptions of vulnerability types to improve the classification performance.
4. Description of the proposed ""Concept-aware Hierarchical"" model is not clear.",1. Some of the dataset statistics reported in Section 4 and appendix I do not seem to correlate.,164,0
R0XABYPVKI,EMNLP_2023,"* The main experiment suffers from a major drawback that it is unclear from the setup of the evaluations how well paraphrasing using a LLM generates good representations, that is, good examples of _external knowledge_ outside of the corpus. The method in the paper does not validate, or convince, that the paraphrases are indeed outside of the corpus knowledge.
* The paper's contribution is limited. The results of the evaluations just verify the known phenomena that is also addressed by the authors related work, and it quite intuitive to understand since Wikipedia is far smaller than the data large LLMs are trained on.","* The main experiment suffers from a major drawback that it is unclear from the setup of the evaluations how well paraphrasing using a LLM generates good representations, that is, good examples of _external knowledge_ outside of the corpus. The method in the paper does not validate, or convince, that the paraphrases are indeed outside of the corpus knowledge.",165,0
1Xht3SKAoY,EMNLP_2023,"* All experiments were performed on a very limited of test sets (100 cases) and train sets (minimal 100 or 200 cases according to Appendix B), potentially causing the risk of randomness in terms of the results.
* The ExpNote framework was only tested with one LLM, i.e., ChatGPT (which was even not mentioned in the main part of the paper, just occuring in the Appendix), which is not solid and persuasive enough.
* The main contribution of this paper is a little incremental to previous work like memory-based and reflection-based prompting methods.","* The ExpNote framework was only tested with one LLM, i.e., ChatGPT (which was even not mentioned in the main part of the paper, just occuring in the Appendix), which is not solid and persuasive enough.",166,0
PyJ78pUMEE,EMNLP_2023,"Minor:
* The method still looks time-consuming. Could you provide a detailed inference time comparison with other methods? It can be interesting to see the tradeoff between score correlation improvement and the extra consumed resources.
* The experiments can have a general NLG evaluation metric for reference for the overall score, such as BARTscore, SEScore, GPTscore etc.","* The experiments can have a general NLG evaluation metric for reference for the overall score, such as BARTscore, SEScore, GPTscore etc.",167,0
ahVTS392C3,EMNLP_2023,"Generally, there are many missing details, biases, etc. in the data creation and testing. Notes:
1. What are the reasons to have 80% of the dialectal data written in Egyptian dialect?
Will this bring biases to the model?
It's known that most tweets are written in Gulf dialects. Why do we have a different distribution here?
""our classifier tags 80% of the predicted AraC4 dialects as Egyptian, 2.86% as Bahraini""
2. You mention perplexity in Table 2. How did you calculate this for Jasmine, AraGPT, and mGPT?
Details are missing.
3. Missing details:
""These are news headlines (5K phrases/sentences), news stories (5K para281 graphs), and these titles (5K phrases/sentences).
All samples are collected from diverse online sources""
4. Details in correct word scrambling are missed.
Ex: Did you words with their contexts or not?
What is the prompt that was given to the model to correct these errors?
A word can be corrected in many different ways, and this depends on the context.
5. In commonsense inference, how did you generate the 3 false answers?
6. What is the explanation that sometimes when we increase the number of shots, the model performance drops (ex: sarcasm from 8-shot to 16-shot)?
Is there anything in the settings or the prompts?
7. Adding SOTA results is very useful in the tables
8. Prompts in tweets are very short, ex: ""tweet: who/we/how..."". Judging the quality of the generated outputs in such cases has no meaning.
9. The criterion of the selection of the occupations in Table E.1 is not clear.
10. ""Gender, Color, Region"" section is not clear, and many details are missing.
""Combined with our list of 100 professions, this gives us 1,600 generated sentences of which we keep only 1,000 sentences that contain professions. Finally, we manually classify the generated sequences into one of three categories from the manually prepared set {high-wage, medium-wage, low-wage}""
11. The model should be tested on more tasks from ORCA benchmark.
12. Where is the novelty in this paper?","1. What are the reasons to have 80% of the dialectal data written in Egyptian dialect? Will this bring biases to the model? It's known that most tweets are written in Gulf dialects. Why do we have a different distribution here? ""our classifier tags 80% of the predicted AraC4 dialects as Egyptian, 2.86% as Bahraini"" 2. You mention perplexity in Table 2. How did you calculate this for Jasmine, AraGPT, and mGPT? Details are missing.",168,0
wrBIS6FOfV,EMNLP_2023,"1. The similar pipeline appears in many subtask. Merely conducting experiment over different combination of LLM/VLM may be lack of insight.
2. Incomplete ablation study in Table 5 and Table 6. Only ChatGPT family is selected as direct QA & Reasoner.",2. Incomplete ablation study in Table 5 and Table 6. Only ChatGPT family is selected as direct QA & Reasoner.,169,0
G12y1Pz3vJ,EMNLP_2023,"1.	The contributions of generating positive samples and exploring loss function are not intuitively related, which makes this paper looks like a patchwork.
2.	Since the positive samples are generated with extra models (and annotations), the proposed method is hard to fairly compare with baseline methods. Moreover, at least the comparison of external tools and supervision should be added in Table 1.","1. The contributions of generating positive samples and exploring loss function are not intuitively related, which makes this paper looks like a patchwork.",170,0
X570XzeYSW,EMNLP_2023,"- The test data is synthetic, generated by a rule-based system; we can't be sure that any of the results (baseline or experimental) accurately reflect performance on real data gathered from native speakers of these dialects. Lines 276-280 and the Limitations section discuss this, but in my opinion the discussion is inadequate. I would need to see citations and a specific, well-reasoned explanation for why it is valid to draw conclusions about real-world performance based on synthetic test data. This is my single largest issue with this paper.
- Many of the results appear to be disappointing; there is not a clear trend of HyperLoRA significantly improving over baseline performance.
- It is not clear that the test sets contain enough dialectal differences to estimate real-world benefit to native speakers.
- Some details about the method were unclear from the text (though I acknowledge that the promised public code release mitigates this somewhat).",- Some details about the method were unclear from the text (though I acknowledge that the promised public code release mitigates this somewhat).,171,0
AjGXZIgvIb,EMNLP_2023,"1. The motivation and central questions of the paper talks about concepts at a very broad scale but the actual implementation is limited to hyponyms and hypernyms.
2. The approach 1 of rethinking LLMs training does not make a lot of sense to me. It would go against the fundamentals of “language modeling” since such concepts don’t explicitly occur as special tokens in language utterances. It is also only a sketch not backed by concrete empirical evidence so I am skeptical about its usefulness.
3. It is not immediately clear to me what the benefit would be to shift existing LLMs from token-level to concept-level by following the second approach presented in the paper. The authors do not demonstrate how such a “concept-aware” LLM would be better at language understanding or reasoning than normal LLMs.",3. It is not immediately clear to me what the benefit would be to shift existing LLMs from token-level to concept-level by following the second approach presented in the paper. The authors do not demonstrate how such a “concept-aware” LLM would be better at language understanding or reasoning than normal LLMs.,172,0
aURCCzSuhc,EMNLP_2023,"1.	Limited set of baseline methods was selected
2.	Experiment setup is basic and not matching with the ideal situation. Why the experiments are performed separately for subtype/supertype and overlapping relations among entity types?
3.	Limited set of entity types is considered. Perhaps, that is the reason, even the basic X-Annot. method has similar performance as with other methods.
4. Manual intervention to get the final sets of entity types if relations among them exist. It may not be a good idea when entity types increase from 100s to 1000s.","3. Limited set of entity types is considered. Perhaps, that is the reason, even the basic X-Annot. method has similar performance as with other methods.",173,0
EkftL7NgtW,EMNLP_2023,"- In lines 071-074, what is the definition of ""compact"" representation? Why more compact cluster representations mean more separable fine-grained categories?
- How to solve the situation that no neighbors left after filtering by three principles?
- The features in query and the features in queue are extracted from different encoder, why the similarity between them can be directly computed?
- In Table 2, Ancor -> Anchor",- How to solve the situation that no neighbors left after filtering by three principles?,174,0
syj9VaxutQ,EMNLP_2023,"- The evaluation setup is rather specific to the game that is described here and it is unclear whether and how this can be tested with other games. Even though the limitations section mentions the limited generalizability, I consider this a serious issue for publication.
- Even though substantial effort is put into data collection, the final dataset that is analyzed is small, only a hundred contributions receive 3 judgments, making the results rather weak.
- In addition, there are factors that make the already small rater group heterogenous, such as play style, experience with the game, and possibly demographic factors (that are impossible to know here). This adds to the low generalizability.
- Since only raters who have already played the game were included in the study (with good reason), it is unclear to me what role familiarity bias would play when rating dialog contributions. Being used to a certain story line might make raters prefer contributions they have seen before or chosen before.
- The related works section mentions only work related to video games. I would like to see here some related work about comparing LLM output with human output and references to papers that have studied what issues there are with LLM-generated dialog contributions.
*Update after rebuttal*
Thank you to the authors for clearly addressing my concerns. I would like to see the details on power analysis as well as additional background references in a final version of the paper.","- In addition, there are factors that make the already small rater group heterogenous, such as play style, experience with the game, and possibly demographic factors (that are impossible to know here). This adds to the low generalizability.",175,0
vgaJRhYVje,EMNLP_2023,"* As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.
* There is no evaluation on standard image-text retrieval tasks (e.g., on COCO), in addition to the evaluation on compositional reasoning benchmarks.","* As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.",176,0
hsjQHAM8MV,EMNLP_2023,"- The paper claim scalability and generalizability. However, none of the common black-box models (model-as-a-service) like GPT3.5, GPT4, and Claude are used for the experiments.
- The experiments focus on a single dataset which is a little weak. One would question if the conclusions can be generalized to other evaluation benchmarks.
- While there are many detailed analyses and experiments, it is unclear how the in-context learning editing affects the model's internal beliefs. Is the editing only on the instance level, or does it also reflect in the reasoning chain and belief graph that the model used to perform reasoning and other tasks?
- The baselines are mainly models that are not highly ranked on open source benchmarks. It's unclear if the method will generalize to the more capable LLMs like LLaMA.",- The baselines are mainly models that are not highly ranked on open source benchmarks. It's unclear if the method will generalize to the more capable LLMs like LLaMA.,177,0
hcDE6sOEfu,EMNLP_2023,"- The authors claim that the competition is designed to study 1) *prompt leaking* (by making the model generate a secret key in its prompt), and 2) *harmful information generation* (by making the model say 'I have been PWNED'). While I would define *prompt leaking* as leaking the full prompt that allows the replication of a service, I consider leaking the secret key as a close enough proxy. However, I believe there is a clear distinction between *goal hijacking* and *harmful information generation*, and I'm not convinced that the competition measures the latter.
- The claim that the data collection/analysis broadly covers six intents of prompt hacking doesn't seem to be well-supported. I suggest that the authors provide additional discussions on how their results could generalize to settings such as *harmful information generation*, *training data reconstruction*, *denial of service* and *token wasting* if they want to make such a claim.
- The ontology of attacks appears to be a key piece of this paper. However, the authors didn't describe the methodology of creating this ontology. There are many terms that are introduced later in the appendix, making it difficult to take much away from S.5 of the paper. I suggest that the authors consider restructuring this section.
- Typos, strange word choices and formatting issues make the paper somewhat difficult to read.","- The ontology of attacks appears to be a key piece of this paper. However, the authors didn't describe the methodology of creating this ontology. There are many terms that are introduced later in the appendix, making it difficult to take much away from S.5 of the paper. I suggest that the authors consider restructuring this section.",178,0
v6VbokqzvP,EMNLP_2023,"1)The paper's evaluation scope is quite narrow. A comprehensive understanding of the method's versatility could be gained by testing it across a wider range of tasks.
2)The lack of in-depth ablation studies on each stage of R3 prompting prevents a clear understanding of their individual contributions to the overall performance.
3)The paper lacks complex examples and a detailed error analysis, which could offer a more nuanced understanding of the method's capabilities and limitations.
4)There is a conspicuous absence of comparison with related works, particularly those involving interactive prompting.
5)The document could provide more information on how the R3 prompting strategy might be adapted or modified for different types of problems or different models.
6)The authors could also explore more about how this strategy could be incorporated into existing LLM training strategies to further enhance performance.",5)The document could provide more information on how the R3 prompting strategy might be adapted or modified for different types of problems or different models.,179,0
jcqBLHFcYA,EMNLP_2023,"1. [Human evaluation] The dataset is only evaluated by two NLP researchers instructed to perform entailment detection. Given this task is fairly challenging for humans when the situation only communicated verbally, I wonder if the verbal set-up has increased unnecessary difficulty for language models. It would be helpful to see baseline human evaluations, to disentangle the challenge to imagine a situation based on languages, and to perform ToM and perspective taking.
2. [Variety of set-ups] The authors have four set-ups (forehead-mud, forehead-mud-mirror, thirst, explicit), but did not provide any detailed by-type analysis on these variations. These details would be important to include to understand the nature of ToM, as these set-ups do not have similar complexity. For example, the forehead-mud-mirror setup requires the understanding the consequence of looking at the mirror in addition to forehead-mud setup.
3. [Statistical association] While the verbalization provides a systematic way to scale up and generate test cases, it hinders the naturalness of language and make it difficult to tease apart reasoning from mimicking the statistical patterns, especially in the case of few-shot reasoning. The dataset would benefit by introducing more varieties of language template other than ""it is publicly announced that someone"", with detailed analysis on how the linguistic properties of the template is affecting the results. For instance, the recency of mentioning the target and the use of discourse connections would likely to change the reasoning accuracy.","3. [Statistical association] While the verbalization provides a systematic way to scale up and generate test cases, it hinders the naturalness of language and make it difficult to tease apart reasoning from mimicking the statistical patterns, especially in the case of few-shot reasoning. The dataset would benefit by introducing more varieties of language template other than ""it is publicly announced that someone"", with detailed analysis on how the linguistic properties of the template is affecting the results. For instance, the recency of mentioning the target and the use of discourse connections would likely to change the reasoning accuracy.",180,0
g1LLeiHX0P,EMNLP_2023,"* Although the paper mentions some related work, such as retrieval-based methods, it misses the opportunity to discuss recent similar works like [1]. Including such references would enrich the context and comparison.
* The experiments could benefit from incorporating newer baselines such as (Zhang et al., 2022a). Including more contemporary comparisons would strengthen the credibility and persuasiveness of the results.
[1] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu. Unified Demonstration Retriever for In-Context Learning. ACL 2023","* Although the paper mentions some related work, such as retrieval-based methods, it misses the opportunity to discuss recent similar works like [1]. Including such references would enrich the context and comparison.",181,0
O36QcmUEDM,EMNLP_2023,"* Though the dataset is diverse and unique in terms of multilingualism, the number of samples still remains a significant concern for the proposed dataset. It may be out of the scope of this work to collect more samples for these languages due to resource constraints. However, the usage of this dataset in the data-driven community will be limited for developing a reliable translation system.
* The primary concern with the dataset is ethical issues. As the dataset is specific to a particular domain, it is challenging to validate the impact of the dataset in a negative direction. Moreover, the authors claim that the data strictly and entirely belongs to JW. It would be good to go through an ethical review to make it available for the data-driven natural language processing community.","* The primary concern with the dataset is ethical issues. As the dataset is specific to a particular domain, it is challenging to validate the impact of the dataset in a negative direction. Moreover, the authors claim that the data strictly and entirely belongs to JW. It would be good to go through an ethical review to make it available for the data-driven natural language processing community.",182,0
JnJsaXfVte,EMNLP_2023,"1. similar taxonomies of approaches for learning from disagreeement have been proposed, and more in general comparisons between different approaches on the same have been made, in a number of previous papers, such as Uma et al 2021 that the authors cite:
Alexandra N. Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. 2021b. Learning from disagreement: A survey. Journal of Artificial Intelligence Research, 72:1385– 1021.
the paper would benefit from some explanation of how the analysis proposed here differs from earlier work other than the different application
2. The proposed model with separate encoders for text and annotators that then alternatively feed into a shared classifier or separate classifiers for each annotator is elegant, but highly reminiscent of a variety of 'learning from crowds' approaches also separately modelling text and annotators, it would be good to have some discussion of the advantages of this particular model.
3. Strangely, the authors do not consider a class of 'hybrid' models which have been shown to be highly effective both in Uma et al 2021 survey and in the recent SEMEVAL Le-Wi-Di shared task (Leonardelli et al, 2023) - namely, the 'soft label' approaches in which the annotator votes are merged but preserving the existence of differences through a probability distribution.","3. Strangely, the authors do not consider a class of 'hybrid' models which have been shown to be highly effective both in Uma et al 2021 survey and in the recent SEMEVAL Le-Wi-Di shared task (Leonardelli et al, 2023) - namely, the 'soft label' approaches in which the annotator votes are merged but preserving the existence of differences through a probability distribution.",183,0
vVrwnY76W1,EMNLP_2023,"There are many reasons to reject this paper:
- The paper is poorly written and structured. It contains words such as ""explore *stuffing* the LLM’s input buffer"" and ""It is *seductive* to look at our experience with GPT-J"" that are not suitable for an academic paper.
- The paper's contribution is minimal. It's trivial to see that including more context for GPT-J will improve the result compared to only giving it the previous action and observation, and including less training data will reduce that result.
- The experiments design are not clearly motivated. Specifically, the **no variations** and the **Up to 18 games** finetuning methods.
- The authors do not test their model on novel unseen task, just a variation of a task seen during training.
In general, I think the paper requires serious revisions and would then might be considered a good workshop submission, but not a conference submission.","- The experiments design are not clearly motivated. Specifically, the **no variations** and the **Up to 18 games** finetuning methods.",184,0
vMpmabFTFw,EMNLP_2023,"- The in-distribution performance is not reported. It is unclear whether the proposed method favors the specific compositional generalization setups studied in this paper, but may not be generally applicable.
- The reported results are only one run. It would be better to include mean and variance from several runs, since most models are small size.","- The in-distribution performance is not reported. It is unclear whether the proposed method favors the specific compositional generalization setups studied in this paper, but may not be generally applicable.",185,0
e8wYLib8HC,EMNLP_2023,"1. The authors should assemble and implement a combination of existing chunking methods and Weight-Sharing methods for a proper baseline, given:
a. The method relies on sequence chunking. It is widely explored in long-sequence modeling.
b. Weight-Sharing, Adaptive-Depth, or their combination are also widely explored in sequence modeling.
2. Long segments might be split into two chunks, which means RegularGPT is sensitive to chunking methods. We can see an example for PARITY. C=2 performs well and is an optimal chunking method (i.e., static size = 2) because RegularGPT can model the composition of two transition functions. Therefore, I think the author should experiment with the different chunking methods (e.g., static, dynamic, overlapping, etc.) to observe the performance and understand RegularGPT in depth.
I am happy to discuss these concerns.
--------------------------After Rebuttal--------------------
I have no major concerns.","1. The authors should assemble and implement a combination of existing chunking methods and Weight-Sharing methods for a proper baseline, given: a. The method relies on sequence chunking. It is widely explored in long-sequence modeling. b. Weight-Sharing, Adaptive-Depth, or their combination are also widely explored in sequence modeling.",186,0
0n92zm014A,EMNLP_2023,"1. The idea that using pseudo-inputs as in-context examples is not novel.
2. The paper lacks an in-depth analysis of the impact of various types of pseudo-inputs. For instance, it would be insightful to understand which kind of pseudo-inputs are most effective as in-context examples and how to evaluate their quality.
3. Despite leveraging diversity hints, the language model may generate a mixture of high and low-quality pseudo data (e.g., incorrect pseudo-labels). Therefore, it is crucial to develop methods for automatically filtering out noise data and mitigating the impact of low-quality examples, rather than relying solely on the use of ""new,"" ""diverse,"" and ""creative"" prompts to enhance diversity.","2. The paper lacks an in-depth analysis of the impact of various types of pseudo-inputs. For instance, it would be insightful to understand which kind of pseudo-inputs are most effective as in-context examples and how to evaluate their quality.",187,0
czxX6jjpVJ,EMNLP_2023,"1. This paper raises a broad question: shortcut reasoning. While as mentioned in section 2.1, the proposed method only discovers word(token) triggered shortcuts. Such kind of phenomenon may only contribute to a small proportion of shortcut reasoning (shallow heuristics, artifacts). Also, the task is limited to sequence classification. I think it could be better if these settings are clearly stated at the very beginning of the paper (abstract or introduction).
2. The method description is somehow confusing in detail. Some evidence may not support the claim. I have raised some questions below.
3. The experiment section is lack of details. I think it is necessary to provide more experiment details (settings such as which split does OOD dataset use, results such as overall f1 score $F1(D_{OOD}, f)$, more analysis on unknown shortcut reasoning) in order for the paper to be accepted (it is fine to place them in the appendix). In addition, There is also no mention of the code being released.","1. This paper raises a broad question: shortcut reasoning. While as mentioned in section 2.1, the proposed method only discovers word(token) triggered shortcuts. Such kind of phenomenon may only contribute to a small proportion of shortcut reasoning (shallow heuristics, artifacts). Also, the task is limited to sequence classification. I think it could be better if these settings are clearly stated at the very beginning of the paper (abstract or introduction).",188,0
DpNUrB6SeZ,EMNLP_2023,"1. This proposed method seems to be the multi-source adoption of an existing method MSDP (https://arxiv.org/abs/2203.08745). It seems to be incremental that the new method just adds multiple knowledge sources and in-context learning with demonstration retrieval (such as https://arxiv.org/pdf/2305.04320.pdf). Thus, the work lacks core novelty.
2. There is no ablation experiment comparing MSDP'performance with in-context learning and that only with zero-shot. There is no experimental support for the claim that in-context learning actually helps LLMs in inferring emotion, topic, and event knowledge.","2. There is no ablation experiment comparing MSDP'performance with in-context learning and that only with zero-shot. There is no experimental support for the claim that in-context learning actually helps LLMs in inferring emotion, topic, and event knowledge.",189,0
jAf0gd0ez4,EMNLP_2023,"1. The description of the model is not clear enough, and there are some confusing points.
2. It does not take into account the error of real users in the input process and lacks the robustness study of the model.
3. For the part of the imitation learning model, the details such as ensuring accuracy are not clearly presented.",2. It does not take into account the error of real users in the input process and lacks the robustness study of the model.,190,0
dnQI76LKQy,EMNLP_2023,"- I'm not entirely sure how metaphors are defined in the images+texts of this dataset. I cannot understand why some images are metaphorical in the Appendix when only looking at the images. The text descriptions are in Chinese.
- This is a dataset paper, and the annotation guidelines are particularly important for metaphors. Metaphor annotation is notoriously challenging because it is fuzzy and subjective, and multimodal metaphors are even more under-explored. However, the paper doesn't provide sufficient detail about the annotation guidelines and justifications (e.g., how a metaphor is defined and recognized, how sentiment is defined), although the details of the annotation process are provided.
- As a dataset paper, I expect more insightful discussion about the data so that I can determine whether I want to use this dataset or not. For example, what are the common disagreements in the annotations? What kind of characteristics do you observe in the multimodal metaphors? They are all advertisements. Are there any interesting things you can observe with regard to that? This kind of discussion can give a better sense of the dataset and the tasks that can be performed with it.
For these reasons, I think this paper needs more work although I appreciate the work.","- This is a dataset paper, and the annotation guidelines are particularly important for metaphors. Metaphor annotation is notoriously challenging because it is fuzzy and subjective, and multimodal metaphors are even more under-explored. However, the paper doesn't provide sufficient detail about the annotation guidelines and justifications (e.g., how a metaphor is defined and recognized, how sentiment is defined), although the details of the annotation process are provided.",191,0
BNcTB8RZfG,EMNLP_2023,"1. This paper includes some concepts that are not clearly defined/explained, e.g., the following ones (also see the questions to the authors):
- User-provided information and user scenarios need to be explained in more detail.
- The meaning of ""the weakly supervised alignment label"" (lines 248-149) is not clear.
- Data augmentation is not well explained. Although the authors claim that ""data augmentation improves all generation metrics"" (474-475) it's not clear how exactly they augment the data.
2. The statements about the performance improvements seem a little bit over-emphasized. The authors claim that ""BiAE significantly outperforms baselines with lightweight decision modules by at least 12.7% in micro accuracy"" (090-092). However, on the ShARC leaderboard the difference between the proposed model and the previous SOTA is only 0.5% not 12.7%.
3. The overall computational budget is only partially reported (e.g., no information about how many hours it takes to tune all the parts of the pipeline and the additional costs of the fine-tuning and data augmentation).
4. The authors mention ""significant improvements"" but as far as I understand they did not perform any significance tests.
5. The paper is slightly longer than 8 pages.","2. The statements about the performance improvements seem a little bit over-emphasized. The authors claim that ""BiAE significantly outperforms baselines with lightweight decision modules by at least 12.7% in micro accuracy"" (090-092). However, on the ShARC leaderboard the difference between the proposed model and the previous SOTA is only 0.5% not 12.7%.",192,0
XJRNw74kXK,EMNLP_2023,"1. The results might not be very general for actual “real-world understanding” of LLMs. Size comparison is still just one part of the bigger picture about real-world understanding.
2. The link to the dataset is not active. So, the claims made about the dataset can’t be verified. I would request the authors to look into it.
3. No details about the human annotation process used for comparison with ChatGPT.",1. The results might not be very general for actual “real-world understanding” of LLMs. Size comparison is still just one part of the bigger picture about real-world understanding.,193,0
1Sn1dpNaP3,EMNLP_2023,"1) The paper lacks significant contributions in terms of introducing new datasets, algorithms, or novel discoveries.
2) Various models underwent training for differing epoch counts, and any performance discrepancies could be linked to this factor.
3) The evaluation, as highlighted in the paper, is confined to a narrow selection of small datasets. It would be advisable to assess the approach across a broader spectrum of datasets.
4) The paper primarily revolves around fine-tuning models for financial data, yet it doesn't introduce any substantial new insights. Hence, it might find better alignment within workshops specifically focused on Finance NLP.","1) The paper lacks significant contributions in terms of introducing new datasets, algorithms, or novel discoveries.",194,0
PNpRxOhVut,EMNLP_2023,"1.	The paper does not provide much interpretation or intuition for the eigenvectors with larger eigenvalues, and what kind of features they capture. It would be preferable to provide some examples or visualizations from a high-level viewpoint.
2.	The paper does not compare or discuss the relation with other works that use spectral methods or feature decorrelation techniques for different tasks or settings. It would be helpful to see how the proposed method relates to or differs from some related works.",2. The paper does not compare or discuss the relation with other works that use spectral methods or feature decorrelation techniques for different tasks or settings. It would be helpful to see how the proposed method relates to or differs from some related works.,195,0
8IrFLWRvuW,EMNLP_2023,"1.continuous diffusion is not an effective way to generate natural language, as it cannot capture the discrete semantic information within natural language. Besides, it also greatly increases the cost to generate the text, as it requires to sample for multiple times for denoising. Recent work[1][2] has shown that discrete diffusion may be a good choice, and can outperform continuous diffusion methods. Authors should try its approach on these methods and show the generality of it.
2.the novelty of this work is worth concerning, as the new schedule is similar to the used one in diffusionBERT[1]. Besides, using self-condition is not a novel way, as it has also been widely-used in diffusion models, e.g., Diffusion-NAT[2].
3.the experiments are conducted on some unpopular datasets, but not the commonly-used summarization, dialog and question generation, such as XSum, Persona-chat and Squad. Although DiffuSeq has used these datasets, readers would be more concerned about the performance of all these methods on the popular datasets. I suggest authors to follow the used datasets and experimental settings in Diffusion-NAT[2].
[1] DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models
[2] Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation","1.continuous diffusion is not an effective way to generate natural language, as it cannot capture the discrete semantic information within natural language. Besides, it also greatly increases the cost to generate the text, as it requires to sample for multiple times for denoising. Recent work[1][2] has shown that discrete diffusion may be a good choice, and can outperform continuous diffusion methods. Authors should try its approach on these methods and show the generality of it.",196,0
2kSufHoYEi,EMNLP_2023,"1. My main concern is about the contributions: (1) Although this paper claimed their first trial on multilingual and multi-cultural norm discovery in multi-turn conversations, some previous multi-lingual information extraction work and tools can be easily applied here. Besides, I cannot find any specific design in NormSage to resolve the multi-cultural challenges (Please correct me if I'm wrong). It seems that this framework can be applied to other tasks? (2) NormSage is a reasonable pipeline framework to prompt GPT-3 to automatically discover and verify norms. Nevertheless, the contribution of a framework with several simple prompts is not sufficient for a technical paper.
2. NormSage is a GPT-3 based framework; however, most baselines in Table 2 are much smaller than GPT-3, such as GPT-2. That's not enough to demonstrate the advantages of NormSage. More competitive baseline models should be incorporated. The same problem occurs for Table 7 regarding the comparison between NormSage (GPT-3 based) and BART or T5-based models.
3. Lack of illustration about how to use the NormKB for downstream tasks.",3. Lack of illustration about how to use the NormKB for downstream tasks.,197,0
QkCYv3TlGk,EMNLP_2023,"- The training methodology is not well explained and is hard to follow.
- The section 2.4. is showing the addition of several loss from each of the components. To me is not clear what L function stands for and most of the different losses are not explained in the text or some weighting parameters as alpha, beta, gamma or lambda, the latter seeming to control the cycle content loss and the speaker id loss.
- It is also hard to understand the MI loss obtained from the C_2 discriminator.
- Low number of speakers are reported in the tables for English.",- Low number of speakers are reported in the tables for English.,198,0
HsGirsKN5l,EMNLP_2023,"Most importantly, the problems that the authors claim are needed to be well-supported by either logic or experiments.
1. The authors claim that LAA is motivated by the assumption that the document-level NMT model may interfere with the attention mechanism since it needs to handle a wide range of context. However, [1] explains that the actual amount of context needed is not as much as the previous few (1~3) sentences. It might be related to the issue of entropy divergence, which is newly addressed by the authors. The paper is stated that it would explain in an appendix but it doesn't exist.
2. The authors also claim that SD is motivated by the assumption that the contextual information may be lost when decoding longer sequences in segments. Considering [1], it is not expected to be a problem, as the necessary context is preserved even if it is split.
The lack of performance gains also supports this suspicion.
[1] Fernandes et al., 2021, Measuring and Increasing Context Usage in Context-Aware Machine Translation","2. The authors also claim that SD is motivated by the assumption that the contextual information may be lost when decoding longer sequences in segments. Considering [1], it is not expected to be a problem, as the necessary context is preserved even if it is split. The lack of performance gains also supports this suspicion. [1] Fernandes et al., 2021, Measuring and Increasing Context Usage in Context-Aware Machine Translation",199,0
hn0B3jTlwE,EMNLP_2023,"1. There is still a lot of room for improvement in the task performance of the model.
2. The evaluation metric for toxicity may not be so reliable. The Pespective API ofen makes mistakes.
3. This method's performance greatly relies on the datastores constructed from the datasets.",2. The evaluation metric for toxicity may not be so reliable. The Pespective API ofen makes mistakes.,200,0
rjd8AqRyW3,EMNLP_2023,"The limitations of this paper lie primarily in the verification process and data characteristics.
- After the data was created by the annotators, the authors of the paper performed a curation process. However, there was no objective verification of this curation process. This raises the question of whether the dataset may be influenced by biases from the authors. It would be necessary to include not just the paper's authors, but also an external expert in the verification step of the curation process.
- As indicated in Table 1, there are 1,266 aspects and 1,310 instances. This implies that the aspects are varied and instances are very sparse on an aspect. The nature of open aspect may naturally lead to a variety of aspects, but it also results in sparseness, making it difficult to conduct experiments based on aspects. Additionally, even when instances share the same aspect, they may come from different domains, limiting their use in domain adaptation studies. This points to limitations in terms of data usability.
- This paper did not provide any inter-rater agreement statistics in the data quality experiments. It was stated that disagreements were resolved, but there was no explanation regarding the extent of discrepancies among the annotators.","- This paper did not provide any inter-rater agreement statistics in the data quality experiments. It was stated that disagreements were resolved, but there was no explanation regarding the extent of discrepancies among the annotators.",201,0
59gI2XQPmH,EMNLP_2023,"1. The paper mainly focuses on the English language, and it is unclear how well the proposed method would generalize to other languages.
2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios.
3. The paper does not provide a detailed analysis of the impact of different hyperparameters on the performance of the proposed method, which could limit the understanding of the robustness of the method.
4. It is better to compare with other LLM-based NER methods, which seems to be more suitable for zero-shot and open-vocabulary.","2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios.",202,0
XbcprEi57p,EMNLP_2023,"1) While the core ideas of the proposed modules, Contextual Embeddings, and Progressive Alignment, are cited as widely referenced in related fields, the paper lacks a clear and thorough clarification of how these concepts differ from existing approaches in the context of the specific problem addressed in this paper. It would be beneficial to provide a more detailed comparison with related works to highlight the novelty of the proposed techniques.
2) The paper could benefit from a more explicit and concise description of the challenges faced in the stage reasoning problem and the precise motivations behind the design of JMCELN and PAN.
3) The visualizations presented in Figures 3 and 4 illustrate the effectiveness of the proposed method. However, the paper could be improved by providing additional examples where the proposed approach might fail or show limitations. This would allow readers to gain a better understanding of the proposed method's capability boundaries and its performance in various scenarios.","3) The visualizations presented in Figures 3 and 4 illustrate the effectiveness of the proposed method. However, the paper could be improved by providing additional examples where the proposed approach might fail or show limitations. This would allow readers to gain a better understanding of the proposed method's capability boundaries and its performance in various scenarios.",203,0
8gYRHspcxK,EMNLP_2023,"- The core assumption that the response from a larger LLM with more and better demonstrations might be better overall is not conprehensively studied. Although the performance of the trained reward model can be viewed as a indirect evidence supporting the assumption, it would still be benefitial to understand when and how the assumption may not hold through a systematic evaluation on the output of various LLM configurations used.
- From Table 3, the heuristic filter seems to play an important role in the synthetis data generation process. While sub-optimal, the filter requires a fair amount of expert knowledge and empirical experience to design and improve, which conflict with the motivation of reducing human effort.
- It is unclear how to use the apporach to align larger models. For example, as the supervised fine-tuning data used to train the 7B model in the paper is produced by a well-prompted 30B model, what would be the teacher model to produce data for aligning a 30B model?","- From Table 3, the heuristic filter seems to play an important role in the synthetis data generation process. While sub-optimal, the filter requires a fair amount of expert knowledge and empirical experience to design and improve, which conflict with the motivation of reducing human effort.",204,0
56UYArtXyA,EMNLP_2023,"1. Lacking comparisons with other baselines. There are many other AL sample-selection strategies, such as [1].
2. Leveraging LLMs as training data generators has been explored before. For instance, SunGen[2] and ZeroGen[3]. Comparison with these sota methods is necessary in my view. Also, the authors should consider some distillation methods as baselines as utilizing LLMs as annotators are kind of distillation.
[1] Yehuda, Ofer, et al. ""Active learning through a covering lens."" Advances in Neural Information Processing Systems 35 (2022): 22354-22367.
[2] Ye, Jiacheng, et al. ""Zerogen: Efficient zero-shot learning via dataset generation."" arXiv preprint arXiv:2202.07922 (2022).
[3] Gao, Jiahui, et al. ""Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning."" The Eleventh International Conference on Learning Representations. 2022.","1. Lacking comparisons with other baselines. There are many other AL sample-selection strategies, such as [1].",205,0
855dPxyaex,EMNLP_2023,"- The data was constructed with only specific patterns of hate, which may introduce biases. Although the authors acknowledge this limitation, they should include some analyses of potential biases or more discussion of this.
- The claim that previous work has only considered generic denouncing arguments or synthetic data is not correct. See note in missing references.","- The data was constructed with only specific patterns of hate, which may introduce biases. Although the authors acknowledge this limitation, they should include some analyses of potential biases or more discussion of this.",206,0
Q9BLbN1p6h,EMNLP_2023,"1. The experiment results are not strong enough to support the effectiveness of the proposed method. Though the authors argue that the proposed method perform considerably better in the four tasks with the highest number of training examples, I don't think improvement less than 1% can be viewed as ""considerably better"". For this claim, I think the authors should at least report the standard deviation in Table 2 to exclude the influence of randomness.
2. About the experiment setup, I think the authors should compare the results of the proposed method with the results of fine-tuning the full PLM. Though I'm aware that the method falls in the PEFT setting, I think the paper should compare the full fine-tuning to see whether the additional linguistic knowledge is indeed beneficial to the PLM.
3. The ablation study is missing. The paper should do ablation study to the involved linguistic information sources and the proposed expert pruning component.","2. About the experiment setup, I think the authors should compare the results of the proposed method with the results of fine-tuning the full PLM. Though I'm aware that the method falls in the PEFT setting, I think the paper should compare the full fine-tuning to see whether the additional linguistic knowledge is indeed beneficial to the PLM.",207,0
KUSzNKRI2g,EMNLP_2023,"- The experimental design for comparing the CONCOCT system with the baseline hierarchical summary generator was insufficient and could be considered biased.
- The evaluation metrics are insufficient. The authors claim in line 2 that inconsistent pacing affects the reader experience negatively, thus it is expected to evaluate whether the proposed system successfully improve the likability of the generated story.","- The evaluation metrics are insufficient. The authors claim in line 2 that inconsistent pacing affects the reader experience negatively, thus it is expected to evaluate whether the proposed system successfully improve the likability of the generated story.",208,0
d0qmGnKfXa,EMNLP_2023,"- One of the main weakness of this paper is the lack of excitement. The idea of using downstream task loss to improve the intermediate component is not new and has been explored by many prior work. For example, Choi et al. 2016 used QA loss as indirect supervision and using RL to improve the sentence retriever; Lewis et al., 2020 used the two different decoder loss to jointly train the retriever and the decoder. This work is an instantiation of such techniques on FEVER.
- I got confused by the equation at line 165. Here, if I understand it correctly, $R_\theta(c, S)$ represents the fine-grained retriever and it returns a ""hard"" sentences selection from $S$. If it's a ""hard"" selection, how is the loss gradient passed to the fine-grained retriever? I also got confused by the results in Table 1 and Table 2. See the following question section for details. Reference:
- Choi, Eunsol, et al. ""Coarse-to-fine question answering for long documents."" Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.
- Lewis, Patrick, et al. ""Retrieval-augmented generation for knowledge-intensive nlp tasks."" Advances in Neural Information Processing Systems 33 (2020): 9459-9474.","- Choi, Eunsol, et al. ""Coarse-to-fine question answering for long documents."" Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.",209,1
hiJ2hzwghq,EMNLP_2023,"1. My main reason to reject is the design to answer the 2nd research question: ""Do biasx enable moderators to think carefully about moderation decisions"".
First, the MTurk workers are randomly assigned to one of the four conditions. There may be chances that some workers performing tasks (even other tasks) very fast may fall in the same condition. Do the authors considering calculate labeling time for each worker during qualification stage, and re-calculate that in each condition to reduce individual labeling speed biases?
Second, while the intuitive makes sense, ""think carefully"" does not necessarily correlate with ""labeling time"". For example, some crowd workers may perform multiple tasks, switch between tabs, which may result in an increase in labeling time too. It would be better to base this on a theory, data or literature. Or mention this in the limitations.
2. ""crowdworker are asked to play the role of content moderators"", it should be better justified. The role, standard, or platform for moderators may vary a lot.","2. ""crowdworker are asked to play the role of content moderators"", it should be better justified. The role, standard, or platform for moderators may vary a lot.",210,0
wtqb7pNL4e,EMNLP_2023,"1.	It is better to conduct more experiments on other LLMs, which can demonstrate the generalization of the proposed framework.
2.	In section 3.3, the core idea of correctness-evaluated instruction, that is, convert the original agreement-measured instruction into correctness-evaluated instruction. It is suggested to explain why does substituting instructions avoid generate a neutral opinion?
3.	Although the focus of this work is to devise a general evaluation framework, it is suggested to further explore the assessment of a specific subject to digging into its deeper significance.","1. It is better to conduct more experiments on other LLMs, which can demonstrate the generalization of the proposed framework.",211,0
sCxiD2Rx4l,EMNLP_2023,"1. The dataset is automatically generated using GPT-4 and GPT-3.5, lacking an assessment of data quality. It would be beneficial to include an evaluation of the dataset's quality.
2. The retrieval-enhanced models use BM25 and GPT-3 Embedding for retrieval. It would be beneficial to add experiments with supervised retrieval models, such as DPR and ANCE, trained on generic QA and retrieval data.
3. Providing evaluation results for recent instruction-tuned models like Vicuna and Llama-2-chat would further enhance the significance of the experimental results for future research.
4. The article should provide more in-depth discussion and analysis of the results in Table 4 related to the Debiasing tasks. It is important to understand why model editing does not consistently improve performance on Split I/II data.",3. Providing evaluation results for recent instruction-tuned models like Vicuna and Llama-2-chat would further enhance the significance of the experimental results for future research.,212,0
JWMIm1EyaE,EMNLP_2023,"Nothing I'm writing here is a reason to reject, really (though I have some questions in the ""Questions For The Authors"" section that may impact my sense of this). But there are some minor weaknesses:
* The method presented relies on external phrasal alignment models (this is also a strength as noted above!)
* Some details are unclear (see ""Questions For The Authors"")
* While the authors argue strongly (and, I think, correctly) that phrasal highlights are more useful than simpler token-based highlights, the human evaluations do not directly test this. It's conceivable that, for example, annotators would find the simpler token-based highlights just as useful as the contrastive phrasal ones. While I think this is unlikely, it undermines the overall argument somewhat. To be clear, I'm not especially bothered by this as the very fact that the authors included application-grounded evaluations is a useful contribution (and we can make some reasonable inferences based on the results from the proxy evaluations).
* There's no indication that the authors will release their code or the data presented to annotators for the studies in sections 5 and 6 (including annotations from the participants). I would highly encourage this, especially since it can help clear up details that might be unclear from the paper's main text (or even in appendices). Similarly, including full details of the ChatGPT prompts in section 5.1 (beyond just footnote 5) would help reproducibility.","* There's no indication that the authors will release their code or the data presented to annotators for the studies in sections 5 and 6 (including annotations from the participants). I would highly encourage this, especially since it can help clear up details that might be unclear from the paper's main text (or even in appendices). Similarly, including full details of the ChatGPT prompts in section 5.1 (beyond just footnote 5) would help reproducibility.",213,0
ICLR_2021_784,ICLR_2021,"- The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2 - In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper. - As the authors of the TFN paper note: in practice not all higher order irreps of the tensor product of the filter and the features are computed. This seems to indicate a big difference between the theoretical analysis - which includes all irreps and thus is computationally intensive even when modelling low order polynomials – and the practical application of TFNs. It would be interesting to know how expressive such practical low order TFNs are. Another difference between the described networks and practical TFN is that in the described networks, all relevant parameters are in the pooling layer, which sums a large number of terms (looking at the proof of lemma 2, exponential in D), while in practical TFNs, the parameters are in the filters.
Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper.
Suggestion for improvement: - Make the big picture clearer by providing more intuition. - Comment on the differences between the class of networks described and TFNs used in practice.
Minor points/suggestions: - P3 Add definition of W^n_T as n direct sums of W_T - P3 “where W_feat is a lifted representation of SO(3)”, what does lifted representation here mean? Just any rep? - I get a bit confused by the wording in Def 1. Unless I am mistaken, it appears like the quantifiers are reversed. Should it mean “for every polynomial …, there exists f_1, … in F_feat and linear functionals Lambda_1, …, : W_feat -> R ”? - Around Eq 5, perhaps the authors could clarify the clarify the domain of the Q functions, which I suppose is Q^r : R^3n -> T_T, where T= r _1 - Around Eq 7, are X_j and x_j the same? - In lemma 4, is A_k any linear map or an equivariant linear map? - In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1? - In the proof of thm 1, it says “p: R^{d \times n} \to W_T”, should that be W_T^n? - In the proof of lemma 2, it says “we see that that exists a linear functional”
Post rebuttal
I thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas. My previous rating still applies.","- In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1?",214,1
ICLR_2021_1986,ICLR_2021,"Strengths:
This paper unifies a number of straight through estimators under a common framework which helps in motivating previously considered 'ad-hoc' rules for gradient backpropagation, and helps understanding of the conditions which may affect the bias/performance of the estimators.
The paper is technical but written in such a way that it is still relatively easy to follow, by keeping notations simple (yet clear) and deferring detailed derivations/proofs to the appendix (which I did not go through in detail).
The paper shows theoretically and experimentally that different design choices for the estimator are possible, as long as different parts (model, initialization and training) are well aligned. Weaknesses:
As a weakness, the paper feels a bit as an enumeration of related but separate contributions (summed up in the paragraph 'contributions'). The experiments mainly concern section 4 and show somewhat the effect of Bias Analysis IV), but it would improve the paper if (toy) experiments would have been conducted relating to the other analysis in section 2.2 as well, which could then justify the practical relevance of these results.
While the paper motivates the reintroduction of ST as principled methods, the experiments give some insights but do not fully convince of the practical use of the proposed extensions.
It is positive that the paper is largely self-contained, but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work. Also, the paper lacks a discussion section and as a result feels a bit unfinished.
3. Recommendation
My current recommendation is to accept the paper.
4. Arguments for recommendation
This paper helps in the theoretical understanding and unifying view of a variety of straight through estimators, which may help advancing these estimators for training networks with binary weights and activations, which is a relevant and difficult problem.
5. Questions to authors
Why is a discussion section omitted?
Figure 1: looking at the y-axis for the top row, it seems that both ARM and ST get worse results with 256 bits than 8 or 64 bits, which hints at underfitting. How does this affect the conclusions?
Additional feedback
Minor comments
Line 152: where does the symbol f for loss come from?
Line 305: a reference is missing: ??
Line 331,5: reminder -> remainder?
Line 372: ELOB -> ELBO? Or -ELBO (negative ELBO)?
Some grammar could be improved, e.g. in abstract 'we … obtains, …, explains'","4. Arguments for recommendation This paper helps in the theoretical understanding and unifying view of a variety of straight through estimators, which may help advancing these estimators for training networks with binary weights and activations, which is a relevant and difficult problem.",215,1
ICLR_2021_343,ICLR_2021,"1. This paper is not well-organized and many parts are misleading. For example, above Eq.3, the author assumes P_{G_{0}} = P_{D}. Does the author take the samples generated by the root generator as the authentic dataset? However, in Section 2 above Eq. 4, the author claims that the authentic data does not belong to any generator. 2. In Eq.4, the key-dependent generator is obtained via adding perturbation to the output of the root model. This setting may be troublesome as :1. These generators are not actually trained. This is different from the problem which this paper tempt to solve. 2. No adversarial loss to guarantee the perturbed data being similar to the authentic data. 2. How to distinguish the samples from different generators. 3. Since Eq.4 is closely related to adversarial attack, the authors are supposed to discuss their connections in the related works. 4. The name of ‘decentralized attribution’ is misleading. Decentralized models are something like federated learning, where a ‘center’ model grasps information from ‘decentralized models’. However, the presented work is not related to such decentralization. 5. Typos: regarding the adversarial generative models ->regarding to the adversarial generative models; along the keys->along with the keys.","3. Since Eq.4 is closely related to adversarial attack, the authors are supposed to discuss their connections in the related works.",216,0
ICLR_2021_1894,ICLR_2021,"1. The graph partition idea is interesting, but more details on how to apply METIS is expected. 2. The experiments can be further enriched.
Detailed comments: 1. In this paper, it has mentioned many related studies. But in the experiment section, only a small subset of the mentioned methods are treated as baselines. Also, for the data partition-based baselines, only throughput is compared. How about the accuracy comparison? Which method needs more epochs to converge, which will affect the run-time in addition to the throughput? For the sampling-based methods, more methods are expected to be compared to the proposed approach. In addition to accuracy, run-time comparison is also expected.
For the sampling-based approach, one more baseline is expected:
Zou et al., Layer-Dependent Importance Sampling for TrainingDeep and Large Graph Convolutional Networks, NeurIPS 2019.","1. The graph partition idea is interesting, but more details on how to apply METIS is expected.",217,0
ICLR_2021_2182,ICLR_2021,"weakness I see in the experiments is that in the introduction, you mention previous work on fuzzy discrete distribution clustering (by de Carvalho et al.). Still, you do not compare your results to their method (does adding the diagonal make such a big difference?).
Minor comments and questions:
• Related work: When you mention the correlation of topological complexity with generalization ability, you miss the work by Rieck et al. [2].
• I am a fan of citing software libraries when they provide how to in their website. Hence I would recommend citing Ripser [3] (see https://github.com/Ripser/ripser for BibTeX entry) and potentially other Software you used in the main paper.
• You say “[…] the vectorization […] required by Lacombe et al.’s algorithm makes it unsuitable for integration into our work”. Could you elaborate on this? Can you not even choose this as a comparison partner?
• What do you mean when you say other persistence-based learning strategies required prior knowledge of a ‘correct’ target topology which can’t plausibly be known? The work by Moor et al. for example, extracts persistence directly from the input, which is “trivial” to know since you can compute it.
• Topological Preliminaries: Why do you introduce the \v{C}ech complex when you use Vietoris-Rips for your experiments?
• One feat of your approach is that you add tuples on the diagonal to make the cardinality of all compared PDs match. I wonder how susceptible your approach is to increasingly different cardinality (that has to be filled). Maybe you can conduct a small ablation study on your synthetic data set and evaluate how “good” your mean PDs are when cardinalities are increasingly different.
[1]: RJGB Campello, ER Hruschka. A fuzzy extension of the silhouette width criterion for cluster analysis. Fuzzy Sets and Systems, 2006 – Elsevier
[2]: Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, Karsten Borgwardt. Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology. ICLR 2019.
[3]: Ulrich Bauer. Ripser: efficient computation of Vietoris-Rips persistence barcodes. Preprint.
====================== Update: Thank you for your rebuttal, I still think this is a promising paper and lean towards acceptance. However, after the discussion and the update of the manuscript, my score will remain the same.",• You say “[…] the vectorization […] required by Lacombe et al.’s algorithm makes it unsuitable for integration into our work”. Could you elaborate on this? Can you not even choose this as a comparison partner?,218,0
ICLR_2021_2674,ICLR_2021,"Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.","- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?",219,0
ICLR_2021_425,ICLR_2021,"It seems like a time complex scheme for decoding based on the formulation
Lacks generated samples in the main paper for qualitative assessment.
Human evaluation missing. Questions:
Current neural text generation models are trained to optimize the cross-entropy loss. In an ideal scenario lower the cross-entropy (hence perplexity), the better should be the samples. Since this work (as well as its corresponding related works) show that humans don't form sentences that have the lowest perplexity (when measured using the trained model), does this work help in highlighting that cross-entropy loss might not be the correct objective for optimization?
In Algo 1: Do you first perform top-m for obtaining s and then perform top-k later based on the value obtained? If so, what is the time complexity of this process, as a function of m?
From Fig 4(c), it seems that humans prefer sentences having cross-entropy rates ~ 5.25 on avg. Why do you suggest 3 as the target average surprise?
The point about ad-hoc settings being applied for top-p and not for mirostat seems weird. This paper suggests 3.0 as the setting for target average surprise (based on empirical data) while top-p suggests ~0.9 as the setting for p (again based on empirical data - and shown in this paper that CE remains more or less constant wrt the number of generated tokens at p=0.9). In the end, a certain bit of ad-hoc tuning needs to be made for both cases. Why would this algorithm be better than nucleus sampling (Given it is more time consuming)? Overall:
a) The theoretical analysis helps in understanding why top-k and top-p perform the way they do.
b) Based on the underlying assumptions:
Zipfian distribution
cross-entropy loss optimization
Lowest perplexity does not always mean good quality generation.
(I somehow feel a lot of disconnect between assumption 2 and 3), this work proposes an algorithm for adaptive top-k decoding to produce arguably ""better"" samples.
Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update:
Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.","3), this work proposes an algorithm for adaptive top-k decoding to produce arguably ""better"" samples. Please let me know if I am misunderstanding something. I am open to revising my assessment based on the answers to the questions raised. Update: Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m.",220,0
ICLR_2021_1744,ICLR_2021,"Weakness:
This work simply applies the meta-learning method into the federated learning setting. I can’t see any technical contribution, either in the meta-learning perspective or the federated perspective. The experimental results are not convincing because the data partition is not for federated learning. Reusing data partition in a meta-learning context is unrealistic for a federated learning setting.
The title is misleading or over-claimed. Only the adaptation phase costs a few rounds, but the communication cost of the meta-training phase is still high.
The non-IID partition is unrealistic. The authors simply reuse the dataset partitions used in the meta-learning context, which is not a real federated setting. Or in other words, the proposed method can only work in the distribution which is similar to the meta-learning setting.
Some meta earning-related benefits are intertwined with reducing communication costs. For example, the author claimed the proposed method has better generalization ability, however, this is from the contribution of the meta-learning. More importantly, this property can only be obvious when the data distribution cross-clients meet the assumption in the context of meta-learning.
The comparison is unfair to FedAvg. At least, we should let FedAvg use the same clients and dataset resources as those used in Meta-Training and Few-Rounds adaptation.
“Episodic training” is a term from meta-learning. I suggest the authors introduce meta-learning and its advantage first in the Introduction.
Few-shot FL-related works are not fully covered. Several recent published knowledge distillation-based few-shot FL should be discussed.
Overall Rating
I tend to clearly reject this paper because: 1) the proposed framework is a simple combination of meta-learning and federated learning. I cannot see any technical contribution. 2) Claiming the few round adaptations can reduce communication costs for federated learning is misleading, since the meta-training phase is also expensive. 3) the data partition is directly borrowed from meta-learning, which is unrealistic in federated learning.
---------after rebuttal--------
The rebuttal does not convince me with evidence, thus I keep my overall rating. I hope the author can obviously compare the total cost of meta-learning phase plus FL fine-tuning phase with other baselines.","2) Claiming the few round adaptations can reduce communication costs for federated learning is misleading, since the meta-training phase is also expensive.",221,0
ICLR_2021_78,ICLR_2021,"weakness in comparisons to comparable virtual environments is given later.
d. The different variant agents being compared in the task benchmark are clearly explained and form an appropriate experimental design.
2. Novelty/Impact
a. The work aims to showcase a new challenge task, evaluation platform, and benchmark agent performance for goal recognition followed by collaborative planning. The work as described compares favourably to similar work in evaluation platforms and benchmarks referenced in the related work section and appendix. The differences are made clear, though the use of some featured distinctions are not demonstrated in the paper (e.g. visual observations are possible but not used in the benchmarking).
3. Experimental Rigour
a. This work is not primarily about demonstrating the benefits of a particular approach over others in a particular application. It demonstrates benchmarks for agent performance in a newly introduce problem setting. From that perspective, the paper has strong experimental rigour. The experimental design is appropriate, comparing multiple baselines and oracles with several sets of experimental variants from both automated planning and reinforcement learning communities.
b. The comparison of experimental variants is conducted with both a computationally-controlled agent and a human-controlled avatar to evaluate collaboration performance in increasingly realistic settings.
c. The claim that the computationally-controlled Alice agent is human-like is repeated throughout the paper. This is not justified in the actual text of the main paper, but is supported to a moderate degree (human-like in strategy/planning if not movement/behaviour) through experiments with human subjects that are described in the appendix.
d. Effort paid to ensure diverse avatars in experimentation.
4. Reproducibility
a. The work is almost entirely reproducible, with details of all agent architectures used for experiments provided with hyperparameters and architecture design. The authors describe that the environment will be released as open-source, which will then make the article wholly reproducible. This reviewer appreciated the level of additional detail provided in the appendix to improve this area of evaluation.
3. Weaknesses
1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".
2. From the motivation provided, i.e. evaluating social perception and collaboration, why is the task specifically about watching then helping and not both together or just helping or just watching or some combination of these activities fluidly occurring throughout the interaction?
3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.
4. It would be clearer to specify that this task is limited to situations where there is explicitly only one goal throughout the entire demonstration + execution episode. This is important since it precludes using this challenge task for research into agents that need to use goal recognition after the initial demonstration, potentially continuously over the course of execution. This second kind of continuous goal monitoring is more similar to real-world applications of watching and helping or assistive agents or social robotics, since the human collaborator can (and often will) change their mind.
5. Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. This is similar to situations like team sports, where all that matters is the final game score. Many real-world scenarios for human-AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For example, in a computer game where Bob represents a team-mate to Alice who is a human player, Bob can choose to steam-roll Alice and win the game by itself. However, this leads to lower subjective user experience for the human team-mate. In this case, the score might be greater than what Alice could accomplish on their own and the game might be won faster than Alice could on their own, but the experience would be different based on whether they are truly collaborating or one is over-shadowing the other.
6. A final assumption, is that there is no difference in expertise between Alice and Bob. The human is expected to be able to competently finish the task and for Bob to linearly cut down the time taken to perform this task. There are many real-world tasks in human-AI collaboration where this assumption does not hold and there could be non-linear interactions between success-rate and speed-up due to different levels of expertise between Alice and Bob.
7. The fixed Alice agent is called human-like through out the article and this was not properly justified anywhere in the main text of the paper. However, the appendix actually describes results that compare the performance of the computationally-controlled and human-controlled variants of Alice to human observers. This potentially justifies this weakness. For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.
8. Why aren't there benchmark results (more than one) for the goal recognition component similar to the planning task experimentation? If both parts of the task are important, it would be valuable to provide additional experiments to show comparisons between goal recognition approaches as well, even if that is in the appendix for space reasons.
9. There could be more analysis of the benchmark agent performance, 1) Why does the purely random agent work relatively well across tasks? 2) Why doesn't HRL work better? Is this due to less hyperparameter tuning compared to other approaches or due to some intrinsic aspect of the task itself? 3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?
10. There are several comments about other environments in the related work section and appendix being toy environments. However, the tasks in the environment demonstrated in this paper only use a small set of predicates as goals. Similarly, it CAN generate visual observations but that isn't used by any of the baselines in the paper. Several comparisons to related virtual environments are made in appendix, but some of the features aren't used here either (humanoid agent - this challenge task works equally well with non-humanoid avatars/behaviours and realism - visual realism is present but it isn't clear if behavioural or physical realism is present due to seeming use of animations instead of physical simulation).
11. None of the tasks described allow the use of communication between agents or evaluate that. Other multi-agent environments like Particle Environments (below) allow for that. Communication is a natural part of collaboration and should have been mentioned if only to distinguish future work or work out of current scope.
a. @article{mordatch2017emergence, title={Emergence of Grounded Compositional Language in Multi-Agent Populations}, author={Mordatch, Igor and Abbeel, Pieter}, journal={arXiv preprint arXiv:1703.04908}, year={2017}}
12. ""planning and learning based baselines"", ""and multiple planning and deep reinforcement learning (DRL) baselines"", etc. - There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.
13. The human-likeness evaluation experiment asked subjects to evaluate performance one agent video at a time. A more rigorous evaluation might compare two agents side by side and ask the human to guess the human performance. This could also be in addition to the current evaluation. The current evaluation is a ceiling on performance while the comparative evaluation is a potential floor.
4. Recommendation:
1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.
5. Minor Comments/Suggestions:
1. Some minor typos in the manuscript:
a. Using the inferred goals, both HP and Hybrid can offer effective. - page 6
b. IN(pundcake, fridge) - appendix table 2
c. This closeness perdition - appendix page 19","3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.",222,0
ICLR_2021_1705,ICLR_2021,"and suggestions for improvement: I have several concerns about the potential impact of both theoretical and practical results. Mainly:
By referring to Wilson et al., 2017, the authors argue that diagonal step sizes in adaptive algorithms hurt generalization. First, I find this claim rather vague, as there has been many followups to Wilson et al., 2017, so I suggest the authors to be more precise and include more recent observations. Moreover, one can use non-diagonal versions of these algorithms. For example, see [2 and Adagrad-norm from Ward et al., 2019], it is easy to consider similar non-diagonal versions of Adam/AMSGrad/Adagrad with first order momentum (a.k.a. AdamNC or AdaFOM), then, are these algorithms also supposed to have good generalization? I think it is important to see how these non-diagonal adaptive methods behave in practice compared to SGD/Adam+ for generalization to support the authors' claim.
I think the algorithm seems more like an extension of momentum SGD, than Adam.
It is nice to improve \eps^{-4} complexity with Lipschitz Hessian assumption, but what happens when this assumption fails? Does Adam+ get standard \epsilon^{-4}?
From what I understand in remark after Lemma 1, the variance reduction is ensured by taking β to 0
. The authors use 1 / T a
for some a ∈ ( 0 , 1 )
. Here, I have several questions. First, how does such a small β
work in practice? If in practice, a larger β
works well and theory requires β → 0
for working, it shows to me that theoretical analysis of the paper does not translate to the practical performance. When one uses β
values that work well in practice, does the theory show convergence?
Related to the previous part, I am also not sure about ""adaptivity"" of the method. The authors need to use Lipschitz constants L , L H
to set step sizes. Moreover β
is also fixed in advance, depending on horizon T
, which is the main reason to have variance reduction on z t − ∇ f ( w k )
. So, I do not understand what is adaptive in the step size or in the variance reduction mechanism of the method.
For experiments, the authors say that Adam+ is comparable with ""tuned"" SGD. However, from the explanations in the experimental part, I understand that Adam+ is also tuned similar to SGD. Then, what is the advantage compared to SGD? If one needs the same amount of tuning for Adam+, and the performance is similar, I do not see much advantage compared to SGD. On this front, I suggest the authors to show what happens when the step size parameter is varied, is Adam+ more robust to non-tuned step sizes compared to SGD?
To sum up, I vote for rejection since 1) the analysis and parameters require strict condition, 2) it is not clear if the analysis illustrates the practical performance (very small β
is needed in theory), 3) practical merit is unclear since the algorithm needs to be tunes similar to SGD and the results are also similar to SGD.
[1] Zhang, Lin, Jegelka, Jadbabaie, Sra, Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions, ICML 2020. [2] Levy, Online to Offline Conversions, Universality and Adaptive Minibatch Sizes, NIPS 2017.
======== after discussion phase ==========
I still think that the merit of the method is unclear due to reasons: 1) It is not clear how the method behaves without Lipschitz Hessian assumption. 2) The method only obtains the state-of-the-art complexity of ϵ − 3.5
with large mini-batch sizes and the complexity with small mini-batch sizes (section 2.1) is suboptimal (in fact drawbacks such as this needs to be presented explicitly, right now I do not see enough discussions about this.). 3) Adaptive variance reduction property claimed by the authors boils down to picking ""small enough"" β
parameter, which in my opinion takes away the adaptivity claim and is for example not the case in adaptive methods such as AdaGrad. 4) The comparison with AdaGrad and Adam with scalar step sizes is not included (the authors promised to include it later, but I cannot make a decision about these without seeing results) and I am not sure if Adam+ will bring benefits over them. 5) Presentation of the paper needs major improvements. I recommend making the remarks after Lemma 1 and theorems clearer, by writing down exact expressions and the implications of these (for example remarks such as ""As the algorithm converges with E [ ∇ F ( w ) 2 ] and β
decreases to zero, the variance of z t
will also decrease"" can be made more rigorous and clearer, by writing down exactly the bound for the variance of z t
by iterating the recursion written with E δ t + 1
and highlighting what each term does in the bound. This way will be much easier for readers to understand your paper).
Therefore, I am keeping my score.","4) The comparison with AdaGrad and Adam with scalar step sizes is not included (the authors promised to include it later, but I cannot make a decision about these without seeing results) and I am not sure if Adam+ will bring benefits over them.",223,0
ICLR_2021_2038,ICLR_2021,"Weakness
The paper lacks enough analyses of behaviors of learned structural landmarks, with only an analysis of choices of their numbers.
The time complexity of the developed method is not analyzed and its running time comparison with other baseline methods are also missing.
The analysis of how the choices of each part of features learned in graph pooling would affect the results is missing.
The experimental part lacks the analysis of why the method performs very well on some datasets while not performing well on others.
Summary: This paper studies the problem of graph classification on chemical and social datasets. Existing graph classification methods with graph neural networks learn node embeddings via aggregation of neighbors and combine all node features into a final graph feature for classification, while such operations usually lack the ability for identifying and modelling the inner interactions of substructures. To remedy the information loss in graph pooling, the authors leverage the learned substructure landmarks to project graphs onto them for modelling the interacting relations between component parts of a graph. In this regard, an inductive neural network model for structural landmarking and interaction modelling is developed to resolve potential resolution dilemmas in graph classification and capture inherent interactions in graph-structured systems. Empirical experiments on both chemical and social datasets validate the effectiveness of the method. Generally speaking, the paper is well written and easy to follow, with clear motivation and organization. However, I have concerns about the lack of analysis for learned structural landmarks, since in the paper only the choice of its number is well discussed. Also, the time complexity of the developed method is not well studied. The detailed comments and suggestions of this paper are as follows.
1.The paper proposes to learn structural landmarks and obtain representations of graphs by projecting them. Therefore, the quality of learned landmarks is crucial while the paper lacks enough analyses for them. I suggest providing a more comprehensive analysis for them.
2.The proposed method generates various kinds of graph-level features while lacking enough analyses of their impacts on results. I suggest conducting more experiments of ablation study for this part.
3.The detailed statistics of benchmark datasets are not mentioned such as the distribution of number of nodes in graphs.
4.Although the results are competitive compared with other baselines, the authors didn’t explain why the method performs well on some datasets while not performing well on others. I suggest analyzing the reasons comprehensively.
5.Only one evaluation metric is used in experiments, which is the accuracy. Since it’s a classification task, I suggest using various metrics to show the effectiveness.
6.There are some typos in the paper that requires double checking: For example, ""breath-first search "" -> ""breadth-first search"", ""an molecule"" -> ""a molecule""",2.The proposed method generates various kinds of graph-level features while lacking enough analyses of their impacts on results. I suggest conducting more experiments of ablation study for this part.,224,0
ICLR_2021_512,ICLR_2021,"- Important pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Schütt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018. - The experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate. - The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank-2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1? - In section 3, the authors speak of “collections of rank-p tensors”. However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2. Except for in sec 3.2.2, in which a p=3 tensor has a=1. - In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it’s the former, I don’t see how the network is equivariant. If it’s the latter, I don’t understand the last paragraph of 3.2.2, which says 1H \in R^{N x f_in}, which looks like a 0-tensor. - Can the authors clarify “To achieve translation equivariance, a constant tensor can be added to the output collection of tensors.”? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance. - Am I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a naïve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations). - The authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices?
Recommendation: In its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I’d like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score.
Suggestions for improvement: - Be clear about what the G object is and what eq 1 means. - Be explicit about types the objects, be more explicit about the indices that refer to the permutation representation, to the indices that refer to the Euclidean representation and the indices that refer to copies of the same representation. I think there is an opportunity to be more clear, more explicit, while reducing notational clutter. - Expand the related work section - Compare to the strong baselines that use the coordinates. - Provide argumentation for the claim to scale to 1M vertices.
Minor points: - Eq 7, \times should be \otimes? - Eq 14, what is j? - The authors write: “A, B and C are X, Y and Z respectively”. Perhaps this could be re-written to the easier to read “A=X, B=Y and C=Z”. This happens each time the word “respectively” is used. - Table 3 typo, gluster -> cluster
Post rebuttal
The authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7.","- The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank-2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1?",225,0
ICLR_2021_860,ICLR_2021,"Weakness 1. The proposed measurement is not helpful for designing new methods. Note that the mutual information in mixup is lower than baseline while mixup still outperforms baseline. 2. Compared to mixup and cutmix, the improvement reported in Table 2 is marginal. 3. The experiments on ImageNet is unconvincing. Both of mixup and cutmix are worse than baseline, which contradicts the existing results. 4. There lacks the discussion for the saliency based mixup methods, e.g., Puzzle Mix [1]. It is closely related to fmix but equipped with a learnable strategy to obtain patches for mixing.
[1] J-H Kim, et al. Puzzle mix: Exploiting saliency and local statistics for optimal mixup","3. The experiments on ImageNet is unconvincing. Both of mixup and cutmix are worse than baseline, which contradicts the existing results.",226,0
ICLR_2021_2330,ICLR_2021,"Weakness
- Method on Fourier domain supervision lacks more analysis and intuition. It's unclear how the size of the grid is defined to perform FFT, from my understanding, the size is critical as local frequency will be changed using different grid size. Is it fixed throughout training? What is the effect of having different sizes?
- The generator has a recurrent structure that supports 10 frame generation, but the discriminator looks at three frames (from figure 1) at a time, which seems to limit the power of temporal consistency.
- In figure 7 result and supplemental video result, SurfGAN produces smoother results (MSE seems closer to the red ground truth in figure 7). This seems contradicts the use of Fourier components for supervision -- what causes this discrepancy?
- Figure 4 is confusing. It's not clear what the columns mean -- it is not explained in the text or caption.
- Notation is confusing. M and N are used without definition. Suggestion
- Spell out F.L.T.R in figure 4
- Figure 1 text is too small to see
- It is recommended to have notation and figure cross-referenced (e.g. M and N are not shown in the figure)","- In figure 7 result and supplemental video result, SurfGAN produces smoother results (MSE seems closer to the red ground truth in figure 7). This seems contradicts the use of Fourier components for supervision -- what causes this discrepancy?",227,0
ICLR_2021_2192,ICLR_2021,"The comparison is between binarized but rescaled networks and binary networks without rescaling but added non-linearity. It is not clear if the good performance of the proposed architecture is due to i) the BN step making the scaling redundant or ii) the additional nonlinearities. In some sense, noting that the inclusion of non-linear transformations can boost the predictive power of a network is not surprising. The extra computational cost associated with the introduction of non-linear activation is not fully discussed. The comparison does not include the case where non-linearities are added to the rescaled version (the one on the left of Figure 1). Questions:
what happens if the binarized part of the network (between the FP first and last layer) is completely removed? Often, a two-layer network may achieve good accuracy on simple tasks such as MNIST-digit recognition.
how does the introduction of extra non-linearity affect the computational budget in the proposed method?
is there a figure presenting the performance of the reduced budget model shown in figure 3?
is the benefit associated with extra non-linearities be expected to extend to other architecture than Bi-Real?","1). Questions: what happens if the binarized part of the network (between the FP first and last layer) is completely removed? Often, a two-layer network may achieve good accuracy on simple tasks such as MNIST-digit recognition. how does the introduction of extra non-linearity affect the computational budget in the proposed method? is there a figure presenting the performance of the reduced budget model shown in figure 3? is the benefit associated with extra non-linearities be expected to extend to other architecture than Bi-Real?",228,0
ICLR_2021_2182,ICLR_2021,"weakness I see in the experiments is that in the introduction, you mention previous work on fuzzy discrete distribution clustering (by de Carvalho et al.). Still, you do not compare your results to their method (does adding the diagonal make such a big difference?).
Minor comments and questions:
• Related work: When you mention the correlation of topological complexity with generalization ability, you miss the work by Rieck et al. [2].
• I am a fan of citing software libraries when they provide how to in their website. Hence I would recommend citing Ripser [3] (see https://github.com/Ripser/ripser for BibTeX entry) and potentially other Software you used in the main paper.
• You say “[…] the vectorization […] required by Lacombe et al.’s algorithm makes it unsuitable for integration into our work”. Could you elaborate on this? Can you not even choose this as a comparison partner?
• What do you mean when you say other persistence-based learning strategies required prior knowledge of a ‘correct’ target topology which can’t plausibly be known? The work by Moor et al. for example, extracts persistence directly from the input, which is “trivial” to know since you can compute it.
• Topological Preliminaries: Why do you introduce the \v{C}ech complex when you use Vietoris-Rips for your experiments?
• One feat of your approach is that you add tuples on the diagonal to make the cardinality of all compared PDs match. I wonder how susceptible your approach is to increasingly different cardinality (that has to be filled). Maybe you can conduct a small ablation study on your synthetic data set and evaluate how “good” your mean PDs are when cardinalities are increasingly different.
[1]: RJGB Campello, ER Hruschka. A fuzzy extension of the silhouette width criterion for cluster analysis. Fuzzy Sets and Systems, 2006 – Elsevier
[2]: Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, Karsten Borgwardt. Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology. ICLR 2019.
[3]: Ulrich Bauer. Ripser: efficient computation of Vietoris-Rips persistence barcodes. Preprint.
====================== Update: Thank you for your rebuttal, I still think this is a promising paper and lean towards acceptance. However, after the discussion and the update of the manuscript, my score will remain the same.",• I am a fan of citing software libraries when they provide how to in their website. Hence I would recommend citing Ripser [3] (see https://github.com/Ripser/ripser for BibTeX entry) and potentially other Software you used in the main paper.,229,1
ICLR_2021_981,ICLR_2021,"a. Novelty/impact and context within related literature:
i. It is difficult to judge the novelty of this work since the related work section is too brief and does not actually describe several works compared against. Additionally, table 4 is not descriptive enough, has potentially relevant work undescribed (Whitehall & Movellan's 2017 POMDP/policy gradient approach), and has work referenced previously missing from it (Yudelson et al. 2013, Pardos & Heffernan 2011).
b. Experimental rigour:
i. Two strong claims are made in the related work section that do not have sufficient experimental evidence. 1) A direct comparison of BKT to HOT-DINA for modelling student knowledge gains is required to show the impact of this central claimed contribution. 2) A direct comparison against Shen et al. 2018 is required to show that PPO is indeed more effective in this domain. This is also important since it is a central claimed contribution.
c. Clarity: The clarity of the paper needs significant effort to improve. Instances below.
i. The structure of the paper reads like a technical report rather than an empirical investigation. The contents would be far easier to understand with a different structure emphasising a research question, background to understand/motivate it, methodology to answer it, results, and discussion.
ii. Several sections are far clearer in the supplementary data document. The reproducibility of the paper is boosted by this. Given the extra space available, several sections could stand to be transferred to the main paper. My original review did not include supplementary data and points around describing data and examples are boosted by adding them to the main paper.
iii. Until significant rereads, it isn't clear what the relationship is between RoboTutor and STEP/this work. With my current understanding, RoboTutor is an external system that has been used to collect data on children learning various skills using it. This data was then used to evaluate STEP in terms of estimated learning gains.
iv. The tutor simulator section describes how RoboTutor functions. The student simulator describes how the knowledge tracing model works. Example differences between activities, skills, steps, etc. would make the content clearer.
v. In the tutor simulation section it states that the child can select activities, so this part is replaced by RL decision-making in agent type 3 and 4, right? This relates to my question about RoboTutor and this work. I am understanding that the current work simulates the exact decision-making process of RoboTutor but can vary/change that process according to the different agent types. Is that correct?
vi. Bayesian Knowledge Tracing needs a citation and a brief explanation in a background section.
vii. HOT-DINA needs a citation and a brief explanation in a background section.
viii. Item Response Theory needs a citation and a brief explanation in a background section.
ix. What is the difference in computational cost between a BKT approach and HOT-DINA?
x. A clearer highlighting of what data was used would make it easier to read the article. What was the contents of the data collected to enable knowledge tracing in the student simulator? This also ties in to the comment about examples. Adding a running example of an activity, skill, step for the tutoring task would enable a description of what data is collected to measure student knowledge in the data set.
xi. What do the guess, slip, learn, etc. Parameters measure?
xii. ""We use MCMC sampling for Bayesian inference with PyStan rather than the OpenBUGS Gibbs sampling package used in the original HOT-DINA work because PyStan is faster and handles larger datasets."" This line needs references for all software used, but most importantly, a reference to the original HOT-DINA work is necessary.
xiii. The sole paragraph on page 3 is difficult to parse and seems important to understand how the student simulator works. The paragraph is dense and conversational in style referencing a sequence of steps without making them clearer and referencing equations that are on the next page. It would be much clearer to have the exposition and equations interspersed and use pseudo-code or a flowchart to explain the steps performed to simulate the student (at least a list).
xiv. ""We can train different types of RL agents depending on their state space and range of actions, which depend on how far they depart from RoboTutor’s current decision policy."" This would make sense as the start of a new section on the experimentation.
xv. A stronger partitioning of content would also be achieved by calling a potential new section at this point methodology, experiments, or evaluation. This would also help organise the next section of state, action, and agent types into a concrete experiment for which the paper is describing the state and action spaces.
xvi. In section 3.2, it is confusing to have states, then actions, then agent types described. It seems like one experiment with 4 experimental variants (agent types) and a baseline (RoboTutor).
xvii. Table 2 does not convey much more information than the explanation before it.
xviii. Figure 4 is very difficult to parse. Instead of showing 36 subplots (with 4 empty subplots) for 8 students and 4 agents comparing against the RoboTutor baseline, it would be far easier to compare performance against students or against agent types, by combining all 8 student type runs for each variant into a single variance-shaded run in a single graph (e.g. using https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.fill_between.html). That would allow for much higher information density and an at a glance comparison between the 5 runs being compared combined across all 8 students. At the very least this should be done by combining all 5 runs into 8 separate graphs, though the previous approach is preferable.
xix. Related work is far too brief and does not make clear what is being compared for many cited works. E.g. It isn't clear why the works that specify reward a certain way in Doroudi et al. (2019) show a disadvantage to the current approach, the works in table 4 don't specify why the current approach is an advance over their contributions.
d. Reproducibility:
i. Section 3 (and the paper in general) contains far too little information about the policy representation, learning hyperparameters, network architecture, etc. to understand the contribution.
4. Recommendation:
a. Per the weaknesses in the review above, I recommend the paper for rejection. I don't think the weaknesses in experimental rigour can be fixed in time, content space, or degree to support acceptance. The significant editing required to fix the other issues also seem unrealistic in time and space.
5. Minor Issues
a. ""(Previous methods used reward=0 or 1 based on correct attempt or something else. Useful to mention this?)"" There is a comment remaining in the paper that should have been removed.",1) A direct comparison of BKT to HOT-DINA for modelling student knowledge gains is required to show the impact of this central claimed contribution.,230,0
ICLR_2021_301,ICLR_2021,"weakness when using GA-MLPs for solving graph problems as compared to GNNs. Along these line the paper the main results can be characterized as follows: 1) The paper identifies a specific instance of identifying non-isomorphic graphs that can be solved via a GNN but not by a GA-MLP framework. 2) The paper provides an empirical and experimental evaluation of the representation power of GNNs versus Graph-Augmented MLPs, and show a separation in expressive power between the two in terms of node level functions on rooted graphs. Specifically, they show that the set of functions that can be represented by a GNN of a certain depth) grows doubly exponentially in k, as opposed to only exponential growth of the function class when considering a similar GA-MLP architecture. They also empirically evaluate the difference in performance of the two models on community detection and counting walk problems.
To obtain the result in 1, the paper uses the recent equivalence between the computation in a depth-k GNN and the WL graph isomorphism test. The authors use this to construct two non-isomorphic graphs that the WL test can distinguish using 2 iterations, but on which a GA-MLP will produce the same augmented embeddings. To obtain the result in 2, the authors count the number of distinct rooted trees that can be produced during the computations of a GNN and GA-MLP, and show a gap between the case of a GNN and a GA-MLP.
My main main concern with the paper is that it does not provide sufficiently rigorous findings on either the theoretical or the experimental front. On the theoretical side, the paper establishes a somewhat expected performance gap between using a full GNN and an approximation such as a GA-MLP. Also it seems that the lower bounds for GA-MLPs only hold for the variant where the linear transformations are of the form A,A^2, and so on. Perhaps the authors can clarify this point? Further there are no sample complexity or generalization bounds provided. On the experimental front, it would have been much nicer to understand the tradeoffs in performance versus scalability or provide guidance on which models are more suitable for common, real-world GNN problems and applications. Counting attributed walks and community detection are not very representative problems for GNNs.",1) The paper identifies a specific instance of identifying non-isomorphic graphs that can be solved via a GNN but not by a GA-MLP framework.,231,1
ICLR_2021_1783,ICLR_2021,"1. The main contribution of this paper is introducing adversarial learning process between the generator and the ranker. The innovation of this paper is concerned. 2. Quality of generated images by proposed method is limited. While good continuous control is achieved, the realism of generated results showed in paper and supplemental material is limited. 3. Visual comparisons and ablation study are insufficient.
Comments/Questions: 1. Could you elaborate more on why proposed method achieves better fine-grained control over the interested attribute? Was it crucial to change the formular of ranker’s loss function from classification to regression? 2. Could you provide more visual comparisons between the proposed method and prior works? 3. There are also some other works focusing on the semantic face editing and they show the ability to achieve continuous control over different attributes, like [1]. Could you elaborate the difference between your work and these papers? 4. Statements in Section 4.2 are somewhat redundant.
Minor: 1. Missing proper expression for the third face image in Figure 2. 2. Missing close parenthesis at the bottom of Page 4. 3. Inconsistent statement and reference for Celeb Faces Attributes Dataset in experiment section.
[1] Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei. “Interpreting the Latent Space of GANs for Semantic Face Editing”, In CVPR, 2020. https://dblp.org/rec/conf/cvpr/ShenGTZ20",1. The main contribution of this paper is introducing adversarial learning process between the generator and the ranker. The innovation of this paper is concerned.,232,1
ICLR_2021_1948,ICLR_2021,"a. Anonymisation Failure in References
i. A reference uncited in the manuscript body contains a non-anonymised set of author names to a paper with the same title as the system presented in this paper. This was not detected during initial review. ""Shuby Deshpande and Jeff Schneider. Vizarel: A System to Help Better Understand RL Agents.arXiv:2007.05577.""
b. Citations
i. There is egregiously missing information in almost all citations. This is quite obvious in all the missing dates in the manuscript text.
c. Clarity
i. There are a few unclear or misleadingly worded statements made as below:
1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).
2) ""stronger feedback loop between the researcher and the agent"" - This is at least confusing. In any learning setting, there is a strong interaction loop between experimentation by the researcher and resulting outcomes for the trained model.
3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.
4) ""For multi-dimensional action spaces, the viewport could be repurposed to display the variance of the action distribution, plot different projections of the action distribution, or use more sophisticated techniques (Huber)."" - It would be clearer to actually state what the sophisticated techniques from Huber are here.
ii. The framework could be clearer that it applies most directly as described on single agent RL. The same approach could be used with multi-agent RL but the observation state and visualisations around that get more confusing when there are multiple potentially different sets of observations. Is this not the case, please clarify?
d. Experimental rigour
i. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. Not every paper requires a user study, however, the contributions proposed by this particular manuscript require validation at some level from actual RL users (even a case study with some feedback from users would address this to some extent).
e. Novelty in Related Work
i. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See a sampling below.
1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }
2) @inproceedings{ Atrey2020Exploratory, title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning}, author={Akanksha Atrey and Kaleigh Clary and David Jensen}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rkl3m1BFDB} }
3) @inproceedings{ Puri2020Explain, title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution}, author={Nikaash Puri and Sukriti Verma and Piyush Gupta and Dhruv Kayastha and Shripad Deshmukh and Balaji Krishnamurthy and Sameer Singh}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SJgzLkBKPB} }
4) @article{reddy2019learning, title={Learning human objectives by evaluating hypothetical behavior}, author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey and Legg, Shane and Leike, Jan}, journal={arXiv preprint arXiv:1912.05652}, year={2019} }
5) @inproceedings{mcgregor2015facilitating, title={Facilitating testing and debugging of Markov Decision Processes with interactive visualization}, author={McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald}, booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, pages={53--61}, year={2015}, organization={IEEE} }
6) @article{puiutta2020explainable, title={Explainable Reinforcement Learning: A Survey}, author={Puiutta, Erika and Veith, Eric}, journal={arXiv preprint arXiv:2005.06247}, year={2020} }
7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }
8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }
9) @misc{sundararajan2020shapley, title={The many Shapley values for model explanation}, author={Mukund Sundararajan and Amir Najmi}, year={2020}, eprint={1908.08474}, archivePrefix={arXiv}, primaryClass={cs.AI} }
10) @misc{madumal2020distal, title={Distal Explanations for Model-free Explainable Reinforcement Learning}, author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere}, year={2020}, eprint={2001.10284}, archivePrefix={arXiv}, primaryClass={cs.AI} }
11) @article{Sequeira_2020, title={Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations}, volume={288}, ISSN={0004-3702}, url={http://dx.doi.org/10.1016/j.artint.2020.103367}, DOI={10.1016/j.artint.2020.103367}, journal={Artificial Intelligence}, publisher={Elsevier BV}, author={Sequeira, Pedro and Gervasio, Melinda}, year={2020}, month={Nov}, pages={103367} }
12) @article{Fukuchi_2017, title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents}, ISBN={9781450351133}, url={http://dx.doi.org/10.1145/3125739.3125746}, DOI={10.1145/3125739.3125746}, journal={Proceedings of the 5th International Conference on Human Agent Interaction}, publisher={ACM}, author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita}, year={2017}, month={Oct} }
4. Recommendation
a. I recommend this paper for rejection as the degree of change needed to validate the contribution through a user study or other validation is likely not feasible in the time and space needed. However, the contribution is potentially valuable to RL, so with the inclusion of this missing evaluation and additions to related work/contextualisation of the contribution, I would consider increasing my score and changing my recommendation.
5. Minor Comments/Suggestions
a. It is recommended to use the TensorFlow whitepaper citation for TensorBoard (https://arxiv.org/abs/1603.04467). This is the official response (https://github.com/tensorflow/tensorboard/issues/3437).","3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.",233,0
ICLR_2021_2892,ICLR_2021,"- Proposition 2 seems to lack an argument why Eq 16 forms a complete basis for all functions h. The function h appears to be defined as any family of spherical signals parameterized by a parameter in [-pi/2, pi/2]. If that’s the case, why eq 16? As a concrete example, let \hat{h}^\theta_lm = 1 if l=m=1 and 0 otherwise, so constant in \theta. The only constant associated Legendre polynomial is P^0_0, so this h is not expressible in eq 16. Instead, it seems like there are additional assumptions necessary on the family of spherical functions h to let the decomposition eq 16, and thus proposition 2, work. Hence, it looks like that proposition 2 doesn’t actually characterize all azimuthal correlations. - In its discussion of SO(3) equivariant spherical convolutions, the authors do not mention the lift to SO(3) signals, which allow for more expressive filters than the ones shown in figure 1. - Can the authors clarify figure 2b? I do not understand what is shown. - The architecture used for the experiments is not clearly explained in this paper. Instead the authors refer to Jiang et al. (2019) for details. This makes the paper not self-contained. - The authors appear to not use a fast spherical Fourier transform. Why not? This could greatly help performance. Could the authors comment on the runtime cost of the experiments? - The sampling of the Fourier features to a spherical signal and then applying a point-wise non-linearity is not exactly equivariant (as noted by Kondor et al 2018). Still, the authors note at the end of Sec 6 “This limitation can be alleviated by applying fully azimuthal-rotation equivariant operations.”. Perhaps the authors can comment on that? - The experiments are limited to MNIST and a single real-world dataset. - Out of the many spherical CNNs currently in existence, the authors compare only to a single one. For example, comparisons to SO(3) equivariant methods would be interesting. Furthermore, it would be interesting to compare to SO(3) equivariant methods in which SO(3) equivariance is broken to SO(2) equivariance by adding to the spherical signal a channel that indicates the theta coordinate. - The experimental results are presented in an unclear way. A table would be much clearer. - An obvious approach to the problem of SO(2) equivariance of spherical signals, is to project the sphere to a cylinder and apply planar 2D convolutions that are periodic in one direction and not in the other. This suffers from distortion of the kernel around the poles, but perhaps this wouldn’t be too harmful. An experimental comparison to this method would benefit the paper.
Recommendation: I recommend rejection of this paper. I am not convinced of the correctness of proposition 2 and proposition 1 is similar to equivariance arguments made in prior work. The experiments are limited in their presentation, the number of datasets and the comparisons to prior work.
Suggestions for improvement: - Clarify the issue around eq 16 and proposition 2 - Improve presentation of experimental results and add experimental details - Evaluate the model of more data sets - Compare the model to other spherical convolutions
Minor points / suggestions: - When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex. - In Def 1 in the equation it is confusing to have theta twice on the left-hand side. It would be clearer if h did not have a subscript on the left-hand side.","- In its discussion of SO(3) equivariant spherical convolutions, the authors do not mention the lift to SO(3) signals, which allow for more expressive filters than the ones shown in figure 1.",234,0
ICLR_2021_2047,ICLR_2021,"As noted below, I have concerns around the experimental results. More specifically, I feel that there is a relative lack of discussion around the (somewhat surprising) outperformance of baselines that VPBNN is aiming to approximate, and I feel that the experiments are missing what I see as key VPBNN results that otherwise leave the reader with questions. Additionally, I think the current paper would benefit from including measurements and discussion around the specifics of computational and memory costs of their method. Recommendation
In general, I think this could be a great paper. However, given the above concerns, I'm currently inclined to suggest rejection of the paper in its current state. I would highly recommend that authors push further on the noted areas!
Additional comments
p. 1: ""The uncertainty is defined based on the posterior distribution."" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data.
p. 2: ""The MC dropout requires a number of repeated feed-forward calculations with randomly sampled weight parameters in order to obtain the predictive distribution."" This should be updated to indicate that in MC dropout, dropout is used (in an otherwise deterministic model) at test time with ""a number of repeated feed-forward calculations"" to effectively sample from the approximate posterior, but not directly via different weight samples (as in a variational BNN). With variational dropout, this ends up having a nice interpretation as a variational Bayes method, though no weight distributions are typically directly used with direct MC dropout.
p. 2: Lakshminarayanan et al. (2017) presented random seed ensembles, not bootstrap ensembles (see p. 4 of their work for more info). They used the full dataset, and trained M ensemble members with different random seeds, rather than resampled data.
p. 4: For variance propagation in a dropout layer with stochastic input, it's not exactly clear from the text how variance from the inputs and dropout is being combined into an output Gaussian. I believe using a Gaussian is an approximation, and while that would be fine, I think it would be informative to indicate that. The same issue comes up with local reparameterization for BNNs with parameter distributions, where they can be reparameterized exactly as output distributions (for, say, mean-field Gaussian weight dists) so long as the inputs are deterministic. Otherwise, the product of, say, two Gaussian RVs is non-Gaussian.
p. 7: Figure 1 is too small.
p. 7: ""Estimation of ρ is possible by observing the outputs of middle layers several times under the approximate predictive distribution. The additional computation cost is still kept quite small compared to MC dropout."" How exactly is ρ
estimated? Is it a one-time cost irregardless of data that can then be used for all predictions from the trained model? Without details, this seems like a key component that can yield arbitrary amounts of uncertainty.
p. 7, 8: For the language modeling experiment, why do you think VPBNN was able to achieve lower perplexity values than MC dropout? The text generally focuses on VPBNN as an approximation to MC dropout, and yet it outperforms it. The text would greatly benefit from more discussion around this point.
p. 8: For the OOD detection experiment, I'm surprised that ρ = 0
was the only VPBNN model used, since Section 5.1 and Figure 1 indicated that it led to overconfident models. Can you include results with other settings of ρ
? Moreover, from Figure 1 we see that (for that model) VPBNN with ρ = 0
qualitatively yielded the same amount of predictive variance as the Taylor approximation. However, in Table 2, we see VPBNN with ρ = 0
outperform MC dropout (with 100 or 2000 samples) and the Taylor approximation. Why do you think this is the case, particularly if the standard deviation was used as the uncertainty signal for the OOD decision. I see that ""This is because the approximation accuracy of the Taylor approximation is not necessarily high as shown in Section B"", but I did not find Section B or Figure 3 to be clear. I think the text would benefit from more discussion here, and from the additional experiments for ρ .
Can you include a discussion and measurements for FLOPS and memory usage for VPBNN? Specifically, given the discussion around efficiency and the implementation that doubles the dimensionality of the intermediates throughout the model, I believe it would be informative to have theoretical and possibly runtime measurements. Minor
p. 1: s/using the dropout/using dropout/
p. 1: s/of the language modeling/of language modeling/
p. 2: s/is the representative of/is representative of/
p. 2: s/In the deep learning/In deep learning/
p. 2: s/This relations/This relation/
p. 5: Need to define s
as the sigmoid function in the LSTM cell equations.",5: Need to define s as the sigmoid function in the LSTM cell equations.,235,0
ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","• It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"".",236,0
ICLR_2021_2953,ICLR_2021,")
The idea of incorporating the training dynamics to the Bayesian optimization tuning process to construct online BO is novel.
The experiments are conducted on two complex datasets CIFAR-10, CIFAR-100. Weaknesses:
No deep analysis is conducted to understand why the proposed method can lead to better generalization.
I feel unclear with several technical details: 1) What is the x-axis in Figures 1 & 2? Is it the number of epochs? 2) How many experiments are repeated in Figures 1&2 and Table 1? 3) How to set the search space S for GSdyn? In your experiments in Section 5, how do the authors set the search space S? 4) What is the objective function for GSdyn and FABOLAS? In Section 5.1, it is mentioned that the DNN’s accuracy is the objective function, but which accuracy? The accuracy on the validation dataset or on the test dataset? 5) To evaluate BO, the standard practice is to find the hyperparameter set with the best accuracy on the validation dataset. Why in this work, the accuracy on the test dataset (but not validation dataset) is compared between baseline methods (Figures 1 &2)? And which accuracies are there in Table 1? I understand that GSdyn leads to good generalization but the accuracy on the validation dataset is also needed to be shown as it is the objective of the vanilla BO? 6) The experiments might include different hyper-parameters, and more hyper-parameters.
Minor comments:
In the figures, the labels of each axis need to be added.
Third bullet in the summarized contributions in Section 1: Beyes --> Bayes
Line 5 of Algorithm 1: Should be either \sigma_0 or \sigma, not a mix of them?
Line 5 of Algorithm 2: What is Sample function? I understand it is the acquisition function but a rigorous formula of the acquisition function needs to be provided.","6) The experiments might include different hyper-parameters, and more hyper-parameters. Minor comments: In the figures, the labels of each axis need to be added. Third bullet in the summarized contributions in Section 1: Beyes --> Bayes Line 5 of Algorithm 1: Should be either \sigma_0 or \sigma, not a mix of them? Line 5 of Algorithm 2: What is Sample function? I understand it is the acquisition function but a rigorous formula of the acquisition function needs to be provided.",237,0
ICLR_2021_1310,ICLR_2021,"Lack of some critical comparisons: 1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN? 2) What about the inference time (or complexity) of NSB-GAN compared to BigGAN?
If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer.
Overall, the suggested work effectively decreases the training time of the BigGAN using a simple idea. However, there are missing comparisons and analyses such as 1) comparison with pre-trained BigGAN -> ESRGAN 2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.",1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN?,238,0
ICLR_2021_1808,ICLR_2021,"weakness?
The paper proposes a novel framework, Knowledge Graph Enhanced Dual-Copy network (KGEDC) for abstractive dialogue summarization. Conversational structure and factual knowledge are incorporated in this framework based on graph network to deal with long-distance cross-sentence dependencies and faithfulness respectively. This framework can be decomposed of 1) a sequence encoder to capture contextual information of dialogues flowing along the sequence, 2) a graph encoder via sparse relation graph self-attention network for cross-sentence dependencies, 3) a factual knowledge graph for representing relational tuples extracted from dialogues, 4) a dual-copy decoder to focus on both the input tokens and the factual knowledge. Experimental results on two datasets show the performance gains of the proposed methods over several previous baselines. STRENGTHS:
The motivation is clear and is in line with the model architecture. Two main unresolved problems in abstractive dialogue summarization are consistently concerned in the paper.
The experimental results is rather strong and solid. The paper includes the results of different baseline methods for comparison, showing considerable boosts in both automatic and human evaluation metrics.
The paper organization and writing is coherent. Besides, the model description is also well detailed. WEAKNESS:
Dialogue graph may not necessarily be sparse. In section 3.2.3, “Sparse Relational Graph Self-Attention Layer” paragraph, the author writes “However, our sparse self-attention operation …, which reduces the quadratic computation to linear”, but no proof is given, actually this seems not to be right. In a dialog where only two persons interchange words in turn, the number of edges for speaker dependency will still be quadratic to turn length.
The constructed factual knowledge graph seems to be sparse since the topics various in different dialogues. As shown in figure 2 (b), there are 7 different edge types in one dialogue session. How does the model deal with this issue?
Maybe the paper should provide more content on ablation and case study. For example, in equation (3), edge type is considered in attention computation, while the impact for distinguishing 3 different edges is not studied. Moreover, cases, where the proposed method doesn’t perform well, may also be interesting while not included.
In figure 1, both Factual Knowledge Graph and Sequence Encoder are used in predicting the next word at the decoder stage, in an attention manner, but why graph encoder is excluded here? Now that utterance representations have been encoded in a graph encoder, attending to these representations may help in doing better decoding.
Typos. Grammar, and Style
just beneath equation (3) in section 3.2.3: w_i^r should be w_i^R.","3) a factual knowledge graph for representing relational tuples extracted from dialogues,",239,1
ICLR_2021_1783,ICLR_2021,"1. The main contribution of this paper is introducing adversarial learning process between the generator and the ranker. The innovation of this paper is concerned. 2. Quality of generated images by proposed method is limited. While good continuous control is achieved, the realism of generated results showed in paper and supplemental material is limited. 3. Visual comparisons and ablation study are insufficient.
Comments/Questions: 1. Could you elaborate more on why proposed method achieves better fine-grained control over the interested attribute? Was it crucial to change the formular of ranker’s loss function from classification to regression? 2. Could you provide more visual comparisons between the proposed method and prior works? 3. There are also some other works focusing on the semantic face editing and they show the ability to achieve continuous control over different attributes, like [1]. Could you elaborate the difference between your work and these papers? 4. Statements in Section 4.2 are somewhat redundant.
Minor: 1. Missing proper expression for the third face image in Figure 2. 2. Missing close parenthesis at the bottom of Page 4. 3. Inconsistent statement and reference for Celeb Faces Attributes Dataset in experiment section.
[1] Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei. “Interpreting the Latent Space of GANs for Semantic Face Editing”, In CVPR, 2020. https://dblp.org/rec/conf/cvpr/ShenGTZ20",2. Could you provide more visual comparisons between the proposed method and prior works?,240,0
ICLR_2021_78,ICLR_2021,"weakness in comparisons to comparable virtual environments is given later.
d. The different variant agents being compared in the task benchmark are clearly explained and form an appropriate experimental design.
2. Novelty/Impact
a. The work aims to showcase a new challenge task, evaluation platform, and benchmark agent performance for goal recognition followed by collaborative planning. The work as described compares favourably to similar work in evaluation platforms and benchmarks referenced in the related work section and appendix. The differences are made clear, though the use of some featured distinctions are not demonstrated in the paper (e.g. visual observations are possible but not used in the benchmarking).
3. Experimental Rigour
a. This work is not primarily about demonstrating the benefits of a particular approach over others in a particular application. It demonstrates benchmarks for agent performance in a newly introduce problem setting. From that perspective, the paper has strong experimental rigour. The experimental design is appropriate, comparing multiple baselines and oracles with several sets of experimental variants from both automated planning and reinforcement learning communities.
b. The comparison of experimental variants is conducted with both a computationally-controlled agent and a human-controlled avatar to evaluate collaboration performance in increasingly realistic settings.
c. The claim that the computationally-controlled Alice agent is human-like is repeated throughout the paper. This is not justified in the actual text of the main paper, but is supported to a moderate degree (human-like in strategy/planning if not movement/behaviour) through experiments with human subjects that are described in the appendix.
d. Effort paid to ensure diverse avatars in experimentation.
4. Reproducibility
a. The work is almost entirely reproducible, with details of all agent architectures used for experiments provided with hyperparameters and architecture design. The authors describe that the environment will be released as open-source, which will then make the article wholly reproducible. This reviewer appreciated the level of additional detail provided in the appendix to improve this area of evaluation.
3. Weaknesses
1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".
2. From the motivation provided, i.e. evaluating social perception and collaboration, why is the task specifically about watching then helping and not both together or just helping or just watching or some combination of these activities fluidly occurring throughout the interaction?
3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.
4. It would be clearer to specify that this task is limited to situations where there is explicitly only one goal throughout the entire demonstration + execution episode. This is important since it precludes using this challenge task for research into agents that need to use goal recognition after the initial demonstration, potentially continuously over the course of execution. This second kind of continuous goal monitoring is more similar to real-world applications of watching and helping or assistive agents or social robotics, since the human collaborator can (and often will) change their mind.
5. Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. This is similar to situations like team sports, where all that matters is the final game score. Many real-world scenarios for human-AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For example, in a computer game where Bob represents a team-mate to Alice who is a human player, Bob can choose to steam-roll Alice and win the game by itself. However, this leads to lower subjective user experience for the human team-mate. In this case, the score might be greater than what Alice could accomplish on their own and the game might be won faster than Alice could on their own, but the experience would be different based on whether they are truly collaborating or one is over-shadowing the other.
6. A final assumption, is that there is no difference in expertise between Alice and Bob. The human is expected to be able to competently finish the task and for Bob to linearly cut down the time taken to perform this task. There are many real-world tasks in human-AI collaboration where this assumption does not hold and there could be non-linear interactions between success-rate and speed-up due to different levels of expertise between Alice and Bob.
7. The fixed Alice agent is called human-like through out the article and this was not properly justified anywhere in the main text of the paper. However, the appendix actually describes results that compare the performance of the computationally-controlled and human-controlled variants of Alice to human observers. This potentially justifies this weakness. For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.
8. Why aren't there benchmark results (more than one) for the goal recognition component similar to the planning task experimentation? If both parts of the task are important, it would be valuable to provide additional experiments to show comparisons between goal recognition approaches as well, even if that is in the appendix for space reasons.
9. There could be more analysis of the benchmark agent performance, 1) Why does the purely random agent work relatively well across tasks? 2) Why doesn't HRL work better? Is this due to less hyperparameter tuning compared to other approaches or due to some intrinsic aspect of the task itself? 3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?
10. There are several comments about other environments in the related work section and appendix being toy environments. However, the tasks in the environment demonstrated in this paper only use a small set of predicates as goals. Similarly, it CAN generate visual observations but that isn't used by any of the baselines in the paper. Several comparisons to related virtual environments are made in appendix, but some of the features aren't used here either (humanoid agent - this challenge task works equally well with non-humanoid avatars/behaviours and realism - visual realism is present but it isn't clear if behavioural or physical realism is present due to seeming use of animations instead of physical simulation).
11. None of the tasks described allow the use of communication between agents or evaluate that. Other multi-agent environments like Particle Environments (below) allow for that. Communication is a natural part of collaboration and should have been mentioned if only to distinguish future work or work out of current scope.
a. @article{mordatch2017emergence, title={Emergence of Grounded Compositional Language in Multi-Agent Populations}, author={Mordatch, Igor and Abbeel, Pieter}, journal={arXiv preprint arXiv:1703.04908}, year={2017}}
12. ""planning and learning based baselines"", ""and multiple planning and deep reinforcement learning (DRL) baselines"", etc. - There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.
13. The human-likeness evaluation experiment asked subjects to evaluate performance one agent video at a time. A more rigorous evaluation might compare two agents side by side and ask the human to guess the human performance. This could also be in addition to the current evaluation. The current evaluation is a ceiling on performance while the comparative evaluation is a potential floor.
4. Recommendation:
1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.
5. Minor Comments/Suggestions:
1. Some minor typos in the manuscript:
a. Using the inferred goals, both HP and Hybrid can offer effective. - page 6
b. IN(pundcake, fridge) - appendix table 2
c. This closeness perdition - appendix page 19","3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?",241,0
ICLR_2021_78,ICLR_2021,"weakness in comparisons to comparable virtual environments is given later.
d. The different variant agents being compared in the task benchmark are clearly explained and form an appropriate experimental design.
2. Novelty/Impact
a. The work aims to showcase a new challenge task, evaluation platform, and benchmark agent performance for goal recognition followed by collaborative planning. The work as described compares favourably to similar work in evaluation platforms and benchmarks referenced in the related work section and appendix. The differences are made clear, though the use of some featured distinctions are not demonstrated in the paper (e.g. visual observations are possible but not used in the benchmarking).
3. Experimental Rigour
a. This work is not primarily about demonstrating the benefits of a particular approach over others in a particular application. It demonstrates benchmarks for agent performance in a newly introduce problem setting. From that perspective, the paper has strong experimental rigour. The experimental design is appropriate, comparing multiple baselines and oracles with several sets of experimental variants from both automated planning and reinforcement learning communities.
b. The comparison of experimental variants is conducted with both a computationally-controlled agent and a human-controlled avatar to evaluate collaboration performance in increasingly realistic settings.
c. The claim that the computationally-controlled Alice agent is human-like is repeated throughout the paper. This is not justified in the actual text of the main paper, but is supported to a moderate degree (human-like in strategy/planning if not movement/behaviour) through experiments with human subjects that are described in the appendix.
d. Effort paid to ensure diverse avatars in experimentation.
4. Reproducibility
a. The work is almost entirely reproducible, with details of all agent architectures used for experiments provided with hyperparameters and architecture design. The authors describe that the environment will be released as open-source, which will then make the article wholly reproducible. This reviewer appreciated the level of additional detail provided in the appendix to improve this area of evaluation.
3. Weaknesses
1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".
2. From the motivation provided, i.e. evaluating social perception and collaboration, why is the task specifically about watching then helping and not both together or just helping or just watching or some combination of these activities fluidly occurring throughout the interaction?
3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.
4. It would be clearer to specify that this task is limited to situations where there is explicitly only one goal throughout the entire demonstration + execution episode. This is important since it precludes using this challenge task for research into agents that need to use goal recognition after the initial demonstration, potentially continuously over the course of execution. This second kind of continuous goal monitoring is more similar to real-world applications of watching and helping or assistive agents or social robotics, since the human collaborator can (and often will) change their mind.
5. Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. This is similar to situations like team sports, where all that matters is the final game score. Many real-world scenarios for human-AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For example, in a computer game where Bob represents a team-mate to Alice who is a human player, Bob can choose to steam-roll Alice and win the game by itself. However, this leads to lower subjective user experience for the human team-mate. In this case, the score might be greater than what Alice could accomplish on their own and the game might be won faster than Alice could on their own, but the experience would be different based on whether they are truly collaborating or one is over-shadowing the other.
6. A final assumption, is that there is no difference in expertise between Alice and Bob. The human is expected to be able to competently finish the task and for Bob to linearly cut down the time taken to perform this task. There are many real-world tasks in human-AI collaboration where this assumption does not hold and there could be non-linear interactions between success-rate and speed-up due to different levels of expertise between Alice and Bob.
7. The fixed Alice agent is called human-like through out the article and this was not properly justified anywhere in the main text of the paper. However, the appendix actually describes results that compare the performance of the computationally-controlled and human-controlled variants of Alice to human observers. This potentially justifies this weakness. For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.
8. Why aren't there benchmark results (more than one) for the goal recognition component similar to the planning task experimentation? If both parts of the task are important, it would be valuable to provide additional experiments to show comparisons between goal recognition approaches as well, even if that is in the appendix for space reasons.
9. There could be more analysis of the benchmark agent performance, 1) Why does the purely random agent work relatively well across tasks? 2) Why doesn't HRL work better? Is this due to less hyperparameter tuning compared to other approaches or due to some intrinsic aspect of the task itself? 3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?
10. There are several comments about other environments in the related work section and appendix being toy environments. However, the tasks in the environment demonstrated in this paper only use a small set of predicates as goals. Similarly, it CAN generate visual observations but that isn't used by any of the baselines in the paper. Several comparisons to related virtual environments are made in appendix, but some of the features aren't used here either (humanoid agent - this challenge task works equally well with non-humanoid avatars/behaviours and realism - visual realism is present but it isn't clear if behavioural or physical realism is present due to seeming use of animations instead of physical simulation).
11. None of the tasks described allow the use of communication between agents or evaluate that. Other multi-agent environments like Particle Environments (below) allow for that. Communication is a natural part of collaboration and should have been mentioned if only to distinguish future work or work out of current scope.
a. @article{mordatch2017emergence, title={Emergence of Grounded Compositional Language in Multi-Agent Populations}, author={Mordatch, Igor and Abbeel, Pieter}, journal={arXiv preprint arXiv:1703.04908}, year={2017}}
12. ""planning and learning based baselines"", ""and multiple planning and deep reinforcement learning (DRL) baselines"", etc. - There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.
13. The human-likeness evaluation experiment asked subjects to evaluate performance one agent video at a time. A more rigorous evaluation might compare two agents side by side and ask the human to guess the human performance. This could also be in addition to the current evaluation. The current evaluation is a ceiling on performance while the comparative evaluation is a potential floor.
4. Recommendation:
1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.
5. Minor Comments/Suggestions:
1. Some minor typos in the manuscript:
a. Using the inferred goals, both HP and Hybrid can offer effective. - page 6
b. IN(pundcake, fridge) - appendix table 2
c. This closeness perdition - appendix page 19","3. Weaknesses 1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".",242,0
ICLR_2021_2674,ICLR_2021,"Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.","- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?",243,0
ICLR_2021_2674,ICLR_2021,"Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.","- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?",244,0
ICLR_2021_2512,ICLR_2021,"Weakness:
The writing is confusing and not clear enough. For example, Para. 4.1, line 6. What is the specific meaning of “main direction of n points”, and what is suitable mathematical expression of “the unit vector e” ?
In Mini-Imagenet N-way K-shot experiments, authors didn’t show specific numbers of filter of the most important comparison object, Reptile, and the final experiments results are not particularly outstanding compared with recent papers, like DPGN[1], SIB[2].
For label noise experiments, it is hard to say the ISPL is indeed effective as results showed in line 1,2,3 of Table 2 in paper.
[1] Yang, Ling, et al. ""DPGN: Distribution Propagation Graph Network for Few-shot Learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020 [2] Hu, Shell Xu, et al. ""Empirical Bayes Transductive Meta-Learning with Synthetic Gradients."" ICLR (2020)..","2020 [2] Hu, Shell Xu, et al. ""Empirical Bayes Transductive Meta-Learning with Synthetic Gradients."" ICLR (2020)..",245,1
ICLR_2021_227,ICLR_2021,", my recommendation and its justification as well as additional detailed comments for the authors. I hope you will find my comments helpful and constructive.
Strengths and weaknesses
Strengths - Providing ML solutions for psychiatry is a very challenging problem, as psychiatric diseases are of a very complex nature and many crucial processes remain opaque at best. I believe that it is very important that the ML community starts to tackle these issues. I thank the authors for trying to solve this difficult real-world problem and not showcasing their approach on toy examples. Even though the overall accuracies are not high, I believe it is very important to show such an unbiased estimate of where we are at with ML applications in psychiatry. This merits recognition. - Choosing EEG data for applications in psychiatry is a very good choice given its feasibility, cost-efficiency and availability in a clinical setting. Often authors do not think carefully enough, whether the data they train their classifier on can actually be acquired in a real-world clinical setting. - I applaud the authors’ choice to emphasize interpretability in this healthcare setting. This is a commendable choice that is often not made in other healthcare applications. This is a crucial prerequisite to improve the chance of such an approach ending up in clinical practice as interpretability will be the precursor for acceptance by patients and medical staff. - Including study site prediction was a very commendable effort to assess geographic confounders, which is a major concern in recent psychiatric studies with the advent of more and more large scale multi-center datasets.
Weaknesses: - Depression outcome label appears to have a proof-of-concept character, rather than addressing a more relevant clinical question (differential diagnosis for bipolar vs unipolar depression for example). - Misleading representation of results speaking to superiority of their approach (SCAN approach is much worse than LR combined with bVAE and VAE; therefore, I would advise a more cautious interpretation with respect to the SCAN results and being more precise in the abstract). - Confidence intervals for classification results are missing. - The claim that their approach reduces ‘hand-engineering’ seems to be unsubstantiated given the description of their methodology.
Recommendation - Overall, I would like to see this paper accepted, should the authors agree to address the concerns raised above. Unfortunately, I cannot support acceptance as it stands, but I would increase my score, if my concerns have been addressed.
Justification of recommendation - I believe the research question addresses a very important problem in trying to identify sparse and interpretable clinical markers from EEG data. The method is well-selected to address this question and takes real-world clinical constraints into account (multi-center data, feasibility of data acquisition => EEG, interpretability of identified features => bVAE, uncertainty regarding clinical labels and about which features are required => transfer learning to new labels and unsupervised feature identification). Therefore, I believe this paper deserves to be accepted and represents a good example that hopefully will inspire others to tackle these challenging questions. - However, I cannot support acceptance as it stands, because of the weaknesses highlighted above. Most of the issues pertain to clarity of writing, over-interpretation (or at least misleading representation of the results) some missing information on methodology, and choice of the clinical outcome label. For more detailed elaboration on these points see below.
Detailed feedback
Abstract - You describe your method (bVAE and SCAN) followed by the sentence: “We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis”. This gives the impression that the bVAE+SCAN approach is substantially superior to the hand-engineered baseline. However, in the results table this specific combination is only marginally better than baseline, while bVAE+LR or indeed AE+LR substantially outperform the baseline. This is misleading in the abstract and should also be discussed later on.
Introduction - You make point to criticize the ‘hand-engineering’ of conventional EEG analysis (section 1, paragraph 2, p. 1-2). You fail to mention that automatic options for eye blink correction exist, for example PCA based methods like Berg & Scherg (1994): https://doi.org/10.1016/0013-4694(94)90094-9 Indeed fully automated pipelines do exist. Furthermore, from your methods description (2.1, EEG preprocessing, p. 4; 2.1 Autoencoder, last paragraph p. 4-5), it appears that you follow the conventional preprocessing and only then feed the EEG into the autoencoder, which is of course, in principle, fine, but I fail to see why you bring up the whole critique of the hand-engineered pipeline. Your approach does not seem to address this issue, or did I miss something?
Methods - Some aspects of the methodological description of the analysis are missing. Please, state which software was used to perform the preprocessing. Were ICA-based eye blink correction results visually inspected? Furthermore, for the AE you state that preprocessed trialwise EEG data goes into it as input, for the bVAE this information is missing. Did you follow this approach here as well? - The choice of your clinical classification label appears arbitrary and in my opinion does not address a clinically important question. Could you elaborate why you lumped these specific diagnoses together and not others? The axis I label is so broad that it is basically meaningless. What was the rationale for this analysis? I also assume that you classify depression and axis I vs. controls, is this correct? If so, this is not very relevant clinically. The challenge does not lie in determining, whether a person is healthy or has a mental disorder (clinicians usually can tell within a few minutes of conversation), but rather differential diagnosis or prognosis. A more clinically relevant comparison would be, for example, classifying MDD vs bipolar depression. This is a challenging clinical question, because both patient groups can present with depressive symptoms initially and only time tells which diagnose and also (importantly) medication is appropriate. Why did you not choose such a classification problem? - In your description of the bVAE you state that each disentangled factor corresponds to an ‘interpretable’ transformation of the data. This is misleading. Interpretability is of course the goal of a reduced latent space, but by no means the default result. Whether a compression is indeed interpretable needs to be carefully assessed. - Was test data selected across subjects (other individuals to test) or within subjects (other parts of the data from the same individual, but all individuals included in the training data)? This is important information as the goal would be to generalize to unseen individuals rather than unseen data from the same individual.
Results - Please, add confidence intervals for balanced accuracy (for example by computing the posterior balanced accuracy: Broderson et al. 2010, https://ieeexplore.ieee.org/document/5597285/) otherwise it is impossible to assess the relative quality of the different classifiers. Statements such as: “the classification accuracy is still significantly higher for deep representations compared to the LPP” should be backed up by a statistical test of some sort. Along the same lines how was “significantly higher than chance“ assessed? By permutation tests, based on confidence intervals/Bayesian confidence intervals? - You state: “This suggests that replacing the more manual canonical LPP pipeline with deep representation learning can allow for both better training data efficiency and a reduction in time that the (potentially vulnerable) participants have to spend in the lab by up to 37x,…” For this interpretation to hold, you would need to show, that you can train your classifier on a single trial, otherwise, you would still need all the data for pretraining to then be able to make predictions based on single trials. I would advice clarifying this. - Lastly, I would like to state, that I was very impressed that you were able to re-discover the anhedonia neurocorrelates with your approach. I think the major challenge is indeed to make sure, that your bVAE identifies meaningful latent variables which I believe you successfully showed.","- You state: “This suggests that replacing the more manual canonical LPP pipeline with deep representation learning can allow for both better training data efficiency and a reduction in time that the (potentially vulnerable) participants have to spend in the lab by up to 37x,…” For this interpretation to hold, you would need to show, that you can train your classifier on a single trial, otherwise, you would still need all the data for pretraining to then be able to make predictions based on single trials. I would advice clarifying this.",246,0
ICLR_2021_1213,ICLR_2021,"weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under “Additional Comments” as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. Summary:
• The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex.
• The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known “random shuffling” sampling strategy.
• Specifically, AdaGrad-window is shown to achieve O ~ ( T − 1 / 2 )
rate of convergence, whereas AdaGrad-truncation attains ( T − 1 / 2 )
convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.
• The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.
• In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition “consistency ratio” over epochs. Strengths:
• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.
• I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.
• Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.
• Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new “consistency condition” is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors’ approach to proving the results. Weaknesses:
• Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn’t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.
• Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r
. I couldn’t figure out how it is possible to compute the value r
ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r
weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.
• The related work which is listed in Table 1, within the group “Adaptive Gradient Methods” prove \emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.
• As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.
• Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion.
• This is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.
Additional Comments:
• I haven’t seen the definition that x t , m + 1 = x t + 1 , 1
in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?
• Second bullet point of your contributions claim that “[consistency] condition is easy to verify”. I do not agree with this as I cannot see how someone could guarantee/compute the value r
ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?
• In Assumption A3, I understand that G t e i = g t , i and G t e = ∑ i = 1 m g t , i
. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.
• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i
enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?
• In Corollary 1, authors state that “the computational complexity is nearly O ( m 5 / 2 n d 2 ϵ − 2 ) ~
”. A similar statement exists in Corollary 2. Could you please explain what “nearly” means in this context?
• In Lemma 8 in the supplements, a a T and b b T
in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that a a T or b b T
correspond to something like g t , j 2 – g t − 1 , j 2
. I am not sure if this construction fits into Lemma 8 because, for instance, the expression g t , j 2 – g t − 1 , j 2
is difference of two rank-1 matrices, which could have rank \leq 2. Hence, there may not exist some vector a
such that a a T = g t , j 2 – g t − 1 , j 2
, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors.
• In the supplements, in section “A.1.7 PROOF OF MAIN THEOREM 1”, in the expression following the first line, I didn’t understand how you obtained the last upper bound to ∇ f ( x t , i )
. Could you please explain how this is obtained? Score:
I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:
I am not convinced about the importance of consistency ratio and that it is a verifiable condition.
Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.
(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.
Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.
======================================= Post-Discussions =======================================
I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score.
Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice.
Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.","• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?",247,0
ICLR_2021_818,ICLR_2021,"1 Results are reported only on one dataset (ImageNet). 2 The appendix needs to be revised for better presentation. 3 The variances of the results are not reported.
This paper is well motivated because the authors observe the negative correlation between the transferability and interactions inside adversarial perturbations, and they provide a possible explanation of why the related research tasks can improve the adversarial transferability. Further mathematical proofs and experiment results verify the observation.
However, I have some questions about this paper.
Questions 1. There are many outliers off the blue shade in the subgraph of Figure 1. Could the authors give some interpretation of why there are so many outliers?
For Equation 4, the value of the expected interactions is equivalent to the expectation of the contribution for each pixel. The authors aim to minimize the value of the expected interactions that is the same to average the contribution to all pixels. That may contradict the idea of the one-pixel attack. More interpretation should be given to understand the concept of interactions.
The authors claim to provide a unified view to understand the enhancement of transferability; however the authors only explain three baseline models. Other types of adversarial example generation methods are not considered, such as Translation-Invariant Attack that the authors already mentioned in the related work. What is more, the Translation-Invariant Attack aims to compose the gradients of neighboring pixels together, which is contradictory to the authors’ idea of reducing the interaction of perturbations.
From the experiment results in Table 1, why the performance of the IR attack is worse than the baseline when attacking RN-152 and DN-201 in the ensemble setting?
In Section 5, Experiments, baselines: “the transferability of each baseline was computed based on the best adversarial perturbation during the 100 steps…” and you also mention that “Previous studies usually set the number of steps to 10 or 20”. I am confused about why you set the step to be 100? For a fair comparison, you should follow the setting in previous studies.
In Section 5, Experiments, baselines: “we set λ = 1
for the IR Attack, when the source DNN was ResNet, and set λ = 2
, for other source DNNs.”, but in Figure 3, part a, the range of λ
is from 0 to 1.2 for DN121, which does not include the \lamda value the authors suggested.
The computational cost is not discussed. As the authors said in Section 5 “the computational cost of the interaction loss is intolerable”. No discussion about the running time is provided. Moreover, the authors choose the step to be 100, which further increases the running time.
The third line on page 5, the term δ p p
is wrong.
It should be $ \delta _{p}$.
Appendix: The is some problem with the reference format about Figure 4 and Figure 7. I also suggest reorganizing the appendix.",2 The appendix needs to be revised for better presentation.,248,0
ICLR_2021_860,ICLR_2021,"Weakness 1. The proposed measurement is not helpful for designing new methods. Note that the mutual information in mixup is lower than baseline while mixup still outperforms baseline. 2. Compared to mixup and cutmix, the improvement reported in Table 2 is marginal. 3. The experiments on ImageNet is unconvincing. Both of mixup and cutmix are worse than baseline, which contradicts the existing results. 4. There lacks the discussion for the saliency based mixup methods, e.g., Puzzle Mix [1]. It is closely related to fmix but equipped with a learnable strategy to obtain patches for mixing.
[1] J-H Kim, et al. Puzzle mix: Exploiting saliency and local statistics for optimal mixup","4. There lacks the discussion for the saliency based mixup methods, e.g., Puzzle Mix [1]. It is closely related to fmix but equipped with a learnable strategy to obtain patches for mixing. [1] J-H Kim, et al. Puzzle mix: Exploiting saliency and local statistics for optimal mixup",249,0
ICLR_2021_1465,ICLR_2021,"Weakness: 1. Math symbols need to be unified. 2. Some repeated words. E.g. “stacking” in “Radial Discretization”, Page 3, “point cloud” in “POINT CLOUD TO CONCENTRIC SPHERICAL SIGNAL”, Page 4. 3. Some math symbols have no explanation, which may make readers cannot get your idea. E.g. d_u、d_v in Eq(1)。 4. I don’t know what is Z_g in Eq(1), and the shape of this parameter. The oversimplified description of Z_g makes me can’t understand why Eq(1) can keep rotation invariant. 5. What is the shape of W_{k+K/2} in Eq(2), is it a matrix ? Please add more description of your math symbels. 6. How to determine the neighbor range of point x in Eq(3) ? Are you use KNN or ball query methods ? Do you have some contrast experiments? 7. Only use the norm-2 to calculate Eq(3) may loss some direction information, how do you think of this question?
Although the paper seems to propose an effective convolutional operation for point cloud representation, it is unclear how to maintain its rotation invariance. Possibly this is because of inconsisent and unclear symbols. Therefore, if the authors could show this factor satisfactory in the rebuttal and address the above concerns, I would like to move to a positive rating.","5. What is the shape of W_{k+K/2} in Eq(2), is it a matrix ? Please add more description of your math symbels.",250,0
ICLR_2021_1731,ICLR_2021,"Weakness: 1. I’m confused about the application scenarios of the proposed method. The authors did not show any application value, though they claimed that MAXENT has a large number of applications in applied machine learning. The proposed method is not easy to understand and implement. The author should at least add one real data application. 2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.","2. Differences between the proposed method and the existing methods (Good, 1970; Zhu et al,. 1997; Pandey & Dukkipati, 2013) should be stressed more. The differences can be presented theoretically and numerically.",251,0
ICLR_2021_1949,ICLR_2021,"While the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better.
As proposed, the PABI framework seems very general—which is good. But the paper only shows how to realize the framework in a couple specific cases, for “inductive” and “transductive” learning independently. This is still more general than previous work, but from the first few pages of the paper I was expecting something even more general.
It seems to me that the combination of inductive and transductive learning may be possible using something close to the paper's proposed methods , but this isn’t addressed by the paper except a glancing mention in Footnote 6.
It also is not clear to me from the paper’s text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. In particular, it seems that in this case the approximation method proposed for transductive learning would indeed have to reduce to training a combined model. Related issues were finally mentioned briefly in the last paragraph of the paper, and something along these lines appears in appendix A.3, but I think a more up-front clarification of the limitations is warranted.
More broadly, the question in the back of my head when I began reading the paper was if this would help explain why and when language model pretraining (and other more flexible related-task pretraining) works well. The paper points to related work in this area, such as Gururangan et al 2020 (“Don’t Stop Pretraining”), leading me to think this paper would shed light on the issue, but in the end the issue was not mentioned and seems perhaps out of scope.
This is fine. All I would ask of the authors is to be more explicit about the limitations of PABI (or the proposed realizations of it) from the beginning, laying out the scope of this work and stating the limitations outright instead of only pointing to the appendix. It seems to me like PABI is more of a foundational framework which is ideal for future work to build into, rather than already being a general solution in itself. I think it would be best to pitch the paper this way. Recommendation
Accept. Important problem, lots of solid content, clear benefits over previous work and directions for the future. Great work.
More comments/questions
I think the point of the formulation in Section 2.2 can be made a bit more explicit. It seems like the point is for applying PABI to partial labels. If that’s true (or there’s more to it) then might as well just say it there, or at least give this case as a motivating example.
Regarding the cross-domain results: why are the incidental supervision sets so small? It seems that there is a ton more incidental supervision available for NER, and in both cases the incidental supervision data is even smaller than the test set. Why not use more? It seems to me that the use case here is when a large amount of incidental supervision is available anyway. It also seems like the low-data setting is not totally fair to the vocabulary overlap baseline.
Typos, style, etc.
When describing your experiments, I think it’s worth mentioning that they are on English text.
Figure 3: I don’t understand which numbers correspond to which model in the caption. This would be much easier to read in a table.
P. 7: something’s wrong with “twitter(Strauss et al., 2016)”
P. 7: The FitzGerald et al 2018 dataset is called “QA-SRL Bank 2.0”.
P. 7: servers -> serves
P. 7: “the lower bound for is”",7: The FitzGerald et al 2018 dataset is called “QA-SRL Bank 2.0”. P.,252,1
ICLR_2021_1944,ICLR_2021,"I have several concerns regarding this paper.
• Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).
• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).
• Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments.
• Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this.
• Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3].
• Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling.
• Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable.
• Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting.
[1] https://arxiv.org/pdf/2003.00982.pdf [2] https://ogb.stanford.edu/ [3] https://paperswithcode.com/task/node-classification ==========
After reading the authors comments.
I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score.
I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.","• Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).",253,0
ICLR_2021_2200,ICLR_2021,"weakness could be innovation and experiments Innovation:
The proposed methods are quite heuristic, and I assume there could be many other improvements:
1)In section 2.2, the combination of Random Walk with Restart (RWR) Matrix and GCN is too heuristic, why not try other feature fusion methods rather than straightforward concatenation.
2)The developed RWR Regularization can be regarded as a formulation of Laplace Regularization, and only thing you do is replacing the Laplace matrix with the RWR matrix. Actually, there are also other graph construction methods (like consine similarity matrix etc) to replace the RWR matrix in RWR Regularization, you need to introduce additional experiments to prove the advantages of RWR matrix. Experiments
1)The experimental results in Table. 1 are not so convinced. I agree with the point that your work don’t focus on defining new state-of-the-art results, but you still need to provide the node classification comparisons with the same train/validation/test split defined by Yang et al. (2016).
2)As your definitions, \lambda is a trade-off hyperparameter, but I miss the setting and ablation study of this important hyperparameter.
3)Why not try AD+RWRREG? From the results, this combination seems could be better (like GCN, Diffpool in Table.2). Questions:
My questions have been included in Weak points part
Additional Feedback
1)Time complexity of constructing Random Walk with Restart (RWR) Matrix.","2)The developed RWR Regularization can be regarded as a formulation of Laplace Regularization, and only thing you do is replacing the Laplace matrix with the RWR matrix. Actually, there are also other graph construction methods (like consine similarity matrix etc) to replace the RWR matrix in RWR Regularization, you need to introduce additional experiments to prove the advantages of RWR matrix. Experiments",254,0
ICLR_2021_2929,ICLR_2021,"Weakness: The major concern is the limited contribution of this work. 1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2]. 2. The use of morphologic information in this paper is simple as the combination of edge detection and segmentation, which are both employed as tools from existing benchmarks (in this paper the author used DeeplabV3, DexiNed-f, employed as off-the-shelf tools for image pre-processing purpose as mentioned in section 4). 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn’t provide insight into this but assumed morphologic segmentation will be invariant. 4. Results compared to other domain adaptation methods (especially generative methods) are missing. There is an obvious lack of evidence that the proposed method is superior.
In brief, the contribution of this paper is limited, the results provided are not sufficient to support the method being effective. A reject.
[1] Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation [2] Image to Image Translation for Domain Adaptation","1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2].",255,1
ICLR_2021_2819,ICLR_2021,"The main weaknesses of the manuscript in my opinion are as follows:
The final method seems to be not based on natural gradient and the claimed connection is not explained:
In fact, the final method simply uses a tanh-based quantization function (Eq. 10) and simply uses SGD with STE approximation. There is no explanation of why this approach closely approximates the natural gradient method, given that this the main claim of the paper and even the title contains ""natural gradient"". Definition 2 might be coming from a textbook (even though no reference is given) and it simply provides an existence condition and it does not say anything about the proposed exponential map corresponds to the manifold induced by the Fisher Information Matrix. This is critical for the paper and without this connection the whole discussion about natural gradient is redundant and could be deemed as ""decorative maths"". Please explain (with supporting theory) how Eq. 10 relates to FIM.
Given that the method is not based on natural gradient, there are many unsubstantiated claims. Examples include: 1) ""unique limitaions of the QNNs to construct curvature information"" 2nd last para in the introduction, 2) ""demonstrated feasibility of training low-precision models using natural gradients"" first para of Sec. 3.2, and 3) ""our method .. avoids the risk of neural networks falling into the local minimum"" in conclusion.
Misinterepretations and inconsistencies in mathematical writing/notations:
Eq. 8 has many issues. First, what is v e c ( d L / d w i )
? If w i
is a scalar, d L / d w i
is a scalar. So what does it mean to vectorize it?
To my understanding F i j = g i g j where g i = d L / d w i
. There is no vector representation and no need for Kronecker product etc. Please clarify.
Please clearly mention the dimensions of each variable and define them appropriately for all the equations. Also, do not use same symbol to denote two things. Examples include Q
(Eq. 5, 10) and α
(Eq. 10, 14).
No references given for the statements related to Riemannian geometry:
For both definitions 1 and 2 no reference is given. It seems like there are existing results and they should be cited so that someone can check/verify their applicability in the stated problem setting.
Recent quantization techniques are not compared:
For cifar experiments only comparison is done against the first quantization method BinaryConnect (Courbariaux, 2015) which is very old. Especially, comparison against other relaxed quantization methods [a,b] are required.
Even for imagenet experiments important comparisons are missing such as [c,d].
Minor Comments
Please provide the equation for FIM before Eq. 4.
Second para in page 4: what is meant by ""closely related""?
Please provide all the experimental details required for reproduction (eg, any layers kept floating point in imagenet experiment?, what initialization used?, starting from scratch or pretrained network, etc.)
After fig. 2, please tone down. The figures only show a marginal improvement in test error not ""significantly preventing overfitting"". References
[a] Bai, Y., Wang, Y.X. and Liberty, E., 2019. Proxquant: Quantized neural networks via proximal operators. ICLR.
[b] Ajanthan, T., Dokania, P.K., Hartley, R. and Torr, P.H., 2019. Proximal mean-field for neural network quantization. ICCV.
[c] Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B., Huang, J. and Hua, X.S., 2019. Quantization networks. CVPR.
[d] Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W. and Cheng, K.T., 2018. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. ECCV.","1) ""unique limitaions of the QNNs to construct curvature information"" 2nd last para in the introduction,",256,0
ICLR_2021_1465,ICLR_2021,"1. The complexity analysis is insufficient. In the draft, the author only provide the rough overall complexity. A better way is to show the comparison between the proposed method and some other methods, including the number of model parameter and network forwarding time.
2. In the converting of point cloud to concentric spherical signal, the Gaussian radial basis function is adopted to summarize the contribution of points. Is there any other function that can accomplish this job? The reviewer would like to the discussion about this.
3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.
4. There are some typos in the draft. The first is the wrong use of ""intra-sphere"" and ""inter-sphere"". The second is the use of two consecutive ""stacking"" in the Spherical Discretization subsection. Please check the full text carefully.
5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.","5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.",257,0
ICLR_2021_342,ICLR_2021,"=====
Very inadequate attribution of ideas. I am not too familiar with the AIR line of work, but I think quite a few ideas can be traced back to prior works. It would be much better if the authors can, in addition to a brief one sentence mention in related works, add clear discussions for the inspirations of the main design choices.
Missing discussions of relevant works that do: 1. unsupervised part decomposition e.g. “UCSG-NET - Unsupervised Discovering of Constructive Solid Geometry Tree”, ""Bae-net: Branched autoencoder for shape co-segmentation"", “CvxNet: Learnable Convex Decomposition”; 2. Learning hierarchical representations e.g. “StructureNet: Hierarchical Graph Networks for 3D Shape Generation”.
I think the comparisons in this work are neither adequate nor fair. I am not convinced that the two toy dataset used here can prove the superiority of the method. The authors claim that “other works can’t work on our dataset”, but I think the burden of the proof is on the authors to show that their method is superior, even under a more specific setting. In other words, if the method is indeed “general” and can learn good decompositions, then I would expect it to perform better even under an slightly unfair setting i.e. comparing against metrics/datasets adopted in other works. Furthermore, the datasets used in this paper appears to be way too simple as compared to real world data. The authors argue that dataset with a single shape is easier, but I disagree: datasets like partnet contains much more complicated part structures, as well as joints between parts, than what is used here, even with only a single shape. (And it is pretty evident from the qualitative examples that the challenging part is decomposing objects into parts, not decomposing scenes into objects). Last but not least, I want to see more evidence that the proposed method is actually useful in real applications.
The learned representation does not seem to be of very good quality, as seen in Figure 4.
A lot of overclaims, to name a few: 1. “GSGN is a general framework for representing and inferring scene graphs of arbitrary depth”: I don’t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet). If the framework can handle more general cases, then show it. 2. “Closely follow the rendering process in graphics engines”: I don’t think applying affine transformations and compositing alone is enough to warrant this claim, it is pretty clear that the learned representation lacks a good sense of “objectness”, as textures, lighting and etc. are all entangled together (evident in Fig 3). There does not seem to be a straightforward way to extend the method to truly parallel the 3D rendering process, neither. 3. “First deep generative model for unsupervised scene-graph discovery”: there are a lot of works that infer structures in an unsupervised way, I don’t think it’s fair to give a very narrow definition of “scene graph” and claim “the first”. 4. “GSGN has capture many predefined object types in the dataset”: I don’t think one can make this claim when there are only three primitives and ten types of objects…
=====Reasons for Score=====
Overall, I have the impression that this work is cherry picking a very specific setting where the proposed architecture works reasonably well. For a work making a quite big claim of being “the first deep generative model that learns …”, I would expect much more comprehensive evaluations than what is currently shown here. Furthermore, many ideas in this work are not attributed properly, making the novelty of the method quite unclear. From my limited knowledge of the direct predecessors of this work, I don’t think there is too much novelty in this paper. I would be more than willing to change my score if the authors can 1. Provide more comprehensive evaluations 2. State the novelty / discuss prior works more clearly. But for now, I tend to give a pretty clear reject.
=====Additional Comments & Questions=====
I am not sure if translation & rotation alone is a good way to handle 2D renders of 3D objects, since any translation & rotation will result in change of perspectives and illumination e.g. rotating the bronze sphere in Figure 3, row 1 will make the specular highlight inaccurate and translating the blue cube in row 3 will make the top surface less visible. Could the authors justify why predicting translation/rotation make sense, when the perspective/illumination of the object already provides a really strong cue?
Following previous point: would like to see examples of the same learned object being used in multiple scenes.
The quality of the learned primitives, as seen in Figure 4, seems to be pretty underwhelming. If the aim of the work is discovering those primitives, would it make more sense to impose a stronger prior on the properties of the primitives?
Table 1 & 2: why are all the ELBO terms the same? I would imagine them to be different, especially for SPACE-O/P, which, if I understand correctly, is a completely different model with different architecture and loss formulation?
Table 2 & 3: why are the metrics so close between SPACE-P & GSCN in table 2 but so different in Table 3? Does that suggest unbalanced dataset?
Still Table 2 & 3: why no comparison between SPACE-O & GSGN for object level occlusion?
The paper claims that being able to handle background is a unique advantage as compared to other works, but the background used in the toy dataset is quite simple. Would like to see more complex examples.","3. “First deep generative model for unsupervised scene-graph discovery”: there are a lot of works that infer structures in an unsupervised way, I don’t think it’s fair to give a very narrow definition of “scene graph” and claim “the first”.",258,0
ICLR_2021_62,ICLR_2021,"and concerns
Proofs The explanations often rely more on intuition than on mathematical proof. While they may be true, it seems to me that it lacks a bit of theoretical grounding. After discussion with the authors, it turns out that this comment was not clear enough. It concerned only the paragraphs after Theorems 3 and 5. I apreciate greatly that the authors made an effort to clarify these paragraphs.
Experiment a. Limitations The experimental study is limited: the estimators have been applied only on two extreme models (a linear model and a BNN), and for each case, only one dataset has been tested. While both are very much used in practice, it is not enough to draw conclusions. There should at least be an emphasis of that, saying that it is only a preliminary, and that more tests will be performed in a more extensive version of the work. Has it been tested on more models (e.g. random forests) or on various architectures of neural networks, as well as on several datasets? As for the choice of the proposal distribution, it seems only one was tested in the linear setting, while a range of them were tested for the BNN. This should be more explicit in the main article, referring to the appendix for more details. b. Reproducibility and relevance of the experiment There are issues on the toy example:
it is a highly nonlinear dataset on which a linear model is trained, which is weird: what happens when using the proposed estimators to train a linear model on (close to) linear data?
the density function on which X
is drawn does not seem to sum to 1.
there seems to be an issue in the generation of y
(Eq. 124), as there no bias added, yet Figure 1 shows an oscillation for y
between 1 and 2.5 and x in [ 1 , 1.5 ]
, while I found an oscillation around 0 by applying the formula as specified
In the modified MNIST dataset: - it seems to me that the proportion of each class used for the pool subset makes this particular dataset somewhat easier rather than harder, as it basically removes the ambiguity between numbers that look alike; for instance, there are very few observations of 9, and a few of 6, and there are 5 times less 7 than 1; in such a case, the accuracy is not a good measure of the quality, as it will be good even if the smaller classes are poorly classified. - Has there been a test on a balanced subset?
In the revision, the authors added a few experiments to complete the first ones, namely adding a third dataset (Fashion MNIST), testing a balanced subset for MNIST, and testing another acquisition function to see its impact on the results. I think this gives more grounding to the nice theoretical results and insights.
Active sampling for test The authors claim that their estimators should have a positive impact on the actively selecting samples at test time. This claim appears in the introduction and in the conclusion, but it seems more like intuition and there is no explanation as to why this may be true. I suggest removing it from the introduction, as it is clearly future work, and if possible add some explanation behind the intuition.
Minor comments:
Please put equation numbers only on equations you refer to, not on all equations.
The name R S U R E
can be confused with Stein's Unbiased Risk Estimator (even though I assume it is so as a reference to that estimator).
In Section 6, the acronym BALD is not defined
In Figure 4a, the colors and order in the legend are reversed for R P U R E and R S U R E
Overall evaluation
I believe this is very promising and insightful work, tackling the important issue of bias. However, I cannot give it a high score due to the issues I raised about the experiment.
The revision answered my concerns about the experiment by enlarging it, which is why I upgrade the score I gave. I think it is a very good and interesting paper.","- it seems to me that the proportion of each class used for the pool subset makes this particular dataset somewhat easier rather than harder, as it basically removes the ambiguity between numbers that look alike; for instance, there are very few observations of 9, and a few of 6, and there are 5 times less 7 than 1; in such a case, the accuracy is not a good measure of the quality, as it will be good even if the smaller classes are poorly classified.",259,0
ICLR_2021_1106,ICLR_2021,"were 1) that experimental methods and results were not sufficiently well explained and 2) there were not enough quantitative results. The paper cannot be accepted to ICLR as-is. I would consider changing my recommendation if these two core issues were addressed in a substantial way.
To improve the paper. The authors should make their explanation of methods substantially clearer, with special attention paid to explaining why they design the experiments and models the way they did. The authors should provide qualitative results for all three tasks.","2) there were not enough quantitative results. The paper cannot be accepted to ICLR as-is. I would consider changing my recommendation if these two core issues were addressed in a substantial way. To improve the paper. The authors should make their explanation of methods substantially clearer, with special attention paid to explaining why they design the experiments and models the way they did. The authors should provide qualitative results for all three tasks.",260,0
ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","• It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them.",261,0
ICLR_2021_1043,ICLR_2021,", which justify the score: • The theoretical developments presented in the paper build on the Rademacher complexity, but ignore the conclusions drawn by Zhang et al. in Section 2.2 of their ICLR 2017 paper (Understanding deep learning requires rethinking generalization). • The theoretical developments build on the assumption that (i) there exists a lower bound, valid for any input, to the distance between the output of each pair of neurons, and (ii) the proposed diversity loss increases this lower bound. Those two assumptions are central to the theoretical developments, but are quite arguable. For example, a pair of neuron that is not activated by a sample, which is quite common, leads to a zero lower bound. • Experimental validation are not convincing. Only shallow networks are considered (2 or 3 layers), and the optimization strategy, including the grid search strategy for hyperparameters selection, is not described.
Minor issue: positioning with respect to related works is limited. For example, layer redundancy (which is the opposite of diversity) has been considered in the context of network pruning: https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.pdf","• The theoretical developments build on the assumption that (i) there exists a lower bound, valid for any input, to the distance between the output of each pair of neurons, and (ii) the proposed diversity loss increases this lower bound. Those two assumptions are central to the theoretical developments, but are quite arguable. For example, a pair of neuron that is not activated by a sample, which is quite common, leads to a zero lower bound.",262,0
ICLR_2021_2181,ICLR_2021,"Novelty and comparison with prior work. This is the primary weakness for this paper and there are a few ways it manifest itself. The main issue is that a quick search for related work turned up the 2006 paper ""Maximum reward reinforcement learning: A non-cumulative reward criterion"" by Quah and Quek [1]. From what I can tell, they propose the exact same problem formulation and algorithm based on a modified Bellman equation as this paper. Granted, the motivation and subsequent theory are different and I assume the authors had no knowledge of the Quah and Quek paper (I did not either before looking for related work). This does not invalidate the usefulness of presenting this max reward formulation again to a new community for new reasons, but this paper should be cited and claims of novelty should be reduced. A broader discussion of prior work that looks at non-cumulative objectives like reward at the final state would also be helpful to contextualize the paper.
Clarity of the motivation for new formulation. Intuitively it is true that a generative model may only care about the maximum reward on a trajectory, but there is not a clear and formal decription of the separation between problems that can be represented as max reward versus cumulative reward problems. For example, it is not clear that doing something like using a time dependent reward that only has nonzero reward at the last state in a trajectory cannot capture most of the relevant generative problems. A more formal description of the sorts of problems that are not representable as cumulative reward problems would be useful.
The toy example problem leaves some unanswered questions. Specifically, the use of a non-markovian problem makes the example somewhat suspect. By using a non-markovian problem, the example is now outside of the setting being discussed in the rest of the paper. It is not clear that the non-markovian nature of the problem is necessary to make the max-Bellman algorithm look better, so I would suggest coming up with a modified example that respects the MDP structure while still showing the utility of max-Bellman.
Description of experimental setting is lacking. It is entirely unclear from reading the paper what exactly the experiments are doing, just that they use the ENAMINE dataset of reactants and several reward functions are cited. The state and action spaces are not defined. The transition dynamics are not defined. And the reward functions are not described or defined in a self-contained way. All of this information should be provided (in the appendix, no need to put it in the main text). Without this information it is difficult to judge how useful the experiments are, especially for someone not intimately familiar with related work from the RL for drug design community (as would be the case for most readers at ICLR).
Presentation of experimental results is unclear. There is not much analysis of each of the figures which leaves some things unclear. For example, in figure 3(a) it appears that MB outperforms cumulative reward at every timestep. This would seem to mean that the MB algorithm in fact gets higher cumulative reward than an algorithm trained to maximize cumulative reward. This would immediately call into question the story about why MB is a useful modification if it actually leads to better performance on the cumulativ reward objective in this specific task. I may be misunderstaning this plot, but this lack of clarity is a problem. As another example, the gaps in table 1 seem relatively small between PGFS and PGFS+MB. A discussion about the scale of the improvement we can expect from MB would be useful to contextualize these results. Recommendation:
I gave this paper a score of 5 (weak reject). This reflects the fact that I think the paper introduces an interesting problem and clean solution, but does not do a good job connecting to prior work and has a few issues with clarity especially in the experiments. I gave a confidence of 3 primarily because I am not very familiar with the literature on ML/RL for drug design so I cannot precisely guage the potential impact of the paper on that subfield.
I am willing to increase my score if the authors provide a more comprehensive connection to prior work and improve the clarity and experiments section based on the weaknesses listed above.
Questions for the authors:
Is there potentially a connection between the proposed maximum reward formulation (especially for chemical synthesis) and the learning to search approach to structured prediction problems (see e.g. [2])?
Is there a connection between the proposed maximum reward formulation and optimal stopping problems?
Additional feedback: Typos:
The last paragraph on page 1 is not grammatically correct. Each phrase should be formulated like ""symbolic regression which is interested in"" instead of ""symbolic regession is interested in"".
In section 2.1 it should be ""RL algorithms easily generalize across states"" instead of ""the RL algorithms are easily generalized across states""
In the leftmost column on table 2, the PGFS score of 7.81 ought to be bolded as well since it is equal to the score of PGFS+MB.
[1] Quah, Kian Hong, and Chai Quek. ""Maximum reward reinforcement learning: A non-cumulative reward criterion."" Expert Systems with Applications 31, no. 2 (2006): 351-359.
[2] Chang, Kai-Wei, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. ""Learning to search better than your teacher."" In International Conference on Machine Learning, pp. 2058-2066. PMLR, 2015.","2 (2006): 351-359. [2] Chang, Kai-Wei, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. ""Learning to search better than your teacher."" In International Conference on Machine Learning, pp. 2058-2066. PMLR, 2015.",263,1
ICLR_2021_774,ICLR_2021,"-- While the analysis applies to a class of feedforward neural networks with analytic activation functions, some common activationswith a perfectly linear component like ReLU, however are not covered by the results in this work.
The second sentence in Remark 7 says, ""If (d) in Propositions 8 does not hold..."" but I don't think you mean to reference Prop. 8 here.","-- While the analysis applies to a class of feedforward neural networks with analytic activation functions, some common activationswith a perfectly linear component like ReLU, however are not covered by the results in this work. The second sentence in Remark 7 says, ""If (d) in Propositions 8 does not hold..."" but I don't think you mean to reference Prop.",264,0
ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","• It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST.",265,0
ICLR_2021_115,ICLR_2021,"1.There are 3 hyperparameters in equation 5, sensitivity analysis may be needed.
2.Is decision threshold sensitive in different datasets? It may affect the practicality of the method.
3.Clarity: What is the meaning of σ
in equation (4)? Is it an activation function? More clarification may be needed to make the paper easier to read.
4.What is the meaning of 'unremovable'? Is this a formal term in English (irremovable)?","1.There are 3 hyperparameters in equation 5, sensitivity analysis may be needed.",266,0
ICLR_2021_1650,ICLR_2021,"Significance: Since the evidence provided in favor of the proposed algorithm is in the form of an empirical evaluation on a synthetically generated dataset, the present impact of the algorithm is limited. In particular, there is no evidence that (i) the algorithm works for larger and/or more complex datasets; and (ii) that natural datasets of interest to the community exhibit a hierarchical structure analogous to the synthetic datasets presented in the submission.
Novelty: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3].
Clarity: Specific details surrounding the relationship between Algorithm 1 and Algorithm 2 are insufficiently discussed: i) Algorithm 2 as it appears in the text is very similar to Algorithm 1 (The OTD algorithm) in [2] with the exception of the new hyperparameter ξ
, and introduces new symbols that do not appear elsewhere in the text. It is therefore not sufficiently adapted for clarity in the context of this work. ii) Whether Algorithm 2 acts as a strict subroutine of Algorithm 2 is not stated. I believe it is not because the clustering decision for a new task relies on tree structures that are ""generated for a training batch,"" although what a ""training batch"" refers to is not clear. Similarly, how the ""online""/""offline"" distinction in the context of the clustering algorithm fits into the training/evaluating setup borrowed from [1] is not made clear. iii) How exactly the task-similarity approach of [3] is employed in Algorithm 2 is not made clear. The only mention of the use of [3] is briefly around Eq. (8) before the main algorithm (Algorithm 1) is introduced, and Algorithm 2 only refers to a generic ""similarity metric"" (as in the original work, [1]). References
[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" arXiv preprint arXiv:1703.03400 (2017).
[2] Menon, Aditya Krishna, Anand Rajagopalan, Baris Sumengen, Gui Citovsky, Qin Cao, and Sanjiv Kumar. ""Online Hierarchical Clustering Approximations."" arXiv preprint arXiv:1909.09667 (2019).
[3] Achille, Alessandro, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. ""Task2vec: Task embedding for meta-learning."" In Proceedings of the IEEE International Conference on Computer Vision, pp. 6430-6439. 2019.","1) is introduced, and Algorithm 2 only refers to a generic ""similarity metric"" (as in the original work, [1]). References [1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" arXiv preprint arXiv:1703.03400 (2017). [2] Menon, Aditya Krishna, Anand Rajagopalan, Baris Sumengen, Gui Citovsky, Qin Cao, and Sanjiv Kumar. ""Online Hierarchical Clustering Approximations."" arXiv preprint arXiv:1909.09667 (2019). [3] Achille, Alessandro, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. ""Task2vec: Task embedding for meta-learning."" In Proceedings of the IEEE International Conference on Computer Vision, pp. 6430-6439. 2019.",267,0
ICLR_2021_343,ICLR_2021,"1. This paper is not well-organized and many parts are misleading. For example, above Eq.3, the author assumes P_{G_{0}} = P_{D}. Does the author take the samples generated by the root generator as the authentic dataset? However, in Section 2 above Eq. 4, the author claims that the authentic data does not belong to any generator. 2. In Eq.4, the key-dependent generator is obtained via adding perturbation to the output of the root model. This setting may be troublesome as :1. These generators are not actually trained. This is different from the problem which this paper tempt to solve. 2. No adversarial loss to guarantee the perturbed data being similar to the authentic data. 2. How to distinguish the samples from different generators. 3. Since Eq.4 is closely related to adversarial attack, the authors are supposed to discuss their connections in the related works. 4. The name of ‘decentralized attribution’ is misleading. Decentralized models are something like federated learning, where a ‘center’ model grasps information from ‘decentralized models’. However, the presented work is not related to such decentralization. 5. Typos: regarding the adversarial generative models ->regarding to the adversarial generative models; along the keys->along with the keys.","1. This paper is not well-organized and many parts are misleading. For example, above Eq.3, the author assumes P_{G_{0}} = P_{D}. Does the author take the samples generated by the root generator as the authentic dataset? However, in Section 2 above Eq. 4, the author claims that the authentic data does not belong to any generator.",268,0
ICLR_2021_2759,ICLR_2021,""" section. Strengths
Excellent writing and well-structured paper.
Intuitive and novel method that is really interesting to read about with good intuition and analytical examples to motivate the method.
Experiments are carried out well and the results are presented in a coherent manner. Each experiment is repeated multiple times. Weaknesses
As mentioned before, I would suggest to tone down the lottery ticket comparisons. I would assume that PHEW does not perform quite as well as the classic lottery tickets derived from IMP, which would not be surprising since IMP requires numerous training iterations. But in any case, I would like to see actual comparisons in terms of the resulting final test accuracy as main comparison tool instead of the comparison metric (Jaccard index, IoU) presented in Section 6.2.
The method seems to be limited to convolutional and fully-connected layers. Could the authors discuss potential generalization to a larger class of network layers? Since unstructured pruning methods (like PHEW) usually do not enable faster training or inference times, I find their main advantage to be that they are network-agnostic. This can thus enable the easy deployment of unstructured pruning to any desired layer or architecture.
As a follow-up to the previous question. How are non-sequential network architectures handled? The experiments include ResNets, which contain residual connections, but to me it is not clear from the paper how this is achieved with PHEW. Also, what about recurrent architectures? Can PHEW handle those? As far as I am aware, the competing methods should not have a problem handling those architectures as well.
How about (at least a few) experiments with the full ImageNet data set? It shouldn't be too computationally expensive to train a ResNet on the full ImageNet data set especially since pruning at initialization does not require multiple retrain cycles like IMP.
Other Minor Feedback
typo, page 4, below Fig. 4: ""...even more better...""
I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs.
Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.","4: ""...even more better..."" I am not convinced that Figure 4(b) indicates that the proposed method results in faster training. It seems a bit far-fetched especially since all networks are trained for the same amount of epochs. Pruning is quite a big field. I believe the paper would benefit from a longer discussion of related work.",269,0
ICLR_2021_512,ICLR_2021,"- Important pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Schütt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018. - The experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate. - The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank-2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1? - In section 3, the authors speak of “collections of rank-p tensors”. However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2. Except for in sec 3.2.2, in which a p=3 tensor has a=1. - In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it’s the former, I don’t see how the network is equivariant. If it’s the latter, I don’t understand the last paragraph of 3.2.2, which says 1H \in R^{N x f_in}, which looks like a 0-tensor. - Can the authors clarify “To achieve translation equivariance, a constant tensor can be added to the output collection of tensors.”? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance. - Am I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a naïve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations). - The authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices?
Recommendation: In its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I’d like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score.
Suggestions for improvement: - Be clear about what the G object is and what eq 1 means. - Be explicit about types the objects, be more explicit about the indices that refer to the permutation representation, to the indices that refer to the Euclidean representation and the indices that refer to copies of the same representation. I think there is an opportunity to be more clear, more explicit, while reducing notational clutter. - Expand the related work section - Compare to the strong baselines that use the coordinates. - Provide argumentation for the claim to scale to 1M vertices.
Minor points: - Eq 7, \times should be \otimes? - Eq 14, what is j? - The authors write: “A, B and C are X, Y and Z respectively”. Perhaps this could be re-written to the easier to read “A=X, B=Y and C=Z”. This happens each time the word “respectively” is used. - Table 3 typo, gluster -> cluster
Post rebuttal
The authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7.","- In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it’s the former, I don’t see how the network is equivariant. If it’s the latter, I don’t understand the last paragraph of 3.2.2, which says 1H \in R^{N x f_in}, which looks like a 0-tensor.",270,0
ICLR_2021_2043,ICLR_2021,"- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.
- Clarify the difference between S-Full and S-PGD from Experiments section. Since S-Full also uses T-step PGD, how it is different than S-PGD? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric, the performance gains are very minimal and not very significant than the baseline OE (Hendrycks et al., 2019b) under two metrics AUROC and AUPR.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on adversarial robustness is significant, there are critical points yet to be addressed. Therefore, I marginally accept this paper.
Post rebuttal: The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper.","- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.",271,0
ICLR_2021_1535,ICLR_2021,"Weakness 1. Motivation is not well-discussed (1) From the introduction section, the author claim that the node-level datasets cannot be centralized. But in practice, at least sensors in IoT networks CAN be centralized to edge servers. (2) Especially, the datasets used in experiments does not have any privacy issue. It is very strange to me why we should concern the privacy of the traffic speed from road network sensors. We can check the road network speed in real-time using Google Map. In other words, the speed of a specific sensor is not sensitive and can be acquired by public service. Currently, the Google Map example demonstrates that the road network sensors can be uploaded to servers from transportation agencies without any privacy concerns. (3) Another serious problem is that the DNN-based model (encoder-decoder used in the proposed method) are not deployable in the sensor device due to computational resource constraint. In practice, the number of road network sensor in transportation is huge, so it is impractical to ask for an expensive upgrading which requires to install a powerful DNN enabled chips. 2. No technical contribution The focus of this work is diverse (both model and training), but lack of contribution in each aspect. (1) The authors argue the contribution of modeling (e.g., 4.1 modeling spatial dependencies, 4.2 “inductive v.s. transductive”). However, for the modeling part, I believe the contribution is from GCN+GRU-based modeling, which is already there in many data mining publications (Equation (1)(2)(3) are not newly proposed models). Please refer to [1][2]. The model used in this paper is a simplified version of these two GCN+RNN-based models in transportation. More advanced models following these two can obtain better performance than the model used in this paper. [1] Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting [2] Diffusion convolutional recurrent neural network: Data-driven traffic forecasting (2) The authors also claim the benefit of alternating optimization (4.3, 4.4). It is good to know such an alternating method works well in practice. However, the author does not provide any in-depth analysis or comprehensive experimental evidence (see my comments 3 below for details). In short, I suggest the author focuses on one aspect of federated learning to consolidate the contribution. At least, the current version raises my concern that this work is a simple combination of many techniques without meta-contribution in each one. If the overall characteristics are good, it should be encouraged. However, the effectiveness is not obvious: higher accuracy, security, fairness, privacy, communication, and computation efficiency, I cannot see any of these advantages of such a combination: [Model Performance] In terms of higher accuracy, as mentioned in (2), the model used is not novel. More advanced models can obtain higher accuracy. See following works of 2-(2) [1][2] for details. [Optimization] The experimental results of the alternating optimization has some counter-intuition results. See 3 for details. [Security/Privacy] As for security and privacy, the hidden vector exchange may leak privacy but this work does not discuss it. See 8 for details. [Computational Efficiency] The model used in edge devices are too computational expensive for resource constrained sensors. See 4 for details. 3. The training Algorithm is novel but the experimental result is not convincing The training method is relatively novel in federated learning. However, the authors should either provide enough intuition to explain why this work (any related works in optimization theory?) or demonstrate it with experimental design. The author attempts to prove that alternating training works better but it’s not convincing. In Section 4.3, all the experimental results are not convincing or counterintuitive: First, I am surprised that the authors have a result to say that split learning performs worse than centralized training. In the essence of optimization, split learning is the same as centralized training. The only difference is that split learning offloads part of the model architecture to the server-side to reduce the communication/computation cost. Second, due to the first reason, that “AT w/o FedAvg” and “AT + FedAvg” works better than SL can NOT demonstrate the effectiveness of alternating optimization. As I mentioned previously, to prove the effectiveness of alternating optimization, we need more analysis or comprehensive experimental design. Third, another counterintuitive result is that AT + FedAvg performs better than centralized training (the red curve is lower than the blue curve). Normally, in federated learning, one should believe that centralized training is the lower bound of training/validation errors. An alternating local SGD method normally leads to variance bias in the training process, thus I believe it’s impossible to obtain a better performance than centralized training. The last concern is that the experiments only perform at one dataset (METR-LA). Please also check whether the same conclusion holds in another dataset. 4. Communication and computational cost The computational cost is evaluated, but what’s the implication of the number showing in terms of FLOPS? I don’t think the resource constrained road network sensor can handle so many FLOPS. Does any running time result in a real-world sensor device? If testing the sensor is prohibited, please provide the running time result at a low-performance smartphone or other IoT devices (NVIDIA edge GPU devices). 5. Overall training time The overall training time and the bandwidth of exchanging hidden vectors should be mentioned. If the network size of the exchange information is too high and the bandwidth is limited in wireless communication (e.g., road sensor networks), a single iteration will cost too much time or failure due to communication protocol constraint (e.g., in IoT setting, maximum payload length is only 65535 bytes), which will lead to a long training time or impractical assumption. Please discuss the size of exchange information (hidden vector/gradient) and the total training time in revision. 6. Scalability issue is ignored The proposed method does not use a client sampling strategy, a common practice in cross-device FL (see the original FedAvg paper [1]), to mitigate the scalability issue. What the performance if we want to learn 10 thousand sensors? Especially in the transportation setting, the scalability is very important. However, the authors do not discuss this. [1] Communication-Efficient Learning of Deep Networks from Decentralized Data. https://arxiv.org/abs/1602.05629 7. Dataset: Non-I.I.D. is not properly discussed A key assumption of Federated Learning is that datasets across devices are non-I.I.D. This is largely ignored by the proposed methods. Slightly mentioning the heterogeneous property is not enough. Please discuss the details of this in revision (e.g., show the distribution diagram). More significantly, DNN models like encoder-decoder architectures normally eat a lot of samples, but the number of samples in each node is small in practice. The assumption that each node has enough dataset to train a good model is too strong (we cannot assume the edge device has the storage capability to store months of time series data since the storage ability is limited at the edge, 4-6 months as the author mentioned). Maybe the alternating optimization method helps, but it lacks analysis. In my opinion, it’s more interesting and practical to construct a graph-level federated learning method since multiple nodes can be centralized to an edge server belongs to a company or an agency like the transportation scenario. 8. Privacy The authors discuss some privacy-preserving methods in related works, but the proposed method does not include any privacy design. For example, [1] analyze potential leakage from the hidden vectors, which proves that the proposed method does not have privacy advantages than exchanging gradient/models. [1] Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer. https://arxiv.org/abs/1912.11279 9. Lack of Related works Since alternating optimization (training algorithm) is the novel part of the proposed method, there is a lack of literature review of this method, either in the general ML optimization literature, or the specific federated learning publications. Without such a comparison, the novelty is not convincing. 10. Some misunderstandings of federated learning (1) In the first paragraph of the introduction, the authors claim that decentralized training can improve latency, but this argument is vague. Improving training latency or inference latency? Federated learning will cost time to train on devices, which requires a long time due to many rounds of communication synchronization. For the inference, to improve the latency, we need model compression techniques, which is also not the main goal of the proposed method. (2) The term “decentralized training” is misused. In federated learning, decentralized training also refers to training using a decentralized topology (no central server). Note that FedAvg used in the proposed method is a centralized training with decentralized datasets. Please clarify the difference in revision, making sure the readers understand which aspect is decentralized, the datasets, the distributed optimization algorithm, or just computation. 11. Reproducibility The proposed algorithm is complex. Without the code release, it’s hard to check the correctness of the implementation. Deep learning normally costs time, I am curious to know how the authors implement such a complex system/algorithm which can train so many nodes in parallel. If it is a simulator, the training time will be too long since so there so many nodes (300+) with DNN models. If the authors can provide code, maybe all my confusion can be addressed, and I am willing to increase my rating. Suggestions (1) I encourage the authors demonstrate the idea with more realistic datasets. For example, in social network, it is possible to run models on the smartphones of Internet users to protect their privacy, which is much better than the transportation example. (2) In the current version, there is no contribution in modeling. Thus, I suggest authors to think about customizing the model to obtain benefit of efficiency and privacy. (3) More intuition and discussion are requires for the training method. Why alternating minimization works well in the GCN setting. (4) Since the training results are not convincing, I suggest authors run more experiments to understand the proposed method and explain why there are some counter-intuitive results. (5) Privacy is not the main focus, but some discussions and related works should be mentioned. Overall Comments Due to so many concerns mentioned above, I encourage the authors to have a deeper understanding of federated learning and address these issues in the revision, especially the motivation, and demonstrating the effectiveness of the proposed alternating training method. After thinking a while, I cannot find a practical scenario that requires GCN-based node-level privacy. At least, the sensor dataset is not a good example. Maybe the authors can search for a practical dataset to demonstrate motivation. In addition, please focus on just one gist rather than intertwine too many aspects and claim all contributions without any trade-off discussion. Besides the training framework, it’s also interesting research if we can see newly developed model architectures can trade-off many important aspects in FL.","4. Communication and computational cost The computational cost is evaluated, but what’s the implication of the number showing in terms of FLOPS? I don’t think the resource constrained road network sensor can handle so many FLOPS. Does any running time result in a real-world sensor device? If testing the sensor is prohibited, please provide the running time result at a low-performance smartphone or other IoT devices (NVIDIA edge GPU devices).",272,0
ICLR_2021_2542,ICLR_2021,"The weaknesses of this paper are summarized below, 1. The main hesitation with this paper is the novelty of the proposed method. Actually, optimizing the graph topology is a hot research direction, and a variety of works are presented about this topic recently. In addition to the AdaEdge, LDS and TO-GCN mentioned in the paper, other works, e.g., ""Graph-Revised Convolutional Network"" (ECML-PKDD 2020, arxiv: 1911.07123), ""Deep Iterative and Adaptive Learning for Graph Neural Networks"" (arxiv: 1912.07832), and ""Graph Structure Learning for Robust Graph Neural Networks"" (arxiv: 2005.10203), also study the same problem. Among them, DIAL-GNN also leverages an iterative and alternating framework to learn GNN and topology, which is similar to this paper. The differences between LDS/AdaEdge and this work are also minor. GRCN and TO-GCN optimize topology and GNN simultaneously in an end-to-end way, which seems more efficient than this work. In summary, the novelty of the presented VEM-GNN is a little bit minor. 2. A minor concern is about the motivation of this paper. The authors claim that ""over-smoothing is caused by ""indistinguishable features of nodes in different classes produced by the message passing along inter-class edges"", so ""adding intra-class edges and removing inter-class edges are helpful to suppress over-smoothing"". However, I have a different understanding about the over-smoothing problem. A more common definition (by Li et al., 2018) about over-smoothing is ""if a GCN is deep with many convolutional layers, the output features may be oversmoothed and vertices from different clusters may become indistinguishable."" In my opinion, the over-smoothing is caused by the depth (or receptive field) of GNN and the message passing manner, but irrelevant to the graph topology. Assuming that we remove all the inter-class edges and connect all the intra-class edges in the graph. In such a situation, when the GNN goes deep, the output embeddings of each node with the same class still become indistinguishable (the embeddings of nodes in the same class will converge to an identical embedding). Maybe we can obtain a perfect classification model by this way, but the embeddings are still failed to represent the property of each node, and they are useless to be applied to other tasks (e.g., anomaly detection). In summary, I agree that topology optimization is beneficial to enhance the node classification performance, but its effect on surpassing over-smoothing is suspicious. 3. The impact of each module/design in the proposed framework is not clearly stated. Specifically, a probability matrix q ¯ ϕ
is acquired by the learned adjacency matrix < q ϕ
>, and then the adjacency matrix for GCN is sampled by < q ¯ ϕ
>. The question is: why don't we define p = 1
directly to obtain a definite adjacency matrix? Such a design can be viewed as a ""DropEdge"", so is that the main contribution term for restraining over-smoothing? Authors should add more ablation study to demonstrate the impact of ""learned topology"" and ""probability matrix"" respectively.
Correctness and Clarity The claims and method are well written without significant errors. The paper is well-organised and written in a logical way.
Additional Comments Here are some additional comments for the authors, 1. More baselines considering topology optimization should be included, such as GRCN, DIAL-GNN, Pro-GNN (the papers of these methods are mentioned in Weaknesses Section). 2. It is better to demonstrate how much edges are added/removed since sparsity is an important factor affecting the efficiency of GCN.",2. It is better to demonstrate how much edges are added/removed since sparsity is an important factor affecting the efficiency of GCN.,273,0
ICLR_2021_2953,ICLR_2021,")
The idea of incorporating the training dynamics to the Bayesian optimization tuning process to construct online BO is novel.
The experiments are conducted on two complex datasets CIFAR-10, CIFAR-100. Weaknesses:
No deep analysis is conducted to understand why the proposed method can lead to better generalization.
I feel unclear with several technical details: 1) What is the x-axis in Figures 1 & 2? Is it the number of epochs? 2) How many experiments are repeated in Figures 1&2 and Table 1? 3) How to set the search space S for GSdyn? In your experiments in Section 5, how do the authors set the search space S? 4) What is the objective function for GSdyn and FABOLAS? In Section 5.1, it is mentioned that the DNN’s accuracy is the objective function, but which accuracy? The accuracy on the validation dataset or on the test dataset? 5) To evaluate BO, the standard practice is to find the hyperparameter set with the best accuracy on the validation dataset. Why in this work, the accuracy on the test dataset (but not validation dataset) is compared between baseline methods (Figures 1 &2)? And which accuracies are there in Table 1? I understand that GSdyn leads to good generalization but the accuracy on the validation dataset is also needed to be shown as it is the objective of the vanilla BO? 6) The experiments might include different hyper-parameters, and more hyper-parameters.
Minor comments:
In the figures, the labels of each axis need to be added.
Third bullet in the summarized contributions in Section 1: Beyes --> Bayes
Line 5 of Algorithm 1: Should be either \sigma_0 or \sigma, not a mix of them?
Line 5 of Algorithm 2: What is Sample function? I understand it is the acquisition function but a rigorous formula of the acquisition function needs to be provided.","5) To evaluate BO, the standard practice is to find the hyperparameter set with the best accuracy on the validation dataset. Why in this work, the accuracy on the test dataset (but not validation dataset) is compared between baseline methods (Figures 1 &2)? And which accuracies are there in Table 1? I understand that GSdyn leads to good generalization but the accuracy on the validation dataset is also needed to be shown as it is the objective of the vanilla BO?",274,0
ICLR_2021_1539,ICLR_2021,"- The authors claim about well-balanced robustness trade-off using their method and also claim that their major objective is only to improve network generalization on clean data. There is a little ambiguity regarding the major contribution of this paper. The authors can make this point more clear. - Isn’t the hypothesis that is stated as “new” in this work already discussed in AdvProp i.e. using different batch normalization for clean and adversarial images improves network generalization, which in turn draw the conclusion that rescaling operation of batch norm could control the robustness and generalization trade-off. Why this hypothesis considered as “new” then ? - The two learned adversarial maskings discussed in section 3.2, it is not clear how they are generated. - Results demonstrate that the proposed approach improves generalization but the performance gain is minimal (only 1%-2%) and not so significant compared to the baselines. Minor point: - I understand that the major objective of this work is to improve performance on clean images but not the adversarial robustness. The results demonstrate higher robustness against PGD based adversarial attacks with perturbation strength lower than 8/255 is interesting but not of practical importance since the method requires perturbation strength as an additional input and very specific to PGD based attack. I wouldn’t consider this as major weakness since it is not the primary objective of this work.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on network generalization are minimal compared to the baselines, this work cleverly addressed the limitations of previous work and extend it with simple modifications. I tend to accept this paper. However, I suggest the authors to also consider the evaluations carried out in AdvProp (Xie et al. (2020)) to improve the significance of their work.","- The two learned adversarial maskings discussed in section 3.2, it is not clear how they are generated.",275,0
ICLR_2021_1539,ICLR_2021,"- The authors claim about well-balanced robustness trade-off using their method and also claim that their major objective is only to improve network generalization on clean data. There is a little ambiguity regarding the major contribution of this paper. The authors can make this point more clear. - Isn’t the hypothesis that is stated as “new” in this work already discussed in AdvProp i.e. using different batch normalization for clean and adversarial images improves network generalization, which in turn draw the conclusion that rescaling operation of batch norm could control the robustness and generalization trade-off. Why this hypothesis considered as “new” then ? - The two learned adversarial maskings discussed in section 3.2, it is not clear how they are generated. - Results demonstrate that the proposed approach improves generalization but the performance gain is minimal (only 1%-2%) and not so significant compared to the baselines. Minor point: - I understand that the major objective of this work is to improve performance on clean images but not the adversarial robustness. The results demonstrate higher robustness against PGD based adversarial attacks with perturbation strength lower than 8/255 is interesting but not of practical importance since the method requires perturbation strength as an additional input and very specific to PGD based attack. I wouldn’t consider this as major weakness since it is not the primary objective of this work.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on network generalization are minimal compared to the baselines, this work cleverly addressed the limitations of previous work and extend it with simple modifications. I tend to accept this paper. However, I suggest the authors to also consider the evaluations carried out in AdvProp (Xie et al. (2020)) to improve the significance of their work.","- Isn’t the hypothesis that is stated as “new” in this work already discussed in AdvProp i.e. using different batch normalization for clean and adversarial images improves network generalization, which in turn draw the conclusion that rescaling operation of batch norm could control the robustness and generalization trade-off. Why this hypothesis considered as “new” then ?",276,0
ICLR_2021_78,ICLR_2021,"weakness in comparisons to comparable virtual environments is given later.
d. The different variant agents being compared in the task benchmark are clearly explained and form an appropriate experimental design.
2. Novelty/Impact
a. The work aims to showcase a new challenge task, evaluation platform, and benchmark agent performance for goal recognition followed by collaborative planning. The work as described compares favourably to similar work in evaluation platforms and benchmarks referenced in the related work section and appendix. The differences are made clear, though the use of some featured distinctions are not demonstrated in the paper (e.g. visual observations are possible but not used in the benchmarking).
3. Experimental Rigour
a. This work is not primarily about demonstrating the benefits of a particular approach over others in a particular application. It demonstrates benchmarks for agent performance in a newly introduce problem setting. From that perspective, the paper has strong experimental rigour. The experimental design is appropriate, comparing multiple baselines and oracles with several sets of experimental variants from both automated planning and reinforcement learning communities.
b. The comparison of experimental variants is conducted with both a computationally-controlled agent and a human-controlled avatar to evaluate collaboration performance in increasingly realistic settings.
c. The claim that the computationally-controlled Alice agent is human-like is repeated throughout the paper. This is not justified in the actual text of the main paper, but is supported to a moderate degree (human-like in strategy/planning if not movement/behaviour) through experiments with human subjects that are described in the appendix.
d. Effort paid to ensure diverse avatars in experimentation.
4. Reproducibility
a. The work is almost entirely reproducible, with details of all agent architectures used for experiments provided with hyperparameters and architecture design. The authors describe that the environment will be released as open-source, which will then make the article wholly reproducible. This reviewer appreciated the level of additional detail provided in the appendix to improve this area of evaluation.
3. Weaknesses
1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".
2. From the motivation provided, i.e. evaluating social perception and collaboration, why is the task specifically about watching then helping and not both together or just helping or just watching or some combination of these activities fluidly occurring throughout the interaction?
3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.
4. It would be clearer to specify that this task is limited to situations where there is explicitly only one goal throughout the entire demonstration + execution episode. This is important since it precludes using this challenge task for research into agents that need to use goal recognition after the initial demonstration, potentially continuously over the course of execution. This second kind of continuous goal monitoring is more similar to real-world applications of watching and helping or assistive agents or social robotics, since the human collaborator can (and often will) change their mind.
5. Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. This is similar to situations like team sports, where all that matters is the final game score. Many real-world scenarios for human-AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For example, in a computer game where Bob represents a team-mate to Alice who is a human player, Bob can choose to steam-roll Alice and win the game by itself. However, this leads to lower subjective user experience for the human team-mate. In this case, the score might be greater than what Alice could accomplish on their own and the game might be won faster than Alice could on their own, but the experience would be different based on whether they are truly collaborating or one is over-shadowing the other.
6. A final assumption, is that there is no difference in expertise between Alice and Bob. The human is expected to be able to competently finish the task and for Bob to linearly cut down the time taken to perform this task. There are many real-world tasks in human-AI collaboration where this assumption does not hold and there could be non-linear interactions between success-rate and speed-up due to different levels of expertise between Alice and Bob.
7. The fixed Alice agent is called human-like through out the article and this was not properly justified anywhere in the main text of the paper. However, the appendix actually describes results that compare the performance of the computationally-controlled and human-controlled variants of Alice to human observers. This potentially justifies this weakness. For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.
8. Why aren't there benchmark results (more than one) for the goal recognition component similar to the planning task experimentation? If both parts of the task are important, it would be valuable to provide additional experiments to show comparisons between goal recognition approaches as well, even if that is in the appendix for space reasons.
9. There could be more analysis of the benchmark agent performance, 1) Why does the purely random agent work relatively well across tasks? 2) Why doesn't HRL work better? Is this due to less hyperparameter tuning compared to other approaches or due to some intrinsic aspect of the task itself? 3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?
10. There are several comments about other environments in the related work section and appendix being toy environments. However, the tasks in the environment demonstrated in this paper only use a small set of predicates as goals. Similarly, it CAN generate visual observations but that isn't used by any of the baselines in the paper. Several comparisons to related virtual environments are made in appendix, but some of the features aren't used here either (humanoid agent - this challenge task works equally well with non-humanoid avatars/behaviours and realism - visual realism is present but it isn't clear if behavioural or physical realism is present due to seeming use of animations instead of physical simulation).
11. None of the tasks described allow the use of communication between agents or evaluate that. Other multi-agent environments like Particle Environments (below) allow for that. Communication is a natural part of collaboration and should have been mentioned if only to distinguish future work or work out of current scope.
a. @article{mordatch2017emergence, title={Emergence of Grounded Compositional Language in Multi-Agent Populations}, author={Mordatch, Igor and Abbeel, Pieter}, journal={arXiv preprint arXiv:1703.04908}, year={2017}}
12. ""planning and learning based baselines"", ""and multiple planning and deep reinforcement learning (DRL) baselines"", etc. - There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.
13. The human-likeness evaluation experiment asked subjects to evaluate performance one agent video at a time. A more rigorous evaluation might compare two agents side by side and ask the human to guess the human performance. This could also be in addition to the current evaluation. The current evaluation is a ceiling on performance while the comparative evaluation is a potential floor.
4. Recommendation:
1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.
5. Minor Comments/Suggestions:
1. Some minor typos in the manuscript:
a. Using the inferred goals, both HP and Hybrid can offer effective. - page 6
b. IN(pundcake, fridge) - appendix table 2
c. This closeness perdition - appendix page 19","- There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.",277,0
ICLR_2021_453,ICLR_2021,"** missing references **
-First, the literature review should be extended to comment on recent approaches relying on part discovery/recognition. All provided references are 10+ years old, and recent works on compositional and part based representations/models for classification should be discussed.
Few examples could be:
CoupleNet, Zhu et al, ICCV 2017
Object-Part Attention Model for Fine-GrainedImage Classification, IEEE TIP, 2018
Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019
Exploiting spatial relation for fine-grained image classification, Pattern Recognition, 2019
Learning Compositional Representations for Few-Shot Recognition, ICCV 2019
While these works are not necessarily closely related to the proposed work and do not reevaluate novelty, they are contemporary part based works that explore part based modelling.
More importantly, authors should mention and discuss
‘Neural activation models’ Simon et al 2015, ICCV
Which aims to integrate a constellation model within a neural network model and is therefore strongly related work.
** clarity/justifications **
-While the approach appears to yield good performance increase, it is difficult to comprehend intuitively why that is the case. Justification for certain model decisions is not clearly provided, and certain claims are not supported by evidence. For example, the claim that the clustering procedure explicitly identifies object parts is not obvious not clearly shown in experiments. Indeed, clusters and their centers are assumed to represent object parts. However, experiments demonstrate that larger numbers of clusters yield stronger performance, suggesting that a coarse superpixel type clustering might be more promising than hoping to identify object parts across images. Similarly, the results in Fig 3 suggest that the discovered clusters are more appearance oriented than identifying parts (cf brown vs white on dog instead of recognising parts e.g. ears).
The clustering process itself lacks clarity. Are cluster centers fixed after training? Is the optimal number of cluster affected by the number of classes/similarities between classes? Are all cluster centers receiving assignments for each batches? Are batches constructed so as to optimise cluster learning? Can authors expand on the role of count parameter s? Are different clusters relevant to e.g. dog classes different from clusters for bird classes? As intuitively these clusters represent object parts, providing more attention to these, both in terms of explanation and experiments would be preferable.
-The motivation behind the use of a distance map is unclear. Could authors elaborate on why performing self attention on the distance map provides relevant information vs e.g. self attention on the clustered feature maps?
-The paper would benefit from a clearer depiction of the constellation model and how the proposed approach relates to them, intuitively. With the current writing, the concept of constellation models is quickly brushed over, and the motivation behind the use of a distance map + attention is not clearly stated but only proposed as an alternative to the probabilistic modelling used in old school constellation models. The paper would strongly benefit from providing justification on why this is a valid alternative, and what we hope to learn using this strategy; and more precisely, why this strategy can be viewed as a constellation type model.
-Several aspects of the few-shot learning formulation should be more clearly explained. While the paper is perfectly understandable for a reader accustomed to FSL methods and settings, the lack of explanations regarding episode training, meta-training/testing, and the protonet model itself would make it very difficult to follow for a non expert. RECOMMENDATION
The proposed work aims to introduce part representation in few-shot models, which is an appealing strategy. Adapting popular traditional models to contemporary settings is a promising idea, and the proposed method reaches SOTA performance on standard benchmark.
The paper in its current state needs a little more attention, and I will be happy to increase my rating if my main concerns are addressed
1- Relating the model and its components more closely to constellation models, and justification as to why the proposed strategy is a better implementation of constellations in deep learning framework than Simon et al. 2015
2- Providing clarifications regarding design decisions, experimental setting, and more intuition. In particular regarding the distance map and cluster centers. If possible, according more attention to interpretability/observed behaviour of cluster centers in the experimental section.
ADDITIONAL COMMENTS - Experiments are missing important details. For example, it is not specified for experiments in Figure 2 and 3 which dataset and parameter configurations are used. In particular for figure 3, is the number of clusters set to 64? Are all cluster centers relevant to a given class? - Examples from the same class provided in this figure look very similar in appearance. What happens when examples of the same class look different? Are same cluster patterns observed? - The multi-branch training strategy is not new and was suggested in TADAM, Oreshkin et al., NeurIPS, 2018. - Regarding ablation experiments, it would be interesting to see the influence of having a single module on the last layer (where levels of abstraction would be higher) vs modules at every layer. - claims regarding ‘explicit modelling of parts’ should be revised. There is no explicit part discovery (nor a guarantee that object parts are indeed discovered), nor a clear, explicit modelling of their interactions. Maybe a more accurate characterisation would be that the approach integrate spatial information between image regions of similar appearance/texture. Similarly, it is not obvious that CNNs extract object parts.",- Examples from the same class provided in this figure look very similar in appearance. What happens when examples of the same class look different? Are same cluster patterns observed?,278,0
ICLR_2021_2576,ICLR_2021,"The paper is well written and generally easy to follow and uses a principled and elegant approach to solving an interesting problem. I especially like the 'natural' way in which a learned policy integrates with A* search, and the resulting desirable properties. This makes the method a very principled approach.
The major limitation of this work is that the proposed method is only compared in terms of solution quality, and not by computational cost. From the specified parameters (up to 1000 training iterations with 40 parallel workers), the necessary compute seems orders of magnitude higher than the baselines, for example the 'random approach' which takes the best of 5 tries. To substantiate the claim that the method ""outperforms baselines with higher connectivity rates and better scalability"", I think at least baselines should be evaluated with the same computational budget, and ideally the trade-off between solution quality and computational cost should be explored.
Also, I wonder, why is the method not compared to Liao et al. 2020, He & Bao, 2020?
Another aspect which is relevant is that, if I understood correctly, the 'policy' consist only of the ranking parameters and the cost maps, and is thus tied to a specific instance of the circuit routing problem which cannot generalize to new instances, such that the training procedure should be run again for each new instance, adding to the high cost.
3. Recommendation
My current assessment is that the paper is marginally below the acceptance threshold.
4. Arguments for recommendation
The paper is of high quality, well written and well motivated, but the experiments lack a comparison of the algorithm and baselines with respect to computational cost, which is relevant to support the claim that this method outperforms baselines and has better scalability.
5. Questions to authors
Could you add experimental results with respect to running time or number of iterations, for example extend table D to a number of samples (40K ?) such that the running time is comparable with the proposed method?
6. Additional feedback
Minor comments/suggestions/compliments/questions:
The term 'score function J(theta)' can be confusing since typically the score function refers to grad log p_theta. Maybe 'objective function' could be an alternative?
Why does A* use Euclidean distance as heuristic and not Manhattan distance which seems more sensible in a grid?
I like the qualitative comparison which only compares length for problems solved by all solvers
What is the key that makes this approach more scalable compared to Liao et al. 2020, He & Bao, 2020?","6. Additional feedback Minor comments/suggestions/compliments/questions: The term 'score function J(theta)' can be confusing since typically the score function refers to grad log p_theta. Maybe 'objective function' could be an alternative? Why does A* use Euclidean distance as heuristic and not Manhattan distance which seems more sensible in a grid? I like the qualitative comparison which only compares length for problems solved by all solvers What is the key that makes this approach more scalable compared to Liao et al. 2020, He & Bao, 2020?",279,0
ICLR_2021_784,ICLR_2021,"- The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2 - In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper. - As the authors of the TFN paper note: in practice not all higher order irreps of the tensor product of the filter and the features are computed. This seems to indicate a big difference between the theoretical analysis - which includes all irreps and thus is computationally intensive even when modelling low order polynomials – and the practical application of TFNs. It would be interesting to know how expressive such practical low order TFNs are. Another difference between the described networks and practical TFN is that in the described networks, all relevant parameters are in the pooling layer, which sums a large number of terms (looking at the proof of lemma 2, exponential in D), while in practical TFNs, the parameters are in the filters.
Recommendation: The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper.
Suggestion for improvement: - Make the big picture clearer by providing more intuition. - Comment on the differences between the class of networks described and TFNs used in practice.
Minor points/suggestions: - P3 Add definition of W^n_T as n direct sums of W_T - P3 “where W_feat is a lifted representation of SO(3)”, what does lifted representation here mean? Just any rep? - I get a bit confused by the wording in Def 1. Unless I am mistaken, it appears like the quantifiers are reversed. Should it mean “for every polynomial …, there exists f_1, … in F_feat and linear functionals Lambda_1, …, : W_feat -> R ”? - Around Eq 5, perhaps the authors could clarify the clarify the domain of the Q functions, which I suppose is Q^r : R^3n -> T_T, where T= r _1 - Around Eq 7, are X_j and x_j the same? - In lemma 4, is A_k any linear map or an equivariant linear map? - In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1? - In the proof of thm 1, it says “p: R^{d \times n} \to W_T”, should that be W_T^n? - In the proof of lemma 2, it says “we see that that exists a linear functional”
Post rebuttal
I thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas. My previous rating still applies.","- The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2 - In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper.",280,0
ICLR_2021_2143,ICLR_2021,"Weakness:
---Lack of novelty--- While the results are impressive (e.g., MOS score) I feel like the proposed approaches lack novelty. For example, 1)Duration prediction has been there for a while since Fastspeech1. 2)GaussianSampling is similar to EATS (https://arxiv.org/abs/2006.03575). 3)FVAE is also not proposed by the authors
---Lack of some experiments--- I assume the biggest advantage of Non-attentive Tacotron is the robustness in generation quality. If so, does the unsupervised Non-attentive Tacotron still has the advantages of less over-generation and word skipping problems? It’d have been nicer if the authors had shown UDR and WDR results in the unsupervised setting too.
---Writings--- Although using FVAE makes the unsupervised experiments successful, the insights on using FVAE to tackle this problem is not clearly stated. The authors must state why is combining FVAE approach expected to predict duration better than the naïve w/o FVAE version. Also, I ask the authors to keep readers in mind little more when writing the paper. The authors should make the writings clearer when explaining the existing methods. For example, in Section3, the depth of explanations on FVAE is too short. Please elaborate more on how it can be formulated as conditional VAE framework.
Rating: I consider it “not bad” paper, but I think the impact of this paper is not strong enough to pass the bar of ICLR because of the lack of novelty (as written in Weakness part). Therefore, I recommend rejection. However, I’d be happy to listen to the authors’ opinion regarding this issue. Questions:
--- Questions on Vanilla upsampling --- It has been shown by Fastspeech1 that the Vanilla upsampling gives similar score to Tacotron2 model (3.84 vs 3.86). In this paper, however, it seems like the Vanilla upsampling is not working very well compare to Tacotron2 (4.13 vs 4.37). I’d like to ask the authors what the source of this difference could be.
--- Questions on training WaveRNN --- The authors have written that WaveRNN model was trained on predicted features. In this case, I assume the ground truth waveform and the predicted features are unaligned because Tacotron2 autoregressively decodes features, which must be different to the ground truth mel-spectrogram. How could WaveRNN be trained well enough in this setting? Were the predicted features predicted using Teacher-forcing?",1)Duration prediction has been there for a while since Fastspeech1.,281,0
ICLR_2021_2491,ICLR_2021,"Even though 2 hyperparameters are considered in the ablation study, its capability and stability with a moderate number of hyperparameters are not well-addressed (in the main experiment only 1 hyperparameter is considered). And the use of a validation set only in hyperparameter update step make me question that this hyperparameter optimization component may cause some issues such as training instability. Can the authors share some thought, experience, and intuition on this?
It is questionable whether the comparison in Figure 2 is fair since DiffAutoML utilizes validation data in its hyperparameter optimization step whereas DSNAS seems to not use validation data.
Similar to above, the numbers for DSNAS in Table 1 seem to be taken from DSNAS paper (Table 3) which are numbers from the validation set.
Recommendation The paper tackles interesting and practical problems and shows the proposed method outperforms baselines. My main concern is whether DiffAutoML uses validation data that is not used by other baselines, which could be a reason for better performance. The concern on considering on a small number of hyperparameters is a minor one if some explanation can be added. As long as the concern on the validation data access is resolved then I will be willing to increase my score favoring acceptance. However, in its current form, I cannot stand for it. Questions
The numbers reported in Table 1 and Table 2 are test performances?
Additional feedback (Irrelevant to the decision assessment)
Since the most of NAS component in the full pipeline is from DSNAS, it would be better for readers if this is detailed in Related Works or Appendix.
At 4th line in Algorithm 1, using the word 'parent' network is more consistent?
In the last line in Abstract, en-to-end -> end-to-end
At 3 lines above eq. 3. ont-hot -> one-hot
In the last paragraph in Introduction, in the 4th line from the last, Differentiable -> differentiable","3. ont-hot -> one-hot In the last paragraph in Introduction, in the 4th line from the last, Differentiable -> differentiable",282,1
ICLR_2021_1948,ICLR_2021,"a. Anonymisation Failure in References
i. A reference uncited in the manuscript body contains a non-anonymised set of author names to a paper with the same title as the system presented in this paper. This was not detected during initial review. ""Shuby Deshpande and Jeff Schneider. Vizarel: A System to Help Better Understand RL Agents.arXiv:2007.05577.""
b. Citations
i. There is egregiously missing information in almost all citations. This is quite obvious in all the missing dates in the manuscript text.
c. Clarity
i. There are a few unclear or misleadingly worded statements made as below:
1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).
2) ""stronger feedback loop between the researcher and the agent"" - This is at least confusing. In any learning setting, there is a strong interaction loop between experimentation by the researcher and resulting outcomes for the trained model.
3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.
4) ""For multi-dimensional action spaces, the viewport could be repurposed to display the variance of the action distribution, plot different projections of the action distribution, or use more sophisticated techniques (Huber)."" - It would be clearer to actually state what the sophisticated techniques from Huber are here.
ii. The framework could be clearer that it applies most directly as described on single agent RL. The same approach could be used with multi-agent RL but the observation state and visualisations around that get more confusing when there are multiple potentially different sets of observations. Is this not the case, please clarify?
d. Experimental rigour
i. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. Not every paper requires a user study, however, the contributions proposed by this particular manuscript require validation at some level from actual RL users (even a case study with some feedback from users would address this to some extent).
e. Novelty in Related Work
i. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See a sampling below.
1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }
2) @inproceedings{ Atrey2020Exploratory, title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning}, author={Akanksha Atrey and Kaleigh Clary and David Jensen}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rkl3m1BFDB} }
3) @inproceedings{ Puri2020Explain, title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution}, author={Nikaash Puri and Sukriti Verma and Piyush Gupta and Dhruv Kayastha and Shripad Deshmukh and Balaji Krishnamurthy and Sameer Singh}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SJgzLkBKPB} }
4) @article{reddy2019learning, title={Learning human objectives by evaluating hypothetical behavior}, author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey and Legg, Shane and Leike, Jan}, journal={arXiv preprint arXiv:1912.05652}, year={2019} }
5) @inproceedings{mcgregor2015facilitating, title={Facilitating testing and debugging of Markov Decision Processes with interactive visualization}, author={McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald}, booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, pages={53--61}, year={2015}, organization={IEEE} }
6) @article{puiutta2020explainable, title={Explainable Reinforcement Learning: A Survey}, author={Puiutta, Erika and Veith, Eric}, journal={arXiv preprint arXiv:2005.06247}, year={2020} }
7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }
8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }
9) @misc{sundararajan2020shapley, title={The many Shapley values for model explanation}, author={Mukund Sundararajan and Amir Najmi}, year={2020}, eprint={1908.08474}, archivePrefix={arXiv}, primaryClass={cs.AI} }
10) @misc{madumal2020distal, title={Distal Explanations for Model-free Explainable Reinforcement Learning}, author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere}, year={2020}, eprint={2001.10284}, archivePrefix={arXiv}, primaryClass={cs.AI} }
11) @article{Sequeira_2020, title={Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations}, volume={288}, ISSN={0004-3702}, url={http://dx.doi.org/10.1016/j.artint.2020.103367}, DOI={10.1016/j.artint.2020.103367}, journal={Artificial Intelligence}, publisher={Elsevier BV}, author={Sequeira, Pedro and Gervasio, Melinda}, year={2020}, month={Nov}, pages={103367} }
12) @article{Fukuchi_2017, title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents}, ISBN={9781450351133}, url={http://dx.doi.org/10.1145/3125739.3125746}, DOI={10.1145/3125739.3125746}, journal={Proceedings of the 5th International Conference on Human Agent Interaction}, publisher={ACM}, author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita}, year={2017}, month={Oct} }
4. Recommendation
a. I recommend this paper for rejection as the degree of change needed to validate the contribution through a user study or other validation is likely not feasible in the time and space needed. However, the contribution is potentially valuable to RL, so with the inclusion of this missing evaluation and additions to related work/contextualisation of the contribution, I would consider increasing my score and changing my recommendation.
5. Minor Comments/Suggestions
a. It is recommended to use the TensorFlow whitepaper citation for TensorBoard (https://arxiv.org/abs/1603.04467). This is the official response (https://github.com/tensorflow/tensorboard/issues/3437).","7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }",283,1
ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","• The parts describing the method are hard to follow, it will be useful to improve their clarity.",284,0
ICLR_2022_1159,ICLR_2022,"Weakness: - The biggest limitation of this work is considering only the multimodal data having two modalities as well as not even discussing how the proposed method behaves in case of having more modalities. - Although the paper presents a task-free method, the experimental analysis were limited to two datasets, both are addressing the same task: emotion recognition. As also mentioned in the introduction there are several other tasks that the proposed method could have been tested on. Indeed, the related work (such as Ma et al. SMIL: ""Multimodal Learning with Severely Missing Modality"".) was tested on several different tasks. I suggest authors to either change the paper including the title, abstract and related work and target categorical emotion recognition or comprehensively extend the experimental analysis such that the proposed method would be tested and validated on several other tasks. - It is also important to mention that the proposed method was tested only for categorical emotion recognition, while emotion datasets are typically multi-labeled (as humans cannot elicit only one emotion at a time) and also include continuous values therefore, regression task might be targeted too - I also found the used datasets limited in terms of their size. In case authors would like to keep the emotion recognition task as the testbed, I suggest them using a much larger dataset called: CMU-MOSEI, also having other modalities than video and audio. Indeed some related work was tested on CMU-MOSEI and/or CMU-MOSI such as Ma et al. SMIL: ""Multimodal Learning with Severely Missing Modality"". - Another limitation regarding the experimental analysis performed is that: as modality only visual and audio data were used. Testing on combinations of several other modalities: text, depth data, data of mocap, accelerometer, gyro-meter would improve the validity of the proposed method. - “In addition, the generalized softmax function we propose….” Generalized softmax function might be misleading, it more sounds like the used softmax has tolerance to the diversity of samples belonging to different classes or somehow a domain adaption is being applied. But these are not the cases. - It is unclear why authors think that the multimodal softmax is a contribution. Eq. 3 and following equations look like standard softmax was written for multimodal data instead of first fusing the data and representing the fused data as a single feature vector. On the other hand, the fusion of the data is performed through standard strategies: addition, concatenation and multiplication. I expect authors to clarify the contribution in this respect. - There are also lack of information regarding how the data is being processed. In detail: a) What does “we take central frame” as visual modality mean? Do you take a bunch of frames and use only the central frame? If so what is the motivation behind this? What is the window size? In fact, it is more frequent to apply spatio-temporal processing, for example, processing motion and appearance in facial images for emotion recognition. Thus, I do not understand the rationale behind discarding the temporal information. b) Another issue is reading the audio data; it is not clear what the audio data chunk selected to calculate the log Mel-spectrogram. c) “On each processed dataset, we split all data into three parts: training set, validation set, and test set. Their proportions are 70%, 15%, and 15%.” Are you randomly picking these splits and applying sort of a k-fold cross validation or these splits are obtained only once and fixed? Do you guarantee that you use exactly the same split for all baseline methods, this is a matter because the used datasets are relatively small? I am aware that prior art on emotion recognition uses 5 or 10 fold cross validation for the same datasets, and I am not sure why authors have selected a different data splitting strategy. - The proposed method was compared with some relatively simpler baselines such as zero padding, but the comparative study should include the SOTA methods, e.g., Ma et al (2021b), Tran et al. (2017), Chen & Zhang (2020), Liu et al. (2021), Suo et al., (2019). Given this lack of comparison, I believe that the claim of authors “……which lead to the information of the modality-missing data not being well exploited to” was not justified as well. - I believe authors should include a better discussion why they tackle with the missing data only in training but never take into account that there could be missing modality in testing as well. I think in a practical scenario it is more possible to train a model with a full set of modalities, while during test some of the modalities are either completely or only for some test samples missing. - Tables should include the results of unimodal data processing to allow reader to understand which modality perform better than other when used alone, and include the results of processing complete data (i.e., no missing modality) as the upper bound. - “When the visual modality is missing, the classification accuracy is lower than that when the audio modality is missing, indicating that the visual modality has a more significant contribution to the classification performance, which is consistent with previous works (Zhang et al., 2017; Ma et al., 2020).” I believe the citations in this sentence is a bit irrelevant. In detail, the authors are not using neither the same feature sets nor the same datasets with the cited works.","- I believe authors should include a better discussion why they tackle with the missing data only in training but never take into account that there could be missing modality in testing as well. I think in a practical scenario it is more possible to train a model with a full set of modalities, while during test some of the modalities are either completely or only for some test samples missing.",285,0
ICLR_2022_2980,ICLR_2022,"Weakness:
Regaing the first contribution, there are some assumptions the authors make: 1) entity sparsity; this seems to be true on datasets with small amount of entity types, but may not hold on fine-grained NER, where incomplete annotations can be a real problem. 2) why only ⌈ λ n ⌉
are sampled (see Question 1)
The second contribution seems very marginal, and it is not clear how much efforts are needed to choose these new hyper-parameters and achieve these improvements. (See suggestion 3)
The incomplete annotations seem to be a specific case of 'noisy labels', however, there are no methods from the category 'training with noisy labels' are mentioned in this paper. Also the chosen baselines which can deal with incomplete annotations are all CRF-based, which I believe by natural are more difficult to train on noisy labels than span-based NER models Questions:
Regarding Theorem 1, the size of sampled negatives is set as ⌈ λ n ⌉
, why it is not ⌈ λ n ( n + 1 ) 2 ⌉
? considering the NER is a span-based model.
what is the difference between Biaffine (Yu et al., 2020) and your span-based model? What is the main reason you think your model outperforms Biaffine on CoNLL2003 (Table 5).
Is the 'held-out training data' described in Section 3 and 4 similar to the 'clean label data' in distant supervision literature? Suggestions:
The described unlabeled entity problem may also relate to nested NER: if only outermoster mentions are labelled and all contained mentions are not explictly labelled in the training data, can the proposed method be used to train a Nested NER model that can recognize both containing and contained entity mentions.?
Suggest to explain the connections between your work and methods work on 'training from noisy labels' (see missing references)
Suggest to add sensitivity analysis regarding different choices of τ and μ
in Equation 5
Suggest to add more explanations regarding the calculation of r i , j
in Equation 5
Missing reference: Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels. Han et al., 2018. NeurIPS Adaptive Sample Selection for Robust Learning under Label Noise Patel and Sastry, 2021 Analysing the Noise Model Error for Realistic Noisy Label Data. Hedderich et al., 2021, AAAI (they also work on NER)","1) entity sparsity; this seems to be true on datasets with small amount of entity types, but may not hold on fine-grained NER, where incomplete annotations can be a real problem.",286,0
ICLR_2022_1126,ICLR_2022,"Weakness - The paper seems to be not ready enough with the lack of content. Specifically, there is significant lack of literature reviews of the recent advances in active learning communities, especially in the family of Bayesian learning (e.g. Bayesian Coreset is directly relevant to this work, which is ignored in the literature review) which usually provide much more reliable uncertainty measurements then ‘doubt’. In addition, there has been multiple works discussing similar direction (combining uncertainty with Coreset) such as [1][2], it might also worth discussing how the current setup is more advanced than related studies. - Another problem is the lack of comparison to recent SOTA baselines. Despite that both BADGE and BatchBALD are mentioned in related work section, the authors only compare to vanilla Coreset which is rather old and non-SOTA. The reasoning that “original core-set algorithm improved significantly from those baselines, we expect improvement over the original core-set algorithm to imply similar or greater improvement as well” does not make sense as the paper was published quite early and only compared to less advanced algorithms at the time. - The key assumption that “doubt acts as a cheap but noisy estimate of the distance to the nearest point with zero error” is a bit questionable to this reviewer, as it is known that neural networks tend to be over-confident in some regions despite the error is high. Some discussions on when the assumption might break and perhaps the consequence in that scenario would be appreciated.
Reference [1] Confident Coreset for Active Learning in Medical Image Analysis, Kim et al [2] Bayesian Active Learning by Disagreements: A Geometric Perspective, Cao et al","- Another problem is the lack of comparison to recent SOTA baselines. Despite that both BADGE and BatchBALD are mentioned in related work section, the authors only compare to vanilla Coreset which is rather old and non-SOTA. The reasoning that “original core-set algorithm improved significantly from those baselines, we expect improvement over the original core-set algorithm to imply similar or greater improvement as well” does not make sense as the paper was published quite early and only compared to less advanced algorithms at the time.",287,0
ICLR_2022_337,ICLR_2022,"(and questions) 1. The paper proposes several sufficient conditions that VI-LCB could have a better worst-case sub-optimality gap. However, it does not provide practitioners a clear guidance on when they should run BC and when they should run VI-LCB, which I think is one of the main purpose of the paper. How should we check if the sufficient conditions are satisfied in real world problems? Some of the parameters are even unknown (e.g., b in Corollary 4.2). 2. I find it difficult to compare these upper bounds. Is it possible that some algorithms get larger bounds simply because the analysis is not tight? Can we say that algorithm A is better than algorithm B because an upper bound (of the sup-optimality gap) for A is lower than an upper bound for B? 3. Some conditions are not well-justified. The paper studies some sufficient conditions that VI-LCB outperforms BC, however, some conditions seems a little arbitrary to me (e.g., condition 3.2, condition 4.1 and condition 4.2). I guess the paper has some specific applications in mind and makes these conditions based on these applications. However, this makes the results less clear and less general. For example, how do the results extend without condition 3.2? do we expect to just see an extra H term in all bounds without the condition? If the paper considers some specific applications, the paper should clearly mention it instead of saying the conclusion holds in general.
4. The paper studies a particular type of offline RL algorithm based on VI with LCB style penalty. However, the title and the paper seems to suggest that this particular algorithm can represent the class of offline RL algorithms? I don’t see how study one algorithm can generalize to a conclusion for offline RL algorithms? 5. I am not sure what is the novelty and significance of the theoretical results. It seems the theoretical results are mostly borrowed from existing literature with some modifications. I want to make sure I am not missing something important there. Can you explain more about the novelty or significance for the theoretical results?
Minor questions and comments:
In the empirical section, does CQL lie in the VI-LCB framework? If not, why is it used in the empirical section while all theoretical results are for VI-LCB.
VI-LCB with Bernstein style penalty is also used in [1], which is not cited in this paper.
[1] Xie et al. (2021) “Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning”","4. The paper studies a particular type of offline RL algorithm based on VI with LCB style penalty. However, the title and the paper seems to suggest that this particular algorithm can represent the class of offline RL algorithms? I don’t see how study one algorithm can generalize to a conclusion for offline RL algorithms?",288,0
ICLR_2022_1759,ICLR_2022,"weakness 1. The idea of this paper is direct. This paper still considers traditional semi-supervised learning, and the multi-view refers to the view generated from instance augmentation. Why only measure the diversity of labeled instances and not extend to unlabeled instances? Are there relevant experimental verifications? 2. What is the advantage of combining diversity with consistency based on underlying probabilistic graphical assumptions? Several multi-view methods have already considered the diversity with consistency simultaneously, but the authors choose to ignore them, e.g., “Exclusivity-Consistency Regularized Multi-view Subspace Clustering”, “End-to-End Adversarial-Attention Network for Multi-Modal Clustering”. More analyses and comparisons are expected, the reviewer considers that these ideas can also be transformed into the semi-supervised framework. 3. Although this paper considers semi-supervised regression, why can't the semi-supervised classification method extend to the regression problem by replacing the loss function? The SOTA comparison method is in 2018, more comparison methods recently are expected.","1. The idea of this paper is direct. This paper still considers traditional semi-supervised learning, and the multi-view refers to the view generated from instance augmentation. Why only measure the diversity of labeled instances and not extend to unlabeled instances? Are there relevant experimental verifications?",289,0
ICLR_2022_3205,ICLR_2022,"Major
I'm quite torn about this submission in the sense that, while it is a nice paper, it lacks a main theoretical or empirical contribution. The experiments conducted in smaller environments are illustrative but are, by themselves, not compelling enough to demonstrate how the concept of cross values aids in accelerating Bayes-adaptive RL in more complex environments. Even one compelling experiment that shows how cross values improve upon the approaches of, for instance, (Zintgraf et al., 2020, Zintgraf et al., 2021) would really ground the paper around a culminating empirical result and take it over the edge. Alternatively, a nice set of theoretical results could also accomplish this.
While the definition of the value of current information in Equation 9 looks reasonable, I can't help but wonder if it is uniquely suited to helping accelerate BADMP learning. Certainly, the literature on PAC-BAMDP methods offers other forms of reward bonuses that do yield provably-efficient learning [12,19]. For instance, rather than integrating out randomness in both e and e', why not take e' to be the environment that has highest likelihood under b_t? Or, alternatively, rather than looking at the expected value across all environments e, perhaps it makes sense to look at the worst case value (minimizing over e) which might have implications for safe or risk-sensitive reinforcement learning? The higher level point here is that, without a corroborating theory, these definitions, while intuitive, seem heuristic and it is unclear if these are the ""right"" quantities to be examining and learning. Why is the augmented reward structure proposed here better than the Bayesian exploration bonus of [12] or the variance-based bonus of [19]? Notably, both of those approaches come with PAC-BAMDP/PAC-MDP guarantees, while the same cannot be said (or at least has yet to proved) of learning under the PCR.
The different in value functions shown in Equation 14 seems to align with the definition of the value of information (VOI) in the context of optimal learning [9,10,15,18]. In the context of multi-armed bandits, such knowledge-gradient algorithms based on VOI will only take an action if there is an immediate improvement in posterior reward. There is an alternative to Thompson sampling [17] which is explicitly designed to address the shortcoming mentioned on page 4: ""a posterior sampling agent cannot learn how to perform actions that are not optimal in any known environment."" The information-directed sampling (IDS) algorithm, while originally limited to bandits[11,13], attempts to strike the appropriate balance between information gain and regret minimization. Section 4.3.3 of [17] provides a few examples of how an exploration criterion based on VOI is still insufficient for addressing exploration; briefly, the issues boils down to the fact that information revealed at a given moment in time need not result in an immediate performance improvement to be useful, so long as it contributes to performance in the long-term. Highlighting the map example mentioned at the top of page 4, consider a nested version of the problem where an agent must navigate to two separate maps in sequence (for context, say one map to get out of some wrong building and then a second to navigate efficiently in the correct building). Acquiring information by navigating to the first map doesn't result in higher expected return, but is clearly a necessary step towards achieving higher returns and ultimately solving the task. Do the authors have any thoughts on this connection and whether or not a similar impediment arises when using PCR rewards? If the same story holds true, it would again call into question whether or not PCR rewards are the ""correct"" quantity for calibrating information gain. Minor
On the point of developing supporting theory for the paper, I can't help but notice that Equation 13 has the exact form of a potential-based shaping function [14], except in the context of a BAMDP rather than the traditional MDP. Have the authors contemplated this connection at any depth? To the best of my knowledge, I don't know of any papers that take the years of work on potential-based reward shaping in MDPs and considers how to invoke those ideas to accelerate BAMDP learning. An updated version of this work could build on that connection as part of a more concrete theoretical contribution. Clarity Strengths
The paper is both well-written and well-organized. The authors do a fantastic job of navigating readers through BAMDPs, cross values, predictive reward cashing, limitations, experiments, and future work. Weaknesses Major
I would have liked to see explicity algorithms with pseudocode for PCR Q-learning and PCR-TD, just to confirm my intuitions about them beyond the exposition of Sections 5-8. Minor
There is some inconsistency in the document between the use of PCR vs. PRC. I believe the authors intended to use PCR and instances of the latter are typos. Originality Strengths
The authors do a good job of contextualizing the potential of their approach in augmenting and improving recent work on Bayes-adaptive deep reinforcement-learning algorithms. Weaknesses Major
The literature review on more classic work on BAMDPs and Bayesian reinforcement learning seems lacking. For a paper that leans so heavily on these topics and offers a foundational contribution, I would expect a more nuanced discussion of the classic literature [2, 3, 8, 20, 5, 4, 6, 7, 1, 12, 16, 19] without deferring readers to the 2015 survey paper on Bayesian reinforcement learning. Minor Significance Strengths
The results do a good job of highlighting the potential utility of cross values in Bayesian reinforcement learning. Weaknesses Major
While the conceptual idea behind cross values is interesting, I suspect there is a nontrivial amount of work needed to realize its benefits (if any) in the context of deep reinforcement learning. Without any confirmation of that success yet, it is difficult to see this paper as having high impact. As mentioned above, an alternative contribution would provide supporting theory for cross values and PCR rewards, offering the community a new perspective on how to handle approximate Bayesian reinforcement learning. Minor References
Asmuth, John, Lihong Li, Michael L. Littman, Ali Nouri, and David Wingate. ""A Bayesian sampling approach to exploration in reinforcement learning."" In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 19-26. 2009.
Bellman, Richard, and Robert Kalaba. ""On adaptive control processes."" IRE Transactions on Automatic Control 4, no. 2 (1959): 1-9.
Dayan, Peter, and Terrence J. Sejnowski. ""Exploration bonuses and dual control."" Machine Learning 25, no. 1 (1996): 5-22.
Dearden, Richard, Nir Friedman, and Stuart Russell. ""Bayesian Q-learning."" In Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 761-768. 1998.
Duff, Michael O. ""Monte-Carlo algorithms for the improvement of finite-state stochastic controllers: Application to Bayes-adaptive Markov decision processes."" In International Workshop on Artificial Intelligence and Statistics, pp. 93-97. PMLR, 2001.
Duff, Michael O. ""Design for an optimal probe."" In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 131-138. 2003.
Duff, Michael O., and Andrew G. Barto. ""Local bandit approximation for optimal learning problems."" In Proceedings of the 9th International Conference on Neural Information Processing Systems, pp. 1019-1025. 1996.
Duff, Michael O'Gordon. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. University of Massachusetts Amherst, 2002.
Frazier, Peter I., and Warren B. Powell. ""Paradoxes in learning and the marginal value of information."" Decision Analysis 7, no. 4 (2010): 378-403.
Frazier, Peter I., Warren B. Powell, and Savas Dayanik. ""A knowledge-gradient policy for sequential information collection."" SIAM Journal on Control and Optimization 47, no. 5 (2008): 2410-2439.
Kirschner, Johannes, and Andreas Krause. ""Information directed sampling and bandits with heteroscedastic noise."" In Conference On Learning Theory, pp. 358-384. PMLR, 2018.
Kolter, J. Zico, and Andrew Y. Ng. ""Near-Bayesian exploration in polynomial time."" In Proceedings of the 26th annual international conference on machine learning, pp. 513-520. 2009.
Lu, Xiuyuan, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and Zheng Wen. ""Reinforcement Learning, Bit by Bit."" arXiv preprint arXiv:2103.04047 (2021).
Ng, Andrew Y., Daishi Harada, and Stuart Russell. ""Policy invariance under reward transformations: Theory and application to reward shaping."" In Icml, vol. 99, pp. 278-287. 1999.
Powell, Warren B., and Ilya O. Ryzhov. Optimal learning. Vol. 841. John Wiley & Sons, 2012.
Poupart, Pascal, Nikos Vlassis, Jesse Hoey, and Kevin Regan. ""An analytic solution to discrete Bayesian reinforcement learning."" In Proceedings of the 23rd international conference on Machine learning, pp. 697-704. 2006.
Russo, Daniel, and Benjamin Van Roy. ""Learning to optimize via information-directed sampling."" Advances in Neural Information Processing Systems 27 (2014): 1583-1591.
Ryzhov, Ilya O., Warren B. Powell, and Peter I. Frazier. ""The knowledge gradient algorithm for a general class of online learning problems."" Operations Research 60, no. 1 (2012): 180-195.
Sorg, Jonathan, Satinder Singh, and Richard L. Lewis. ""Variance-based rewards for approximate Bayesian reinforcement learning."" In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pp. 564-571. 2010.
Strens, Malcolm. ""A Bayesian framework for reinforcement learning."" In ICML, vol. 2000, pp. 943-950. 2000.","1 (2012): 180-195. Sorg, Jonathan, Satinder Singh, and Richard L. Lewis. ""Variance-based rewards for approximate Bayesian reinforcement learning."" In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pp. 564-571. 2010. Strens, Malcolm. ""A Bayesian framework for reinforcement learning."" In ICML, vol. 2000, pp. 943-950. 2000.",290,1
ICLR_2022_3095,ICLR_2022,"- Throughout the paper, the biomedical and clinical domains are often conflated. Three of the four information retrieval datasets are biomedical, but the MIMIC-derived dataset as well as the 4 clinical outcome prediction tasks are clinical tasks (i.e. derived from electronic health record notes). The paper itself notes that clinical text has idiosyncratic language (e.g. medical abbreviations, semi-structured text, etc.), yet on page 1, it states that BioBERT is trained on in-domain data. This is particularly important because BioBERT is well suited for the biomedical tasks, but not as well suited for the clinical tasks. A better in-domain model would be a clinicalBERT model. It seems a bit strange for the paper to emphasize that KIMERA can improve on an in-domain model when the BioBert model isn’t really “in-domain” for most of the tasks. Relatedly, the only task in which KIMERA outperforms BioBERT is on MIMIC CAPR task, which is likely more so due to the ill fit of BioBERT to clinical text compared to biomedical text. - While the KIMERA model does outperform BERT-base and the BioBERT + KIMERA model does provide benefit over BioBERT alone, KIMERA doesn’t outperform BioBERT. This begs the question of when a KIMERA model provides benefit. It appears that in-domain pretraining is better than general BERT+KIMERA, but it’s unclear whether in-domain pretraining + KIMERA is better than in-domain pretraining alone. While the paper does compare BioBERT to BioBERT + KIMERA, this doesn’t seem to be a fair comparison on the clinically oriented tasks. A more natural comparison might be to compare a ClinicalBERT model and ClinicalBERT + KIMERA. ClinicalBERT model performances are already reported in the van Aken et al. paper on the Clinical Outcome Prediction tasks, but are not included in this paper for some reason. Some discussion of this seems warranted, especially because these models and the new model in van Aken et al. outperform the models presented in this work. If the authors are trying to state that their domain adaptation approach is more efficient, they’d benefit from more explicitly comparing the time to further pretrain BERT on in-domain data (beyond just train time of BioBERT) vs time to train KIMERA. - There is no comparison to other approaches for incorporating domain knowledge into transformer models (e.g. UMLSBert or Kim et al. 2020) so it’s difficult to make any conclusions about the relative pros/cons of different strategies.
Other Comments: - Please specify which results are taken from other papers and which are rerun yourself. I noticed that the BERT & BioBERT results on the OP tasks were identical to those in the van Aken paper. - I’d be interested to see what the performance would be when you randomly select which attention heads to retrain compared to the soft/hard masking approaches. This would help evaluate how good these approaches are at identifying useful attention heads to retrain. - Overall, I like Figure 1, but it’d be helpful to explain what “r Index: 4” means in the table. - How are cases handled where there are multiple correct answers (i.e. multiple entities that would suffice) for the entity prediction pretraining task? Do you mask out all word pieces corresponding to the entity, and if so, does the number of masked word pieces inherently give a clue as to the entity masked? - Additional details about the filtering of UMLS should be included in the appendix. How were “relevant relation types” defined? Which sources in the Metathesaurus were used? Etc. - “the process halts once a threshold T of the overall performance is reached.” Can you clarify how the range of values for T were determined? Further clarification of the T parameter would be useful. - The paper states that negative examples for triplet classification were generated by “replacing one of the three components with the same component from a different randomly selected triplet.” Instead of just replacing e.g. an “object” with an “object” from another triplet, did you consider selecting an “object” associated with the same relation as your positive example? This process will generate harder negative examples and could improve the pretraining performance. - It’s hard to understand the format of the CAPR tasks from the description alone. An example query and passage(s) would help clarify this. In particular, further information is needed to specify how MIMIC-III was used as a CAPR task. Why were the discharge summaries truncated to 376 when the max length of BERT is 512? - Clarify why the masks are generated from separate held out parts of the test sets of each dataset (pg 6) instead of the train set. - Are there any measures of variance in model performance in table 1? - The appendix A1 includes results for the CAPR tasks. What does this analysis look like for the clinical outcome prediction tasks where the benefits of KIMERA were less prominent? - What is the standard deviation of model performance in the A4 figure?",- Clarify why the masks are generated from separate held out parts of the test sets of each dataset (pg 6) instead of the train set.,291,0
ICLR_2022_708,ICLR_2022,"/ Concerns:
As this work seems to be a direct response to Yehuda et. al. (2020) (see e.g. Abstract and Introduction of this work), I believe it would be very helpful to compare and clarify the relation of their results.
1. Data generation. Yehuda et. al. (2020) argues there is no efficient data generation procedure that is complete for these NP-hard problems. This work is no exception as otherwise it contradicts the NP-hardness assumption.
2. Easier subproblem. The sound and efficient perturbation model has to be incomplete, i.e. it only covers a subset of problem instances no greater than Yehuda et. al. (2020) dictates. That said, it is possible (but not necessarily true) that with the adversarial perturbation procedure, the space covered in the training set is enlarged, and is closer to the size upper bound implied by Yehuda et. al. (2020).
3. Spurious features. While adding adversarially perturbed data to the training set could potentially eliminate some spurious features, so could adding more random data, as long as random data sampling distribution covers the space of adversarial samples. Since adversarial samples require more computation to generate relative to random samples, from a practical perspective it would be helpful to compare the overall efficiency of the two training paradigms, including sample efficiency and the time required to obtain a single sample.
4. Novelty. While there seems to be no previous work on adversarial training of neural combinatorial solvers, this idea falls into the ""Data Augmentation"" category for data generation in Yehuda et. al. (2020), with some loosely related previous work. Reference:
Gal Yehuda, Moshe Gabel, and Assaf Schuster. (2020) It’s Not What Machines Can Learn, It’s What We Cannot Teach.
Finally -- this is not so important -- there are a few typos / word duplicates that could be spotted.","3. Spurious features. While adding adversarially perturbed data to the training set could potentially eliminate some spurious features, so could adding more random data, as long as random data sampling distribution covers the space of adversarial samples. Since adversarial samples require more computation to generate relative to random samples, from a practical perspective it would be helpful to compare the overall efficiency of the two training paradigms, including sample efficiency and the time required to obtain a single sample.",292,0
ICLR_2022_3002,ICLR_2022,"1.Why the GNN architecture and graph topology should be simultaneously optimized is not clear. E.g., what about optimizing one after optimizing the other. Or, the idea is a simple combination of two types of search spaces. 2.The model is somehow incremental, the main difference is adding augmentation search space, the search strategy is borrowed from others, which makes the contribution limited. 3.In 3.2.1, the author says augment graph can be written as a Haramard product, but how is AddEdge written? Besides, why AddEdge is working tends to be unclear for me. 4.The graph augmentation settings seem like a hyperparameter for me, so why not using HPO methods to optimize the graph augmentation setting? These methods are also baselines.","3.In 3.2.1, the author says augment graph can be written as a Haramard product, but how is AddEdge written? Besides, why AddEdge is working tends to be unclear for me.",293,0
ICLR_2022_2618,ICLR_2022,"Weakness: Method:
1. Problem Formulation:
A key step in the proposed method is to convert the multi-objective optimization problem into different constrained single-objective optimization problems. However, many conversion methods have already been proposed [1-3] which can tackle the non-convex Pareto frontier. None of them are discussed in this work. Is the proposed problem formulation new or adopted from other works?
What are its advantages (and disadvantages) over other methods?
What is J(\pi_k) in (7)?
2. Finding the Whole Pareto Front:
Theorem 1 is a crucial motivation of this work to find the whole Pareto front. However, the theoretical property of Theorem 1 is relatively weak. It can only guarantee that the optimal solution of the constrained single-objective problem can dominate other solutions that satisfy the same constraint. Therefore, the obtained solution is not guaranteed to be Pareto optimal (can be dominated by other feasible solutions out of the constrained set). In addition, even assuming all the single-objective problems can be successfully optimized, there is also no guarantee that all Pareto solutions can be found by the proposed method.
It is also confusing why the obtained solutions can exactly lie on the unit vector in Figure 1. In my understanding, the proposed method can find a solution that satisfies the preference-based constraint while minimizing the L2 norm of all objectives. Since the problem has an inequality constraint, the obtained solution should be close to the unit preference vector but not guaranteed to be exactly on the unit vector.
3. The Model Structure:
To incorporate the preference into the TSP-Net model, the proposed method simply adds the preference embedding to each node (city) embedding. According to the ablation study in Appendix, the preference embedding network can be a simple single-layer FC. The rest model is the standard TSP-Net where the whole transformer-based encoder can be totally replaced by a single 1-D convolution layer. Why can this simple structure work so well for MOTSP? Will it significantly hurt the model performance of each individual preference compared to a single objective model?
More concerns on the results are listed below in the experiment part.
4. Other Problems and Related Work:
One important advantage of the learning-based method is its flexibility to solve different problems [4]. Although this work focuses on MOTSP, I believe it could have a larger impact by showing its ability to solve other problems.
Many works on deep multi-task learning and multi-objective RL have been cited and discussed multiple times in the related work section, while they are not the most related work to this paper. I think it is better to shorten this part into a single paragraph, and leave more space to discuss the related work on learning-based solvers (e.g., [4]) and other approaches for MOTSP. Experiments:
5. Competitive Baselines:
According to the experimental results, the learning-based solvers are much better than the heuristic-based solvers. However, for the single objective TSP, the SOTA heuristic-solver (e.g., Concorde) usually has the best performance. Since the obtained Pareto front is not highly non-convex (as in Figure 2), the results for linear scalarization + Concorde should be included for a better comparison.
6. Unfair Comparison:
The comparison to other learning-based solvers is questionable. First of all, one contribution of DRL-MOA is the transfer learning based training method. The required training epoch (and time) is far less than those reported in this paper. Hence the results reported in Table 1 are misleading. It seems that all the PA-Nets are trained on the 120-city instances for 2,3 and 5 objective MOTSP. However, the original DRL-MOA is only trained on the 40-city MOTSP. As reported in this work, ""The trained model of bi-objective TSP for DRL-MOA (Li et al.,2020) is used."", will it lead to unfair comparison?
The TSP-Net is more powerful than the Ptr-Net used in DRL-MOA, so it is expected that it can have better performance. However, it only moderately outperforms DRL-MOA on the 2 and 5 objective problems, and has worse performance on the 3-obj problems. A more reasonable baseline is to use the TSP-Net in DRL-MOA to check whether the preference-based model could lead to worse performance.
To calculate the hypervolume, this work reports the results with {100,500,500} preferences for the 2, 3, and 5 objective MOTSP instances, while it only reports the results of {100, 91, 40} networks for DRL-MOA. Although it is understandable that one advantage of PA-Net is to generate a dense approximation, the results of {91, 40} preferences should also be reported for a clear comparison. Since PA-Net (with 500 preferences) are already outperformed by DRL-MOA (with only 91 models) for the 3-objective MOTSP, will it be significantly outperformed by DRL-MOA with the same number of solutions?
7. Missing Experiment Settings:
How many training samples and epochs are used to train the PA-Net and DRL-MOA? How many test instances are used to measure the performance for different methods? If there is only one instance for each kind of problem, the results are not convincing. What is the run time (inference) for each method to solve the MOTSP instance with different numbers of objectives and cities?
The details for Hypervolume calculation (e.g., the reference points for different instances) are missing.
8. Surprising Results on the Ablation Studies:
It is quite surprising that the transformer-based encoder is not needed for the TSP-Net, and only a simple one-layer FC is needed for preference embedding. With this setting, the whole PA-Net should have a much smaller scale. What are the total numbers of parameters for these ablation settings (1, 2, and 3)? With the 1-D Conv encoder, the ablation 3 model should be even smaller than the Prt-Net. Why it still needs ~12 hrs to train the model? Why can this model still outperform the Ptr-Net model on the 2-objective MOTSP? Reference:
[1] Das, Indraneel, and John E. Dennis. ""Normal-boundary intersection: A new method for generating the Pareto surface in nonlinear multicriteria optimization problems."" SIAM journal on optimization 8, no. 3: 631-657, 1998.
[2] Mavrotas, George. Effective implementation of the ε-constraint method in multi-objective mathematical programming problems. Applied mathematics and computation 213, no. 2: 455-465, 2009.
[3] Miettinen, Kaisa. Nonlinear multiobjective optimization. Vol. 12. Springer Science & Business Media, 2012.
[4] Kool, Wouter, Herke van Hoof, and Max Welling. Attention, Learn to Solve Routing Problems!. ICLR 2019.","2: 455-465, 2009. [3] Miettinen, Kaisa. Nonlinear multiobjective optimization. Vol.",294,1
ICLR_2022_2933,ICLR_2022,"1. The main claim mismatches the research problem. This paper claims to propose dataset compression method and motivates their method from related dataset compression (distillation) methods. However, the work they actually do is (single) image compression, which is another research topic. The proposed method is not designed for reducing the training set size (i.e. dataset compression). Instead, the authors compress the single images individually. Unfortunately, they are not clear about what they are doing. 2. Chaotic compression measurement. We know that images are saved with specific compression algorithms/standards (like JPEG) in computers. It is neither suitable nor fair to compare the matrix/tensor element numbers of the original image and that “compressed” by this method. A fair comparison can be the storage size in a hard disk. 3. Unclear short/long-range correlation information. The paper claims that they “compress datasets by filtering long-range correlation information” and “retain short-range correlation information”. However, no correspondence between short/long-range correlation and image pixels/textures is given. It is unknown whether they really removed the “long-range correlation information”. 4. Unconvincing experiments. No repeated results, no standard deviation. As stated in the paper, “Finally, we report results on testing datasets with the best model on evaluation datasets.” In addition, the results in Sec 5.2 are unconvincing. It is unacceptable to use a pre-trained resnet and then fine-tune it on 75% size of training data of ImageNet and get only 52.10% testing accuracy. 5. No ablation study. No comparison to methods in the past decade.",5. No ablation study. No comparison to methods in the past decade.,295,0
ICLR_2022_932,ICLR_2022,"weakness points.
Generalization and transfer knowledge. The PrefixLM is a generative modeling loss compared to the MLM's classification loss. For this reason, it may be keen on zero-shot generalization. They show competitive or better performance on zero-shot image captioning, zero-shot cross-modality transfer, and open-ended visual question answering tasks. Weakness
The PrefixLM is the paper's central argument, aside from the weak supervision dealt with in ALIGN (Jia et al., 2021) and others (e.g., CC 12M, Changpinyo et al., 2021). However, this paper lacks solid validation of their argument, as summarized below.
W1. Can the PrefixLM replace MLM? The critical ablation study to see the effectiveness of PrefixLM compared to MLM is 1) limitedly shown on VQA and image captioning in Table 6, a limited number of downstream tasks compared to the other experiments, although it is a critical study to validate their argument. Moreover, 2) this ablation study is performed for text-only data, not vision-language data in Section 4.4 Ablation Study. For this reason, their argument is fragile to be nullified in the further validation in the current status.
Could you provide any evidence that the PrefixLM is better than MLM in the controlled experiment (ablation study) for multimodal data?
W2. Can the PrefixLM be used as a standalone objective? For instance, the ITM loss is known to be effective combining with MLM (e.g., ViLT, Kim et al., 2021). They argue that a single objective is enough, but there is no validation of whether this is true. The natural questions on this matter include combining with ITM or other dataset-dependent losses would be helpful or result in mediocre performance.
Could you provide any evidence that PrefixLM + ITM method is not significantly different from PrefixLM only method? If not (PrefixLM + ITM is better), what would be the advantage of using the PrefixLM only?
W3. Fair comparison issue with the ALIGN dataset. The crucial problems in the experiments are that 1) it is irreproducible for its inaccessible collected datasets outside of the associated groups, and 2) it is hard to assess the contribution in the state-of-the-art comparison due to different pretraining datasets, different model architectures, and pretraining objectives. The controlled experiment still has the problems as described in W1.
For the reproducibility, could you provide the scores pretraining on WIT (Srinivasan et al., 2021) only? Writing
In the RHS of Eqn.3, if t = T p
, is P ( x T p x [ T p , T p ] , x < T p )
the intended one? Should x [ T p , T p ]
be null?
It is ambiguous that ""which decouples encoding from generation is conducive to the improvement of downstream task"" in the 1st paragraph, Section 3.3. What do you mean by ""decouple""?
In the 2nd paragraph of Section 3.3, ""we additionally add 2D relative attention for the image patches within transformer layers,"" which needs some elaboration for reproducibility and self-contained explanation.
Using C4 dataset was crucial? Why do you choose to use C4 additionally?
In the 4.1 Setup, you mentioned that ""512 text-only documents are in each batch,"" but, in this reading moment, cannot follow the context.
In Table 2, ""'Pre.' indicates the model is pretrained"" on what? Are you mentioning that the corresponding models are pretrained on CoCo or NoCaps? Here, all SimVLM is also pretrained on both ALIGN and C4 datasets, but you don't say all your model is pretrained.
In 4.3.1, please be aware that your Figure 2 is placed in Appendix and indicate explicitly.
In Table 2, could you specify the meaning of each row section? Why are two SimVLM_{huge} scores different in the 2nd and 3rd sections?
Explanation on transformer decoder is simply skipped. Could you provide implementational details for reproduction?","2) this ablation study is performed for text-only data, not vision-language data in Section 4.4 Ablation Study. For this reason, their argument is fragile to be nullified in the further validation in the current status. Could you provide any evidence that the PrefixLM is better than MLM in the controlled experiment (ablation study) for multimodal data?",296,0
ICLR_2022_1970,ICLR_2022,"... however, at the moment the work seems almost entirely heuristic driven. The paper needs more work before being ready for publication, as I will detail below.
Following on what I mentioned in the summary, while I can understand that, empirically, changes of sign in the weight norm happen in concomitance with what the authors say, I do not think that it is proper to present the method as it is done at the moment. From what I can read and from the pseudocode of the algorithm, ABEL decays the learning rate after registering two changes in the direction in which the weight norm is progressing. This does not necessarily mean that the first change happens at a (local) minimum while the second is because of noise. Just to be fully clear, what I mean is that, in principle, the first change could also be a local maximum. Then, empirically you can observe what the authors say, but, in order to strengthen the position of the paper, I think it would be necessary to show some more organized results about this phenomenon. For instance, reporting something on this line: ""over N experiments changing Y and Z conditions (e.g. varying the random seed, the initial learning rate, dataset, network architecture, etc), we observe that pair of changes in weight progression are in X% of the cases linked to [....]"". In these experiments, particular care must be taken not to confuse the causal direction of the interplay between weight norm and learning rate.
About the point above, one of the phenomena that worries me the most is that changes could be linked to noise. In fact, I noticed that in the code there is also a parameter called meas_freq that counteracts the presence of noise in some settings. This is a hyperparameter of the method which seems not to be mentioned in the main paper, which is probably used for some of the experiments (e.g. looking at the last row of Figure S8). Could the authors explain this? How sensible is ABEL w.r.t. this hyperparamter?
I can agree that ABEL is not as linked to the training budget as the cosine scheduler is, but I substantially disagree with the claim that ABEL does not require (at all) a fixed training budget. The training budget is even a parameter of ABEL in the code. This is not a major concern in my eyes, but I think the claim must be played down from what it is currently written.
There is ample space to improve the clarity of the paper. Here below are some points about this: a) For starters, I do not think that the authors mention explicitly what t
denotes (I assume it to be epochs, but could be e.g. also iterations...). This is quite important, especially when paired with observation 2. b) The introductory discussion about the learning rate decay depicts a rather partial picture (e.g. decay comes up also in the convergence proofs of subgradent methods for non-smooth optimization). c) The the sentence Given that stochastic gradients are used in deep learning, we will use a simple schedule as our baseline: a constant learning rate with one decay close to the end of training. is quite obscure to me. Why using stochastic gradients should imply that a simple schedule is a good baseline? d) The overall organization makes it difficult to pinpoint the actual contribution of the paper, as observations and experimental results are mixed with comments and parenthesis. e) Equation (1) lacks context. How is the loss defined? Are the authors considering networks that use batch norm or not (or both)? If yes, are the parameters of the batch norm layers included in w? Generally, the paper would greatly benefit from a clearer framing of the context. What is the typical setting that the authors have in mind and that ABEL is designed for?
The utility of (automatic) learning rate schedulers can be twofold: 1) improve on the state of the art, possibly starting from standard hyperparameter settings 2) ease the burden of hyperparameter tuning, especially when dealing with novel learning settings and datasets. As ABEL seems not to excel at 1) I think the authors should focus more on experiments to support 2). These could be done e.g. setting up time-controlled (repeated) experiments as in [1] .
I do not think that section 4 adds much value to the paper. Could the authors comment and summarize their findings reported in sec 4. and what do they add to e.g. Wan et al. (2020)? Typos:
which depend explicitly in (on) the number of training
to have a bouncing weight norm is that is (it?) has L2 regularization
[1] Donini, Michele, et al. ""MARTHE: Scheduling the Learning Rate Via Online Hypergradients."" IJCAI 2020.
Post-rebuttal: I thank the authors for their reply and modifications to the text that addressed some of my points. However, I remain of the opinion that this paper needs more work on several fronts before recommending acceptance. Among these, I would remark on: better treatment of the background material, clearer identification on when the weight norm behaviour happens beside L 2
norm (possibly looking also for counter-examples!), rethinking section 6, and a more convincing set of experiments (for showing convincing evidence about e.g. 5.2). Regarding this last point, I want to clarify that in my review I mentioned [1] not for the grid search, but rather for the time-controlled experiments. If you go with random search for selecting the hyperparameters of the learning rate adaptation methods. I personally think that a recipe to make the comparison fair enough is to choose a prior distribution (e.g. uniform/log uniform) that covers reasonable values (e.g. as used for different datasets) with mean equal/close to the known well-performing (""optimal"") value.
I think the main idea of the paper - i.e. linking weight norm to learning rate - is a quite interesting, novel and practical take. I do not raise my score because I think that the work would benefit greatly from a thorough review which could lead to a stronger submission to a later conference.
For 4.b, see e.g. Bertsekas, Dimitri P. (2015). Convex Optimization Algorithms (Second ed.). Belmont, MA.: Athena Scientific.","1) improve on the state of the art, possibly starting from standard hyperparameter settings",297,0
ICLR_2022_889,ICLR_2022,"1.Negative transfer and catastrophic forgetting are the common issues in the transfer learning field, which are not specific for multilingual learning. Besides, the proposed Sequential Reptile method is also general (simply modify the task sampling strategy to a sequential way during the inner optimization), which also has not special designs for the multilingual setting. I am curious why the authors choose this setting instead of more general transfer learning settings to verify the effectiveness of the proposed method.
2.The authors claim that finetune a model with MTL objective will gradually enforce the model to memorize task-specific knowledge. Did the authors compare with some transfer learning methods like multi-task adversarial training to learn task-invariant representations?
3.The experiments only focus on two tasks, i.e., QA and NER. It would be better to verify on more multilingual tasks, e.g., XGLUE and Xtreme.
4.Are the improvements statistically significant?",2.The authors claim that finetune a model with MTL objective will gradually enforce the model to memorize task-specific knowledge. Did the authors compare with some transfer learning methods like multi-task adversarial training to learn task-invariant representations?,298,0
ICLR_2022_2933,ICLR_2022,"1. The main claim mismatches the research problem. This paper claims to propose dataset compression method and motivates their method from related dataset compression (distillation) methods. However, the work they actually do is (single) image compression, which is another research topic. The proposed method is not designed for reducing the training set size (i.e. dataset compression). Instead, the authors compress the single images individually. Unfortunately, they are not clear about what they are doing. 2. Chaotic compression measurement. We know that images are saved with specific compression algorithms/standards (like JPEG) in computers. It is neither suitable nor fair to compare the matrix/tensor element numbers of the original image and that “compressed” by this method. A fair comparison can be the storage size in a hard disk. 3. Unclear short/long-range correlation information. The paper claims that they “compress datasets by filtering long-range correlation information” and “retain short-range correlation information”. However, no correspondence between short/long-range correlation and image pixels/textures is given. It is unknown whether they really removed the “long-range correlation information”. 4. Unconvincing experiments. No repeated results, no standard deviation. As stated in the paper, “Finally, we report results on testing datasets with the best model on evaluation datasets.” In addition, the results in Sec 5.2 are unconvincing. It is unacceptable to use a pre-trained resnet and then fine-tune it on 75% size of training data of ImageNet and get only 52.10% testing accuracy. 5. No ablation study. No comparison to methods in the past decade.",2. Chaotic compression measurement. We know that images are saved with specific compression algorithms/standards (like JPEG) in computers. It is neither suitable nor fair to compare the matrix/tensor element numbers of the original image and that “compressed” by this method. A fair comparison can be the storage size in a hard disk.,299,0
ICLR_2022_1344,ICLR_2022,"Weakness: 1. Many technical details are missing, making it like a preliminary report. - In Sec. 4, when introducing estimating users’ preferences, the inference process is demonstrated in detail. On the contrary, descriptions about training a user predictive model given the historical data are vague, which is equally important. - For example, how to obtain $P(z^{\pi '}H z^{\pi }\text{T+1}) i n S e c .4 .1 ( E s t i m a t i o n u n d e r k n o w n i n t e r n a l s t a t e d y n a m i c s ) ? − E q . 3 s e e m s t o h a v e a t y p o , "" = "" a n d "" \approx "" m i g h t s w a p t h e i r p o s i t i o n s ? − A u t h o r s h a v e n o t r e l e a s e d t h e i r e x p e r i m e n t c o d e , w h i c h m a k e s i t d i f f i c u l t f o r o t h e r c o l l e a g u e s t o f o l l o w . 2. E x p e r i m e n t d e t a i l s a r e a l s o n o t c l e a r . F o r e x a m p l e , − I n g r o u n d t r u t h h u m a n d y n a m i c s , g i v e n b_t^H(s) a n d
u_t$, how is the distribution over users’ next timestamp preferences generated? - Does the counterfactual preference estimation used in evaluating the possible influence of recommendation policies? - What is the myopic method used in experiments? - What is the physical meaning of the “SUM” (of engagement)? Authors claim in the abstract that their framework can avoid manipulative behaviors but still generate engagement. What is the meaning of generating engagement? It seems to me that this framework can improve engagement under the initial or naturally shifted user preferences according to Table 1/2. 3. Presentation quality can be improved by polishing up the wording and rearranging the figures and tables in Sec. 6&7.","1. Many technical details are missing, making it like a preliminary report.",300,0
ICLR_2022_917,ICLR_2022,"Via the top-K approach, it appears like the authors make an implicit assumption about what fraction of edges is important in the graph (a fixed fraction, can't be lower or higher) - in my opinion, this certainly limits the wide applicability of the method. Alternatively, in GNN Explainer [1], they allow up to K edges to explain the label.
The applicability of the method is limited by the graph encoder used (here GNN). 1-WL GNN's are known to be unable to predict links or identify and distinguish certain classes of subgraphs [2][3][4] i.e. GNN's are not well suited for Eq. 5
The work appears to miss relevant baselines like GNN Explainer[1] / CF-GNN Explainer [5]. Moreover the authors ideally should compare with baselines, which enrich GNN's with structural features [6][7][8] (As they are explainable to a certain extent as well). Minor:
The separability of a graph into two subgraphs the causal and non causal might not always be possible? (would this need an encoder which is able to accurately capture the discrete topology over graphs of all orders and sizes - if not I can just make a house into a clique and the label would be incorrectly assigned as 1 because the edges required for the house are present) Please elaborate on this.
The set {s} employed in the test are limited to the ones seen in train.
Moreover, consider a graph with say 20 nodes, and the case where the causal part for instance consists of a house motif and a tree base and the label assigned is 1 or 0 based on the output of House Motif XOR Tree Base - would this be captured by the proposed method (appears like it wont)? References:
1.Ying, Rex, et al. ""Gnn explainer: A tool for post-hoc explanation of graph neural networks."" arXiv preprint arXiv:1903.03894 (2019).
2.Srinivasan, Balasubramaniam, and Bruno Ribeiro. ""On the equivalence between positional node embeddings and structural graph representations."" arXiv preprint arXiv:1910.00452 (2019).
3.Dwivedi, Vijay Prakash, et al. ""Benchmarking graph neural networks."" arXiv preprint arXiv:2003.00982 (2020).
4.Chen, Zhengdao, et al. ""Can graph neural networks count substructures?."" arXiv preprint arXiv:2002.04025 (2020).
5.Lucic, Ana, et al. ""CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks."" arXiv preprint arXiv:2102.03322 (2021).
6.Bouritsas, Giorgos, et al. ""Improving graph neural network expressivity via subgraph isomorphism counting."" arXiv preprint arXiv:2006.09252 (2020).
7.Bodnar, Cristian, et al. ""Weisfeiler and lehman go topological: Message passing simplicial networks."" arXiv preprint arXiv:2103.03212 (2021).
8.Bodnar, Cristian, et al. ""Weisfeiler and lehman go cellular: Cw networks."" arXiv preprint arXiv:2106.12575 (2021).","5.Lucic, Ana, et al. ""CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks."" arXiv preprint arXiv:2102.03322 (2021).",301,1
ICLR_2022_1323,ICLR_2022,"Major
While numerous mathematical statements are made throughout the paper, some are already well-established in the reinforcement learning literature (the chain of statements ending Section 2.2 should all hold with equality and dates back to the successor representation[1]) and others are simply stated as fact without proof, although surrounding text would suggest that the authors are claiming them as technical contributions of this work. While the introduction of variational approximations leads to lower bounds that resemble those shown in the paper, the various notations and distributions introduced make it difficult to confirm their validity right away. The authors should (at a minimum) include proofs for novel technical contributions of this work and further include proofs for supporting results that may help the reader understand the nature of their contributions.
In Equations 9 and 10, (my understanding is that) the included log-likelihood ratio is between the variational approximation of the behavior policy visitation and that of the optimal policy. It seems rather likely that this ratio may suffer from numerical errors (namely, division by zero) when the exploratory behavior policy visits a region of the state-action space not covered by the optimal policy. Could the authors comment on why this is not an issue? Or are the authors making some kind of absolute continuity assumption?
Ultimately, this paper is deeply concerned with addressing the challenge of exploration. Can the authors comment on why this formulation under probabilistic inference is reasonable given recent work which shows how the approximations needed for these methods to be applied in practice fail to pass even basic sanity checks? [2] Minor Clarity Strengths
The clearest element in the paper is Algorithm 1, which at least conveys the generic structure of what the authors are trying to achieve. Unfortunately, everything leading up to Section 3 is incredibly unclear stemming from a variety of sources including non-standard mathematical notation, undefined jargon/terminology, and (to a lesser extent) grammatical errors. The authors have wasted a page describing details of widely-known Mujoco environments that could instead be allocated to adding clarity to Sections 2 and 3, which seem to articulate the bulk of the contribution. Weaknesses Major
The mathematical notation used throughout the paper is terribly difficult to parse and inconsistent with the broader reinforcement-learning literature. As a concrete example, Section 2.3 contains undefined notation (\mathcal{T}\pi(s)), ambiguous notation (s... notation, s+ appears to denote a state and yet is later said to denote a sequence of future states but then used as a normal state in Equation 3), and nonsensical notation (the second paragraph of Section 2.5 has Q(s,a) \sim \frac{r(s+,a+)}{(1-\gamma)}, which is entirely vacuous without being defined beforehand). The authors seem to jump back and forth between states indexed by time vs. not.
The authors make a bad habit of introducing assumptions unnecessarily. In Section 2.4, if the first assumption on Boltzmann policies is made, then why is the second assumption on the existence of K(s) needed as well? The statement should hold under the first assumption and K(s) can be written out explicitly. Why is the first assumption of Section 2.5 needed as if it re-defines the optimal policy? Unless the policy class over which the argmax is taken is a subset of all policies, this is the definition of the optimal policy for a MDP.
The authors introduce their own terminology without any formal definitions or visualizations to assist the reader in understanding what the terms are meant to convey. As a result, the first paragraph of Section 2.7 reads vacuously without knowing what is meant by ""pathways"", ""conducts"", ""adverse"", through"", or ""width.""
The paper has several grammatical errors throughout. While a small handful of these would not constitute a major weakness (and, in fact, could be easily enumerated and corrected), there are enough of them to impact readability of the paper. Minor
The use of the phrase ""probability matching"" in this paper is misleading since it is widely associated with Thompson sampling. Distribution matching would be a more accurate phrase for what the author is trying to convey. Originality Weaknesses Major
The paper lacks a section dedicated to providing readers with an overview of related prior work. Consequently, it is difficult to precisely identify how the authors' variational formulation extends or expands beyond some of the prior work mentioned in brief at the top of page 2. Minor Significance Weaknesses Major
The authors haven't included details of how many random seeds are used to generate the results in Figure 1; consequently, I'm skeptical of their significance and reproducibility. That said, just examining the average return column (which is the standard metric), it seems that the proposed approach only leads to meaningful performance improvements in simpler control problems, while results in Swimmer and Humanoid are only on par with SAC.
Simpler experiments in domains where things can be concisely and cleanly visualized would go a long way towards driving home the central claim that this approach facilitates improved exploration in practice. Minor References
Dayan, Peter. ""Improving generalization for temporal difference learning: The successor representation."" Neural Computation 5, no. 4 (1993): 613-624.
O'Donoghue, Brendan, Ian Osband, and Catalin Ionescu. ""Making Sense of Reinforcement Learning and Probabilistic Inference."" In International Conference on Learning Representations. 2019.","4 (1993): 613-624. O'Donoghue, Brendan, Ian Osband, and Catalin Ionescu. ""Making Sense of Reinforcement Learning and Probabilistic Inference."" In International Conference on Learning Representations. 2019.",302,1
ICLR_2022_70,ICLR_2022,"Why is the proposed method very fast compared to others? Although a summary of computing requirements is provided, it would be interesting to perform a deeper analysis as to why the proposed transformer-based method is faster than other transformer-based methods, smaller feature dimensions? The fewer number of blocks? This could be helpful to the community in the future to develop efficient and high-performance transformer-based prediction methods
The ablation experiment can be expanded. Currently, the only ablation experiment is done in Table 3 on TrajNet with two variations of removing social modeling. However, the TrajNet challenge is a bit outdated and data annotation is imprecise. Would be more convincing to perform the ablation on the nuScenes or Argoverse dataset. Also, there are other components of the proposed method that are not ablated. For example, how is the proposed transformer-based approach compared to RNN-based or Graph-based methods? How is the map affecting the performance? How are the seed parameters useful? How many time/social blocks are needed?
The novelty is a bit weak considering other works have similar transformer-based motion prediction methods, especially [A1] as mentioned in section 5. Even ignoring this “concurrent” work, other works including [A2, A3] also use stacked transformers to do both social and temporal modeling. It is incorrect to me that the paper claims [A2, A3] did not encode the time and social dimensions using transformers. It would be great if the differences with these works can be explained in a more detailed and precise way
The experiments on TrajNet and Omniglot are not very convincing, where no quantitative comparison is properly made with prior work. For example, on the TrajNet challenge, there are plenty of prior motion prediction methods that have been evaluated on this dataset, so it should be easy to have a table for comparison. For stroke completion, I am not very familiar with this dataset, but only comparing with a simple LSTM baseline is not convincing. Including experiments on more datasets is great, but would be good to show convincing experiments. References
[A1] AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. ICCV 2021
[A2] Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction. ECCV 2020
[A3] Multimodal Motion Prediction with Stacked Transformers. CVPR 2021
Post-rebuttal review
After carefully reading other reviews and the authors’ comments, I would like to increase my rating to a score of 8 -- accept good paper (or a score of 7 if there is such an option). The change to my score is mainly because I am satisfied with most of the authors’ comments to my concerns, which include: 1) the clarification on why the proposed method is much faster; 2) the expanded ablation experiments; 4) the clarification for experiments on Omnlglot. I strongly appreciate the authors' efforts on clarifying these questions and concerns!
As a result, the main comment left on my side is 3) the limited novelty compared to prior work. This does not mean that I find the authors’ reply unconvincing. In fact, it is quite clear and honest. But essentially, the main difference to prior work seems to be just the use of seed parameters that increases performance and also runtime speed, since other points are already proposed in prior work, including joint social-temporal modeling in encoders/decoders using transformers, and scene-consistent predictions. But I would like to emphasize that this relatively incremental novelty is fine to me, and personally, I would like to have a try on the seeing parameters proposed in this work. So it would be wonderful for the community if the code can be carefully documented and released (currently I did not find it in the supp or any anonymous URL). Finally, as a minor point, it would be great if the map or context image can be overlaid on the trajectories for the provided video results.
In short, if there is no other significant downside/concern pointed by other reviewers, I would like to stick to my rating above and recommend accepting this paper","4) the clarification for experiments on Omnlglot. I strongly appreciate the authors' efforts on clarifying these questions and concerns! As a result, the main comment left on my side is",303,1
ICLR_2022_2856,ICLR_2022,"1. The abstract should be re-worked.
There are too many acronyms in the abstract. The authors even do not define all acronyms. For example, I had no idea of the meaning of VIPS if I only read the abstract. The authors should improve the abstract.
2. Sec 1 should be improved.
It is difficult for readers to understand Sec 1 since the authors list too many methods without properly describing them. Readers cannot tell the difference between VON and VOGN/GM since the authors do not define VON.
VON is originally proposed in [1]. Note that VON uses an unbiased second-order estimator while VOGN and GM use biased first-order estimators for the covariant matrix.
3. Positive-definite constraint in eq 4 (the key technical issue)
The authors should discuss why the covariance matrix in Eq 4 is guaranteed to be positive-definite. To update the covariance matrix, the R matrix is used according to Eq4. As mentioned in Sec 2.1.1, the R matrix is just symmetric and is updated according to Eq 6.
It is unclear why the updated covariance matrix in Eq 4 is positive-definite due to the unclear presentation. The authors should discuss this point since the proposed method also suffers from the same issue.
VON has to use a line search algorithm to select a step-size to handle this issue as mentioned in [1]. However, this line search could lead to a very small step-size and greatly slow down the optimization procedure as reported in [5]. It is likely that MORE and VIPS also have to select a small step-size to handle this issue. GM and VOGN are proposed to address this positive-definite issue. However, GM and VOGN are not unbiased estimators.
4. Missing some important related works
Tran et al 2019 [2] consider this positive-definite issue and propose to use a retraction map originally designed for positive-definite matrices.
Salimbeni et al 2018 [3] suggest using a Cholesky factor instead of the covariance matrix to handle this issue and propose a natural-gradient method.
Lin et al 2020 [5] also consider this issue in more general settings. Lin et al 2020 show that the retraction map for positive-definite matrices can be obtained from the geometry of a Gaussian distribution in a systematic way.
Moreover, Lin et al 2020 [5] show that an unbiased first-order estimator for the covariance matrix can be used to replace the second-order estimator in VON. This unbiased first-order estimator is known as the re-parametrizable gradient for the full covariance matrix (see Theorem 4 in [4]). In other words, the method in [5], which is an extension of VON, can use unbiased first-order information instead of second-order information to update the covariance matrix. Thus, [5] is related to gMORE.
VON can not be directly applied to Gaussian mixture cases since the Fisher information matrix of the Gaussian mixture is not well-defined. Lin et al 2019 [6] extend VON to Gaussian mixtures and establish the connection to natural gradient descent. Lin et al 2020 [5] address the positive-definite issue in Gaussian mixture cases and propose an unbiased first-order estimator in Gaussian mixture cases. Thus, [5] and [6] are related to gVIPS and vonVIPS.
Last but not least, Lin et al 2021 [7] propose natural-gradient methods for a flexible class of structured Gaussians and structured Gaussian mixtures beyond the low-rank-plus-diagonal covariance structures. Lin et al 2021 also unify some zero-order, first-order, second-order methods in evolution strategies/gradient-free search, variational inference, and numerical optimization.
5. No justification of the main contribution
Readers wonder how the authors come out with this loss function defined in Sec 3.2. The authors should justify this loss function and tell readers how the proposed method relates to natural gradient descent.
6. Unbiasedness of the proposed method
The authors mention that MORE is unbiased in Sec 1. However, it is unclear whether the proposed method is unbiased or not since the proposed method is clearly different from MORE. I wonder whether MORE and VIPS are unbiased when the ridge regression is used as mentioned in Sec 3.1 and Sec 3.2.
7. Additional tuning parameter for the ridge regression
In Sec 3.2, an additional tuning parameter for the ridge regression is included. The authors should discuss how to select this parameter since the selection can affect the performance of the proposed method in practice.
8. Unfair comparison for VON (the key empirical weakness)
The authors consider only experiments from the VIPS paper, where the tuning parameter for the ridge regression and the step-size could be well-tuned. On the other hand, readers have no idea whether the step-size for VON is fairly selected. The authors should also consider other experiments.
No VON-type method is considered in the GMM experiments. The method in [5] is an extension of VON and is suitable for Gaussian mixtures. Moreover, Lin et al 2020 [5] consider fitting a 300-dimensional mixture of Student's T distributions with 20 components, which is more challenging than the 100-dimensional Target GMM with 10 components considered in this work.
The authors should include the method [5] in the GMM experiments as a baseline.
As shown in [5], the positive-definite issue can greatly slow down the training process in the example of mixtures of Student's T distributions. The authors should consider fitting a mixture of Student's T distributions using the proposed method and discuss whether the positive-definite issue slows down the training process of the proposed method (e.g., using a very small step-size due to this issue).
9. VON does not appear in Figure 4
VON is mentioned in the caption of Figure 4 while VON does not appear in Figure 4. Please note that VON and vogVIPS are different methods. As mentioned before, VON is designed only for Gaussians while VIPS is designed for Gaussian mixtures. References:
[1] Khan, Mohammad, and Wu Lin. ""Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models."" Artificial Intelligence and Statistics. 2017.
[2] Tran, Minh-Ngoc, Dang H. Nguyen, and Duy Nguyen. ""Variational Bayes on manifolds."" arXiv preprint arXiv:1908.03097 (2019).
[3] Salimbeni, Hugh, Stefanos Eleftheriadis, and James Hensman. ""Natural gradients in practice: Non-conjugate variational inference in Gaussian process models."" International Conference on Artificial Intelligence and Statistics. PMLR, 2018.
[4] Lin, Wu, Mohammad Emtiyaz Khan, and Mark Schmidt. ""Stein's Lemma for the Reparameterization Trick with Exponential Family Mixtures."" arXiv preprint arXiv:1910.13398 (2019).
[5] Lin, Wu, Mark Schmidt, and Mohammad Emtiyaz Khan. ""Handling the positive-definite constraint in the bayesian learning rule."" ICML 2020
[6] Lin, Wu, Mohammad Emtiyaz Khan, and Mark Schmidt. ""Fast and simple natural-gradient variational inference with mixture of exponential-family approximations."" ICML 2019.
[7] Lin, Wu, et al. ""Tractable structured natural gradient descent using local parameterizations."" ICML 2021",2. Sec 1 should be improved. It is difficult for readers to understand Sec 1 since the authors list too many methods without properly describing them. Readers cannot tell the difference between VON and VOGN/GM since the authors do not define VON. VON is originally proposed in [1]. Note that VON uses an unbiased second-order estimator while VOGN and GM use biased first-order estimators for the covariant matrix.,304,0
ICLR_2022_2929,ICLR_2022,"I feel this paper owes readers a more in-depth explanation of the intuition behind this RL + ZO pipeline. Training an RL agent requires possibly many samples, and it is not very clear to me, at least when I read the paper for the first time, why this is more efficient than directly using these samples to run vanilla ZOO. It seems that the idea is to feed RL agents with random samples (which should have roughly the same time complexity as the vanilla ZOO) and hope the actor network can regress the mapping from x to its gradients. To me, it is only in the later stage (after the RL agent learns to behave less randomly) that the proposed method starts to save time and samples compared with vanilla ZOO methods. Having one paragraph explaining the intuition more clearly before or after Alg. 2 would be ideal.
The hard switch from rho < eps to rho >= eps in Alg. 2 is questionable to me. The whole algorithm reads like a two-stage algorithm, where the first stage (rho < eps) is dedicated to collecting true gradients and regressing, then suddenly it switches to the second stage (rho >= eps) and does not collect true gradients anymore. I do not see how the actor network can improve itself as a true-gradient estimator in the second stage since it will no longer see new supervision. Why not use the standard epsilon-greedy algorithm?
The choice of network structures (CNNs) seems not very well-motivated. I am not sure using CNNs in this approach for inputs that are not images or videos is a good idea, unless this paper is dedicated to minimizing objectives on images/videos.
I think the experiment section needs quite a lot of improvement:
I was expecting to see results from more challenging problems/datasets instead of MNIST. Even on MNIST, the improvement seems marginal to me in some cases (Fig. 4). Given that Alg. 2 is much harder to implement and tune than a vanilla ZO-ES, I would expect a substantial performance boost to justify the value of the proposed approach.
There is not enough information for people to replicate the results: the dataset link at the end of page 8 seems broken, and information about the hyperparameters used in DDPG and Alg. 2 is sparse.
Epsilon seems to be a crucial hyperparameter in Alg. 2, and an ablation study should be provided.
It looks like the actor did not really learn to predict a good gradient direction, as can be seen from the cosine angle in Fig. 6. Therefore, it is hard to directly attribute the performance boost (if there is any) to the cleverer choice of gradient samples as motivated at the beginning of the paper. Some ablation studies on each component in Alg. 2 should be provided to demystify the source of efficiency (if there is any).
I would also suggest an experiment, or at least a discussion, on the generalizability of the RL agent: from what I understand, whenever there is a change to the loss function f, the RL agent needs to be re-trained. Although this issue is not uncommon in RL problems, it is still desirable to have some theoretical justification or experimental proof on whether/how the trained RL agent can be generalized to a new optimization problem, e.g., by warm-starting Alg. 2 when solving a slightly modified f, or by some meta-learning on a family of {f} instead of a single instance.
Some minor, less technical comments:
Last line on page 1: I believe 1/mu is missing in “lim f(x+mu * u) - f(x)” if you want to claim it is “exactly calculating the directional derivative”.
Fig. 1, 2, and 3 do not seem to be very informative compared with the space they occupy. In particular, these three figures seem to present the same idea multiple times (at least to some extent).
The review of DDPG (Sec. 3.1) and batch normalization (Sec. 3.2) feel redundant and can be largely shrunk or completely removed. I think it is pretty safe to assume ICLR paper readers are familiar with these concepts.
The margin above and below Sec. 4 title seems unusually small.
Fig. 4, 5, and 6: It is probably better to have (semi-)transparent curves instead of solid curves that occlude each other.
The first baseline is named “ZO-GS” in the main text but “ZO-SGD” in Fig. 4, 5, and 6. Also, I thought ZO-GS should predict a pretty accurate gradient direction, albeit with possibly much more samples. From Fig. 6, it seems that ZO-GS failed to estimate the gradient direction correctly. Why?","2 should be provided to demystify the source of efficiency (if there is any). I would also suggest an experiment, or at least a discussion, on the generalizability of the RL agent: from what I understand, whenever there is a change to the loss function f, the RL agent needs to be re-trained. Although this issue is not uncommon in RL problems, it is still desirable to have some theoretical justification or experimental proof on whether/how the trained RL agent can be generalized to a new optimization problem, e.g., by warm-starting Alg.",305,0
ICLR_2022_2324,ICLR_2022,"Adding convolution into self-attention and capturing phrase information have been well studied before;
The proposed dynamic masking strategy has flaws;
Writing needs improvement;
Experimental details are not always clear;
Ablation study is incomplete and comparison should be enhanced.
Details: 1) Adding convolutional models and capturing phrase information for self-attention has been well explored in the context of machine translation. Take the following two papers as an example: [1] Yang et al., Convolutional Self-Attention Networks [2] Hao et al., Multi-Granularity Self-Attention for Neural Machine Translation These existing studies reduce the novelty of this paper. Also, a direct comparison with these studies is required. 2) The masking strategy is essentially another attention layer, but its formulation indicates that no gradient will be back-propagated into W^{Q_1} and W^{K_1} as shown by Eq. 10, 11, 12, 13. The authors don’t explain how to optimize them. 3) Writing needs improvement. In particularly, logistic should be improved. 4) How did you implement your convolutional model? Did you apply a vanilla convolution or depth separable convolution, or dynamic convolution? 5) The proposed model is used for fine-tuning after BERT encoding. One noticeable thing is that, although BERT outputs representation for each word, its encoding is fully contextualized. To large extent, BERT encoding contains sentence-level information rather than simply word-level. The authors need more stronger motivation to tell the necessity of convolution in self-attention. 6) One ablation is missing. What if you stack another vanilla attention layer for finetuning without convolution? This should be added for comparison.","1) Adding convolutional models and capturing phrase information for self-attention has been well explored in the context of machine translation. Take the following two papers as an example: [1] Yang et al., Convolutional Self-Attention Networks [2] Hao et al., Multi-Granularity Self-Attention for Neural Machine Translation These existing studies reduce the novelty of this paper. Also, a direct comparison with these studies is required.",306,0
ICLR_2022_2096,ICLR_2022,"1)More details about the existing attack algorithms after adapted and how the algorithms work on the discrete-time raster could be explained. 2)Some explanations about the meaning of the variable symbols such as ς ,r_i ,α_i and so on in the appended algorithms could be added. 3)More attack algorithms such as PGD and probabilistic PGD could be used on neuromorphic chip and calculate success rate in order to compare with the algorithm Sparsefool.",1)More details about the existing attack algorithms after adapted and how the algorithms work on the discrete-time raster could be explained.,307,0
ICLR_2022_831,ICLR_2022,"Theorem 1 does not say anything about stability of TRADES: Although Theorem 1 strengthens the claim made by the authors explaining why PGD-AT has a greater gradient instability than standard training, it does not say anything on how TRADES can overcome it. As far as I understand it, a similar result might be true for the TRADES regularization term. This slightly weakens the theoretical arguments of the authors.
Connection between robust overfitting and label noise could be more compelling: As I said, I appreciate the bold claim made by the authors to explain the phenomenon of robust overfitting, and its connections to memorization in AT. However, I still believe some of their arguments are a bit speculative and could be better supported by the data. In particular, the fact that models share the ranking of difficulty of different examples is, in my opinion, a rather indirect way to measure the presence of label noise in a dataset. Something that would make the claim of the authors more convincing is to show that the ""hard"" instances they identified in the dataset do correspond to incorrectly classified or very noisy samples.
Too many details off-loaded to appendix: In my opinion, the authors postpone too many important discussions to the appendix which hurts the readability of the paper. Specifically, I believe that Fig. A8 and Appendix A2 should belong to the main body of text.
Other comments
Fine-tuning experiments: I am of the opinion that the paper, in its current form, deserves to be accepted to the conference. However, I still believe there are certain experiments which, if included in the paper, would greatly strengthen the relevance of this work, and could lead me to increase my score. Specifically, there are two important works in the literature which study the effect of pre-training with random labels in performance (Liu et al. 2020, Maennel et al. 2020) and whose transferrability to the adversarial setting is currently unknown. Considering the fact that some of the experiments in this work point towards the importance of the early stages in training in the memorization behavior of AT, I wonder how different pre-training schemes with clean and random labels affect the findings of the authors. Namely, does temporal ensembling work better when the network is standardly trained on clean labels? Do neural networks robustly generalize when adversarially pretrained on random labels? Do they learn meaningful representations? I truly believe that answering any of these questions would be of great significance to the community, and therefore would make this work much more relevant.
Shengchao Liu, Dimitris Papailiopoulos, Dimitris Achlioptas. ""Bad Global Minima Exist and SGD Can Reach Them."" NeurIPS 2020
Hartmut Maennel, Ibrahim Alabdulmohsin, Ilya Tolstikhin, Robert J. N. Baldock, Olivier Bousquet, Sylvain Gelly, Daniel Keysers. ""What Do Neural Networks Learn When Trained With Random Labels?"". NeurIPS 2020
Inconsistent x-axis in Fig. 4: Fig.4a and Fig. 4b are technically showing complementary views of the instability of gradients in the PGD loss landscape. However, the x-axis in this plot is not consistent. One of these plots shows the change of gradient around a specific weight location, and the other the change of gradient during training. This looks slightly suspicious (although I feel it is minor) and I would encourage the authors to either use the same metric in both plots, or show the analogous result for the other x-axis in the appendix.","4: Fig.4a and Fig. 4b are technically showing complementary views of the instability of gradients in the PGD loss landscape. However, the x-axis in this plot is not consistent. One of these plots shows the change of gradient around a specific weight location, and the other the change of gradient during training. This looks slightly suspicious (although I feel it is minor) and I would encourage the authors to either use the same metric in both plots, or show the analogous result for the other x-axis in the appendix.",308,0
ICLR_2022_1794,ICLR_2022,"1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.
2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.
3 On the segmentation mask involved with cancer on CSAW-S, the segmentation results of DEEPLAB3-DEIT-S cannot be concluded as better than DEEPLAB3-RESNET50. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance.
Questions: 1 For the grid search of learning rate, is it done on the validation set?
Minor problems: 1 The n number for Camelyon dataset in Table 1 is not consistent with the descriptions in the text in Page 4.","1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.",309,0
ICLR_2022_2810,ICLR_2022,"The clarity of the writing could be improved substantially. Descriptions are often vague, which makes the technical details harder to understand. I think it's fine to give high-level intuitions separate from low-level details, but the current writing invites confusion. For example, at the start of Section 3, the references to buffers and clusters are vague. The text refers readers to where these concepts are described, but the high-level description doesn't really give a clear picture, making the text that follows harder to understand.
Ideas are not always presented clearly. For example:
may only exploit a small part of it, making most of the goals pointless.```
- Along the same lines, at the start of the Experiments section, when reading ```the ability of DisTop to select skills to learn``` I am left to wonder what this ""ability"" and ""selection"" refers to. This is not a criticism of word choice. The issue is that the previous section did not set up these ideas.
- Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.
- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.
- The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better.
### Questions/Comments:
- The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.
- The same applies to 2.2. The technical details are hard to follow.
- You claim ""In consequence, we avoid using a hand engineered environment-specific scheduling"" on page 4. Does this suggest that the $\beta$ parameter and the $\omega'$ update rate are environment independent?
- Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?
- It is somewhat strange phrasing to describe Skew-Fit as having ""favorite"" environments (page 6).",- The same applies to 2.2. The technical details are hard to follow.,310,0
ICLR_2022_1703,ICLR_2022,"Scalability to practical settings: It is not clear whether the proposed method scales for contemporary architectures and large-scale datasets. Since the low-rank structure of the GGN has been investigated before by others (as the authors also pointed out), the main contribution of this paper is the compute/memory efficiency and scalability of the proposed tool. However, the experiments are performed on CIFAR-10 with a small, 6-layer neural network and it is not clear how it would scale to architectures such as ResNet and datasets such as ImageNet. In case the method does not work well in these scenarios, the applicability of the tool is limited.
Overhead: In-depth study on the compute and memory overhead of the method would be valuable. For example statements such as 'However, the practical overhead is expected to be smaller' should be backed by concrete experiments. Even though the memory requirements for different accuracy settings are shown, it would be useful to see how it translates to wall-clock time overhead.
Accuracy of the approximation: It is clear that an approximation of V is necessary to make the tool feasible in practical settings. However, the paper doesn't investigate the accuracy of the approximate spectra obtained from the method. Authors claim that the main contribution of the paper is delivering directional derivatives and GGN curvatures, but the accuracy of those quantities is not verified in any way in the paper. Providing some guarantees on the accuracy would be crucial, especially since the method is proposed as an analysis tool. For instance, taking a look at Figure 1 shows that the exact GGN and MC approximate spectra show similar characteristics, but how large is the difference in the top eigenspaces? Small errors in top eigendirections can greatly impact downstream applications where the quantities are used. An in-depth quantitative analysis of the cost savings-accuracy trade-off would be essential. Furthermore, I am concerned about the accuracy of the method when applied to datasets with larger number of classes. For instance, the CIFAR-100 results in Figure S.5 show that the MC approximate spectrum looks very different from the exact spectrum, even qualitatively. This can be a serious issue when scaling the method to datasets such as ImageNet.
Damping: The directional damping method is interesting and is a neat application for the proposed tool, but 1) if it is meant only as a proof-of-concept, the discussion takes up a very significant portion of the paper (almost 2 pages) that could be used for supporting the main claims of the paper or 2) if it is meant as a main contribution and a practical method, then much more in-depth studies are needed to establish whether it improves damping in second-order methods.
Minor: What does K denote exactly (first appearing in 4.1)? Is K=NC?","2) if it is meant as a main contribution and a practical method, then much more in-depth studies are needed to establish whether it improves damping in second-order methods. Minor: What does K denote exactly (first appearing in 4.1)? Is K=NC?",311,0
ICLR_2022_985,ICLR_2022,"• More motivation and derivations of the different Fisher scores U x
(for GANs, VAEs, supervised) would be beneficial for understanding better the paper.
• More discussions on the non-diagonal version of the Fisher information matrix models (see comments below) would be beneficial.
• Discussion on the dependence of the quality of the NFK embedding on the quality of the pre-trained neural network.
• Discussion on the choice of the low-dimensional embedding dimensionality k
General comments:
• Using the diagonal of the Fisher information matrix (FIM) seems desirable from a computational reason, however a natural question is what happens if one tries to use the full matrix. Given the size of the parameters θ
in a neural network, estimating the whole matrix would indeed be extremely computationally expensive, but by discarding them, one loses significant information. Could the authors comment on that? Is the diag of FIM related to other known concepts in statistics? Does using only the diagonal imply that the off-diagonal elements are zero meaning the parameters are orthogonal? How does this affect the results and interpretation?
• Could the authors give the derivation of U x
in eq (1) for GANs (I see part of the derivation is in the paper by Zhai et al, 2019)? For VAEs and supervised case, FIM I
is the inner product using U x
, but for GANs it acts in the output space of the generator. Why is this the case (some explanations are given in the original paper but would be helpful to discuss this a bit more here)? The derivation of eq (4) would also be useful.
• Low-rank structure of NFK and Alg 1: How does one choose the feature dimensionality k
? Many methods that rely on kernels and manifold learning make the assumption of low-dimensionality/low-rankness and show that a small number of eigenfunctions is sufficient to reconstruct the input data. How is this different for NFK? The way I understand “low rank” is that the data has its own rank, which is low, and could potentially be learned. However here the authors input the dimensionality/rank k
which might be or not close to the true rank in real applications.
• How does this work relate to the work of Belkin et al (2018) – “To understand deep learning we need to understand kernel learning”, where the authors look at other kernels (Laplacian and Gaussian)?
• Could the approach be used for neural networks that are not pre-trained, as the neural tangent kernel NTK?
• The experimental results are nice, however the focus on computation is not so relevant given that it only uses the diagonal of the Fisher information matrix. Comparisons using the whole matrix would also be needed. What error is used in Table 3 (MSE, MAE, RMSE)? The goal of the paper is to present a method for supervised and unsupervised settings, however in the results an example on semi-supervised is also presented. I wonder if the examples on semi-supervised and knowledge distillation could leave room to improve the supervised and unsupervised settings discussions, and potentially be moved to the Appendix?
Other comments:
• Please update reference (Jaakkola et al): year, conference, also there should be no “et al” there are only two authors
• Doesn’t, don’t, won’t, it’s , etc -> does not, do not, would not, it is
• Both the concepts of “data representation” and “feature representation” are used. Do they always refer to the same thing? If yes, would be good to specify that.
• Expression of K f i s h e r
=> second U
should be subscript z not x ?
• “FIM defined as the variance of the score …” -> the FIM matrix is defined between all pairs of parameters θ i and θ j
, so it should be a covariance?
• Appendix Fig 3: Not sure I fully understand this example. Could one try the reconstruction of the digits using a simple method, such as PCA using the first 100 principal components as a baseline?
• Not familiar with the “Fisher vector” terminology, except in image classification and the “Adversarial Fisher vector” from Zhai et al, 2019. Are there other references?",• More discussions on the non-diagonal version of the Fisher information matrix models (see comments below) would be beneficial.,312,0
ICLR_2022_354,ICLR_2022,"The authors claim there are no studies beyond network classification and a lack of theoretical support for the available methods. In my opinion, the claim is not correct. Many papers include different graph-level tasks in their experiments and several of them provide also theoretical support about the proposed architecture; I can refer the authors to some of them below. The authors also claim ""the efficacy of these methods [on graph matching] in learning from network-valued data remains unexplored"". The authors should be more precise about what remains unexplored. Nevertheless, I may agree that extra studies and theoretical analyses can positively contribute to the research in this field, and I recognize the value of considering the small sample size setting. https://ogb.stanford.edu/
Dwivedi, Vijay Prakash, et al. 2020. Benchmarking graph neural networks
Bouritsas, Giorgos, et al. 2020. Improving graph neural network expressivity via subgraph isomorphism counting.
Zambon, D., Alippi, C., & Livi, L. 2020. Graph Random Neural Features for Distance-Preserving Graph Representations.
Sato, Ryoma. 2020. A survey on the expressive power of graph neural networks.
Maron, Haggai, Heli Ben-Hamu, and Yaron Lipman. 2019. Open problems: Approximation power of invariant graph networks.
Loukas, Andreas. 2019. What graph neural networks cannot learn: depth vs width.
Kriege, Nils M., et al. 2018. A Property Testing Framework for the Theoretical Expressivity of Graph Kernels.
Chen, Hao, and Jerome H. Friedman. 2017. A new graph-based two-sample test for multivariate and object data.
The graph transformation of G to G^\sigma seems to be ill-defined. Although the graphon can have unique node degrees, graphs sampled from it can result in nodes with the same degree, so 1) there might not exist a monotonically increasing permutation but only non-decreasing, and 2) the permutation is not unique, in general.
The claim ""NCLM is the only known clustering strategy for graphs of different sizes"" is incorrect. Basically, any kernel-based and distance-based clustering method that does not require explicit vector embeddings can be applied to graphs. It is customary to choose the most appropriate distance or kernel that better fits the problem at hand; a simple example is k-means. The claims seem unjustified to me also because, as far as I can tell, NCLM constructs vector representations of graphs.
The methods chosen for the comparison can give only limited insights. The considered methods appear to be more or less arbitrarily chosen combinations of graph functions with clustering methods. For example, this approach does not allow us to understand whether it is the proposed distance or the considering clustering method that brings the most advantages. I might be wrong, but it seems to me that there is no limitation in performing experiments with the proposed graphon-based distance with other clustering methods, and the considered clustering methods with other distances/kernels.
Regarding the two-sample hypothesis test, it is unclear how a critical region of a given significance level can be constructed without having knowledge about the underlying graphons. From what I see, in the paper, the critical region is computed by sampling from the original graphons. I would to have some clarifications in this respect.
Questions and unclear parts:
The sorting-and-smoothing estimator is mentioned even in the abstract, but never introduced in the paper. Why is it the only one that can meet the above requirements? Moreover, the above requirements (I guess the authors refer to Assumptions 1-3) are about the graphons, not their estimators.
Theorem 1. It is counterintuitive for me that for a bounded number of nodes, the more graphs we have, the larger the rate of erroneous clustered graph increases. I would expect that for m -> infinity, the rate of misclustered graphs converges to the optimal value (which is not necessarily zero), and the variance of such observed rate decreases. Could you comment on that?
It is not clear to me to what extent the discussed clustering methods had to be adapted to work with the proposed distance. It is also unclear whether or not these clustering methods would be consistent regardless of the specific graph distance, provided that the graph distance is sufficiently expressive (eg, it is metric).
Because the considered method is based on edge histograms, I wonder if there are known limitations or important differences when considering sparse graphs instead of denser ones. Are there different best practices to operate in the two regimes?
Other comments and suggestions:
Say explicitly that graphs are symmetric and without node or edge attributes (if this is the case).
Expand the comment about indistinguishable inhomogeneous random graph models. What is the argument to show that although different, they are indistinguishable? What are we losing by ignoring such graphs, in what cases are we interested in considering also them?
Prop 1. Please, expand the discussion about the variable n which seems to be undefined.
Cor 1. Shouldn't it be: given m, n_0 and w-w' , we need to find a small enough constant C?","1) there might not exist a monotonically increasing permutation but only non-decreasing, and",313,0
ICLR_2022_1400,ICLR_2022,"/questions In general, I think the results should be treated more carefully and some theoretical motivation is needed. Following are some of my questions. 1.in section 4.1, the conclusion of ""This result shows that no additional exploration mechanism, often in a form of an exploration noise (Lillicrap et al., 2016; Fujimoto et al., 2018; Wang et al., 2020), is required for the diverse data collection and it can even hinder training."" seems rather strong judging from fig.2 (I would say they are roughly the same). 2. ""Figure 3 shows that ED2 magnifies the beneficial effect coming from the deterministic exploration."", wouldn't it make better sense if you compare against the ones that are also ensemble, e.g. ensemble SOP instead of SOP? How do you know what brings the performance, is it the ensemble or the determinist action, or both? 3. fig.4 again, why not compare the ensemble baselines? I think it's important to ablate the design choices that are actually important
Some minor problems.
maybe more introduction for SOP (e.g. what's the ere replay buffer?) 2.sec.3, ""These two choices ensure coherent and temporally-extended exploration similarly to Osband et al. (2016)."", I do not understand why is the exploration ""coherent and temporally-extended"". 3.in sec.3, I guess the used and not used should be comparable and matched, but why is action normalization compared with ""observations and rewards normalization""? 4.fig.5, what does it mean for the Humanoid velocities? could you elaborate?","3. fig.4 again, why not compare the ensemble baselines? I think it's important to ablate the design choices that are actually important Some minor problems. maybe more introduction for SOP (e.g. what's the ere replay buffer?) 2.sec.3, ""These two choices ensure coherent and temporally-extended exploration similarly to Osband et al. (2016)."", I do not understand why is the exploration ""coherent and temporally-extended"".",314,0
ICLR_2022_1119,ICLR_2022,", starting from the most significant ones.
Assumptions and Threat Model? This is probably the only “true” problem of the paper, which should be absolutely rectified. I was not fully able to understand the assumptions made by Tesseract. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? Or does it also protect against different attacks? In general, Section 2.2, Threat Model, is not very comprehensive. The authors should better expand this section by clearly pointing out all the assumptions and requirements of the proposed method. This is especially true because the Fang et al. attack was proposed in 2020, and some of its assumptions are not yet well-known. Specifically, this statement is suspicious: “We assume a full-knowledge (white-box) attack where the attackers have access to the current benign gradients.”. Does it mean that Tesseract only works under this assumption? I.e., the attacker knows, and exploits, the current benign gradients? This is a rather “unrealistic” assumption: I understand the willingness to work against “worst case” scenarios; yet, if such “worst case” scenarios are not realistic in the first place, then what is the purpose of the proposed mechanism? What benefit is there in protecting against an attack that will never happen in the first place? I invite the authors to restructure this section by using the common taxonomies adopted in adversarial ML papers [I].
Problem or Feature Space attacks? The authors perform their experiments on four well-known datasets: MNIST, CIFAR, Shakespeare, FEMNIST; for each dataset, a different (deep) ML model is targeted. Three of these datasets are of images, whereas Shakespeare contains text data. There are different ways to create “adversarial examples”, depending on the ‘space’ where the perturbation is applied. As far as I am aware, the adversarial examples considered in this paper to perform the poisoned updates are created in the feature space. It would be a lot more interesting if at least one evaluation included adversarial examples generated in the “problem” space [A]—or, at the very least, considered samples generated by “physically realizable” adversarial perturbations [B]. I acknowledge that the method should work even in these circumstances, as the proposed Tesseract defense is agnostic of the process used to apply the perturbation. However, considering the strong relationship with (real) security that permeates the paper, I believe that a more convincing use-case would dramatically improve the quality of the paper. This is also motivated by the current state-of-the-art: after almost a decade of adversarial attacks, more recent efforts are leaning towards evaluation that consider more realistic circumstances, where the attacker is constrained by the limitations of the real world; this is even more true in “distributed system” scenarios, such as Network Intrusion Detection Systems, which bear a strong relationship with federated learning (e.g., [C, D, E, F]). As such, I invite the authors to perform an additional “proof-of-concept” experiment where they consider adversaries with constrained capabilities. This is also motivated by the fact that some perturbations may yield different effects when created in the problem space (as shown in [A]).
Tradeoff? A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients. Even if the baseline performance does not decrease, what is the overhead of the proposed method? For instance, in Table 2 the authors report some results for “Attack=None”, which I assume represent the accuracy when no attack takes place. However, all the rows of these experiments (namely, FedSGD, Tesseract, Faba, FoolsGold, FLTrust) consider hardening FL techniques; for instance, on MNIST the proposed Tesseract has an accuracy of 92.52 when no attack takes place—the best among all other defences. Despite being appreciable, I am interested in knowing the performance when NO defense is applied. Surely, the test accuracy in a “fully trusted” FL setting should be superior than 92.52. Hence, I ask: what is the ‘cost’ of Tesseract?
Lack of a concrete use-case. I believe that the paper could be further improved with a concrete use-case, where the authors explain, step-by-step, how a (single, or multiple) attacker can compromise a federated learning system, and how the proposed method can help in solving such problem. Hence, I request the description of a concrete use-case explaining the abstract scenario reported in Figure 1. Such use-case can be at the basis of the “constrained” attack that I invite the authors to perform in my ""problem space perturbations"" suggestion.
Some additional issues:
• In the Introduction, the authors state: “To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL…”. I believe that “Byzantine Clients” is a wrong term: what is countered by Tesseract are not byzantine clients, but ""unloyal"" clients, that are “against” the byzantine clients (at least by referring to the well-known problem of the byzantine generals, which should agree on a method to reach consensus in the presence of unloyal generals).
• The caption of Figure 1 has a typo “c out of m clients maybe be malicious”.
• In Figure, the gradient “LM_{c-1}” is out of place.
• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?
• Figure 1 appears before Figure 2, but in the text it is referenced after Figure 2.
• Putting Figure 2 so early on is very confusing. The “flip score” is a measure introduced for the first time in this paper. As such, any reader would be thrown off by such graphs before reading the paper, meaning that the findings of Figure 2 are difficult to interpret---during the Introduction---, as the flip score has not been defined yet. As such, such graphs are ultimately meaningless: I have to trust the authors that they correspond to “interesting” observations and “fair” experiments, which is not scientific.
• The presentation and notation in the “Flip-score” (page 5) is very ugly and difficult to follow.
• Section 5 should be merged in Section 6
• W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.
• W.r.t. Table 2, the authors state “We have not shown the test loss curve for Krum aggregation because of the large loss values.”. I invite the authors to report such values in Table 2, because the different “formats” of the three subtables (None, Full-Krum, Full-Trim) make this table very hard to interpret.
EXTERNAL REFERENCES
[A]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[B]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Conaml: Constrained adversarial machine learning for cyber-physical systems."" Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021.
[F]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.
[I]: ""Wild patterns: Ten years after the rise of adversarial machine learning."" Pattern Recognition 84 (2018): 317-331.","• Section 5 should be merged in Section 6 • W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.",315,0
ICLR_2022_3169,ICLR_2022,"Weakness **
The graph assembling design may cause misleading on performance:
The post-processing assembling in many previous works [1,2,3] adopt the minimum matching cost between detected objects and interaction predictions, rather than top-k. All of them tend to avoid too much predicates in one pair by using this optimal matching.
Generally, the performance of SGG with multiple predicates in one triplet are much better than that with single predicate in one triplet. [6]
The graph assembling in this paper allocates K
subjects and objects to each predicate. Then, the authors take K ⋅ N r
triplets for evaluation. So it actually breaks the rule that limits the number of predicates in one pair to a low level. The multiple predicates definitely lead to higher performance than any other method.
In a nutshell, it is necessary to use the optimal matching in assembling like HOTR [2] for comparison with these methods. At least this will prove the gain of performance is from the new matching cost, instead of the top-k trick.
The authors do not discuss how their work outperform other methods in detail. Why is the performance quite higher on mR but lower on R relative to other model such as BGNN [5]? Some other detailed analysis should also be done. For instance, visualization, analysis on visual relationship detection quality, etc.
More ablation studies should be done. The authors show the enough complicated triplet matching cost in the supplementary, especially the modified GIOU cost and predicate cost. So why devise such a complicated cost? The matching cost selected in the previous transformer-based works are relatively simple [2,3]. How performance varies if the matching cost is changed into the plain format? Some ablation studies should be considered.
The lack of comparison. A lot of baselines are only compared on X101-F but not on R101 backbone in Tab. 1. Some classic and powerful methods such as Motifs [6] and VCTree [7] are not contained in Tab. 1.
Some reference links are not properly displayed in the supplementary. Many citations in the supplementary are in form of question mark in this PDF.
Some description are not easy to understand. In the supplementary, the meaning of this sentence is not clear: ""To circumvent this, we relax the matching threshold to prevent the NMS mechanism from learning."" Does it mean using the post-assembling to replace NMS?
[1] Liao, Yue, et al. ""PPDM: Parallel Point Detection and Matching for Real-time Human-Object Interaction Detection"". CVPR 2020.
[2] Kim, Bumsoo, et al. ""HOTR: End-to-End Human-Object Interaction Detection with Transformer"". CVPR 2021.
[3] Chen, Mingfei, et al. ""Reformulating HOI Detection as Adaptive Set Prediction"". CVPR 2021.
[4] Tang, Kaihua, et al. Unbiased Scene Graph Generation From Biased Training. CVPR 2020.
[5] Li, Rongjie, et al. ""Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation"". CVPR 2021.
[6] Rowan Zellers, et al. ""Neural motifs: Scene graph parsing with global context"". CVPR 2018.
[7] Tang, Kaihua, et al. ""Learning to Compose Dynamic Tree Structures for Visual Contexts"". CVPR 2019.",1. Some classic and powerful methods such as Motifs [6] and VCTree [7] are not contained in Tab.,316,0
ICLR_2022_826,ICLR_2022,"- While there are some gains in performance, it feels incremental both in % increase and scale. MNIST and FMNIST are very similar small, almost binary valued datasets, and weakly supports the claim of scalability. - In UCI datasets, where features are real numbers, and probably contain more information per feature, supMIWAE performs at par with MIWAE. - Most aspects of the technique are leveraged from the prior works.
General comments: ● How does the supervision and joint training impact the training complexity of the system? ● Interestingly, supMIWAE performs definitely better than MIWAE for MNIST, and FMNIST, however such gains are no longer visible for UCI datasets. The gains of joint training seem to be limited to a certain type of data. ● In terms of practical applications, where do the authors see their method bring the most gains? Image, language, or statistical datasets.","- In UCI datasets, where features are real numbers, and probably contain more information per feature, supMIWAE performs at par with MIWAE.",317,1
ICLR_2022_2408,ICLR_2022,"It is not clear how the proposed formulation may change the derived combinatorial solutions besides the tightened gap for objective function values. This is related to the computation of objective function values in Algorithms 2 and 3 (in appendix), which basically took the best derived objective function values among the sampled transport maps for the GK-TopK formulation. Are the reported objective function values in Tables 1 and 2 based on these values? To access how this changes the solution quality, should the authors also provide the actual objective values with the derived combinatorial solutions? For now, it is difficult to tell whether the proposed method will have meaningful improvement in derived solution in practice.
It is not clear to me how significant is this proposed improvement over Soft-TopK. Based on the presentation, in particular, Theorem 1 and Remarks on page 6, by introducing Gumble noise, the derived bounds now have two terms. It may address the diverging gap issue when the sorted probabilities at the boundary are the same ( x K = x K + 1
). But why when GS-TopK will always have tighter bounds than Soft-TopK even when x K ≠ x K + 1
? Why adding Gumble noise to the original optimal transport formulation of Soft-TopK can always improve the bounds? In Figure 1, I assume that the actual optimal objective value is 2.4 while Soft-TopK has the gap at 2.0. What is the actual objective value for the derived solution by Soft-TopK? What about GS-TopK? Also, it seems the choice of τ
does not affect the gap by Soft-TopK in this simple illustrative example but it does change GS-TopK gaps. But τ
appears in the derived bounds for both formulations. Why?
For real applications, will different τ and σ
change the performances significantly?
Mathematical notations and the proofs have numerous issues. Here are a few examples: 1) In equation (6), x k and x k + 1
have not been defined yet. They were explained until the beginning of page 5. 2) Below equation (13) and the text after that, should ""the optimal solution to Eq. (12)"" T ~ be T ~ ∗
instead? 3) In equations (17) and (18), is x i
a vector? Please make sure about the meaning of bold and regular fonts in these equations. Also, regular font x
's have been used for sorted probabilities. Different notations may need to be used to avoid confusion here. 4) In Appendix A.1 above equation (30), ""By Lemma 1..."" but this part is to prove Lemma 1. Do the authors mean by ""Lemma 3"" instead? Even that is the case, there appear to lack some steps to connect Lemma 3 with equation (30). 5) On page 19 above ""Condition 1"", ""... except for the following condition: x i = x k , x i = x k
."" There must be typos for these two same equations. 6) Figure 4 and the proof for Theorem 2, do we need to worry about the cases with x i ≤ x j
for Conditions 3 and 4?
There are missing information. For example, it is not clear what sample size #G was used in the experiments. It is not clear how exactly shrinking procedure was scheduled for homotopy GS-TopK as there is no detail provided in Algorithm 2. It would be also nice to discuss more why in Figure 3, the approximate formulations always have lower bounds. The lemmas, theorems or their proofs do not seems to indicate that will always happen?
Presentation can be improved significantly. For example, Figure 2 appears to be a general illustration instead of being the ""overview of our pipeline"" for max covering as indicated on page 6. There are also numerous typos, for example, ""... important hcardinality constrained ..."" in the last line of the paper on page 9.","4) In Appendix A.1 above equation (30), ""By Lemma 1..."" but this part is to prove Lemma 1. Do the authors mean by ""Lemma 3"" instead? Even that is the case, there appear to lack some steps to connect Lemma 3 with equation (30).",318,0
ICLR_2022_2107,ICLR_2022,"weakness of the method with biological systems. The non-QM property part is not as clearly written and I found it harder to understand which dataset belongs to which statement. - Page 4 line 5: I believe the statement that it learns quantum mechanical (QM) interactions is not backed up. The 3D structures of molecules are partially determined by steric hindrance and not purely by QM interactions. E.g. when the model is pretrained on a smaller set of elements and used for elements it has not seen this shows it learned the structure and not QM. - Page 3 line 3: What is SE(3) symmetry? - Table 1: Is there a reason why two properties (cv and alpha) are better predicated with the pretraining of QMugs than with the same dataset (QM9)?
Some very minor points: - For the review is would be with advantage to add a column with line numbers. Further, exchanging numbers for the citations instead of name and year, this improved the fluent reading experience. - Page 1 figure 1: MI could be mentioned in the Figure comment or before on page 1 paragraph 4 (our solution) as this abbreviation was not used yet. - Abstract, Page 2 line 4, page 6: Inconsistency with the number of quantum mechanical properties. 8 are given in the tables but often 10 are mentioned. - Page 2 last line: Year is missing in the citation. - Page 4 last line: The word hardest is confusing here. - Page 7 Table 1: The targets are never mentioned or explained. Only the units are given in the SI. Table 6, here also the names of the variables would be great.
Page 8 after table 4, last sentence of the paragraph: Incomplete and I do not understand it.
Page 10ff: The citations are not normed, e.g. PMID",- Page 4 line 5: I believe the statement that it learns quantum mechanical (QM) interactions is not backed up. The 3D structures of molecules are partially determined by steric hindrance and not purely by QM interactions. E.g. when the model is pretrained on a smaller set of elements and used for elements it has not seen this shows it learned the structure and not QM.,319,0
ICLR_2022_1447,ICLR_2022,"Weakness]
1 In my opinion, this paper's novelty is not strong, e.g., sub-models should be diverse, sub-models are adversarially robust, and MIMO training strategy.
2 Sections 3.1 and 3.2 is not clearly written. E.g., what does V in equation 5 mean?
3 Compared with MSD (Maini et at.), the proposal MAT has marginal improvements against multiple attacks. [Questions]
1 Could authors illustrate more about Eq. 6 and Eq. 7. From the current writing, I do not understand cross gradient in detail.
2 A.2 PSEUDO CODE, where does penalty of cross gradient apply?","2 A.2 PSEUDO CODE, where does penalty of cross gradient apply?",320,0
ICLR_2022_350,ICLR_2022,"Writing. The writing quality and the presentation of the paper can be substantially improved. For example, in section 2, It might be better to move the loss functions to the experimental section, as they are not part of the algorithm.
Novelty. Overall, I don't see enough originality, and it's not very challenging to adapt the sequential testing to this setting. The main idea is to detect the bad distribution shifts by nonparametric sequential testing based on comparing the risk function on the target data and the risk function on the source data. The monitoring statistics are the risk functions (and their CI).
Comments. 1. The proposed method requires the labels from the test data, which is a restrictive setting for practical use. The paper claims that the label can be revealed in a delayed fashion, but we could also do batch detection for the past observations (and wait for a period and redo the testing). The paper doesn't provide any expected sample size analysis. I think it'd be better to give some comparison with the batch detection algorithm. 2. Since the algorithm requires to compute the CI at each time step, how efficient is it?","2. Since the algorithm requires to compute the CI at each time step, how efficient is it?",321,0
ICLR_2022_353,ICLR_2022,"Weakness: 1. It is an interesting phenomenon that ViT supernet training suffers from the gradient conflict issue. But I am curious about the performance of other supernet training methods that don’t utilize KD from supernet, like once-for-all (ICLR20), DNANAS (CVPR20) and Cream (NeurIPS20). 2. I am not clear why the authors introduce the switchable scaling. I don’t see the relationship between it and the gradient conflict issue, also the improvement in Table 7 looks marginal. Also Figure 3 is not very clear, e.g., what does c1 and c2 means? 3. There is no experiment to support the statement “the supernet and the sub-networks are more likely to conflict with each other in the presence of stronger data augmentations and strong regularization.”. There is performance improvement shown in Table 7, but it is not clear whether this is caused by the conflict between supernet and sub-networks. And if so, I am wondering is there still an improvement for weak DA & Reg if the gradient projection is already applied. 4. In Table 8, it seems that AlphaNets trained with alpha-divergence doesn’t benefit from your method. Maybe you should show the gradient conflicts ratio for it to give some insights. 5. Since the gradient projection leads to slow convergence, the authors should provide the training time cost before and after using the proposed gradient projection, as the training time cost is also important. 6. How about the training time consuming after using the other two techniques. Does it reduce the training time caused by gradient projection for its slow convergence? Detailed information should be presented. 7. Why do the authors use EfficientNet-B5 as the teacher? Why not B6，B7 or other networks? The reasons are not mentioned in this paper. It is confusing. 8. To be more general and convincing, though the evaluations of the smallest and the largest networks are provided, I think the results of the middle-size network in needed experiments are also critical. For example, in Fig 2, Tab 3, and so on. 9. The authors only adopt gradient projection on CNN-based supernet while conducting ablation studies. The generalizability of the other two techniques in CNN should be verified as well like Tab.7.","2. I am not clear why the authors introduce the switchable scaling. I don’t see the relationship between it and the gradient conflict issue, also the improvement in Table 7 looks marginal. Also Figure 3 is not very clear, e.g., what does c1 and c2 means?",322,0
ICLR_2022_1755,ICLR_2022,"Weakness: 1. This paper demonstrates why self-knowledge distillation as a prior distribution is a form of regularization with theoretical analysis on the gradients. However, there is no theoretical result about the effectiveness of assigning the self-knowledge distillation to label smoothing. 2. The authors claim that “There are a number of benefits of adopting the adaptive smoothing parameter”. However, they only show that the hyperparameter search on \alpha is removed and the adaptive smoothing parameter can be connected to the gradient rescaling effect on self-distillation. More details should be reported to show the benefits of adopting the adaptive smoothing parameter. 3. In the ablation study, the authors only consider the fixed \alpha as the base. However, \alpha could also be changed in the training process. Therefore, the dynamic \alpha by hyperparameter searching should also be added as a base.
[1] Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing, ICLR 2020. [2] Adaptive label smoothing, arXiv 2020.","3. In the ablation study, the authors only consider the fixed \alpha as the base. However, \alpha could also be changed in the training process. Therefore, the dynamic \alpha by hyperparameter searching should also be added as a base. [1] Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing, ICLR 2020. [2] Adaptive label smoothing, arXiv 2020.",323,0
ICLR_2022_2575,ICLR_2022,"Weakness:
1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?
2). 80.0% top-1 accuracy on ImageNet is not hard. Does the design principle can scale well across different (data size, model parameters, FLOPs)?
3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?",3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?,324,0
ICLR_2022_3090,ICLR_2022,"**
W1. Although the motivation of this paper itself (to control the image clustering results by phrases) is practical and useful, the proposed method is not a direct solution to realize this. The proposed method performs clustering by assigning images to K phrases out of a larger set of phrases, where the assignment of image features to phrases is automatically determined based only on the feature similarities. The example in Fig. 1 shows that by including phrases such as ""drinking"" and ""brushing teeth"" in the phrase list, clusters relevant to these concepts can be obtained. However, if the phrase list contained other phrases like ""male"" or ""child"", these images would still likely be assigned to the clusters relevant to these phrases, which is not desirable in the sense of the motivation of this paper. In the end, the proposed method alone is not sufficient to realize the motivation of this paper, and another mechanism such as a ""deny-list"" is needed to ensure that the phrases ""male"" or ""child"" are not included in the phrase list, or that clusters associated with these phrases are not formed. It would be necessary to reconsider the discussion so that the motivation and the proposed method are consistent, or to clarify the position of the proposed method with respect to the motivation.
W2. There are some unclear points in the proposed method.
W2-1. The problem is formulated as an uncapacitated facility location problem, but the solution is essentially K-means + ad hoc post-processing (i.e., replacing centroids with its nearest neighbor phrase and special care for empty/large clusters). Theoretical properties of this algorithm (guarantees of approximation, for example) are not discussed at all.
W2-2. The cluster update algorithm (Sec. 4.3) is ambiguous and needs to be clarified. It says to select K-p and p phrases from the previous clusters and the current clusters, respectively, and terminate if the loss does not decrease. Does it mean that after K-means, once all the centroids are replaced with their nearest neighbor phrases, up to K-p of them will be swapped with the phrases in the previous clusters? The order in which the centroids are swapped should change the results, but is there any rule about the order? There should be a possibility that the exact same phrase is included, but no special care is taken about that?
W2-3. I could not find how the ""excessively large cluster"" is detected.
W3. The comparisons are insufficient. The proposed method uses both image and phrase features for image clustering, but the compared clustering methods only use image features, which does not demonstrate the effectiveness of the proposed method. There are several clustering methods that use multiple features. For example, majority of co-clustering / bi-clustering methods (e.g., spectral co-clustering, information theoretic co-clustering, non-negative tri-factorization, Bayesian co-clustering, bipartite graph segmentation [2], etc.) can be compared. The CLIP features map both images and phrases into a common space, and thus can naturally compute the similarities between arbitrary images and phrases in this space. Also, some of multi-modal / cross-modal clustering methods are applicable to unpaired data (e.g., [3]), so they can be compared too.
** Other minor issues **
Eq. 5: The variable in the set-builder notation should not be v_i, but c_k'.
** References **
[1] Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward, Better Guarantees for k-Means and Euclidean k-Median by Primal-Dual Algorithms, FOCS 2017.
[2] Feiping Nie, Xiaoqian Wang, Cheng Deng, and Heng Huang, Learning A Structured Optimal Bipartite Graph for Co-Clustering, NeurIPS 2017.
[3] Yangbangyan Jiang, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang, DM2C: Deep Mixed-Modal Clustering, NeurIPS 2019.","5: The variable in the set-builder notation should not be v_i, but c_k'. ** References ** [1] Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward, Better Guarantees for k-Means and Euclidean k-Median by Primal-Dual Algorithms, FOCS 2017. [2] Feiping Nie, Xiaoqian Wang, Cheng Deng, and Heng Huang, Learning A Structured Optimal Bipartite Graph for Co-Clustering, NeurIPS 2017. [3] Yangbangyan Jiang, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang, DM2C: Deep Mixed-Modal Clustering, NeurIPS 2019.",325,0
ICLR_2022_3147,ICLR_2022,"The novelty is limited. the idea is to have a mixed image with style and content from different images, which is similar to StyleMix.
The proposed approach needs more computational resources to obtain good performances: 2000 epochs on Cifar10/100 (300 epochs in StyleMix). How long does the entire training take? In some cases, it might be a problem. I would suggest to precise more details about the computational resources in the paper. Moreover, as the approach takes longer to train and the inference is the same for all the competitive approaches, it is not convincing that the approach is more efficient in terms of computational complexity.
Table1.b Results are not comparable, some approaches seem to be trained much fewer epochs than the proposed one. For example, StyleMix only trained for 100 epochs, while the proposed approach trained for 300 epochs. It would be better to clarify this difference by also reporting results in Table.5.
Some important training details are missing, for example, image resolution plays an important role in the performance of image classification[1]; Are there other data augmentations used during the training?
During the training, the proposed algorithm needs to sample from 4 (5 for alignMix/AE) cases, I would suggest an analysis on these cases. For example, we could remove one case to see how the performance drops. The goal is to understand where the improvement comes from.
It is interesting to see how the proposed approach improves over state-of-the-art approaches. For example: Deit[2], Swin transformer[3] etc. Clarity:
The approach is in general well-presented, some details are missing:
Table 1.b: it would be better to precise MESC/BATCH also in the caption of the table.
PreActResnet18 seems to be different from the original paper [He et. 2016], if so, please clarify the difference.
Supplementary B. Parameter setting. The following description is confusing: “We also train R-50 on ImageNet for 100 epochs, following the training protocol described in Kim et al. (Kim et al., 2021).” I guess it refers to the results in table 5, but the associated text should be also there.
The description of the architecture is confusing, the dimension of the output A should be different for CIFAR10 and ImageNet.
The ablation study is hard to follow (Appendix C). The notations are confusing: “c ∈ R 512
” but “e is a 128 × 2 ×
2 tensor”. Different sets in the column LAYERS (TABLE 8) are also hard to understand. I would suggest revising this part and making the messages clear.
[1] Touvron, H., Vedaldi, A., Douze, M., & Jégou, H. (2019). Fixing the train-test resolution discrepancy. NeurIPS 2019
[2] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR.
[3] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. ICCV 2021","2000 epochs on Cifar10/100 (300 epochs in StyleMix). How long does the entire training take? In some cases, it might be a problem. I would suggest to precise more details about the computational resources in the paper. Moreover, as the approach takes longer to train and the inference is the same for all the competitive approaches, it is not convincing that the approach is more efficient in terms of computational complexity.",326,0
ICLR_2022_1139,ICLR_2022,"Weakness: 1. The importance of fine-tuning is not well justified. The pre-trained decision transformer model has already achieved sufficiently good performance without reward annotations. Why do we need to fine-tune the pretrained model on a subset of the pretrained dataset with reward annotations? The significance of fine-tuning can be shown on more diverse and challenging datasets. 2. Pre-training a decision transformer on a large dataset is expensive. Fine-tuning a full pre-trained decision transformer on downstream dataset is not efficient. It would be better if a light-weight fine-tuning method can be applied. 3. Despite the authors provided ablations, the discussion and conclusion on which factor is most effective is sufficient. For example, Why PDT with masking reward token during pre-training works slightly better than PDT without masking?
Other questions: 1. Table 1 shows the fine-tuning results of PDT. How does the pre-trained decision transformer perform? Is it better than the reported PDT results? If the pretrained decision transformer outperform the PDT fine-tuned on smaller downstream datasets, then the fine-tuning is not that significant. 2. Some important experimental details are missing. For example, how many epochs were used for fine-tuning and how many steps in each epoch? If the fine-tuning is expensive, then why not train the DT on the small dataset with reward annotations from scratch?",2. Pre-training a decision transformer on a large dataset is expensive. Fine-tuning a full pre-trained decision transformer on downstream dataset is not efficient. It would be better if a light-weight fine-tuning method can be applied.,327,0
ICLR_2022_2113,ICLR_2022,"Weakness 1). There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. However, it may not be necessarily this case. The authors are suggested to prove first that the latent code layer must be a sparse vector (or tensor) 2). Usually, PSNR or SSIM are also used for evaluating the image restoration tasks. It would be great to provide the scores of these two traditional metrics in the paper in order to have an all-around evalutions. 3). The paper does not clearly disclose the detailed parameters of the proposed method, for example, what is the selected threshold λ
value in L0 norm? And the reason to use these threshold ? What are the weighting terms of the three different normalization constraints?","1). There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. However, it may not be necessarily this case. The authors are suggested to prove first that the latent code layer must be a sparse vector (or tensor)",328,0
ICLR_2022_1929,ICLR_2022,"The presentation of RNP does not seem to imply that it is a purely theoretical construction. For example, the argument that the complexity of RNP depends on the sparsity of the graph, contrary to higher-order GNNs is compelling (note that this argument is introduced early on, in the abstract, and this perhaps exceedingly raises expectations). However, the experimental section currently points towards the opposite direction. Can the authors clarify their position w.r.t. that?
For example, I would expect the triangle counting task to be relatively easy for this network, but RNP does not seem to be better than other more expressive GNNs. The same holds for the 3-star where GraphSage performs much better (question: is GraphSage permutation sensitive here?). How do the authors comment on that? What is the depth of the recursion that you used (is it 2 as expected from the theory)? In general, what is it that prevents RNP from being implemented in a practical form and why isn't it clearly better in these two experiments. Is it only the computational complexity (that perhaps makes it harder to do more extensive experimentation)? Does it have to do with optimization and/or generalisation (it wasn't clear to me if there is a hold-out test here)?
Unless the method is purely theoretical, I think the authors would benefit a lot from creating a working instantiation of their model and a simple experimental section with more convincing results. Suggestions: (1) counting various substructures and contrasting their r parameters with those predicted by the theory, (2) real-world datasets where substructure counting is important (a single one would be OK – it might be useful here to measure the runtime of your method and contrast it with the sparsity of the graphs) and optionally (3) predicting graph properties.
Although the paper is generally well-written, it is notation-heavy and at some points requires significant effort to parse the mathematical expressions. In my opinion, the authors should try to simplify some parts in order to make it more accessible to readers not versed in the theory. For example:
Architecture - Eq (3-7). This is the heart of the proposed method, but it is a bit hard to follow. Notation can be off-loaded at some points (e.g., Eq. (5) is very important since it is the recursion formula, yet it’s quite complicated - what is G’? Why do you write h_{u,v} in Eq. (5) and h_{v,u} in Eq. (6)?)
Could the authors provide an explanation by unrolling the recursion? This might help the reader develop intuition and compare RNP to conventional iterative GNNs.
Other comments and questions: (I don’t expect all these to be addressed in the rebuttal, but it might be useful to clarify in the next revision of the paper) - How important are the identifiers used to augment the node embeddings (Eq. (3))? Can RNP retain its expressive power without them or it might collapse to that of an MPNN? Regarding the connection with the reconstruction conjecture: How important is the removal of the central node while recursing? In terms of complexity it’s beneficial to remove the central node, but is it crucial in terms of expressive power? - What happens in case we want to count a subgraph that is disconnected? How can you define the covering sequence in this case? I think the authors make such an argument in the proof of Theorem 2, but this is not clear from the main paper. - I believe at least the proof idea for Theorem 1 should be given in the main paper. - As far as I understand, the lower bound of Theorem 4 applies only to functions of the form of Eq. (12), i.e., those that encode t
subgraphs with s
different unique values and then take the resulting histogram. Is that general enough? Also, although, as I said above, I found the construction of clique-containing graphs very clever, I am wondering if the lower bound is very loose since it is based on counting these graphs only. Finally, personally, I would call it a counting argument rather than information-theoretic (I was expecting to see something else in the proof when I first read it).
Minor: - You wrote: (Background. Higher-Order GNNs.) “At initialization, each k-tuple is labelled such that two k-tuples are labelled differently if their induced subgraphs are not isomorphic”. If I am not mistaken, k-WL labels k-tuples based on their isomorphism types, which is different from isomorphism classes since it takes into account the order of the vertices in the tuple as well. - The following sentence is unclear: “In particular, we show how the aggregation “augments” local encodings, if they play together and the subgraphs are selected appropriately”. - Proposition 6: You state “there is no difference between counting induced attributed graphs and counting induced unattributed graphs in RNP-GNNs”, however, if I am not mistaken your argument does not go both ways. Is that correct? - Definition 7 requires further clarifications/is quite hard to follow (e.g., I am not sure if I understand the term homomorphism here since homomorphisms allow vertex repetitions). - “However, it is known that MPNNs can count at most star structures or edges”: missing citation: ""On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties"", Arvind et al., FCT’19
--------------- After rebuttal ---------------
My major concerns were not addressed by the rebuttal, hence I will keep my recommendation unchanged. Please see my final comment to the authors for more details.","- Proposition 6: You state “there is no difference between counting induced attributed graphs and counting induced unattributed graphs in RNP-GNNs”, however, if I am not mistaken your argument does not go both ways. Is that correct?",329,0
ICLR_2022_3310,ICLR_2022,"-Clarity a) Section 4 needs a revision to improve its readability. Too many important information is moved to the appendixes.
-Originality/Technical. Somehow the theoretical analysis of Eigen reptile is not novel. The manuscript provides a detailed solution to overcome the computing cost of the eigenvalues and eigenvectors. But the formulation proposed can be considered as a standard PCA derivation.
-Technical correctness: a) I don't consider as correct that the proposed Eigen-reptile can be used as an approach to deal with the overfitting problem. To update the meta-parameters by the main direction of task-specific parameters does not necesarrily alleviate the overfitting of the meta-learner. Actually, this process only helps to mitigate the wrong updates of the parameters when noisy labels occur. Overall, I don't consider the proposed model as a solution for the overfitting problem in meta-learning. b) Technically, in a FSL scenario, where just a small number of samples for each tasks are available, is it appropriate to consider as representative for a task, the main direction of the task-specific parameters? In my opinion, this would make sense if we have many samples, which would make the direction representative for the class. Some methods (such as (Cao et al., 2019)) are used to increase the training shots in this approach, and I wonder how the approach would work when these are not used. c) I have a direct questions for the authors: Are the proposed ideas applicable to any gradient-based meta-learning approach? This discussion is not included in the paper, so , for the moment, I assume that they are not.
-Experimental setup a) Section 5 devotes too much space to the analysis of Eigen-Reptile in one scenario without addressing the issue of noisy samples, simply comparing it with Reptile. Although this experimental discussion is necessary, I believe it should be shortened in the article. Section 5.1 and 5.2 are too long, please, join and shorten them. This way, maybe some important details included in the appendixes can be incorporated into the paper (e.g. the pseudo code of the algorithms). b) Eigen-Reptile+ISPL has not been benchmarked with the rest of meta-learning methods. Authors should report these number in Table 1. This experiment would show how the complete approach works even with clean labels. Actually, in a real-world scenario, new tasks, with new annotations, arrive to the system, and one does not know beforehand whether they are noisy. Eigen-Reptile+ISPL should be able to deal with this situation as well. c) In the noisy label experiments I think I have found some deficiencies: 1) When clean labels are used ( column 1, p = 0.0 ) the numbers do not coincide with those reported in table 1, this needs and explanation. 2) Under this situation (p=0.0), ISPL does not improve the results, but degrades it. 3) When noisy labels are included, ISPL does not help standard Reptile, and just Eigen-Reptile. See the results AS with p=0.2 or S and AS for p=0.5. These numbers do not reveal the benefits assumed for the ISPL model. d)Instead of so many experiments in scenarios without noise (section 5.1 and 5.2), the article should also include results in other databases (e.g. Ominglot), and with noise. In this way we will be able to better judge the proposed ideas, analyzing whether the behavior holds with different databases. Results provided in CIFAR do not consider noise.
Minor comments:
-Page 1, Paragraph 2, lines 6 (P1P2L6): etc.. -> etc. -P1P2L10: the model will not only learns -> the model will not only learn -In figure 1, the example for label noise is not clear. Wouldn't it be better to plot the main direction of the gradients plot the green arrows for the noisy label? -Please, note that ""et al."" is a plural subject. Revise the manuscript to properly use this term. -Please, punctuate the equations. -P4P1L2 -> q(·) is not used in Eq 1.","2) Under this situation (p=0.0), ISPL does not improve the results, but degrades it.",330,0
ICLR_2022_3045,ICLR_2022,"Weakness
1. The organization and writing of the paper lacks logic, which makes it hard to understand and follow.
2. The motivation of the proposed three components in AnoSeg is not clearly presented in introduction. For example, in addition to textual explanation, some experimental or theoretical analysis should be given to convince reader that the proposed method is reasonable.
3. The novelty of the proposed hard augmentation, adversarial learning and coordinate channel concatenation is slim. For example, hard augmentation has been proposed in Tack et al. and Li et al. Numerous methods have adopted adversarial learning for anomaly detection.","1. The organization and writing of the paper lacks logic, which makes it hard to understand and follow.",331,0
ICLR_2022_1761,ICLR_2022,"weakness is that the writing is not clear. For the pre-training tasks, what is the cross entropy loss of “L_{CS}” over the masked entity and relation tokens? And the same for the loss of “L_{L}”
Some clarification questions: 1). In all the result tables, what are the results of KMLM-XLM-R_{large}(ours)? Seems the authors did not report all the performance of KMLM-large without logical reasoning.","1). In all the result tables, what are the results of KMLM-XLM-R_{large}(ours)? Seems the authors did not report all the performance of KMLM-large without logical reasoning.",332,0
ICLR_2022_2470,ICLR_2022,"Weakness:
The idea is a bit simple -- which in of itself is not a true weakness. ResNet as an idea is not complicated at all. I find it disheartening that the paper did not really tell readers how to construct a white paper in section 3 (if I simply missed it, please let me know). However, the code in the supplementary materials helped. White paper is constructed as follow:
white_paper_gen = torch.ones(args.train_batch, 3, 32, 32)
It offers another way of constructing white paper, which is
white_paper_gen = 255 * np.ones((32, 32, 3), dtype=np.uint8)
white_paper_gen = Image.fromarray(white_paper_gen)
white_paper_gen = transforms.ToTensor()(white_paper_gen)
white_paper_gen = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(white_paper_gen)
The code states that either version works similarly and does not affect the performance. I wonder if there are other white papers as well, for example np.zeros((32, 32, 3)) -- most CNN models add explicit bias terms in their CNN kernel. Would a different white paper reveal different bias in the model? I don't think the paper answers this question or discusses it. 2. Section 4 ""Is white paper training harmful to the model?"" -- the evidences do not seem to support the claim. The evidences are 1). Only projection head (CNN layers) are affected but not classification head (FCN layer); 2). Parameter changes are small. None of these constitute as a direct support that the training is not ""harmful"" to the model. This point can simply be illustrated by the experimental results 3. Section 5.1 and 5.2 mainly build the narrative that WPA improves the test performance (generalization performance), but they are indirect evidence to support that WPA does in fact alleviate shortcut learning. Only Section 5.3 and Table 6 directly show whether WPA does what it's designed to do. A suggestion is to discuss the result of Section 5.3 more. 4. It would be interesting to try to explain why WPA works -- with np.ones input, what is the model predicting? Would any input serve as white paper? Figure 2 seems to suggest that Gaussian noise input does not work as well as WPA. Why? Authors spend lot of time showing WPA improves the test performance of the original model, but fails to provide useful insights on how WPA works -- this is particularly important because it can spark future research directions.","2. Section 4 ""Is white paper training harmful to the model?"" -- the evidences do not seem to support the claim. The evidences are",333,1
ICLR_2022_917,ICLR_2022,"Via the top-K approach, it appears like the authors make an implicit assumption about what fraction of edges is important in the graph (a fixed fraction, can't be lower or higher) - in my opinion, this certainly limits the wide applicability of the method. Alternatively, in GNN Explainer [1], they allow up to K edges to explain the label.
The applicability of the method is limited by the graph encoder used (here GNN). 1-WL GNN's are known to be unable to predict links or identify and distinguish certain classes of subgraphs [2][3][4] i.e. GNN's are not well suited for Eq. 5
The work appears to miss relevant baselines like GNN Explainer[1] / CF-GNN Explainer [5]. Moreover the authors ideally should compare with baselines, which enrich GNN's with structural features [6][7][8] (As they are explainable to a certain extent as well). Minor:
The separability of a graph into two subgraphs the causal and non causal might not always be possible? (would this need an encoder which is able to accurately capture the discrete topology over graphs of all orders and sizes - if not I can just make a house into a clique and the label would be incorrectly assigned as 1 because the edges required for the house are present) Please elaborate on this.
The set {s} employed in the test are limited to the ones seen in train.
Moreover, consider a graph with say 20 nodes, and the case where the causal part for instance consists of a house motif and a tree base and the label assigned is 1 or 0 based on the output of House Motif XOR Tree Base - would this be captured by the proposed method (appears like it wont)? References:
1.Ying, Rex, et al. ""Gnn explainer: A tool for post-hoc explanation of graph neural networks."" arXiv preprint arXiv:1903.03894 (2019).
2.Srinivasan, Balasubramaniam, and Bruno Ribeiro. ""On the equivalence between positional node embeddings and structural graph representations."" arXiv preprint arXiv:1910.00452 (2019).
3.Dwivedi, Vijay Prakash, et al. ""Benchmarking graph neural networks."" arXiv preprint arXiv:2003.00982 (2020).
4.Chen, Zhengdao, et al. ""Can graph neural networks count substructures?."" arXiv preprint arXiv:2002.04025 (2020).
5.Lucic, Ana, et al. ""CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks."" arXiv preprint arXiv:2102.03322 (2021).
6.Bouritsas, Giorgos, et al. ""Improving graph neural network expressivity via subgraph isomorphism counting."" arXiv preprint arXiv:2006.09252 (2020).
7.Bodnar, Cristian, et al. ""Weisfeiler and lehman go topological: Message passing simplicial networks."" arXiv preprint arXiv:2103.03212 (2021).
8.Bodnar, Cristian, et al. ""Weisfeiler and lehman go cellular: Cw networks."" arXiv preprint arXiv:2106.12575 (2021).","3.Dwivedi, Vijay Prakash, et al. ""Benchmarking graph neural networks."" arXiv preprint arXiv:2003.00982 (2020).",334,1
ICLR_2022_1212,ICLR_2022,"1) The experimental settings are not hard enough to evaluate the performance of FSL. There is no doubt that there is information loss when the devices transmit only the ranking of scores. This kind of information loss is not serious when the rankings on different devices are similar (the local subnetwork structures are similar due to similar data distributions). In this paper, the user uses Dirichlet distributions to construct non-IID data for MNIST and CIFAR10. Even though the data distributions are different across devices, each device still holds all classes of data and local subnetwork structures would not show significant difference. And I guess the robustness results have the same problem since the authors use a voting mechanism to update the global ranking. I am wondering whether FSL would perform well under the non-IID setting in FedAvg paper, where each client only has two classes of data rather than all classes of data. 2) The improvement over baselines is not significant on some dataset. For example, “Top-K 10%” achieves even higher accuracy than “FSL” with lower communication cost on FEMNIST dataset. 3) The idea of utilizing “supermask” seems novel, but this paper seems just simply combining “supermask” with FL. It is okey to do “A plus B” things, but you need to provide some scientific contributions like providing a theoretical analysis about why “supermask plus FL” works, and what challenges that you solved make it deserve an acceptance by a top avenue like ICLR.","2) The improvement over baselines is not significant on some dataset. For example, “Top-K 10%” achieves even higher accuracy than “FSL” with lower communication cost on FEMNIST dataset.",335,0
ICLR_2022_2810,ICLR_2022,"The clarity of the writing could be improved substantially. Descriptions are often vague, which makes the technical details harder to understand. I think it's fine to give high-level intuitions separate from low-level details, but the current writing invites confusion. For example, at the start of Section 3, the references to buffers and clusters are vague. The text refers readers to where these concepts are described, but the high-level description doesn't really give a clear picture, making the text that follows harder to understand.
Ideas are not always presented clearly. For example:
may only exploit a small part of it, making most of the goals pointless.```
- Along the same lines, at the start of the Experiments section, when reading ```the ability of DisTop to select skills to learn``` I am left to wonder what this ""ability"" and ""selection"" refers to. This is not a criticism of word choice. The issue is that the previous section did not set up these ideas.
- Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.
- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.
- The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better.
### Questions/Comments:
- The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.
- The same applies to 2.2. The technical details are hard to follow.
- You claim ""In consequence, we avoid using a hand engineered environment-specific scheduling"" on page 4. Does this suggest that the $\beta$ parameter and the $\omega'$ update rate are environment independent?
- Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?
- It is somewhat strange phrasing to describe Skew-Fit as having ""favorite"" environments (page 6).","- It is somewhat strange phrasing to describe Skew-Fit as having ""favorite"" environments (page 6).",336,0
ICLR_2022_486,ICLR_2022,"1. It seems that the loss (1) is an incremental formulation by combining existing low-dimensional embedding methods and the simplified topological loss function (2). From (3), the topological loss of this paper is constructed from the embeddings instead of the input data. However, this may be thought as a straightforward extension. 2. Authors did not explain why only the regular part of (2) is used. It looks like only partial power of the topological optimization is considered by this work. Some explanation for choosing only the regular part is needed. 3. The claim that “a topological model that is naturally present in the data should be represented well by many subsets of the data” is not quite convincing. Taking the clustering problem as an example, this claim might not true if there are many small clusters when uniform sampling is used. Since it Is the key assumption for (4), authors might want to specify the limitation of the loss (4). Even for the experiments using (4), the analysis on the impact of f_s and n_s is not studied. 4. Experiments are conducted on both synthetic data and real data, but only the very basic approaches are compared. It is more like the ablation study of the proposed model. Authors need to compare the results obtained by the proposed model with the state-of-the-art methods. At least, the results from the paper using the same real data should be reported as the most relevant baselines. 5. From the visualization results, it seems that top. optimization embedding is very similar to the top. regularized embedding on most of the data sets expect Figure 7. Does it mean top. loss is sufficient enough to obtain relatively good results?",2. Authors did not explain why only the regular part of (2) is used. It looks like only partial power of the topological optimization is considered by this work. Some explanation for choosing only the regular part is needed.,337,0
ICLR_2022_1416,ICLR_2022,"Weakness:
Because of the surprising method simplicity, more analysis would be interesting to add that could shad light on the nuances of interplay between gradients and the CL, and why it helps. Some suggestions:
Does CL lead to fewer instance of gradient clipping compared to the baseline?
""The largest variance on certain dimensions"" is mentioned as a problem in the intro and in the last sentences of sec. 5.3, but no experiment measures it w/ and w/o CL.
Gradient similarity between neighbouring batches w/ and w/o CL
A simulated experiment: while doing CL, insert a sequence of a few baseline (full-length) batches, -- will you see the ""instability spikes"" like in baseline learning curves?
Given that Platanios et al reported good results both with length- and word rarity-based curricula, it would be nice to run a few experiments with the word-rarity difficulty definition.
Regarding tuning the T hyperparameter: since the validation set exhibits fluctuation for some T, I wonder if it's simply because, for larger T, the curriculum hasn't yet ramped up to the actual lengths of validation data? Esp. since the fluctuation seem to fade away closer to the 10K cutoff. If that is true, could you simply set T to a function of some length statistic of the validation set?
Clarification questions/remarks:
When trimming the batches to the current length, what happens to the shorter sentences? If they stay in the batch, than, effectively it's sampling from all lengths below the current seqlen_t?
The two-stage curriculum has actually two spikes (@20K in Fig. 3(f) and later @30K), what is the reason for the 2nd spike if the transition to full-length has already happened?
Figure 4: if CL60K is preferred here, does it mean it overtakes the validation curves for CL100K and CL80K? When does that happen?
Some of the prior work is mistakenly described as using fine-tuning (e.g. most references in the paragraph starting with ""In the natural language processing area,...""), while they are actually using one-stage training, without fine-tuning, as is standard in machine translation.
Minor remarks:
""extremely huge"" -> ""extremely large"" or simply ""huge""
""the gradient variance norm"" -> ""the norm of this variance"" (to be more specific)
""Inspired of the highly organized curriculum"": what does the definite article refer to? probably, can omit it
""human and animal,"" -> ""humans and animals,""
could you add a citation to the sentence ending with "", and model divergence."" in sec. 2?
something is wrong with grammar in the sentence ""To quantitatively measure the token..."", sec 5.1
Table 2: I'd suggest using more intuitive '+' and '-' instead of arrows-up and arrows-down
Tables 1 and 2 are hard to parse because they contain different types of information: consider splitting into 2 half-tables horizontally in the middle (i.e. on table for target ppl and the rest in the other)
a coma missing after ""Because of 1)""
please use more specific wording instead of ""we find that this is not ideal"" (both in sec. 5 and A.2). What do you mean - not ideal for 117M or in general? What is ""ideal"" here?
Figure 2. ""the first 60K"" -> ""the first T=60K""
please explain the term ""token reduction"" in sec 5.4","1)"" please use more specific wording instead of ""we find that this is not ideal"" (both in sec. 5 and A.2). What do you mean - not ideal for 117M or in general? What is ""ideal"" here? Figure 2. ""the first 60K"" -> ""the first T=60K"" please explain the term ""token reduction"" in sec 5.4",338,0
ICLR_2022_1158,ICLR_2022,"1. Typos, e.g. , “simiar” in page 2; “do ConvMixers” in page 8, etc. 2. At the end of Section 1, the authors claim that “This suggests that, at least to some extent, the patch representation itself may be the most critical component to the superior performance of newer architectures like Vision Transformers.” I don’t think this is true, and the authors have not fully verified such a claim. There are many other factors, like learnable parameters, effective receptive fields, etc. This statement seems to be overclaimed. 3. Most importantly, I don’t think this method achieves “better” results than others. i), Comparing with ResNet152, the throughout is 89 vs. 872. The slow speed of ConvMixer is not acceptable in real-world applications; ii) Besides, the performance of ResNet would be higher than reported if trained with appropriate strategies [2]. iii) Small patch properly can achieve better performance. I would suggest authors try some other Transformer models, like Swin Transformer [3]. 4. In future work of page 4, authors claim that “We are optimistic that a deeper ConvMixer with larger patches ... and hyperparameter tuning”. However, this may not be true. Please see Appendix A. I have not seen the strong superiority of ConvMixer over the ResNet in the accuracy with similar parameters. And the ConvMixer’s speed is far below the ResNet. 5. In VITs and MLPs (including their variants), we have two skip connections (it is not called residual connection as shown in Fig 2) in a block. However, authors designed with only one (tuned on cifar10). I just want to know if we add two skip connections in ConvMixer, what is the result for ImageNet. 6. I’m not a fan of cifar10 dataset due to the dataset size and inevitable randomness in results. Considering the limited computational resources, some other datasets may be helpful (like cifar100, Tiny-ImageNet, etc.) or authors can report the mean/std results of 5 runs on cifar10. 7. I could not get the idea of “A note on paper length” paragraph in a conference paper, considering some of the important experiments are missing. It is nothing to do with the main idea of ConvMixer. 8. Some important ablation studies are missing, like the influence of depth, kernel size, patch sizes, etc (I mean detailed ablation studies). I would also be interested in the performance of ConvMixer on some downstream tasks like detection.
[1] Brendel, Wieland, and Matthias Bethge. ""Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet."" International Conference on Learning Representations. 2018.
[2] Wightman, Ross, Hugo Touvron, and Hervé Jégou. ""ResNet strikes back: An improved training procedure in timm."" arXiv preprint arXiv:2110.00476 (2021).
[3] Liu, Ze, et al. ""Swin transformer: Hierarchical vision transformer using shifted windows."" arXiv preprint arXiv:2103.14030 (2021).","7. I could not get the idea of “A note on paper length” paragraph in a conference paper, considering some of the important experiments are missing. It is nothing to do with the main idea of ConvMixer.",339,0
ICLR_2022_31,ICLR_2022,"- I don’t understand what’s happening in theorem 4. It considers a subsample μ ^
of F(X) to be ‘good’ when the symmetrizer that uses the subsample is epsilon-close to the full F(X) symmetrizer. Then it says that the probability of one particular good subsample is bounded below. However, that bound seems vacuus, as plugging any reasonable number brings the bound quickly close to 0. Also, it’s counter-intuitive why the bound should become looser as epsilon grows or as k grows. What one would want instead is giving a lower bound of the probability that we get any μ ^
that is epsilon-close to the full F(X) symmetrizer. And we want this bound to get higher when epsilon or k increases. The line below theorem 4 draws a conclusion that would follow from a theorem as I propose it above, not from the theorem in the paper. As it is currently stated – and I’m not completely misunderstanding – theorem 4 can best be removed from the paper. - It is a bit unclear when the results apply to finite and infinite groups and frames F(X). Everywhere, a summation symbol is used, but in some places, F(X) is infinite. In the infinite cases, which measure should then be used? Can one always use some canonical Haar-like measure? In particular, in the proofs of theorem 1 and theorem 4 this should be discussed. - The writing of the paper can be improved. I don’t follow the choice of F(X) for the E(d) case. Which are the 2^d O(d) matrices? Perhaps the authors can elaborate in more detail one of the examples how to construct F(X) in the main paper, and then do the other two in the appendix. - I would like some more theoretical discussion about the choice of F(X). Does the choice of F(X) affect the output? If so, how? Is F(X) required to be continuous / does a continuous (non-trivial) F(X) always exist? What does the topology on 2^G look like? How does this affect the continuity of the symmetrized function? If it is discontinuous, does that affect the universality? When can F(X) be chosen to be finite?
Other comments: - Why does GA-MLP and GA-GIN+ID only get 50% score on EXP-classify? Are you there using a finite subsample of G or F(X)? And could you give any insight into why we’d expect then complete failure for a G subsample and complete success for a F(X) subsample? - In the proof of theorem 5, which norm is used for rho_2(g) ? It can’t be the max K-norm because K is a subset of the input of phi, not the output. Is it the operator norm? - I would like theorem 1 to be put in the main paper, as it shows why the key construction is correct. - I think a citation would be appropriate to Finzi et al 2020, “Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data”, as they also consider sampling from the group to build equivariant networks. - Why have the authors chosen the name “frame” for F(X)? I know frame as a set of vectors or in the context of a frame bundle.","- I think a citation would be appropriate to Finzi et al 2020, “Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data”, as they also consider sampling from the group to build equivariant networks.",340,0
ICLR_2022_3257,ICLR_2022,"1). This paper lacks novelty. The three modules of the final model are not originally proposed by the authors. The cross-lingual language model is proposed and trained by (Conneau et al., 2020) and (Devlin et al., 2019), the idea of incorporating speaker and dialogue turn information is from (Xu et al. 2021), the hierarchical encoding practice is widely used in dialogue modeling area (e.g., Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue, Liu et al. 2021). The first two pre-training objectives are from other papers.
2). More ablation studies need to be performed. The authors do not provide key ablation studies to analyze the source of performance gains, such as replacing SC-Encoder or PA-Encoder with naive methods (or remove them), or replacing the Concat & Norm trick with the original Add & Norm. Besides, do the improvements come from the additional number of parameters? Will simply adopting additional Transformer layers (without the speaker and turn information) improve the performance? 3). More kinds of languages should be studied. The authors conduct experiments only on English corpus as zero-shot study, which is insufficient to demonstrate the effectiveness of the zero-shot transfer ability of the proposed model.
More detailed comments and questions, 1. Some details in the model are vague, please see below questions: a) How does the SC-Encoder obtain dialogue turn embedding t and speaker role embedding r (to obtain timestep encoding s)? It seems that they cannot be generated from UOR and SPI pre-training objects, due to step for generating timestep encoding is before the step for UOR and SPI pre-training. b) What type of activation function do you use in Formula 2 and 4? c) Does the symbol ⊕ mean vector connection in all the formulas? Related description is missing. 2. The proposed model is much more complex than the baseline with more parameters and additional data for pre-training of CLM module, so that: a) The overall performance improvement cannot match the increased parameter volume and additional pre-training consumption. b) A considerable proportion of improvement may come from the increased parameter volume and additional pre-training consumption, instead of the proposed methodology and architecture. c) This paper does not report the parameter volumes of proposed model and its baseline. 3. Some detail analysis of the ablation result (Table 1) is missing or not rigorous enough: a) Why do the performances of model w/o SPI and UOR improve in F1_all and F1_intra for Persona-Chat and CMU-DoG? b) The performance of model w/o TLM and HPSI increases in F1_intra for CMU-DoG, compared to all obiects model (59.24 v.s. 58.82), so “removing TLM and HPSI objectives hurt performance consistently but slightly across all metrics on all datasets” is not rigorous enough. 4. Lack of some ablation studies: a) The ablation of model architecture is missing, e.g., w/o SC-/PA-Encoder. b) The ablation of replacing u’ to u in SC-Encoder. 5. More compared models and statistics application in downstream English conversational tasks are need: a) More comparable models with their performance in other typical works are needed in Table 3 and 4, instead of only Seq2Seq. b) The authors claim that questions with required information completion/ responses containing entities mentioned in histories benefit from CSRL, but do not provide the detailed (error/correct) case statistics or analysis. 6. There are some typos in grammar. For example, in Page 4, lin2 2, denote->with denoting.","5. More compared models and statistics application in downstream English conversational tasks are need: a) More comparable models with their performance in other typical works are needed in Table 3 and 4, instead of only Seq2Seq. b) The authors claim that questions with required information completion/ responses containing entities mentioned in histories benefit from CSRL, but do not provide the detailed (error/correct) case statistics or analysis.",341,0
ICLR_2022_1786,ICLR_2022,"Novelty. Being a combination of implicit variational processes, functional variational inference, and basic concepts from probabilistic multi-class classification, the novelty of the contribution is somewhat limited. I find the claim that this work provides ""a unifying view of prior work which use the Dirichlet distribution and function-space regularization"" to be a slight over-statement. A method that combines concepts from two areas is not necessary a unifying theory thereof. One significant technical challenge that is tackled is the MC estimation and minimization of the KL divergence, which is addressed using a quasi-Newton method. Questions
Section 4.4 considers image classification at five increasing levels of corruption. It is understood that each of these numbers corresponds to a particular setting of brightness, contrast, saturation, etc. However, what these settings are remains unclear. Is this detailed anywhere?
Do you actually make use of the ability to incorporate class-biased predictive prior that the proposed approach unlocks in any experiments other than the qualitative results shown in Figure 3? More generally, are there concrete problems in which having strong informative predictive prior for multiclass classification is actually helpful, and where conventional Bayesian approaches, through posterior inference with its data-fit component are unable to adapt to the class-biases. It would be compelling to see quantitative results that support the claims concerning class-biased predictive priors.
I am not fully convinced about the results shown in Section 4.4, in particular in Figure 5 for CIFAR100 where the accuracy and log-likelihoods are not distinctly better across corruption levels, particularly with Ensembles. Furthermore, the ECE is only lower for the highest levels of corruption. The explanation offered is that the method underfits, which is supported by the relatively low log-likelihood and accuracy. Obviously one can easily attain good ECE with OOD data by deliberately underfitting, which is exactly why the log-likelihood and accuracy are important to consider as well. Therefore, I don't understand why the following comment from the authors isn't actually done? ""This suggests the function-space prior should be fine-tuned (i.e. empirical Bayes) if superior ECE is desired in-distribution."" When would this not be desired?
Miscellaneous Issues
Pg. 2: ""which use Gaussian weight priors and posteriors"" → ""that uses [...]""
Pg. 4: ""evidence lower bound (ELBO)"" - the ELBO and, more generally, variational inference, long predates Hoffman et al. 2013.
Pg. 4: ""Further details [...], which we consider, [...]"" - which → that; extraneous commas
Pg. 8: ""LLH"" this abbreviation was never defined.
The abbreviation ""OOD"" has even been explicitly defined. Likewise, the abbreviation ""ECE"" has never been defined, nor is any explanation given as to what it is. Yet it is one of the pivotal metrics considered in this line of work.","4: ""Further details [...], which we consider, [...]"" - which → that; extraneous commas Pg.",342,0
ICLR_2022_2475,ICLR_2022,"The algorithm is built on empirical observations of a specific image classification workload. The experiments only include image classification workloads, so whether this algorithm generalizes to other tasks (e.g. NLP tasks) is an open question.
Many of the design decisions made for the algorithm seem arbitrary, and there are many additional hyperparameters that seem important to tune. The method 1) approximates the full-batch loss using many mini-batch evaluations, 2) reuses the same step size for some consecutive steps, 3) increases the batch size with a piecewise constant schedule as training progress, and 4)uses a slightly larger step size value than the actual optimum given by the parabola by multiplying by a factor between 1 and 2. There are no ablation studies to understand which of these choices are important. Furthermore, they introduce many additional hyperparameters. In fact, looking at Section G.1 in the Appendix, it seems like LABPAL has the most number of hyperparameters.
There is no comparison to SGD with Momentum, which performs the best in the workloads considered in the experiments. Given that the method has so many additional hyperparameters, and the comparison with the best-performing optimizer is lacking, the practicality of the proposed method is questionable.
Section 4.3 (adaptation to varying gradient noise) doesn’t make much sense to me. It seems like all that is happening is the batch size is being increased. So then the comparison with other methods doesn’t seem fair, because the batch size is no longer the same.
Convergence analysis is missing.
Comments and questions:
The distinction between step size and learning rate is unclear. What exactly is the difference? Could you make things clearer in the paper? For example, it seems like in Equation 6, learning rate = \lambda = alpha * s_{min,t}/ g_t , but in the algorithm box, it seems like learning rate = learning rate * alpha.
In Section G.1, there are some parameters under LABPAL that I’m not sure were mentioned in the main paper: initial measuring step size, parabolic approximation sample step size, and approximation batch size.
Update: I have read the reviews from the other reviewers and the author responses. I agree with the authors that the paper's contributions are real and valid. However, as other reviewers also noted, I don't think the contributions are enough for acceptance, therefore, I am keeping my score. I would be excited to see a future version of the paper that has a wider range of experiments (maybe showing that the properties also hold for other deep learning tasks).","3) increases the batch size with a piecewise constant schedule as training progress, and",343,1
ICLR_2022_1905,ICLR_2022,"Weakness: 1.The author claim that ‘The observation consistently shows that only parts of subdivision splines are useful for decision boundary; and the goal of pruning is to remove those (redundant) subdivision splines and find winning tickets.’, however, in theoretical part, the author didn’t provide how the proposed algorithm in detail to remove the subdivision splines. Will the algorithm need extra computation cost for such space partition building? 2. When the author introduces the proposed algorithm, the author didn’t analysis if such method has the same convergence guarantee as Lottery Ticket Hypothesis. If so, what is the bound of the error probability? 3.In the experiment, the author didn’t consider Vision Transformer, which is an important SOTA model in image classification. And it is unsure if such technique is still working for larger image dataset such as ImageNet. Will the pruning strategy will be different in self attention layers?","2. When the author introduces the proposed algorithm, the author didn’t analysis if such method has the same convergence guarantee as Lottery Ticket Hypothesis. If so, what is the bound of the error probability?",344,0
ICLR_2022_2182,ICLR_2022,"Grammatical Issues: A few grammatical issues throughout, it doesn't detract too much but occasionally causes a hiccup in the flow when reading, so please fix these.
Method Clarity:
The paper figures do not explain the method too well. Figure 2 is a decent figure, but the caption does not really explain the right half, nor does it draw any parallel between the two parts of the figures. I think this figure should be simplified, labelled, and used to better show how the reward redistribution using conservation score mechanism works in the biological sequence alignment case, and in the reward redistribution case. Figure 3 is nice, but it should have some text explaining things better in the caption. As the actual explanation for how the method + alignment strategy works is very complicated, these figures are critical for showing it clearly at a higher level.
Furthermore, there should be some more intuition presented earlier about why sequence alignment is easier than RUDDER's LSTM.
In Section 3, ""Reward Redistribution by Sequence Alignment,"" to someone unfamiliar with biological alignment techniques, you should explain why alignment will intuitively help first, before diving into details.
In fact, in general, I think at least to me the amount of detail given to the alignment explanation should be redistributed for a conference like ICLR. The authors should rewrite the method section to have far fewer details (except those explicitly needed) about the alignment algorithm itself, this should all be in the appendix. Instead, the entire reward redistribution scheme should be used to explain both intuitively and in detail how this 1) encourages alignment of similar trajectories in terms of states and actions, 2) better alignment scores result in a better reward redistribution scheme, and 3) is far easier to learn with few demonstrations than RUDDER's LSTM. Of course, some details should be kept but in general I think there is too much detail placed in the nitty-gritty of how alignment is done in the main paper.
Experiments: To me, the experimental setup seems valid. But along the same lines as what was stated above, there should be a bit more analysis in the main text about why Align-RUDDER performs the way it does in comparison to the other methods. Questions
Appendix page 33: ""We...transform images into feature vectors using a standard pre-trained network,"" what standard pre-trained network was used?
Minor Issues
Contribution 2 and 4 in the intro are saying basically the same thing
""Sub-tasks"" in page 5, is this meant to have a new line before and be bolded instead?
Page 7, describing the hierarchical setup, ""more details can be found in the appendix,"" please link an appendix section for easier referencing here
The reference to learning methods should be a clickable link back to that subsection describing the learning method (i had forgotten the learning methods by the time I reached page 6 where it's mentioned in the experiments)","1) encourages alignment of similar trajectories in terms of states and actions,",345,1
ICLR_2022_1887,ICLR_2022,"- My main issue with this paper is that its two main contributions (i.e., [1] proposing generative classification for class-incremental learning and [2] proposing an online variant of NCM classification for class-incremental learning) have been recently proposed by two other papers, but this paper does not discuss, compare against or cite either of them. Specifically, Van De Ven et al. (2021, CVPR-W; https://arxiv.org/abs/2104.10093) argued for addressing class-incremental learning using generative classification, and De Lange & Tuytelaars (2021, ICCV; https://arxiv.org/abs/2009.00919) proposed an online version of an NCM-based class-incremental learning method. - Another relevant paper that would have been good to discuss (and compare against) is Lomonaco et al. (2020, CVPR-W; https://arxiv.org/abs/1907.03799), which also proposes an online class-incremental learning method that addresses the logits bias problem. - It is unclear to me exactly what the authors mean by “online continual learning”. Does it mean that there are no clear task boundaries, or does it mean that task boundary information is not provided to the network? At the top of section 4.1, it is stated that “task boundaries are not informed during training”. However, it seems to me that because class labels are provided, task boundaries can trivially be derived? It is also unclear to me why task-incremental learning cannot be performed in the online setting? (As claimed in the introduction.) - I’m not sure how relevant the motivation of generative classification as a more efficient method in the low data regime is for this paper. My feeling is that for all problems considered in this paper, if the data is provided to the algorithm in an i.i.d. stream, the discriminative classifier would perform better than the generative classifier.
Minor comments: - An accuracy of 15% on CIFAR-100 or miniImageNet, although perhaps better than the compared methods, is not of any practical value for real-world applications. Perhaps this performance improvement is of interest from an academic perspective, but it does not seem to be of value for any practical applications. I think it would be good to discuss this. - What is meant by “virtual catastrophic forgetting”? It would be good to expand a bit on that.","- My main issue with this paper is that its two main contributions (i.e., [1] proposing generative classification for class-incremental learning and [2] proposing an online variant of NCM classification for class-incremental learning) have been recently proposed by two other papers, but this paper does not discuss, compare against or cite either of them. Specifically, Van De Ven et al. (2021, CVPR-W; https://arxiv.org/abs/2104.10093) argued for addressing class-incremental learning using generative classification, and De Lange & Tuytelaars (2021, ICCV; https://arxiv.org/abs/2009.00919) proposed an online version of an NCM-based class-incremental learning method.",346,0
ICLR_2022_3217,ICLR_2022,"-- The technical novelty of this paper is limited. The distribution based data augmentation originates from the ISDA method, and the paper extends it to sample-specific fashion. The extension itself is a simple regression task to estimate the mean and covariance. The memory mechanism is also a well-known technique in literature.
-- A few key assumptions this paper makes don’t have a theoretical justification. For example, 1) given that mini-batch is random, how to justify the ground truth mean and covariance are reliable GT for training ? 2) Why does learning STG on head classes generalize well on long-tail classes? 3) For the class-specific token, is one single vector sufficient for each class with large variances in the semantic space ? Isn’t such one holistic vector strategy conflicting with the whole sample-specific assumption this paper rests upon.
-- Some key experiments are missing. 1) While sample-specific learning is appealing, a natural question arises on how the method works comparing with the region-specific learning (i.e., grouping similar samples as a set (smaller than a class) and learning a transformation for it). 2) The whole idea of feature augmentation and memory mechanism is related to the self-attention mechanism in transformers. How does the method work compared with attention based long tail classification methods in literature? e,g., Zhu et al. Inflated Episodic Memory with Region Self-Attention for Long-Tailed Visual Recognition. CVPR 2020. 3) Is the sample-specific strategy hurting accuracy on the head classes ?",2) Why does learning STG on head classes generalize well on long-tail classes?,347,0
ICLR_2022_850,ICLR_2022,"The theoretical analysis for the partially observable setting is lacking. Since the paper looks at reactive or memoryless policies, it cannot make any theoretical claims about optimality preservation in the partially observed case (see also detailed comments). While analyzing the fully observable case is valuable, the paper could be much stronger with an appropriate analysis of the partially observable case.
The paper should make a stronger link between the theory and empirical results. Now, all experiments are in domains that break some of the assumptions for the theory. The paper could benefit from experiments also in a simpler toy example where hypotheses could be verified. The connection of the policy sharing and communication aspects of the paper could also be clearer. See below for further comments.
The paper should discuss the relation of the homogeneous MG to other formulations where agent identity does not affect the reward or transition, or policies are averaged or a consensus is condiered, such as 1) Collective Dec-POMDPs, Nguyen et al., ""Policy gradient with value function approximation for collective multiagent planning"", NIPS 2017, or 2) Yang et al., ""Mean Field Multi-Agent Reinforcement Learning"", ICML 2018, to mention a couple of examples.
Detailed comments
The connection of the policy sharing and communication aspects of the paper could be clearer. I think both are useful topics to consider, but I think I am missing why joint treatment in a single paper for both is helpful. Maybe the authors could help clarify this for me.
In a standard Markov game (and the cooperative MG defined in the paper), the state is observable by all the players. Why do we need to postulate the existence of a common observation space in Definition 1 (iii)? Is it not enough to work with the permutation invariance w.r.t. state to prove Theorem 1 (for state-based policies)?
The examples in 3.1 give valid reasoning about the practical importance of the common observation space, but I find the overall presentation somewhat confusing given that at the start of Sect. 3 it is stated the section only concerns the fully observed case. The same conflation of full and partial observability also appears in 3.2. To be clear, I think the results are correct for the fully observed case, but the presentation is confusing and should be improved.
The paper should be very careful about treatment of and claims about the partially observable case, since the analysis is not performed in the Dec-POMDP framework. The paper cannot make any claims to preserve optimality in the partially observable case, simply because it already foregoes optimality by restricting to memoryless policies (specifically, policies that depend only on the latest observation, not the history of observations/actions). Perhaps the best memoryless policy is preserved under the conditions given in the paper, but this may be arbitrarily worse than an optimal policy.
I am not an expert in networked MARL methods, but I could not quickly decipher how Theorem 2 differs from Theorems 4.6 and 4.7 in Zhang et al. (2018), or if this is a statement of the same result in a different form. Additional comments here would be helpful. Why is the convergence result for homogeneous MGs not a straightforward implication of the convergence proofs for general MGs in Zhang et al.?
A figure would be quite helpful to better understand the overall flow of the practical algorithm in Sect. 4.
On the experimental results:
The empirical evaluation is nice in the sense that it considers the MPE domains which are standard in MARL. Nevertheless, focusing on MPE also means the findings are demonstrated in a single class of domains only.
In all the domains considered, the full observability and other assumptions required in the theoretical analysis are broken to some degree. Why not include experiments in some toy domains where the implications of the theory can be demonstrated when the conditions required are indeed satisfied? Then, the claim ""we develop the first multi-agent actor-critic algorithm for homogeneous MGs that enjoys asymptotic convergence guarantee..."" can truly be verified.
It is surprising that the proposed method with communication does not do any better than random in Prey 30 (Fig. 1, rightmost plot). Why is this the case?
I am a puzzled by what Fig. 6 is telling us. I would assume that naturally throughout the course of the training, the need for consensus-making decreases as the agents converge. This is even encouraged when communication has a cost (as defined end of Sect. 4). Is this the message of Fig. 6?
Minor: In the statement of Thm. 1: S \times O should be O \times A ? In several places throughout the paper, curly braces denoting sets are not displayed, e.g. 0, 1 should be {0, 1}. Quality of figures should be improved overall, e.g., including axis labels and cleaner formatting
Comments added after the rebuttal, edited after author response:
I thank the authors for addressing my concerns and providing a rebuttal. I am generally satisfied with the response.
On the positive side, the rebuttal addresses all of my minor concerns. The new experiment in a toy domain is likewise a welcome addition. The rebuttal makes clear the differences to collective Dec-POMDPs and similar models.
The rebuttal does provide clarity regarding the setting addressed in the paper (communication in Dec-MDPs). The authors' comment also further clarifies this and it can be argued the formulation in the paper is sufficient, although it can be further improved by including the table. My main concerns have been addressed, and I increased my score to reflect this.","1) Collective Dec-POMDPs, Nguyen et al., ""Policy gradient with value function approximation for collective multiagent planning"", NIPS 2017, or",348,1
ICLR_2022_1344,ICLR_2022,"Weakness: 1. Many technical details are missing, making it like a preliminary report. - In Sec. 4, when introducing estimating users’ preferences, the inference process is demonstrated in detail. On the contrary, descriptions about training a user predictive model given the historical data are vague, which is equally important. - For example, how to obtain $P(z^{\pi '}H z^{\pi }\text{T+1}) i n S e c .4 .1 ( E s t i m a t i o n u n d e r k n o w n i n t e r n a l s t a t e d y n a m i c s ) ? − E q . 3 s e e m s t o h a v e a t y p o , "" = "" a n d "" \approx "" m i g h t s w a p t h e i r p o s i t i o n s ? − A u t h o r s h a v e n o t r e l e a s e d t h e i r e x p e r i m e n t c o d e , w h i c h m a k e s i t d i f f i c u l t f o r o t h e r c o l l e a g u e s t o f o l l o w . 2. E x p e r i m e n t d e t a i l s a r e a l s o n o t c l e a r . F o r e x a m p l e , − I n g r o u n d t r u t h h u m a n d y n a m i c s , g i v e n b_t^H(s) a n d
u_t$, how is the distribution over users’ next timestamp preferences generated? - Does the counterfactual preference estimation used in evaluating the possible influence of recommendation policies? - What is the myopic method used in experiments? - What is the physical meaning of the “SUM” (of engagement)? Authors claim in the abstract that their framework can avoid manipulative behaviors but still generate engagement. What is the meaning of generating engagement? It seems to me that this framework can improve engagement under the initial or naturally shifted user preferences according to Table 1/2. 3. Presentation quality can be improved by polishing up the wording and rearranging the figures and tables in Sec. 6&7.","- In Sec. 4, when introducing estimating users’ preferences, the inference process is demonstrated in detail. On the contrary, descriptions about training a user predictive model given the historical data are vague, which is equally important.",349,0
ICLR_2022_486,ICLR_2022,"1. It seems that the loss (1) is an incremental formulation by combining existing low-dimensional embedding methods and the simplified topological loss function (2). From (3), the topological loss of this paper is constructed from the embeddings instead of the input data. However, this may be thought as a straightforward extension. 2. Authors did not explain why only the regular part of (2) is used. It looks like only partial power of the topological optimization is considered by this work. Some explanation for choosing only the regular part is needed. 3. The claim that “a topological model that is naturally present in the data should be represented well by many subsets of the data” is not quite convincing. Taking the clustering problem as an example, this claim might not true if there are many small clusters when uniform sampling is used. Since it Is the key assumption for (4), authors might want to specify the limitation of the loss (4). Even for the experiments using (4), the analysis on the impact of f_s and n_s is not studied. 4. Experiments are conducted on both synthetic data and real data, but only the very basic approaches are compared. It is more like the ablation study of the proposed model. Authors need to compare the results obtained by the proposed model with the state-of-the-art methods. At least, the results from the paper using the same real data should be reported as the most relevant baselines. 5. From the visualization results, it seems that top. optimization embedding is very similar to the top. regularized embedding on most of the data sets expect Figure 7. Does it mean top. loss is sufficient enough to obtain relatively good results?","4. Experiments are conducted on both synthetic data and real data, but only the very basic approaches are compared. It is more like the ablation study of the proposed model. Authors need to compare the results obtained by the proposed model with the state-of-the-art methods. At least, the results from the paper using the same real data should be reported as the most relevant baselines.",350,0
ICLR_2022_513,ICLR_2022,"i) The authors have shown all their results in section 3 by only blackenning rather than any other baseline value. Blackenning is majorly known to be an incorrect replacement for the removed pixels [a, b]. Other common methods are blurring, graying (using the mean values) or random noise. It would be interesting to see if at all these baselines also show the same effects of missingness bias. ii) The authors have used LIME as the case study for generating explanations [sec 4]. Some relevant approaches (e.g., [a, b, c]) provide much smoother saliency maps which might be worth trying especially to check inconsistency and indistinguishability of explanations as well as the effect dependence on maze/crossword like pattern [Figs. 2 and 3]. iii) It would have been good if authors could show the results on more models like EfficientNet or even VGG instead of only ResNet. This would show that this problem exists in CNNs in general and is not specific to only ResNet. iv) (Fig. 8) Does it make sense to find a measure of agreement for different baseline colors in ViT. These baseline colors are not used at all while making ViT prediction for these masked images. So, it is quite natural that all these so called ViT variations will give exactly the same features with same importance for all these baseline methods. Also it would be good to know what are the 8 different baseline colors? v) Instead of being an weakness, this is rather an academic query: There is no major alternative to the current method followed for removal of regions in images for CNNs. This problem is not talked of much. After analyzing an already existing problem, it would have been great if the authors would have talked about any method to overcome it. Shifting to transformer based architecture may not be a solution for explaining and debugging CNNs.
[a] Petsiuk et. al., RISE: Randomized Input Sampling for Explanation of Black-Box Models, BMVC 2018 [b] Fong et. al., Interpretable Explanations of Black Boxes by Meaningful Perturbations, ICCV 2017 [c] Fong et. al., Understanding Deep Networks via Extremal Perturbations and Smooth Masks, ICCV 2019",2 and 3]. iii) It would have been good if authors could show the results on more models like EfficientNet or even VGG instead of only ResNet. This would show that this problem exists in CNNs in general and is not specific to only ResNet. iv) (Fig.,351,1
ICLR_2022_2327,ICLR_2022,"weakness] Experiment:
The experiment setting and comparison need to be clarified:
In Section 4.1, the paper claims that the proposed methods use 8 style reference images and 8 content reference images. In contrast, in Fig 4, the number of styles and content reference images are used for the compared methods and the proposed model should be explained.
Which fonts does the model use as query images and content reference images?
More evaluation metrics for image generation should be used, such as FID、LPIPS.
Lacks comparison to latest font generation methods:
[1] Few-shot Font Generation with Localized Style Representations and Factorization. S Park, S Chun, J Cha, B Lee, H Shim. AAAI 2021.
[2] DG-Font: Deformable Generative Networks for Unsupervised Font Generation. Y Xie, X Chen, L Sun, Y Lu. CVPR 2021.
[3] RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering. Y Huang, M He, L Jin, and Y Wang. ECCV 2020. Modeling:
4. The query image is one of the content images. What attention does content glyph attention learn? I would like to see the visualization of the attention map for both style glyph attention and content glyph attention.
5. The writing is not clear. In Sec 3.3.3, more explanation and notation should be clarified. Also, I would like to know how many binary classification heads are used for content and style discriminators.",4. The query image is one of the content images. What attention does content glyph attention learn? I would like to see the visualization of the attention map for both style glyph attention and content glyph attention.,352,0
ICLR_2022_1107,ICLR_2022,"1. We believe that the example in Figure 1 is not comprehensive and does not explain the difference between completeness and soundness well. We want to know what will happen if the saliency maps of the category labels do not overlap? 2. Section.3 mentioned that this paper uses the images of the training set to fill the pixels outside the saliency map area S. Wouldn't other objects in the background area interfere with the result unpredictably? Perhaps more explanation should be given here. 3. From the results in Table 1, compared with the method in Phang et al.(2020), the proposed method has no special advantages. 4. The data in Table 2 confuses me. The completeness of different methods is very different, but the soundness is very close, especially the second column of data. How should we understand this result?",1. We believe that the example in Figure 1 is not comprehensive and does not explain the difference between completeness and soundness well. We want to know what will happen if the saliency maps of the category labels do not overlap?,353,0
ICLR_2022_2384,ICLR_2022,".
The paper is not as self-contained as it could be, i.e. it is near impossible to understand without reading the core referred literature first. It would help readability a lot if core intuitions and concepts would be briefly discussed when introduced. E.g. when introducing MCR2 in equation 4, it would help the reader a lot what the target of each of the two terms is. Another example, after equation 6 it is stated that this measures the volume - why? (if this is not easy to explain, then there should be a reference to where this is explained). The math also needs more clarification, for example what is the union in equation 6 (Z and \hat Z are both \in Rˆ{dxn}, or)? Is the \Delta R in equation 4 and equation 6 really the same (or is it semantically overloaded)? writing (and logic of the writing) needs to be worked on.
Experiments. The comparisons on generative models in Table 1 are done with rather old baselines (all being at least 4 years old). E.g. DCGAN (Radford 2015) could be replaced by StyleGAN2/3 etc (similar for the VAE methods). It should also be reported what the training time for the method on the datasets is, and how it scales with dataset size and resolution. Also, it should be discussed that the model only roughly encodes the semantics, and sometimes disregards even color (e.g. as seen in Figure 5 where the red car turns yellow). In Figure 14 in the abstract it can be seen that the the model seems to sometimes collapse inputs into the same output? If so this should be discussed.
Already in the Abstract it is claimed that the model learns a discriminative representation, and it is shown in Table 3 to perform in the ballpark of VAE methods on MNIST data. As the model is also trained on ImageNet (see Fig5), why is no discrimnative comparison performed on ImageNet? It'd be interesting to see if the model can scale up to more classes and more complex datasets (what is the scaling behaviour theoretically of the model in the latent space? Does the dimensionality need to be proportional to the number of classes to ensure the possibility of orthogonality?) .
Minor Issues.
The Introduction should start with a section contextualizing the work, i.e. review of context and background, what has been done by others broadly in the field that led to the current work.
Equation 2,3 etc. - the result of function g is not a tuple (X, \hat X). The figure should be changed.
After eq. 3 it is written ""later studies [...] surrogate to earth mover's distance"". I think this is wrong. The original GAN paper showed that GAN's minimize Jensen-Shannon distance, and Arjovsky et al showed how to build the WGAN that operates on earth mover's distance.
Also on pg 2. - Combination of AE and GAN: "".. started with somewhat different motivation, they have evolved..."". While true that both methods are quite different, as both are generative models, it was this motivation (generative modeling) that they become successful in modeling of real-world data. So, the logic behind this sentence is confusing.","- the result of function g is not a tuple (X, \hat X). The figure should be changed. After eq. 3 it is written ""later studies [...] surrogate to earth mover's distance"". I think this is wrong. The original GAN paper showed that GAN's minimize Jensen-Shannon distance, and Arjovsky et al showed how to build the WGAN that operates on earth mover's distance. Also on pg 2.",354,0
ICLR_2022_852,ICLR_2022,"1. Spatial graph attention and spatial convolution indeed improve the model’s ability to extract structural information from input graphs, but these modules are not innovative enough. Actually, spatial graph attention is just a multi-head attention mechanism applied to graphs and spatial convolution is a Euclidean distance version of GNN’s convolution operation. Some new modules are expected to be employed by the model to make it more powerful. 2. As is seen from the experimental results, the docking scores and SA value are better than other existing algorithms indeed. But the model’s performance on other metrics is unsatisfactory and particularly, Diversity is even the worst among algorithms in Table 1. Although this has been explained in the paper as a general drawback of fragment-based algorithms, more optimizations may need to be realized. 3. Some statements in the paper may make readers confused, which are listed below. Equation 3 and 4 don’t share a consistent representation of the element in the adjacency matrix. Table 1 lacks some necessary annotations like what is the definition of Diversity and what is the meaning of the values in bold. The right plot in Figure 4 also lacks some necessary explanations and this plot seems to be redundant now.","1. Spatial graph attention and spatial convolution indeed improve the model’s ability to extract structural information from input graphs, but these modules are not innovative enough. Actually, spatial graph attention is just a multi-head attention mechanism applied to graphs and spatial convolution is a Euclidean distance version of GNN’s convolution operation. Some new modules are expected to be employed by the model to make it more powerful.",355,0
ICLR_2023_4310,ICLR_2023,"1. A major concern is the limited novelty. It seems that the key contribution of the paper is the pre-trained mechanism applied to the task of few-shot graph learning. While the main idea seems plausible, it is not exciting and feels a bit complicated. 2. The theoretical explanation of the success of the proposed approach should be discussed. 3. Some experiments are not convictive enough. Concretely, more latest baselines should be considered for comparison. Recommendation",2. The theoretical explanation of the success of the proposed approach should be discussed.,356,0
ICLR_2023_604,ICLR_2023,"1. The subsampling is achieved with a narrow objective of preserving the number of communities (rather than the content/quality of community structure which is possibly crucial to other global network summaries). A discussion on the latter and specifically the importance of the number of communities is currently not convincing. 2. If the observed graph is assumed to be generated from a stochastic blockmodel, then the true number of communities is fixed and the problem makes sense. However, this is an assumption and may not be valid in practice. In such cases, estimating communities is simply a way to approximate the more general network generating process (e.g. Olhede and Wolfe, 2014) and a range of possible values for M are feasible (to lead to a reasonable approximation). This must be clarified in the discussion/introduction. 3. I would expect the performance of the proposed approach to additionally depend on the edge density of the network. This is an important feature as real networks often get sparser with increase in the number of nodes. This not discussed in the paper - neither in the theoretical results nor in the implementation. 4. Based on the OR curvature theory, are there any other network summaries concerning communities that the sub sampling procedure may possibly preserve?",1. The subsampling is achieved with a narrow objective of preserving the number of communities (rather than the content/quality of community structure which is possibly crucial to other global network summaries). A discussion on the latter and specifically the importance of the number of communities is currently not convincing.,357,0
ICLR_2023_941,ICLR_2023,"Weakness:
The claim in the paper about “transfer of representations” seems general (and tad too strong) however this is only evaluated over vision heavy transfer learning benchmarks. It is not clear if the behaviour would be similar for transfer learning on vision and language related benchmarks.
The salient observations (relating to language - especially descriptiveness and variability) in the paper are perhaps fairly trivial considering the vast amount of similar work from the area of vision and language and semiotics [1, 2, 3, 4, 5 inter alia].
In general, the paper lacks empirical rigour, the paper contains a list of experimental interventions, but none are clear (expanding as the next point).
[1] Automatic description generation from images: A survey of models, datasets, and evaluation measures. Bernardi et al. 2016
[2] On the use of human reference data for evaluating automatic image descriptions. van Miltenburg. 2020
[3] Ways of seeing. Berger. 2008
[4] Semiotics: the basics. Chandler. 2007
[5] Underspecification in Scene Description-to-Depiction Tasks. Hutchinson et al. 2022",2007 [5] Underspecification in Scene Description-to-Depiction Tasks. Hutchinson et al. 2022,358,1
ICLR_2023_1195,ICLR_2023,"- The assumption that a set of analytical derivative functions is available is a very strong hypothesis so the number of cases where this method can be applied seems limited. - The high dimensional tensor can be also compactly represented by the set of derivative functions avoiding the curse of dimensionality, so it is not clear what is the advantage of replacing the original compact representation by the TT representation. Maybe the reason is that in TT-format many operations can be implemented more efficiently. The paper gives not a clear explanation about the necessity of the TT representation in this case. - It is not clear in which cases the minimum rank is achieved by the proposed method. Is there a way to check it? - In the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon? - The last two bullets in the list of main contributions and advantages of the proposed approach are not clear to me (Page 2). - The method is introduced by an application example using the P_step function (section 2.2). I found this example difficult to follow and maybe not relevant from the point of view of an application. I think, a better option would be to use some problem easier to understand, for example, one application to game theory as it is done later in the paper. - Very relevant ideas and results are not included in the main paper and referred instead to the Appendix, which makes the paper not well self-contained. - The obtained performance in terms of complexity for the calculation of the permanent of a matrix is not better than standard algorithms as commented by the authors (Hamilton walks obtained the result with half of the complexity). It is not clear what is the advantage of the proposed new method for this application. - The comparison with the TT-cross method is not clear enough. What is the number of samples taken in the TT-cross method? What is the effect to increase the number of samples in the TT-cross method. I wonder if the accuracy of the TT-cross method can be improved by sampling more entries of the tensor.
Minor issues: - Page 2: “an unified approach” -> “a unified approach” - Page 2: “and in several examples is Appendix” -> “and in several examples in the Appendix” - In page 3, “basic vector e” is not defined. I think the authors refers to different elements of the canonical base, i.e., vectors containing all zeros except one “1” in a different location. This should be formally introduced somewhere in the paper. - Page 9: “as an contraction” -> “as a contraction”","- The high dimensional tensor can be also compactly represented by the set of derivative functions avoiding the curse of dimensionality, so it is not clear what is the advantage of replacing the original compact representation by the TT representation. Maybe the reason is that in TT-format many operations can be implemented more efficiently. The paper gives not a clear explanation about the necessity of the TT representation in this case.",359,0
ICLR_2023_360,ICLR_2023,"1. The final step is based on SVR method, so how to ensure that the shapes obtained have good details? As shown in the experiment part, the image does not have a lot of fine details. 2. Although using more and richer texts to obtain the final model, it seems that the method cannot generate results that not in the training domain.","2. Although using more and richer texts to obtain the final model, it seems that the method cannot generate results that not in the training domain.",360,0
ICLR_2023_3520,ICLR_2023,"1. The motivations are litter confused. As we all know, the memory mechanism and the transformer mechanism are time-consuming, but this work aims to achieve a real-time video inpainting performance with such two mechanisms. 2. In the Introduction section, the authors are suggested to summarize the technical novelties of this method. 3. It seems that the transformer mechanism has been widely used for video restoration. The authors should compare the developed method with them. 4. It is possible to extend the developed online, real-time memory-based transformers in this work for other video restoration tasks? 5. In Table 1 and Table 2, the authors are suggested to compare the model size of different methods.","2. In the Introduction section, the authors are suggested to summarize the technical novelties of this method.",361,0
ICLR_2023_3031,ICLR_2023,"- The technique seems to be in its early stages and it seems as if tuning needs to be performed for every new dynamical system and setting. First, the different training phases are difficult to follow and makes me wondering if the pipeline is robust enough. Also, please explain how was chosen to use a 4-layer U-Net for the Lorenz-63 and only a 3-layer U-Net for the Lorenz-96. - Network structure: it is not clear which losses are minimized at the different phases, in particular, the perturbator+flow operator is not clear. From eq. 11, it looks as if the dynamics and the data fidelity losses were applied on the same current state x_hat, while my understanding was that L_rec was calculated on the perturbator's output while L_dyn was calculated on the flow operator's output. If the hybrid loss is calculated on the final output of the 2 blocks, I don't se how we can enforce the decoupling of the two goals as stated. - It is difficult to understand the size of the input data and latent data. In particular, please give axis labels on Figure 1 images, as it first looks like a 2D spatial problem, while later it is explained that the input is of size time and location. Please also clarify what does T represent in Figure 2: I guess the time dimension, and in this case where is the location dimension? - Please explain best the following sentence: 'For the case in which the prior dynamics are unbiased': in a real setting, how can we know if the prior dynamics are biased or not? - 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive? - Why stopping at 8 and 10 blocks for the experiments, while it looks as if the results are always improving with more blocks?
Typos: Specifically, The perturbator
The perturbator uses the observations and labels it has learned to perturb the reconstructed states to make it deviate from the original flow --> please rephrase
Figure 3: the color do not match Figure 1, as here the colors are also linked to training/no training. Maybe use another sign to indicate training/no training, as a red line surrounding the box.","- The technique seems to be in its early stages and it seems as if tuning needs to be performed for every new dynamical system and setting. First, the different training phases are difficult to follow and makes me wondering if the pipeline is robust enough. Also, please explain how was chosen to use a 4-layer U-Net for the Lorenz-63 and only a 3-layer U-Net for the Lorenz-96.",362,0
ICLR_2023_4216,ICLR_2023,"It is stated that FP32 latency is not well-correlated to INT8 latency. That's plausible but goes against the conventional wisdom on vector processors where a reduction in bit-width typically leads to a linear increase in SIMD size, and a linear drop in memory bandwidth requirements. Have you investigated why this is the case in your experiments?
Something is wrong with Fig. 3 It suggests that on pixel-4 a conv1x1 with 64 channels is just as fast as a conv1x1 with half the flops (32 channels). I can confirm that this doesn't line up with my own measurements on that device. If I understand Fig. 3 correctly, it suggests that you get higher speedups with more channels monotonically. How is this possible? The SIMD size of snapdragon CPUs at 8-bit is just 16, meaning it can do a maximum 16 operations per cycle, so I would expect a sawtooth curve with period of 16 in Fig. 3.
Would it be possible to see your code during the review process. I am actually finding it hard to trust some of the results, but of course, I may be wrong. It would be good to see the code to either refute or confirm my concerns. It would also be good to get a sense of how optimized your code is? Some of the unexpected results may be simply because of unoptimized code. I'd be happy to increase my score if there are reasonable replies to the concerns above.","3. Would it be possible to see your code during the review process. I am actually finding it hard to trust some of the results, but of course, I may be wrong. It would be good to see the code to either refute or confirm my concerns. It would also be good to get a sense of how optimized your code is? Some of the unexpected results may be simply because of unoptimized code. I'd be happy to increase my score if there are reasonable replies to the concerns above.",363,0
ICLR_2023_2621,ICLR_2023,"Weakness: There are several points in the article that confused me, as follows:
In Table 4, the number of parameters of Pretraining + BCT is 43.59, while the number of parameters of Pretraining + BCT + Fix Trunk is 2.29. It seems unreasonable.
As can be seen from Table 4, it is the model distillation operation that provides the largest performance improvement across multi-resolution and same resolution task. What puzzles me is: 1) whether distillation is capable of such a significant improvement and 2) if the distillation operation can improve to such an extent, then the innovation points claimed in this paper may not be reliable and more gains come from the distillation operation.
The task of this paper is to obtain a better cross-resolution representation, and I think it may be more intuitive to visualize it using t-SNE.",1) whether distillation is capable of such a significant improvement and,364,1
ICLR_2023_802,ICLR_2023,"- There are no experiments to support the claim that A-DGN can specifically alleviate/mitigate oversquashing.
- There are no experiments to support the claim that A-DGN can effectively handle long-range dependencies specifically on graph data requiring long-range reasoning.
- The poor long-range modelling ability of DGNs is attributed to oversquashing and vanishing/exploding gradients but the poor performance could also be due to oversmoothing, another phenomenon observed in the context of very deep graph networks [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, In AAAI'18].",- There are no experiments to support the claim that A-DGN can specifically alleviate/mitigate oversquashing.,365,0
ICLR_2023_648,ICLR_2023,"• There is a significant overhead introduced by the dual memory mechanism so it will be fair to compare with other single memory approaches by maintaining a fixed overall memory.
• In general, the related works need to be more comprehensive and explicit in explaining how the proposed work is different from the literature and compare if needed [6] : The idea of error sensitivity-based modulation seems to be explored before in the local learning-based approaches [1,2,3]. Why only compare with replay-based approached when comparing the performance. Other non-replay-based approaches have shown superior performance and have the advantage of not requiring the memory buffer [2,4].
• Not clear if the improved results hold when a larger memory buffer (eg., 5k) is used.
• Only a single metric (accuracy) is used for comparison. Other metrics such as average forgetting [5] shall be used to
• Effect of task sequence not considered! do the results hold when the task sequence is changed?
• Why does ESMER have lower accuracy (on test data) for the task it is trained on? As seen in figure 3 diagonal elements.
• The ablation does not show the effect of just keeping the error sensitivity modulation without the semantic memory or reservoir sampling.
• Do the results on label corruption (fig2) hold for Cifar-100 data as well?
• Does this approach work in the online continual learning setting?
[1] Dellaferrera, G., & Kreiman, G. (2022). Error-driven Input Modulation: Solving the Credit Assignment Problem without a Backward Pass. arXiv preprint arXiv:2201.11665.
[2] Madireddy, S., Yanguas-Gil, A., & Balaprakash, P. (2020). Neuromodulated neural architectures with local error signals for memory-constrained online continual learning. arXiv preprint arXiv:2007.08159.
[3] Kudithipudi, Dhireesha, et al. ""Biological underpinnings for lifelong learning machines."" Nature Machine Intelligence 4.3 (2022): 196-210.
[4] Li, S., Du, Y., van de Ven, G. M., & Mordatch, I. (2020). Energy-based models for continual learning. arXiv preprint arXiv:2011.12216.
[5] Mai, Zheda, et al. ""Online continual learning in image classification: An empirical survey."" Neurocomputing 469 (2022): 28-51.
[6] Pham, Q., Liu, C., & Hoi, S. (2021). Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34, 16131-16144.",• Why does ESMER have lower accuracy (on test data) for the task it is trained on? As seen in figure 3 diagonal elements.,366,0
ICLR_2023_4878,ICLR_2023,"1. The proposed sparsity technique seems to be limited in scope. While it has been shown to work well for a particular misinformation detector, there is no guarantee that it will work well for other networks. 2. Though the experimental results are encouraging, it is not clear why the simple pre-fixed should work well. No explanation is provided. 3. The dataset used for evaluation is not a widely used dataset. Basically only previous work has used it and this work is an extension of the previous work. 4. There is no novelty in the methodological aspects of the work.
Questions for the authors: 1. Why do misinformation detection models need to be deployed on smartphones? Can you give a real-world use case? 2. How does the proposed sparsity pattern compare with masks that are inferred from a pretrained model or learned during training? 3. Wu et al use event level detection results for document: ""these two tasks are not mutually independent. Intuitively, document-level detection can benefit from the results of event-level detection, because the presence of a large number of false events indicates that the document is more likely to be fake. Therefore, we feed the results produced by a well-trained event-level detector into each layer of the document-level detector."" Their ablation study (Table 4) shows that event level detection is crucial for getting best document level results (86.76 vs 84.57 F1). This seems to go against your claim that document level detection model needs to be separated from event level detection model. 4. Results in Table 1 (doc classifier exit #4) doesn't match with those of Wu et al. Why is sparse model giving better results than unpruned? 5. HSF, GROVER and CDMD are fake news detection algorithms whereas MP and LTH are sparse n/w methods. Which fake news detection algorithm is used in conjunction with MP and LTH? (Table 4) 6. Doc classifier exit #1 is nearly as good as exit # 2. And there's not a whole lot of difference b/w exit #1 and exit #4. Is this because document level event detection is a easy problem or something to do with the dataset? 7. Why no event level results for 90% sparsity in Table 4? Do the event level results degrade more drastically than even level as the sparsity is increased? 8. Why no results for a random sparsity pattern? That would be a good baseline for relative assessment of sparsity patterns. 9. In Tables 4,5 and 6, why is SMD 90% sparsity better than SMD 50%? 10. Looks like all sparsity patterns do almost equally well. No insight provided as to what is happening here. Is this something unique to the sparsity detection problem or is this true for GNN in general?
Section 4.3: presentation bits --> representation bits",4. Results in Table 1 (doc classifier exit #4) doesn't match with those of Wu et al. Why is sparse model giving better results than unpruned?,367,0
ICLR_2023_1498,ICLR_2023,"1. All the proposed manipulation tasks are based on robotic arms and lack support for other actuators (e.g. Dexterous hands). 2. Since different simulators have different parameters that may drastically affect the overall performance, please include the details of parameters for each simulator (e.g. Did you use the GPU pipeline in IsaacGym or just used the CPU pipeline, the substeps for physics simulation, the number of the total vertex in the simulation). 3. The performance comparison is carried out on RGBD input settings. The performance of pure state input and point-cloud input is not reported. 4. The effectiveness and accuracy of demonstration conversion are neither discussed in detail nor measured quantitatively. 5. Point-cloud observations in some tasks contain ground truth segmentation, which cannot be easily obtained in the real world. This barrier for sim-to-real transfer could be avoided with a better observation design.",3. The performance comparison is carried out on RGBD input settings. The performance of pure state input and point-cloud input is not reported.,368,0
ICLR_2023_3261,ICLR_2023,"The authors have missed important previous work that introduces such techniques. Namely, lifted networks [1] propose a two-stage optimization scheme by introducing latent variables and matching the output of each later with the latent variables. More importantly, LocoProp [2] introduced the idea of setting layerwise targets and iteratively minimizing layerwise losses to match the targets. The method proposed in this paper is a special case of LocoProp where 1) only a single target is formed for the last layer via a Newton step (this was already discussed in LocoProp in terms of a natural gradient descent (NGD) targets, and the authors show that the final update wrt to the parameters corresponds to NGD), 2) the loss is set to be the squared loss whereas LocoProp uses more advanced Bregman divergences, 3) the model parameters are updated using a single step whereas, in LocoProp, the authors try multiple steps on the fixed point problem and show that the final update on the parameters corresponds to a preconditioned update.
[1] Miguel Carreira-Perpinan, and Weiran Wang. ""Distributed optimization of deeply nested systems."" In Artificial Intelligence and Statistics, pp. 10-19. PMLR, 2014.
[2] Ehsan Amid, Rohan Anil, and Manfred Warmuth. ""Locoprop: Enhancing Backprop via local loss optimization."" In International Conference on Artificial Intelligence and Statistics, pp. 9626-9642. PMLR, 2022.","1) only a single target is formed for the last layer via a Newton step (this was already discussed in LocoProp in terms of a natural gradient descent (NGD) targets, and the authors show that the final update wrt to the parameters corresponds to NGD),",369,0
ICLR_2023_3218,ICLR_2023,"weakness: Q1) The authors claim that “Most of the research is focused on adversarial attacks targeting neural network models because of the nature of their continuous learning space…” It is inaccurate statement. We study adversarial example against NN because 1) NN has been outperformed all other methods on many datasets, 2) Its performance is very high, in some cases, even outperforming human experts, 3) DNN has a lot of application and commercial values. However, decision trees are not comparable DNN in all these aspects.
Q2) The authors claim that “Tree-based models continue to be very popular (Nielsen 2016)”. 1) the reference is 6 years ago, and it is a master thesis. Please give better justification on this statement.
Q3) The authors mention that “Of the eleven datasets tested, seven shown a decrease in accuracy”. Decrease in accuracy is not necessary a problem. The question is how much.
Q4) Section 3,2 N1 is not defined although it is understandable.
Q5) Similar idea, extracting information from classifier to train an adversarial example detector has been proposed for DNN. They also have been broken. There is no theory to support that the proposed method is secure.
Q6) Since a lot of methods and tools have been developed for NN for classification, explanation and secure them, the necessity for using decision tree is weakened.","1) the reference is 6 years ago, and it is a master thesis. Please give better justification on this statement.",370,0
ICLR_2023_3147,ICLR_2023,"The most severe weakness of the paper is that the theorem and experiment did not support the motivation of the algorithm.
1. Communication-efficient. The motivation of the GLASU algorithm is to save communication. However, neither the theorem nor the experiment discuss the communication cost, which undermines the contribution of the paper.
2. Privacy is an important aspect in federated learning. But privacy is not discussed in this paper.
3. The experiment results are not clear enough. For example, in Table 2 and table 4, it is hard to see how Q affects the results. In Table 3, it is hard to see how K affects the results. The experiment results are not strong enough to support the claims of the paper.","1. Communication-efficient. The motivation of the GLASU algorithm is to save communication. However, neither the theorem nor the experiment discuss the communication cost, which undermines the contribution of the paper.",371,0
ICLR_2023_3896,ICLR_2023,"The novelty of combining three types of knowledge is limited. Introducing entity description, sub-graph and document-level graph are all leveraged in the previous papers, and the combination techniques are also not new.
More discussions are required to better understand the proposed model. For example, 1) What is the base encoder of the proposed model? or, do the authors train the model from scratch? 2) what if removing each component to see the ablation study? 3) what if comparing with more knowledge enhanced pre-trained language models?","1) What is the base encoder of the proposed model? or, do the authors train the model from scratch?",372,0
ICLR_2023_3780,ICLR_2023,"1. The motivation is unclear. The authors consider that semantics used in both synthesizing visual features and learning embedding functions will introduce bias toward seen classes. However, some methods [1][2] using semantics seem to get better results. Please compare with them and give more explanations for the motivation of the proposed paper. Moreover, it would be more convincing to add comparative experiments that use semantics but do not decouple. 2. The comparison on the results of the CUB is unfair. Most of the methods, such as FREE and f-VAEGAN-D2, utilize 312-dimensional attributes as the auxiliary semantic information. You need to experiment with attributes rather than 1024-dimensional semantic descriptors if you want to compare with these methods. 3. Some recent methods like [2] and [3] are ignored to be compared. 4. I wonder why the results are so low using only ML in the ablation experiments. The results are even lower than some simple early methods like f-CLSWGAN [4] and f-VAEGAN-D2 [5]. More explanations can be given. 5. A minor problem. In section 3.4, the authors said that synthesized features including both seen and unseen classes are used to train the final classifier. However, it seems that only the synthesized unseen features are used.
[1] Generative Dual Adversarial Network for Generalized Zero-shot Learning. CVPR 2019. [2] Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification. ECCV 2020. [3] Contrastive Embedding for Generalized Zero-Shot Learning. CVPR 2021. [4] Feature Generating Networks for Zero-Shot Learning. CVPR 2018. [5] F-VAEGAN-D2: A feature generating framework for any-shot learning. CVPR 2019.","5. A minor problem. In section 3.4, the authors said that synthesized features including both seen and unseen classes are used to train the final classifier. However, it seems that only the synthesized unseen features are used. [1] Generative Dual Adversarial Network for Generalized Zero-shot Learning. CVPR 2019. [2] Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification. ECCV 2020. [3] Contrastive Embedding for Generalized Zero-Shot Learning. CVPR 2021. [4] Feature Generating Networks for Zero-Shot Learning. CVPR 2018. [5] F-VAEGAN-D2: A feature generating framework for any-shot learning. CVPR 2019.",373,0
ICLR_2023_879,ICLR_2023,"The ablations for the different pre-training tasks in section 4.5 / Figure 6 are a bit puzzling. It does seem that the CRD task has destructive value on that particular binding affinity prediction task since: a) the performance of CRD + MLM or CRD + PPI leads to both lower performance Vs MLM or PPI alone respectively b) the performance of CRD + MLM + PPI is also lower vs just using MLM + PPI. This seems particularly important from a practical standpoint, and additional experiments are needed to confirm whether: 1) that problem applies to other downstream tasks or is just specific to binding affinity prediction — and if so, why? 2) there is something fundamentally wrong with the CRD pre-training as currently implemented? 3) there is a way to anticipate ex ante (or post fine tuning) which tokens should be used to ensure optimal task performance ?
The ablation in section in Table 3 is a bit puzzling as well: it appears that the performance of PromptProtein without layer skip is lower than the performance from the conventional MTL. Could you please explain why that might be the case? (I would have assumed intermediate performance between conventional MTL and full PromptProtein as I presume the attention masks are still used in that ablation?)
Several points (in section 4 primarily) were not fully clear (see clarity paragraph below).
The following claim in conclusion does not seem fully substantiated: “PromptProtein beats state-of-the-art baselines by significant margins”. Authors do report the relevant baselines listed in the FLIP paper [1]. But since that paper was released, several methods have shown markedly superior performance for protein modeling & achieving high spearman with deep mutational scanning assays — see for example, [2] and [3]. I would suggest adding these two baselines to the analysis or tone done the SOTA claims.
[1] Dallago, C., Mou, J., Johnston, K.E., Wittmann, B.J., Bhattacharya, N., Goldman, S., Madani, A., & Yang, K.K. (2022). FLIP: Benchmark tasks in fitness landscape inference for proteins. bioRxiv.
[2] Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B.L., Sercu, T., Lerer, A., & Rives, A. (2022). Learning inverse folding from millions of predicted structures. bioRxiv.
[3] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.",2) there is something fundamentally wrong with the CRD pre-training as currently implemented?,374,0
ICLR_2023_2626,ICLR_2023,"Weakness:
-- The main contribution is a little incremental. In one hand, the framework is based on existing techniques. In the other hand, this paper does not provide any novel theoretical insights on this issue.
-- In my opinion, the experiments are not quite sufficient in following two perspectives. 1) The defective attributes are not covered in the experiments. One of the main contributions is to jointly tackle missing and defective attributes. The adopted datasets naturally have missing attributes, but there are no evidences that the attributes of these datasets are defective. It's better to change the original attributes randomly or exploit some simple adversarial attacks on attributes to make the original attributes defective. 2) There could be more downstream tasks other than node classification. Attribute reconstruction is widely used in many scenarios e.g. anomaly detection, unsupervised or self supervised learning. It's better to show that the proposed ARHGA framework could support different downstream tasks to prove the effectiveness of ARHGA.
-- Some typos and small writing problems, e.g. the mapping ϕ
in Equation (3) is not explained in the paper.","-- The main contribution is a little incremental. In one hand, the framework is based on existing techniques. In the other hand, this paper does not provide any novel theoretical insights on this issue. -- In my opinion, the experiments are not quite sufficient in following two perspectives.",375,0
ICLR_2023_3312,ICLR_2023,"It is unfortunate, but I have to say that after reading the paper, I believe a reasonable user of MAE will still use the original MAE recipe. So the primary goal of the paper is not achieved. There are many reasons for this, for example: 1) the final performance is still not beating MAE significantly. There could be some reproducibility issues, but to claim ""robust and effective"", just with improvements that close the ""reproducibility"" gap is definitely not enough. 2) Right now the experimental section is not complete, e.g., I could not find the influence of final ImageNet accuracy with different ways or normalization, so this means it is not conclusive. 3) In order to show the recipe is indeed robust, I would very much like to see its effect on even larger models (ViT-H). 4) How robust is the technique in terms of compositionally?
I can tell that the paper is done in a rush. The experiments are not yet complete, there are quite a few broken (and lengthy) sentences, and lots of space is devoted to less important things (I believe for a report like this, an introduction should just be half a page long -- 1.5 pages are too long and way more space can be devoted to experiments if they are done.","4) How robust is the technique in terms of compositionally? I can tell that the paper is done in a rush. The experiments are not yet complete, there are quite a few broken (and lengthy) sentences, and lots of space is devoted to less important things (I believe for a report like this, an introduction should just be half a page long -- 1.5 pages are too long and way more space can be devoted to experiments if they are done.",376,0
ICLR_2023_3550,ICLR_2023,"I am not entirely sure about the use of individual training accuracy gain as part of the metric, I think we should technically look at user-level accuracy, where we have at least a single sample held out test for a given training sample thereby measuring generalization accuracy gains. I think measuring the gain on training samples is a bit meaningless as technically the default accuracy on those samples could be 100% for that user, if we do like a KNN classifier.
The measurement of privacy expenditure is inconsistent, there should just be 4 privacy budget epsilons and all experiments done on those, as opposed to different epsilons and even different ways of reporting. Like for one method epsilon is reported,for another the standard deviation of the noise added is reported (which could be converted to epsilon and I think should be for presenting results). All this said, I find stacking all these inconsistent guarantees together like that in Figure 3 in appropriate.
I am not sure I fully understand the two lines of reasoning for why the naive definitions of IF are bad, in section 3.2: • A training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users’ data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense. • A training algorithm is individually fair if the difference in privacy risks between any pair of users is small. This definition can be satisfied by reducing privacy risks with privacy-preserving ML because the differences are small if all users’ privacy risks are small. However, strong privacy protection with DP is known to degrade classification performance --> There are two issues with this: a) the only way to get similar privacy risks is not by applying privacy preserving methods, there could be other ways out there, so to say that this is achieved by strong DP guarantees is a bit inaccurate, and b) there are recent papers that show DP can be achieved with little loss to accuracy [3-5]
I am not really sure what the conclusion “necessity of the proposed IF for NNs'' really means. If it means we need improvements, there are some group level improvements, why not try them and then test? Improvments like [1-2]. I think the fact that the only conclusion from the paper is that individual level fairness is not good in privacy preserving methods is a bit repetitive gien prior work.
[1] Tran, Cuong, Ferdinando Fioretto, and Pascal Van Hentenryck. ""Differentially private and fair deep learning: A lagrangian dual approach."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021.
[2] Fioretto, Ferdinando, et al. ""Differential Privacy and Fairness in Decisions and Learning Tasks: A Survey."" arXiv preprint arXiv:2202.08187 (2022).
[3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12. (ICLR 2022)
[4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13. (ICLR 2022)
[5] Tramer, Florian, and Dan Boneh. ""Differentially Private Learning Needs Better Features (or Much More Data)."" International Conference on Learning Representations. 2020.","2021 Oct 13. (ICLR 2022) [5] Tramer, Florian, and Dan Boneh. ""Differentially Private Learning Needs Better Features (or Much More Data)."" International Conference on Learning Representations. 2020.",377,1
ICLR_2023_2862,ICLR_2023,"Weakness: the method is not real domain generalization because the shift in test data need to be known during training. The method is complicated, because it needs to train a model for every interpolation type with a specific {alpha}. Typos, especially in equations, makes it difficult to follow the main idea. The performance is much lower than augmentation in some cases. The relationship of this work to ""parameter subspaces"" in the title is not clear.
some concerns are listed, 1.It looks more like a model level information combination method rather than the title claimed “compressed”. 2.Legends in Figure 1 is too small. 3.The definition 1 is confusing, it says that x^ is a interpolation between x0 and x_i^det, but the equation contains only x_i^det, the definition of i is more confusing. 4.Symbol N has two different meaning is not a wise choice. 5.Bottom of page2, Where does x_i come from? 6.Page 6 on task interpolation, the equation for x^ , is it true that alpha is only related to index i? why? 7.Table1, Why the performance of CPS sometimes are better than augmentation and sometimes much worse than augmentation? Especially for the stylization and rotation cases. For CPS with rotation, the performance on clean test set is destroyed seriously, why?
8.What about CPS when comparing to some domain generalization or domain adaptation methods?
Typos: 1.Page2 “ that can multiple types of” 2.Algorithm 1, there are two D_i s in row10, what’s the difference? 3.Algorithm 1, how to compute the loss “cos()” in row 13?","3.Algorithm 1, how to compute the loss “cos()” in row 13?",378,0
ICLR_2023_579,ICLR_2023,"Weakness
While I do not closely follow this topic of learning with label hierarchy, it seems there are multiple previous works on deep representation learning with label hierarchy. For example, [Zhang et al., 2016], [Bertinetto et al., 2020]. In the current writeup, the related work on learning with label hierarchy is too short and narrow. Also the baseline considered seems a bit too weak and naive.
Experiments on one-shot generalization is not particularly interesting nor significant. For example, in experiment authors introduced ""coarser"" or ""mid"" level, which are both coarser level of ""coarse"" and ""fine"" class hierarchy. Even though one can train a new classifier using one data point from with ""coarser"" or ""mid"" level labels, aren't we supposed to do it in a ""zero-shot"" way by simply leveraging ""coarse"" and ""fine"" grained classifiers (i.e., for mid-level classifier, one can simply define a rule on fine-grained classifier)? Generally, I do not think this is an ""one-shot"" problem since 1) ""coarser"" and ""mid"" level labels can be derived from the ""coarse"" and ""fine"" level labels (e.g., ""mid"" level labels of odd & <= 5 is a union of 1, 3, 5 at fine-grained level labels), 2) many ""coarse"" and ""fine"" level labels are given during the model training already.
In this regard, more meaningful experiments would be one-shot or few-shot generalization to unseen classes, i.e., can we learn a fine-grained representation of class ""0"" by leveraging knowledge of its superclass (""even"") and neighbors (""2, 4, 6, 8"")?","2) many ""coarse"" and ""fine"" level labels are given during the model training already. In this regard, more meaningful experiments would be one-shot or few-shot generalization to unseen classes, i.e., can we learn a fine-grained representation of class ""0"" by leveraging knowledge of its superclass (""even"") and neighbors (""2, 4, 6, 8"")?",379,0
ICLR_2023_2570,ICLR_2023,"Implicit assumption that demographic features A
are independent of the X
. In many real-life datasets, there is almost an exact correlation between A
and some X i
. E.g. Race and Zip Code. 2. It would be beneficial to directly state these assumptions. The threat model is not clear. From what I gather, privacy is not guaranteed for X
only for A
. This departs from usual formulations and warrants discussion.
Setting exact fairness as the goal could be better motivated. The authors follow previous work in this set of assumptions. But since the privacy mechanism is randomized anyways, investigating approximate notions of fairness seems natural. Another issue is for certain distributions θ ∗
simply does not exist since the optimization problem is subject to exact fairness constraints.","2. It would be beneficial to directly state these assumptions. The threat model is not clear. From what I gather, privacy is not guaranteed for X only for A . This departs from usual formulations and warrants discussion. Setting exact fairness as the goal could be better motivated. The authors follow previous work in this set of assumptions. But since the privacy mechanism is randomized anyways, investigating approximate notions of fairness seems natural. Another issue is for certain distributions θ ∗ simply does not exist since the optimization problem is subject to exact fairness constraints.",380,0
ICLR_2023_1713,ICLR_2023,"Weakness]
W1: The significance of the proposed algorithm seems a bit limited, as it only targets fairness techniques that do not access sensitive attributes in training data. Since many more algorithms have been proposed for fair training with sensitive attributes, it would be helpful if the paper can clarify 1) the significance of this work and 2) how the proposed algorithm can be extended in other scenarios.
W2: Several design choices in the proposed framework Antigone are not fully justified. For example, why using the previous idea of handling noisy group attributes is the most appropriate way to solve the target problem? Why the current approach is better than other approaches like training a weak-labeler on groups?
W3: More importantly, the experimental results are insufficient.
The baselines are not enough. One of the key advantages the paper argues is that Antigone can be used for any algorithm of not accessing sensitive attributes on training data. However, the paper only shows its effectiveness on JTT. Thus, it is hard to believe that Antigone will effective in other algorithms like LfF and DRO.
Also, all experimental results are reported without error range, which makes the observations less reliable. For example, in several rows of Table 1, the improvements in Antigone compared to GEORGE are a bit marginal, so it is questionable how it changes after multiple runs. It would be much better if the paper shows the results with error ranges to clarify the performance gain.
Minor Typo: First paragraph in Section 2.1) target labels (X) => target labels (Y)",2) how the proposed algorithm can be extended in other scenarios.,381,0
ICLR_2023_4253,ICLR_2023,"1.Lock of novelty.
2.The reviewer thought the result in this paper was not very satisfactory. Not SOTA performance and VQAv2 SOTA is 84+ for now.
3.This article is very terribly written, perhaps with the intention of fooling the reviewers? For example, the authors claim that they propose a method to reduce flops, but in Table 2(a) of ablation, we can see that Method-Add and Method-Ours_gated have the same flops.",1.Lock of novelty.2.The reviewer thought the result in this paper was not very satisfactory. Not SOTA performance and VQAv2 SOTA is 84+ for now.,382,0
ICLR_2023_2648,ICLR_2023,"1) In section 4.3, the authors used the normalizing flow to infer the local data manifolds. However, there is no motivation to separate the latent space into two parts. What is the point of doing so? 2) The authors utilized two bijective transformation including g_{\theta} and h_{\Phi} to transform the original data manifold to the lower projected data manifold. However, the authors did not explain how to learn the parameters of the two transformation in the method and experiment section. 3) In Equation (16), the authors required client i to collaborate with certain clients who have a higher client similarity. But it is confusing to minimize the loss of the model on other clients’ datasets. 4) In Section 4.4, the final objective in Equation (14) uses both the data of client i and part of the data of other clients to train the local model. However, this private data should be kept locally on each client. So how did the authors train models with the data at the same time? 5) The authors should experiment with the sensitivity of the parameters of the proposed method, such as the threshold \epsilon, which has a very large impact on efficiency. 6) Does the code link in the paper expose the author information? (PhD student of Tsinghua University in China, Department of Automation).","4) In Section 4.4, the final objective in Equation (14) uses both the data of client i and part of the data of other clients to train the local model. However, this private data should be kept locally on each client. So how did the authors train models with the data at the same time?",383,0
ICLR_2023_4459,ICLR_2023,"1. Intuitively, related classes and samples are more transferable, and irrelevant classes and samples could lead to negative samples. The experiments in this paper just confirm the intuition. However, I expect that the authors propose a framework to adaptively estimate the transferability of different classes and find the common transferable classes rather than enumerating all classes for different target tasks, which is very time-consuming. 2. The target task is very similar to the source task. The authors conduct experiments on many target datasets, e.g., cifar10, cifar100, CALTECH, which is very similar to ImageNet. In practice, the downstream task could be significantly different from the source domain, e.g., cartoon images. In addition, the downstream task could be heterogeneous tasks, e.g., VQA. How does the proposed method perform on these complex tasks? 3. It seems that the proposed framework only works for supervised pre-training tasks. However, in recent years, there are more self-supervised pre-trained models, e.g., BERT, RoBERTa, MAE. It seems the proposed framework could not inspire these works. 4. The proposed method estimates the transferability relying on the target task. In practice, the target tasks could be unavailable during the pre-training stage. Thus, estimating the transferability without the target tasks is more important. In addition, there have been many methods to estimate the transferability of samples, e.g., tradaboost, how does the proposed method perform comparing with these methods? 5. Generally, pre-training a big model is very time-consuming. Is it necessary to pre-train a big model by selecting transferable classes for specific target task?","5. Generally, pre-training a big model is very time-consuming. Is it necessary to pre-train a big model by selecting transferable classes for specific target task?",384,0
ICLR_2023_1913,ICLR_2023,"1)Although the proposed spatial reasoning network is effective alone, the quality of generated images still relies heavily on the image generation network, which is implemented by off-the-shelf framework GLIDE. To verity the generalization of spatial reasoning network, I think author should further explore different frameworks of image generation. 2)According to my understanding, the visual element generation module needs to be sequentially called on each box from the blueprint. When the spatial reasoning module provide all the blueprint, image generation network could also directly inpaints all the masked area while keep the global semantic and spatial information. So, I think this sequential manner maybe a little redundant. 3)In section 3.3, author choose 20% as the stop time, but it is not discussed in the experiment. I suggest to add relevant comparative experiments. 4)In practical application, some large-scale pre-trained models could directly realize the similar function for the task of CSG, even under the condition of zero-shot. I think the quality of generated images maybe better than the cases you present in experiments. Meanwhile, the propositional logic mentioned in this paper also requires additional manage. So, how do you view this problem?","1)Although the proposed spatial reasoning network is effective alone, the quality of generated images still relies heavily on the image generation network, which is implemented by off-the-shelf framework GLIDE. To verity the generalization of spatial reasoning network, I think author should further explore different frameworks of image generation.",385,0
ICLR_2023_4411,ICLR_2023,"Weakness • The reviewer thinks the authors need to elaborate how the output labels are defined for density assessment. In section 3. Datasets, it seems the authors gives confusing definitions of density and BIRADS findings like “we categorized BI-RADS density scores into two separate categories: BI-RADS 2 and 3 as benign and BI-RADS 5 and 6 as malignant”. There is no description about what “Density A”, “Density B”, “Density C”, and “Density D” mean. Also, as the reviewer knows, benign or malignant classification can be confirmed with biopsy results not BIRADS scores. Even though the reviewer is not familiar with the two public datasets, the reviewer thinks the datasets should have biopsy information to annotate lesions whether malignant or benign. • As a preprocessing step, the authors segmented and removed the region of the pectoral muscle from MLO views. However, the authors did not explain how the segmentation model was developed (they just mentioned employed the prior work) and the review has a concern that important features can be removed from this preprocessing step. It might be useful to compare model performance using MLO views with and without this preprocessing step to confirm the benefit of this pectoral muscle removal. • How did you calculate precision/recall/F1-score for 4-class classification of breast density? Also, for breast cancer detection, researchers usually report AUC with sensitivity and specificity at different operating points to compare model performance. It might be more informative to provide AUC results for comparisons. • The reviewer thinks comparison of their proposed approach with the single-view result is unfair. This is because information that multi views contain is 4x larger than the one that the single view has. So, to demonstrate the benefit of using the proposed fusion strategy, they need to report performance of multi-view results with simple fusion approach like the average/maximum of 4 view scores, or max over mean values of each breast. • Are the results reported in this study based on patient/study level? How did you calculate performance when using single views? Did you assume that each study has only one view? • What fusion strategy was used for results in Table 2? Are these results based on image level?","• The reviewer thinks comparison of their proposed approach with the single-view result is unfair. This is because information that multi views contain is 4x larger than the one that the single view has. So, to demonstrate the benefit of using the proposed fusion strategy, they need to report performance of multi-view results with simple fusion approach like the average/maximum of 4 view scores, or max over mean values of each breast.",386,0
ICLR_2023_2147,ICLR_2023,"Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?","2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594. Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?",387,0
ICLR_2023_2142,ICLR_2023,"Weakness:
W1: the idea is not novel and the solution is straightforward. The submission claims in the related work section “Where all these models search for a unified global model, a key distinction of our work with these works is that we aim to search for a personalized model for each client.” However, this is not true. Many existing work such as the following two are also providing NAS for different clients in FL.
1). Personalized Neural Architecture Search for Federated Learning, Minh Hang et al. Neurips Workshop 2021. 2). Personalized Federated Learning via Heterogeneous Modular Networks. Tianchun Wang, et al. ICDM 2022, 2022.
Actually, these two papers are using more elegant solution for PFL. The proposed solution adopts the mixture method of ideas of NAS and MAML. It needs to maintain two networks (one global and one local) that causes additional communication/storage burden in FL. The second term (regularization) term in formula (5) is very straightforward.
W2: the notations used are not well-presented, making the draft hard to follow. The notations used are not formal. Typically, matrix, vectors, and scalars are using different fonts for easy reading. However, all the notations are using the same fonts.
W3: There are too many typos and grammar errors indicating the low-quality of the draft. Even in the abstract, there are clear typos, e.g., “all the clientsin FL”. More examples, in the intro “we demonstrate accuracy gain of 10%, 6%, 4% over”==>”we demonstrate an accuracy gain of 10%, 6%, 4% over “ In the related work “ meeting clients efficiency budgets”==> meeting clients’ efficiency budgets
W4: The experiments are far from complete. The authors only compared to very few PFL baselines. Many more related works are not compared. For example, the most relevant “Personalized Neural Architecture Search for Federated Learning, Minh Hang et al. Neurips Workshop 2021.” Was not compared.
W5: There is no theoretical analysis if the algorithm will converge or not.","2). Personalized Federated Learning via Heterogeneous Modular Networks. Tianchun Wang, et al. ICDM 2022, 2022. Actually, these two papers are using more elegant solution for PFL. The proposed solution adopts the mixture method of ideas of NAS and MAML. It needs to maintain two networks (one global and one local) that causes additional communication/storage burden in FL. The second term (regularization) term in formula (5) is very straightforward.",388,0
ICLR_2023_3142,ICLR_2023,"Weakness Major:
As the authors mentioned also, the current model takes vector state as input. However, a model that can handle visual input is more desired. A related work, SMORL[1], has shown promising results given visual input. Similar results can also be obtained from there.
-------------After rebuttal-----------------------
Thanks for the author's responses. I decide to increase my score to 5. 1)I still think the proposed work shares some common motivations with SMORL, such as the reduced size of effective state space. The authors did investigate more architecture than the SMORL work. However, the experiment's only finding is that utilizing the factorized entity-based structure improves the performance, which does not have much novelty. Similar findings have been demonstrated in prior work like SMORL. 2)Though given the ground-truth entity states and goal state as input, the proposed work is solving a much simpler task than SMORL, I'm convinced that the OOD experiments are meaningful, and I'd like to increase my score to 5. However, I am leaning toward rejection based on the current experimental results.
[1]: Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement learning with object-centric representations.","2)Though given the ground-truth entity states and goal state as input, the proposed work is solving a much simpler task than SMORL, I'm convinced that the OOD experiments are meaningful, and I'd like to increase my score to 5. However, I am leaning toward rejection based on the current experimental results. [1]: Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement learning with object-centric representations.",389,0
ICLR_2023_2171,ICLR_2023,"1. All the tasks in this paper seem based on grasping. The difficulties of these tasks are too homogeneous. Perhaps refer to these projects to design more tasks, such as [1] and [2]. 2. The State Space contains the pose and velocity of the object without any vision-based input, thus resulting in missing geometric features of the object. This may limit the out-of-domain generalization ability. Recently, lots of dexterous hand works contain visual input with a strong generalization ability, such as [3] and [4]. 3. There seem to be no ablation experiments on synergies. 4. The experiment seems to be done with only one seed, and the authors should consider doing several more seeds in order to better demonstrate the robustness of the method. 5. The previous work [5], which also presents a dexterous manipulation benchmark with musculoskeletal hands, detracts from the novelty of this work.
[1] https://bi-dexhands.ai/ [2] https://openreview.net/pdf?id=k2Ml8FGtJZp [3] https://openreview.net/pdf?id=k2Ml8FGtJZp [4] https://openreview.net/pdf?id=tJE1Yyi8fUX [5] https://arxiv.org/pdf/2205.13600.pdf",3. There seem to be no ablation experiments on synergies.,390,0
ICLR_2023_2635,ICLR_2023,"1.The introduction and the abstract can be carefully revised, which is hard to follow. Lots of background is missing, which makes the readers very confused.
2.The experimental results show that the improvement is very small; why? And can you explain it? Could you provide some case studies？
3.Some small typos such as "", Several works also"" >, ""several works also""","1.The introduction and the abstract can be carefully revised, which is hard to follow. Lots of background is missing, which makes the readers very confused.",391,0
ICLR_2023_2562,ICLR_2023,"Even though there are experiments with VRNL augmenting prior label-noise learning approaches, it would have been very compelling to include results with VRNL alone, without knowledge or reasoning about the transition matrix. For instance, a performance comparison of VRNL vs. no VRNL on a simple CIFAR10 example where, e.g., 10% of the labels are artificially made to be incorrect, would go a long way.
The method requires tuning of the hyperparameter α
and a wide range of values for α
is used for the various experiments on different scenarios in Sec. 4. From the plots in the appendix (Fig. 7), the method’s performance is highly sensitive to the precise value of α
. The authors did address that a validation data set can be used in a reliable way for CIFAR10, however, it is not clear whether this is always the case.","4. From the plots in the appendix (Fig. 7), the method’s performance is highly sensitive to the precise value of α . The authors did address that a validation data set can be used in a reliable way for CIFAR10, however, it is not clear whether this is always the case.",392,0
ICLR_2023_4250,ICLR_2023,"• The paper proposes a system which is essentially based on NeRF with input using a point cloud. • The paper tries to demonstrate that the proposed approach can do many things and hence, there is a lack of focus. For example, from the title of the paper, it suggests that the method is focused on lighting and deformation. However, there are only two short sections on the experimental results. While the results on lighting estimation appear competitive, the results on deformation failed to demonstrate its advantages, in particular, with lighting. The shadows of the lego studs are not synthesized (see Fig. 4). • In Fig. 2, it is unclear why the results using the proposed method are good. For example, in the bottom row of the BlendedMVS, the results using the proposed method appear to be quite noisy and quite different from that of the GT while the results using VolSDF appear to be closest. In the paper, it says “… Table 1 and Fig. 2 demonstrate the effectiveness of SDF-based presentations of learning scene geometry.” This statement confuses me. • In the proposed method, there are quite a few hyperparameters, e.g. 3 in Eq. 11 and 2 in Eq. 12. There are no ablation studies to evaluate their significance.","• In Fig. 2, it is unclear why the results using the proposed method are good. For example, in the bottom row of the BlendedMVS, the results using the proposed method appear to be quite noisy and quite different from that of the GT while the results using VolSDF appear to be closest. In the paper, it says “… Table 1 and Fig. 2 demonstrate the effectiveness of SDF-based presentations of learning scene geometry.” This statement confuses me.",393,0
ICLR_2023_4710,ICLR_2023,"I think it may be better to focus on the details of only one situation, and give more experimental results.
A general framework is proposed in this paper, covering cover learning with noisy labels, partial label learning, and semi-supervised learning. I think it may be better to focus on the details of one situation.
As to the supervision and counter-supervision signals, whether additional artificial work is needed or not? 3.Only two data sets (CIFAR-10 and CIFAR-100) are used and the experiment may be inadequate.",3.Only two data sets (CIFAR-10 and CIFAR-100) are used and the experiment may be inadequate.,394,0
ICLR_2023_4842,ICLR_2023,"Novetly: Incremental. Primarily because it glues together two ideas, (1) diversity based dataset partitioning, (2) MAE based missing patch reconstruction. However, when viewed from the point of view of learned image compression, the approach is fairly basic and unlikely to compete with existing compression methods in terms of the compressed dataset size.
Writing: I found the writing unclear and paper hard to understand. See the section after weaknesses below for a few example clarification questions.
Missing description of dataset usage: It is not clear how the compressed dataset is used once it is created. My best guess is that :1) Somehow all patches corresponding to an image are retrieved. (This could be done by sampling a bin and then sampling an image id in that bin?), 2) A MAE model is used to reconstruct the missing patches. 3) Full image is now assembled using the available patches and the reconstructed patches, 4) True label is not retrieved for this image (perhaps also stored as side information). Authors please comment on whether my gist is correct.
Evaluation: Since the proposed method has two stages, I expected at least an evaluation each for both. However, all evaluation focuses on second stage (of patch sampling), while binning strategy is not evaluated. Again, the simplest baseline is random binning, that is, the dataset is randomly partitioned into N equal sized bins.
Downstream dataset usage cost: While the proposed method reduces the dataset compression cost (Fig 1(a)), the downstream dataset usage cost is significantly increased because now an expensive MAE model needs to be run to create each image. A related ablation is missing, what is the downstream training speed for a given compressed dataset size in comparison to baselines? How many training steps are required to reach peak accuracy? How does that compare to no compression? Recall, that the original motivation was to save on the CO2 emissions resulting from long training (abstract). Does the proposed approach deliver on that promise?
Baselines for downstream tasks missing: Comparison with other methods for downstream evaluation missing. Specially the model trained with random sampling since it performs pretty well as in Fig 4.
Since the proposed method is more like a learned compression method (due to the MAE model to reconstruct missing patches), how does this compare with a purely image compression based approach (e.g. [1] below) in terms of the final dataset size and accuracy? Clarifications
What is the x-axis in fig 3(c) and 3(d)?
What is regrouping of the patches? How is this done?
What other information is stored? Perhaps the indexes of the patches that were not dropped? Corresponding image ids and labels?
How is the compressed dataset used to training a model? How is a minibatch of image label pairs created?
Other minor nits
In algorithm 1, the third inner loop is unnecessary with x* written as an argmax over the set in question.
Perhaps LHS of eqn 4 has a typo? It should have subscripts indicating which pixel a^c is being calculated for? Alternatively, what is the size of the a^c map?
[1] Variational image compression with a scale hyperprior (Balle et al.)",2) A MAE model is used to reconstruct the missing patches.,395,1
ICLR_2023_3808,ICLR_2023,"• The writing of the paper is at times confusing. For example, it is unclear of the significance of the \cap architecture. No detail is included in the paper. • The rationale of dividing the sequence of intermediate results into 11 sets is not provided. • No ablation studies are provided for \mu and \lambda, and the size of each set. In fact, their values are not provided in the paper (unless I missed them). • Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue. • It appears that there are two types of sets (Table 1 and 2), one for style and one for content. But in the paper, they are regarded to be the same, which is quite confusing. • There is insufficient discussion of the novelty of the proposed method, e.g. what the proposed method can do that other recently proposed style transfer methods cannot do? For example, in Fig. 6, the only discussion is one single sentence on p. 5 stating that the proposed method preserve the semantic structure of the content image and simulate the strokes of the style image from the style domain. Such a statement, which is quite generic, can be easily applied to other style transfer methods. • There is no quantitative comparison, e.g. style loss, user study, and efficiency, between the proposed method and other methods. • One issue not discussed is the importance of using the diffusion model. What can it provide to style transfer that no other methods can? • There are some occasional typos and grammatical mistakes.","• Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue.",396,0
ICLR_2023_3225,ICLR_2023,"1: The paper only presents a few key formulas about the VC theory, but provides no theoretical derivation except a few references of prior works. As the VC-theory is the key of the paper, the authors should include a full analysis of all the details of the theory. The theory presented in the paper should be self-contained, such that readers with some background can understand the analysis/theory with minor or no reference to literature.
Without giving details of the VC-theory, I cannot judge the correctness of the paper.
2: Explaining double descent includes two parts: 1, theoretically connecting test error with weight norm square, or VC-dimension; and 2, theoretically explaining the behavior of weight norm square and VC-dimension. The paper does not theoretically explain the behavior of the weight norm square. Even assuming all the formulas in the paper are correct, one still cannot claim the double descent is explained.
3: The formula of VC-dimension, in Eq.(3), only applies to linear models. However, the most interesting part of double descent is for non-linear models, e.g., over-parameterized neural networks. For the non-linear models, there is still no explanation.
4: The values of a 1 and a 2
are not from theoretical analysis, but from empirical guesses. Then, I don’t think the key equation, Eq.(2), is theoretical.","4: The values of a 1 and a 2 are not from theoretical analysis, but from empirical guesses. Then, I don’t think the key equation, Eq.(2), is theoretical.",397,0
ICLR_2023_3830,ICLR_2023,"Weakness: - Table 1 suggests that the baseline models have a lower training but much higher validation error. This clearly suggests overfitting. Which regularization methods have been used on the baseline models? How do the validation curves look like? - I expect the proposed PDNO architecture to better model different time horizons, i.e. dependencies on t? Or different grids? Yet in the paper, I only see experiments with fixed gird sizes and time horizons. Would an extension to non-regular output grids be possible? - As shown in Table 5 in the appendix, the proposed method is roughly a factor 5 slower than the FNO baseline, can this be mitigated? - Am I right that the symbol networks for each PDNO block get the same inputs, which are the coordinates in the spatial and the Fourier domain. Isn’t that a bit redundant? Maybe I misunderstand, please clarify. - In general, I think it would massively help the paper to compare to the experiments which are introduced in Lu et al. or at least discuss this direction.
Lu, Lu, et al. ""A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data."" Computer Methods in Applied Mechanics and Engineering 393 (2022): 114778.","- Am I right that the symbol networks for each PDNO block get the same inputs, which are the coordinates in the spatial and the Fourier domain. Isn’t that a bit redundant? Maybe I misunderstand, please clarify.",398,0
ICLR_2023_1106,ICLR_2023,"1. As far as I understand, the finally found dual graph lottery ticket is obtained after the training of the GNNs, i.e., the ticket contains fully trained parameters. This makes me feel the method proposed in this paper is more like a graph/model pruning method where we simultaneously train and prune a GNN until we finally find a sparse subgraph/subnetwork with non-dropped accuracy. If so, the dual perspective of the lottery ticket hypothesis does not that novel or interesting to me. I would like to see more explanations or justifications from the authors on this point. 2. If my understanding above is true, the authors are also encouraged to compare their method with standard compression-based methods. 3. As stated in the introduction, the Graph Lottery Ticket hypothesis can simultaneously simplify the input graph and prune the GNNs without compromising model performance. However, there exists an unignorable gap between the performance of the pruned GNNs with a pruning ratio above ~60% or 70% and the full-graph baselines on Citeseer and Ogbl-Collab. Is there any possible explanation for this phenomenon?
Minor concerns: 1. All experiment results in the current manuscript do not have standard deviations. Authors are strongly encouraged to add them by multiple random runs since on several datasets the margin between some methods is very close. Also, single-run-based results could sometimes be tricky due to e.g., cherry-picking. 2. Since this paper also tackles the compression of input graphs, I believe it is more meaningful to include “truly” large-scale graphs in the experiments. I believe nowadays large-scale graphs are typically at least in the scale of the millions in terms of the number of nodes, e.g., OGB-Products and ogbl-citation2. 3. Missing references to some (potentially) very related paper: Inductive Lottery Ticket Learning for Graph Neural Networks, 2022.","3. Missing references to some (potentially) very related paper: Inductive Lottery Ticket Learning for Graph Neural Networks, 2022.",399,0
ICLR_2023_4090,ICLR_2023,"Weakness: 1. The baselines are relatively weak. There exist quite a few papers investigating 3D point cloud embedding by self-supervised/unsupervised/contrastive pre-training (e.g. Point-BERT (CVPR2022), POS-BERT(arxiv2022), Point-M2AE (NIPS2022), PointGLR (TPAMI2022)). However, this paper does not cite them and compare with them. 2. The improvements over the weak baselines are small and the proposed method performs worse than other pre-training methods mentioned above. 3. The limitation is not fully discussed.",2. The improvements over the weak baselines are small and the proposed method performs worse than other pre-training methods mentioned above.,400,0
ICLR_2023_3942,ICLR_2023,"Weakness: 1.Motivation: In fact, there are some transfer learning methods that do not require retraining the source model [1] or even adopt decomposition [2] of the source representations. The need for the controllable discriminative representation with paired images is unclear for me, except for some generative tasks (e.g., StyleGAN-related work). 2.Method: The main component includes the Invertible Interpretable Network (IIN) proposed to factorize the hidden representation in another paper, which limits the overall novelty. 3.Experiment: This paper claims the effect of large pre-trained models on the representation transfer in introduction. However, the overall experiments are conduct on small datasets (e.g., MNIST) and models (e.g., 6-layer CNN). More analysis on standard benchmarks (e.g., ImageNet) and large models (e.g., resnet and vit [3]) should be considered. [1]. Mallya, Arun, Dillon Davis, and Svetlana Lazebnik. ""Piggyback: Adapting a single network to multiple tasks by learning to mask weights."" Proceedings of the European Conference on Computer Vision (ECCV). 2018. [2]. Kanakis, Menelaos, et al. ""Reparameterizing convolutions for incremental multi-task learning without task interference."" European Conference on Computer Vision. Springer, Cham, 2020. [3]. Dosovitskiy, Alexey, et al. ""An image is worth 16x16 words: Transformers for image recognition at scale."" arXiv preprint arXiv:2010.11929 (2020).","1.Motivation: In fact, there are some transfer learning methods that do not require retraining the source model [1] or even adopt decomposition [2] of the source representations. The need for the controllable discriminative representation with paired images is unclear for me, except for some generative tasks (e.g., StyleGAN-related work).",401,0
ICLR_2023_2074,ICLR_2023,"1 There might be problems in the formulation of the third contribution:
1.1 First of all, a naming criticism: The similarity is fixed. This the authors also acknowledge themself. This is okay but it is unclear why it is named contrastive, as no change or training of similarity is performed based on the augmentations of the test sample. Yes, they initialize the similarity using contrastive learning, but the test sample is not involved in this first step. It is averaging over copies obtained by data augmentation, and thus not more contrastive than the original MMD with self-supervised initialization.
Contrastively-initialized outlier detection is more precise here.
1.2 Secondly, and most importantly, equation (6) departs from the idea of an MMD score substantially. The MMD score as in eq (4) measures a difference between self-similarities and cross-similarities. Equation (6) performs an addition where MMD as in eq (4) would perform a subtraction.
Comparing to eq (4) also a self-similarity term between validation data is missing. For thresholding it can be omitted, but it would impact the variance of the estimators. However the main issue is the mentionned addition in place of substraction of the cross-similarity.
equation (6) becomes by that a statistic without any motivation behind it. Why it is chosen as addition when (4) suggest a substraction ?
1.3 Furthermore by matching equation (7) with algorithm 2 (on page 7) one can see that the calibration to compute gamma uses only data from the same in-distribution. But then asymptotically gamma should become 1, as m^{out} and m^{in} are computed over data drawn from the same distribution an thus their variance should be the same.
If gamma is not 1, as in Table 5, then something in the implementation seems to be unexpected (although gamma seems to be a constant across the datasets in Table 5).
1.2 and 1.3 look as if they wanted to do something slightly different. gamma becomes non-trivial if it involves a mix out validation and augmented variants of the test sample.
2 it would be good to try adversarial detection with other types of adversarial attacks, which are generated by different principles
3 self-supervised outlier detection is not novel itself (https://openaccess.thecvf.com/content/CVPR2021/papers/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.pdf , https://arxiv.org/abs/2103.12051 ), but see strengths, point 4","2 it would be good to try adversarial detection with other types of adversarial attacks, which are generated by different principles 3 self-supervised outlier detection is not novel itself (https://openaccess.thecvf.com/content/CVPR2021/papers/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.pdf , https://arxiv.org/abs/2103.12051 ), but see strengths, point 4",402,0
ICLR_2023_905,ICLR_2023,"Weakness: 1. According to algorithms 1 and 4, the unlearning process requires the help of removal set XR. This requirement is naturally fulfilled in the context of centralized unlearning but may not be available. In such case, the retraining in line 3 in algorithm 4 will always be required and significantly increase the complexity. 2. The unlearning strategy in this paper relies heavily on the k-means++ algorithm, which approximates the k-means problem proposed in 2007. The paper exploits the nature of k-means++ initialization with no technical improvement, making the novelty and contribution questionable. 3. Line 2 in algorithm 1 implies that the centroid set C can remain the same if the removal point is not inside the centroid set C without further restrictions. This is very questionable when a certain number of biased data points are removed within a single request. In the unlearning experiment setting, the unlearning requests are carried out with one data point per client at each round of unlearning, which is insufficient to evaluate the performance of the proposed unlearning method.","1. According to algorithms 1 and 4, the unlearning process requires the help of removal set XR. This requirement is naturally fulfilled in the context of centralized unlearning but may not be available. In such case, the retraining in line 3 in algorithm 4 will always be required and significantly increase the complexity.",403,0
ICLR_2023_2537,ICLR_2023,"The method part is at different parts very hard to follow, and I found various parts confusing. Below is a list of points that I think need to be addressed: • p. 3, section 3.1: The lower primitive policy π L j
is introduced with superscript j
, where j
denotes ""an indicator of the current subgoal reaching capability of the lower primitive"". In my opinion this sentence does not sufficiently explain j
. The indicator j
is not further clarified, does not change in the pseudocode, and is dropped later (section 3.2). So why is the superscript j
needed? • p. 4, Definition 1: What is D T V
? I'm assuming total variation, but it is not introduced. • p. 4, The proof for the suboptimality of the lower primitive is missing. It can probably be derived from the proof for the upper policy, however, seeing that they differ with respect to lambda it is necessary to explain where this difference comes from. • p. 5, Eq. 10: What is ϵ
? This needs to be introduced. • p. 5, section 3.4: Why does Eq. 10 incentivize the higher policy to make ""reasonable progress towards achieving the final goal""? Does it not just incentivize generating subgoals that are hard to distinguish from the subgoals of dataset D g
? • p. 5, section 3.5: It reads like the off-policy RL objectives can be referred to as J D H and J D L
. Shouldn't the off-policy objective be J θ H H
and J^L_{\theta_L} of Eq. 11 and Eq. 12? Because J D H
is defined above as the IRL objective (Eq. 10). J D L
is not defined before. The authors need to be more precise about the different objectives. • p. 5, Eq. 11 & 12: λ
is here introduced as a hyperparameter whereas in section 3.2, λ
was used as a factor for the upper bound of suboptimality. To avoid confusion different names should be used.
From the experiments it is not clear how much the approach relies on the expert trajectories to learn to solve the tasks. How many expert trajectories were used for each of the two tasks? How were the expert trajectories generated? How does the number of expert demonstrations influence the system's performance? For example, how do fewer or more demonstrations affect the performance? Could the system deal with ""bad expert demonstrations"" that are not useful in solving the task? For example, what would happen when a couple of random rollouts are included in the demonstration dataset? I think these questions need clarification in the description of the experiments, further discussion, and maybe additional experiments.
What is the effect of resetting the subgoal buffer, i.e., the hyperparameter u
? How does u
affect the sample efficiency of the approach? An ablative study evaluating different values of u
would help understand how resetting the subgoal dataset affects the method.
How is CRISP-BC different to CRISP-IRL? Why does it perform better than CRISP-IRL in the Pick and Place task? Highlighting the difference between the two versions would also help understanding the approach.
The method requires that the simulator can be reset to an arbitrary state. This is a major restriction for applying the method. The authors acknowledge this limitation but a more detailed discussion on how this limitation could be weakened would improve the paper. For example, it prohibits applying the system on real robots. Thus, the claim of the abstract that the method “[…] is suitable for most real world robotic control tasks” is too strong.
Please fix the citation style. Use citet only of the citation is part sentence, grammatically (as a subject or object). Otherwise ,use \citep. Here an example: “Learning effective hierarchies of policies has garnered substantial research interest in RL Barto & Mahadevan (2003)” should be “Learning effective hierarchies of policies has garnered substantial research interest in RL (Barto & Mahadevan, 2003)”. Minor corrections and suggestions:
typo on p. 1: ""balnced""
bottom of p. 5: it should be Algorithm 2 instead of Algorithm 0
typo on p. 9: ""tn maze navigation""
Figure 1 should be referenced in the text.
introduce the abbreviation IRL on p. 3 when first mentioning inverse reinforcement learning","• p. 5, section 3.4: Why does Eq. 10 incentivize the higher policy to make ""reasonable progress towards achieving the final goal""? Does it not just incentivize generating subgoals that are hard to distinguish from the subgoals of dataset D g ?",404,0
ICLR_2023_2603,ICLR_2023,"1.It seems that there is no analysis about the influence of initial model (line 1) on the experimental results in this paper. In fact, the influence of initial model exists (assuming that the best model is set at the beginning). It is suggested to add analysis about the influence of initial model. 2.The experiment is too simple. ""We train a convolutional neural network LeNet-5 using Fashion MNIST"". Such a specific experiment may affect the degree of confidence of the experiment results. 3.This paper proposes two strategies(constant and sequential) for selecting anchor and miner groups. But it would be better if there were a strategy based on the performance of each client. 4.The analysis of g(i)t,k (line 19)is reasonable, but it is worth discussing whether a factor α is needed for second and third items. 5.For the analysis of Figure 1, it seems that only Communication Rounds=400 is considered(why is 400?). Such analysis seems to be one-sided, and there are other considerations in fact, such as the change rate of the training loss.","4.The analysis of g(i)t,k (line 19)is reasonable, but it is worth discussing whether a factor α is needed for second and third items.",405,0
ICLR_2023_4870,ICLR_2023,"Weakness:
One of the advantages of a deep network is that it can be applied to a wide arrange of images. In contrast, conventional methods often need to optimize energy functions on each specific image. In this paper, an image-specific NAS technique is proposed which aims to provide optimal network architectures for each specific image. It means the network architecture is highly specific and has nearly zero robustness. Although the search for architecture is training-free, we still need a certain amount of time to optimize the searched network. The time and computation complexities should also be considered. Also since the setting is close to conventional methods setting, conventional methods should also be included in the comparison, in terms of accuracy and processing time.
The assumption/ observation made by this paper is dependent on the complexity of images. However, there is no definition or quantitative measurement of this property, i.e., why Lena has lower complexity than Baboon? Also, how to define an image as a fine-grained image or a coarse-grained image? Without a proper definition, the assumptions/findings of the paper cannot be held.
The experiments are not convincing. 1). The evaluation is only performed on 3 small datasets with less than 100 images in total. Image denoising is a common low-level vision task and can be easily extended to thousands or millions of images. Whether the findings still hold in general images is yet to be proven. It can be seen that there are some ambiguities in the BSD68 dataset and it only contains 68 images. 2). The paper only evaluates the denoising performance of hand-crafted Gaussian white noise. Since this noise is almost distinct from nature images in frequency, the performance of real noise images is yet to be proven. 3). What are the PSNR and the SSIM of the noised images? 4). As the noise level increases, the performance of the proposed method degenerates faster than competitors. Is that mean the network may have trouble handling large noise? 5). What are the ground truth widths? How to guarantee they are the optimal widths?","4). As the noise level increases, the performance of the proposed method degenerates faster than competitors. Is that mean the network may have trouble handling large noise?",406,0
ICLR_2023_763,ICLR_2023,"- It seems only synthetic problems are considered in the experiments.
- The authors claim in the discussion that their approach outperforms MC approximations in terms of performance. However, the experiments only seem to indicate that the proposed approach is faster. MC approximations seem to perform better.
- No error curves are given in Table 1 and 2. However, since the test set is large. E.g. 100k points it may not be a problem.",- It seems only synthetic problems are considered in the experiments.,407,0
ICLR_2023_308,ICLR_2023,"1. The paper is not well organized and hard to follow.
For example, the abstract is too concise to include necessary information regarding the background and motivation of the problem that the paper is going to solve and the introduction of the proposed method. The introduction lacks a clear and coherent line to follow. I am interested in seeing 1) the typical task of theorem proving and proof search; 2) the limitations of existing methods in theorem proving; 3) the introduction of expert iteration with an illustration of the target task; 4) the design of the proposed method in this paper; 5) the designs of the experiments and the main results. I am not very comfortable with the organization of the related work section as most of the content is put in the appendix. In the methodology section, it would be better to give an example of the miniF2F benchmark and an illustration of the Lean environment. I am struggling to understand the task, the dataset, and the environment that the work is working on.
2. The experiments are not extensive enough.
Most of the experiments are conducted on the miniF2F dataset, which consists of 244 validation and 244 test formalized statements of mathematical problems. However, miniF2F is limited to a small data scale, making the results not solid enough. Also, the paper fails to compare more baselines or search strategies in the experiments.
3. The writing could be improved.
It would be nice to provide a reference when mentioning some work for the first time. For example, the paper misses the reference when mentioning Go on page 2 and misses the reference for Lean in the related work section. There are some typos in the paper. For instance, ""Proof datasets extraction"". The statement ""These two differences make a naive application of reinforcement learning to formal mathematics unlikely to succeed."" lacks the necessary supporting facts in the paper.","1. The paper is not well organized and hard to follow. For example, the abstract is too concise to include necessary information regarding the background and motivation of the problem that the paper is going to solve and the introduction of the proposed method. The introduction lacks a clear and coherent line to follow. I am interested in seeing",408,0
ICLR_2023_2807,ICLR_2023,"W1. Depth estimation use-case is not convincing
W2. The claimed contributions are weak.
W3. The convergence bound is not convincing (in an adversarial sequence)
W4. Weak or no comparison in related work or results Discussion
W1. In Section 1.1, the paper introduces online depth estimation as a motivational example for justifying risk controlling. Here, the paper considers (I guess) depth measurements from accurate sensors (e.g., Lidar) as the ground truth depth, and claimed the necessity for uncertainty quantification for speed-up depth sensing. I think this may be a good example, but there are many missing pieces. First, I’m not sure if there are actual benefits in speed-up by using depth estimation via RGB instead of Lidar. In close range, Lidar can collect accurate depth very quickly, but for far objects, anyway we don’t need accurate depth, for example, in self-driving scenarios. Secondly, this motivation example uses the sensor measurements from an accurate sensor as ground truth, but it is unrealistic. For example, Lidar can be very noisy or wrong in snowy weather or puddles in the ground. So, it provides wrong labels to online learning algorithms. At least these two points undermine the justification on the necessity of risk controlling in online settings. Could you refine the motivation example more?
W2. The paper claims three contributions: (1) validity on the risk of constructed prediction sets, (2) compatibility with online learning methods, and (3) fast reaction to distribution shift.
For the risk validity, it is novel in online settings (considering risk controlling is possible under iid), but I’m not convinced why we need risk controlling as mentioned in W1.
For the second contribution, I think this holds for any conformal approaches, including ACI; I don’t think this is a unique contribution of this paper.
For the fast reaction to the distribution shift, I feel that this is a bit contradictory. The paper proposes several interesting stretch functions to reduce prediction set size, while maintaining a desired coverage. But, choosing a stretch function is equivalent to hyperparameter tuning. Meaning that, a good stretch function depends on the property of a data sequence, but simultaneously online algorithms, including Rolling RC, want to work for any distributions. I think I mainly get this impression on the way of choosing hyperparameters of this paper, i.e., hold out an initial subsequence in the entire sequence and test on the later subsequence. This hold-out-first-sequence approach does not make sense in distribution shift, as the initial subsequence cannot represent the later sequence. Could you justify this tuning approach?
W3. I was trying to understand the convergence bound in the last equation of Theorem 1 proof. Mainly, I was curious about why this deterministic algorithm could work in adversarial settings; recall that traditional online learning algorithms are randomized such that an adversary could not entirely fool the algorithms (or need a realisability assumption, e.g., for the perceptron algorithm).
To make the analysis simple, let m=0, M=1, \gamma=1, \theta_1=0, risk is the indicator loss, and r=0.1. Then, this convergence bound means the absolute difference of the average coverage rate and a desired coverage rate r is bounded by 1/T. By setting T = 10, this means after 10 time steps, the average coverage rate is at most 0.2 for any sequences. So, an adversary can choose an odd-subsequence (i.e., data arrive in odd time) such that a prediction set makes an error. Then, the \theta changes as follows:
t=1: 0 -> 0.9, t=2: 0.9 -> 0.8, t=3: 0.8 -> 1, t=4: 1 -> 0.9, t=5: 0.9 -> 1, … As can be seen, at the odd time, \theta is not one, the adversary can generate a datum to make an error. This means the average coverage rate is 0.5, which is larger than the bound by theorem. Could you correct my understanding? If not, what are additional assumptions to achieve the desired coverage rate?
W4. The risk controlling aspect is new in online learning, but I’m not sure why we cannot use the risk controlling idea along with ACI? As mentioned, ACI updates the coverage rate, while Rolling RC updates the threshold directly. What is the reason that we cannot control risk in ACI? If that’s possible, what’s the benefit of using risk controlling in threshold update? This may be justified empirically.","1 -> 0.9, t=5: 0.9 -> 1, … As can be seen, at the odd time, \theta is not one, the adversary can generate a datum to make an error. This means the average coverage rate is 0.5, which is larger than the bound by theorem. Could you correct my understanding? If not, what are additional assumptions to achieve the desired coverage rate?",409,0
ICLR_2023_4405,ICLR_2023,"as well.
W1. The soft contrastive learning loss is not motivated very well. The authors borrow the idea of t-distribution kernel (Li et al., 2021a, Zang et al., 2022) to compute the similarity in the contrastive learning objective. However, they do not provide why using the t-distribution kernel for similarity measure would encourage additional robustness to label noise compared to using other distributions e.g., Gaussian, or using conventional cosine similarity.
W2. The performance gain is not very significant on large-scale datasets. On the DomainNet, NSCL w/o AIO shows 51.0% H score while NSCL with CL shows 50.5% H score. Also, adding an all-in-one classifier only shows 0.6 points improvement on this dataset (NSCL 51.6% vs. NSCL w.o. AIO 51.0%). On the VisDA dataset, NSCL w/o AIO shows 70.1% H score while NSCL with CL shows 69.9% H Score. These results might imply the proposed method does not have much merit when there are a lot of training data.
W3. Poor presentation quality. The presentation is quite unprofessional. There are a lot of typos, inconsistent/redundant/wrong/awkward sentences, and wrong figures throughout the paper. To list a few below:
“robustness of noisy labels” -> robustness to noisy labels
“we cannot know” -> we do not know
“The UNDA is a uniform and practical setup”
“However, these methods contain multiple open-set classifiers, each corresponding to a single known class.” And “These methods combine closed-set classifier and open-set classifiers to identify known and unknown classes. However, each of the open-set classifiers only corresponds to a single known class. “ -> these two are redundant
“As a result, inadequate competition between open-set classifiers” -> what does it mean by inadequate?
“such imperfect data is almost inevitable…” -> what does it mean by such imperfect data?
“As a result, the proposed NSCL outperforms various baselines that explicitly or implicitly employ the ratio of unknown samples. “ -> awkward sentence
“Fu et al. (2020) seems to validate the threshold using labeled data, which is not a realistic solution “ -> “seems” shall not be used in related work section.
“The similarity S(z_i,z_j) is typically defended by cosine similarity “ -> the verb defend is awkward
"" to improve the discriminative performance of the potential space” -> what does it mean by potential space?
“top_n softmax active function” -> what does it mean by active function?
“we vary the number and compare it with other baselines.” -> awkward
Fig. 3: SCLN in the legend is absent in the caption and OVANet+AI in the caption is absent in the legend.
Fig. 4. There are no blue marks in (c) and (f). There are no orange marks within the private cluster in (a), (b), (d), and (e).
“global joint local feature alignment paradigm and adaptive energy uncertainty calibration strategy“ -> I do not understand this.
“a previous SOTA method tailed for the ODA” -> tailored?
In Section 4.1 L_src is used instead of L_scl. Also, NSCL w/o. CL is not shown in Table 5.
“AIO Classifier brings consistency improvements” -> awkward
Fig. 5 caption: “H-score and accuracy” -> H-score and Banalce H-Score?
“The results show that the NSCL and NSCL w/o RCL exceed the baseline more significantly as the proportion of noise increases.” -> what is RCL?
“We found that the open set classifier in OVANet corresponding to the private class (the orange scatters) had very sharp classification boundaries “ -> where are these sharp boundaries in the figure?
I have a few clarification questions for the authors.
Q1. Regarding Fig. 1 (a), top, I do not agree that the third example is noisy due to the domain gap. It is more like a zoom-in vs. zoom-out problem. Why this noise cannot occur in the Product domain?
Q2. What are the three numbers in the heading row of Tables 1, 2, 3, and 4?
Q3. In the evaluation metric paragraph, what does it mean by “pairing one more sample from the known category leads to a significant increase in A_c”?",3: SCLN in the legend is absent in the caption and OVANet+AI in the caption is absent in the legend. Fig.,410,0
ICLR_2023_3937,ICLR_2023,"Weakness: 1. When aggregating the set's representation, the choice of pooling operation is not clear. Although the performance of different poolings are analyzed. It might also be reasonable to use other approaches; 2. The advantage of set-based over instance-based contrastive learning is well illustrated, while it can be more interesting to see the advantage of constructing sets based on input over based on hidden features.","2. The advantage of set-based over instance-based contrastive learning is well illustrated, while it can be more interesting to see the advantage of constructing sets based on input over based on hidden features.",411,0
ICLR_2023_1527,ICLR_2023,".
The method.
The novelty of the proposed method, FCFD, is very limited. Note that the core designs of FCFD are two kinds of lateral network parameter reuses: feeding dimension-aligned student features at a certain layer to the paired teacher layer, and feeding dimension-aligned teacher features at a certain layer to the paired student layer. However, these two ideas have already been well explored in many existing knowledge distillation works such as cross distillation [1], residual distillation [2], explicit connection distillation [3], softmax regression representation learning [4]. Unfortunately, these works are completely missed by the authors. Given the existence of these works, FCFD has no new technical contribution, to the best of my knowledge.
[1] Few Shot Network Compression via Cross Distillation, AAAI 2020.
[2] Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts, NeurIPS 2020.
[3] Explicit Connection Distillation, ICLR 2020 submission.
[4] Knowledge distillation via softmax regression representation learning, ICLR 2021.
The experiments.
Experimental comparisons are not convincing enough.
For experimental comparisons on image classification datasets (CIFAR100 and ImageNet), in Table 4, etc., it seems that the authors directly use the results reported in the papers of Review KD and DKD as the baselines. This is not fair, as for ablation, the comparison of FCFD+DKD/FCFD+SimKD vs. DKD/SimKD needs to run on the same training machines and code settings.
For experimental comparisons on object detection dataset MS-COCO, experimental settings are also not optimal: 1) for object detection task, mainstream feature distillation methods, such as [1-3], already reuse/share feature pyramid network (FPN, the neck) of the teacher backbone as the FPN for the student. In this context, is it necessary to apply the proposed FCFD? 2) For experimental comparisons, FCFD should be compared to mainstream feature distillation methods for object detection, but not FD methods for image classification as they usually perform much worse.
In the final formulation of FCFD, there are five loss terms, how to set proper weights to them?
How about the training cost of FCFD? As it will lead to heavy extra memory cost, it is necessary to compare training cost of FCFD with other FD methods.
[1] Distilling Object Detectors with Feature Richness, NeurIPS 2021.
[2] Instance-Conditional Knowledge Distillation for Object Detection, NeurIPS 2021.
[3] Focal and global knowledge distillation for detectors, CVPR 2022. ----Update----
Balancing both positive and negative aspects of the rebuttal from the authors, although I raised the score from 3 to 5 (I would more like to give a score of 4 if there is such a choice), I still think this paper is not good enough to reach the acceptance bar of ICLR.","1) for object detection task, mainstream feature distillation methods, such as [1-3], already reuse/share feature pyramid network (FPN, the neck) of the teacher backbone as the FPN for the student. In this context, is it necessary to apply the proposed FCFD?",412,0
ICLR_2023_3850,ICLR_2023,"Weakness:
Overfitting: the network is trained with single clothing in a specific pose, and therefore, the network should not be generalizable to other poses, which is highly impractical. While it might be possible to handle various poses and shapes as claimed by the authors, I believe their quality should be significantly degraded (here, I guessed it because the authors didn’t show those results).
Weak motivation and justification for the requirement of a neural network: for surface reconstruction of “a single clothing”, why do we even need a neural network where high-qaulity scan is already given? For example, 1) we can directly get 3D surface using existing reconstruction methods, e.g., Poisson reconstruction, and apply rigging for correspondences; and 2) we can even simply approximate the uvn value of 3D surface without neural networks.
Lack of physical plausibility: since the proposed volumetric representation (uvn) is completely “local”, i.e., the network is not conditioned by global pose information, it is not possible to handle correct physical properties of clothing, e.g., pose correctiveness that depends on the human body pose.
Weak demonstration and experiments: The authors should design the visualization, application, and experiments in a way that highlights the effect of the proposed ideas. For example, 1) the authors must demonstrate all applications described in 4.2, where showing the results of shape and pose changes are really important to highlight the proposed uvn representation and its benefits (explicit correspondences). If it is not possible to demonstrate, please don’t mention those things in the document; 2) virutal try-on is possible without proposed method. Given high-quality 3D scans of clothes, why do we even need a neural network to enable virtual 3D clothing try-on? 3) Ineffective evaluation: the Chamfer distance is not effective for validation in a fair way. For example, a naive surface reconstruction that uses original 3D point clouds will give us 100% accuracy; and 4) please demonstrate the results from all baseline methods in a qualitative way.",4) please demonstrate the results from all baseline methods in a qualitative way.,413,0
ICLR_2023_1491,ICLR_2023,"The presentation needs further improvements. For example, 1) fig. 3 and Section 4.2.1 is not clear. What exactly are the e_h and e_t in the inputs? Why are there two [PAD]? Are there two inputs or just one? What is the [MASK] for? why the [MASK] needs to multiply E_e1 and E_ri? 2) How to ensure the quality of the proposed dataset? Even though I have checked the Appendix, I still cannot find the annotation guideline or agreement among annotators.
The experiments need further discussion to show the necessity of the task and the quality of the dataset. 1) The multi-modal analogical reasoning task can be decomposed into two sub tasks: cross-modal alignment and single-modal analogical reasoning. What is the advantage of evaluating them at the same time? 2) What is the main reason for the unsatisfactory performance, low-quality data? difficult cross-modal alignment or analogical reasoning? I would suggest to evaluate the two sub-tasks separately for further investigation. 3) From the cases in fig. 6, the predicted entities varies a lot. Why?","2) What is the main reason for the unsatisfactory performance, low-quality data? difficult cross-modal alignment or analogical reasoning? I would suggest to evaluate the two sub-tasks separately for further investigation.",414,0
ICLR_2023_2869,ICLR_2023,"In Section 1, the third contribution claimed by the authors is ""An off-policy formulation to effectively promote the reuse of previously collected data while stabilizing model training, which is important to address data scarcity in recommender system"". However, in this work, there is no experimental analysis demonstrating the proposed method can address the data scarcity challenge.
In Figure 1 and Table 1, the examples show that existing methods fail to capture the user's dynamic preference to obtain the maximum reward. However, the description about the user's long-term preference is missing in these cases.
In Section 4.3, for the evidence network, the authors introduce that ""e_{ik} is the evidence collected for rating k for item i"". However, it is not clear how to collect the learned evidence e_{ik} from the current action a_t and the item pool I using the evidence network.
The description about the experimental datasets are not clear enough. For example, the number of interactions in the pre-processed Netflix dataset, the number of users and the number of items on Yahoo dataset.
Even though this paper includes many baselines, some classical top-N item recommendation methods should be considered as baselines, e.g., BPRMF and LightGCN. For dynamic recommendation models, the authors need to consider the dynamic top-N recommendation methods as baselines, for example, Dynamic poisson factorization, RecSys 2015.
The RL-based recommendation baselines, i.e., DRN, LIRD, and CoLin, are published in 2018 or earlier. The authors need to consider state-of-the-art RL-based recommendation methods as baselines. More RL-based recommendation methods can be found in these surveys: 1) Reinforcement learning based recommender systems: A survey, ACM Computing Surveys 2022, 2) A Survey on Reinforcement Learning for Recommender Systems, arXiv:2109.10665.
Does the input feature of the model contain category information (e.g., movie genre)? What factors can help improve the diversity of genre in the evidence-based exploration?
The link to the source code is expired.","1) Reinforcement learning based recommender systems: A survey, ACM Computing Surveys 2022,",415,1
ICLR_2023_4469,ICLR_2023,"1. The improvement of proposed module in this paper is limited, especially in Table 1, the improvement of “channel concatenation” is negligible compared with other formulations.
2. The novelty of this paper is limited. In specific, it’s not hard to think the idea of concatenation along the channel dimension.","1. The improvement of proposed module in this paper is limited, especially in Table 1, the improvement of “channel concatenation” is negligible compared with other formulations.",416,0
ICLR_2023_2494,ICLR_2023,"My general comment is that it is hard to place this work both wrt to existing numerical methods, but also with respect to neural solvers, neural surrogates. That makes it really tough to properly weigh the pros and cons wrt to e.g. Proper Orthogonal Decomposition, Dynamic Model Decomposition, but also operator learning methods or PINNs. Furthermore, I would like to see a test against a model which does not preserve time reversibility. - The only comparison is against Proper Orthogonal Decomposition (POD) which as stated in the introduction “is flawed in that it ignores the temporal dependence of state variables”. Naturally the question is for example how Dynamic Model Decomposition (DMD) is doing for the problems. - Also from a Deep Learning perspective, e.g. Figure 4 could be learned by an operator learning that takes the initial state as input and outputs the state of the system after e.g. every 100th step. On the other hand, if done with e.g. PINNs this could be performed completely without training data. Just using the boundary conditions for the loss and the equation for the residual loss. It would be really interesting to have a comparison wrt to speed and accuracy. - PyTorch and more specifically the automatic differentiation capability of Pytorch is used as an optimization tool. It is however hard to judge from the paper what exactly the parameters are that are optimized. How many parameters are optimized for the different problems? Algorithms 1 and 2 in the appendix help a great deal but it took me a great deal to scroll back and forth. Is it possible to place the algorithms in the main paper and write refer to the important parts in Section 3,4? - The loss should be a bit more central to the writing of the paper. In Fig 1 the most important components are the reduced-order fluid model and the trajectory-wise discrepancy loss. However, Sections 4.3 takes a huge part of the main paper and is actually hard to follow. In my opinion the readability of Section 4 can be improved by focusing more on the relevant parts and not let the reader figure out what they are. - Why is e.g. JAX not used for optimization? Did the authors consider that? - How do optimization time vs inference time relate between the different models? - How do the individual components of the algorithm relate to the performance, e.g. how does the performance change for other loss terms? - Is it correct that for the first benchmark a single trajectory is used, whereas in the second benchmark more trajectories are used for optimization? On which trajectories is the testing done afterwards? Shouldn’t there be more than just one trajectories to get a better performance estimate?",- The only comparison is against Proper Orthogonal Decomposition (POD) which as stated in the introduction “is flawed in that it ignores the temporal dependence of state variables”. Naturally the question is for example how Dynamic Model Decomposition (DMD) is doing for the problems.,417,0
ICLR_2023_2782,ICLR_2023,"Weakness • It seems that the authors want to evaluate a form of reliability or trust by looking at what LLMs' ""knows"" but no rigorous definitions are given. • It is not clear that whether Conceptual consistency between LLMs' CSQA and the QA question-answers extracted from ConceptNet really reveals ""what LLMs know"". There are heuristic syntactic procedures to extract QA background facts from ConceptNet for CSQA question-answer pairs. The influence of heuristic procedures needs to be formally characterized and bounded for the ""what LLMs know"" conclusion to be rigorously valid. There is a logical gap between ""what LLMs know"" and the ""conceptual consistency definition"" as it is now.","• It seems that the authors want to evaluate a form of reliability or trust by looking at what LLMs' ""knows"" but no rigorous definitions are given.",418,0
ICLR_2023_4079,ICLR_2023,"• There are considerable similarities with another paper [1] (see below references). The work in this paper is not novel and there are no citation given to [1]. EM approach, regeneration approach is mentioned in [1]. • The experimental results are not convincing. The paper provides that joint learning on CIFAR- 100 dataset gives 39.97% accuracy when tested on class incremental learning. However, there seems to be more accurate results obtained with CIFAR-100 dataset on class incremental learning. For example, the paper [2] obtains 58.4% accuracy. In addition, the memory size is 10 times lower than this setup. The experiments do not contain the paper [2]. Other relevant papers [3, 4] whose accuracies are listed higher for this dataset are not compared and referenced either. • Although it is provided that a 6-fold cross-validation is used for every dataset, the reason for cross-validation is not understood because other papers that this work compares to did not use the cross- validation in their papers. Therefore, it is not clear why 6-fold cross-validation is required for this problem. • The notation for results is not clear. The paper claims the improvement for CIFAR-10 is 3%p but it is not clear what %p stands for. • Although there is a reference to [2] and other types of rehearsal based continual learning methods, the experiments do not contain any of the rehearsal methods. • The setup for the experiments is missing. The code is not provided. • The effect of memory size is ambiguous. An ablation study containing the effect of memory size should be added for justifying the memory size selection. • In Table-1, the experimental results for CelebA dataset are written in caption. However, there are not any experiments with CelebA dataset. [1] Overcoming Catastrophic Forgetting with Gaussian Mixture Replay (Pfülb and Geppert, 2021) [2] Gdumb: A simple approach that questions our progress in continual learning (Prabhu et al., 2020) [3] Supervised Contrastive Learning (Khosla et al., 2020) [4] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network (Kim and Choi, 2021)",• The effect of memory size is ambiguous. An ablation study containing the effect of memory size should be added for justifying the memory size selection.,419,0
ICLR_2023_3291,ICLR_2023,"although this paper uses formal data symbolic description for the proposed method, there is still no framework diagram to help the method understanding, which makes the algorithm of the article slightly inferior in the narration and implementation process.
although this paper introduces various attack methods in detail, it does not show more attack methods in experimental comparison, such as ISSBA. as a novel attack method, the authors should give more experimental comparison and analysis of the attack. 3, the author mentioned in the paper the advantages of the algorithm can also be mentioned in the attack on high efficiency. But for this part, I don't seem to see more theoretical analysis (convergence) and related experimental proofs. I have reservations about this point.
What is the main difference between the authors and ISSBA in terms of the formulation of the method? I would like the authors further to explain the contribution in conjunction with the formulas.
Some Questions: 1.How is the computational efficiency of extracting the trigger? Unlike previous backdoor attack algorithms, the method needs to analyze and extract data from the entire training dataset. Does this result in exponential time growth as the dataset increases? 2. The effectiveness and problem of the algorithm are that it requires access to the entire training dataset. Have the authors considered how the algorithm should operate effectively when the training dataset is not fully perceptible?
Overall: The trigger proposed in this paper is novel, but the related validation experiments are not comprehensive, and the time complexity of the computation and the efficiency of the algorithm are not clearly analyzed. In addition, I expect the authors to further elucidate the technical contribution rather than the form of the attack.","1.How is the computational efficiency of extracting the trigger? Unlike previous backdoor attack algorithms, the method needs to analyze and extract data from the entire training dataset. Does this result in exponential time growth as the dataset increases?",420,0
ICLR_2023_3498,ICLR_2023,"Weakness 1.The authors should provide more details on the empirical evaluation. First, how important hyper-parameters are set like the number of iteration of message-passing k. Also it would be better to have an experiment on the sensitivity of the parameters. Second, for hyper-edge classification, what is the evaluation protocol e.g. how are the negative samples generated and what is positive to negative ratio etc. In Table 4, does HQ the same as HyperQuery. Also, what is the exact definition of the minimax operation and the bilinear operation used? 2.Besides hyper-graph based approaches, the authors should compare the proposed method to graph-based methods as well. A neutral choice would be casting the problem as a graph classification problem by flattening all hyper-edges. Comparison to some simple heuristics like graph structure features like number of edges would also be very helpful. 3.The authors mention the classification of hyperedge types. Seems no experiment is carried out in section 6? Also in section 6.2, why use 70% for test and only use 20% for training?","1.The authors should provide more details on the empirical evaluation. First, how important hyper-parameters are set like the number of iteration of message-passing k. Also it would be better to have an experiment on the sensitivity of the parameters. Second, for hyper-edge classification, what is the evaluation protocol e.g. how are the negative samples generated and what is positive to negative ratio etc. In Table 4, does HQ the same as HyperQuery. Also, what is the exact definition of the minimax operation and the bilinear operation used?",421,0
ICLR_2023_3073,ICLR_2023,"Weakness]
1). Assumptions behind the local linear model may be too strict for real applications.
The proposed local regression model assumes the samples lie in small neighborhood of another point. However, in real applications using binary similarity measurements in computer vision, perception tasks, and recommendation systems, the compared samples may be far from each other and the assumption of localness may be violated.
2). Insufficient empirical verifications for real applications.
Through experiments on unit spheres, spirals, and taxi duration data, the authors provide nice visualizations of the estimate geometric parameters. However, neither comparison results with other methods nor experimental results on modern CV&ML datasets are given.","2). Insufficient empirical verifications for real applications. Through experiments on unit spheres, spirals, and taxi duration data, the authors provide nice visualizations of the estimate geometric parameters. However, neither comparison results with other methods nor experimental results on modern CV&ML datasets are given.",422,0
ICLR_2023_921,ICLR_2023,"Weakness: 1) The sample-efficiency of the algorithm has very limited insight given previous works. 2) It will be more clear for the readers to understand the contribution of the paper if the paper can explicitly analyze how much computation cost is reduced, rather than just from the reduction of planning calls. In deed, the new algorithm still require to calculate a new covariance matrix in each iteration. When re-planning is triggered, it requires solving a QCQP, which is not required in Algorithm 1. 3) It is not very clear how strong the cross product structure in Assumption 3 is. Does it includes the case where there is only finite number of possible w ?",3) It is not very clear how strong the cross product structure in Assumption 3 is. Does it includes the case where there is only finite number of possible w ?,423,0
ICLR_2023_4072,ICLR_2023,"Weakness:
The value of the proved generalisation bounds are limited in that: 1. It is based on a ""theoretical version"" of the REF complexity (eq. 2) which differs from eq. 3 that is actually used in practice. 2. The bounds are limited to convex or linear regression problems. 3. Even in these constrained cases, the tightness of the bounds are unclear; empirical validation in even toy-ish problems would help.
The main empirical results are based on correlations obtained from only varying the batch size and learning rate. It would be more helpful if the correlation can be shown across architectures (even for different channel configurations), which arguably more critically affect generalisation. While this is unclear from the paper, inferring from fig. 1's numbers across ResNet variants, the REF complexity seemed to be less well at capturing the test accuracy for varying architectures.","3. Even in these constrained cases, the tightness of the bounds are unclear; empirical validation in even toy-ish problems would help. The main empirical results are based on correlations obtained from only varying the batch size and learning rate. It would be more helpful if the correlation can be shown across architectures (even for different channel configurations), which arguably more critically affect generalisation. While this is unclear from the paper, inferring from fig. 1's numbers across ResNet variants, the REF complexity seemed to be less well at capturing the test accuracy for varying architectures.",424,0
ICLR_2023_4079,ICLR_2023,"• There are considerable similarities with another paper [1] (see below references). The work in this paper is not novel and there are no citation given to [1]. EM approach, regeneration approach is mentioned in [1]. • The experimental results are not convincing. The paper provides that joint learning on CIFAR- 100 dataset gives 39.97% accuracy when tested on class incremental learning. However, there seems to be more accurate results obtained with CIFAR-100 dataset on class incremental learning. For example, the paper [2] obtains 58.4% accuracy. In addition, the memory size is 10 times lower than this setup. The experiments do not contain the paper [2]. Other relevant papers [3, 4] whose accuracies are listed higher for this dataset are not compared and referenced either. • Although it is provided that a 6-fold cross-validation is used for every dataset, the reason for cross-validation is not understood because other papers that this work compares to did not use the cross- validation in their papers. Therefore, it is not clear why 6-fold cross-validation is required for this problem. • The notation for results is not clear. The paper claims the improvement for CIFAR-10 is 3%p but it is not clear what %p stands for. • Although there is a reference to [2] and other types of rehearsal based continual learning methods, the experiments do not contain any of the rehearsal methods. • The setup for the experiments is missing. The code is not provided. • The effect of memory size is ambiguous. An ablation study containing the effect of memory size should be added for justifying the memory size selection. • In Table-1, the experimental results for CelebA dataset are written in caption. However, there are not any experiments with CelebA dataset. [1] Overcoming Catastrophic Forgetting with Gaussian Mixture Replay (Pfülb and Geppert, 2021) [2] Gdumb: A simple approach that questions our progress in continual learning (Prabhu et al., 2020) [3] Supervised Contrastive Learning (Khosla et al., 2020) [4] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network (Kim and Choi, 2021)","• In Table-1, the experimental results for CelebA dataset are written in caption. However, there are not any experiments with CelebA dataset. [1] Overcoming Catastrophic Forgetting with Gaussian Mixture Replay (Pfülb and Geppert, 2021) [2] Gdumb: A simple approach that questions our progress in continual learning (Prabhu et al., 2020) [3] Supervised Contrastive Learning (Khosla et al., 2020) [4] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network (Kim and Choi, 2021)",425,0
ICLR_2023_3850,ICLR_2023,"Weakness:
Overfitting: the network is trained with single clothing in a specific pose, and therefore, the network should not be generalizable to other poses, which is highly impractical. While it might be possible to handle various poses and shapes as claimed by the authors, I believe their quality should be significantly degraded (here, I guessed it because the authors didn’t show those results).
Weak motivation and justification for the requirement of a neural network: for surface reconstruction of “a single clothing”, why do we even need a neural network where high-qaulity scan is already given? For example, 1) we can directly get 3D surface using existing reconstruction methods, e.g., Poisson reconstruction, and apply rigging for correspondences; and 2) we can even simply approximate the uvn value of 3D surface without neural networks.
Lack of physical plausibility: since the proposed volumetric representation (uvn) is completely “local”, i.e., the network is not conditioned by global pose information, it is not possible to handle correct physical properties of clothing, e.g., pose correctiveness that depends on the human body pose.
Weak demonstration and experiments: The authors should design the visualization, application, and experiments in a way that highlights the effect of the proposed ideas. For example, 1) the authors must demonstrate all applications described in 4.2, where showing the results of shape and pose changes are really important to highlight the proposed uvn representation and its benefits (explicit correspondences). If it is not possible to demonstrate, please don’t mention those things in the document; 2) virutal try-on is possible without proposed method. Given high-quality 3D scans of clothes, why do we even need a neural network to enable virtual 3D clothing try-on? 3) Ineffective evaluation: the Chamfer distance is not effective for validation in a fair way. For example, a naive surface reconstruction that uses original 3D point clouds will give us 100% accuracy; and 4) please demonstrate the results from all baseline methods in a qualitative way.","2) virutal try-on is possible without proposed method. Given high-quality 3D scans of clothes, why do we even need a neural network to enable virtual 3D clothing try-on?",426,0
Zh047FhXqI,ICLR_2024,"1. The Related Work does not seem comprehensive enough to me. It should include more recent works on OPE and MBORL. I did not find citation of OPE later than 2020. For MBORL, COMBO, ROMI and many more later works are missing. I suggest adding a more comprehensive Related Work.
2. I am not fully convinced by the experiment. As mentioned in the paper, PCM adopt more advanced network architecture to enhance performance so that even the PAM achieves better performance. Does this mean that the advantage of PCM may not come from the algorithmic novelty but from the better network architecture instead?
3. As an experiment paper, I do not think the experiments conducted are extensive enough, more tasks should be tested.
Minor Mistakes:
Figure 2 and 3 seem to have spacing issues. The legends are partially screened by the captions
In some formulae (e.g. 5 and 8), the parentheses should wrap the content inside.","3. As an experiment paper, I do not think the experiments conducted are extensive enough, more tasks should be tested. Minor Mistakes: Figure 2 and 3 seem to have spacing issues. The legends are partially screened by the captions In some formulae (e.g. 5 and 8), the parentheses should wrap the content inside.",427,0
39cPKijBed,ICLR_2024,"1. Introducing time-dependent methods might lead to heightened computational complexities in contrast to their time-independent counterparts. It would be beneficial to include a comparative analysis or discussion addressing this aspect.
2. One pivotal query that emerges is whether the efficiency and overall success of the proposed method are intrinsically tied to the performance of the time-dependent discriminator. It would be enlightening to elucidate this relationship.
3. There's a pertinent concern regarding the model's robustness. If the model isn't aptly regularized or subjected to limited training data, it might inadvertently heighten the risk of overfitting.
4. The research paper titled ""Fair Diffusion"" by Friedrich et al. [1] is notably aligned with the theme of fairness in diffusion models. However, its conspicuous absence in the current work, either as a benchmark or a referenced study, is intriguing. The authors might want to consider explicating their rationale behind not incorporating it as a baseline or offering a comparative analysis.
[1] Friedrich et al. ""Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness.""","3. There's a pertinent concern regarding the model's robustness. If the model isn't aptly regularized or subjected to limited training data, it might inadvertently heighten the risk of overfitting.",428,0
MGWsPGogLH,ICLR_2024,"- The finding that ""Transformers with a finite context length is not Turing complete"" has been known. Prior work's Turing completeness is based on infinite precision and infinite context length. Assuming a finite context length naturally makes the Transformer not Turing complete.
- The descriptions of the experiment are not detailed enough for reproduction; I'm not sure how the perfect results on multiplication and dynamic programming are achieved, and hence cannot verify its correctness.
- Many pieces of writing are overly assertive and inaccurate.
- Sec 2.1: It's not accurate to say that ""It is unusual to use complexity theory to study transformers"", since there has been a significant body of work on connecting transformers and formal languages and complexity classes, some of which the paper has cited but didn't discuss properly.
- Lemma 2.10: ""any autoregressive model"" is inaccurate: it should be ""any autoregressive model with a finite context window. For example, the Turing completeness of RNNs has long been established.
- In the discussion on related work in Sec A, the paper states that prior work use either limited number of steps or limited memory -- I'm not sure why this is true, since to my understanding, Perez et al. 2019 and Giannou et al. 2023 both rely on the Transformer being applied recursively and for unlimited times. Please let me know if I'm missing something.
- Missing related work: Selection-Inference: ""selection"" is similar to the Find Transformer, and ""Inference"" is similar to the Replace Transformer.
- Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning. By Antonia Creswell, Murray Shanahan, Irina Higgins.
- Other minor writing points:
- Definition 2.9: $\cup_{n< c} \Sigma^n \rightarrow \Sigma^{n+1}$ is problematic notation: $n$ is being quantified by $c$ and hence should not appear alone on the right hand side.
- Section C: the two move data pointer operations don't compile properly. Please use the math environment.
- Section 2 can be made much terser with details left to the appendix. There should be more details on the Find+Replace Transformer.",- Section C: the two move data pointer operations don't compile properly. Please use the math environment.,429,0
wDd4Zcnc08,ICLR_2024,"However, I hold concerns for the significance and generality of the proposed method.
1. The major contribution is a graph construction method without structural information, basically how to determine the edge connections and how to determine edge features, for a constrained HOIPs system with format $ABX_3$. The solution is somehow straight forward and may not be able to extend to general cases.
2. Although there are limited approaches for HOIP systems, but constructing heterogeneous graphs to capture interactions between different components of chemical systems is not new, and this proposed method is far from the first.","2. Although there are limited approaches for HOIP systems, but constructing heterogeneous graphs to capture interactions between different components of chemical systems is not new, and this proposed method is far from the first.",430,0
OatZMyMuIo,ICLR_2024,"I am concerned about the soundness of some of the claims:
1. The path from $U_{xy}$ to $Y$ is not influenced by any intervention on $Z_s$. Hence, if $p^s(U_{xy}) \neq p^t(U_{xy})$, it should also be the case that $p^s(y \mid x, do(z_s)) \neq p^t(y \mid x, do(z_s))$. This seems to contradict what is stated at the end of Sec. 3.1.
2. It is not clear how calculating the expectation of $p(y \mid x, do(z_s))$ over $p(z_s \mid x)$ (as done so in Eq. 2) is considered marginalizing out $z_s$. It is also not clear why this is preferable to just choosing some arbitrary $z_s$ to intervene.
3. How are $p(u_x)$ and $p(u_{xy})$ modeled in Eq. 3 if they are unobserved and change between source and target?
4. What justifies that the learned representations $Z_S$ and $Z_C$ truly follow the causal diagram in Fig. 1? Given the generative process of learning these representations (i.e. through $q(z_s \mid x)$ and $q(z_c \mid x)$), it could be argued that $Z_S$ and $Z_C$ are caused by $X$ rather than the other way around. Further, it is difficult to believe that a learned representation can contain more information about $Y$ than $X$, but this is what is implied by the graph (i.e. $Y$ and $X$ are independent given $Z_S$ and $Z_C$ but $Y$ is not independent of $Z_S$ and $Z_C$ given $X$?).
In addition, there are a few points that could use more elaboration:
5. At the beginning of Sec. 3.1, it is explained that the consideration of $U_x$ and $U_{xy}$ address two types of biases: selection bias and stereotype bias. This seems to be an interesting point and could be expanded.
6. Under Alg. 1, the paper mentions the necessity of assumptions to compensate for the lack of observations of $Z$ and $U$. These should be explicitly stated, as this seems to be the crux of the reasoning behind why the model works. Further, are some of these assumptions only relevant to certain types of data (e.g. images)?
I cannot recommend acceptance while I have these doubts, but I look forward to having them clarified in the authors’ responses.","1. The path from $U_{xy}$ to $Y$ is not influenced by any intervention on $Z_s$. Hence, if $p^s(U_{xy}) \neq p^t(U_{xy})$, it should also be the case that $p^s(y \mid x, do(z_s)) \neq p^t(y \mid x, do(z_s))$. This seems to contradict what is stated at the end of Sec. 3.1.",431,0
zkE2js9qRe,ICLR_2024,"1. For reconstruction task, OE achieves better performance than BINDER with fewer dimension. Thus, BINDER does not show superiority over OE.
2. BINDER may still suffer from the limitation of optimization, leading to inferior performance.
3. It is better to report the mean results, rather than the best results.","2. BINDER may still suffer from the limitation of optimization, leading to inferior performance.",432,0
8VHCeoBGxB,ICLR_2024,"1. This study delves into a captivating topic, examining two modes of prediction. However, the experimental approach presented by the author seems more like a clever technique than a genuine academic investigation. It would be beneficial if the author could furnish theoretical substantiations, perhaps drawing inspiration from tactics in the field of numerical methods, to further discuss the accumulation of errors during recursive predictions.
2. The author posits that meticulous frame-by-frame processing might overlook short-term spatiotemporal information redundancies, leading to inefficiencies. On the other hand, simply stacking frames in chronological order might miss out on deeper temporal dependencies. Yet, I couldn't seem to find experimental data in the text supporting this view. If I missed something, please enlighten me. As far as I'm aware, Simvp employs a method of stacking frames sequentially and multiplies T with C. Their performance in the temporal dimension seems commendable, which makes me question the author's conclusions.
3. The baseline provided in this study seems somewhat lacking. I would suggest the author to consider the relevant research in references [1-2] and attempt to integrate them into the discussion.
4. In the paper, the datasets chosen by the author predominantly have equal input and output lengths. To bolster the author's stance on long-term predictive capabilities, it would be advisable to choose more challenging datasets where the prediction length far exceeds the input length.
[1] Ning, Shuliang, et al. ""MIMO is all you need: a strong multi-in-multi-out baseline for video prediction."" In *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 2, pp. 1975-1983. 2023.
[2] Seo, Minseok, et al. ""Implicit Stacked Autoregressive Model for Video Prediction."" *arXiv preprint arXiv:2303.07849* (2023).","4. In the paper, the datasets chosen by the author predominantly have equal input and output lengths. To bolster the author's stance on long-term predictive capabilities, it would be advisable to choose more challenging datasets where the prediction length far exceeds the input length. [1] Ning, Shuliang, et al. ""MIMO is all you need: a strong multi-in-multi-out baseline for video prediction."" In *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 2, pp. 1975-1983. 2023. [2] Seo, Minseok, et al. ""Implicit Stacked Autoregressive Model for Video Prediction."" *arXiv preprint arXiv:2303.07849* (2023).",433,0
Ur25Xxvzsg,ICLR_2024,"1. Is there any obvious advantage to using TDCM instead of using an unsupervised model on the target domain? Is it faster or does it have less cost?
2. How to decide the number of updating blocks L?
3. The experiment is too weak. 1) For the synthetic dataset, how about changing the number of centroids for the target domain? How about changing the size of each clustering? 2）Only two simple real-world datasets are used. A larger and more complicated dataset should be used. 3) For CIFAR-10, using only CenterCrop to create a target domain is too weak.
4. The last paragraph of Sec. Introduction and the first paragraph of Sec. The methodology is of highly repetitive content.",1. Is there any obvious advantage to using TDCM instead of using an unsupervised model on the target domain? Is it faster or does it have less cost?,434,0
oM7Jbxdk6Z,ICLR_2024,"1. The authors did not discuss the training time cost of different pretraining methods.
2. A series of pertaining baselines are missed in related works and comparisons. For example:
[1] Xu M, Wang H, Ni B, et al. Self-supervised graph-level representation learning with local and global structure. ICML 21.
[2] Zhang Z, Liu Q, Wang H, et al. Motif-based graph self-supervised learning for molecular property prediction. NeurIPS 21.
[3] Zaidi S, Schaarschmidt M, Martens J, et al. Pre-training via denoising for molecular property prediction. ICLR 23.
3. The theoretical analysis is good. However, could the authors provide more insights into why MOLBLEND overperforms existing methods theoretically?
4. How does the different choice of encodings for 2D/3D modalities influence the pretaining?",4. How does the different choice of encodings for 2D/3D modalities influence the pretaining?,435,0
Xvfz8NHmCj,ICLR_2024,"1. The overall method is quite simple, which in itself is fine, but would warrant extensive ablation on the design choices. For instance, the algorithm depends on MAE as a way to generate good pre-trained features. How would the algorithm perform with alternative representation learning strategies? Furthermore, the buffer threshold is determined via cross-validation. How difficult is this cross-validation to perform and how many time steps / tasks are needed to learn it?
2. The ablations provided seem to compare against the semi-supervised algorithms. However, it is clear from Table 1 that the comparable semi-supervised algorithms are outperformed by DietCL, whereas supervised techniques such as ER and ER-ACE are competitive and almost as good as DietCL. It would have been more interesting to see ablations against these supervised learning baselines, for example to demonstrate the diet limits under which the proposed method is challenged.
3. Notation in page 6 defining A(t) and A seem to be incorrect due to overloading $t$","2. The ablations provided seem to compare against the semi-supervised algorithms. However, it is clear from Table 1 that the comparable semi-supervised algorithms are outperformed by DietCL, whereas supervised techniques such as ER and ER-ACE are competitive and almost as good as DietCL. It would have been more interesting to see ablations against these supervised learning baselines, for example to demonstrate the diet limits under which the proposed method is challenged.",436,0
lAhWGOkpSR,ICLR_2024,"1. The manuscript requires structural refinement. Although the paper proposes a new methodology, the empirical validation predominantly resides in the appendix. The core content appears to be overly enmeshed in granular theoretical derivations, leading to a somewhat convoluted narrative structure.
2. The delineation between the figures, tables, and the main text is ambiguous. It is advisable to render the captions in a font slightly smaller than the primary text to enhance clarity. The layout of the illustrations also warrants improvement. For instance, in Figure 2, it would be beneficial to incorporate descriptions of specific letters either within the caption or designated sections of the image. The term ""PE"" is presented here, yet a precise elucidation is deferred until Section 3.3.
3. The visualization results do not substantiate the issues you've raised. The quantified outcomes displayed in Figure 5 merely illustrate the discriminative results of the proposed network in comparison to Segformer for a specific image region. If the method you presented exhibits a substantial improvement over Segformer, I believe it would be acceptable. However, given that the data only indicates approximately a 1% enhancement relative to Segformer. There is a lack of targeted visualization results addressing the issues of scale inadequacy and field inactivation.
4. The author introduced ""varying window attention,"" but neglected a comparative analysis with other similar methods, such as the 'Shifted Window' in the Swin-Transformer.
5. Some state-of-the-art segmentation methods like HRViT, SegViT-v2, ViT-adapter, Lawin Transformer, etc. could be compared in the experiments to better verify the superiority of your proposed architecture. Sincerely,","1. The manuscript requires structural refinement. Although the paper proposes a new methodology, the empirical validation predominantly resides in the appendix. The core content appears to be overly enmeshed in granular theoretical derivations, leading to a somewhat convoluted narrative structure.",437,0
LtuRgL03pI,ICLR_2024,"- More of a system paper: combines many ideas from previous works on 3D scene generation and works on diffusion-based generation model. Seems that there is not too much truly new ideas here (and clarifications on where these happens would be great). I'm fine with it, to be honest, because this paper ticks my checkboxes for what a good system paper should be: clever/novel combination of ideas/components, solid execution, exceptional results. However, this is technically still a ""weakness"" I guess, as it limits the further applicability of the proposed method in other domains.
- Questionable choice of baselines. I get that previous graph-based/text-based works are not trained on 3D-FRONT, but it should be possible to compare with them in some capacities, or at least discussing the issues with them in the evaluation section,
- Despite the good discussions of issues of the way prior works handle latent object features (my impression was that they don't help much), I am left unconvinced that the proposed method actually make use of such features e.g. 1. would manually changing semantic feature f of an object result in notable difference in the final scene? 2. Can the model pick up stylistic information from the training data? Do objects that frequently occur together in the training data behave similarly in the generated scenes? 3. Is there any noticeable degrade in scene layout quality if semantic features are not encoded (i.e. just sample CAD models by category and size, which is a common practice for prior works) or if they are encoded with a smaller feature space?
- A user study would have been useful as it seems that humans, although not always consistent at rating the quality of the scenes, would be very good at checking if the generated scenes matches the text description.
- More discussions on how the qualitatively the proposed method works better than prior works would be very helpful. I can clearly that this is the case from the supplemental figures, and it would be great to highlight this in the main paper as well with a paragraph or so, as the proposed method clearly avoids many issues with prior works, especially on larger rooms. The set of quantitative metrics, while comprehensive, are known to be not the most sensitive metrics when it comes to evaluating indoor scenes, especially when it comes to layout/appearance details. More in depth analysis of the qualitative examples would make it a much stronger case that the proposed method is clearly superior.
- Lack of examples on how the proposed method can generalize & generate a diverse set of rooms from a single prompt. I think there can be many valid graphs for a single prompt and many valid layouts given a fixed semantic graph. It would be great to show examples that the proposed method can indeed achieve this.
- Most of the instructions are very specific, which makes sense consider how they are generated. However, in real applications, fuzzier instructions are also very important e.g. a room in the style of {certain style}, a room for {certain type of occupants}, a room that supports {certain functionalities}. Discussion on how to incorporating these instructions would be good.","- More of a system paper: combines many ideas from previous works on 3D scene generation and works on diffusion-based generation model. Seems that there is not too much truly new ideas here (and clarifications on where these happens would be great). I'm fine with it, to be honest, because this paper ticks my checkboxes for what a good system paper should be: clever/novel combination of ideas/components, solid execution, exceptional results. However, this is technically still a ""weakness"" I guess, as it limits the further applicability of the proposed method in other domains.",438,0
WvVyG8qBCt,ICLR_2024,"* __[W1]__ If data is long-tailed, the hand-wavy definition of this means that some parts of the data are over-represented and some parts are under-represented. As such, if the objective is to get high average accuracy (or its variant for recommendation problem), then ignoring the low frequency elements shouldn't be a problem. This paper seems to be looking at average metrics but it mentions long-tailed data in the title.
* __[W2]__ The paper proposes a phenomenon called `attention distraction'. The suggestion is that tokens with high variance will be unfairly selected by the attention mechanism. But it is not very clear to me what this means formally. In addition, the theory seems to assume that every token's attention value is an independent gaussian, which is also unrealistic as all tokens are dependent on each other due to previous layers and I don't see why adding gaussian noise to gradients results in gaussian distribution for token activations. The whole section 4 assumes these two things but there isn't any empirical or theoretical justification for this.
* __[W3]__ The experimental comparison is not thorough at all. Please see question 3 below for a more detailed explanation of why I think that is the case.
Minor Comments
1. What do the metrics used here actually mean ? Maybe I missed it but I don't see a definition of ndcg@10 and HiT@10.
2. Figure 1 is not very understandable as there are terms like $a_s$ and $e_s$ which is not clarified.
3. In general, the text is very heavy which can be distracting to readers. I would recommend (personally) to have shorter sentences to the point.
4. Eq 1 isn't clear what G is. 5.",* __[W3]__ The experimental comparison is not thorough at all. Please see question 3 below for a more detailed explanation of why I think that is the case. Minor Comments 1. What do the metrics used here actually mean ? Maybe I missed it but I don't see a definition of ndcg@10 and HiT@10.,439,0
BCocsAF7MY,ICLR_2024,"1. Notation clarification: lot of notations have been used before defining them. For example, on page 2, it is not clear what is R, n and f_i?
Similarly in Theorem 3.4, what does x_t \leq R and x_{t+1} \leq R mean?
2. The bounds presented in theorems seem to be pretty loose, seems like the approximation error will increase with dimension of examples? Also, how does it behave with number of examples?
3. From Fig. 2, prefix-tuning looks like a great contender to proposed algorithms and if I understand correctly, prefix-tuning is less computationally expensive than proposed algorithms.
4. Experiments in the paper are on data generated by the authors. It will be more effective if RICL was able to demonstrate bias mitigation on some public dataset (as simple as few-shot classification tasks).",4. Experiments in the paper are on data generated by the authors. It will be more effective if RICL was able to demonstrate bias mitigation on some public dataset (as simple as few-shot classification tasks).,440,0
N1hk66bz5m,ICLR_2024,"-	The contributions of the paper are limited in the light of recent research. Similar scaling laws have been shown for similar augmentation techniques for the same datasets. The analysis of answer augmentation and query augmentation as well as their combination has also been explored in Yu et al. 2023 ""MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"" which has been cited in this article.
-	The evaluation on Out-of-Domain Generalization lacks evidence to support the strong claim in the title of the paper. Furthermore, the authors seem to be agnostic of previous research on Out-of-Domain generalization its well known challenges, and existing approaches. The title is also in so far misleading as it suggests there is no generalization effect between domains of mathematics (e.g., algebra vs calculus).
-	It is not clear how the reasoning paths have been sampled. A Case study on the prompting methods would improve the exposition.
- The writing of the paper could be improved.","- The contributions of the paper are limited in the light of recent research. Similar scaling laws have been shown for similar augmentation techniques for the same datasets. The analysis of answer augmentation and query augmentation as well as their combination has also been explored in Yu et al. 2023 ""MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"" which has been cited in this article.",441,0
jvveGAbkVx,ICLR_2024,"Even though the paper is nicely and clearly written, there are a few points that could confuse the reader:
In the Paragraph “Stage I: Integer Programming. We approximate h_A and h_F…” “approximate” is confusing as $h_A$ and $h_F$ are already defined as binary parameters.
In the optimization problem in section 3.1 the abstention rate and the no harm constraints are not defined for any $z \in \mathcal{Z}$, whereas in the IP-Main these constraints are defined for each $z \in \mathcal{Z}$. If IP-Main is a way to practically solve of the optimization problem in section 3.1, the definitions should be consistent. If there is a reason why these definitions should be different, this reason should be made clear.
It is not clear what is the motivation for section 4.2. Since in IP-Main one does provide a desired abstention rate constraint for each $z \in \mathcal{Z}$ it is not clear what benefit would bring further constraints on the difference on the abstention rates. Especially in the case that the cardinality of $\mathcal{Z}$ is large, additional pairwise constraints for each pair $z,z’ \in \mathcal{Z}$ would add significant overhead in solving (3). Also, in (3) $z’$ is not defined.
The reported results in Figure 3 are over only 5 different runs. One could argue that this is a quite limited evaluation. Given that the results do look promising and the error bars are relatively small, showing results over more runs would strengthen the significance of the results. If there are computational limitations that prevented the authors from evaluating their method for more runs, these should be made clear. The same applies for the results of Table 2. In addition, Table 2 is missing confidence intervals and the type of the error bars in Figure 3 are not specified.
The authors should consider adding a (brief) discussion on limitations of their approach and on perspectives for future work. Typos/Misc:
- 1st paragraph in section 2 “i.e.” —> “i.e.,” and “e.g.” —> “e.g.,”
- 2nd paragraph “to determine which samples to abstain” is not very clear. Suggestion “to determine from which samples the classifier should abstain”
- Section 4 first paragraph “hyperparameter” —> “hyperparameters”
- Bottom of page 5 “as the models are neural network” —> “as the models are neural networks”
- Top of page 6 right most column of the Table “TBD in 3.1” do the authors mean “TBD in 4.1”?
- “An objection may arise that the model’s excessive abstention from a particular group, while not observed in others.” This seems as an incomplete sentence. What do the authors mean here?
- Missing “.” In footnote 2.
- Page 8 top “DO”—> “DP”
- Conclusion “our abstaining process incur” —> “our abstaining process incurs”","- 1st paragraph in section 2 “i.e.” —> “i.e.,” and “e.g.” —> “e.g.,” - 2nd paragraph “to determine which samples to abstain” is not very clear. Suggestion “to determine from which samples the classifier should abstain” - Section 4 first paragraph “hyperparameter” —> “hyperparameters” - Bottom of page 5 “as the models are neural network” —> “as the models are neural networks” - Top of page 6 right most column of the Table “TBD in 3.1” do the authors mean “TBD in 4.1”?",442,1
aqTipMg9CZ,ICLR_2024,"1) In paragraph 2 of the Introduction, the authors mentioned that ""in molecule graphs the relationships between adjacent sub-units are mostly irrelevant"". Is there any reference for this? I don't agree because the sub-units of the molecules are relevant to each other, and therefore, it will be helpful in reconstructing the molecule structure from the given molecule.
2) Weak experimental results.
- The most important baseline [1] is missing. It should be compared to demonstrate the effectiveness of reconstructing reaction centers instead of just learning from chemical formulas.
- In Table 1, it would be more convincing why fingerprint-based methods outperform deep models [2].
- In Table 2, comparing Mole-BERT and GraphLoG will be helpful since those methods outperform GraphMVP in Table 3.
- In Table 4, why many baseline models in previous tables are missing? Overall, various self-supervised learning methods should be compared in all tasks since they can be applied to all tasks.
- Moreover, in MolR [1], they have a task for chemical reaction classification, which aims to predict the reaction class that a chemical reaction belongs to. It would be great if REMO outperforms MolR in the task, thereby demonstrating the superiority of MolR in understanding underlying chemical reaction knowledge.
[1] CHEMICAL-REACTION-AWARE MOLECULE REPRESENTATION LEARNING, ICLR 2022.
[2] Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?, arxiv 2023.
3) No codes are available.",2) Weak experimental results.- The most important baseline [1] is missing. It should be compared to demonstrate the effectiveness of reconstructing reaction centers instead of just learning from chemical formulas.,443,0
fCQe7ei2f5,ICLR_2024,"* The notation $\tilde p(X, H)$ is a bit confusing since this is actually an estimator of the marginal $p(X)$, but it looks like a joint distribution.
* The claim in the introduction ""the dimension of the additional latent variable is limited to one"" from the reference paper is only for deep GP. The latent of GPLVM has no such strong drawback.
* Typo: undajusted.
* No definition of the abbreviation: LV-GP or LVGP, MH, HMC.
* Overall, the experiments are okay and the results are good. However, the experiments are not that adequate to fully convince me the validatiy of the new methods with the other two baseline methods. For example, is it possible to compare them on a synthetic dataset that the data is really from GPLVM. By this, we will fully understand the new method improves the learning performance of GPLVM than other traditional solvers, rather than some other reasons.
* In summary, a detailed analysis on these seems to be necessary, since introducing a sequence of bridge distribution for the proposal distribution is delicate, complicated and needs to be thouroughly explained and clarified for readers.
* The quality of the experiment results can be improved, see questions.","* The claim in the introduction ""the dimension of the additional latent variable is limited to one"" from the reference paper is only for deep GP. The latent of GPLVM has no such strong drawback.",444,0
rAX55lDjtt,ICLR_2024,"1. The authors claimed ""Diverse audio-related tasks are formulated in a sequence-to-sequence manner without imposing any constraints on input sequences"", which is not true since the input sequence could not be speech or music, as claimed in the limitation section. There might also be a maximum input sequence length imposed by the use of the Audio-MAE encoder, and the Q-Former aligner.
2. The authors claimed that one of the key contributions of the paper is: ""this is the first attempt to unify fully-supervised learning with in-context learning."" It is not clear to me what this means precisely. The authors need to make the motivation and benefits of combining multitask training with in-context learning clear.
3. The performance of the proposed approach is not satisfying based on Table 2, in particular on audio tagging on the AudioSet dataset.
4. The model is not tuned to follow instructions and can only perform a small of tasks, which makes the use of LLM less reasonable.
5. It sounds less sensible to me to use a standard Q-Former to convert audio input sequences into a fixed number of 32 tokens. Generating a fixed number of tokens is a good choice for images with fixed sizes, but not so much for audio input sequences due to their variable input lengths.
6. It is not clear what are the benefits of having the ATM, ATC, and AGTG multitask training on the Q-Former aligner.","3. The performance of the proposed approach is not satisfying based on Table 2, in particular on audio tagging on the AudioSet dataset.",445,0
YM0aPHTDe8,ICLR_2024,"The paper has a lot of flaws in analysis.
- In the abstract, the authors assert that the HFTD achieves linear speedup. Yet, the sample complexity is expressed as $O\left(\frac{1}{\epsilon} \log \frac{1}{\epsilon}\right)$, which doesn't scale with the number of agents $N$. This reveals an inconsistency between the theoretical analysis and the assertions made in the main text.
- The average mean gradient $\bar{g}(\theta)$ doesn't equal to the gradient of Eq(6). This discrepancy arises because $\bar{A}$ is nonsymmetric, whereas a true gradient should be such that the hessian is symmetric. Although Eq. (6) defines the objective of the HFTD algorithm as minimizing the MSPBE, the algorithm actually converges to $\theta^*$, which satisfies the average mean gradient $\bar{g}\left(\theta^*\right)=0$. Hence, $\theta^*$ does not denote the MSPBE's minimum. As such, HFTD failed in finding the optimal value of the MSPBE.
- The paper's main body contains multiple conflicting statements. The abstract mentions a mixed environment as the average of $N$
heterogeneous environments. However, in Sec 4, it's conveyed that this environment is randomly drawn from the heterogeneous environments. These descriptions are contradictory.
- The authors didn't provide a fair comparison between the results of their paper and those of existing works. In table (1), the objective of Wang (2023) was to find the optimal value function of $i$-th agent, which more focused on the personalization. However, in this paper, the objective function is to find the optimal value function of all $N$ environments, which was in the average sense. If the authors changed the optimality to $i$-th agent's optimal value function, their methods still cannot converge exactly without the gradient heterogeneity in Assumption 2.
- The bound was quite loose and coarse. The standard convergence result in FL supervised learning [1] is $O(\frac{1}{NTK})$. However, this paper only gave a result of $O(\frac{1}{T}})$, which can not be scaled by $N$ and $K$.
[1] Karimireddy, Sai Praneeth, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. ""Scaffold: Stochastic controlled averaging for federated learning."" In International conference on machine learning, pp. 5132-5143. PMLR, 2020.","- The bound was quite loose and coarse. The standard convergence result in FL supervised learning [1] is $O(\frac{1}{NTK})$. However, this paper only gave a result of $O(\frac{1}{T}})$, which can not be scaled by $N$ and $K$. [1] Karimireddy, Sai Praneeth, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. ""Scaffold: Stochastic controlled averaging for federated learning."" In International conference on machine learning, pp. 5132-5143. PMLR, 2020.",446,0
voLFfrWzFI,ICLR_2024,"1) The motivation is task generalization is not clear -- why do we care about task generalization in DFL settings? If we have a new task, why don't we start training from scratch? Task generalization in prediction tasks makes more sense, e.g., when you have zero-shot or few-shot data. But here in DFL we don't have such issues.
2) The two methods of task generalization are mostly heuristic, and there is no guarantee or in-depth analysis of whether/why they can work. Task representation using GNNs is not new, either.
3) The empirical results are not entirely convincing and is not reproducible with important setting description missing.
- Not sure if I missed anything, but I did not see where you described how many sampled tasks you need to have to train? Beyond reproduction, this is also very important to understand if the comparison is fair, and to understand what computational overhead is required.
- For the base task, the generalized methods are even better than those trained on the specific base task. This is a bit counter-intuitive. Is it because the generalization takes in more training data? If so, this does not appear a fair comparison.
- The advantage of the explicit generalization is very small compared to the implicit method in Tables 1 and 3. It is not clear how the proposed task representation method works.
4) The writing needs a lot of improvement. E.g.,
- The abstract and introduction spend the majority of the space explaining the motivation/challenges of DFL, which has been there in existing DFL papers. However, it rarely touches on the motivation to do task generalization in DFL, which is the FOCUS of this paper.
- The references are a bit messy. Also, on page 3, Niepart et al. 2021 is mentioned twice under two different ways of handling integer variables.
- Typos. E.g.,
Page 2: Given is;
Page 6, in Figure 2 is are;
Page 7, the caption of Table 1: on the both the base
5) The paper does not have a related work section, and many important DFL papers are not discussed, not even the two earlier and classic DFL papers (where the majority of the ideas discussed in the Introduction are from!):
- Donti et al 2017, Task-based end-to-end model learning in stochastic optimization
- Amos et al 2017, Optnet: Differentiable optimization as a layer in neural networks","- Donti et al 2017, Task-based end-to-end model learning in stochastic optimization - Amos et al 2017, Optnet: Differentiable optimization as a layer in neural networks",447,1
wu9nGGYvAX,ICLR_2024,"I am afraid that the definition of same-different visual relation should be beyond comparing objects at a pixel level. Thus, the observations and analyses may be of limited insights. The reasons are explained below:
- In the human-visual system, two objects that are considered the same are usually based on some specific semantic and/or attribute similarities, rather than counting how many pixels that exactly have the same values. It means the dataset/task should define same-different visual relations by more criteria, such as geometry, texture, color, categories, identities, and etc. Moreover, the dataset should consider more visual distortions in real scenarios, such as cluttered background, color jittering, slight/moderate shape distortion, or allowing overlapping between objects. Sec. 4.2 tries to dissociate color, texture and shape, but more test scenarios can be included.
- Defining the same-different visual relations at the pixel level actually gives quite a strong clue that can be captured by DNNs. Possibly it is the reason why the predication results are almost saturated (even can be achieved 100% accuracy if fine-tuned) by different pretrained models, especially when the texture similarity is involved (i.e., SHA and NAT datasets). Therefore, some conclusions drawn from these experiments may not be sound enough. For example, CLIP pretrained ViT models can achieve 100% in-distribution and nearly 100% out-of-distribution test accuracy is not a universal conclusion, but depending on the carefully designed testing datasets. The conclusion that fine-tuning abstract shape leads to a more generalizable predication may come from that all the four datasets (SQU, ALPH, SHA, NAT) can classify the same-different relations by shape consensus. If evaluating OOD generalization onto texture-based relations, the observations may be quite different.","- In the human-visual system, two objects that are considered the same are usually based on some specific semantic and/or attribute similarities, rather than counting how many pixels that exactly have the same values. It means the dataset/task should define same-different visual relations by more criteria, such as geometry, texture, color, categories, identities, and etc. Moreover, the dataset should consider more visual distortions in real scenarios, such as cluttered background, color jittering, slight/moderate shape distortion, or allowing overlapping between objects. Sec. 4.2 tries to dissociate color, texture and shape, but more test scenarios can be included.",448,0
hkL8djXrMM,ICLR_2024,"1. The presentation of the paper should be polished. The learning and sampling algorithm is difficult to find. I suggest the authors shorten/defer the discussion section to Appendix and add algorithm boxes in the main text. Moreover, I have several questions on the training process: (1) Are $\phi$ and $\theta$ jointly trained or alternatively trained? (2) How are the hyper-parameters of $F_\phi$ set? Are they similar to the x-prediction network?
2. Certain constraints of $F_\phi(x_t, t)$ should be satisfied. For example, when $t=0$, I think it must satisfy $F_\phi(x_0, 0) = x_0$. I wonder how the authors ensure that.
3. The proposed method brings additional computation overhead in the inference time when simulating Eq. (12) and Eq. (13). Because $F_\phi$ is a neural network and the authors use the same U-Net for both $F_\phi$ and $\hat{x}$ , it at least doubles the computation. Visually, Figure 2 shows that $F_\phi$ actually gives very close prediction to the real data $x$. I guess maybe simply double the size of the original $\hat{x}_\theta$ can get the similar results as NDM.
4. Ablation studies are missing. At least the influence of (1) the various choices of $\alpha_t$ and $\sigma_t$, and (2) the various choices of the network structure and number of parameters of $F_\phi$ should be investigated to verify that NDM brings consistent improvement.
5. Relationship to learnable interpolation in [1] should be discussed in more detail, although I notice there are several sentences in the related works. The two methods are actually very close to each other (I think the only difference is the objective).
Overall, I think using learnable marginal distribution for improve likelihood estimation is reasonable, and there is empirical improvement. However, the poor presentation quality and unstatisfying empirical evaluation makes the paper borderline.
[1] Building normalizing flows with stochastic interpolants. https://arxiv.org/abs/2209.15571","4. Ablation studies are missing. At least the influence of (1) the various choices of $\alpha_t$ and $\sigma_t$, and (2) the various choices of the network structure and number of parameters of $F_\phi$ should be investigated to verify that NDM brings consistent improvement.",449,0
RC2yS5QZQc,ICLR_2024,"1. The comparison result in Table 2 is mixed with a large portion settings that the proposed GCSCoOp is actually worse than other baseline methods.
2. The proposed method heavily depends on hyper-parameter ρ, β1 and β2 and it seems that performance is not quite stable with respect to these parameters.
3. Symbols are not consistent. For example, in Figure 2(b) the authors used v_t but v_k was used in section 3.2.
4. Writing needs to be improved as there are several typos. For example, 'Benefited from the rich nature language supervision' on page 3 should be 'Benefited from the rich natural language supervision'.","2. The proposed method heavily depends on hyper-parameter ρ, β1 and β2 and it seems that performance is not quite stable with respect to these parameters.",450,0
VhFSqBOYLs,ICLR_2024,"- Theoretical Limitations:
- **Curvature Sampling Limitations**: The utilization of curvature information to capture high-frequency surface details introduces certain limitations when employing mean and Gaussian curvatures. Sampling-based on mean curvature may overlook saddle regions where the curvature is positive in one direction but negative in another. Similarly, sampling based on Gaussian curvature might miss developable regions where one direction exhibits zero curvature while the other has high curvature. Although Figure 11 demonstrates the distinct biases of Gaussian and mean curvatures in sampling, the manuscript does not address their potential impact on performance.
- **Noise Sensitivity in Curvature-Based Sampling**: Sampling-based on curvature inherently exhibits sensitivity to noise. In the presence of a noisy depth map, all pixels tend to exhibit high curvature and, consequently, receive heavy sampling. The manuscript does not discuss strategies to mitigate this sensitivity to data noise effectively.
- **Discontinuities in Voxelized Representation**: The adoption of a voxelized representation with only a single center point and no interpolation between neighboring voxels can inherently introduce discontinuities in the SDF, as inferred from Equation 4 and the descriptions on Page 5.
- Inadequate Experiment Results:
- While NeuroSURF is compared against many recent generic representations such as SIREN and application-specific approaches such as Neural-Pull, the baselines in both cases are somewhat inadequate.
- More recent generic representations such as Instant NGP [Müller et al, 2022] are not included, which potentially can resolve the lack of high frequency details issue.
- As for converting depth/point cloud to surfaces, there are also techniques such as recent NKSR [Huang et al, 2023], and differentiable possion SAP [Peng et al, 2021], which are robust to noise while recovering the geometric details.
- The formulation (Equation 18-21) proposed in Appendix A.2.2 is similar to truncated signed distance function (TSDF) [Curless and Levoy, 1996]. In this case, I wonder if TSDF is adequate enough for recovering the surfaces accurately.
-	Unclear descriptions:
- The manuscript lacks a clear explanation of how uncertainties and SDF values are computed from the depth images. This information is crucial for understanding the constraints and potential limitations of the surface recovery process from depth data.
- The use of notations within the manuscript can be confusing. It remains unclear what represents network estimates and what are the inputs, despite the presence of Table 4 in the appendix. Additionally, the manuscript does not provide formal definitions for certain notations like $\psi^x$ in Equation 5. References:
Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4), 1-15.
Huang, J., Gojcic, Z., Atzmon, M., Litany, O., Fidler, S., & Williams, F. (2023). Neural Kernel Surface Reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4369-4379).
Curless, B., & Levoy, M. (1996, August). A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques (pp. 303-312).
Peng, S., Jiang, C., Liao, Y., Niemeyer, M., Pollefeys, M., & Geiger, A. (2021). Shape as points: A differentiable poisson solver. Advances in Neural Information Processing Systems, 34, 13032-13044.","- More recent generic representations such as Instant NGP [Müller et al, 2022] are not included, which potentially can resolve the lack of high frequency details issue.",451,0
smy4DsUbBo,ICLR_2024,"1.	This work chooses to predict a 4-th ordered tensor – “stiffness tensor”, whose symmetry has been maintain well with existing models like TFN, PAINN. This paper does not propose theoretical innovations in maintaining equivariance, so this methodology and the emphasis on “fourth-order” in abstract are not directly related (the Positive Semi-Definite Layer is based on physical meaning, regardless of whether it is a fourth-order tensor).
2.	Since the whole backbone is built with existing MACE layer, the main novelty is to propose the “Positive Semi-Definite Layer”(let us call it “PSD-layer”). About this, here are some questions:
- a) To maintain the equivariance of the total model, we must ensure each layer in such a model is equivariant. But for the PSD-layer, the PSD matrix $A$ is based on $M$, which is created by arrange $n(n+1)/2$ entries of the output of an equivariant model.
- **i**. Is it an equivariant operator to arrange $n(n+1)/2$ entries to get $M$? This requires a formal mathematical proof.
- **ii**. Is it an equivalent operator of the function $\rho$ to turn $M$ into $A$? In the end of Section A.2.2, “If the overall model had been equivariant with respect to vectors in $U$, it will remain equivariant after eigenvalues are made positive” is not trivail, it may also require a formal mathematical proof.
- **iii**.	For now, we assume that the previous question (function $\rho$ is equivariant) has a good proof. It is necessary to discuss whether this design will reduce the representational capacity of the entire neural network. Assume that this neural network is a bijection (we consider an equivalence class divided based on different perspectives as an input. But the function $\rho$ may not a bijection, e.g. $(\pm \Lambda) ^2$ will get a same result. The Wigner-D based network build a faith representation of the group, but the induction of function $rho$ will lead to an unfaith representation, it requires more analysis. For example, an analysis for the increased stiffness-based errors of CGC+ve, NNConv+ve, and MACE+ve in Table 1 may be a manifestation of the decline in the ability of network representation.
- b)	Let assume the PSD-layer is equivariant, there are some questions about the experiments:
- **i**.	In Section 3 (Related Work) and Section 5(Conclusion), Finite element (FE) modelling is mentioned, but the experiment could not find the data comparison (e.g. time cost or accuracy) between your model and FE.
- **ii**.	More powerful baselines should be discussed, especially the geometric GNNs like TFN[1], NequIP [2], SCN[3], GMN[4], SE(3)-Transformer [5].
3.	Here are some possible typos and recommended symbol modifications:
- a)	Almost all the inline formulas in the article lack punctuation at the end.
- b)	In Section 2.1, the rotation of the stiffness tensor $R_{ia}R_{jb}C_{abcd}R_{kc}R_{ld}$ may be better to generalize to $n$-th order tensors with the format of changing bases of a tensor($ R_{ia}R_{jb} R_{kc}R_{ld} C_{abcd}$).
- c)	In Eq. (2), the Wigner-D matrix generally written in the form $D_{m’,m}^{(l)}(R)$.
- d)	In Section 4, the third line in the paragraph “Positive Semi-Definite Layer”, “in line wih”, may be “in line with”. Reference:
- [1] Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.
- [2] E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials
- [3] Spherical Channels for Modeling Atomic Interactions
- [4] Equivariant Graph Mechanics Networks with Constraints
- [5] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks",- **i**. Is it an equivariant operator to arrange $n(n+1)/2$ entries to get $M$? This requires a formal mathematical proof.,452,0
bA5o5eZplk,ICLR_2024,"I have a few concerns and questions regarding the paper:
1) One of my biggest concerns about this paper is its clarity of presentation. The paper is hard to understand. One factor is that the authors did not describe enough about how the diffusion process works on graphs. Section 3.2 only explains the generic diffusion process on regular data (x). How it applies in the context of graphs, which contain structure and attributes, is not clearly explained. After explaining Section 3.2, the authors directly jump to the graph observations without explaining how the diffusion process on graphs works.
2) Since the authors did not clearly explain the diffusion process on graphs, many questions may arise about this, such as:
- How does the structure of the graph (A) contribute to the next node feature representation (X_{t+1})?
- Any effect of the neighboring node on a particular node representation in the next step?
- The diffusion process itself tries to model the data distribution, i.e., for graph cases, it models the data distribution of G={A, X}. As the first approach utilizes the diffusion information, why do the authors claim that the approach does not need to learn P(A, X)?
- And more questions that depend on the answer to the questions above.
3) The principle derived from the first observation is not surprising, i.e., ""The denoising neural network should be capable of capturing the local information in egonets"". In fact, I would argue that this principle is the bedrock of nearly all graph anomaly detection models.
4) The baselines used in the experiments are relatively inadequate, particularly in the second group. I would suggest the authors to add more baselines in the comparisons, such as:
- GHRN [1]
- GDN [2]
- H2-FDetector [3]
- GAGA [4]
- etc.
[1] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Addressing heterophily in graph anomaly detection: A perspective of graph spectrum. In Proceedings of the ACM Web Conference 2023, pp. 1528–1538, 2023a.
[2] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Alleviating structural distribution shift in graph anomaly detection. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pp. 357–365, 2023b.
[3] Fengzhao Shi, Yanan Cao, Yanmin Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. H2-fdetector: A gnn-based fraud detector with homophilic and heterophilic connections. In Proceedings of the ACM Web Conference 2022, pp. 1486–1494, 2022.
[4] Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng Ma, Yu Sun, Dianhai Yu, Fang Dong, Jiahui Jin, et al. Label information enhanced fraud detection against low homophily in graphs. In Proceedings of the ACM Web Conference 2023, pp. 406–416, 2023.","3) The principle derived from the first observation is not surprising, i.e., ""The denoising neural network should be capable of capturing the local information in egonets"". In fact, I would argue that this principle is the bedrock of nearly all graph anomaly detection models.",453,0
dlIMcmlAdk,ICLR_2024,"- While the explanation is intuitively presented, it remains somewhat challenging to discern the fundamental distinction from the negative prompt trick.
- In Sec. 5, the paper asserts that NFSD is notably more efficient than VSD, despite sharing a similar working mechanism. Although this claim appears obvious, I would recommend providing quantitative evidence to substantiate this advantage when compared to other baseline methods. It is conceivable that dropping the noise term could even speed up the convergence of ancestral sampling by using fewer optimization steps.
- Further ablation studies are needed to validate the assertions put forth in this paper. In comparison to SDS, two terms have been omitted according to Eqs. 5 and 7: the noise prediction $\delta_N$ and the noise ground truth $\epsilon$. However, it remains unclear which of these terms plays the most pivotal role in improving the final results.","- In Sec. 5, the paper asserts that NFSD is notably more efficient than VSD, despite sharing a similar working mechanism. Although this claim appears obvious, I would recommend providing quantitative evidence to substantiate this advantage when compared to other baseline methods. It is conceivable that dropping the noise term could even speed up the convergence of ancestral sampling by using fewer optimization steps.",454,0
WLRlL3zR7f,ICLR_2024,"- It seems that in the numerical experiments, there is no noise added to the data. Predicting data without noise is not a difficult task from a data-fitting perspective, especially with large models like deep networks.
- The details of the finite element method and linear solver are missing, which is crucial to the generation of the data
- It seems that in the calculation of the aggregation frequency response, the paper only describes how it is done, but no justification is given. Are there any references which can justify the calculation steps?
- In the results, it seems that the the sizes (e.g., number of parameters) of the model are not reported. This is important because the proposed model is outperforming because it uses larger networks. Also, it does not mention how the parameter tuning is performed.","- It seems that in the numerical experiments, there is no noise added to the data. Predicting data without noise is not a difficult task from a data-fitting perspective, especially with large models like deep networks.",455,0
nhgTmx1TZJ,ICLR_2024,"- The paper is a very dense read with some details relegated to the appendix while some details difficult to glean from the text. For example, the details about the audio tokenizer is moved to the appendix and no specifics are mentioned in the main text. Also, the text and the figure in the model architecture section are a little difficult to understand. I think I got it after reading through a few times, but adding some annotations to the figure and its caption will greatly improve readability.
- While the results for speech are decent, I found some issues w.r.t words being skipped. Also, the results on text to music/audio are pretty poor. The authors have not discussed any of the limitations in terms of quality of the generated audio despite the model being competitive with current state-of-the-art.
- The paper does not offer too much in terms of insights. Not to take away from the effort it takes to setup such a large scale experiment, but my main takeaways from the paper are as follows:
1. Existing audio codecs are not suitable for wide range of tasks, so one needs to train them on more diverse data.
2. As long as we collect 100k+ hours of audio data we can achieve such performance using the multi-scale transformer.
The multi-scale transformer architecture is definitely interesting to the community but the architecture contribution is not well ablated, and in my opinion, that is the most important part of the paper.
- Minor correction: In Table 3, subjective and objective metrics column headers should be interchanged.","2. As long as we collect 100k+ hours of audio data we can achieve such performance using the multi-scale transformer. The multi-scale transformer architecture is definitely interesting to the community but the architecture contribution is not well ablated, and in my opinion, that is the most important part of the paper.",456,0
lG6frgiJAU,ICLR_2024,"1. The writing quality could be improved with a clearer problem motivation and a better explanation of some attack techniques. More intuition is needed in places.
2. This paper depends on re-trained diffusion models, which could potentially constrain the generalizability of the method. If there's a significant discrepancy between the data distribution of the target model and that of the diffusion model, the success rate of the attack will decrease.
3. The method enables the generating of vicious images that could spread misinformation or be used for malicious purposes. However, the authors do not discuss how to prevent misuse or unethical applications.
4. The paper lacks reproducibility statements or code references. In addition, the appendix should be included within the same document as the main content, but this paper appears to lack such integration.","3. The method enables the generating of vicious images that could spread misinformation or be used for malicious purposes. However, the authors do not discuss how to prevent misuse or unethical applications.",457,0
m8KWOgE0Cn,ICLR_2024,"- While adapting FENDA's concept to the FL scenario required some modifications, the application of FENDA in this methodology appears overly incremental. The approach of combining features to utilize both global and local information, as opposed to APFL's method of aggregating logits, is not particularly novel.
- The methodological contribution to the ICLR community unclear. The application of FENDA to federated learning is straightforward, ""federated checkpointing"" is conceptually a combination early stopping with federated learning.
- The paper lacks a thorough ablation study on federated checkpointing methods. Additionally, while it benefits from not restricting the network architectures between global and local feature extractors, it would have been informative to show performance variations with the use of diverse network architectures.
- The paper could greatly improve the visibility of performance differences between methodologies in its figures. Currently, the color-coding for each method is not distinct enough, and the representation of all performances via bar graphs makes it challenging to discern if the differences, including standard deviations, are statistically significant. In terms of visibility, it falls significantly short.","- The paper could greatly improve the visibility of performance differences between methodologies in its figures. Currently, the color-coding for each method is not distinct enough, and the representation of all performances via bar graphs makes it challenging to discern if the differences, including standard deviations, are statistically significant. In terms of visibility, it falls significantly short.",458,0
bsKMPAFHO7,ICLR_2024,"- There is no improvement in regrets when using CFR based on UTC dynamics. Although Figure 2 shows that the regret of CFR outperforms that of Farina & Pipis (2023), the paper does not provide an explanation for this.
- The paper provides only an informal proof of why UTC dynamics have better computational efficiency than Farina & Pipis (2023) in Section 1. A more formal discussion is required and should be placed in Section 6, rather than in the introduction.
- Some notations, such as ${\text co}{\mathcal X}$, are used before being defined.
- I cannot locate a formal definition of linear deviations. Given its frequent use in this paper, at the very least, an explanation is necessary.","- I cannot locate a formal definition of linear deviations. Given its frequent use in this paper, at the very least, an explanation is necessary.",459,0
ajRRisV1n1,ICLR_2024,"- In general, I had a hard time following the writing style of the paper. The sentences are sometimes unnecessarily complicated and/or too long, which makes it harder to understand the ideas presented in the paper. My overall comment is that the paper needs a significant amount of rewriting and polishing.
- The improper citation notation is one of my major concerns about this paper. The existing citations are not inside parentheses which interrupts the flow of the text for the readers.
- There is no need to capitalize Machine Learning in the first paragraph of the introduction.
- As far as I checked, the c.f. usage is wrong throughout the paper (https://blog.apastyle.org/files/apa-latin-abbreviations-table-2.pdf). This document says it is used to provide contrasting information but I observed that it is probably used instead of i.e. or e.g. in this paper.
- This sentence in the introduction needs citations: ""Originally GT was applied for identifying infected individuals in large populations using pooled tests...""
- The words in the parentheses can be added to this sentence: ""More information, applications and links could be found in the seminal book (of) Du et al. (2000) and recent literature (by) Klonowski et al. (2022); Kowalski & Pajak (2022b).""
- LocAS is mentioned in the footnote of page 3 before it has been introduced in the main body of the text.
- The problem is not motivated enough beyond the group testing setting.
- The frequent use of the dash (-) gives an informal tone to the paper.
- ""Intuitively, having elements from other clusters in the query may negatively influence, or even clash, the learning process within a given cluster – hence, the goal is to do local learning of k hidden elements within the cluster and simultaneously avoiding the other ℓ “bad” clusters."": No need for a comma after the clash in this sentence. (Page 3)
- Typo in ""qyery"". (Page 5)
- Did you mean existing result instead of existential result on Page 7? If not, what is an existential result?
- Typo in ""guarantying"". (Page 9)
- The beeping feedback mention on Section 5.2 needs citations.
Note: I am aware that almost all of the remarks I make under weaknesses related to language and grammar concerns. I am also ware that some of them are easily fixable. I do not want to discriminate against anybody based on their linguistic abilities. My main problem here is that, when the frequency of such mistakes increase, it makes it harder to focus on the actual contents of the paper. Plus, it signals me that the sufficient effort to express the novelty of the paper clearly and efficiently has not been made. I strongly believe that expressing our ideas clearly is a skill we all should aim to advance ourselves as researchers.",- The improper citation notation is one of my major concerns about this paper. The existing citations are not inside parentheses which interrupts the flow of the text for the readers.,460,0
DslxExr5Kn,ICLR_2024,"1.	Using auxiliary tasks to help the actor’s decision-making is not a new thing. Although the authors claim that predicting the critic’s global presentations is better than other forms, there is no theoretical or experimental evidence in the paper to support this claim.
2.	Besides adding the predicting loss, the authors also utilize the residual connections in the network design, the authors do not show the ablation study of this trick and whether it could boost the performance of the compared baselines.
3.	The performance improvements are slight, especially compared with MAT-Dec. In most cases, the performances of APC and MAT-Dec are very similar.","3. The performance improvements are slight, especially compared with MAT-Dec. In most cases, the performances of APC and MAT-Dec are very similar.",461,0
u3dHl287oB,ICLR_2024,"1. The dataset assumption is strong, which only focuses on 2 datasets, and can be converted by orthogonal transformation.
2. It is unclear how DOTS can compare with the notion of similarity in related works.","1. The dataset assumption is strong, which only focuses on 2 datasets, and can be converted by orthogonal transformation.",462,1
itrOA1adPn,ICLR_2024,"- My main issue is the comparison of a simple CNN to an animal vision system, i.e. ""We modeled the CNN after the early mammalian visual system: the base layers were grouped sequentially into the photoreceptor (PR), bipolar (BP), retinal ganglion cell (RGC), lateral geniculate nucleus (LGN), and primary visual cortex (V1) layers.” As far as I understand, it’s just a different number of channels and kernel sizes? Naming the different layers in a network after biological brain regions does not directly make them more biologically realistic.
- Environments in nature are much more complex than the ones proposed in this paper. To study visual ecology, it seems our agent environments need to be more complex as well.
- There is a lot of related work in the evolutionary community, which isn’t mentioned, where survival is the only reward mechanism.
- In conclusion, I would argue that a computational framework on visual ecology has to go beyond one experiment/domain with a slight variation on a deep RL setup","- There is a lot of related work in the evolutionary community, which isn’t mentioned, where survival is the only reward mechanism.",463,0
w2GlpOHdg1,ICLR_2024,"- The novelty is limited. The skip-gram method for embedding words and the LSTM architectures are both out-of-dated on handling text data. I would expect this method propose a novel text-based learning technique on obtaining good materials representations from text.
- The compared baselines are inadequate. Some more recent and powerful methods [1, 2] have been proposed and should be reported against the proposed method empirically.
Overall, I would recommend this paper to resubmit to some domain-specific journals.
[1] Yan, Keqiang, et al. ""Periodic graph transformers for crystal material property prediction."" Advances in Neural Information Processing Systems 35 (2022): 15066-15080.
[2] Lin, Yuchao, et al. ""Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction."" arXiv preprint arXiv:2306.10045 (2023).","- The compared baselines are inadequate. Some more recent and powerful methods [1, 2] have been proposed and should be reported against the proposed method empirically. Overall, I would recommend this paper to resubmit to some domain-specific journals. [1] Yan, Keqiang, et al. ""Periodic graph transformers for crystal material property prediction."" Advances in Neural Information Processing Systems 35 (2022): 15066-15080. [2] Lin, Yuchao, et al. ""Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction."" arXiv preprint arXiv:2306.10045 (2023).",464,0
MNFKm4AStf,ICLR_2024,"The paper is poorly written. Here are some of the confusing points:
1. What are “generative factors” as discussed in Prop. 1? Are they in the dataset? If so, what is the difference between $y$ and $x$?
2. What does “proposition” mean in this paper? Are they claims with proofs? They seem to simply be declaring or defining something.
3. In Prop. 2, what does it mean to say that “the identification of the causal effect $p(x \mid do(y_c))$ provides the recipe to control for dependencies between the generating factors producing entanglements through a combination of d-separation constraints and data from $p(x, y)$”? Further, does this imply that $p(x \mid do(y_c))$ is the desired query of interest? And is the discussion about d-separating $X$ from $Y$, or is it about d-separating some variables of $Y$ from other variables of $Y$?
4. How exactly is ReI defined (as in Prop. 3)? Is it the second term of Eq. 2? This could be clarified better.
5. What is the motivation of having the objective of $p(z \mid x, do(y)) = p(x \mid z)p(z \mid do(x))$? How was this derived? If Bayes rule is used here, doesn’t that drop $p(x \mid do(y))$? Also, these terms seem very different from what was introduced earlier.
6. Is Prop. 4 an assumption? This has to be clarified.
7. What exactly is the issue of collider bias with other approaches, and how does this approach avoid it?
Some factual issues are highlighted below. There may be more that I did not catch due to the clarity issues from above.
1. The factorization in Eq. 1 is not enough to argue that a DAG is causal. For example, the graph $X \rightarrow Y$ factorizes as $P(X, Y) = P(Y \mid X)P(X)$. However, the same equality holds even if the graph is $X \leftarrow Y$, despite having a different causal interpretation. It must be clarified that the arrows of the DAG represent causal relationships.
2. Def. 3 is not quite accurate. Some causal queries can be identified even without a d-separating set.
For these reasons, I cannot recommend acceptance of this paper.","5. What is the motivation of having the objective of $p(z \mid x, do(y)) = p(x \mid z)p(z \mid do(x))$? How was this derived? If Bayes rule is used here, doesn’t that drop $p(x \mid do(y))$? Also, these terms seem very different from what was introduced earlier.",465,0
YmQyEdLIkU,ICLR_2024,"The paper is narrow in scope and parts of the paper appear to be hastily written. It is unclear whether the contribution is complete enough to justify a top-tier publication. In particular, the fact that the paper limits its consideration to adversarial points that are infinitesimally close to data points is probably unrealistic.
The definition of adversarial examples appears a bit different than the usual definitions, relying on a local limiting assumption. Some work needs to be done to explain the relationship between this paper’s definition and the working definition used of the broader community.
The figures are poorly presented, and captions could be clarified.
The paper says “Our results, however, are currently only directly applicable to FCNs and regression tasks. Nevertheless, we anticipate that the core insights from our research could be extended to encompass classification tasks as well as other neural architectures; this is a clear path for future research.” It is not clear in the experiments that the authors are training regression tasks on MNIST, if this is the case, this needs to be clarified in the paper.
Small issues:
* Please check for style, e.g. correct use of \citet and \citep.
* At the end of Section 1, the authors say they “estimated the integral of relevant quantities near zero.” Please make this more specific to outline contributions up front.
* Space before comma 2nd line from the bottom on page 1.
* At the end of Section 6, add a citation for the claim “Researchers have demonstrated that a technique involving the iterative elimination of the dominant eigenvalue direction in the Fisher Information Matrix leads to the generation of adversarial examples.”","* At the end of Section 1, the authors say they “estimated the integral of relevant quantities near zero.” Please make this more specific to outline contributions up front.",466,0
SkF7NZGVr5,ICLR_2024,"- The paper doesn't evaluate across different architectures, and only considers extremely small MLPs. Given that prior work has noted that an intervention's efficacy often heavily depends on the architecture to which it is applied, it is important to evaluate an intervention on a range of architectures to get a full view of its robustness.
- Saying that the curvature of the neural network is reduced because of a reduction in srank might go against existing notions of curvature in the literature on deep neural network training, which often characterize the curvature by the maximal Hessian eigenvalue. In particular, a reduction in srank of the features is often caused by the largest eigenvalue growing faster than the remainder. It is possible that a similar phenomenon is happening here, wherein the largest eigenvalue of the Hessian is growing faster than the other eigenvalues. A visualization of the eigenvalue distribution at the start vs at the end of training would be useful. I would be inclined to call a reduction in the srank an increase in the 'degeneracy' of the Hessian, rather than a reduction in curvature.
- The rank of the Hessian with respect to the last two layers of the network is likely to be highly correlated with the rank of the penultimate feature layer. Figures 1 and 2, for example, show that the rankings given by the srank of the Hessian and the srank of the features largely agree with each other. Similar issues in the correlation between the feature srank and the error over time also emerge in the Hessian srank, for example the leaky-ReLU network in Figure 2, middle column. The evaluation of the srank of the features in Figure 1 appears to have either been performed with a very narrow network or a very small batch size, making it difficult to evaluate the significance of these measurements. I think the paper would benefit from a closer examination of the difference between the Hessian rank and the feature rank in these networks.
- One of the biggest issues that emerges from the paper is the observation that the Hessian srank often exhibits similar inconsistencies to other quantities, like the srank of the features, which were used as justification to dismiss these quantities as good explanations of plasticity. For example, weight decay and regenerative regularization both exhibit similar Hessian srank, but weight decay exhibits greater plasticity loss in Figure 4.
- The choice to study the rank of the Hessian, as opposed to the rank of the empirical NTK or the features, is not given sufficient motivation. Each of these quantities reveals something meaningful about the loss landscape, and it is not immediately obvious that the Hessian should be preferred over the other two.
- While the proposed regularizer is reasonable, it feels very much orthogonal to the curvature of the network. The causal connection between preserving the distribution of the parameters and maintaining the rank of the hessian remains vague, and the benefits of the regularizer on the rank appear incidental.
- Because of the limited set of evaluations, it is difficult to say whether the proposed regularizer will be useful in other contexts, where it may be necessary for the distribution of weights to shift in ways that increase the Wasserstein distance from initialization, or where the regularizer may slow down learning too much to be practical. It would be useful to evaluate on other datasets and to also show the learning curves; for example, the worse online learning results for the wasserstein regularization coupled with the better end-of-training results suggest that not only does the loss start out much higher, but that it stays higher than the other methods for a long time -- in other words, it is resulting in much slower learning.","- One of the biggest issues that emerges from the paper is the observation that the Hessian srank often exhibits similar inconsistencies to other quantities, like the srank of the features, which were used as justification to dismiss these quantities as good explanations of plasticity. For example, weight decay and regenerative regularization both exhibit similar Hessian srank, but weight decay exhibits greater plasticity loss in Figure 4.",467,0
IPayPEGwdE,ICLR_2024,"1.From the experimental results, it is observed that the choice of λ significantly influences the algorithm's performance comparison. Could authors provide the variation curve under larger λ values?
2.Moreover, how should λ be specifically adjusted, especially when dealing with entirely new contexts in a new scenario? Additionally, when λ is less than nk (e.g., λ=390), CE's performance is inferior to that of UE. How can this be explained?
3.There are too few experimental results, and is it possible to provide the source or generation rules of the experimental data? Although the theoretical aspects are solid, the experimental section needs further improvement, especially in terms of interpretability. The motivation given earlier pertains to a cold start scenario. Could authors provide further explanations for the experiments?","3.There are too few experimental results, and is it possible to provide the source or generation rules of the experimental data? Although the theoretical aspects are solid, the experimental section needs further improvement, especially in terms of interpretability. The motivation given earlier pertains to a cold start scenario. Could authors provide further explanations for the experiments?",468,0
jkvZ7v4OmP,ICLR_2024,"1. Proposition 1 and 2 - are simple extensions of two known results from linear algebra - why is this presented without citations - to linear algebra books from Strang, Roman, etc.?
2. Novelty is definitely present - but not something completely unexpected and draws and builds upon existing literature","1. Proposition 1 and 2 - are simple extensions of two known results from linear algebra - why is this presented without citations - to linear algebra books from Strang, Roman, etc.?",469,0
0SgPbbyrWh,ICLR_2024,"1.	Graph-based methods and vector quantization methods are widely used for vector search now and show significantly better performance than LSH. The authors state that “Whereas graph-based techniques perform very well in low dimensions, their performance is suboptimal in higher dimensions, and cannot be used in distributed settings.” This is NOT true! The HNSW paper shows that proximity graph works quite well for high dimension vectors, and the Pyramid paper uses proximity graph for distributed vector search. Also, the FAISS library uses many variants of PQ and the IVF index for vector search. I am not convinced of the practical impact of this paper unless the authors show that the proposed LSH achieves better time-recall curve than graph and quantization methods. Note that time-recall curve is a standard method to evaluate the performance of ANNS, and examples can be found in the HNSW paper.
[1] Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs (HNSW)
[2] Pyramid: A general framework for distributed similarity search on large-scale datasets (Pyramid)
[3] Product Quantization for Nearest Neighbor Search (PQ)
2.	Experiments are very limited. (1) Nowadays, million or even billion scale datasets are common for vector search (check the Pyramid paper) but the authors use a datasets of 10,000, which are extremely small. (2) The proposed method needs to check every vector in the dataset to compute Hamming distance, which is infeasible for very large datasets. (3) Pls use standard time-recall curve to compare the performance of the authors in the main experiments. The influence of m/d can be used as a micro experiment to check the influence of parameters.
3.	I think the paper is not ready for publication is its current state and would benefit from substantial revision for another conference. For instance, there are multiple missing references even in the introduction. Some statements are not logical or scientific due to the lack of checking. For example, “LSH…, where the imperative is to swiftly identify similar items without sacrificing accuracy.”, well, LSH actually sacrifices accuracy to identify similar items swiftly; “To balance between accuracy and query time, we can make sure the centroids used for the space partitioning are maximally separated”, balance is a vague term, and how does maximal separation leads to such balance?; “Existing LSH has …(ii) limited use cases”, what exactly are the use cases your proposed method can handle and existing LSH cannot?; “These methods however are limited for use with a specific hash length or embedding size”, why is the case? To my knowledge, at least random projection allows to handle arbitrarily long embeddings and flexibly set the length of the embeddings.
4.	How does the method handle more general cases, i.e., when dataset vectors are not on the unit sphere? The optimization objective in (1) is different from the max-min objective of OSC. What is the relation of the solution produced by Algorithm 1 w.r.t. the solution of OSC? Can you show that it is a good approximation?","4. How does the method handle more general cases, i.e., when dataset vectors are not on the unit sphere? The optimization objective in (1) is different from the max-min objective of OSC. What is the relation of the solution produced by Algorithm 1 w.r.t. the solution of OSC? Can you show that it is a good approximation?",470,0
sRBnyzoqkU,ICLR_2024,"- The method has very limited novelty over Task Vectors and is hacky. It is also not described very rigorously. The selection process for masking relies on the absolute magnitudes of the weights, but how do you decide what is ""excessively large"" or ""relatively small""?
- There is no clear justification for the hacky masking method. Presumably, it is just having some sort of regularizing effect, but this is not explored and there are other ways that people can regularize models to improve performance.
- In addition, the experimental evaluation is quite limited, even compared to the original Task Vectors paper, which considered other benchmarks (including language tasks) as well. Additionally, the authors claim that they do not need a validation set but in fact it seems like they still do in order to tune hyperparameters on at least some of the tasks.
- Overall, the paper presents very limited new insights over the existing Task Vectors paper.","- Overall, the paper presents very limited new insights over the existing Task Vectors paper.",471,0
amjNJMpBiq,ICLR_2024,"1.	Relevance: The greatest weakness of this paper is the lack of relevance of the problem studied. Specifically, it is very well known that computers perform finite-precision arithmetic, and for any computation (much beyond adversarial robustness), the results might have a minuscule margin of error due to the floating point approximations involved. In the light of this, any application that requires exact precision must implement explicit safeguards (e.g. interval arithmetic) to protect against rounding problems. For the case of adversarial examples, it would be good if the authors could provide examples of applications where an application would care about getting the certified radius exactly right (this is not true for instance on many commonly studied applications like image classification, object detection, etc.).
2.	Trivial Solution?: A naïve fix to the above problem, in the context of certified robustness, which the paper also mentions on P2, is that for super-sensitive applications, one can simply report $R(x) - \gamma$ as the certificate for a small $\gamma$, instead of $R(x)$ to ensure that the output certificate is valid. $\gamma$ can be really small, for instance, P8 mentions that for MNIST, the approximation errors are at the 13th decimal point. The paper argues that finding a fixed $\gamma$ is not possible in general, but fails to provide convincing evidence on any real tasks. On a synthetically constructed task, $\gamma$ is showed to be potentially large (0.1), but then the scale of $R$ is orders of magnitude larger leading to the same solution. Generally, depending on the task at hand, there should be a principled way to choose $\gamma$ depending on scale of the error that can be tolerated for the problem.
3.	Proposed method exponential in dimension?: The proposed approach to finding adversarial perturbations, is roughly to perturb each dimension of the input a bit till a misclassification is achieved. A slightly smarter way of doing these floating point flips is utilized, but the approach is essentially exponential in dimension. How much time does the proposed algorithm take per image? from P8, it seems like a time-out of 15 minutes is set for every image.
4.	Proposed Method ReluPGD same as PGD applied to max-margin objective? Could the authors comment on how the ReLUPGD algorithm differs from a standard PGD attack using the max-margin loss (e.g., loss(positive class) - loss(negative class) for the binary case).
5.	Proposed Method’s applicability in real scenarios: In practice, most certification methods for Neural Networks produce certificates that are very pessimistic for most input points. In light of this, it is hard to believe that a very high decimal place attack would even be successful for real networks. The examples shown in this work linearize the network or study synthetic problems with linear boundaries, and this effect does not show up.
6.	Writing: The writing could be improved at several places, and some portions could be cut to make way for the core contributions:
1.	P2 Contribution 1: “invalidate the implementations” — invalidate might be a too strong word here.
2.	P2 Contribution 2: “floating point norm” is not defined till now.
3.	Statement unclear: “in the case where the library computes …”, which library are we talking about here?
4.	P2 Contribution 3: Cohen et. al.’s main certificate is probabilistic.
5.	P3: No reference needed for distance of a point to a plane.
6.	P4: The main contribution, the “neighbors” algorithm could be moved to the main text for clarity, currently it is unclear what this is.
7.	P5: It is unclear whether the entire warmup section is needed, since the end result is a PGD-like algorithm, and PGD is quite standard in the literature now.
8.	P5: The algorithm ReLUPGD should appear in the main paper, since it is one of main contributions.
9.	Fig 2, P6: For the synthetic experiments, what is the setup, is $x, w, b$ fixed and $D$ is changing? If so, how were these values decided? Are there any average statistics over these values?
10.	P9: Theorem 1 is a standard statement of interval arithmetic, and as such it is unclear whether it should be a theorem.","7.P5: It is unclear whether the entire warmup section is needed, since the end result is a PGD-like algorithm, and PGD is quite standard in the literature now.",472,0
80wh3jjCZf,ICLR_2024,"I should first couch my review by noting I'm not the most familiar with the literature in this area, so this is an educated guess:
- The maze domain seems a bit too trivial to be useful, it is informative for a demonstration of the method, but unlike the inventory management environment, I'm not sure what the real-world justification might be. I would prefer to have at least one more grounded and complex environment (perhaps a video game with many different possible actions, perhaps a multi-agent setting? I'm not the most familiar with the domains in this subfield).
- Perhaps this is my unfamiliarity with the domain, but sometimes the text is quite hard to follow. I think part of this is the reliance on lots of acronyms (LDAS, SLDAS, MILP, SA), but also the claim in Section 2 ""we formulate the task of finding a Q-value maximizing a as a mixed-integer linear program"" was a bit confusing on first read, as really the authors are proposing an approximate solution, so saying this upfront might be a bit more clear.
- Would be nice to see MILP results in Figure 5 against the proposed method and other baselines in Figure 3, and the discussion at the end of Appendix B is informative and it may be nice to have a note in the text","- Perhaps this is my unfamiliarity with the domain, but sometimes the text is quite hard to follow. I think part of this is the reliance on lots of acronyms (LDAS, SLDAS, MILP, SA), but also the claim in Section 2 ""we formulate the task of finding a Q-value maximizing a as a mixed-integer linear program"" was a bit confusing on first read, as really the authors are proposing an approximate solution, so saying this upfront might be a bit more clear.",473,0
IBACinPJG5,ICLR_2024,"* The proposed method reads as a large, complex collection of unmotivated components, and little insight is given as to why they are necessary. I would encourage authors to narrow down the key, novel elements of their method and to propose a more fundamental motivation and rigorous analysis of their added value.
* Empirical results consist of some illustrative examples (are these random examples or cherry-picked?) and few rigorous numerical analyses.
* Not sure I understand the premise of interpretability/explainability used in paragraph 1 of the introduction.
* Presentation: I found the paper confusing and tiresome to read.
* Figure 1 is overcrowded and confusing. Most elements are undefined. Poor quality (delineation of underlying elements, poor alignment). Figure 2: what is CCE loss? What is shift loss?
* Figures in the experimental results section are generally illegible with little or no labeling. Fig 4, for example: what are alternate features? What do the medical variables correspond to, and how does this correlate to medical insight? There is no legend for what dashed/solid lines correspond to.
* A lot of notation is undefined. e.g. what is index $w$? Difference between $Z$ and $z$?
* Definition of abbreviations (many in abstract!)
* Language is overcomplicated (see abstract again) with many undefined or unclear ideas: “proactive comprehension of trajectory to an extremity”, “observations are competitively mapped”, “part of its learning stride”, “collaboratively trained”, “results are assuring”, “Recollect that”, “agrressive” typo, “movement to criticalities on temporally chaotic datasets”, “Providing Ground explanations” etc.
* Please put references in parentheses when they do not form part of the sentence.
* Missing hyphenation between words (“outcome oriented”, “down stream”, “scale varying”) and punctuation.
Unfortunately, with such major issues unaddressed, the manuscript is not ready for publication.",* Please put references in parentheses when they do not form part of the sentence.,474,0
eZAlb8fX5y,ICLR_2024,"- While ternary computations can simplify attention-related calculations, the authors haven't quantified the reduction in latency or the number of FLOPs saved by their method.
- Given that ternary representation uses 2 bits to represent -1, 0, or +1, its memory footprint might surpass binary-based quantization. A comparison of memory usage between previous quantization methods and the proposed approach is essential.
- The correlation between sparsity and computational reduction doesn't directly equate to reduced latency or improved throughput. Instead of merely highlighting channel sparsity, tangible hardware benefits should be assessed.
- What is the net effect on inference? The paper would benefit from a thorough estimation or actual measurement results.","- The correlation between sparsity and computational reduction doesn't directly equate to reduced latency or improved throughput. Instead of merely highlighting channel sparsity, tangible hardware benefits should be assessed.",475,0
sk7RRHFk7M,ICLR_2024,"Weakness:
- **Ethics issues. Ethics reviewers are required to review whether double-blind regulations have been violated.**
- In code `config/__init__.py`, I found there exist Chinese characters and some codes like `MSRA PC Node`. As we know, MSRA means Microsoft Research Asia. This would make the reviewer to infer the authors’ nationality and possibly Microsoft affiliation.
- The authors did not discuss possible ethical issues with this study. Including issues such as race and gender bias.
- About Citation
- It seems that authors are not clear on the difference between `\cite` and `\citep`. For example, “… ControlNet branch with the pre-trained U-Net weight following (Zhang & Agrawala, 2023).” should be“… ControlNet branch with the pre-trained U-Net weight following Zhang & Agrawala (2023).” There are more than one similar problem. During rebuttal, please list all of similar issues and revise them, I will check them one by one.
- I would like to point out serious issues with the wrong citation on Grounded-SAM. The first implementation of Grounded-SAM is the https://github.com/IDEA-Research/Grounded-Segment-Anything, which has been accepted as an ICCV demo. Please cite it correctly. Besides, if authors used it, please cite Grounding DINO.
- Instructpix2pix was accepted CVPR, not an arxiv paper. Please cite it correctly. There is more than one similar problem. During rebuttal, please list all similar issues and revise them, I will check them one by one.
- Missing comma in the equation of Section 3.1.
- When summarizing contributions, `To address this problem, ...` should be `To address these problems, ...`.
- Reproduction
- When I try to run the code. I found it missing README and I cannot run the code correctly. I am not sure whether the codes are appropriate.
- Missing video demo. This is very essential for me to check the results. And the video comparison with baselines is needed.
- Technical Design
- It is hard for readers to check the technical designs in Figure 2. The CLIP feature is fed into TM module (ResBlock and TransBlock). How do the lines without arrows connect to the module? How do the lines with arrows connect to the module?
- Do the features output by each layer need to be processed by BG ControlNet and Pose ControlNet and then added to the middle layer? If there are $X$ down layers, in the UNet middle layer, will features be added about $2X$ times?
- Why not compare with Follow-your-pose?
My main concern comes from ethics issues and writing issues. There is something confusing that makes readers hard to follow this work. I spent about 8 hours checking this submission and tried to run the codes (but failed). I plan to rate it as 4 but the system only supports 3 or 5. Therefore, I rate it as 3 now. After the rebuttal and reviewer discussions, I will revise my rating to provide a clear rating of accept or reject.","- **Ethics issues. Ethics reviewers are required to review whether double-blind regulations have been violated.** - In code `config/__init__.py`, I found there exist Chinese characters and some codes like `MSRA PC Node`. As we know, MSRA means Microsoft Research Asia. This would make the reviewer to infer the authors’ nationality and possibly Microsoft affiliation.",476,0
NLbRvr840Q,ICLR_2024,"1. To the terminology (and the preliminary math tools) of dynamical systems used in this paper, it seems that the authors does not soundly cook its article based on the strict math that has been widely accepted in math and engineering. See Queation 1, Weakness point 2 and 3.
2. The discussion of statibility seems wrong. That's why we say the authors may not well pick up knowledge of (linear/nonlinear) dynamical systems. Sec. 5.1 discussed the stability of HDS-ODE, where the statement of the first sentence in this section is basically wrong. The so-called ""control"" term also iterate over time. It cannot be simplified the stability analysis of HDS-ODE as simply a dissusion of linear system X_dot = A X. Let us use the eq.(3) to clarify the point simply. Supposing we accept the split of the general state-space equation X_dot = f(X, t) as eq.(3), it is obviously that the stability analysis is a general stability analysis of nonlinear system, besides AX(t) there also exists g(X(t))! If you said your discussion of HDS-ODE refer to the neural implementation, we can see that in eq. (6) the nonlinear term is still there, except it is in NN form, and then embeds into eq.(7) to complete the iteration t+1. Furthermore, these propositions on stability for your ""simplified"" linear systems are well-known in the field of electrical engineering.
3. Your abstract and contribution summary tell that yours studies the ""controllability"", which we could find anything related to it. And regarding the starting point of the most general eq.(2), there cannot be any controllablity-related problem can be formulated. Maybe you refer to something different? We strongly recommend the authors to learn essentials of dynamical systems, it helps to avoid conceptual misunderstandings and misuse for better communications.
To help you with essential knowledge on dynamical systems, you may refer to the following classic textbooks (basics on linear, nonlinear systems):
- Zhou, K., Doyle, J. C., & Glover, K. (1995). Robust and Optimal Control. Pearson.
- H. K. Khalil, “Nonlinear Systems,” 3rd Edition, Prentice Hall, Upper Saddle River, 2002.
Your idea may be valuable and appreciated, considering your sound performance in experiments. However, you really have to first carefully deal with theory and fix any possible mistakes.","1. To the terminology (and the preliminary math tools) of dynamical systems used in this paper, it seems that the authors does not soundly cook its article based on the strict math that has been widely accepted in math and engineering. See Queation 1, Weakness point 2 and 3.",477,0
d2YjPbSpDZ,ICLR_2024,"Despite its clear strengths, the paper suffers from several problems, which are listed below. I would increase my rating if the major concerns can be addressed, as the paper is among the few works that provides insight into the generalization performance of FL.
1.	The model, i.e. the consideration of a linear model with i.i.d. Gaussian data, is very simple and unrealistic. However, as mentioned by the authors, this is now a commonly used model as a first step in understanding the theory.
2.	In particular, there is an inherent alignment between empirical risk and generalization error in the considered setup. That is, they are somehow simultaneously minimized, which is in sharp contrast to most real-world scenarios. In fact, throughout the paper, the authors study ""model error,"" a quantity that one would also study for empirical risk. For this reason, I am not sure if the findings on generalization behavior using this setup can be extended or useful for realistic setups.
3.	In addition, the authors stated that the model error can be shown to be equal to the expected test error for noise-free data. What happens if the data is not noise free? (I guess this is the interesting case, right?). How does the ""model error"" relate to the generalization performance in this case?
4.	Continuing the point 2, a good “generalization bound” typically decreases with the total number of used symbol (here, it would be $m\times n \times t$ at the end of iteration $t$, in the simple case). How does the provided bounds behave with these quantities?
5.	It was observed and partially shown in previous papers (e.g. Gu et al. 2022 or arXiv:2306.05862) that the generalization error of the federated learning is smaller than that of centralized one (i.e. if we keep $m\times n$ fixed). Do authors observe similar phenomena using these theoretic results?
6.	All plots provide the numerical evaluation of the bounds on the model errors. It would be useful to plot the estimated model error as well as the estimated generalization error in the plots to observe how well the provided bounds capture the correct behavior of both the model error and the generalization error.
7.	Some relevant references are missing, e.g. arXiv:2306.03824 and arXiv:2306.05862.
8.	The constants defined in equations (10) to (12) require some explanation and intuition. What is each term made of and what do they represent? In its current form, it's very hard to parse and understand. Similarly, theorem 1 is very hard to parse (except for the simple form of (14)). Similarly, for other theorems.
9.	Theorem 1 (and other results) suggest that increasing the learning rate $\alpha$ increases the model error. While this behavior makes sense from an empirical risk point of view, it's often the opposite for generalization behavior: larger learning rates, at least in some cases and for the classical centralized setup, lead to better generalization performance.
10.	Furthermore, Theorem 1 suggests that more heterogeneity leads to higher generalization error. This is also counterintuitive to me. After all, this is exactly what I would expect for empirical risk, but I would expect the opposite for generalization.
11.	If I'm not mistaken, equation (16) (and Proposition 1) is developed for the ""constant learning rate"" scenario. While for this scenario the existence of an optimal $K$ makes sense, I was wondering what would be the behavior with respect to $K$ for the carefully tuned decreasing learning rates?
12.	The bound of Theorem 3 interestingly suggests a double descent phenomenon. However, the paper would greatly benefit from a numerical verification of this.
13.	It is written that ""In Figure 3, we plot the model error against p..."". However, the caption of Figure 3 states that ""The curves are theoretical values from Theorem 3"". It is a bit confusing whether the exact ""model error"" is plotted or the established bound on it? It would be useful to plot both (as well as the generalization error).
14.	The proofs are very long and technical, and it takes a lot of time to go through them. I think it would be helpful (and perhaps even necessary) to give a proof sketch in the main text, explaining the main steps, ideas, and tools used from the literature, and what the authors have added. In the current format, it is not clear what technical novelty the authors bring to this work.
15.	The supplementary material (in particular Appendix A) may be more appropriately organized, with a table of contents to guide the reader to the various sections.
16.	It would be good to summarize the provided insights in the contribution part of the introduction.","2. In particular, there is an inherent alignment between empirical risk and generalization error in the considered setup. That is, they are somehow simultaneously minimized, which is in sharp contrast to most real-world scenarios. In fact, throughout the paper, the authors study ""model error,"" a quantity that one would also study for empirical risk. For this reason, I am not sure if the findings on generalization behavior using this setup can be extended or useful for realistic setups.",478,0
ycv2z8TYur,ICLR_2024,"- Novelty. Many design components of EMerNeRF have been proposed in previous work, including separated static and dynamic fields [2][3], sky head [1], shadow head [2], flow field [4]. The paper lacks a detailed discussion highlighting the differences compared to these previous works.
- Generalizability. The proposed method lacks verification in existing dynamic scene datasets used in baselines such as Nerfies [5], HyperNeRF [6]. Those datasets contain more complex deformations and a significant proportion of dynamic components. It is necessary to evaluate the robustness and understand the limitations of the proposed design modules, including dynamic density regularization and self-supervised flow field.
- Ambiguity of equation 6. The meaning and formulation of the expectation of the dynamic density remain unclear.
- Baseline. The quantitative evaluation lacks a more recent baseline [7].
[1] Konstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 12932–12942, 2022
[2] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. Dˆ2nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. *Advances in Neural Information Processing Systems*, 35:32653–32666, 2022.
[3] Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. ""Nerf in the wild: Neural radiance fields for unconstrained photo collections."" In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 7210-7219. 2021.
[4] Du, Yilun, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, and Jiajun Wu. ""Neural radiance flow for 4d view synthesis and video processing."" In *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 14304-14314. IEEE Computer Society, 2021.
[5] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 5865–5874, 2021a.
[6] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. *arXiv preprint arXiv:2106.13228*, 2021b.
[7] Cao, Ang, and Justin Johnson. ""Hexplane: A fast representation for dynamic scenes."" In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 130-141. 2023.",- Ambiguity of equation 6. The meaning and formulation of the expectation of the dynamic density remain unclear.,479,0
PXD3FAVHJT,ICLR_2024,"- Missing Related Work
In fact, there is a theoretical comparison of RLHF and SFT-style methods in the framework of imitation learning [1]. Indeed, LLMs are imitating human speech. In that framework, RLHF corresponds to adversarial imitation learning (AIL) methods, and SFT corresponds to behavioral cloning (BC). To the best knowledge of the reviewer, that theoretical study reveals that AIL (RLHF) methods can have better out-of-distribution generalization performance than BC (SFT) because AIL methods optimize their policy on out-of-distribution states (prompts) and rigorously prove this phenomenon. I believe this related work is insightful for studying the advantages of RLHF over SFT, and this paper should be mentioned in the related work.
[1] Xu, Tian, et al. ""On the generalization of adversarial imitation learning and beyond."" *arXiv preprint arXiv:2106.10424* (2021).
- Typos and Writing Suggestions
1. There are two minus symbols in Equation (1).
2. It seems unusual to draw a conclusion in Section 6.3 while presenting empirical evidence in Appendix I.",2. It seems unusual to draw a conclusion in Section 6.3 while presenting empirical evidence in Appendix I.,480,0
RTL8fWgJaS,ICLR_2024,"- The proposed method, self-specialization, is an extension of the self-instruct [1] work to recover certain expertise in the LLMs. However, the method is only designed and evaluated for one specific domain without showcasing its generalization ability to other domains.
- The compared methods in the paper are based on zero-shot/few-shot settings, while the proposed method uses LoRA for finetuning. It would be beneficial to include stronger task-specific comparison methods to better illustrate the effectiveness of the proposed approach.
- Since the method has multiple components, it would be helpful to show the contribution of each component to the overall performance. For example, how does domain-specific response generation impact the final results?
- The performance in the knowledge sparse domain is uncertain. It is understood that domain response generation involving harnessing knowledge is one of the main contributors to the specification, but it is important to understand the method's bottleneck when existing knowledge is limited for certain domains.
- The paper should discuss potential data contamination and address how the authors ensure that the data for downstream testing does not overlap with the generated data.
[1] Wang, Yizhong, et al. ""Self-instruct: Aligning language model with self generated instructions."" arXiv preprint arXiv:2212.10560 (2022).","- The performance in the knowledge sparse domain is uncertain. It is understood that domain response generation involving harnessing knowledge is one of the main contributors to the specification, but it is important to understand the method's bottleneck when existing knowledge is limited for certain domains.",481,0
NmaXXAiJJC,ICLR_2024,"1. The conclusion regarding the speed-up performance is not convincing. There is no practical speed-up results presented in the work and the reported FLOPs reduction is only calculated theoretically. Moreover, traditional dense matrix multiplication has been highly optimized in recent years on modern GPUs, e.g., cuDNN. Whether or not a network with vector quantization infers faster than its dense counterparts on modern GPUs remains uncertain. Thus, the lack of inferring time and practical speedup results does not adequately justify their work.
2. Some aspects of the VQ-DNN illustration require further clarification. For instance, the specific process of updating W_jb and e in equation (1) needs to be clarified. Are they updated alternately? Additionally, the method of mapping weights to the codebook is not clear. Does the mapping change during training? It is necessary to provide more detailed explanations for these concerns.
3. The experimental section is confusing probably due to the unclear selection of compared methods. The related work section does not introduce the compared methods in Table 3, 5, and 6, making it difficult to understand the state of the arts. Furthermore, the authors introduce four categories of model compression in the related work section, such as pruning, and quantization, but they do not present the performance comparison between them and the proposed methods.
4. The experimental setup could be improved. ResNet50 is not reasonable for being used on CIFAR10 since it is too large and prone to overfitting, potentially making the experimental results noisy. Additionally, all experiments are focused on classification. The proposed method should be extended to other tasks to provide more generalized results.
5. The quality of figures and tables should be improved. For example, the quality of Figure 2 is poor, which makes it difficult to capture useful information. Similarly, the proposed method's key results in the tables should be highlighted for better readability.","1. The conclusion regarding the speed-up performance is not convincing. There is no practical speed-up results presented in the work and the reported FLOPs reduction is only calculated theoretically. Moreover, traditional dense matrix multiplication has been highly optimized in recent years on modern GPUs, e.g., cuDNN. Whether or not a network with vector quantization infers faster than its dense counterparts on modern GPUs remains uncertain. Thus, the lack of inferring time and practical speedup results does not adequately justify their work.",482,0
TLBPjECC5D,ICLR_2024,"In terms of novelty, I think it’s important to point out that this paper is a direct application of Discrete Key-Value Bottleneck (Trauble 2022).
1. DKVB was proposed for class-incremental learning, and class unlearning is quite literally the inverse task.
2. The original work also shows improvements on CIFAR10/100, the same benchmarks in this paper.
3. DKVB improves class-incremental learning since each class can be learned by disjoint key-value pairs, thus the model updates for learning new classes can be localized to certain parameters. Thus, why DKVB would help with unlearning might be quite trivial – simply deleting the key-value pairs corresponding with a certain class.
There are also a couple improvements that could be made to improve the quality of the paper.
1. The paper focuses on performance evaluation, but there’s a lack of empirical analysis of their conceptual motivation. For example, how often are key-value pairs shared between classes? Is it close to 0? Can this be visualized?
2. A more diverse range of benchmarks is necessary to test the unlearning capabilities of DKVB. It’s unclear how successful the unlearning method is because the evaluation benchmarks are quite similar to each other (CIFAR10/100, LACUNA100). It might be interesting to evaluate the method on more diverse benchmarks such as benchmarks with outlier classes that are quite similar to each other, or training to distinguish between superclasses and unlearning a specific class in a superclass.","2. A more diverse range of benchmarks is necessary to test the unlearning capabilities of DKVB. It’s unclear how successful the unlearning method is because the evaluation benchmarks are quite similar to each other (CIFAR10/100, LACUNA100). It might be interesting to evaluate the method on more diverse benchmarks such as benchmarks with outlier classes that are quite similar to each other, or training to distinguish between superclasses and unlearning a specific class in a superclass.",483,0
cxt2Auexc3,ICLR_2024,"- Motivation for the task is quite weak. I am not convinced that the task is novel. It is probably style transfer or conditional text generation by prompting LLMs, where personality defines the style of text. While they contrast it with style transfer, the justification on how it is different is not well supported.
- While the paper is easy to follow given the simplicity of the task, it fails to give you a comprehensive overview of the properties of the dataset. Minor:
- In abstract the last statement says ""We anticipate that out work can provide the NLP community with insights! "", but what are the insights? Please consider reformulating this sentence.
- Language use in the paper is in some places a bit odd. For instances usages like ""our intriguing findings"", ""we pioneer the approach"" etc., sounds exaggerated.","- Motivation for the task is quite weak. I am not convinced that the task is novel. It is probably style transfer or conditional text generation by prompting LLMs, where personality defines the style of text. While they contrast it with style transfer, the justification on how it is different is not well supported.",484,0
sFyTZEqmUY,ICLR_2024,"I expect that this paper will comfortably clear the bar for acceptance. However, there are two main issues I believe should first be addressed. I've set my score relatively low because of these, but anticipate increasing it following a revised version.
1) Whilst it's difficult to accuse the paper of overclaiming in any specific place, the writing and framing risk feeling a little showy. The title is very general, and applies to any paper on model-based RL for the real-world rather than something specific to this paper, and naming the method a ""universal simulator"" feels grandiose. (Happy to collect other revewiers' opinions on this.) The connection between POMDP's and action-conditioned video generation is more-or-less assumed by any world model paper (e.g. [1]), and shouldn't be highlighted as a main contribution of the paper.
2) One of the recurring claims throughout the paper is that the major novelty is UniSim's ""orchestration of diverse datasets, each providing a different aspect of the overall experience"" into a single model. Yet no hard evidence is given for this combination being important -- aside from two vague figures in Appendix E. At a minimum, it would be important to train a version of UniSim on say, datasets from the Language Table environment _only_, and report numbers for when synthetic data was generated from this, in Table 2 and 3. This would help support the claim that dataset diversity is valuable.
Other issues (in decreasing priority)
- I think it'd be useful to investigate how entwined the effect of actions is with the dataset distribution. For example, could camera commands (zoom in etc) successfully be applied to kitchen scenes as in Figure 3? The fact that the name of the dataset had to be included as part of the action during training, makes me suspect actions may not be able to generalise well to new kinds of video. This would not be a dealbreaker for the paper's acceptance, but is important to show readers how general this data mixing is.
- A lack of strong baselines might be expected for this kind of scale-up work. But in their absence, ablations become more important, to verify that the various components of the model were all necessary. The paper only presents a brief study of which frames to condition on.
- The model section is poorly written. The use of $\mathcal{T}$ is (I think) slightly misleading -- usually the transition fn of a POMDP is defined as operating on the states, $\mathcal{T}(s_t,a_t) \to s_{t+1}$, and there is a separate emission function producing the observations, $p(o_t|s_t)$. Eq. 1 implicitly combines these -- I might recommend renaming it $f$ or $g$ or whatever. I didn't follow why $o_l$ notation needed to be introduced, since it's immediately unrolled into $[o_t, o_{t+1}]$ and never referred to again. I also didn't understand why the model conditions on the noised, rather than clean, previous observations. It's said the last four frames are concatenated from $o_{t-1}$, which confused me -- does $o_{t-1}$ represent four frames, or should it read $o_{t-1:t-4}$ or similar?
- It's a shame to give the model details only in the Appendix C, as I believe many readers would be interested in them. I hope some of these can be shifted to the main body, particularly key details around the diffusion architecture (such as the core and super-resolution modules) and the amount of compute required.
- Any algorithmic or model novelty is light (more or less straightforward video diffusion).
- The two main experiments were conducted on environments that were within the training distribution of UniSim. It would have been more impressive to investigate the performance on new environments.
- The wordy description of all datasets in 2.1, I felt was much better summarized by Table 5 in the Appendix (perhaps with the addition of a column explaining how an action space is defined and handled), and might be swapped. (Optional!)
Minor issues/questions
- Appendix says 1M steps on 512 TPUs with batchsize 256 -- this seemed a low ratio of training updates to available compute. Did performance saturate beyond this?
- What was the wall clock time of the model training?
- How many parameters were in the model?
- Will the model weights be open-sourced?
[1] Transformers are Sample-Efficient World Models","1) Whilst it's difficult to accuse the paper of overclaiming in any specific place, the writing and framing risk feeling a little showy. The title is very general, and applies to any paper on model-based RL for the real-world rather than something specific to this paper, and naming the method a ""universal simulator"" feels grandiose. (Happy to collect other revewiers' opinions on this.) The connection between POMDP's and action-conditioned video generation is more-or-less assumed by any world model paper (e.g. [1]), and shouldn't be highlighted as a main contribution of the paper.",485,0
WBCPdhQPuz,ICLR_2024,"1. The motivation behind this paper is not clear. What is the advantage of the adaptive method and why do we need these types of methods in the distributed setting? Although few papers study the application of adaptive methods in distributed minimax problems, are adaptive methods necessary in distributed or federated learning compared with non-adaptive algorithms?
2. Some recent related distributed minimax works are missing.
[1] A faster decentralized algorithm for nonconvex minimax problems, NeurIPS 2021
[2] Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning, NeurIPS 2021
[3] FedNest: Federated bilevel, minimax, and compositional optimization. ICML 2022
[4] Decentralized Riemannian Algorithm for Nonconvex Minimax Problems. AAAI 2023
[5] Solving a Class of Non-Convex Minimax Optimization in Federated Learning. NeurIPS 2023.
3. For the convergence results, if this paper focuses on the nonconvex-strongly-concave, results should include the depends on $\kappa$.
4. The baselines in experiments seem to only solve the issues about the design of the adaptive method in distributed learning. It does not present why we need adaptive algorithms in (minimax) distributed problems.","1. The motivation behind this paper is not clear. What is the advantage of the adaptive method and why do we need these types of methods in the distributed setting? Although few papers study the application of adaptive methods in distributed minimax problems, are adaptive methods necessary in distributed or federated learning compared with non-adaptive algorithms?",486,0
5nF9rGNpi3,ICLR_2024,"- FFR, the central contribution of this work, is based on the assumption that a model can differentiate between real and synthetic images and is likely to leverage correlations that are not stable between these two sources of data to make predictions, which could lead to biased predictions in the real dataset. However, there is no discussion or result (theoretical or empirical) in the manuscript to support this rather strong claim. (See the next section for questions related to this point)
- The authors consider a two-step training approach, but only report and discuss in the manuscript the performance of the second step, making it hard to fully understand whether (and to what extent) the model is successful at prediction after being trained with balanced synthetic data only. Moreover, the manuscript does not report/investigate whether models are encoding biases or not after training with balanced synthetic data.
- This contribution heavily relies on the use of synthetic datasets, however, there is no discussion regarding the quality of the generated data. (See the next section for questions related to this point)
- No error bars or confidence intervals were reported in the results and it seems that a single training was performed for each model, making to hard to evaluate how robust the finding from the experimental evaluation are.
- It is not clear from the manuscript whether all approaches trained with synthetic data had access to the same amount of synthetic instances at training time. If not, the observed conclusions from the experiment could have been confounded by the different number samples in each training dataset.
- The authors used an off-the-shelf generative model to try to approximate the real data distribution, but it seems to me that training specialized models for each dataset could yield higher quality data while also helping to mitigate bias due to unbalanced subgroups [1].
[1] Ktena et al., Generative models improve fairness of medical classifiers under distribution shifts, 2023.","- This contribution heavily relies on the use of synthetic datasets, however, there is no discussion regarding the quality of the generated data. (See the next section for questions related to this point) - No error bars or confidence intervals were reported in the results and it seems that a single training was performed for each model, making to hard to evaluate how robust the finding from the experimental evaluation are.",487,0
qTlcbLSm4p,ICLR_2024,"1. This paper focuses on high-resolution image generation. However, the biggest image resolution used in this paper is only 256x256, which is much smaller than the existing definition of ‘high-resolution‘. I would expect an experiment result that has a resolution at least 512 or 1024 to see whether RDM still works.
2. The authors claim that any artifacts in the low-res images can be corrected in the high-res stage, we expect some qualitative cases in experiments to verify such a claim.","1. This paper focuses on high-resolution image generation. However, the biggest image resolution used in this paper is only 256x256, which is much smaller than the existing definition of ‘high-resolution‘. I would expect an experiment result that has a resolution at least 512 or 1024 to see whether RDM still works.",488,0
UqY0SEe5pC,ICLR_2024,"1. The paper considers the regularized score matching loss rather than the standard one.
2. The paper studies the convergence of Langevine dynamics rather than the more widely used backward process in DDPM papers.
3. The Gaussian data distribution in experiments look too simple compared to the ones in practice.",2. The paper studies the convergence of Langevine dynamics rather than the more widely used backward process in DDPM papers.,489,0
YxzEPTH4Ny,ICLR_2024,"* The experimental setup is quite basic, and it's unclear how these findings apply to current Large Language Models. The research primarily focuses on binary addition and multiplication using a simplistic model, which might not be representative of more complex, real-world scenarios.
* The paper could benefit from clearer writing. Specifically, the abstract and introduction lack clarity regarding the nature of the investigation. It's not immediately apparent what the central findings are, the experiments conducted to arrive at these conclusions, and how this differs from existing knowledge in the field.
* There is a noticeable absence of some relevant literature. The authors should consider reviewing ""A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis (Stolfo et al. 2023),"" as well as other works cited in that paper, which seem pertinent to this study.
* Prior research indicates that certain types of positional encoding, such as Sinusoidal Positional Embedding—a fixed form of positional encoding—struggle with extrapolation to unseen positions. The use of this method in the toy models might result in outcomes more attributable to positional embedding issues rather than the arithmetic aspects under investigation.","* There is a noticeable absence of some relevant literature. The authors should consider reviewing ""A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis (Stolfo et al. 2023),"" as well as other works cited in that paper, which seem pertinent to this study.",490,0
6hvtSLkKeZ,ICLR_2024,"* The encoder-decoder pipeline is quite common in machine learning solvers for combinatorial optimization problems. The authors make some specific adaptations for the bin packing problem, while in general, the results are not too surprising under such a framework.
* It will be better to have more insights into the bin packing problem and what machine learning can help to inspire future researchers.","* The encoder-decoder pipeline is quite common in machine learning solvers for combinatorial optimization problems. The authors make some specific adaptations for the bin packing problem, while in general, the results are not too surprising under such a framework.",491,0
AhCdJ93Wmi,ICLR_2024,"1. The article lacks sufficient innovation and is merely a combination and application of existing methods, such as Bootstrap loss, SGC, graph augmentation, and reconstruction.
2. In section 4.3, there is only 'Figure 3' to demonstrate the capability of SSL-GM for inference acceleration. However, it is necessary to provide detailed experimental results regarding accuracy and inference time. These results should be obtained from a wider range of datasets and classification settings.
3. In section 3.3, it would be better to introduce representation shift in detail and explain how reconstruction helps in mitigating representation shift.","1. The article lacks sufficient innovation and is merely a combination and application of existing methods, such as Bootstrap loss, SGC, graph augmentation, and reconstruction.",492,0
5tSLtvkHCh,ICLR_2024,"- The presentation of the paper leaves a lot to be desired.
- W1. I don't fully understand why this work takes such a confrontational stance with respect to the ICA community. From my perspective, and please correct me if I am wrong, everything the paper does is taking the same ICA framework as [1] (non-linear ICA with z independent given another random variable), and assume that the generation process is invertible _given that same random variable_ (in this case, the previous sources). This is quite commendable and interesting, and complements the existing body of work, rather than being obfuscated on ""non-invertibility"" (which is not completely true).
- W2. The manuscript does little effort in providing explanations and justifying certain statements (e.g. the entire paragraph before section 3).
- W3. Similarly, the mathematical notation is far from standard, convoluted, sometimes wrong, and unnecessarily unwelcoming. E.g.:
- In Eq. 3 $T \circ \pi \circ m$ should be in parentheses.
- (I think that) the union symbol $\cup$ is used in places where the Cartesian product is meant to be (e.g. the continuity condition).
- Conditions like those from Eq. 6 are overly convoluted for no reason as, e.g., $v_{k,t}$ being linearly independent could be much simplified by saying that the Hessian has non-zero determinant (i.e. is invertible).
- Jargon is non-consistent, e.g., secondary differentiable, second-order differentiable, and second order differentiable. Similarly, normalizing flows are then called normalized flows.
- W4. I also find section 4 a bit too convoluted to read, and it takes several reads to understand how exactly looks the network proposed by the authors. My advice would be to try to make more explicit the connections between each network component and the theory/data-generating process.
About the experiments:
- W5. I am surprised that there are no comparisons with iVAE, despite being cited.
- W6. Number of parameters as well as training times for the real-world experiments seem necessary to me.
- W7. While real-world results are ok, I find the discussion deceivingly positive, since CMCIR obtains better results on average and beats CaRiNG quite significantly in some individual question types.
[1] Khemakhem, I., Kingma, D., Monti, R., & Hyvarinen, A. (2020, June). Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics (pp. 2207-2217). PMLR.","- W7. While real-world results are ok, I find the discussion deceivingly positive, since CMCIR obtains better results on average and beats CaRiNG quite significantly in some individual question types. [1] Khemakhem, I., Kingma, D., Monti, R., & Hyvarinen, A. (2020, June). Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics (pp. 2207-2217). PMLR.",493,0
1xyar0Ko3E,ICLR_2024,"- It would be great if the authors can provide the training cost comparison between this work and prior work. Since the main target was to reduce the training cost, just listing the ratio of used training samples may not be sufficient to justify the efficiency. I would like to see the real computation cost including core samples selection, KD, SGD training of the paper. I saw Tab.3 compared the training time, but it did not compared with other SOTA QATs.
- It seemed that this work can not achieve a lossless model in comparison to using the entire training samples. In the caption of Tab.1 and Tab.2, using full training set can almost always get a higher accuracy than using a subset, even if the ratio if 80%. If this was the case, we may not rely on coreset selection during QAT. Although the proposed methods always worked better than random selection, it was still important to show that the methods can reach the same accuracy level as using the full training set in QAT.","- It seemed that this work can not achieve a lossless model in comparison to using the entire training samples. In the caption of Tab.1 and Tab.2, using full training set can almost always get a higher accuracy than using a subset, even if the ratio if 80%. If this was the case, we may not rely on coreset selection during QAT. Although the proposed methods always worked better than random selection, it was still important to show that the methods can reach the same accuracy level as using the full training set in QAT.",494,0
rGvDRT4Z60,ICLR_2024,"- Fairness is ""enforced"" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.
- A discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).
- The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).","- Fairness is ""enforced"" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.",495,0
MDXfiEpEEP,ICLR_2024,"1. Writing can be improved. There are simple errors that should have been avoided by proof reading. For example. “when considering the presence of the noise source yellow class, it becomes evident that these noisy samples are closer to their true label class.” Where is “yellow” class in the figure? This causes confusion as “yellow” class is also referred in Equation 2.
2. The applicability of the method should be made more clear and more intuition should be provided.
3. Experiment setup can be improved to better justify the claims.",3. Experiment setup can be improved to better justify the claims.,496,0
5rhgOIu4Tr,ICLR_2024,"- Due to the unclear writing, I cannot understand the contribution of this paper. For example, the following terms were used but not explained:
- adversarial optimization
- loss function manifold
- projected distillation
- bootstrap the importance?
- ""class-wise prior knowledge""
Without further explanation, I cannot correctly assess the proposed method.
- A unique challenge of multi-label learning is the dependency between labels. However, this paper suggested ""_decomposing the multi-label solution space into multiple binary label solution spaces_"", which is questionable.
- Synthetic symmetric noise is not a good proxy for real-world noisy multi-label problems.",- Synthetic symmetric noise is not a good proxy for real-world noisy multi-label problems.,497,0
nRHD9fAj10,ICLR_2025,"1. The novelty in composing the generalization offered is mild at best (the change in the loss), as the method is very similar to CM and CTM.
2. As the key benefits of the proposed approach are the ability to invert the generation process and leverage it, I find several flaws/weaknesses in the proposed experiments and results:
a. The inversion error is not negligible, and without much control over its magnitude. Could something be done to reduce this error using more NFE's?
b. In JPEG restoration, it is not enough to show FID versus NFE as Figure 8 does. Are the obtained images consistent with the given compressd image in PSNR terms? i.e. if we recompress the image using the same Quality factor, will we get close enough to the starting image? This performance measure (CPSNR) should be reported. Also, example image results should be included.
c. On the same JPEG topic - there are other techniques that are far better than PALETTE that should have been used in the comparisons (e.g. https://openreview.net/forum?id=O3WJOt79289, DPS: https://openreview.net/forum?id=OnD9zGAGT0k, PiGDM: https://openreview.net/forum?id=9_gsMA8MRKQ).
d. The inpainting results are far from satisfactory, as evident esspecialy in Figure 9. If 14 NFE are necessary, a comparison to methods beyond CM should be provided, in the spirit of the comment above - DPS, PiGDM and DDRM (https://ddrm-ml.github.io/) should be compared. Indeed, rich work in the past 3 years offers inpaiting techniques via diffusion and these should be mentioned and compared with. See [1-12] below.
e. To summarize, if the key benefit of thsi work is the reverse diretion that CM and CTM do not provide, this work is failing to convience of the benefits in this new capability. See my later comment on editing - perhaps this should have been the main application outlet to this work.
[1] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. In The Twelfth International Conference on Learning Representations, 2024.
[2] Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, and William T. Freeman. Score-based diffusion models as principled priors for inverse imaging. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10520–10531, October 2023.
[3] Gabriel Cardoso, Yazid Janati el idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided denoising diffusion models for bayesian linear inverse problems. In The Twelfth International Conference on Learning Representations, 2024.
[4] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023.
[5] Ciprian Corneanu, Raghudeep Gadde, and Aleix M. Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 4334–4343, January 2024.
[6] Anji Liu, Mathias Niepert, and Guy Van den Broeck. Image inpainting via tractable steering of diffusion models. In The Twelfth International Conference on Learning Representations, 2024.
[7] Andreas Lugmayr, Martin Danelljan, Andr´es Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11451–11461, 2022.
[8] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[9] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. In The Twelfth International Conference on Learning Representations, 2024.
[10] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. The Eleventh International Conference on Learning Representations, 2023.
[11] Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi S. Jaakkola, and Shiyu Chang. Towards coherent image inpainting using denoising diffusion implicit models. CoRR, abs/2304.03322, 2023.
[12] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2024.","1. The novelty in composing the generalization offered is mild at best (the change in the loss), as the method is very similar to CM and CTM.",498,0
v9fQfQ85oG,ICLR_2025,"-	The paper focuses on finding Pareto-stationary solutions, as the authors state in line 161, instead of Pareto-optimal solutions. While I understand the authors’ reasoning and their arguments for focusing on stationarity, it still is a rather severe limitation. Is there any possibility to extend the method to Pareto optimality (other than assuming a convex problem)?
-	Lemma 2 appears to be almost identical to Lemma 3 in (Qu et al., 2020a) and therefore provides no new significant insights. Also, the appendix section title “A.1 The Detailed Proof of Lemma 2” suggests a detailed proof but the proof essentially only refers to the existing result of (Qu et al., 2020a) which makes the section title misleading in my opinion.
-	As the authors themselves state, Lemma 3 is also similar to results in (Qu et al., 2020a). This raises the question whether there are any substantial novelties in Subsection 3.1. The authors should explain more precisely how their results relate to those in (Qu et al., 2020a).
-	The experiment section is somewhat limited because there is just one robot example on two rather small networks with few agents. It would be helpful if the authors could include further examples and elaborate on the scalability of their algorithm with respect to the number of agents and the size of the network.
Minor comments:
-	Maybe remove the mathematical expressions from the abstract, e.g., just write “state-action” and skip “(s, a)” in the abstract
-	Line 74: introduce the neighborhood state-action notation $(s_{\mathcal N}, a_{\mathcal N})$ before using the mathematical expression. I would suggest to just move the mathematical terms like $(s_{\mathcal N}, a_{\mathcal N})$ to Section 2.
-	Page 2, footnote: Personally, I would remove the footnote and include the information either in the main text or defer it to the appendix.
-	Line 98 and following: Are there any restrictions or assumptions on the local state and action space. For example, are they finite or continuous?
-	Line 116: If I understand correctly, $s_0$ refers to the initial state of the whole system and not just the state of one agent. Could you emphasize this by adding something like $s_0 \in \mathbb{S}$?
-	Line 131: Maybe I missed it, but are $S_i$ and $A_i$ assumed to be finite?
-	Figure 1: The font size is very small which makes it hard to read the figure. This is especially unfortunate since the figure seems to visualize many of the key ideas in the paper and provides an important overview.
-	In general, the authors could mention the assumptions underlying each theoretical result, for example: “Lemma 2: Under Assumption 2, the MOMARL problem satisfies …”
-	To my understanding, Theorem 1 is immediately obtained by just plugging equality (18) into inequality (15). I am not sure if this requires a detailed proof in Appendix A.4.
-	There are some typos in the paper, such as “peoposed” (line 247) and “shwn” (line 483)
-	Lines 378-379: Isn’t the sentence “In order to analyze the Pareto-stationary convergence of Algorithm 1.” missing a second sentence part?","- In general, the authors could mention the assumptions underlying each theoretical result, for example: “Lemma 2: Under Assumption 2, the MOMARL problem satisfies …” - To my understanding, Theorem 1 is immediately obtained by just plugging equality (18) into inequality (15). I am not sure if this requires a detailed proof in Appendix A.4.",499,0
kDZKEtDnT1,ICLR_2025,"1. Findings 1 is not well-supported
- Figure 1 is missing leading. Given that different GFMs differ significantly in the number of parameters (e.g., 450k v.s. 500M), I don't understand what the authors aim to convey with this figure. A randomly initialized large model achieving better performance than a pre-trained small model does not suggest the pre-training is not helpful.
- A pairwise comparison of the pre-trained and random versions of the same model, as shown in Figure 2, is much more meaningful. In fact, as shown in Figure 2, the pre-trained model performs better than the random counterparts in most cases, which actually suggests the effectiveness of pre-training. The improvement of pre-training over randomized is more significant on more distinguishable datasets like histone (i.e., different models achieve distinct performances) and less significant on promoter/enhancer. I would attribute this insignificance mainly to the datasets (See below).
2. As shown in Figure 2, all the models ranging from 450k to 500M parameters achieve near-identical performance on many datasets of the NT benchmark, especially on promoter and enhancer. This indicates this benchmark may not be suitable for differentiating different models' capabilities. If the dataset could not differentiate different pre-trained models, it's hard to believe it can effectively differentiate pre-trained/random models. I would suggest more experiments on other benchmarks/datasets that can clearly differentiate the performance of different models. You may consider the datasets used in HyeneDNA (species classification), DNABERT-2 (GUE), and Caduceus (Genomics Benchmark).
Overall, I appreciate the authors' efforts in conducting this amount of experiments. Yet the results are not convincing, and the main conclusion (findings 1) is not well-supported. I will consider increasing my scores if more convincing results are presented.","1. Findings 1 is not well-supported - Figure 1 is missing leading. Given that different GFMs differ significantly in the number of parameters (e.g., 450k v.s. 500M), I don't understand what the authors aim to convey with this figure. A randomly initialized large model achieving better performance than a pre-trained small model does not suggest the pre-training is not helpful.",500,0
zf777Odl6J,ICLR_2025,"- The main weakness of the paper is that it feels like a straightforward application of Kolmogorov–Arnold Networks to GATs, without providing a strong justification for doing so. The paper lacks deeper insights into why this integration is particularly necessary or impactful.
- The rationale for combining multiple GNN layers, such as GCN and GAT, in a single framework is unclear. It appears as if they were stacked together without a clear logical chain, raising concerns about whether sufficient tuning was done to optimize this architecture. It may limit the generalizability of the model and questions the necessity of introducing KAN. It would be interesting to see how the model performs using simpler configurations of GCN or GAT without these combinations.
- Related to the above, the authors themselves acknowledge that the model is complex due to the many components used. While KAN is introduced with the motivation of reducing the computational complexity of MLPs, the resulting model’s complexity seems to contradict this goal, casting doubt on the motivation behind the paper.
- In the limitations section, the suggestion that future work could explore techniques like model compression or pruning feels generic and lacks depth. The authors need to critically rethink the necessity and motivation for introducing KAN into GNNs, as the current reasoning is not well substantiated.
- The experiments are insufficient, as they are only conducted on small, simple datasets like Cora and Citeseer. These datasets may not be representative enough to support the claim that ""traditional GNNs often fall short when dealing with high-dimensional features,” as Cora’s and Citeseer’s feature dimensions are not particularly high, which weakens the argument that KAN is essential for handling high-dimensional data.
- Also, the claim that ""traditional GNNs often fall short when dealing with high-dimensional features"" lacks sufficient evidence. More justification and empirical support are needed for this assertion.
- In Section 4.4, the authors simply re-express the results in table form in the text form, but do not provide enough detailed analysis or interpretation of these results. A more thorough discussion of the findings would strengthen the paper.","- The main weakness of the paper is that it feels like a straightforward application of Kolmogorov–Arnold Networks to GATs, without providing a strong justification for doing so. The paper lacks deeper insights into why this integration is particularly necessary or impactful.",501,0
Rg2JxBZZ0g,ICLR_2025,"1) Lack of Table Related to Model's Layer Details: The absence of a hyperparameter table, including layer sizes and configurations (such as ResMamba block specifications), affects reproducibility
2) Freezing Layer Comparison: Te model’s fine-tuning involved freezing all layers except the final one. However, the effects of freezing additional layers (e.g., last two or three) on performance and stability remain unexplored, limiting insights into how deeper layer freezing might affect knowledge transfer and model robustness.
3) Limited Evaluation Across Datasets: The model’s evaluation is limited to the UK Biobank dataset, so less generalizability , might use AMP MDS-UPDRS dataset.","2) Freezing Layer Comparison: Te model’s fine-tuning involved freezing all layers except the final one. However, the effects of freezing additional layers (e.g., last two or three) on performance and stability remain unexplored, limiting insights into how deeper layer freezing might affect knowledge transfer and model robustness.",502,0
Z30Mdbv5jO,ICLR_2025,"- The lack of precise mathematical formulation raises doubt about proposition 1 and its proof:
- The main inequality in equation 17 (appendix) is justified only verbally and is not obvious to me.
- A counterexample for the inequality in line 841 could be a dataset mainly consisting of dark rooms with all ground truth renderings being black except for one where the lights are on (with possibly very complex geometry or even transparent and reflective materials). For this case, fitting the marginal distribution can be easier than fitting the conditional distribution, if the given scene s happens to be the one with lights on.
- Moreover, if the 3D structure (point cloud) is obtained from 2D images only via stereo reconstruction (DUSt3R [1]), then an image encoder should be theoretically able to encode the images in the same way as the composition of DUSt3R and the point cloud encoder used in the method. Following this argument, a strict inequality as in proposition 1 is not reasonable.
- Some lack of clarity:
- What exactly is the implementation of the learnable embedding function PosEmb in equation 3?
- What is meant with ""uncertainty of unconstrained images"" (line 265)? Does this mean that generated images are not fully 3D-consistent?
- Regarding the confidence-aware 3DGS optimization in section 4.4, it is unclear how the loss in equation 6 is still used / relevant for the final loss in equation 7, which does not use equation 6 anymore.
- Somewhat unfair comparison with baselines:
- The proposed method relies heavily on the strong priors learned by DUSt3R [1] and the video diffusion model.
- All baselines are trained more or less from scratch on small-scale datasets compared to the large-scale datasets for training video diffusion models.
- Therefore, it is not that surprising that the proposed method generalizes much better to other datasets out of the training/finetuning data distribution.
- Restriction to view interpolation:
- Given that the method makes use of a pre-trained video diffusion model, the restriction to interpolation of the camera trajectory between two input views is unsatisfactory.
- In many cases, two views as conditioning already eliminate uncertainty in the reconstruction task entirely such that deterministic approaches like pixelSplat [4] and MVSplat [5] already produce detailed and sharp reconstructions.
- The proposed generative approach with a strong prior trained on large-scale single view data should be able to extrapolate from input views to some extent, which the paper misses to evaluate.
- This is especially critical, if the paper motivates the method with the limitation of generalizable 3D reconstruction methods that ""struggle to generate high-quality images in areas not visible from the input perspectives"" (lines 143f.).
- Missing relevant related work and possible baseline:
- latentSplat [2] aims to bridge the gap between regression models for generalizable NVS and generative models for 3D reconstruction and is therefore a very relevant work and potential baseline.
- GeNVS [3] was one of the first approaches to generative novel views with a diffusion model conditioned on 3D-aware (pixelNeRF) features and is therefore important related work.
Minor comments:
- The paper sometimes uses the terms ""conditioning"" and ""guidance"" interchangeably, although they have different meanings to me (conditioning: giving something as additional input to the diffusion model; guidance: using CFG or also gradients (e.g. classifier guidance) to affect the sampling process). A more precise use of these terms would be helpful.
- The ablation study in table 3 could be extended to contain all different combinations of leaving out individual components in order to find possible dependencies between them. References:
- [1] DUSt3R: Geometric 3D Vision Made Easy. CVPR 2024
- [2] latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction. ECCV 2024
- [3] Generative Novel View Synthesis with 3D-Aware Diffusion Models. ICCV 2023
- [4] pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction. CVPR 2024
- [5] MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images. ECCV 2024",- All baselines are trained more or less from scratch on small-scale datasets compared to the large-scale datasets for training video diffusion models.,503,0
tVNZj27pb3,ICLR_2025,"1. The rationale behind this work stems from the observation that current parameter isolation methods often hinder the acquisition of new task knowledge. Thus, the authors suggest pathway protection based on the sparsity exhibited by activation channels in deep networks. However, some parameter isolation methods are specifically tailored to leverage this sparsity. The clarification on how the proposed method avoids impeding the learning of new task knowledge remains ambiguous.
2. The terms ""pathway"" and ""channel"" lack clarity. A precise definition of these concepts and a comparison with prior works such as Piggyback, Packnet, and MEAT [1] would enhance the understanding. The proposed method appears conceptually similar to these existing works.
3. The language used in the paper is perplexing. For instance, in the abstract: ""Given the sparsity of activation channels in a deep network, we introduce a novel misaligned fusion method within the context of continuous learning."" The meaning of ""misaligned fusion"" and its relevance is unclear.
4. Line #80~81: ""Therefore, pathways protection is all you need."" Such a statement lacks empirical support. It would be beneficial for the authors to provide more rigorous evidence beyond mere examples.
5. Line #239~241: ""Graph matching bears resemblance to a quadratic assignment problem (QAP) (Loiola et al., 2007), with the objective of establishing correspondences between the nodes in an image and the edges connecting these nodes."" The reference to ""nodes in an image"" requires clarification. Additionally, line #246: ""In our framework, a deep network is conceptualized as an image."" The authors offer no elucidation on how a deep network can be likened to an image.
Given the uncertainties raised, evaluating the work poses a challenge. The authors appear to lack sufficient writing and publishing experience. I will proceed with the review and reassess the work following a response from the authors.
### Reference:
[1] Meta-Attention for ViT-Backed Continual Learning, CVPR 2022.","4. Line #80~81: ""Therefore, pathways protection is all you need."" Such a statement lacks empirical support. It would be beneficial for the authors to provide more rigorous evidence beyond mere examples.",504,0
BAglD6NGy0,ICLR_2025,"1. The authors mention that ""Most training-based methods only incorporate the ⟨Question, SQL⟩ pairs for SFT, resulting in degraded performance in other tasks, such as schema linking."" However, our approach usually incorporates a ⟨Question, Schema, SQL⟩ tuple for SFT. Additionally, a reduction in schema linking performance cannot be seen as a limitation of existing methods. If a specific task is not included in training, optimal results for that task are not expected. Therefore, this should not be considered a limitation; instead, one could state that training with schema linking can achieve better outcomes.
2. The authors state that ""Training LLMs on a single SFT task poses a significant risk of overfitting, which may diminish the model's capability to understand instructions."" However, overfitting is not further addressed in the subsequent sections. Could the authors clarify what overfitting entails in the context of SQL tasks, and explain how multi-task training specifically mitigates this risk? Additionally, training on more data and achieving good results may also suggest a potential overfitting scenario.
3. The authors mention that ""This strategy leverages collaboration across several SQL-related tasks to reduce hallucinations during SQL generation."" However, the term ""SQL hallucinations"" is not defined, nor is there any discussion in the experimental section explaining how hallucinations are reduced. This claimed advantage, therefore, remains unclear.
4. If Schema Linking, Noise Correction, and Continuation Writing are considered important, could the authors provide the relative improvement metrics for these tasks?
5. There are inconsistencies in writing style, such as using both ""text-to-SQL"" and ""Text-to-SQL"" interchangeably. Ensuring uniform terminology would improve the clarity and professionalism of the writing.","2. The authors state that ""Training LLMs on a single SFT task poses a significant risk of overfitting, which may diminish the model's capability to understand instructions."" However, overfitting is not further addressed in the subsequent sections. Could the authors clarify what overfitting entails in the context of SQL tasks, and explain how multi-task training specifically mitigates this risk? Additionally, training on more data and achieving good results may also suggest a potential overfitting scenario.",505,0
JkCJBoNUcU,ICLR_2025,"1. What is the technical contribution of the proposed method RealDGen? It seems that the effectiveness of the proposed methods mainly comes from the powerful diffusion model, and a similar idea of separating the degradation and content features has been investigated by previous methods [A].
2. During the training phase of DDPM, the authors finetune partial parameters of the extractor. However, it is unclear the motivation and the effect of finetuning partial parameters. Please provide more discussions.
3. The proposed method is tested on several SR benchmarks, but more experiments on diverse data with various types of degradation (e.g., motion or defocus blur [B, C]) would better showcase the framework’s generalization ability and robustness in a wider range of real-world scenarios.
4. The proposed RealDGen relies on a large real LR dataset, which may be a potential limitation for scenarios where collecting such data is difficult or infeasible. It would be useful to investigate the effect of different amounts of real LR data on data generation.
5. The authors use a contrastive learning approach for the degradation extractor but do not provide enough detail on how different contrastive learning configurations (e.g., different negative sampling strategies) could affect the performance of data generation. It would be better to conduct more additional ablations to investigate the effect of different contrastive learning methods, such as [D].
6. Since the proposed method requires training a DDPM from scratch, the data generation cost is substantially high. Please provide a cost comparison with other existing methods to facilitate a clearer understanding of the computational demands and efficiency of the proposed approach.
[A] Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling, TPAMI 2024.
[B] Benchmarking Neural Network Robustness to Common Corruptions and Perturbations, ICLR 2019.
[C] Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction, NeurIPS 2023.
[D] Unsupervised Degradation Representation Learning for Blind Super-Resolution, CVPR 2021.","1. What is the technical contribution of the proposed method RealDGen? It seems that the effectiveness of the proposed methods mainly comes from the powerful diffusion model, and a similar idea of separating the degradation and content features has been investigated by previous methods [A].",506,0
Nb3a8aUGfj,ICLR_2025,"1. The reviewer raises doubts about the novelty and contribution of the work, seemingly only using the encoding of the language model as a condition to generate solutions of PDEs.
2. Why do the two experiments adopt different baselines and lack baselines such as FNO for cylinder flow?
3. From line 047 to 050, while this statement may seem obvious, it's not a scientific issue. It is necessary to have physics and numerical knowledge to build PDE solvers. At the same time, approaches based on FNO, DeepOnet and similar numerically-driven methods can learn accurate PDE solvers without requiring physics knowledge.
4. From line 053, this work deviates from the challenges described above, and there are concerns about potential overclaiming. Based on the author's two experiments, the proposed model only works in specific PDEs, spatiotemporal dimensions, and simple scenarios where the required physics knowledge is quite basic. In other words, the authors failed to address a crucial question: for complex PDE systems, how to represent the system using language, when mathematical formulas are clearly more suitable, while it is the main challenge the reviewer understand from the previous paragraph.
5. Spatial and temporal resolution is crucial for PDE solvers. How does this work handle different spatial resolutions? Specifically, how does this method distinguish between initial conditions with different resolutions within the same system?
6. Could you provide more details about the autoencoder in Sec. 3.1? This part is quite abstract and difficult for readers to understand intuitively. Please briefly explain the autoencoder's architecture, input/output structure, and training methodology. Additionally, during the diffusion model training, is the encoder trained simultaneously or are its parameters fixed?
7. In line 235, the decoder appears to be CNN-based, which imposes constraints on the spatial dimensions of the output solutions. Furthermore, interpolation operations can lead to inaccuracies in the results, even when solutions on regular grids are accurate.
8. In the method section, the authors have placed too many crucial details in the appendix, like Appendix C.2, B.1, B.2, D, making it impossible to understand the method without referring to the appendix.
9. The selected parameters for cylinder flow are overly simplistic, only covering laminar flow cases. Generally, the Kármán vortex street phenomenon is more common in cylinder flows, yet the authors did not include this scenario.
10. All data used is 2D, and it's unclear whether this model can handle 1D and 3D experiments. The lack of such experiments significantly reduces the persuasiveness of the work.
11. Without knowing the scale of the data, reporting only L1 loss makes evaluation difficult. Could authors additionally report the relative L2 loss?
12. As the authors acknowledge, the inference time of diffusion models limits their application, which is confirmed by the statistical results. The authors should consider using inference acceleration methods such as DDIM and verify that the method remains effective under DDIM sampling.","1. The reviewer raises doubts about the novelty and contribution of the work, seemingly only using the encoding of the language model as a condition to generate solutions of PDEs.",507,0
Sw10tbj0gM,ICLR_2025,"- The major weakness of the paper is the limited novelty. Using diffusion model for time series imputation has been explored in a few existing work. Particularly, MSTI (Alcaraz & Strodthoff, 2023) explored using Mamba as the backbone of diffusion for time-series data. Given the existing work, it seems that the main technical contribution is to replace S4 with another SSM variant, S6. The technical contribution looks incremental to me.
- Most experiments limit the training epochs to 50, which might not be sufficient for models to converge, especially for generative models.
- In the limitations section, it is argued that the paper addresses the missing-at-random (MAR) problem. Please clarify if the paper addresses MAR or missing-completely-at-random (MCAR). How is the probability of missingness in MAR being considered?","- In the limitations section, it is argued that the paper addresses the missing-at-random (MAR) problem. Please clarify if the paper addresses MAR or missing-completely-at-random (MCAR). How is the probability of missingness in MAR being considered?",508,0
2L1OxhQCwS,ICLR_2025,"1. The paper lacks code and detailed implementation information for both the Transformer and LSTM models, which limits reproducibility.
2. The novelty of the proposed approach is limited. While the authors introduce a DLSTM model to improve performance, the idea of decomposition was previously explored in models like DLinear [1], diminishing the originality of the contribution. Beyond the comparative analysis, additional innovation is also limited.
3. The decomposition strategy appears to be applied only to the LSTM model. For a fair comparison, a decomposition approach for the Transformer model should also be included. In Table 3, DLSTM significantly outperforms LSTM, which suggests that a decomposed Transformer might also show improved results.
4. The paper does not include several state-of-the-art (SOTA) Transformer-based models, such as PatchTST [2], Crossformer [3], and iTransformer [4], in the comparison, which limits the comprehensiveness of the analysis.
5. The statement ""Transformer-based models exhibit only a marginal advantage in predicting absolute price sequences, whereas LSTM-based models demonstrate superior and more consistent performance in predicting differential sequences such as price differences and movements"" requires further investigation. A deeper analysis into the underlying causes of this observed difference is missing, which weakens the interpretability of the results.
[1] Zeng, Ailing, et al. ""Are transformers effective for time series forecasting?."" Proceedings of the AAAI conference on artificial intelligence. Vol. 37. No. 9. 2023.
[2] Nie, Yuqi, et al. ""A Time Series is Worth 64 Words: Long-term Forecasting with Transformers."" The Eleventh International Conference on Learning Representations.
[3] Zhang, Yunhao, and Junchi Yan. ""Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting."" The eleventh international conference on learning representations. 2023.
[4] Liu, Yong, et al. ""iTransformer: Inverted Transformers Are Effective for Time Series Forecasting."" The Twelfth International Conference on Learning Representations.","5. The statement ""Transformer-based models exhibit only a marginal advantage in predicting absolute price sequences, whereas LSTM-based models demonstrate superior and more consistent performance in predicting differential sequences such as price differences and movements"" requires further investigation. A deeper analysis into the underlying causes of this observed difference is missing, which weakens the interpretability of the results. [1] Zeng, Ailing, et al. ""Are transformers effective for time series forecasting?."" Proceedings of the AAAI conference on artificial intelligence. Vol.",509,0
FbQLFsBbTe,ICLR_2025,"1. This paper is not designed for general CLIP training but is instead built on the assumptions and techniques from prior work[1]. As a result, its applicability may be limited to scenarios that align with these specific assumptions, restricting its generalizability to broader CLIP training tasks.
2. The paper primarily builds on existing techniques, such as compositional optimization and gradient reduction strategies, without introducing fundamentally new concepts. While it refines and optimizes these methods for CLIP training, the core ideas themselves are not particularly novel.
3. The paper's writing lacks clarity, making it difficult to follow at times. The structure of the paper would benefit from significant revision to improve its readability and logical consistency.
4. The experiment compares FastCLIP with OpenCLIP, which is designed for large-scale clusters and requires large batch sizes. In contrast, FastCLIP is tested on a relatively smaller cluster (up to 32 GPUs), violating the original conditions under which OpenCLIP was evaluated. Additionally, the results in experiments 4 and 5 are inconsistent, with different methods showing unstable performance. This lack of stability weakens the conclusions and limits the insights that can be drawn for future work.
[1] Yuan Z, Wu Y, Qiu Z H, et al. Provable stochastic optimization for global contrastive learning: Small batch does not harm performance. ICML 2022.","3. The paper's writing lacks clarity, making it difficult to follow at times. The structure of the paper would benefit from significant revision to improve its readability and logical consistency.",510,0
qezVbskHmi,ICLR_2025,"- Although assumed a multi-modal setting, the solution is only evaluated on time series data.
- The proposed solution is only evaluated under static modality drop. Whereas in reality, the modality variance is more dynamic.
- Although the authors provided empirical evidence to support entropy-based metric for client selection, they only seem to consider model performance and did not provide much discussion on fairness.","- Although the authors provided empirical evidence to support entropy-based metric for client selection, they only seem to consider model performance and did not provide much discussion on fairness.",511,0
9SvRqu21m7,ICLR_2025,"1. The authors state in Line 257 that 'Conditions within each partition should be more semantically similar than those in other partitions, so networks require less capacity to achieve a set quality on their partition.' However, there are no experiments presented to support this claim. I believe that implementing this idea is challenging and will demand additional computational resources. I recommend including relevant experiments and source code to facilitate a comprehensive review.
2. The statement in Line 15 that 'the student model’s inference speed is limited by the size of the teacher architecture' is misleading, as the inference speed of the student model is independent of the teacher model; the student only depends on the teacher during the distillation training phase. I recommend proofreading the entire paper to ensure clarity and professionalism.
3. The proposed method introduces multiple student models; therefore, comparisons and analyses of the model parameters should be a focal point of the paper.
4. The proposed method leverages adversarial distillation with the expectation of enhancing the distillation effect. However, there is no comparison with standard distillation methods or other variants to validate the adversarial distillation’s anticipated advantages.
5. In the ablation studies section, only a quantitative analysis of the generation effect is presented. I believe that a qualitative analysis should also be included, as the paper aims to enhance generation quality.
6. I can not conduct a comprehensive review of the technological accuracy of this paper, as it is empirical rather than theoretical, and the implementation code is not provided.","1. The authors state in Line 257 that 'Conditions within each partition should be more semantically similar than those in other partitions, so networks require less capacity to achieve a set quality on their partition.' However, there are no experiments presented to support this claim. I believe that implementing this idea is challenging and will demand additional computational resources. I recommend including relevant experiments and source code to facilitate a comprehensive review.",512,0
gQPwP1JFwC,ICLR_2025,"1. The paper is generally hard to follow, making it difficult to grasp the contribution and novelty of the work.
2. There is a significant amount of irrelevant information of the algorithm and writing typos. For example, several areas in the paper lack clarity:
- In the abstract, there is *[...]* which is weird.
- The start of the introduction is nearly a rephrasing of the abstract.
- The introduction primarily repeats points from the general conclusion (e.g., RMSDrop performs well for deep models), diminishing the value of the information.
- The introduction reads like a collection of related work, making it difficult to follow and affecting the logical flow.
- Line $112$ contains a vague statement without added insight.
- The figure provides limited information and is challenging to interpret.
3. There are three algorithms, all of which rely solely on descriptive text without any mathematical representation.
4. The algorithm is validated only on MNIST and Fashion MNIST data, which are relatively simple, toy examples for deep learning tasks.","4. The algorithm is validated only on MNIST and Fashion MNIST data, which are relatively simple, toy examples for deep learning tasks.",513,0
iBExhaU3Lc,ICLR_2025,"- This study mostly benchmarks on transformer models, leaving its effectivness on other architectures like CNNs and RNNs a bit underlooked; more benchmarking could either show it’s limitations or confirm its ability to adapt across other models types.
- The influence of the Hessian-based learning rate grouping on different gradient structures wasn’t deeply investigated, and comparing it with fully adaptive methods would make it’s efficiency clearer.
- Optimizer stability over long training durations hasn’t really been tested, so longer training runs would give more confidence in its reliability, especially for deep networks or tasks that need extended training.
- Adam-mini’s sensitivity to hyperparameters hasn’t been very explored, which might make tuning it more difficult, and more understanding here could show if it raises computational cost.
- Even though they briefly mention it, there’s no strong comparison with newer optimizers like Sophia and Lion; a head-to-head test would help show Adam-mini's specific advantages or where it could improve.
- Performance of the optimizer on tasks like fine-tuning and transfer learning is not talked about, and testing it on practical applications could confirm its strengths beyond just primary training.
- The grouping of parameters lacks deep mathematical support in the study, and adding a stronger theoretical basis would back up the partitioning method chosen and possibly lead to more improvements.","- Adam-mini’s sensitivity to hyperparameters hasn’t been very explored, which might make tuning it more difficult, and more understanding here could show if it raises computational cost.",514,0
r0QqfaCkF8,ICLR_2025,"I will combine the *Weaknesses* section and the *Questions* section. My concerns are as follows:
- The reported results of TorDiff in Table 1 are significantly worse than those in the original paper [1]. Compared to the results in the original paper, FADiff is worse than TorDiff. Can you provide an explanation for this?
- There is no analysis on runtime. I strongly suggest to compare time efficiency of the proposed methods with baselines (e.g., Table 2 in [1]).
- TorDiff provided the errors of E, ∆ε, Emin, and μ for the generated ensembles. It would be great to provide the same comparison in this paper, as it would allow for a more robust comparison.
- As the authors acknowledged, the fragmentation method should be carefully chosen under the proposed framework to ensure the preservation of key chemical properties.
- On line 96, the term molecular generation was used, which is different from molecular conformation generation. --- **References:**
[1] Jing et al., Torsional diffusion for molecular conformer generation, NeurIPS, 2022.","- The reported results of TorDiff in Table 1 are significantly worse than those in the original paper [1]. Compared to the results in the original paper, FADiff is worse than TorDiff. Can you provide an explanation for this?",515,0
UnpxRLMMAu,ICLR_2025,"- The paper is written in a very convoluted way for a relatively simple method.
- Especially the experiments are described in a way that I find extremely hard to follow even after repeatedly reading the section. Hence i have some questions regarding this paper.
1) What are you expecting to see in Figure 2? ""The naive estimator overstates the length bias"" How do you know the ground truth length bias and how can you claim yours does better? This part is very unclear to me and seems to be the crux of the misunderstanding. If you could clarify I would be more than happy to raise my score.
2) Figures 4 and 5 seem to show the effects of rewriting rewrites. In Figure 4, you see minor changes. In Figure 5 the trend does not seem to be consistent across models. Hence my question is, what are you expecting to see in this figure? what is the ground truth? and how would you pick in practice whether to rewrite the rewrite.
3) Theorem 1 in the paper seems like a standard causal inference setting. and you assume noise assumptions that are just not controllable in the LLM setting. Hence my question to you is what the point of that theorem is? Can you please justify the additive reward structure and what the motivation for that is?","2) Figures 4 and 5 seem to show the effects of rewriting rewrites. In Figure 4, you see minor changes. In Figure 5 the trend does not seem to be consistent across models. Hence my question is, what are you expecting to see in this figure? what is the ground truth? and how would you pick in practice whether to rewrite the rewrite.",516,0
nsozLtutE6,ICLR_2025,"1.The motivation behind the paper is not clearly articulated. The author mentions: “While single-scale frequency domain decomposition offers a global perspective of time series data in the frequency domain, it lacks the ability to localize specific frequency components within the sequence.” This statement requires further explanation—why is it unable to localize specific frequencies? Similarly, the author needs to further elaborate on why MMFNet can handle non-stationary series (along with detailed examples). These are key points and contributions of the paper.
2.Using only the MSE metric is insufficient. Does the model show similar performance when evaluated using other metrics such as MAE?
3.The visualization work in the paper is not particularly sufficient. The authors should include more comparative figures between Single-Scale Frequency Transformation and Multi-Scale Frequency Transformation, as well as visualizations of the mask effects, and prediction showcases of different models.
4.The authors should compare the models across more dimensions, such as parameter count, training and inference time.
5.The baseline models are not very rich; the current research primarily focuses on analysis at the temporal scale. Recently, there has been some work exploring the variable perspective, yielding good results, such as with the iTransformer and Client models. It is recommended to include these comparisons as well.
6.The authors claim that ""Transformer-based architectures have demonstrated exceptional capacity in capturing complex temporal patterns by effectively modeling long-range dependencies through self-attention mechanisms at the cost of heavy computational workload, particularly when facing large-scale time series data, which significantly limits their practicality in real-time applications."" This claim is not comprehensive; a more significant limitation of Transformer-based models is their tendency to overfit.",2.Using only the MSE metric is insufficient. Does the model show similar performance when evaluated using other metrics such as MAE?,517,0
iOltCu4TPS,ICLR_2025,"1. The presentation of results is convoluted, with tables that are difficult to interpret, reducing the impact of the findings.
2. The paper does not adequately discuss the potential for foundation models to have encountered test data during training, which is crucial for interpreting their performance accurately.","2. The paper does not adequately discuss the potential for foundation models to have encountered test data during training, which is crucial for interpreting their performance accurately.",518,0
sClhxLqfnP,ICLR_2025,"1. Lack of Real Data Testing: The experiments were conducted mostly on synthetic data, the paper only presents two qualitative results on real data in section 4.3. The authors could conduct more experiments on real data, especially providing some quantitative results.
2. Insufficient Analysis on Viewpoint Consistency: The paper lacks an analysis of material consistency between generated multi-view images and the input image. Without ensuring consistency in material properties across views, the relightable 3D-GS model may have limited relevance, potentially restricting its applicability.
3. Moderate Novelty: The design of the relightable 3D Gaussian splatting model and the generative model shows limited innovation compared to existing methods. While performance improvements are achieved, the methodological design lacks significant originality.","3. Moderate Novelty: The design of the relightable 3D Gaussian splatting model and the generative model shows limited innovation compared to existing methods. While performance improvements are achieved, the methodological design lacks significant originality.",519,0
lBrLDC7qXF,ICLR_2025,"1. The article only uses two datasets in its experiments and lacks large-scale datasets. The authors should consider supplementing the datasets.
2. The creation of Figure 1 is evidently too rough, including misaligned text and meaningless graphics in the small image on the left.
3. The model section of this paper is very brief, with the model being simply a BERT that takes in information from neighboring nodes. While this approach may be effective, I question whether the paper's innovation and interpretability are sufficient for acceptance at ICLR.
4 The paper repeatedly emphasizes the advantages of the proposed model on effiency; therefore, it would be beneficial to include comparative experiments on time complexity or runtime performance between the proposed model and the baseline.","2. The creation of Figure 1 is evidently too rough, including misaligned text and meaningless graphics in the small image on the left.",520,0
AAXBfJNHDt,ICLR_2025,"1. Though combining DDPM and graph spectral decomposition is new to me, there are already works that use DDPM to generate graphs (e.g., DiGress) and works that use SDE diffusion on graph spectrum (e.g., GSDM). In my opinion, this paper is experimenting with a different combination of diffusion approach and signal domain, which can produce useful results but lacks significant novelty.
2. The model uses the graph eigendecomposition and performs diffusion on the eigenvectors and eigenvalues. However, it is well-known that eigendecomposition (of Laplacian) is highly unstable. I think the authors should theoretically address this issue.
3. Using a part of the spectrum has the advantage of reducing complexity. However, information is lost. What is the balance between these two factors? Is it true that most of the high-frequency components are not important?
4. Is it possible for the diffusion process to generate eigenvectors and eigenvalues that cannot be obtained from the eigendecomposition of any graph?
5. For some datasets (e.g., QM9), the proposed method does not seem to show a clear advantage with a few performance metrics, even though the entire graph spectrum is used. One may gain more insights if the authors can also show the results when a partial graph spectrum is used.
6. How to choose $k$? Is there a principled approach?","1. Though combining DDPM and graph spectral decomposition is new to me, there are already works that use DDPM to generate graphs (e.g., DiGress) and works that use SDE diffusion on graph spectrum (e.g., GSDM). In my opinion, this paper is experimenting with a different combination of diffusion approach and signal domain, which can produce useful results but lacks significant novelty.",521,0
lGWaAIC9gU,ICLR_2025,"- The paper introduces a new task, prompt adaptation which aims to adapt existing prompts for one model to another model or to a different language. While this task is fairly reasonable, it is not clear to me from reading the paper whether LCP accomplishes the goal. It seems like the performance could vary widely between ``Last`` and ``Best`` performance and also between different transfer settings (Table 2). In Table 3, it seems like query translation is a better approach for cross-language applications. I think the paper could spell out the take-away from these experiments better and provide a heuristic guideline for when one should use LCP.
> our LCP adaptation framework creates a balance between strengths of source and target models.
- I am not sure how I should interpret this sentence or how it relates the empirical results. I think more explanation would be helpful.
- There is also no failure analysis about when LCP might fail or is not suitable.
- The paper could also benefit from having a pseudocode block that illustrates the process clearly.",- The paper could also benefit from having a pseudocode block that illustrates the process clearly.,522,0
AZR4R3lw7y,ICLR_2025,"A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.
1. Although RP and multi-view strategies are theoretically shown to improve feature separability, there is limited discussion on the theoretical upper bound of generalization error.
2. The manuscript does not provide theoretical guidance on choosing the optimal projection dimension in RP, which can greatly impact model performance for different tasks.
3. The proposed self-improvement process for prompt selection could increase the model’s inference complexity, but this potential trade-off is not fully analyzed, especially in scenarios with many tasks.
4. Minor spelling errors, like “sammple” in line 250.
5. More experiment regarding hyper-parameters like number of views, projection dimensions, and threshold settings for the voting mechanism should be conducted.","3. The proposed self-improvement process for prompt selection could increase the model’s inference complexity, but this potential trade-off is not fully analyzed, especially in scenarios with many tasks.",523,0
3Hy00Wvabi,ICLR_2025,"1. Important technical details are missing. The paper mentions using ChatGPT for data annotation and optimization multiple times but doesn't specify the model version. For work with data construction as its core contribution, this seriously affects reproducibility and rigor.
2. Lacks methodological innovation and mechanism analysis. Although data construction is an important contribution, the paper lacks in-depth analysis of how this data enhances model capabilities. Specifically, it doesn't investigate whether the improvements come from enhanced logical reasoning abilities or simply better format matching. Without such analysis, it's difficult to determine if the model has truly learned to reason about complex workflows or has merely memorized patterns from the training data.
3. Missing critical ablation experiments. The paper doesn't compare the performance difference between the Annotator Model and WorkflowLLM, despite both models using the same training method and data type. This results in question the necessity of the data synthesis strategy and weakens the core argument.
4. The paper mentions that the Annotator Model can generate workflows with over 70 actions, and theoretically WorkflowLLM should be capable of the same. Given the general limitations of LLMs in long sequence generation, including such long-sequence workflows in the appendix would further demonstrate the work's contribution.
5. Given that the core contribution is data construction, whether the dataset will be open-sourced directly affects the practical value of the work. The authors should consider open-sourcing the dataset to promote development in this field.","5. Given that the core contribution is data construction, whether the dataset will be open-sourced directly affects the practical value of the work. The authors should consider open-sourcing the dataset to promote development in this field.",524,0
Sk2mND99Wp,ICLR_2025,"1. The window shifting mechanism is based on the continuity assumption of scene disparity or depth, and then sets a threshold to crop the cost volume. This is more like a trick, rather than an innovative module. And this article does not explain the potential negative impacts that this may bring.
2. Lack of novelty in SGM based cost aggregation. The method of cost aggregation has been widely and persistently applied in geometric estimation tasks. However, since this paper has not made many improvements to it, we believe that this cannot be considered as the contribution point of this paper.
3. This article lacks an explanation for changing the original RNN uploader to PatchMatch.
4. The explanation of some legends is not clear enough. For example, in Fig 6, it is not explained what the different colored bands represent. Although it is mentioned in the main text, it caused difficulties in reading.
5. The performance on the actual benchmark differs from the performance described in the paper. Line 454 ""Our GCAP Stereo ranks 2 nd place on the bad 1.0 metric and 1 st place on EPE metric respect"", but when we checked the actual benchmark, we found that the method is not the first. And there is a lack of comparison with the latest methods, such as CVPR2022 CREStereo and others, CVPR2024 LoS: Local Structure guided Stereo Matching, Confidence Aware Stereo Matching for Realistic Cluttered Scenarios. ICIP 2024 also includes a large number of methods that have not been publicly published as papers. Overall, we believe that ranking first is an exaggeration.","1. The window shifting mechanism is based on the continuity assumption of scene disparity or depth, and then sets a threshold to crop the cost volume. This is more like a trick, rather than an innovative module. And this article does not explain the potential negative impacts that this may bring.",525,0
NDLmZZWATc,ICLR_2025,"1.Figure 1 highlights the variability in GPT-based descriptions, but this diversity can actually be beneficial in image classification. The goal of descriptions is to enrich the semantic variety within each class, helping the model handle diverse images within the same category, rather than strictly aligning with the class name. By dismissing this variability, the proposed approach might miss out on the robustness provided by diverse descriptions.
2.Mapping prompt descriptions to the GPT embedding space may reduce interpretability, as this space is abstract and lacks human-interpretable structure. The optimizations in this space might rely more on implicit semantic relationships rather than explicit, understandable features, which could obscure the interpretability of the learned representations.
3.The paper does not reference or explain Algorithm 1: Training process of DeMul, which leaves readers without guidance on understanding the step-by-step training procedure of the proposed method.
4.The description of Equation (4) suggests averaging classification probabilities across prompts, but the actual implementation seems to involve averaging the text embeddings of prompts instead.
5.The Preliminaries and Background section lacks appropriate citations, which limits the clarity.
6.The paper specifies fixed values for the regularization weight and the loss balance parameter, but does not provide ablation studies to justify these choices.
7.The Related Work section lacks a clear articulation of the limitations in existing methods and how the proposed approach addresses these gaps.
8.The paper claims that the weights $\{w_{ij}\}$ for each class are normalized, ensuring that $\(\sum_{j=1}^{M} w_{ij} = 1\)$. However, in Figure 5, the sum of the weights for the five prompts appears to exceed 1, which contradicts this claim.
9.In the ablation study presented in Table 1, the difference in accuracy between ""Ours w/o weighted"" (85.2) and ""Ours"" (85.3) in the 16-shot setting is minimal, with only a 0.1% improvement when using the proposed weighting mechanism. This marginal difference raises questions about the practical impact of the weighted approach.","3.The paper does not reference or explain Algorithm 1: Training process of DeMul, which leaves readers without guidance on understanding the step-by-step training procedure of the proposed method.",526,0
rapXZIfwbX,ICLR_2025,"- **Writing**: The empirical observations in the paper feel overly repetitive, while the theoretical sections come across as overly technical and challenging to follow. For example, Section 3 spends three pages describing that the diagonal elements and bias become uniform while off-diagonal elements become noisy. Section 4 includes excessive technical detail in the derivations, which could be condensed into formal theorems, with the technical details moved to the appendix for easier readability.
- **Theory**: Many theoretical discussions in this paper lack rigor. For instance, the discussion between lines 353 and 361 is confusing and does not seem to follow rigorous mathematical reasoning. I will provide further points of confusion in the questions section. Notably, as a theoretically-focused paper, it lacks any formal theorems, making it difficult to digest. Summarizing results into well-defined theorems with clearly explained notations would improve clarity.
- **Experiments**: The experimental scope appears limited. The paper only presents results on a toy model with input data following a sparse, uniform, permutation-symmetric distribution. However, real-world data, such as natural language, does not exhibit permutation symmetry. Including experiments on real-world data would help demonstrate the applicability of these results.
In summary, the experimental scope of this paper seems narrow, and the theoretical sections lack the rigor expected of a theoretical work. I recommend that the authors refine their presentation of theoretical results and expand their experiments. I’m open to any clarifications if there has been a misunderstanding of the paper's theory from my end.","- **Writing**: The empirical observations in the paper feel overly repetitive, while the theoretical sections come across as overly technical and challenging to follow. For example, Section 3 spends three pages describing that the diagonal elements and bias become uniform while off-diagonal elements become noisy. Section 4 includes excessive technical detail in the derivations, which could be condensed into formal theorems, with the technical details moved to the appendix for easier readability.",527,0
bgcdO9lmug,ICLR_2025,"1. The method can be understood as an combination of ReAct and Automatic Prompt Optimization. However, it is not clear why the Act part that provides feedbacks during generation of the plan can improve the prompt optimization. Further ablation study is necessary, for example, optimizing prompts without Act and inferencing with/without Act in figure 1.
2. The paper does not have any formulation or an algorithm box. Some descriptions are very vague and the references to some components are not consistent. For example, in figure 1, the work flow involves a LLM summarizer, but line 186 indicates using a summarization team. Also, line 182 mentions ""focus point"". This is very illy-defined. If it is emerged from a prompt, the author should explicitly show the prompt that instruct LLMs to label ""focus point"". More vague descriptions like ""some interaction schemes with some kind of feedback provider"" (line 179). Such process should be more precisely described with formulation and algorithm box. Right now, I don't believe the Methods Section is describing a method that is replicable and able to be applied to other domains by the community.
3. The difference with existing methods are not clear. For example, the reference to ProTeGi says ""Compared to previous work in automatic prompt engineering, including APE (Zhou et al., 2023) and PROTEGI (Pryzant et al., 2023), we do a summarization over a batch, and we prevent the prompt optimization from overfitting on a single outlier data point."" This reference is incorrect as ProTeGi also optimizes the prompt over a minibatch.","1. The method can be understood as an combination of ReAct and Automatic Prompt Optimization. However, it is not clear why the Act part that provides feedbacks during generation of the plan can improve the prompt optimization. Further ablation study is necessary, for example, optimizing prompts without Act and inferencing with/without Act in figure 1.",528,0
I5S1a1NKxo,ICLR_2025,"* The use of text-to-image generation to create synthetic data can introduce computational overhead, potentially making the framework slower or more resource-intensive.
* The paper primarily experiments with EfficientNet B0 as the small model architecture. How does SIDCLIP perform with different or larger backbones? Testing with a broader range of architectures would help demonstrating the generalizability of SIDCLIP.
* While SIDCLIP focuses on task-specific performance, its robustness in handling out-of-domain data (datasets significantly different from those in CLIP’s pretraining) is questionable. Since most of CLIP’s parameters remain frozen, there may be limitations in SIDCLIP’s adaptability to new domains, which could restrict its effectiveness in real-world, diverse data settings.
* The combination of synthetic data generation, initialization, and distillation steps may limit SIDCLIP’s practicality in scenarios requiring minimal preprocessing. The entire pipeline involves multiple stages that could increase setup and training time. It would be beneficial to provide the computation requirements compared to existing methods.","* The use of text-to-image generation to create synthetic data can introduce computational overhead, potentially making the framework slower or more resource-intensive.",529,0
CEE9cAQJ10,ICLR_2025,"1. The motivation is not clear enough, authors point out the limitations of existing method, including limited scalability, high cost, and similarity to seed data. However, the similarity to seed data remains questionable and lack a quantitative investigation. Moreover, why the graph-based synthetic method can solve these limitations is not very clear in the introduction section.
2. In the KP extraction process, did authors check the quality of the generated KPs and the impact of each filter and clustering operation to the quality of KPs.
3. Would be nice to conduct study to present the diversity of the generated dataset is better than other synthetic methods.
4. The baseline results based on Qwen and Llama-3 models are missing, would be nice to present these results.
5. Would be nice to conduct case study to show how the KP graph works and show the superiority compared to existing methods.","2. In the KP extraction process, did authors check the quality of the generated KPs and the impact of each filter and clustering operation to the quality of KPs.",530,0
zfgYC3sDt6,ICLR_2025,"1. Previous works [1,2] have examined the calibration performance of pre-trained CLIP after fine-tuning. However, your paper lacks experimental results comparing your method with these studies. We recommend that you include such comparisons in your work.
2. Given that your method is based on experimental observations from CoOp and KgCoOp, we have concerns about its generalizability. For example, in Table 1, your method underperforms compared to Vanilla TCP in half of the settings.
3. This article primarily selects outliers from WordNet. We are curious whether using different lexical databases significantly affects the results.
If you can include relevant experiments and address my questions, I will consider increasing the score.
[1] Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models.
[2] Towards Calibrated Robust Fine-Tuning of Vision-Language Models.","2. Given that your method is based on experimental observations from CoOp and KgCoOp, we have concerns about its generalizability. For example, in Table 1, your method underperforms compared to Vanilla TCP in half of the settings.",531,0
NqkSUwMc0K,ICLR_2025,"1. The results are limited to convex and smooth objective functions. Given that many machine learning problems are essentially nonconvex and nonsmooth, it would be important to relax this assumption.
2. The applicability of the approach also seems limited. The paper only discusses SVM and Softmax regression applications. It will be interesting to know if the approach also applies to other applications, particularly deep learning applications.
3. The design of differential privacy seems straightforward. It is unclear if there are any challenges.","2. The applicability of the approach also seems limited. The paper only discusses SVM and Softmax regression applications. It will be interesting to know if the approach also applies to other applications, particularly deep learning applications.",532,0
ZPTHI3X9y8,ICLR_2025,"- About the Cascade Mask-RCNN head:
- Do you use a separate Cascade Mask R-CNN detector or just the detector head which is then connected with the vision encoder of the LVLM?
- If the former, you cannot get the conclusion in Sec. 2 since the capabilities of the detector and the vision encoder of LVLM might differ from each other.
- If the latter, how do you do that? Is the detection head exactly pre-trained with the same vision encoder of the LVLM, or do you need further training?
- About PATCH:
- On the one hand, detection information might be redundant.
- In lines 226-227, the authors suggest that,
> In scenarios where no extra detection information is required, the LVLM can revert to processing the input using its standard capabilities without PATCH involvement.
- I wonder how to do that, since the detection module of PATCH is query-irrelevant, suggesting that the detector will detect all objects in the image regardless of the question.
- On the other hand, detection information is limited by the granularity of the detector.
- PATCH aims at directly providing the object information detected by the detector to LLMs, which, however, cannot provide information that cannot be detected by detectors. For example, attributes (colors and shapes) cannot be detected, which are more common phenomenon of LVLM hallucinations nowadays.
- Moreover, objects beyond the recognizable classes of the detector are also not recognizable.
- Therefore, beyond experiment numbers, an analysis of why PATCH eases LVLM hallucination is also feasible. For example, will PATCH help if the queried object is *without* or *beyond* the class set of the pre-trained detector?
- About experiments:
- Baselines: HA-DPO and HACL are both methods requiring training. Do you train them with the same training set with PATCH?
- We care about hallucination, but we do not want to hurt the utility of LVLMs. Therefore, besides experiments on hallucination benchmarks like POPE and PhD, results on utility benchmarks (e.g., MME, MMBench, and SEED) are also important to demonstrate that hallucination mitigation is not at the cost of utility.
- Overall, I think this is an interesting paper, which, however, needs further work on 1) method clarification, 2) effectiveness analysis (i.e., what it can and cannot do), and 3) utility experiments.","1) method clarification,2) effectiveness analysis (i.e., what it can and cannot do), and",533,1
cLtE4qoPlD,ICLR_2025,"1. Compared with originla LRR, the change is relatively trivial with not enough research novelty.
2. Used datasets and model architectures are not large-scale like mentioned goal in the abstract. Adding more larger datasets and models will be more supportive.
3. The training configurations for cifar and imagenet may not reach the fully convergence for model training, where 100 and 120 epochs are used for these two, respectively.
4. Overall, the paper is not very clear to read, such as the figures on the main results section. Which line represents the proposed method and which one is for baselines? how to indicate the proposed method outperforms others?
5. In addition, in table.1, more sparsity ratios for experiments will be helpful. And using more other and larger networks will further support the paper claim. More comprehensive empirical studies are needed to support the draft conclusion.",2. Used datasets and model architectures are not large-scale like mentioned goal in the abstract. Adding more larger datasets and models will be more supportive.,534,0
SmxM4POTBk,ICLR_2025,"1. A very big issue is that the baselines (0-shot, 3-shot) are unfairly chosen because they are too weak, and as a result I am not at all convinced that the method proposed by the authors would outperform stronger baselines. Since the main contribution of the paper is improving MT quality, this is something that definitely should be improved. At the absolute minimum, additional baseline results for n-shot should be included, where n is chosen such that the computational costs are similar to the method proposed by the paper. Ideally, some other popular methods that improve LLM-based MT without fine-tuning (from related work Section 2.1) will also be included.
Let's consider example 3 from the appendix, which has an interpolation path of 7 sentences. If I understand correctly, this means that we start from the first sentence and do a 0-shot translation, and then recursively do (n-1)-shot translations until we arrive at the final sentence in the interpolation path. The total costs for X are thus:
- s1 -> 0-shot translation +
- s2 -> 1-shot translation with input (s1) +
- s3 -> 2-shot translation with input (s1,s2) +
- s4 -> 3-shot translation with input (s1,s2,s3) +
- s5 -> 4-shot translation with input (s1,s2,s3,s4) +
- s6 -> 5-shot translation with input (s1,s2,s3,s5) +
- s7 -> 6-shot translation with input (s1,s2,s3,s5,s6)
(Note that this ignores the costs for step 0: start sentence pool creation, step 1: start sentence selection, step 3: MT results aggregation. It also assumes a single start sentence, otherwise costs will be multiplied by number of start sentences (assuming same interpolation sentences).)
2. The method is very computationally expensive compared to the baselines. This is true even for much stronger baselines (e.g., n-shot with relatively large n) that are not included in the paper. The authors mention ""IntGrad MT introduces some computational overhead"", but this seems a severe understatement. The computational overhead of the method is not properly analyzed, but this analysis should have been included to help readers understand these trade-offs.
3. As acknowledged by the authors, their method only works when English is the source language. This is not necessarily a prohibitive issue, but when we have few-shot (with larger n) available as alternative, it is unclear why one would use this more complicated method over that (current results fail to convince me of the improvements over stronger baselines, see first point).
4. I would not call LLMs translation capability ""inherent"", as you do in the abstract. See e.g. https://aclanthology.org/2023.acl-long.524/, which shows that translation capabilities are mostly due to (incidental) bilingualism, i.e., parallel data.
5. While the authors briefly discuss related work, the discussion lacks depth. It is not clear how the authors method is different from this discussion. (It can be inferred by a reader who is knowledgeable about the related works, but that is insufficient.)
6. A lot of clarity and typo issues, I list some below:
- Figure 1 is a bit unclear, nowhere is mentioned what blue/red colours mean. I think I can infer it but it is not clear enough.
- line 89: ""To what sentences IntGradMT can be effective?"" awkward sentence, rephrase to ""For which sentences is IntGradMT effective?""
- line 135: ""which is a prompting technique asks model"" -> ""that asks the model""?
- line 214: ""list of translation"" -> ""list of translations""
- lots of issues with missing space before opening parenthesis, e.g. line 235: ""Nemo(Mistral-Nemo""; line 281: ""selection(step 1)""
- line 372: ""theee"" -> ""three""","4. I would not call LLMs translation capability ""inherent"", as you do in the abstract. See e.g. https://aclanthology.org/2023.acl-long.524/, which shows that translation capabilities are mostly due to (incidental) bilingualism, i.e., parallel data.",535,0
cazOlqncU6,ICLR_2025,"* The main weakness is that overall, while results for adaptive attack and case studies look promising, the experiments are only using Cifar-10 – which has only 10 classes of relatively low-resolution images (32x32 pixels), and each analysis is only performed on one NN architecture. This is not comprehensive enough to establish that the performance of Data Probe generalizes to diverse datasets and model architectures.
* a missed related work: “DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks” by Park et. al. (ACSAC ’23). This work identifies dataset use in model training with no invasiveness or harm in the approach. It may also be possible to compare overall accuracy of Data Probe with their model.
* It is not stated what scores are shown in gray in Table 4.
* In the adaptive attack experiment, the difference between the data D Vs D* is not clear.
* In the case study – duplicating data to create extra data is not very compelling. It would be more interesting to see something like the SVHN dataset augmented with some MNIST data. Typos/etc:
- in the abstract: “data-drobe-based"".
- Please check the definition of Conf(M,x) on page 7. What is the maximum taken over?
- Table 4 caption: “socres” should be “scores”",* a missed related work: “DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks” by Park et. al. (ACSAC ’23). This work identifies dataset use in model training with no invasiveness or harm in the approach. It may also be possible to compare overall accuracy of Data Probe with their model.,536,0
MBBRHDuiwM,ICLR_2025,"A lot of work has gone in to the paper, but unfortunately it is quite difficult to follow; and the proposed method seems very expensive (computing entropy, eigenvalues, clustering, etc). On the first point, to reach a broad audience, specific effort should be made regarding aspects less familiar to the general ML community. I find the method and data transformations difficult to parse.
* Clarity
* [036-041] Unclear references to topology and stationarity - core aspects of the paper
* Is the topology property that dimensions are ordered such that correlation (or similar) is expected between the signal at ""nearby"" dimensions?
* Stationarity seems very entwined with topology, does stationarity require topology? If not how?
* What assumptions *is* the model making?
* [225] unclear how permuting pixels leaves stationarity intact
* [259] unclear how reducing density prevents redundant sampling.
* [095] A fully connected NN doesn't assume topology/spatial invariance and seems a key benchmark, but is not considered/mentoined.
* [183] How does a vector denote a cluster? the cluster mean, indexes of cluster members?
* [337] Given $\beta$-VAE results in Table 2 are close on the real datasets, have optimal parameters/architecture been used? Why is this not compared to in Table 1?
* The proposed method seems expensive but no mention of this is made. Minor
* [031] unsupervised representation learning (UL) and self-supervised representation learning (SS) are not the same. SSL is a subset of UL, e.g. a $\beta$-VAE performs UL but not SSL, whereas SimCLR performs SSL (and so UL).
* Several typos
* [037] ""arised"", [055] ""singal"" etc",* [225] unclear how permuting pixels leaves stationarity intact * [259] unclear how reducing density prevents redundant sampling.,537,0
SgAPzJdAHi,ICLR_2025,"1. The novelty and contribution of this paper should be further justified as some of the related papers are not included and discussed:
[1] One-Shot Learning as Instruction Data Prospector for Large Language Models. (ACL24)
[1] proposed a method that utilizes one-shot in-context learning with a validation set to find the data that suits the LLMs to be trained, which looks similar to your step2 method “The one-shot in-context learning performance of the student model”.
[2] Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning. (ACL24 findings)
The motivation of [2] is “Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned”, which seems related to yours “Current methods for generating these instruction-tuning datasets tend to focus solely on the quality of the questions and rationales from the teacher model’s perspective, often neglecting the learning preferences of the student language model.”
[3] LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement. (ACL24 findings)
[3] also proposed a teacher-student interactive pipeline that “uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task”. It also seems to try to find the data that student LLMs are not good at, which is related to your work.
I understand the difficulty of doing a thorough literature review, but I think the inclusion and discussions of these papers are highly prefered.
2. As far as I know, GPT-4-LLM, Tulu-v2, UltraChat, and WizardLM datasets are originally built for general instruction-following abilities and they do not explicitly contain the reasoning task training data, while your method seems to use the training data. This might lead to potential unfair comparisons. For example, in Table 3, Vanilla Gemma-2B + previous datasets settings lead to a really low performance compared with your ablations. Maybe a general instruction-following benchmark like Alpaca_Eval is required if you want to compare your method with the previous ones.
3. Related to the previous point, I think Insight 1 should be further justified. More analysis should be made on why “the more detailed the rationale does not necessarily mean the better the performance”. Is it possible that it is only because the evaluation metric is not suitable to the long and diverse responses?
4. For both Insight 1 and 2, I think you might need to constrain your conclusion into “finetuning on sub-task”, because these conclusions might not be true for the general instruction-following abilities, especially for this benchmark:
[4] Evaluating Large Language Models at Evaluating Instruction Following. (ICLR24)","4. For both Insight 1 and 2, I think you might need to constrain your conclusion into “finetuning on sub-task”, because these conclusions might not be true for the general instruction-following abilities, especially for this benchmark: [4] Evaluating Large Language Models at Evaluating Instruction Following. (ICLR24)",538,0
9NfHbWKqMF,ICLR_2025,"- Although some experiments using real-world datasets are conducted, all involved datasets are still mainly object-centric. It is still a problem that if this learning-based method can be applied to real-world and non-object-centric scenes with more complex foreground and background. The corresponding data are much more difficult to collect than the object-centric data, and also more difficult to process and use in training.
- Lack of reporting geometry results. Although there are many comparisons in appearance, it's another important problem that how much can the refinement benefit the reconstructed geometry. However, there are no results like depth and surface normal are shown.","- Lack of reporting geometry results. Although there are many comparisons in appearance, it's another important problem that how much can the refinement benefit the reconstructed geometry. However, there are no results like depth and surface normal are shown.",539,0
fmHS8aBfuH,ICLR_2025,"- Limited Generalizability: Does not scale well to long-form output. No clear pathway for extending to longer text. Should include quantitative analysis showing how performance degrades with output length.
- Even for the short form of generations, the accuracy of the semantical similarity is questionable: The core assumption that semantic similarity can identify undesired content lacks rigorous validation.
- Semantic meaning isn't always indicative of toxicity. Many problematic outputs (like information leakage, implicit bias) may not have obvious semantic markers. For example, Information leakage examples show harmful content may not be semantically toxic. The paper should provide controlled experiments demonstrating the relationship between semantic similarity and different types of unsafe content.
- In addition, this method requires comparison with the pre-defined negative concepts R. What is the coverage of your pre-defined negative concepts? How could this be scaled up? It is not feasible to cover all possible negative responses. This is expensive and not scalable.
- The proposed method introduces too many additional hyper-parameters to tune, such as (b=20 tokens, α=0.98). It might be helpful to understand the sensitivity of the proposed method to these values.
- Limited comparison to SOTA methods like Llama Guard",- Limited Generalizability: Does not scale well to long-form output. No clear pathway for extending to longer text. Should include quantitative analysis showing how performance degrades with output length.,540,0
7RVJxmtzTj,ICLR_2025,"1. It is a more engineering-oriented work that lacks novelty, mainly combining various large models rather than being a research-focused work.
2. The paper lacks some relevant references. It uses 2D models to segment 3D scenes, as do the following works. However, this paper does not provide a comparative discussion with them:
[1] GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields. Yunsong Wang, etc. CVPR, 2024.
[2] Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D. Mukund Varma T, etc. CVPR, 2024
[3] GNeSF: Generalizable Neural Semantic Fields. Hanlin Chen, etc, NeurIPS, 2023.
3. The paper is unclear in certain aspects. For example, is the scale factor a hyperparameter or a learnable parameter? If it is a hyperparameter, what is the set value?
4. The paper lacks implementation details.","2. The paper lacks some relevant references. It uses 2D models to segment 3D scenes, as do the following works. However, this paper does not provide a comparative discussion with them: [1] GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields. Yunsong Wang, etc. CVPR, 2024. [2] Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D. Mukund Varma T, etc. CVPR, 2024 [3] GNeSF: Generalizable Neural Semantic Fields. Hanlin Chen, etc, NeurIPS, 2023.",541,0
mIjblC9hfm,ICLR_2025,"1. The authors suggest that integrating GNN layers into LLMs offers a theoretical advantage over standard LLMs when it comes to modeling graph structures. However, I did not find any accompanying mathematical formulations or equations to substantiate this claim in the appendix.
2. GOFA performs significantly worse than the LLM baselines on the SceneGraphs dataset, despite the authors attributing this to the instruction-tuning data containing information-dense texts. It's concerning that a domain-specific foundation model, despite careful design, does not demonstrate competitive performance.
3. As the authors note, using a frozen LLM compressor while tuning only the GNN layers appears questionable. This approach might impede optimal alignment between information from various modalities, potentially limiting the model's overall effectiveness.","2. GOFA performs significantly worse than the LLM baselines on the SceneGraphs dataset, despite the authors attributing this to the instruction-tuning data containing information-dense texts. It's concerning that a domain-specific foundation model, despite careful design, does not demonstrate competitive performance.",542,0
OD1MV7vf41,ICLR_2025,"•	For clarity, it would be better to differentiate $H$ from $B$ in Section 3.1, as $H$ plays a similar role as $M$ in Section 2.2 which represents the dimension of the random features, while the bottleneck dimension $B$ is analogous to the hidden layer dimension in NNs.
•	A more comprehensive discussion of related work would enhance the paper’s contribution. For instance, [1] investigated representation learning through the approximation of kernels using random Fourier features. Additionally, [2] explored the connection between deep ensembles trained with squared-error loss and GP posteriors.
•	As noted in Section 2.2, random Fourier features are applicable only to stationary kernel functions. However, spatiotemporal data may exhibit non-stationary trends, which presents a potential limitation of the proposed method. A discussion on this point (e.g., extensions to handle non-stationarity) will be appreciated. References:
[1]. Jiang, Z., Zheng, T., Liu, Y., & Carlson, D. (2022). Incorporating prior knowledge into neural networks through an implicit composite kernel. arXiv preprint arXiv:2205.07384.
[2]. He, B., Lakshminarayanan, B., & Teh, Y. W. (2020). Bayesian deep ensembles via the neural tangent kernel. Advances in neural information processing systems, 33, 1010-1022.","• A more comprehensive discussion of related work would enhance the paper’s contribution. For instance, [1] investigated representation learning through the approximation of kernels using random Fourier features. Additionally, [2] explored the connection between deep ensembles trained with squared-error loss and GP posteriors.",543,0
dSneEp59yX,ICLR_2025,"1. I understand the proposed method is a generalization (more flexible version) of sliding window attention, but the main problem is that this paper does not clearly explain why the proposed cascading sub-cache is effective for KV cache eviction (intuitively). It is better to explain more about the background of cascading structure and the key motivations/observations/insights for introducing cascading KV cache.
2. Although experiments are sufficient, the baselines are kind of weak. For example, when evaluating on LongBench, there are more recent dynamic KV cache method such as PyramidKV [1], PyramidInfer[2], InfLLM [3], Quest [4], and FastGen [5]. It will be better to compare with one/two more recent dynamic KV cache method to further support your conclusions.
3. In Section 3.1, it is not clear that why use trunks can achieve linear attention complexity during the prefill stage, and what is the key difference between the proposed method and FlashAttention which also uses tiling to save the memory.
4. From Section 3.2, each sub-cache relies on the previous sub-cache. The sequential procedure seems to have an impact on the inference efficiency. How to avoid this problem and achieve much higer efficiency compared with StreamingLLM.
5. Token selection algorithm leverages exponential moving average (EMA) to track the historical attention score. This method seems similar to directly using accumulated attention score proposed by H2O. Can I understand this process as considering the different importance degrees of current and historical attention score based on the method of H2O, which is controlled by the hyper-parameter gamma.
Related references:
[1] PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling (arXiv 2024)
[2] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference (ACL 2024)
[3] InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory (arXiv 2024)
[4] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference (ICML 2024)
[5] Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs (ICLR 2024)","1. I understand the proposed method is a generalization (more flexible version) of sliding window attention, but the main problem is that this paper does not clearly explain why the proposed cascading sub-cache is effective for KV cache eviction (intuitively). It is better to explain more about the background of cascading structure and the key motivations/observations/insights for introducing cascading KV cache.",544,0
hWF0HH8Rr9,ICLR_2025,"1) Clarity of writing and presentation: There are multiple instances where the paper lacks clarity in terms of writing and the meaning can only be understood with someone who are in this domain.
2) Novelty: The paper does not propose any innovative approach. The use of Transformer or Transformer type architectures with RL and MARL is a well-known approach. This paper showcases the implementation of the architecture for the TSC problem.
3) Baseline comparison: There should be a comparison to traditional TSC algorithms such as SCOOT. Without a comparison to traditional baseline methods, it is not possible to infer the effectiveness of the method.
4) The abstract can be more meaningful. The abstract should cover the outline of the paper. But that’s not the case here.
5) What is the 'static' baseline comparison? Please have a formal definition for it.
6) Figure 7b and 7c: it would be nice if a different color palette and markers were used to represent these plots. In its present for it is very difficult to interpret.
7)Font size for all the plots can be increased",2) Novelty: The paper does not propose any innovative approach. The use of Transformer or Transformer type architectures with RL and MARL is a well-known approach. This paper showcases the implementation of the architecture for the TSC problem.,545,0
FhhH14jso4,ICLR_2025,"1. This paper mainly focuses on robustness evaluation and thus lacks novelty. Developing new attack methods tailored for GraphLLMs or proposing novel defense strategies based on the robustness findings might be interesting.
2. While they evaluate both structural and textual attacks, it lack the full scope of adversarial strategies, like joint structural-textual attack or node injection.
3. The types of evaluation datasets are limited. More datasets can be used like molecular dataset, bioinformatic dataset, social network dataset.
4. This paper only include results of simple architectures like Vanilla GNN and MLP. More complicated models should be evaluated.","3. The types of evaluation datasets are limited. More datasets can be used like molecular dataset, bioinformatic dataset, social network dataset.",546,0
mnB4hDTIDr,ICLR_2025,"- For the goal of holistic analysis, DCScore involves pairwise similarity comparison, which is quadratic complexity as the number of samples, making it hard to scaling for large datasets, e.g., million-level samples, even billion level samples. In Line 63, previous methods struggles to offer a holistic evaluation of a dataset due to its reliance on pairwise similarity while DCScore seems also suffers to this issue.
- The improvements in Table 2 is modest. And it wound be better to analysis whether larger DCScore brings better performance when the models are optimized on the corresponding datasets.
- Section 4.3 can be simplified, most of the content are introducing some details about baseline method, which is not the contribution of this work.
- Seems like the main different between DCScore and VendiScore is the diversity summarization where VendiScore involves the eigenvalue computation, and DCScore use a more efficient softmax operation. More insights about why the eigenvalue computation is redundant wound further enhance the quality of this work.",- The improvements in Table 2 is modest. And it wound be better to analysis whether larger DCScore brings better performance when the models are optimized on the corresponding datasets.,547,0
6gUrqzDNsQ,ICLR_2025,"Unfortunately, the paper is extremely flawed in conception and execution.
- This work falls within the sub-domain of machine learning for optimization, see e.g. [1]. Fundamentally, these are optimization problems, where ML is employed to parameterize a solution or generalize across problem instances. Here, neither is done and I struggle to classify this as an ML paper -- it is primarily a problem-specific optimization algorithm. While two NNs (""encoder"" and ""decoder"") are employed, it is not clear why these are even necessary:
- instead of ""learning the encoder"", why not optimize for each center $s_i$ directly?
- instead of ""learning the decoder"" to decode noisy circle samples back to indices, and computing cross-entropy to the input, why not pose a loss directly on the geometric circles without introducing additional variance and parameters?
While I do not see a principled reason for this, I am open to the possibility that I am wrong and this helps empirically -- but this must be demonstrated, e.g. using ablation studies, of which there are none overall.
- The evaluation is massively flawed. Even though the original statement is a constrained optimization problem, the employed metric is only the objective, i.e., the packing density, with the feasibility being completely ignored. Even visually it is obvious that the constraints are not satisfied as the circles often overlap significantly. As such, the metric and the results are misleading.
- Even with this flawed metric, the empirical results are at best comparable to one other method. However, there is no report of the runtimes. In such optimization problems, there is almost always a trade-off between optimality and runtime, which must be respected for a fair comparison.
- The method is stated to apply to arbitrary shapes, while the parametrization in line 50 and thus the constraint block applies to star-shaped domains only.
- There are several poor presentation choices, e.g., >10 significant digits reported, a poorly formatted table, or repeated references to solutions visually agreeing, while the Packomania solutions are never shown.
- The limitations are not discussed.
[1] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 2021.","- There are several poor presentation choices, e.g., >10 significant digits reported, a poorly formatted table, or repeated references to solutions visually agreeing, while the Packomania solutions are never shown.",548,0
fXkoROek1M,ICLR_2025,"* This approach does not differ significantly from the original DDPO method. For instance, the content in Sections 3.1-3.2 and 4.2 is very similar to Sections 3-4 of the DDPO paper. It seems that the core contribution of this work lies in Section 4.3, where the window selection schemes are proposed. These schemes should be discussed in greater detail, as they appear to be the main novel aspect of this paper.
* Section 4.3 is poorly written compared to the earlier sections. For example, the definition of $\pi_{best}$ is presented twice, which could lead to ambiguity. Additionally, the algorithm tables are too informal, making implementation difficult. It would also be helpful to introduce the definition of the cosine distance for the distance metric $D$, as this would provide greater clarity.
* In Tables 4 and 5, it appears that this paper uses different hyperparameters for DDPO compared to the original settings in the DDPO paper, and this discrepancy should be explained. Additionally, in deep RL, the number of gradient updates per iteration is closely related to the number of samples and the batch size. Typically, PPO performs multiple gradient updates at each iteration. Therefore, having only one gradient update per iteration across all algorithms seems very unusual.","* Section 4.3 is poorly written compared to the earlier sections. For example, the definition of $\pi_{best}$ is presented twice, which could lead to ambiguity. Additionally, the algorithm tables are too informal, making implementation difficult. It would also be helpful to introduce the definition of the cosine distance for the distance metric $D$, as this would provide greater clarity.",549,0
BoRmf8wDZ7,ICLR_2025,"- In supervised tasks, the model primarily utilizes the ViT encoder, without incorporating Gaussian representations. The effectiveness of Gaussian representations is demonstrated in unsupervised tasks. Demonstrating a positive impact on image generation would significantly enhance the paper’s contributions.
- The limited number of Gaussians employed constrains the model’s reconstruction capabilities for image generation. If increasing the Gaussian count presents a bottleneck, this limitation could hinder its application in image generation tasks.
Overall, my main concern lies in the scalability of the method. But as an initial attempt, I think the paper is above the acceptance threshold .","- The limited number of Gaussians employed constrains the model’s reconstruction capabilities for image generation. If increasing the Gaussian count presents a bottleneck, this limitation could hinder its application in image generation tasks. Overall, my main concern lies in the scalability of the method. But as an initial attempt, I think the paper is above the acceptance threshold .",550,0
No2PNOiKgb,ICLR_2025,"- The writing of this paper is hard to follow. Specifically the paper is based on a prior work, yet the prior work is not fully explained beforehand. Therefore the general method reads disconnected and fails to explain the method well enough.
- It is not clear on the relationship between the proposed method and works that perform CSG decomposition, such as [Du et al. 2018]. However, the negative boolean primitive discussed in this work is relevant to the traditional CSG decompostion of 3D geometry. Moreover, can the proposed method be applied to more general CSG grammar?
- The experiment setting of the evaluation section is not clearly illustrated. Specifically the major metric is the reconstruction metric, which might not convey enough message since the main goal of doing decomposition is to facilitate understanding. Moreover the experiment setting of the proposed method and prior works can be pretty different, since prior works do not predict negative primitives.
- Bringing negative primitives to the prediciton result, although improving reconstruction accuracy, yet can bring other issues. For example, the segmentation and understanding can become more tricky when there are negative primitives.
[Du et al. 2018] InverseCSG: automatic conversion of 3D models to CSG trees.","- It is not clear on the relationship between the proposed method and works that perform CSG decomposition, such as [Du et al. 2018]. However, the negative boolean primitive discussed in this work is relevant to the traditional CSG decompostion of 3D geometry. Moreover, can the proposed method be applied to more general CSG grammar?",551,0
cznqgb4DNv,ICLR_2025,"1. The proposed framework does now allow for the slow gradient computation, as gradients cannot be delayed. This is because the framework does not allow for computation and communication sporadicity being correlated (per assumption 4.3.), and according to (2) each gradient has to be computed at the latest local model $\theta_i$. This means that if some node computed the gradient for a long time, and in the meantime local model is being updated due to the communication on this node occurring, the node has to drop its previous computations and start computing its gradient from scratch on the new model so that it can return the gradient at the latest model.
There are two possible solutions to this: either allow for communication and computation being correlated (i.e. while node computes, no communication happens), or allow for the gradients to be delayed.
To me this is a big limitation of the framework.
3. Only asymptotic convergence is proven (i.e. when the number of iterations K goes to infinity), while previous frameworks such as (Even 2024) give convergence bound for any number of iterations K.
4. Paper has several flows in presentation: e.g. first it states that framework the paper analyzes is (2), but then actual analyzed algorithm is in equations (3)-(4) that is not equivalent to (2) (difference comes from re-normalization of the diagonal element p_{ii}^k).
5. Convergence rate derived for non-convex functions is hard to understand, as it depends on some parameters $w_1, w_2, w_3, w_4, w_5$ that are defined only in appendix.
6. Paper does not tune the learning rate, but uses the same learning rate for different algorithms - that might give advantage to some algorithms.
7. Experimental comparison to (Even 2024) is missing. Is there a specific reason why you excluded that baseline from comparison?","3. Only asymptotic convergence is proven (i.e. when the number of iterations K goes to infinity), while previous frameworks such as (Even 2024) give convergence bound for any number of iterations K.",552,0
ekADgawLgI,ICLR_2025,"**Major points**.
1. I am confused by Assumption 3.2 (c'), where the authors said $x$ is an arbitrarily small constant, do you mean $x$ is not fixed? But if $x$ can be changed, this assumption seems very strong. Could the authors clarify this assumption further?
2. Line 782, the author claimed that $\epsilon_t$ is non-increasing. However, I cannot find why. If this is an assumption, please clearly state it. Moreover, if this is necessary, this is an extra condition compared to the original Robbins-Monro condition and the existing works, which weakens the impact of the paper.
3. Line 789, there should be some coefficient in front of $\epsilon_t^{p-1}$ on both L.H.S. and R.H.S. to make the inequality hold, for example, $1/2$. Subsequently, Line 796 should be changed accordingly.
4. Line 803, please define $\theta_{\xi_t}$, which I assume is a convex combination of $\theta_t$ and $\theta_{t+1}$.
5. In Eq. (19), what is $\epsilon$? Do the authors mean $x$? Additionally, I can't find why Line 859 is a direct consequence of Eq. (19). Could the authors elaborate more on this step?
6. The proof is not unified as it is separated by two cases, $p \leq 3$ and $p>3$. What is the obstacle to finding a unified proof?
7. Instead of making assumptions on $g_t$, is it possible to only impose conditions on the stochastic noises, i.e., $g_t-\nabla f(\theta_t)$, like Mertikopoulos et al. (2020)?
**Minor points**.
1. The subscripts $t$ and $n$ are confusing. For example, in the description of Algorithm 1, Lines 120-122, Lines 270-271. Please carefully proofread and make them consistent.
2. The statement for each theorem and for $2<p\leq 3$ still uses Item (c), which I believe should be Item (c') instead.
3. It's better to mention that $\\|\cdot\\|$ denotes $2$-norm somewhere.
4. In Assumptions 3.2 (c) and (c'), please either use $\theta$ (like $f(\theta)-f^*<C_p$) and $\nabla f(\theta_t:\xi_t)$ (to replace $g_t$) or $\mathbb{E}[\\|g_t\\|^{2p-2}\mathbb{I}_{event}]\leq M_p^{\frac{2p-2}{p}}$ directly since the current statement is not mathematical rigorously.
5. Line 166, the authors stated ''while Item (c) and Item (d) together are fully equivalent to...'', could you provide proof or a reference?
6. Line 208, ''we'' should be ''We''.
7. Line 247, replace ''.'' by ''and''?
8. Line 256, ''Appendix'' should be ''appendix''.
9. In addition to the above typos, many others exist. I suggest the authors carefully go through the paper again.","5. Line 166, the authors stated ''while Item (c) and Item (d) together are fully equivalent to...'', could you provide proof or a reference?",553,0
SsWMJ42hJO,ICLR_2025,"1.	The paper appears to conflate two distinct concepts: dimensional collapse and neural collapse. Dimensional collapse in semi-supervised learning (SSL) typically refers to the limited dimensionality of learned SSL features, while neural collapse is associated with a desirable state in supervised training marked by several good qualities such as intra-class alignment and inter-class separation (properties NC1-NC4).
2.	The claim regarding orthogonal structures achieving the most distinguishable classes relative to the Simplex ETF structure is unclear. Simplex ETF theoretically maximizes angular separation, reaching a pairwise cosine value of -1/(k-1), whereas orthogonal structures attain pairwise cosine values of zero.
3.	The issue of dimensional collapse is primarily addressed by the SSL community; however, the paper's focus is on semi-supervised methods, yet it lacks comparison with other semi-supervised baselines.
4.	The proposed method should be compared with ETF-based(or uniform variants) methods[1] to adequately demonstrate its effectiveness. Moreover, to comprehensively address dimensional collapse, additional self-supervised experiments are recommended, including comparisons with ETF-based SSL methods [2] and other methods targeting dimensional collapse [3,4].
5.	Experiments are limited to small-scale datasets, such as CIFAR and Tiny-ImageNet, which may restrict the generalizability of the findings.
6.	The performance improvements are not significant on larger batch sizes.
[1] Targeted supervised contrastive learning for long-tailed recognition.
[2] Combating Representation Learning Disparity with Geometric Harmonization.
[3] Understanding dimensional collapse in contrastive self-supervised learning.
[4] Variance-invariance-covariance regularization for self-supervised learning.","2. The claim regarding orthogonal structures achieving the most distinguishable classes relative to the Simplex ETF structure is unclear. Simplex ETF theoretically maximizes angular separation, reaching a pairwise cosine value of -1/(k-1), whereas orthogonal structures attain pairwise cosine values of zero.",554,0
b87H1A3sxm,ICLR_2025,"1. The Introduction lacks the summary about the main problem mentioned in the third paragraph of this section. Authors state that data which are near the decision boundary are in general more susceptible, and then intrdouce the definition of proposed metrics. However, how does the proposed defense solve this problem? Providing additional analyses about it may be better.
2. The evaluations with and without synthetic data are somewhat confusing in terms of the used model’s capacity. In Table 2, why the smaller model WRN-28-10 was used for the larger dataset containing synthetic data instead of using the larger WRN-34-10? Please provide explanations for such an inconsistency.
3. It seems that ablation studies for hyper-parameters are not shown. Adding comprehensive ablation studies may be better for verifying the effectiveness of each module. Additionally, the authors can ablate the individual components of the proposed method (e.g. robust CMI vs robust separation) to demonstrate the contribution of each.
4. Cross-norm evaluations may be more convincing for the generalization ability of the proposed method. There are various evaluations about the robustness under L∞-norm, while the L2-norm attacks also deserve to be evaluated. The authors can include evaluations against specific L2-norm attacks (e.g. PGD-L2, C&W-L2) in their main results tables, in addition to the existing L$_\infty$-norm evaluations.","3. It seems that ablation studies for hyper-parameters are not shown. Adding comprehensive ablation studies may be better for verifying the effectiveness of each module. Additionally, the authors can ablate the individual components of the proposed method (e.g. robust CMI vs robust separation) to demonstrate the contribution of each.",555,0
FZa1UCC9SC,ICLR_2025,"**Weak Points**
Since there is no dedicated space to provide one, I will write my ""Overall Comment"" here. I relegate a ""Detailed Feedback"" to the Questions, where I corroborate more on the points below.
**Overall Comment:** The paper addresses interesting questions and demonstrates potential through its mathematically sound approach. To enhance the quality and impact of the manuscript, I recommend focusing on several key areas for improvement, which are detailed below. Given the current status of the paper, I recommend **rejection**: Addressing these points could significantly strengthen the work which certainly deserves it.
1. **Inclusion of Related Works:** To strengthen the manuscript, it would be beneficial to discuss existing literature on continuous-time modeling of optimizers, particularly the Weak-Approximation (WA) Framework introduced by Li et al. (2019). Including this framework and comparing it with your approach could provide valuable context and demonstrate how your work contributes to the field. Additionally, since the SDE of Adam has been derived by Malladi et al. (2022), and given that SignSGD is a special case of Adam, incorporating a comparison with these results would enhance the depth of the analysis. Including relevant references when stating key facts will also improve the manuscript's credibility (See below for details).
2. **Assumption Justification and Validation:** Clarifying and justifying the assumptions upon which your analysis is based would strengthen the paper. Providing theoretical or experimental validation for these assumptions will help readers understand their appropriateness and relevance. It would also be valuable to explain any advantages your setup may have over the Weak-Approximation framework. For instance, elaborating on the necessity of the high-dimensional setup in your SDE derivations, especially when such a requirement is not present in weak approximations, could clarify the benefits of your approach.
3. **Addressing Conceptual Missteps:** It's important to ensure that the continuous-time models derived in your work faithfully represent the behavior of the optimizers they are intended to model. While your paper provides guarantees for the risk (as per Theorem 1), extending these guarantees to other critical aspects—such as the gradient norm and the norm of the iterates (as defined in Definition 2 of Li et al.)—would strengthen the validity of your models. **Currently, insights are derived primarily about the SDEs rather than the optimizers themselves but are framed as insights on the optimizers.** To effectively carry these insights over to the optimizers, it would be beneficial to provide guarantees that the SDEs closely track the behavior of their respective optimizers or to include experimental verifications demonstrating this correspondence.
4. **Inclusion of Experimental Validation:** Incorporating experimental validation of your results and insights would greatly enhance the manuscript. Validating the theoretical findings empirically will demonstrate the practical applicability of your SDE models and confirm that they are informative. For instance, if you derive a bound on the risk using an SDE, empirically verifying this bound for the respective optimizer would provide strong support for your theoretical claims.
5. **Improving Structure and Clarity:** Enhancing the organization and clarity of the manuscript would significantly improve its readability and impact. Formalizing key results as Lemmas, Propositions, or Theorems, rather than discussing them in free text, would make the arguments more rigorous and easier to follow. Clearly defining all symbols and terms, such as ""Vanilla ODE,"" will help avoid confusion. Additionally, ensuring that figures are accessible to all readers, including those who are colorblind, by choosing appropriate color schemes and providing clear labels, will enhance the overall quality of the presentation.
I have provided detailed feedback on these points in the **Questions** section below, where I elaborate further on how the manuscript can be improved. **References:**
- Li et al. (2019) *""Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations""*.
- Malladi et al. (2022) *""On the SDEs and Scaling Rules for Adaptive Gradient Algorithms""*.","**Weak Points** Since there is no dedicated space to provide one, I will write my ""Overall Comment"" here. I relegate a ""Detailed Feedback"" to the Questions, where I corroborate more on the points below. **Overall Comment:** The paper addresses interesting questions and demonstrates potential through its mathematically sound approach. To enhance the quality and impact of the manuscript, I recommend focusing on several key areas for improvement, which are detailed below. Given the current status of the paper, I recommend **rejection**: Addressing these points could significantly strengthen the work which certainly deserves it.",556,1
GmE8ovvXaJ,ICLR_2025,"1. The content in Sec 4.2 MULTI-LEVEL CORRELATION COMPUTATION AND AGGREGATION is somewhat ambiguous. The overall process flow should be: Shape the correlation map from 4D to 1D -> order -> reorder -> reshape the correlation map from 1D to 4D. Is the sorting performed along the $H \times W \times H \times W$ dimension or along the $2L$ dimension? If it is along the $H \times W \times H \times W$ dimension, is the sorting directly based on similarity in descending order? How are the potential spatial relationships between correlation elements encoded?
2. Similarity-aware Selective Scanning: Mamba relies on its inherent order to capture the relative positional relationships between sequence tokens. When correlation elements are sorted by similarity before scanning, does this mean no longer need to consider the spatial locations of these elements?
3. Parameters of Feature Aggregation: Line 215 states that ""The feature aggregators share the same weights across all levels to maintain efficiency."" However, Lines 320-322 describe the feature aggregation layer as consisting of two layers of 2D convolution with a kernel size of 5, with output channel dimensions of 64 and 14, respectively, and a ReLU activation function in between. This implies that feature aggregation includes four 2D convolutional layers and two ReLU activations. However, in Table 6, feature aggregation is reported to have 42.5M parameters, which appears inconsistent. Please provide a more detailed explanation.","3. Parameters of Feature Aggregation: Line 215 states that ""The feature aggregators share the same weights across all levels to maintain efficiency."" However, Lines 320-322 describe the feature aggregation layer as consisting of two layers of 2D convolution with a kernel size of 5, with output channel dimensions of 64 and 14, respectively, and a ReLU activation function in between. This implies that feature aggregation includes four 2D convolutional layers and two ReLU activations. However, in Table 6, feature aggregation is reported to have 42.5M parameters, which appears inconsistent. Please provide a more detailed explanation.",557,0
I4YAIwrsXa,ICLR_2025,"1. I have various doubts listed in the question section.
2. I'd love to see a short technical details section in the main body, model sizes, and the number of tokens used in training. Btw. what is the number of tokens in the RL training?
3. I'd love to see more ablations (e.g., other models). I realize that such experiments are costly, still, perhaps feasible with smaller models, to study in more detail the influence of design choices.","2. I'd love to see a short technical details section in the main body, model sizes, and the number of tokens used in training. Btw. what is the number of tokens in the RL training?",558,0
I0mQlersGk,ICLR_2025,"1.limited Applicability: The model performs less effectively on datasets with high node degrees (e.g., ogbn-products), suggesting that SVQ may suffer from information loss in highly connected graphs.
2.Lack of Technical Detail: The practical impact of SVQ and CGSA on computational complexity, hardware requirements, and performance trade-offs is not sufficiently analyzed, potentially limiting the model's real-world applicability.
3.Unclear Advantage of Innovation: The paper could better articulate the distinct advantages of SVQ over traditional quantization methods, especially regarding its unique application in SNN-based quantization.
4.The authors should set up ablation experiments to verify the model improvement on the speed of inference.
5.The description of how the node information of a graph is converted into a codebook is poor.",5.The description of how the node information of a graph is converted into a codebook is poor.,559,0
MB53uAZKSc,ICLR_2025,"1. **Limited Insights**: The paper does not provide new insights into continual pretraining. The experimental results indicate that cyclic learning rate schedules, data replay, and regularization methods are insufficient for maintaining both strong in-domain performance and reduced forgetting. A clearer explanation of why each method fails under different conditions would enhance the understanding. The experimental section reads more like a report than an analysis, which limits its usefulness for future researchers.
2. **Unconvincing Experiments**: The performance of EWC is heavily influenced by the weight of the regularization term, creating a trade-off between plasticity and stability. There is a lack of detailed discussion regarding the hyperparameter analysis of EWC. While Appendix C mentions that “λ = 10^7 performed best when tuning between 10^1 and 10^9,” the varying performance of EWC across different evaluation datasets (as shown in Table 3) raises concerns. It’s possible for EWC to perform well on one dataset while underperforming on another. This issue extends beyond EWC to other methods as well, suggesting a sensitivity to hyperparameters that warrants further exploration.
3. **Poor Presentation**: The overall presentation, particularly in the experimental section, is lacking. Important analyses are often relegated to figure captions, such as those for Figure 4 and Table 2, which detracts from the clarity of the findings.","1. **Limited Insights**: The paper does not provide new insights into continual pretraining. The experimental results indicate that cyclic learning rate schedules, data replay, and regularization methods are insufficient for maintaining both strong in-domain performance and reduced forgetting. A clearer explanation of why each method fails under different conditions would enhance the understanding. The experimental section reads more like a report than an analysis, which limits its usefulness for future researchers.",560,0
kZvor5aaz7,ICLR_2025,"I do not find any big weakness of this paper. There is one claim that sounds not precise to me and I hope the author can revise it.
- In line 468 and Fig.5 caption, authors claim that SlotAdapt ""generates nearly perfect reconstructions"". I do not think this is the case, as apparently the reconstructed images are clearly different from the input image. This result itself is not a weakness of the method, as slots are compressed representations and cannot preserve all visual details. However, I do believe the statement itself is wrong. Minor:
- Please unify the name of U-Net in the paper. In Fig.1 and line 230 it is called ""Unet"", in line 207 ""U-Net"", in line 223 and 224 ""UNet"".
- Line 320 ""v1.5 for MOVi-E; and COCO"", I think this "";"" is redundant.
- The generation results of SlotAdapt are impressive, yet authors only put small figures in the main paper (Fig. 5 and 6). I understand this is due to page limit. Please add text to refer readers to large figures in Appendix Fig. 8 and 10.","- The generation results of SlotAdapt are impressive, yet authors only put small figures in the main paper (Fig. 5 and 6). I understand this is due to page limit. Please add text to refer readers to large figures in Appendix Fig. 8 and 10.",561,0
ZqM9mZkrRB,ICLR_2025,"0. Innovation. Please notice the paper: Guo et al (2023, NeurIP 2023) and Zach et al (2023, SSVM). The ideas seem quite similar. Moreover, there are a lot of work using non-Gaussian priors, or actually more general priors.
1. Although the authors mention 'not all scenarios where diffusion models are used enjoy access to extensive training re-sources', the scale of the experiments are far too small. Have you tried scaling up?
2. What if the number of clusters are quite large? Solving large clusters problem can be another new issues. If having numerous number of clusters, the prior can be ill-posed. It can be close to nonparametric case and the computation is quite expensive. This method already restricts the dim of the problem. It seems that the number of clusters also needs restricted.
3. Theoretically, from Gaussian to GMM, people just need one step, using one more layer like in the hierarchical models.
4. CIFAR-10 and NIST datasets are a little bit small. And in the experiment, only original DDPM and SGM compared. FIDs are far higher than SOTA, maybe indicating the lack of training.","2. What if the number of clusters are quite large? Solving large clusters problem can be another new issues. If having numerous number of clusters, the prior can be ill-posed. It can be close to nonparametric case and the computation is quite expensive. This method already restricts the dim of the problem. It seems that the number of clusters also needs restricted.",562,0
r7wMVdGFro,ICLR_2025,"- In Table 1, the authors use only 30 words for in-distribution canaries, compared to 50 words for synthetic canaries (as mentioned in a footnote). A larger canary size generally results in higher AUC-ROC numbers. Even for SST-2, the in-distribution canaries perform better than synthetic canaries, which diminishes the contribution of the authors regarding the efficiency of their canaries. This pattern is also visible in Tables 6 and 7.
- The AUC-ROC scores for model attacks in Table 1 suggest that the fine-tuned model is overfitted (likely due to a high n_rep=12). Knowing this, results of MIA scores for synthetic data are little bit low. The scores using SIM_jac and SIM_emb are nearly random, with only the 2-gram model scores slightly better. It also raises the question of the practicality of having high n_rep values like 12 or 16 in real-world synthetic data privacy auditing. These results do not strongly support the new MIA contribution. Additionally, Figure 1 (a and b) show near-random curves for n_rep=2 or 4.","- In Table 1, the authors use only 30 words for in-distribution canaries, compared to 50 words for synthetic canaries (as mentioned in a footnote). A larger canary size generally results in higher AUC-ROC numbers. Even for SST-2, the in-distribution canaries perform better than synthetic canaries, which diminishes the contribution of the authors regarding the efficiency of their canaries. This pattern is also visible in Tables 6 and 7.",563,0
p8qhVIo980,ICLR_2025,"1.Considering that the paper finalizes the relevant selection strategies by taking the intersection of three selection methods (Forward Selection, Backward Selection, Datamodels Selection), does this combined approach demonstrate a performance advantage over using single-strategy methods?
2.The paper evaluates three independent weighting schemes. Are there other feasible weighting schemes beyond these three? Alternatively, could an adaptive mechanism be introduced to adjust these weights based on the characteristics of the dataset? Further explanation on this aspect would aid in understanding the flexibility and effectiveness of the ACSESS approach.
3.While ACSESS demonstrates excellent performance in text and image classification tasks, could this method also be applied to other few-shot learning scenarios, such as more challenging tasks like object detection, semantic segmentation, or depth estimation?","1.Considering that the paper finalizes the relevant selection strategies by taking the intersection of three selection methods (Forward Selection, Backward Selection, Datamodels Selection), does this combined approach demonstrate a performance advantage over using single-strategy methods?",564,0
2vaTZH31oR,ICLR_2025,"- There is a general lack of technical insights
- FlexRM stage already proposed (Stage 2): Previous works [1,2] in feed-forward 3D generation already proposed last year to decode triplane features into 3D Gaussian attributes.
- Multi-view image generation already proposed (Stage 1): MVDream [3] and follow-up works already turn pre-trained image generators into multi-view generators.
- Multi-view image generation with video model already proposed (Stage 1): Previous works [4,5] already proposed to use video generators for novel view synthesis given an image as an input.
- Conditioning with camera already proposed and marginal (Stage 2): previous works such as SV3D [5] already proposed to condition the generation with camera matrices. In this work it is used in the image encoder DINO. However, the ablation in Tab. 3 shows that the model with “No stronger camera cond” only shows very marginal improvement?
- Imperfect data simulation with marginal improvements (Stage 2): the data simulation part in the method section sounds rather complicated and unnecessary given its little impact in Tab. 5? Similar to the camera conditioning, the metrics only show very marginal improvement?
- No computational cost analysis: The method seems very complicated, it would be good to compare training and inference time against previous works. References:
- [1] Zou et al., Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers, arXiv 2023
- [2] Xu et al., AGG: Amortized Generative 3D Gaussians for Single Image to 3D, TMLR 2024
- [3] Shi et al., MVDream: Multi-view Diffusion for 3D Generation, ICLR 2024
- [4] Kwak et al., ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models, CVPR 2024
- [5] Voleti et al., SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion, ECCV 2024",- Multi-view image generation already proposed (Stage 1): MVDream [3] and follow-up works already turn pre-trained image generators into multi-view generators.,565,0
xawA8X5dHq,ICLR_2025,"1. The claim that ""entirely fictional content"" removes pre-existing data influence is overstated. While the fictional setup minimizes real-world knowledge's impact, it does not fully eliminate it. The generated content of the fictional gland using LLMs stills adhere to the knowledge framework of medicine. Thus, there is pre-existing foundational medical knowledge from LLMs embedded in the content and questions even though no pre-existing data specific for the fictional gland exists. LLMs’ reasoning ability with prior knowledge is not entirely ruled out. This claim, central to the paper’s message, is thus an overstatement. This is a limitation of the method in isolating LLMs' test-taking abilities.
2. The statistical analysis is appreciated, but the authors do not clearly explain the apparent disparity between the small and negligible performance differences derived statistically and the visible differences shown in Figure 1. This may confuse readers unfamiliar with advanced statistical analysis.
3. A known issue with multiple-choice evaluation is selection bias/position bias for option order and IDs. Studies show LLMs may have positional preferences (e.g., favoring option A), making it necessary to control for option order, as positional bias could affect model performance significantly. The authors do not address this issue.
4. The discussion on the distribution of correct answers and models' ""inferential strategies"" lacks clarity. Terms like ""inferential strategies"" are uncommon in this field and require further elaboration. Additionally, the paragraph addressing language in the discussion section could benefit from better clarity.
5. While the paper highlights issues with current multiple-choice evaluation, it does not propose and develop solutions. A method that can mitigate the issue and suggest potential paths forward would make the contribution more impactful, moving beyond identifying a problem to offering ways to address it.
6. Presentation flaws are present, such as incomplete sentences in the limitations section, which disrupt readability. For instance, the ""Knowledge coherence"" paragraph in the limitations section is unfinished, and the ""Model selection"" paragraph in the same section ends abruptly with ""This selection.""","3. A known issue with multiple-choice evaluation is selection bias/position bias for option order and IDs. Studies show LLMs may have positional preferences (e.g., favoring option A), making it necessary to control for option order, as positional bias could affect model performance significantly. The authors do not address this issue.",566,0
G0dksFayVq,ICLR_2025,"While the experiments generally show that the resulting benchmark is challenging and has good separability, I believe there are more points that could be further dug into. The limited analysis makes the paper's contribution less significant.
1. The ""key insight"" mentioned in L229-231 is not verified carefully. The authors state: ""if a model struggles to consistently generate correct, coherent responses to a challenging question, it will find it difficult to differentiate between those responses."" From Table 4, this doesn't seem to be the case for all setups, especially for reasoning, math, and coding, where we see that stronger models can be weaker judges (Claude-3.5 and Llama-3.1 in reasoning).
2. Also, in Table 1, we see that many LLMs-judges perform less than random-chance. For example, JudgeLM gets a 11.96 accuracy in evaluating coding questions. Does this mean it always predicts the opposite answer? Because that would be another type of bias. There is no failure case analysis or type analysis for these LLM-based judges.
3. In paper [1], the authors notice that reasoning-heavy questions are hard for LLM-based judges to evaluate. Therefore, it is better for them to first construct its own reference answer, and then evaluate the candidate answer. They call this the reference-based judge. Have you considered similar simple prompting strategies to improve performance? How would they perform?
4. In the section on L426, the authors simply state that advancing the reasoning ability of LLM-based judges could help increase performance. This is a rather generic conclusion. It would be nice to have more contributions in producing a better LLM judge that could score higher on JudgeBench, using the insights that the authors have discovered. If not so, this paper stands only on the analysis level. And the analysis is not so deep.
5. The amount of questions collected (350) is small. Could you show the confidence intervals of the resulting estimations? Reference:
[1] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena","2. Also, in Table 1, we see that many LLMs-judges perform less than random-chance. For example, JudgeLM gets a 11.96 accuracy in evaluating coding questions. Does this mean it always predicts the opposite answer? Because that would be another type of bias. There is no failure case analysis or type analysis for these LLM-based judges.",567,0
Jj4XIKX4TJ,ICLR_2025,"1) The main concern I have is that with the SO(3) averaged flow, the method could potentially benefit more from scaling the model size since more model capacity is needed to learn the density to be SO(3) invariant. However, the author only shows the method could reach a comparable performance with less number of parameters. This causes a concern about whether the proposed method could be effective in a larger setting either with more model parameters or larger molecules. The current experimental results do not show how the method performs with more parameters.
2) Part of the contribution is applying the reflow strategy, a commonly used method for accelerated sampling, to molecular conformer generation. The results are quite promising on the GEOM-QM9 dataset (Table 1), but on a larger dataset GEOM-Drugs (Table 2) the performance degrades significantly for both 'AvgFlow-reflow' and 'AvgFlow-Distill'. This first poses the concern about the necessity of applying the reflow to the problem with huge performance degradation, and second goes back to the question if the SO(3) averaging flow could benefit more from using a large model on a larger dataset.
3) I am also a bit concerned about the quality-speed tradeoff the author mentions in the paper. Ideally, both the accuracy and inference speed should be pushed at the same time.
4) Section 3.1 is a bit unclear to me. From algorithms 1, it seems like the training is operated by augmenting the training set with different rotations. Hence, I am not quite sure about the meaning of presenting the idea with the symmetry group in section 3.1. If the author wants to justify the theoretical motivation and advantages, the writing could be improved by explicitly highlighting the theoretical results.","4) Section 3.1 is a bit unclear to me. From algorithms 1, it seems like the training is operated by augmenting the training set with different rotations. Hence, I am not quite sure about the meaning of presenting the idea with the symmetry group in section 3.1. If the author wants to justify the theoretical motivation and advantages, the writing could be improved by explicitly highlighting the theoretical results.",568,0
NIPS_2017_217,NIPS_2017,"- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].
- ""Embedding"" is an overloaded word for a scalar value that represents object ID.
- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.
- Keypoint detection results should be included in the experiments section.
- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?
- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.
Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.","- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck. Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.",569,0
NIPS_2017_480,NIPS_2017,"and limitations.
Other comments:
* Section 2.1: maybe itâs not necessary to introduce discounts and rewards at all, given that neither are used in the paper?
* Section 3.1: the method for finding the factors seems very brittle, and to rely on disentangled feature representations that are not noisy. Please discuss these limitations, and maybe hint at how factors could be found if the observations were a noisy sensory stream like vision.
* Line 192: freezing the partitioning in the first iteration seems like a risky choice that makes strong assumptions about the coverage of the initial data. At least discuss the limitations of this.
* Section 4: there is a mismatch between these options and the desired properties discussed in section 2.2: in particular, the proposed options are not âsubgoal optionsâ because their distribution over termination states strongly depends on the start states? Same for the Treasure Game.
* Line 218: explicitly define what the âgreedyâ baseline is.
* Figure 4: Comparing the greedy results between (b) and (c), it appears that whenever a key is obtained, the treasure is almost always found too, contrasting with the MCTS version that explores a lot of key-but-no-treasure states. Can you explain this?","* Section 3.1: the method for finding the factors seems very brittle, and to rely on disentangled feature representations that are not noisy. Please discuss these limitations, and maybe hint at how factors could be found if the observations were a noisy sensory stream like vision.",570,0
NIPS_2017_356,NIPS_2017,"- I would have liked to see some analysis about the distribution of the addressing coefficients (Betas) with and without the bias towards sequential addressing. This difference seems to be very important for the synthetic task (likely because each question is based on the answer set of the previous one). Also I don't think the value of the trade-off parameter (Theta) was ever mentioned. What was it and how was it selected? If instead of a soft attention, the attention from the previous question was simply used, how would that baseline perform?
- Towards the same point, to what degree does the sequential bias affect the VisDial results?
- Minor complaint. There are a lot of footnotes which can be distracting, although I understand they are good for conserving clarity / space while still providing useful details.
- Does the dynamic weight prediction seem to identify a handful of modes depending on the question being asked? Analysis of these weights (perhaps tSNE colored by question type) would be interesting.
- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures.",- Does the dynamic weight prediction seem to identify a handful of modes depending on the question being asked? Analysis of these weights (perhaps tSNE colored by question type) would be interesting.,571,0
NIPS_2017_130,NIPS_2017,"weakness)?
4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)?
5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data?
6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss.",5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data?,572,0
NIPS_2017_194,NIPS_2017,"of the method, makes it easy to understand whether the proposed algorithm can be useful for one's problem setting.
My reasons for not giving a higher rating are
1. The restricted applicability
3. I'm not entirely sure about the fit to NIPS in terms of originality. On the one hand it is 'only' as an extension to an existing algorithm, but on the other hand this extension is non-trivial and requires a considerable amount of thought.
** Quality
The paper is technically sound with both a theoretical analysis and convincing experimental results. Strengths and also restrictions of the algorithm are clearly stated and discussed, giving a good understanding of when the algorithm could be useful.
** Clarity
The overall quality of the paper is good. For the most part it is well written and easy to follow due to good explanations.
However, I think section 4.2 ('Filtered-System Prediction Phase') could be a bit more detailed. While it is possible to understand the explanation, I think it would be easier to do so if it were more explicit in terms of how various random variables influence each other.
Some examples/ideas (please correct me if necessary):
- In equation (5), I assume \mu^m_{t|t-1} is as known from the previous iteration step?
- Also in equation (5), is mu^x_t computed according to the 'standard' PILCO algorithm? The only other mentioning of mu^x is somewhere in equation (1) without explicitly discussing it. Also, I guess instead of /tilde{mu}^x_{t-1} (like in PILCO), one computes it based on mu^{\tilde{m}}?
Overall I just think that this section should be at least slightly longer and more detailed as it is describing one of the two core parts of the new method.
The following is a very minor point as it's fairly clear what is meant:
I am unsure about the notation of the believe b (for example in line 174) as b is sometimes used as if it is a random variable (e.g. line 156) and sometimes as a distribution (e.g. line 175). I think it boils down to (for me), that to me the equation in line 174, namely p(x|z,u) \sim N(m,V), should use an equal sign instead of \sim. A clearer notation here might also resolve that between equations (21) and (22) p(M) and M are used interchangeably?
A few small typos I spotted:
- Line 7: There's 'with' missing
- Line 19: of => for (I think)
- Line 127: and => in
- Line 147: detailed _in_ section 4.1
** Originality
The proposed algorithm is an extension to a widely used algorithm, building upon several earlier works with a similar goal. It does not propose entirely new concepts (i.e. both filtering and PILCO are widely known) put combines them in a new and non-trivial way.
** Significance
The proposed algorithm is useful in a restricted set of problem that are suitable for PILCO but additionally have (close to?) gaussian observation noise. However, given that PILCO is widely used and quite powerful and that the authors of this paper clearly state the strengths and weaknesses of their approach, I believe this to be a valuable contribution.","- In equation (5), I assume \mu^m_{t|t-1} is as known from the previous iteration step?",573,0
NIPS_2017_369,NIPS_2017,"of their presented approach, showing promising results on MNIST variants and drawbacks on more realistic tasks like Cifar10. Clarity
The paper is well-organized, but some details are confusing or unclear.
- Discuss difference to original capsule work.
- Why are 1-3 refinement iterations chosen, what happens after more iterations?
- How many iterations were necessary for Cifar10?
- Compare the computational cost of baseline and capsules, as well as the cost of the refinement steps.
- What happens when the invariant sum over the coupled prediction vectors in equation (2) and the associated non-linearity are replaced by a simple linear layer and standard non-linearity?
- line 135: ""We test using a single model with no ... data augmentation"". A couple of lines before, the authors mention they do moderately augment data with shifts. Why do shifts improve performance, given that the authors claim capsules are designed to be robust to such variations? Originality
The presented work is original as it introduces a new routing principle for capsule networks.
However, novelty with respect to classical capsules should be discussed more clearly.
Relevant related work, either dealing with separating filter response into magnitude and orientation, estimating keypoints or doing locally adaptive filtering, as opposed to global normalization of STNs: https://arxiv.org/abs/1701.01833 https://arxiv.org/abs/1703.06211 https://arxiv.org/abs/1612.04642 https://arxiv.org/abs/1706.00598 https://arxiv.org/abs/1605.01224 https://arxiv.org/abs/1605.09673 Significance
The presented results are not very strong and it is hard to say how significant the findings are, as the authors do not thoroughly investigate more interesting domains than digits.
Performance on MNIST is a very limited metric, given that:
i) Saturated at this point
ii) Scattering has shown that local invariance wrt deformation, translation and rotation is enough to achieve very good performance
iii) It lacks ambiguity and many other properties that make natural images so challenging, especially the assumption of only one entity per location becomes questionable under clutter
The robustness results on affine and overlapping MNIST are promising, but should be validated on more realistic tasks with more challenging local statistics.
It would be great if the authors would provide the reader with insight into strengths and weaknesses on more realistic problems.
Some suggestions:
i) More thorough evaluation + visualisations on Cifar10. The results seem weak for now, but might shed some light on failure modes and work to be accomplished by follow-up papers
ii) Check if affine robustness holds for Cifar10 as well to similar degree, this would change my vote on the paper
iii) The iterative Tagger (Graeff et al.) might give some inspiration for additional experiments with more natural ambiguity and should be discussed in related work as well
A strong analysis on the drawbacks of the presented method and open problems would make this a very useful paper, but in its current form, it is hard to tell what the suggested method achieves beyond being potentially more representationally efficient and robust on variants of MNIST.","- Compare the computational cost of baseline and capsules, as well as the cost of the refinement steps.",574,0
NIPS_2017_302,NIPS_2017,"1.	Related Work: As the available space allows it, the paper would benefit from a more detailed discussion of related work, by not only describing the related works, but also discussing the differences to the presented work.
2.	Qualitative results: To underline the success of the work, the paper should include some qualitative examples, comparing its generated sentences to the ones by related work.
3.	Experimental setup: For the Coco image cations, the paper does not rely on the official training/validation/test split used in the COCO captioning challenge.
3.1.	Why do the authors not use the entire training set?
3.2.	It would be important for the automatic evaluation to report results using the evaluation sever and report numbers on the blind test set (for the human eval it is fine to use the validation set). Conclusion:
I hope the authors will include the coco caption evaluation server results in the rebuttal and final version as well as several qualitative results.
Given the novelty of the approach and strong experiments without major flaws I recommend accepting the paper.
It would be interesting if the authors would comment on which problems and how their approach can be applied to non-sequence problems.","2. Qualitative results: To underline the success of the work, the paper should include some qualitative examples, comparing its generated sentences to the ones by related work.",575,0
NIPS_2017_351,NIPS_2017,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?",- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.,576,0
NIPS_2017_567,NIPS_2017,"Weakness:
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly.
Here are some examples:
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)?
(2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.
2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN.
3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper.
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.","3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper.",577,0
NIPS_2017_182,NIPS_2017,"Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).
Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.
[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. âModeling Relationships in Referential Expressions with Compositional Modular Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. âModeling Context Between Objects for Referring Expression Understanding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. âSubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.â In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378â86. Curran Associates, Inc.",2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22] 3. It is unclear what SPAT means in Table.,578,0
NIPS_2017_226,NIPS_2017,"- An important reference is missing
- Other less important references are missing
- Bare-bones evaluation
The paper provides an approach to solve linear inverse problems by reducing training requirements. While there is some prior work in this area (notably the reference below and reference [4] of the paper), the paper has some very interesting improvements over them. In particular, the paper combines the best parts of [4] (decoupling of signal prior from the specific inverse problem being solved) and Lista (fast implementation). That said, evaluation is rather skim â almost anecdotal at times â and this needs fixing. There are other concerns as well on the specific choices made for the matrix inversion that needs clarification and justifications.
1) One of the main parts of the paper is a network learnt to invert (I + A^T A). The paper used the Woodbury identity to change it to a different form and learns the inverse of (I+AA^T) since this is a smaller matrix to invert. At test time, we need to apply not just this network but also ""A"" and ""A^T"" operators.
A competitor to this is to learn a deep network that inverts (I+A^T A). A key advantage of this is that we do not need apply A and A^T during test time. It is true that that this network learns to inverts a larger matrix ... But at test time we have increased rewards. Could we see a comparison against this ? both interms of accuracy as well as runtime (at training and testing) ?
2) Important reference missing. The paper is closely related to the idea of unrolling, first proposed in, âListaâ http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf
While there are important similarities and differences between the proposed work and Lista, it is important that the paper talks about them and places itself in appropriate context.
3) Evaluation is rather limited to a visual comparison to very basic competitors (bicubic and wiener filter). It would be good to have comparisons to
- Specialized DNN: this would provide the loss in performance due to avoiding the specialized network.
- Speed-ups over [4] given the similarity (not having this is ok given [4] is an arXiv paper)
- Quantitative numbers that capture actual improvements over vanilla priors like TV and wavelets and gap to specialized DNNs. Typos
- Figure 2: Syntehtic",3) Evaluation is rather limited to a visual comparison to very basic competitors (bicubic and wiener filter). It would be good to have comparisons to - Specialized DNN: this would provide the loss in performance due to avoiding the specialized network.,579,0
NIPS_2017_390,NIPS_2017,"- I am curious how the performance varies quantitatively if the training ""shot"" is not the same as ""test"" shot: In realistic applications, knowing the ""shot"" before-hand is a fairly strong and impractical assumption.
- I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.","- I am curious how the performance varies quantitatively if the training ""shot"" is not the same as ""test"" shot: In realistic applications, knowing the ""shot"" before-hand is a fairly strong and impractical assumption.",580,0
NIPS_2017_631,NIPS_2017,"- I don't understand why Section 2.1 is included. Batch Normalization is a general technique as is the proposed Conditional Batch Normalization (CBN). The description of the proposed methodology seems independent of the choice of model and the time spent describing the ResNet architecture could be better used to provide greater motivation and intuition for the proposed CBN approach.
- On that note, I understand the neurological motivation for why early vision may benefit from language modulation, but the argument for why this should be done through the normalization parameters is less well argued (especially in Section 3). The intro mentions the proposed approach reduces over-fitting compared to fine-tuning but doesn't discuss CBN in the context of alternative early-fusion strategies.
- As CBN is a general method, I would have been more convinced by improvements in performance across multiple model architectures for vision + language tasks. For instance, CBN seems directly applicable to the MCB architecture. I acknowledge that needing to backprop through the CNN causes memory concerns which might be limiting.
- Given the argument for early modulation of vision, it is a bit surprising that applying CBN to Stage 4 (the highest level stage) accounts for majority of the improvement in both the VQA and GuessWhat tasks. Some added discussion in this section might be useful. The supplementary figures are also interesting, showing that question conditioned separations in image space only occur after later stages.
- Figures 2 and 3 seem somewhat redundant.
Minor things:
- I would have liked to see how different questions change the feature representation of a single image. Perhaps by applying some gradient visualization method to the visual features when changing the question?
- Consider adding a space before citation brackets.
- Bolding of the baseline models is inconsistent.
- Eq 2 has a gamma_j rather than gamma_c
L34 'to let the question to attend' -> 'to let the question attend'
L42 missing citation
L53 first discussion of batch norm missing citation
L58 ""to which we refer as"" -> ""which we refer to as""
L89 ""is achieved a"" -> ""is achieved through a""","- As CBN is a general method, I would have been more convinced by improvements in performance across multiple model architectures for vision + language tasks. For instance, CBN seems directly applicable to the MCB architecture. I acknowledge that needing to backprop through the CNN causes memory concerns which might be limiting.",581,0
NIPS_2017_235,NIPS_2017,"weakness even if true but worth discussing in detail since it
could guide future work.) [EDIT: I see now that this is *not* the case, see response below]
I also feel that the discussion of Chow-Liu is missing a very
important aspect. Chow-Liu doesn't just correctly recover the
true structure when run on data generated from a tree. Rather,
Chow-Lui finds the *maximum-likelihood* tree for data from an
*arbitrary* distribution. This is a property that almost all
follow-up work does not satisfy (Srebro showed bounds). The
discussion in this paper is all true, but doesn't mention that
""maximum likelihood"" or ""model robustness"" issue at all, which
is hugely important in practice.
For reference: The basic result is that given a single node $u$,
and a hypothesized set of ""separators"" $S$ (neighbors of $u$) then
there will be some set of nodes $I$ with size at most $r-1$ such
that $u$ and $I$ have positive conditional mutual information.
The proof of the central result proceeds by setting up a
""game"", which works as follows:
1) We pick a node $X_u$ to look at.
2) Alice draws two joint samples $X$ and $X'$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of neighbors of $X_u$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
Here I first felt like I *must* be missing something, since
this is just establishing that $X_u$ has mutual information
with it's neighbors. (There is no reference to the ""separator""
set S in the main result.) However, it later appears that this
is just a warmup (regular mutual information) and can be
extended to the conditional setting.
Actually, couldn't the conditional setting itself be phrased
as a game, something like
1) We pick a node $X_u$ and a set of hypothesized ""separators""
$X_S$ to look at.
2) Alice draws two joint samples $X$ and $X'$ Both are
conditioned on the some random value for $X_S$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of nodes (not including $X_u$ or $X_S$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
I don't think this adds anything to the final result, but is
an intuition for something closer to the final goal.
After all this, the paper discusses an algorithm for greedily
learning an MRF graph (in sort of the obvious way, by
exploiting the above result) There is some analysis of how
often you might go wrong estimating mutual information from
samples, which I appreciate.
Overall, as far as I can see, the result appears to be
true. However, I'm not sure that the purely theoretical result
is sufficiently interesting (at NIPS) to be published with no
experiments. As I mentioned above, Chow-Liu has the major
advantage of finding the maximum likelihood solution, which the current method
does not appear to have. (It would violate hardness results
due to Srebro.) Further, note that the bound given in Theorem
5.1, despite the high order, is only for correctly recovering
the structure of a single node, so there would need to be
another lever of applying the union bound to this result with
lower delta to get a correct full model.
EDIT AFTER REBUTTAL:
Thanks for the rebuttal. I see now that I should understand $r$ not as the maximum clique size, but rather as the maximum order of interactions. (E.g. if one has a fully-connected Ising model, you would have r=2 but the maximum clique size would be n). This answers the question I had about this being a generalization of Bressler's result. (That is, this paper's is a strict generalization.) This does slightly improve my estimation of this paper, though I thought this was a relatively small concern in any case. My more serious concerns are that a pure theoretical result is appropriate for NIPS.","4) Alice picks a random set of neighbors of $X_u$, call them $X_I$.",582,1
NIPS_2017_631,NIPS_2017,"- I don't understand why Section 2.1 is included. Batch Normalization is a general technique as is the proposed Conditional Batch Normalization (CBN). The description of the proposed methodology seems independent of the choice of model and the time spent describing the ResNet architecture could be better used to provide greater motivation and intuition for the proposed CBN approach.
- On that note, I understand the neurological motivation for why early vision may benefit from language modulation, but the argument for why this should be done through the normalization parameters is less well argued (especially in Section 3). The intro mentions the proposed approach reduces over-fitting compared to fine-tuning but doesn't discuss CBN in the context of alternative early-fusion strategies.
- As CBN is a general method, I would have been more convinced by improvements in performance across multiple model architectures for vision + language tasks. For instance, CBN seems directly applicable to the MCB architecture. I acknowledge that needing to backprop through the CNN causes memory concerns which might be limiting.
- Given the argument for early modulation of vision, it is a bit surprising that applying CBN to Stage 4 (the highest level stage) accounts for majority of the improvement in both the VQA and GuessWhat tasks. Some added discussion in this section might be useful. The supplementary figures are also interesting, showing that question conditioned separations in image space only occur after later stages.
- Figures 2 and 3 seem somewhat redundant.
Minor things:
- I would have liked to see how different questions change the feature representation of a single image. Perhaps by applying some gradient visualization method to the visual features when changing the question?
- Consider adding a space before citation brackets.
- Bolding of the baseline models is inconsistent.
- Eq 2 has a gamma_j rather than gamma_c
L34 'to let the question to attend' -> 'to let the question attend'
L42 missing citation
L53 first discussion of batch norm missing citation
L58 ""to which we refer as"" -> ""which we refer to as""
L89 ""is achieved a"" -> ""is achieved through a""","- Given the argument for early modulation of vision, it is a bit surprising that applying CBN to Stage 4 (the highest level stage) accounts for majority of the improvement in both the VQA and GuessWhat tasks. Some added discussion in this section might be useful. The supplementary figures are also interesting, showing that question conditioned separations in image space only occur after later stages.",583,0
NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.",584,0
NIPS_2017_351,NIPS_2017,"1.	The approach mentions attention over 3 modalities â image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.
2.	From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.
3.	Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).
4.	The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.
5.	Other errors/typos:
a.	L38: mechanism -> mechanisms
b.	L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets?
Post-rebuttal comments:
Although the authors' response to the concern of ""Proposed model not outperforming existing models for 2 modalities"" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt.
I have a question about one of the responses from the authors --
> Authors' response -- âMCB vs. MCTâ: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results.
Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%.",4. The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.,585,0
NIPS_2017_502,NIPS_2017,"Weakness]
- This paper is poorly written.
- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
- Sharing the style of citations and bullet items is confusing.
- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing.
- The descriptions of baseline models are unclear.
- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled. [Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results. [Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.
Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article.
In sum, as far as I am concerned this work makes a contribution but is insufficient.","- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.",586,1
NIPS_2017_488,NIPS_2017,"-
- No major weaknesses. It would be good to have a quantitative evaluation of the
generative performance, but that is understandably hard to do.
Minor commenpt and typos-
- ""For the transition operator to produce smooth mixing, it is necessary for
the latent variable q(z|x)."" : Something missing here ?
- In Fig 5, it might help compare results better if the same half-images were
used for both ALI and GibbsNet. Overall
The proposed model is novel and has many desirable properties. It provides a
good example of using adversarial training to learn an interesting generative
model. This idea seems quite powerful and can in future be applied on more
challenging problems.","- In Fig 5, it might help compare results better if the same half-images were used for both ALI and GibbsNet. Overall The proposed model is novel and has many desirable properties. It provides a good example of using adversarial training to learn an interesting generative model. This idea seems quite powerful and can in future be applied on more challenging problems.",587,1
NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.",5. L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?,588,0
NIPS_2017_114,NIPS_2017,"Weakness-
- Comparison to other semi-supervised approaches : Other approaches such as variants of Ladder networks would be relevant models to compare to. Questions/Comments-
- In Table 3, what is the difference between \Pi and \Pi (ours) ?
- In Table 3, is EMA-weighting used for other baseline models (""Supervised"", \Pi, etc) ? To ensure a fair comparison, it would be good to know that all the models being compared to make use of the EMA benefits.
- The proposed model benefits from two factors : noise and keeping an exponential moving average. It would be good to see how much each factor contributes on its own. The \Pi model captures just the noise part, so it would be useful to know how much gain can be obtained by just using a noise-free exponential moving average.
- If averaging in parameter space is being used, it seems that it should be possible to apply the consistency cost in the intermediate layers of the model as well. That could potentially provide a richer consistency gradient. Was this tried ?
Minor comments and typos-
- In the abstract : ""The recently proposed Temporal Ensembling has ... "": Please cite.
- ""when learning large datasets."" -> ""when learning on large datasets.""
- ""zero-dimensional data points of the input space"": It may not be accurate to say that the data points are zero-dimensional.
- ""barely applying"", "" barely replicating"" : ""barely"" -> ""merely""
- ""softmax output of a model does not provide a good Bayesian approximation outside training data"". Bayesian approximation to what ? Please explain. Any model will have some more generalization error outside training data. Is there another source of error being referred to here ? Overall-
The paper proposes a simple and effective way of using unlabelled data and
improving generalization with labelled data. The most attractive property is
probably the low overhead of using this in practice, so it is quite likely that
this approach could be impactful and widely used.","- Comparison to other semi-supervised approaches : Other approaches such as variants of Ladder networks would be relevant models to compare to. Questions/Comments- - In Table 3, what is the difference between \Pi and \Pi (ours) ?",589,0
NIPS_2017_560,NIPS_2017,"weakness, but this seems not to be a problem in most examples.
3. Equation 2.6 is wrong as written; as it does not make sense to divide by a vector. (easy to fix, but I surprised at the sloppiness here given that the paper well written overall).
4. Just for clarity, in eq 1.1, state clearly that F_\theta(x) is submodular in x for every \theta.
5. Can some nonconvex constraint sets which have an easy projection be handled as well?
6. What if the projection onto set K can be computed only approximately?",5. Can some nonconvex constraint sets which have an easy projection be handled as well?,590,0
NIPS_2017_434,NIPS_2017,"---
This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?
2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated. This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.
3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?
4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?
5. Are the learned object state embeddings interpretable in any way before decoding?
6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
* How many different kinds of physical interaction can be in one simulation?
* How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?
Preliminary Evaluation ---
Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.",5. Are the learned object state embeddings interpretable in any way before decoding?,591,0
NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?","- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.",592,0
NIPS_2017_390,NIPS_2017,"+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode, etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained. Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.
Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.","- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16) - This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.",593,0
NIPS_2017_502,NIPS_2017,"Weakness]
- This paper is poorly written.
- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
- Sharing the style of citations and bullet items is confusing.
- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing.
- The descriptions of baseline models are unclear.
- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled. [Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results. [Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.
Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article.
In sum, as far as I am concerned this work makes a contribution but is insufficient.","- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.",594,0
NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.",9. Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?,595,0
NIPS_2017_365,NIPS_2017,"It seems the proposed method requires the a priori knowledge of the number of latent modes. How would the performance change if this number were incorrectly chosen?
The clarity of the paper could be improved. Notably, it would help the reader if the overall objective function of the proposed were stated once. Otherwise it is a bit too implicit in my opinion.
Besides, I think more emphasis should be put on the fact that the method needs auxiliary information (velocity, previous two actions and car damage). Currently I donât think the method is fully end-to-end as claimed in the conclusion.
Moreover I feel the abstract overstates the achievement of the proposed method. I wouldnât qualify what the method achieves in the two tasks, turn and pass, as producing different driving styles.
Other comments:
- The authors should recall the definition of the notation E_{\pi_\theta}[f(s, a)] for a given function f.
- In (2), Q -> Q_\psi
- l.191: D -> D_\omega
- l.232: output that indicates
- l.317: it -> It","- The authors should recall the definition of the notation E_{\pi_\theta}[f(s, a)] for a given function f.",596,0
NIPS_2017_604,NIPS_2017,"-
- Experimental comparisons with other discrete latent variable VAE approaches
are not reported. It is mentioned that the proposed model is the first to
achieve comparable performance with continuous latent variable models, but it
would be useful to provide a quantitative assessment as well.
- The motivation for having discrete latent variables is not entirely
convincing. Even if the input space is discrete, it is not obvious why the
latent space should be. For example, the paper argues that ""Language is
inherently discrete, similarly speech is typically represented as a sequence of
symbols"" and that this makes discrete representations a natural fit for such
domains. However, the latent semantic representation of words or speech could
still be continuous, which would make it possible to capture subtle inflections
in meaning or tone which might otherwise be lost in a discretized or quantized
representation. The applications to compression are definitely more convincing. Overall-
The use of VQ to work with discrete latent variables in VAEs is a novel
contribution. The model is shown to work well on a multiple domains and
datasets. While the qualitative results are convincing, the paper can be made
more complete by including some more comparisons, especially with other ways of
producing discrete latent representations.","- - Experimental comparisons with other discrete latent variable VAE approaches are not reported. It is mentioned that the proposed model is the first to achieve comparable performance with continuous latent variable models, but it would be useful to provide a quantitative assessment as well.",597,0
NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.","4. In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?",598,0
NIPS_2017_645,NIPS_2017,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.,599,0
NIPS_2017_262,NIPS_2017,"that are addressed below:
* Most of the theoretical work presented here are built upon prior work, it is not clear what is the novelty and research contribution of the paper.
* The figures are small and almost unreadable
* It doesn't clearly state how equation 5, follows from equation 4
* It is not clear how \theta^{t+1/2} come into the picture. Explain
* S^{*} and S^{~} are very important parameters in the paper, yet they were not properly defined. In line 163 it is claimed that S^{~} is defined in Assumption 1, but one can see the definition is not proper, it is rather cyclic.
* Since the comparison metric here is wall-clock time, it is imperative that the implementation of the algorithms be the same. It is not clear that it is guaranteed. Also, the size of the experimental data is quite small.
* If we look into the run-times of DCPN for sim_1k, sim_5k, and sim_10k and compare with DC+ACD, we see that DCPN is performing better, which is good. But the trend line tells us a different story; between 1k and 10k data the DCPN run-time is about 8x while the competitor grows by only 2x. From this trend it looks like the proposed algorithm will perform inferior to the competitor when the data size is larger e.g., 100K.
* Typo in line 106","* If we look into the run-times of DCPN for sim_1k, sim_5k, and sim_10k and compare with DC+ACD, we see that DCPN is performing better, which is good. But the trend line tells us a different story; between 1k and 10k data the DCPN run-time is about 8x while the competitor grows by only 2x. From this trend it looks like the proposed algorithm will perform inferior to the competitor when the data size is larger e.g., 100K.",600,0
NIPS_2017_104,NIPS_2017,"---
There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.
* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?
* How does this setting relate to question answering or visual question answering?
* How does the model perform on the same train data it's seen already? How much does it overfit?
* How hard is it to find intuitive attention examples as in figure 4?
* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.
* The related works section would be better understood knowing how the model works, so it should be presented later.",* How hard is it to find intuitive attention examples as in figure 4?,601,0
NIPS_2017_236,NIPS_2017,"Weakness:
1. The real applications that the proposed method can be applied to seem to be rather restricted. It seems the proposed algorithm can only be used as a fast evaluation of residual error for 'guessing' or 'predetermining' the range of Tucker ranks, not the real ranks.
2. Since the sampling size 's' depends on the exponential term 2^[1/(e^2K-2)] (in Theorem 3.4), it could be very large if one requires the error tolerance 'e' to be relatively small and the order of tensor 'K' to be high. In that situation, there won't be much benefits to use this algorithm. Question:
1. In the Fig.1, why blue curve with large sample size 's=80' achieves the worst error compared with that of red curve with small sample size 's=20'?
Overall, although proposed algorithm is theoretically sound, but appears be limited in applications for practical propose.","1. The real applications that the proposed method can be applied to seem to be rather restricted. It seems the proposed algorithm can only be used as a fast evaluation of residual error for 'guessing' or 'predetermining' the range of Tucker ranks, not the real ranks.",602,0
NIPS_2017_65,NIPS_2017,"1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification
2) the optimization procedure used to solve the multi-objective optimization problem is not discussed in adequate detail
Detailed comments below:
Methods and Evaluation: The proposed objective is interesting and utilizes ideas from two well studied lines of research, namely, privileged learning and distribution matching to build classifiers that can incorporate multiple notions of fairness. The authors also demonstrate how some of the existing methods for learning fair classifiers are special cases of their framework. It would have been good to discuss the goal of each of the terms in the objective in more detail in Section 3.3. The part that is probably the most weakest in the entire discussion of the approach is the discussion of the optimization procedure. The authors state that there are different ways to optimize the multi-objective optimization problem they formulate without mentioning clearly which is the procedure they employ and why (in Section 3). There seems to be some discussion about the same in experiments section (first paragraph) and I think what was done is that the objective was first converted into unconstrained optimization problem and then an optimal solution from the pareto set was found using BFGS. This discussion is still quite rudimentary and it would be good to explain the pros and cons of this procedure w.r.t. other possible optimization procedures that could have been employed to optimize the objective.
The baselines used to compare the proposed approach and the evaluation in general seems a bit weak to me. Ideally, it would be good to employ baselines that learn fair classifiers based on different notions (E.g., Hardt et. al. and Zafar et. al.) and compare how well the proposed approach performs on each notion of fairness in comparison with the corresponding baseline that is designed to optimize for that notion. Furthermore, I am curious as to why k-fold cross validation was not used in generating the results. Also, was the split between train and test set done randomly? And, why are the proportions of train and test different for different datasets?
Clarity of Presentation:
The presentation is clear in general and the paper is readable. However, there are certain cases where the writing gets a bit choppy. Comments:
1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear.
2. In Section 3.3, it would be good to discuss the goal of including each of the terms in the objective in the text clearly.
3. In Section 4, more details about the choice of train/test splits need to be provided (see above).
While this paper proposes a useful framework that can handle multiple notions of fairness, there is scope for improving it quite a bit in terms of its experimental evaluation and discussion of some of the technical details.",1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear.,603,0
NIPS_2017_356,NIPS_2017,"- I would have liked to see some analysis about the distribution of the addressing coefficients (Betas) with and without the bias towards sequential addressing. This difference seems to be very important for the synthetic task (likely because each question is based on the answer set of the previous one). Also I don't think the value of the trade-off parameter (Theta) was ever mentioned. What was it and how was it selected? If instead of a soft attention, the attention from the previous question was simply used, how would that baseline perform?
- Towards the same point, to what degree does the sequential bias affect the VisDial results?
- Minor complaint. There are a lot of footnotes which can be distracting, although I understand they are good for conserving clarity / space while still providing useful details.
- Does the dynamic weight prediction seem to identify a handful of modes depending on the question being asked? Analysis of these weights (perhaps tSNE colored by question type) would be interesting.
- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures.","- I would have liked to see some analysis about the distribution of the addressing coefficients (Betas) with and without the bias towards sequential addressing. This difference seems to be very important for the synthetic task (likely because each question is based on the answer set of the previous one). Also I don't think the value of the trade-off parameter (Theta) was ever mentioned. What was it and how was it selected? If instead of a soft attention, the attention from the previous question was simply used, how would that baseline perform?",604,0
NIPS_2017_15,NIPS_2017,"WEAKNESS AND CONCONERN
1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
3) The authors should also cite recent work on frame prediction. this is very related.",3) The authors should also cite recent work on frame prediction. this is very related.,605,0
NIPS_2017_349,NIPS_2017,"- The paper is not self contained
Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility.
I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.
- Use of deep-reinforcement learning is not well motivated
The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.
- > We argue that such a mechanism is more realistic because it has an effect within the game itself, not just on the scores
This is probably the most unclear part. It's not clear to me why the paper considers one to be more realistic than the other rather than just modeling different incentives? Probably not enough space in the paper but actual comparison of learning dynamics when the opportunity costs are modeled as penalties instead. As economists say: incentives matter. However, if the intention was to explicitly avoid such explicit incentives, as they _would_ affect the model-free reinforcement learning algorithm, then those reasons should be clearly stated.
- Unclear whether bringing connections to human cognition makes sense
As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".
* Minor comments
** Line 16:
> [18] found them...
Consider using \citeauthor{} ?
** Line 167:
> be the N -th agentâs
should be i-th agent?
** Figure 3:
Clarify what the `fillcolor` implies and how many runs were the results averaged over?
** Figure 4:
Is not self contained and refers to Fig. 6 which is in the supplementary. The figure is understandably large and hard to fit in the main paper, but at least consider clarifying that it's in the supplementary (as you have clarified for other figures from the supplementary mentioned in the main paper).
** Figure 5:
- Consider increasing the axes margins? Markers at 0 and 12 are cut off.
- Increase space between the main caption and sub-caption.
** Line 299:
From Fig 5b, it's not clear that |R|=7 is the maximum. To my eyes, 6 seems higher.","- Use of deep-reinforcement learning is not well motivated The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.",606,0
NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.",607,0
NIPS_2017_182,NIPS_2017,"Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).
Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.
[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. âModeling Relationships in Referential Expressions with Compositional Modular Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. âModeling Context Between Objects for Referring Expression Understanding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. âSubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.â In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378â86. Curran Associates, Inc.","5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig.",608,1
NIPS_2017_53,NIPS_2017,"--
-- The authors claim that their model is interpretable, but do not support this claim by any studies/analyses/qualitative examples. I agree that the ablation studies show the significance of each of the modules used in the proposed model, but it is not clear if the model is actually using the visual features from the correct module for a given question, e.g. for the question âDoes the woman look happy?â, is it really the face analysis module which provides the correct signal to accurately predict the answer? This can be studied by analysing (e.g. making a pie chart of) the questions whose predictions were incorrect when the face analysis module was absent but are correct after adding the face analysis module. Similar studies can be done for all the modules. Without any such analysis, I am not convinced if the model is actually doing what the motivation of the paper says. If the author can report any studies/analyses supporting this, I am happy to increase my rating.
-- The paper does not talk about the weaknesses of the proposed model. Discussions, ideally using failure cases, about limitations of the proposed model and what is needed to improve it are very important for continuous research in this area and should be an integral part of any paper.
-- I am confused about the mathematics of some equations. In Eq 1, how is the matrix multiplication of 2 matrices P and W resulting in a vector c? In Eq 3, what dimension is the max pooling happening over? In Eq 4, should it be transpose of r_{k} which is being premultiplied with tensor W_{k}?
-- In Table 2, the MCB numbers reported are test-dev numbers instead of test-standard. MCB only reports test-standard numbers for their ensemble model with data augmentation. Please fix.
-- For classification modules, have the authors tried using soft probability scores instead of picking top k class labels?
-- Minor:
- A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.
- r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.
- Line 9: âdoes not limitâ â âlimitsâ
After author rebuttal: I thank the authors for the additional analyses. The rebuttal addresses all my concerns. I think this paper introduces an interesting and novel approach for VQA, which would be useful to the VQA research community. Therefore, I recommend the paper for acceptance. I have changed my rating to 7.","- r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.",609,0
NIPS_2017_12,NIPS_2017,"* Results in Figure 4 are not charming. When there are no Byzantine failures, the proposed algorithm lags far behind simple averaging. It starts to show its power only after one third of the workers are Byzantine. This looks like an extremely high failure rate. Apparently, averaging will remain to be extremely competitive when the failure rates are realistically low, making the motivation of the proposed approach questionable.
* While the paper introduces an idea that starts to become useful when the data set is dramatically large, the reported experiments are on extremely small data sets. MNIST has 60000 instances, spambase has 4601. The real effect of the divergence between the true and the stochastic gradient starts to become visible when the data set is large enough. With today's hardware technology, a modest workstation with a few-thousand-dollar GPU can be easily trained on MILLIONS of data points without any need for such protocols as Krum. Furthermore, whether more data points than a few million are needed depends totally on applications. In many, the performance difference between 10 Million and 100 Million is ignorably small. For a machine learner to be convinced to slow down her model for the sake of safe distributedness, the authors should pinpoint applications where there is a real need for this and report results on those very applications.
* The proposed algorithm is for updating global parameters from stochastic gradients calculated on minibatches. Although this approach is fundamental to many machine learning models, the central challenge of the contemporary techniques is different. Deep neural nets require distributed computing to distribute operations across ""neurons"" rather than minibatches. The fact that the proposed algorithm cannot be generalized to this scenario reduces the impact potential of this work significantly.
Minor point: The paper would also be stronger if it cited earlier pioneer work on distributed machine learning. One example is:
C.T. Chu et al., Map-Reduce for Machine Learning on Multicore, NIPS, 2007 ---
Preliminary Evaluation:
While I appreciate the nice theoretical work behind this paper and that distributed machine learning is an issue of key importance, the reported results shed doubt on the usefulness of the proposed approach. ---
Final Evaluation:
I do acknowledge that 33% Byzantine failure rate is a standard test case for general distributed computing tasks, but here our concern is training a machine learning model. The dynamics are a lot different from ""somehow"" processing as large data bunches as possible. The top-priority issue is accuracy, not data size. According to Figure 4, Krum severely undermines the model accuracy if there is no attack. This literally means a machine learner will accept to use Krum only when she is ABSOLUTELY sure that i) a 10-20 million data point subset will not be sufficient for satisfactory accuracy (hence distributed computing is required), and ii) at least 33% of the nodes will act Byzantine (hence Krum is required). As a machine learner, I am trying hard but not managing to find out such a case. Essentially, it is not the readership's but the authors' duty to bring those cases to attention. This is missing in both the paper and the rebuttal. I keep my initial negative vote.","* Results in Figure 4 are not charming. When there are no Byzantine failures, the proposed algorithm lags far behind simple averaging. It starts to show its power only after one third of the workers are Byzantine. This looks like an extremely high failure rate. Apparently, averaging will remain to be extremely competitive when the failure rates are realistically low, making the motivation of the proposed approach questionable.",610,0
NIPS_2017_382,NIPS_2017,"weakness that there is much tuning and other specifics of the implementation that need to be determined on a case by case basis. It could be improved by giving some discussion of guidelines, principles, or references to other work explaining how tuning can be done, and some acknowledgement that the meaning of fairness may change dramatically depending on that tuning.
* Clarity
The paper is well organized and explained. It could be improved by some acknowledgement that there are a number of other (competing, often contradictory) definitions of fairness, and that the two appearing as constraints in the present work can in fact be contradictory in such a way that the optimization problem may be infeasible for some values of the tuning parameters.
* Originality
The most closely related work of Zemel et al. (2013) is referenced, the present paper explains how it is different, and gives comparisons in simulations. It could be improved by making these comparisons more systematic with respect to the tuning of each method--i.e. compare the best performance of each.
* Significance
The broad problem addressed here is of the utmost importance. I believe the popularity of (IF) and modularity of using preprocessing to address fairness means the present paper is likely to be used or built upon.","* Clarity The paper is well organized and explained. It could be improved by some acknowledgement that there are a number of other (competing, often contradictory) definitions of fairness, and that the two appearing as constraints in the present work can in fact be contradictory in such a way that the optimization problem may be infeasible for some values of the tuning parameters.",611,0
NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.","1. The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.",612,0
NIPS_2017_104,NIPS_2017,"---
There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.
* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?
* How does this setting relate to question answering or visual question answering?
* How does the model perform on the same train data it's seen already? How much does it overfit?
* How hard is it to find intuitive attention examples as in figure 4?
* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.
* The related works section would be better understood knowing how the model works, so it should be presented later.","* The related works section would be better understood knowing how the model works, so it should be presented later.",613,0
NIPS_2017_188,NIPS_2017,"of different approaches.
5) Some formulations are not quite clear, such as âlimited stochasticityâ vs âpowerful decoderâ in lines 88 and 96. Also the statement in line 111 about âapproximately optimizing the KL divergenceâ and the corresponding footnote looks a bit too abstract - so do the authors optimize it or not?
6) In the bibliography the authors tend to ignore the ICLR conference and list many officially published papers as arxiv.
7) Putting a whole section on cross-domain relations to the appendix is not good practice at all. I realize itâs difficult to fit all content to 8 pages, but itâs the job of the authors to organize the paper in such a way that all important contributions fit into the main paper.
Overall, I am in the borderline mode. The results are quite good, but the novelty seems limited.","7) Putting a whole section on cross-domain relations to the appendix is not good practice at all. I realize itâs difficult to fit all content to 8 pages, but itâs the job of the authors to organize the paper in such a way that all important contributions fit into the main paper. Overall, I am in the borderline mode. The results are quite good, but the novelty seems limited.",614,0
NIPS_2017_104,NIPS_2017,"---
There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.
* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?
* How does this setting relate to question answering or visual question answering?
* How does the model perform on the same train data it's seen already? How much does it overfit?
* How hard is it to find intuitive attention examples as in figure 4?
* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.
* The related works section would be better understood knowing how the model works, so it should be presented later.","* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?",615,0
NIPS_2017_235,NIPS_2017,"weakness even if true but worth discussing in detail since it
could guide future work.) [EDIT: I see now that this is *not* the case, see response below]
I also feel that the discussion of Chow-Liu is missing a very
important aspect. Chow-Liu doesn't just correctly recover the
true structure when run on data generated from a tree. Rather,
Chow-Lui finds the *maximum-likelihood* tree for data from an
*arbitrary* distribution. This is a property that almost all
follow-up work does not satisfy (Srebro showed bounds). The
discussion in this paper is all true, but doesn't mention that
""maximum likelihood"" or ""model robustness"" issue at all, which
is hugely important in practice.
For reference: The basic result is that given a single node $u$,
and a hypothesized set of ""separators"" $S$ (neighbors of $u$) then
there will be some set of nodes $I$ with size at most $r-1$ such
that $u$ and $I$ have positive conditional mutual information.
The proof of the central result proceeds by setting up a
""game"", which works as follows:
1) We pick a node $X_u$ to look at.
2) Alice draws two joint samples $X$ and $X'$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of neighbors of $X_u$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
Here I first felt like I *must* be missing something, since
this is just establishing that $X_u$ has mutual information
with it's neighbors. (There is no reference to the ""separator""
set S in the main result.) However, it later appears that this
is just a warmup (regular mutual information) and can be
extended to the conditional setting.
Actually, couldn't the conditional setting itself be phrased
as a game, something like
1) We pick a node $X_u$ and a set of hypothesized ""separators""
$X_S$ to look at.
2) Alice draws two joint samples $X$ and $X'$ Both are
conditioned on the some random value for $X_S$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of nodes (not including $X_u$ or $X_S$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
I don't think this adds anything to the final result, but is
an intuition for something closer to the final goal.
After all this, the paper discusses an algorithm for greedily
learning an MRF graph (in sort of the obvious way, by
exploiting the above result) There is some analysis of how
often you might go wrong estimating mutual information from
samples, which I appreciate.
Overall, as far as I can see, the result appears to be
true. However, I'm not sure that the purely theoretical result
is sufficiently interesting (at NIPS) to be published with no
experiments. As I mentioned above, Chow-Liu has the major
advantage of finding the maximum likelihood solution, which the current method
does not appear to have. (It would violate hardness results
due to Srebro.) Further, note that the bound given in Theorem
5.1, despite the high order, is only for correctly recovering
the structure of a single node, so there would need to be
another lever of applying the union bound to this result with
lower delta to get a correct full model.
EDIT AFTER REBUTTAL:
Thanks for the rebuttal. I see now that I should understand $r$ not as the maximum clique size, but rather as the maximum order of interactions. (E.g. if one has a fully-connected Ising model, you would have r=2 but the maximum clique size would be n). This answers the question I had about this being a generalization of Bressler's result. (That is, this paper's is a strict generalization.) This does slightly improve my estimation of this paper, though I thought this was a relatively small concern in any case. My more serious concerns are that a pure theoretical result is appropriate for NIPS.","1) We pick a node $X_u$ and a set of hypothesized ""separators"" $X_S$ to look at.",616,1
NIPS_2017_351,NIPS_2017,"1.	The approach mentions attention over 3 modalities â image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.
2.	From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.
3.	Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).
4.	The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.
5.	Other errors/typos:
a.	L38: mechanism -> mechanisms
b.	L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets?
Post-rebuttal comments:
Although the authors' response to the concern of ""Proposed model not outperforming existing models for 2 modalities"" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt.
I have a question about one of the responses from the authors --
> Authors' response -- âMCB vs. MCTâ: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results.
Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%.","2. From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.",617,0
NIPS_2017_143,NIPS_2017,"For me the main issue with this paper is that the relevance of the *specific* problem that they study -- maximizing the ""best response"" payoff (l127) on test data -- remains unclear. I don't see a substantial motivation in terms of a link to settings (real or theoretical) that are relevant:
- In which real scenarios is the objective given by the adverserial prediction accuracy they propose, in contrast to classical prediction accuracy?
- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?
- In l50 they claim that ""pershaps even in most [...] practical scenarios"" predicting accurate on the majority is most important. I contradict: in many areas with safety issues such as robotics and self-driving cars (generally: control), the models are allowed to have small errors, but by no means may have large errors (imagine a self-driving car to significantly overestimate the distance to the next car in 1% of the situations).
Related to this, in my view they fall short of what they claim as their contribution in the introduction and in l79-87:
- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).
- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).
- Related to this, in the experiments it would be interesting to see the comparison of the classical squared/absolute error on the test set as well (since this is what LSE claims to optimize).
- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet. REMARKS:
What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff?
Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y.","- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet. REMARKS: What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff? Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y.",618,0
NIPS_2017_110,NIPS_2017,"of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: ""Then we made it explicit"" instead of ""Then we have explicit it""","- Throughout, the authors use the term constrains and should change to constraints.",619,0
NIPS_2017_262,NIPS_2017,"that are addressed below:
* Most of the theoretical work presented here are built upon prior work, it is not clear what is the novelty and research contribution of the paper.
* The figures are small and almost unreadable
* It doesn't clearly state how equation 5, follows from equation 4
* It is not clear how \theta^{t+1/2} come into the picture. Explain
* S^{*} and S^{~} are very important parameters in the paper, yet they were not properly defined. In line 163 it is claimed that S^{~} is defined in Assumption 1, but one can see the definition is not proper, it is rather cyclic.
* Since the comparison metric here is wall-clock time, it is imperative that the implementation of the algorithms be the same. It is not clear that it is guaranteed. Also, the size of the experimental data is quite small.
* If we look into the run-times of DCPN for sim_1k, sim_5k, and sim_10k and compare with DC+ACD, we see that DCPN is performing better, which is good. But the trend line tells us a different story; between 1k and 10k data the DCPN run-time is about 8x while the competitor grows by only 2x. From this trend it looks like the proposed algorithm will perform inferior to the competitor when the data size is larger e.g., 100K.
* Typo in line 106","* Since the comparison metric here is wall-clock time, it is imperative that the implementation of the algorithms be the same. It is not clear that it is guaranteed. Also, the size of the experimental data is quite small.",620,0
NIPS_2017_414,NIPS_2017,"weakness of the paper to me is that I found the flow of the mathematical argument at times hard to follow. For ex.:
- the role and usefulness of Lemma 1 and Corollary 1.1 appears much later than where they are introduced.
- It is not shown that Eq 8 corresponds to a lower bound on the log-likelihood (as Eq 1 is known to be); is it or is it not? The argumentation of l 144 to 148 feels a little convoluted and indirect. I agree that Eq 8 is maximized when p(x,z) and q(x,z) match, but it is unclear to me, and from the paper, what the tradeoff between its two terms achieves when q does not have the capacity to match p. This should be discussed.
l 183: Proposition 1. ""equilibrium â¦ is achieved""
About GAN objectives you mentioned earlier in l 119 that ""This objective mismatch may lead to the well-known instability issues associated with GAN training"". Now your objective in Eq 12 uses a similar min-max objective. Couldn't there be similar instability or collapse issues?
l 238: experimental evaluation of NLL, ""estimated via the variational lower bound""; is this with the traditional VAE variational lower bound of Eq 1 ? Or with some later equation?
In several places, you specify + const. It would be useful to briefly afterwards state what that const is and that it is constant w.r.t. what, as this is not always clear.
There are a number of errors in some of the equations:
l 145: ""E_p(z) log p_\theta(z) = "" should be ""E_p(z) log q_\phi(z) = ""
Eq 10: I think $log q_\phi(x|z)$ should be $log q_\phi(z|x)$
Eq 11: has the term ordering (hence the sign) reversed in comparison to Eq 10. It should be changed to the same terms ordering as Eq 10 (to be consistent with Eq 10 and Eq. 12).
l 150,152: ""is not possible, as it requires an explicit form for"", not only that, log p_\theta(x) is intractable.",- the role and usefulness of Lemma 1 and Corollary 1.1 appears much later than where they are introduced.,621,0
NIPS_2017_369,NIPS_2017,"* (Primary concern) Paper is too dense and is not very easy to follow; multiple reads were required to grasp the concepts and contribution. I would strongly recommend simplifying the description and explaining the architecture and computations better; Figure 7, Section 8 as well as lines 39-64 can be reduced to gain more space.
* While the MNIST and CIFAR experiments is promising but they are not close to the state-of-art methods. It is not obvious if such explicitly dynamic routing is required to address the problem OR if recent advances such as residual units that have enabled significantly deeper networks can implicitly capture routing even with simple schemes such as a max-pooling. It would be good if authors can share their insights on this.",* While the MNIST and CIFAR experiments is promising but they are not close to the state-of-art methods. It is not obvious if such explicitly dynamic routing is required to address the problem OR if recent advances such as residual units that have enabled significantly deeper networks can implicitly capture routing even with simple schemes such as a max-pooling. It would be good if authors can share their insights on this.,622,0
NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.",6.L222: Why is the official test set of MSCOCO not used for reporting results?,623,0
NIPS_2017_74,NIPS_2017,"- Theorem 2 which presentation is problematic and does not really provide any convergence guaranty.
- All the linear convergence rates rely on Theorem 8 which is burried at the end of the appendix and which proof is not clear enough.
- Lower bounds on the number of good steps of each algorithm which are not really proved since they rely on an argument of the type ""it works the same as in another close setting"".
The numerical experiments are numerous and convincing, but I think that the authors should provide empirical evidences showing that the computational cost are of the same order of magnitude compared competing methods for the experiments they carried out.
%%%% Details on the main comments
%% Theorem 2
The presention and statement of Theorem 2 (and all the sublinear rates given in the paper) has the following form:
- Given a fixed horizon T
- Consider rho, a bound on the iterates x_0 ... x_T
- Then for all t > 0 the suboptimality is of the order of c / t where c depends on rho.
First, the proof cannot hold for all t > 0 but only for 0 < t <= T. Indeed, in the proof, equation (16) relies on the fact that the rho bound holds for x_t which is only ensured for t < = T.
Second the numerator actually contains rho^2. When T increases, rho could increase as well and the given bound does not even need to approach 0.
This presentation is problematic. One possible way to fix this would be to provide a priori conditions (such as coercivity) which ensure that the sequence of iterates remain in a compact set, allowing to define an upper bound independantly of the horizon T.
In the proof I did not understand the sentence ""The reason being that f is convex, therefore, for t > 0 we have f (x t ) < = f (0).""
%% Lemma 7 and Theorem 8
I could not understand Lemma 7.
The equation is given without any comment and I cannot understand its meaning without further explaination. Is this equation defining K'? Or is it the case that K' can be chosen to satisfy this equation? Does it have any other meaning?
Lemma 7 deals only with g-faces which are polytopes. Is it always the case? What happens if K is not a polytope? Can this be done without loss of generality? Is it just a typo?
Theorem 8:
The presentation is problematic. In Lemma 7, r is not a feasible direction. In Theorem 8, it is the gradient of f at x_t. Theorem 8 says ""using the notation from Lemma 7"". The proof of Theorem 8 says ""if r is a feasible direction"". All this makes the work of the reader very hard.
Notations of Lemma 7 are not properly used:
- What is e? e is not fixed by Lemma 7, it is just a variable defining a maximum. This is a recurent mistake in the proofs.
- What is K? K is supposed to be given in Lemma 7 but not in Theorem 8.
- Polytope?
All this could be more explicit.
""As x is not optimal by convexity we have that < r , e > > 0"". Where is it assumed that $x$ is not optimal? How does this translate in the proposed inequality?
What does the following mean?
""We then project r on the faces of cone(A) containing x until it is a feasible direction""
Do the author project on an intersection of faces or alternatively on each face or something else?
It would be more appropriate to say ""the projection is a feasible direction"" since r is fixed to be the gradient of f. It is very uncomfortable to have the value of r changing within the proof in an ""algorithmic fashion"" and makes it very hard to check accuracy of the arguments.
In any case, I suspect that the resulting r could be 0 in which case the next equation does not make sense. What prevents the resulting r from being null?
In the next sentences, the authors use Lemma 7 which assumes that r is not a feasible direction. This is contradictory with the preceeding paragraph. At this point I was completely confused and lost hope to understand the details of this proof.
What is r' on line 723 and in the preceeding equation?
I understand that there is a kind of recursive process in the proof. Why should the last sentence be true?
%% Further comments
Line 220, max should be argmax
I did not really understand the non-negative matrix facotrization experiment. Since the resulting approximation is of rank 10, does it mean that the authors ran their algorithm for 10 steps only?",- Theorem 2 which presentation is problematic and does not really provide any convergence guaranty.,624,0
NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?",- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?,625,0
NIPS_2017_122,NIPS_2017,"* It is not clear if the ability of the model to detect fall height is because of the absolute timing of the simulations. Falling from a greater height leads to a longer delay before the first impact. This is obvious to an algorithm analyzing fixed-sized wav files, but not to a human listening to sound files with somewhat unknown silent beginnings. A fairer comparison would be to add a random amount of delay before starting the sounds for both listeners.
* The comparison method is changed between the synthetic and real tasks, which seems unfair. If it is necessary to use a more complex comparison method for the real task, then also use it for the synthetic one.
* Line 226 reports several analysis parameters in samples, but never states the sample rate. Please describe these quantities in seconds or ms or provide the sample rate so the reader can perform the conversion themselves.
Overall, this is a strong paper that has gotten a relatively old and appealing idea to work much better than in the past.","* Line 226 reports several analysis parameters in samples, but never states the sample rate. Please describe these quantities in seconds or ms or provide the sample rate so the reader can perform the conversion themselves. Overall, this is a strong paper that has gotten a relatively old and appealing idea to work much better than in the past.",626,0
NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.",627,0
NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","3.L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.",628,0
NIPS_2017_143,NIPS_2017,"For me the main issue with this paper is that the relevance of the *specific* problem that they study -- maximizing the ""best response"" payoff (l127) on test data -- remains unclear. I don't see a substantial motivation in terms of a link to settings (real or theoretical) that are relevant:
- In which real scenarios is the objective given by the adverserial prediction accuracy they propose, in contrast to classical prediction accuracy?
- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?
- In l50 they claim that ""pershaps even in most [...] practical scenarios"" predicting accurate on the majority is most important. I contradict: in many areas with safety issues such as robotics and self-driving cars (generally: control), the models are allowed to have small errors, but by no means may have large errors (imagine a self-driving car to significantly overestimate the distance to the next car in 1% of the situations).
Related to this, in my view they fall short of what they claim as their contribution in the introduction and in l79-87:
- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).
- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).
- Related to this, in the experiments it would be interesting to see the comparison of the classical squared/absolute error on the test set as well (since this is what LSE claims to optimize).
- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet. REMARKS:
What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff?
Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y.","- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?",629,0
NIPS_2017_182,NIPS_2017,"Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).
Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.
[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. âModeling Relationships in Referential Expressions with Compositional Modular Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. âModeling Context Between Objects for Referring Expression Understanding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. âSubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.â In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378â86. Curran Associates, Inc.","1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.",630,0
NIPS_2017_130,NIPS_2017,"weakness)?
4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)?
5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data?
6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss.",6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss.,631,0
NIPS_2017_53,NIPS_2017,"--
-- The authors claim that their model is interpretable, but do not support this claim by any studies/analyses/qualitative examples. I agree that the ablation studies show the significance of each of the modules used in the proposed model, but it is not clear if the model is actually using the visual features from the correct module for a given question, e.g. for the question âDoes the woman look happy?â, is it really the face analysis module which provides the correct signal to accurately predict the answer? This can be studied by analysing (e.g. making a pie chart of) the questions whose predictions were incorrect when the face analysis module was absent but are correct after adding the face analysis module. Similar studies can be done for all the modules. Without any such analysis, I am not convinced if the model is actually doing what the motivation of the paper says. If the author can report any studies/analyses supporting this, I am happy to increase my rating.
-- The paper does not talk about the weaknesses of the proposed model. Discussions, ideally using failure cases, about limitations of the proposed model and what is needed to improve it are very important for continuous research in this area and should be an integral part of any paper.
-- I am confused about the mathematics of some equations. In Eq 1, how is the matrix multiplication of 2 matrices P and W resulting in a vector c? In Eq 3, what dimension is the max pooling happening over? In Eq 4, should it be transpose of r_{k} which is being premultiplied with tensor W_{k}?
-- In Table 2, the MCB numbers reported are test-dev numbers instead of test-standard. MCB only reports test-standard numbers for their ensemble model with data augmentation. Please fix.
-- For classification modules, have the authors tried using soft probability scores instead of picking top k class labels?
-- Minor:
- A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.
- r_{q} and r^{l} has been used interchangeably. At many places, subscripts and superscripts have been used interchangeably. Please fix.
- Line 9: âdoes not limitâ â âlimitsâ
After author rebuttal: I thank the authors for the additional analyses. The rebuttal addresses all my concerns. I think this paper introduces an interesting and novel approach for VQA, which would be useful to the VQA research community. Therefore, I recommend the paper for acceptance. I have changed my rating to 7.",- A lot of cited papers have been published. It might be better to cite the conference version instead of arXiv.,632,0
NIPS_2017_502,NIPS_2017,"Weakness]
- This paper is poorly written.
- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
- Sharing the style of citations and bullet items is confusing.
- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing.
- The descriptions of baseline models are unclear.
- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled. [Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results. [Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.
Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article.
In sum, as far as I am concerned this work makes a contribution but is insufficient.","- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.",633,0
NIPS_2017_560,NIPS_2017,"weakness, but this seems not to be a problem in most examples.
3. Equation 2.6 is wrong as written; as it does not make sense to divide by a vector. (easy to fix, but I surprised at the sloppiness here given that the paper well written overall).
4. Just for clarity, in eq 1.1, state clearly that F_\theta(x) is submodular in x for every \theta.
5. Can some nonconvex constraint sets which have an easy projection be handled as well?
6. What if the projection onto set K can be computed only approximately?","4. Just for clarity, in eq 1.1, state clearly that F_\theta(x) is submodular in x for every \theta.",634,0
NIPS_2017_631,NIPS_2017,"1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model â favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.
2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?
3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.
4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?
5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).
6.	The first two bullets about contributions (at the end of the intro) can be combined together.
7.	Other errors/typos:
a.	L14 and 15: repetition of word âimagineâ
b.	L42: missing reference
c.	L56: impact -> impacts
Post-rebuttal comments:
The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper.
However I still have few comments --
1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).
3. The citation for the MRN model (in the rebuttal) is incorrect. It should be -- @inproceedings{kim2016multimodal,
title={Multimodal residual learning for visual qa},
author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle={Advances in Neural Information Processing Systems}, pages={361--369}, year={2016} }
4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.","3. In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.",635,0
NIPS_2017_585,NIPS_2017,"weakness of the paper is in the experiments: there should be more complete comparisons in computation time, and comparisons with QMC-based methods of Yang et al (ICML2014). Without this the advantage of the proposed method remains unclear.
- The limitation of the obtained results:
The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive.
- Eq. (3):
What is $e_l$?
Corollaries 1, 2 and 3 and Theorem 4:
All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).
- The equation in Line 170:
What is $e_i$?
- Subsampled dense grid:
This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful.
- Reweighted grid quadrature:
(i) It looks that there is no theoretical guarantee with this method.
(ii) The approach reminds me of Bayesian quadrature, which essentially obtains the weights by minimizing the worst case error in the unit ball of an RKHS. I would like to look at comparison with this approach.
(iii) Would it be possible to derive a time complexity?
(iv) How do you chose the regularization parameter $\lambda$ in the case of the $\ell_1$ approach?
- Experiments in Section 5:
(i) The authors reported the results of computation time very briefly (320 secs vs. 384 seconds for 28800 features in MNIST and ""The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..."" in TIMIT). I do not they are not enough: the authors should report the results in the form of Tables, for example, varying the number of features.
(ii) There should be comparison with the QMC-based methods of Yang et al. (ICML2014, JMLR2016). It is not clear what is the advantage of the proposed method over the QMC-based methods.
(iii) There should be explanation on the settings of the MNIST and TIMIT classification tasks: what classifiers did you use, and how did you determine the hyper-parameters of these methods? At least such explantion should be included in the appendix.","- Subsampled dense grid: This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful.",636,0
NIPS_2017_217,NIPS_2017,"- The model seems to really require the final refinement step to achieve state-of-the-art performance.
- How does the size of the model (in terms of depth or number of parameters) compare to competing approaches? The authors mention that the model consists of 4 hourglass modules, but do not say how big each hourglass module is.
- There are some implementation details that are curious and will benefit from some intuition: for example, lines 158-160: why not just impose a pairwise relationship across all pairs of keypoints? the concept of anchor joints seems needlessly complex.",- The model seems to really require the final refinement step to achieve state-of-the-art performance.,637,0
NIPS_2017_480,NIPS_2017,"and limitations.
Other comments:
* Section 2.1: maybe itâs not necessary to introduce discounts and rewards at all, given that neither are used in the paper?
* Section 3.1: the method for finding the factors seems very brittle, and to rely on disentangled feature representations that are not noisy. Please discuss these limitations, and maybe hint at how factors could be found if the observations were a noisy sensory stream like vision.
* Line 192: freezing the partitioning in the first iteration seems like a risky choice that makes strong assumptions about the coverage of the initial data. At least discuss the limitations of this.
* Section 4: there is a mismatch between these options and the desired properties discussed in section 2.2: in particular, the proposed options are not âsubgoal optionsâ because their distribution over termination states strongly depends on the start states? Same for the Treasure Game.
* Line 218: explicitly define what the âgreedyâ baseline is.
* Figure 4: Comparing the greedy results between (b) and (c), it appears that whenever a key is obtained, the treasure is almost always found too, contrasting with the MCTS version that explores a lot of key-but-no-treasure states. Can you explain this?","* Section 4: there is a mismatch between these options and the desired properties discussed in section 2.2: in particular, the proposed options are not âsubgoal optionsâ because their distribution over termination states strongly depends on the start states? Same for the Treasure Game.",638,0
NIPS_2017_357,NIPS_2017,"- the manuscript is mainly a continuation of previous work on OT-based DA
- while the derivations are different, the conceptual difference is previous work is limited
- theoretical results and derivations are w.r.t. the loss function used for learning (e.g.
hinge loss), which is typically just a surrogate, while the real performance measure would
be 0/1 loss. This also makes it hard to compare the bounds to previous work that used 0-1 loss
- the theorem assumes a form of probabilistic Lipschitzness, which is not explored well.
Previous discrepancy-based DA theory does not need Prob.Lipschitzness and is more flexible
in this respect.
- the proved bound (Theorem 3.1) is not uniform w.r.t. the labeling function $f$. Therefore,
it does not suffice as a justification for the proposed minimization procedure.
- the experimental results do not show much better results than previous OT-based DA methods
- as the proposed method is essentially a repeated application of the previous work, I would have
hoped to see real-data experiments exploring this. Currently, performance after different number
of alternating steps is reported only in the supplemental material on synthetic data.
- the supplemental material feels rushed in some places. E.g. in the proof of Theorem 3.1, the
first inequality on page 4 seems incorrect (as the integral is w.r.t. a signed measure, not a
prob.distr.). I believe the proof can be fixed, though, because the relation holds without
absolute values, and it's not necessary to introduce these in (3) anyway.
- In the same proof, Equations (7)/(8) seem identical to (9)/(10)
questions to the authors:
- please comment if the effect of multiple BCD on real data is similar to the synthetic case ***************************
I read the author response and I am still in favor of accepting the work.","- the supplemental material feels rushed in some places. E.g. in the proof of Theorem 3.1, the first inequality on page 4 seems incorrect (as the integral is w.r.t. a signed measure, not a prob.distr.). I believe the proof can be fixed, though, because the relation holds without absolute values, and it's not necessary to introduce these in (3) anyway.",639,0
NIPS_2018_430,NIPS_2018,"- The authors approach is only applicable for problems that are small or medium scale. Truly large problems will overwhelm current LP-solvers. - The authors only applied their method on peculiar types of machine learning applications that were already used for testing boolean classifier generation. It is unclear whether the method could lead to progress in the direction of cleaner machine learning methods for standard machine learning tasks (e.g. MNIST). Questions: - How where the time limits in the inner and outer problem chosen? Did larger timeouts lead to better solutions? - It would be helpful to have an algorithmic writeup of the solution of the pricing problem. - SVM gave often good results on the datasets. Did you use a standard SVM that produced a linear classifier or a Kernel method? If the former is true, this would mean that the machine learning tasks where rather easy and it would be necessary to see results on more complicated problems where no good linear separator exists. Conclusion: I very much like the paper and strongly recommend its publication. The authors propose a theoretically well grounded approach to supervised classifier learning. While the number of problems that one can attack with the method is not so large, the theoretical (problem formulation) and practical (Dantzig-Wolfe solver) contribution can possibly serve as a starting point for further progress in this area of machine learning.",- It would be helpful to have an algorithmic writeup of the solution of the pricing problem.,640,0
NIPS_2018_92,NIPS_2018,"0. Novelty: While the motivation is quite different, the key schema of this submission is still quite close to [8]/[20], i.e., in each iteration, two domains ((a) and (b)) are sampled as meta-train (train set) and meta-test (validation set). This somewhat caps the novelty. 1. Presentation/Explanation: â Eq(2) seems like un-necessary âMathinessâ. Makes it look fancily Bayesian, but then the paper doesnât then proceed to actually do anything Bayesian. Sec 3.2 could be removed without affecting the contribution. â The key details of the method is explained in L134-168. However particularly Sec 3.3 L139-168 are rather densely/unclearly written and very hard to parse. If I wasnât very familiar with this topic and methodology, I would have little hope to understand what is going on. Fig 1(b) is too vague to be of any help deciphering this paragraph. Better to spend the extra space of Sec 3.2/Eq 2 explaining everything better. 2. Meta-Train/Train mismatch: Meta-learning setups normally strive to mimic the way in which the method will be used in practice. But here it seems there is a mis-alignment of train/test conditions for episodic meta-learning. The meta-learning is trained â1-1â: working on pairs of domains (a,b) \in Source Domains by training on domain a, and evaluating on domain b. However, the way in which the regulariser is actually used is âMany-1â, training on *all* the sources. IE: the Meta-Reg parameters are tuned to work with much less data than they are actually exposed to when used in practice. This may lead to sub-optimal outcomes. 3. Competitors: - It would have been good to have some other DG baselines to put the results, particularly on the sentiment dataset, in context. EG: Domain-adversarial, [A] CrossGrad (ICLRâ18), [B] CCSA, Doretto, ICCVâ17, [C] DICA, Muandet, ICMLâ13. - For Tab4, it would be helpful to have a baseline with conventional (\phi_i=constant) l1 regularisation. Randomly weighted l1 seems like a weird, and possibly bad baseline. 4. Intuition. While the method is intuitive and appealing at a high level, it would be nice with some more analysis about exactly how it works (i.e. beyond Fig 2), and what are the assumptions it needs. - EG: How works: For example is its effect via reducing overfitting per-se, or via leading the optimizer to find an equally good or better fit that is a more robust minima? (EG: Something like worse train loss vs same or better train loss compared to baseline?). - EG: Assumptions: Are there any assumptions? For example does it only make sense if all train+test domains used are independently sampled from the same distribution over domains, or is no such assumption necessary? Could one imagine a specific group of source vs target domains that are different in some (possibly pathological) way such that this method definitely goes wrong and exhibits ""negative transfer"" to the new domain? ( Providing a synthetic example to break it should not be a reason to reject, but rather a positive to clarify to readers the conditions under which the method is/isnât expected to work). 5. Related work. Although this paper is focused on DG. The mechanism is to learn a regularisation hyper parameter. It would therefore be good to explain how it relates to the growing body of related work on gradient-based hyper-parameter learning so readers can understand the connections. EG: [Gradient-based hyperparameter optimization through reversible learning. ICMLâ15; Forward and Reverse Gradient-Based Hyperparameter Optimization, NIPSâ17]. Other minor: - L155: âIt is important to note that dependence of \Phi of \theta comes from gradient steps in Eq 4â. Should it say dependence of \theta on \phi? Typo: Line 158, pictoral -> pictorial Fig.2 Description: regularizaton/ regualrizer -> regularization/regularizer Line 249, intial -> initial Fig 1 (b) âl stepsâ is visually similar to â1 stepsâ, maybe authors can use another symbol.","- For Tab4, it would be helpful to have a baseline with conventional (\phi_i=constant) l1 regularisation. Randomly weighted l1 seems like a weird, and possibly bad baseline.",641,0
NIPS_2018_591,NIPS_2018,"- In the evaluation, there is no comparison to simple baselines with no learning, like Genetic Algorithms, Hill Climbing, random search, etc. which makes it difficult to evaluate how much benefit does sophisticated learning-based approaches provide. - In the optimization tasks, the running time, computational budget, and the number of function evaluations should all be controlled across the various algorithms (at least approximately) so that the comparisons are fair. There is no mention of these factors in the paper. Since each optimization algorithm represents a trade-off between solution quality and time/computation/evaluation budget, it is not clear where on that trade-off curve each algorithm being compared is and whether they are even comparable. (If one only considers solution quality, even brute force search will do well.) This makes it hard to decide whether the results are interesting. Comments/Questions: - How are the potentially unstable learning dynamics of adversarial learning and RL handled? Insights into the challenges can be very useful. - Lines 171-172 mention using ""small"" positive and negative rewards to discourage infeasible actions. What value is determined to be ""small"" and how is it determined? - If the policy becomes deterministic, then it will always generate the same molecule. How is the policy kept stochastic so that it generates diverse molecules? - There is a missing reference in the Supplementary Material.",- How are the potentially unstable learning dynamics of adversarial learning and RL handled? Insights into the challenges can be very useful.,642,0
NIPS_2018_50,NIPS_2018,"weakness (albeit small) of the approach since it requires having solved (even approximately) NP-Hard problems. This is echoed in Neural Combinatorial Optimization with Reinforcement Learning (NCO), where the authors show that RL training clearly outperforms Supervised Learning training without requiring the need for costly solutions. - The S2V-DQN baseline is quite weak in my opinion: *) I take exception that S2V-DQN outperforms prior deep learning approaches such as NCO: the results that they report when reimplementing NCO are worse than those initially reported in NCO paper, they used a moving average baseline instead of a critic, they don't use the fact that NCO can sample many solutions as one etc... *) As correctly pointed out by the authors, S2V-DQN only outputs one solution while their approach considers multiple solutions. In comparison, NCO and Attention solves your TSP, approximately (which uses a Transformer-Pointer Network instead of an RNN Pointer Network) can output many solutions (when sampling) making them better baselines to the proposed approach. *) Attention solves your TSP, approximately does outperform prior learning-based approaches (and actually shows that the NCO approach outperforms S2V-DQN) This contrasts to the overall tone of the paper that their method clearly surpasses recent work (""Our algorithm convincingly outperforms recent work"" in the conclusion) - The authors should also cite recent machine learning / combinatorial optimization work such as NeuroSAT, Attention solves your TSP, Learning the Multiple Traveling Salesmen Problem with Permutation Invariant Pooling Networks, etc. ================================================= ) Random+GR+LS: The 12.7% performance of this baseline outlines how important it is to consider multiple solutions. 2) Relying on supervised data is a weakness: I misunderstood that the labels relied on solvers. I agree that this is not an issue if the labels are generated without a solver and the authors have shown strong generalization. 3) The S2V-DQN baseline is quite weak. Out of the considered learning based approaches (ie NCO, ASTSP, the authors' method and S2V-DQN), S2V-DQN is the only method that considers a single solution. NCO and ASTSP can easily consider multiple solutions (by sampling) but it looks like the authors use the greedy version ASTSP only (this is hard to tell from the rebuttal but given that ASTSP solves 0% while random search solves 12.7% this seems to be the case). 4) I also invite the authors to reconsider the following statements which are incorrect / misleading. (a) ""Most recently, Dai et al. [10] used reinforcement learning to train a deep Q-network (DQN) to incrementally construct solutions to graph-based NP-hard problems, and showed that this approach outperforms prior learning-based techniques."" - For tasks where the methods are comparable (and compared), Dai et al report worse results when reimplementing NCO than those initially reported in NCO paper. Careful reading indicates that they used a moving average baseline instead of a critic, they don't use the fact that NCO can sample many solutions as one, etc... This is also echoed in a figure from the independent ASTSP paper which present NCO and ASTSP as outperforming Dai et al. (b) ""First, we do not use reinforcement learning, which is known as a particularly challenging optimization problem. Rather, we show that very strong performance and generalization can be achieved with supervised learning, which benefits from well-understood and reliable solvers."" - Previous work in combinatorial optimization has shown that RL is better suited than supervised learning (at least for pointer network models). - Reliance on solvers is a weakness in general. (c) (rebuttal) ""RL-based approaches consider problems up to 100 variables"". This is not true (although the authors have indeed tackled larger problems that prior work). It also not clear why this statement is relevant to the comparison of RL vs supervised learning: I think it has more to do with the types of models being used rather than the training method. In summary, I have a significant presentation issue with this paper. I agree that % of solved instances is an important practical metric but it obviously greatly favors methods that consider multiple solutions. The paper does a poor job at outlining this fact and compares only to learning based methods that only consider a single solution (ie S2V-DQN). The fact that all other learning based are outperformed by the random search further demonstrates my point. Methods tend to be much closer when looking at the average independent set size metric. (The authors also didn't report this metric for ASTSP or Random+GR+LS in the rebuttal). Relying on search (ie trying multiple solutions) to get good results is not a weakness of the paper in itself and I actually think it's a promising direction to combine both classic search methods with machine learning. However comparing to learning-based methods that only consider a single solution when considering to a metric that favors multiple solutions can be misleading . Especially that these methods can easily be augmented by search (ie sampling in NCO and ASTSP). The other incorrect statements about related work (mistakenly saying that the RL-based approaches consider up to 100 variables, etc.) also don't play in favor of the work. If this wasn't for these issues, this would be a strong paper. I appreciate the thorough response of the authors and I have updated my score to 4.",2) Relying on supervised data is a weakness: I misunderstood that the labels relied on solvers. I agree that this is not an issue if the labels are generated without a solver and the authors have shown strong generalization.,643,1
NIPS_2018_185,NIPS_2018,"Weakness: ##The clarity of this paper is medium. Some important parts are vague or missing. 1) Temperature calibration: 1.a) It was not clear what is the procedure for temperature calibration. The paper only describes an equation, without mentioning how to apply it. Could the authors list the steps they took? 1.b) I had to read Guo 2017 to understand that T is optimized with respect to NLL on the validation set, and yet I am not sure the authors do the same. Is the temperature calibration is applied on the train set? The validation set (like Guo 2017)? The test set? 1.c) Guo clearly states that temperature calibration does not affect the prediction accuracy. This contradicts the results on Table 2 & 3, where DCN-T is worse than DCN. 1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes? 1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain? 2) Uncertainty Calibration From one point of view it looks like temperature calibration is independent of uncertainty calibration, with the regularization term H. However in lines 155-160 it appears that they are both are required to do uncertainty calibration. (2.a) This is confusing because the training regularization term (H) requires temperature calibration, yet temperature calibration is applied after training. Could the authors clarify this point? (2.b) Regarding H: Reducing the entropy, makes the predictions more confident. This is against the paper motivation to calibrate the networks since they are already over confident (lines 133-136). 3) Do the authors do uncertainty calibration on the (not-generalized) ZSL experiments (Table 2&3)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 4) Do the authors do temperature calibration on the generalized ZSL experiments (Table 4)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 5) The network structure: 5.a) Do the authors take the CNN image features as is, or do they incorporate an additional embedding layer? 5.b) What is the MLP architecture for embedding the semantic information? (number of layers / dimension / etc..) ##The paper ignores recent baselines from CVPR 2018 and CVPR 2017 (CVPR 2018 accepted papers were announced on March, and were available online). These baseline methods performance superceed the accuracy introduced in this paper. Some can be considered complementary to this work, but the paper canât simply ignore them. For example: Zhang, 2018: Zero-Shot Kernel Learning Xian, 2018: Feature Generating Networks for Zero-Shot Learning Arora, 2018: Generalized zero-shot learning via synthesized examples CVPR 2017: Zhang, 2017: Learning a Deep Embedding Model for Zero-Shot Learning ## Title/abstract/intro is overselling The authors state that they introduce a new deep calibration network architecture. However, their contributions are a novel regularization term, and a temperature calibration scheme that is applied after training. I wouldnât consider a softmax layer as a novel network architecture. Alternatively, I would suggest emphasizing a different perspective: The approach in the paper can be considered as more general, and can be potentially applied to any ZSL framework that outputs a probability distribution. For example: Atzmon 2018: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Ba 2015: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions Other comments: It will make the paper stronger if there was an analysis that provides support for the uncertainty calibration claims in the generalized ZSL case, which is the focus of this paper. Introduction could be improved: The intro only motivates why (G)ZSL is important, which is great for new audience, but there is no interesting information for ZSL community. It can be useful to describe the main ideas in the intro. Also, confidence vs uncertainty, were only defined on section 3, while it was used in the abstract / intro. This was confusing. Related work: It is worth to mention Transductive ZSL approaches, which use unlabeled test data during training, and then discriminate this work from the transductive setting. For example: Tsai, 2017: Learning robust visual-semantic embeddings. Fu 2015: Transductive Multi-view Zero-Shot Learning I couldnât understand the meaning on lines 159, 160. Lines 174-179. Point is not clear. Sounds redundant. Fig 1 is not clear. I understand the motivation, but I couldnât understand Fig 1.","1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes?",644,0
NIPS_2018_122,NIPS_2018,"- Figure 1 and 2 well motive this work, but in the main body of this paper I cannot see what happens to these figures after applying the proposed adversarial training. It is better to put together the images before and after applying your method in the same place. Figure 2 does not say anything about details (we can understand the very brief overview of the positions of the embeddings), and thus these figures could be smaller for better space usage. - For the LM and NMT models, did you use the technique to share word embedding and output softmax matrices as in [1]? The transformer model would do this, if the transformer implementations are based on the original paper. If so, your method affects not only the input word embeddings, but also the output softmax matrix, which is not a trivial side effect. This important point seems missing and not discussed. If the technique is not used, the strength of the proposed method is not fully realized, because the output word embeddings could still capture simple frequency information. [1] Inan et al., Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling, ICLR 2017. - There are no significance test or discussions about the significance of the score differences. - It is not clear how the BLEU improvement comes from the proposed method. Did you inspect whether rare words are more actively selected in the translations? Otherwise, it is not clear whether the expectations of the authors actually happened. - Line 131: The authors mention standard word embeddings like word2vec-based and glove-based embeddings, but recently subword-based embeddings are also used. For example, fasttex embeddings are aware of internal character n-gram information, which is helpful in capturing information about rare words. By inspecting the character n-grams, it is sometimes easy to understand rare words' brief properties. For example, in the case of ""Peking"", we can see the words start from a uppercase character and ends by the suffix ""ing"", etc. It makes this paper more solid to compare the proposed method with such character n-gram-based methods [2, 3]. [2] Bojanowski et al., Enriching Word Vectors with Subword Information, TACL. [3] Hashimoto et al., A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks, EMNLP 2017. *Minor comments: - Line 21: I think the statement ""Different from classic one-hot representation"" is not necessary, because anyway word embeddings are still based on such one-hot representations (i.e., the word indices). An embedding matrix is just a weight matrix for the one-hot representations. - Line 29: Word2vec is not a model, but a toolkit which implements Skipgram and CBOW with a few training options. - The results on Table 6 in the supplementary material could be enough to be tested on the dev set. Otherwise, there are too many test set results. * Additional comments after reading the author response Thank you for your kind reply to my comments and questions. I believe that the draft will be further improved in the camera-ready version. One additional suggestion is that the title seems to be too general. The term ""adversarial training"" has a wide range of meanings, so it would be better to include your contribution in the title; for example, ""Improving Word Embeddings by Frequency-based Adversarial Training"" or something.","- It is not clear how the BLEU improvement comes from the proposed method. Did you inspect whether rare words are more actively selected in the translations? Otherwise, it is not clear whether the expectations of the authors actually happened.",645,0
NIPS_2018_986,NIPS_2018,"The paper does not provide enough details on the game and various concepts in the game to justify the design of the proposed infrastructure. Due to the lack of sufficient details on the game to justify and inform the design of the deep network architecture, it is hard to convince readers why the approach works or does not work. The reproducibility is therefore affected. The take-away from the paper or generalization beyond StarCraft is also limited. Other general comments: - Tables and figures should appear near where they are mentioned, so Table 4 and Figure 4 should not be in the appendix. - Too many papers referenced are from non-peer reviewed websites, such as Arxiv and CoRR, weakening the paper in terms of argumentation. The authors are highly encouraged to replace these papers by their peer-reviewed counterparts - The selected baselines are too simple. They are all simple, rule-based, and myopic, with no attempt to learn anything from the provided training data, which the authorsâ architecture makes use of. It would be a fairer comparison should authors have tried to design a little more sophisticated baselines that incorporate some form of learning (via classical prediction models, for example). Other detailed comments: - Various statements about the game and how players behave/act in the game miss explanation and/or supporting reference * Page 1, line 31: âTop-level human players perform about 350 actions per minuteâ * Page 3, line 86: â... humans generally make forward predictions on a high level at a long time-scale; ...â * Page 6, line 240-241: âSince units often move very erratically...â: not really, units often move according to some path. This also illustrates why the selected baselines are too weak; they make no use of the training data to try to infer the path of the moving units. - Page 2, line 81: Full state is not defined properly. What are included in a full state? - Page 3, line 96: what are âwalk tilesâ? - Page 3, line 119: What is âthe faction of both playersâ? - (minor) Figure 4 does not print well in black and white - Page 3, line 131-132: The author refers to the g_op_b task that is introduced later in the paper. It would be better if the authors can describe such concepts in a section devoted to description of the games and related tasks of interest. - Page 4, line 139: âhâ is not defined yet. - Page 5, line 208-210: What game rules were used for this baseline? In general, how do game rules help? - Page 7, line 271-276: This part needs more elaboration on what modules are available in the bots, and how they can use the enhanced information to their advantage in a game. - Page 7, line 286-287: The reason to exclude Zerg is not convincing nor well supported externally. The author is encouraged to include a Zerg bot regardless, then in the discussion section, explain the result and performance accordingly.","- (minor) Figure 4 does not print well in black and white - Page 3, line 131-132: The author refers to the g_op_b task that is introduced later in the paper. It would be better if the authors can describe such concepts in a section devoted to description of the games and related tasks of interest.",646,0
NIPS_2018_283,NIPS_2018,"1) The implementation of l_Î¸1 and g_Î¸2 are stacked by multiple neural layers, which not naturally satisfy the local-Lipschitz-continuity in Proposition 1 (As it mentioned in line 136-137). The author proposed to smooth the mapping by add Gaussian noise to training data. But I donât see how this trick can guarantee the satisfaction. Thus, thereâs risk of non-converging iteration. 2) Why l_Î¸1 and g_Î¸2 satisfy Lipschitz constant L<1? The sentence âif the initial start S(0) surrounds â¦ sufficientlyâ in Proposition 1 is a little bit vague. 3) The proposed method stands on an important assumption that thereâs a one-to-one correspondence of caption sentence and temporal segment in video. This mainly relates to what data is used during experiments. Thus, it would be worthwhile to show some evidence why this assumption is valid. 4) During test time, a random initial point is needed.","4) During test time, a random initial point is needed.",647,0
NIPS_2018_172,NIPS_2018,"- L198-205: It is not clear how exactly the FSA is constructed. Do you select 3 labels out of all the visual labels (novel ones) for the image? Is this selection ever changed? How are synonyms determined? How is the FSA constructed to handle synonyms - do you add synonyms to all the disjunctions and create separate sets? The paper spends little time explaining details of it's major idea/contribution. This makes reproducibility hard, although the authors do say they will release code (L83). - A minor observation from Table 1: the in-domain score is highest when not using any out-of-domain data (row 1). Does this indicate that model capacity is limited because using out-of-domain data reduces in-domain (especially for CIDEr) performance. - A missing comparison from Table 2 is using LRCN+CBS with a ResNet-101 CNN. Moving from VGG16 to ResNet101 gives significant improvement for the NBT baseline, and I suspect it should do the same for LRCN. A comprehensive comparison would benefit the authors and future research. After rebuttal ------------------ I do not agree with the authors' assertion that ResNet-50 results for LRCN ""should be similar to ResNet-101"". I strongly suggest running the experiment instead of suggesting similarity. I will keep my rating unchanged.","- L198-205: It is not clear how exactly the FSA is constructed. Do you select 3 labels out of all the visual labels (novel ones) for the image? Is this selection ever changed? How are synonyms determined? How is the FSA constructed to handle synonyms - do you add synonyms to all the disjunctions and create separate sets? The paper spends little time explaining details of it's major idea/contribution. This makes reproducibility hard, although the authors do say they will release code (L83).",648,0
NIPS_2018_385,NIPS_2018,"- the approach inherits the disadvantages of wake-sleep. Thus, there is no clear objective optimized here and the method might even diverge. Furthermore, there is an additional approximation step which was not present in the original wake-sleep algorithm, namely the prediction of the required expectations/gradients. While the authors mention that the original wake-sleep algorithm has these drawbacks, they don't discuss them at all in their approach. - the motivation to fix VAEs' problem to learn deep models is addressed rather briefly in the experiments, by learning a model over binarized MNIST and 3 hidden sigmoid layers. This is not too convincing. Quality: The paper is technically sound, in that way that the choices made are clear and very convincing. However, as the approach is based in wake-sleep, it is clear that the approach is heuristic, in particular as it includes a further approximation (expectations/gradients used for learning). The consequences of this fact and weaknesses of the model should be discussed in a more open way and ideally be addressed in the experimental evaluation (e.g. showing fail cases, or discussing why there are none). Clarity: The paper is very well written and easy to follow. Originality: The approach presented is, to the best of my knowledge, novel, natural and elegant. Significance: Although the proposed is heuristic, it has a high potential to trigger further research in this direction. Summary: while the paper has its weaknesses (heuristic, which should be better discussed), its originality and potential inspiration for further work in this direction are the main reasons why I recommend an accept. *** EDIT *** I read the authors' reply and I'm rather happy with it. While their point is convincing that the approximation in DDC-HM can be made arbitrarily close, I'm still not sure if we can guarantee stable training. The authors claim that it can be shown that the algorithm approaches the vicinity of a stationary point of the likelihood, if the gradient approximation error is small. I think this might be a crucial point, but I don't fully understand if this implies convergences (rather than oscillatory behavior) and how small the error needs to get. In any case, the authors should include this result in their paper. And again, nice and refreshing work, I stick with my original rating.","- the approach inherits the disadvantages of wake-sleep. Thus, there is no clear objective optimized here and the method might even diverge. Furthermore, there is an additional approximation step which was not present in the original wake-sleep algorithm, namely the prediction of the required expectations/gradients. While the authors mention that the original wake-sleep algorithm has these drawbacks, they don't discuss them at all in their approach.",649,0
NIPS_2018_45,NIPS_2018,"---------- The main section of the paper (section 3) seems carelessly written. Some obvious weaknesses: - Algorithm 1 seems more confusing, than clarifying: a) Shouldn't the gradient step be taken in the direction of the gradient of the loss with respect to Theta? b) There is no description of the variables, most importantly X and f. It is better for the reader to define them in the algorithm than later in the text. Otherwise, the algorithm definition seems unnecessary. - Equation (1) is very unclear: a) Is the purpose to define a loss function or the optimization problem? It seems that it is mixing both. b) The optimization variable x is defined to be in R^n. Probably it is meant to be in R^k? c) The constraints notation (s.t. C(A, x)) is rather unusual. - It is briefly mentioned that an alternating direction method is used to solve the min-min problem. Which method? - The constraints in equation (2) are identical to the ones in equation (3). They can be mentioned as such to gain space. - In section 4.1, line 194, K = 10, presumably refers to the number of atoms in the dictionary, namely it should be a small k? The same holds for section 4.4, line 285. - In section 4.1, why is the regularizer coefficient gamma set to zero? Intuitively, structured sparcity should be particularly helpful in finding keypoint correspondences. What is the effect on the solution when gamma is larger than zero? The experimental section of the paper seems well written (with a few exceptions, see above). Nevertheless, the experiments in 4.2 and 4.3 each compare to only one existing work. In general, I can see the idea of the paper has merit, but the carelessness in the formulations in the main part and the lack of comparisons to other works make me hesitant to accept it as is at NIPS.",- The constraints in equation (2) are identical to the ones in equation (3). They can be mentioned as such to gain space.,650,0
NIPS_2018_599,NIPS_2018,"of the paper are that it claims universality and optimality fairly strongly in the beginning, but requires upper bounds on the gradient and the domain size up front. While this is not necessarily a major detriment the writing should make this abundantly clear in the beginning. Also, the experiments of the paper could be strengthened. Why the particular experimental settings are chosen and what they really say about the practicality of AcceleGrad could be improved. Furthermore, it isnât clear there is something particular novel in the analysis or how well known the open problem being addressed is. Nevertheless, this paper provides a very interesting algorithm that advances the state of the art for convex optimization and unifies the analysis of a number of techniques in a clever way. Consequently, it could make for a nice addition to the program. Specific comments: - 65: âare exploreâ --> âare exploredâ - 78: âwe assume to be givenâ --> âwe assume we are givenâ - 83: âNote that we allow to choose point outside Kâ â I would clarify this further. - 83: âWe also assume that the objective function I Gâ â throughout the paper or just here, I would be clear. - Figure 1 â what is rho and epsilon in the charts, would make this clearer. EDIT: Thank you for your thoughtful response. Thank you for clarifying and for agreeing to make certain changes. However, I believe novelty in analysis requires more than being the first to analyze a particular algorithm, it requires an argument of why previous analysis techniques might be stuck, a simple approach might fail, etc. I still think this paper could be a nice addition to the program, but a more concrete case would be need to raise my score much more.","- 83: âWe also assume that the objective function I Gâ â throughout the paper or just here, I would be clear.",651,0
NIPS_2018_707,NIPS_2018,"weakness of the paper is the lack of experimental comparison with the state of the art. The paper spends whole page explaining reasons why the presented approach might perform better under some circumstances, but there is no hard evidence at all. What is the reason not to perform an empirical comparison to the joint belief state approach and show the real impact of the claimed advantages and disadvantages? Since this is the main point of the paper, it should be clear when the new modification is useful. 3) Furthermore, there is an incorrect statement about the performance of the state of the art method. The paper claims that ""The evidence suggests that in the domain we tested on, using multi-valued states leads to better performance."" because the alternative approach ""was never shown to defeat prior top AIs"". This is simply incorrect. Lack of an experiment is not evidence for superiority of the method that performed the experiment without any comparison. 4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately. 5) As explained in 1), the presented method is quite heuristic. The algorithm does not actually play the blueprint strategy, only few values are used in the leaf states, which cannot cover the whole variety of the best response values. In order to assess whether the presented approach might be applicable also for other games, it would be very useful to evaluate it on some substantially different domains, besides poker. Clarity: The paper is well written and organized, and it is reasonably easy to understand. The impact of the key differences between the theoretic inspiration and the practical implementation should be explained more clearly. Originality: The presented method is a novel modification of continual resolving. The paper clearly explains the main distinction form the existing method. Significance: The presented method seems to substantially reduce the computational requirements of creating a strong poker bot. If this proofs to be the case also for some other imperfect information games, it would be a very significant advancement in creating algorithms for playing these games. Detailed comments: 190: I guess the index should be 1 339: I would not say MCCFR is currently the preferred solution method, since CFR+ does not work well with sampling 349: There is no evidence the presented method would work better in stratego. It would depend on the specific representation and how well would the NN generalize over the types of heuristics. Reaction to rebuttal: 1) The formulation of the formal statement should be clearer. Still, while you are using the BR values from the blueprint strategy in the computation, I do not see how the theory can give you any real bounds the way you use the algorithm. One way to get more realistic bounds would be to analyze the function approximation version and use error estimates from cross-valiadation. 2) I do not believe head-to-head evaluation makes too much sense because of well known intransitivity effects. However, since the key difference between your algorithm and DeepStack is the form of the used leaf evaluation function, it would certainly not take man-years to replace the evaluation function with the joint belief in your framework. It would be very interesting to see comparison of exploitability and other trade-offs on smaller games, where we can still compute it. 4) I meant the use of the example for save resolving. 5) There is no need for strong agents for some particular games to make rigorous evaluation of equilibrium solving algorithms. You can compute exploitability in sufficiently large games to evaluate how close your approach is to the equilibrium. Furthermore, there are many domain independent algorithms for approaximating equilibriua in these games you can compare to. Especially the small number of best response values necessary for the presented approach is something that would be very interesting to evaluate in other games. Line 339: I just meant that I consider CFR+ to be ""the preferred domain-independent method of solving imperfect-information games"", but it is not really important, it was a detailed comment.","1) The formulation of the formal statement should be clearer. Still, while you are using the BR values from the blueprint strategy in the computation, I do not see how the theory can give you any real bounds the way you use the algorithm. One way to get more realistic bounds would be to analyze the function approximation version and use error estimates from cross-valiadation.",652,0
NIPS_2018_120,NIPS_2018,"Writing - Primary MTL should be differentiated from alternative goals of MTL (such as improving the performance of all tasks or saving memory and computational cost by sharing computation in a single network) early on in the abstract and introduction - In the abstract, a point is made about residual connections in ROCK allowing auxiliary features to explicitly impact detection prediction. This needs to be contrasted with standard MTL where the impact is implicit. - The claim about ROCK handling missing annotations modalities is unclear. The MLT dataset needs to be described in Sec 4.3. - The introduction describes Transfer Learning (TL) and Fine-tuning (FT) as sequential MTL. I do not completely agree with this characterization. TL is a broader term for the phenomenon of learnings from one task benefitting another task. Fine-tuning is a sequential way of doing it. Standard MTL is a parallel means to the same end. - The first 2 paragraphs of introduction read like literature review instead of directly motivating ROCK and the problem that it solves. - Need to describe Flat MTL and include a diagram similar to Fig 2 that can be directly visually compared to ROCK. - Fig 1 is not consistent with Fig 2. Fig 2 shows one encoder-decoder per auxiliary task whereas Fig 1 shows a single shared encoder-decoder for multiple tasks. - L88-89 try to make the case that ROCK has similar complexity as the original model. I would be very surprised if this true because ROCK adds 8 conv layers, 3 pooling layers, and 1 fusion layer. Inference timings must be provided to make this claim. Weaknesses: Experiments - Ablation showing the contribution of each auxiliary task on object detection performance in ROCK as well as in standard MTL. This can be done by comparing the full model (DNS) with models where one task is dropped out (DS, NS, DN). - Cite MLT dataset in Table 2 caption and describe Geo, presumably some kind of geometric features. - More details are needed for reproducibility like backbone architecture, activations between conv layers, number of channels, etc. Weaknesses: Originality - While primary MTL setting considered in this work is very useful, especially in data-deficient domains, the main contribution of this work, the ROCK architecture, comes across as a little incremental. Summary My current assessment is that in spite of the weaknesses, the model is simple, reasonably well motivated and shows decent performance gains over appropriate baselines. Hence I am currently leaning towards an accept with a rating of ""6"". I am not providing a higher rating as of now because the writing can be improved in several places, an ablation is missing, and the novelty is limited. My confidence score if 4 because I have only briefly looked at [19] and [35] which seem to be the most relevant literature. --Final Review After Rebuttal-- The authors did present an ablation showing the effect of different tasks on multitask training as requested. A significant number of my comments were writing suggestions which I believe would improve the quality of the paper further. The authors have agreed to make appropriate modifications. In spite of some concern about novelty, I think this work is a valuable contribution towards an understanding of multitask learning. All reviewers seem to be more or less in agreement. Hence, I vote for an accept and have increased my rating to 7 (previously 6).","- L88-89 try to make the case that ROCK has similar complexity as the original model. I would be very surprised if this true because ROCK adds 8 conv layers, 3 pooling layers, and 1 fusion layer. Inference timings must be provided to make this claim. Weaknesses: Experiments - Ablation showing the contribution of each auxiliary task on object detection performance in ROCK as well as in standard MTL. This can be done by comparing the full model (DNS) with models where one task is dropped out (DS, NS, DN).",653,0
NIPS_2018_114,NIPS_2018,"1. Generalizability. In general, I think the authors need to show how this approach can work on more problems. For example, it looks to me that for most deep net problem A2 is not true. Also, some empirical verification of assumption A1 alone on other problems would be useful to convince me why this approach can generalize. 2. Stability evaluation/analysis is missing. How sensitive is the performance to the lingering radius (i.e. theta or equivalently delta(x, i))? Could the authors give some theoretical analysis or some empirical evaluation? 3. Memory consumption. For many real-world applications, the stochastic gradient methods mentioned in this paper are not acceptable due to huge memory consumption. Could the authors explain how to generalize this approach to other methods, e.g. stochastic gradient descent with fixed batch size? I would expect the growing number of lingering stochastic gradients to be an issue. Some Typos: L532: difference --> different L540: report report --> report ------- Response to Author's feedback: 1. For A1, I agree that if we have explicit written f_i(x) then we can compute radius in a easy way. My original concern is when the function is too complicated that the radius do not have a easy close form, then can we at least empirically evaluate the radius. I guess if the authors want to focus outside DL then this might not be a big issue anymore. 2. I think my concern is no longer a issue if the function can be written explicitly. 3. Originally, I was imagining a deep net setting, where storing O(nd) is not acceptable. And I have concerns about the overhead of computing this on the fly. But I guess it's not a problem whenever SVRG is acceptable.","2. Stability evaluation/analysis is missing. How sensitive is the performance to the lingering radius (i.e. theta or equivalently delta(x, i))? Could the authors give some theoretical analysis or some empirical evaluation?",654,0
NIPS_2018_55,NIPS_2018,"weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved. 1) The statements in the MNIST experiment such as ""While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable."" are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims. 2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant. 3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"". 4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions: I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence. line 218: an CAE -> a CAE line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.","3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"".",655,0
NIPS_2018_710,NIPS_2018,"- My general reservation about this paper is that while it was helpful in clarifying my own understanding of BN, a lot of the conclusions are consistent with folk wisdom understanding of BN (e.g. well-conditioned optimization), and the experimental results were not particularly surprising. Questions: - Taking Sharp Minima Hypothesis at face value, Eq 2 suggests that increasing gradient variance improves generalization. This is consistent with the theme that decreasing LR or decreasing minibatch size make generalization worse. Can you comment on how to reconcile this claim with the body of work in black-box optimization (REBAR, RELAX, VIMCO, Reinforcement Learning) suggesting that *reducing* variance of gradient estimation improves generalization & final performance? - Let's suppose that higher SGD variance (eq 2) == better generalization. BN decreases intra-unit gradient variance (Fig 2, left) but increases intra-minibatch variance (Fig 4, right). When it is applied to a network that converges for some pair \alpha and B, it seems to generalize slightly worse (Fig 1, right). According to the explanations presented by this paper, this would imply that BN decreased M slightly. For what unnormalized architectures does the application of BN increase SGD variance, and for what unnormalized architectures does BN actually decrease SGD variance? (requiring LR to be increased to compensate?) How do inter-layer gradient variance and inter-minibatch gradient variance impact on generalization? - For an unnormalized network, is it possible to converge AND generalize well by simply using a small learning rate with a small batch size? Does this perform comparably to using batch norm? - While Section 4 was well-written, the argument that BN decreases exponential condition number is not new; this situation has been analyzed in the case of training deep networks using orthogonal weights (https://arxiv.org/pdf/1511.06464.pdf, https://arxiv.org/pdf/1806.05393.pdf), and the exploding / vanishing gradient problem (Hochreiter et al.). On the subject of novelty, does this paper make a stronger claim than existing folk wisdom that BN makes optimization well-conditioned?","- While Section 4 was well-written, the argument that BN decreases exponential condition number is not new; this situation has been analyzed in the case of training deep networks using orthogonal weights (https://arxiv.org/pdf/1511.06464.pdf, https://arxiv.org/pdf/1806.05393.pdf), and the exploding / vanishing gradient problem (Hochreiter et al.). On the subject of novelty, does this paper make a stronger claim than existing folk wisdom that BN makes optimization well-conditioned?",656,0
NIPS_2018_149,NIPS_2018,"- While the proposed method outperforms certain restricted variants of DeepLab V2 [7], it performs significantly worse than the full DeepLab V2 on both of the two datasets used for comparisons. - The experimental results show that the proposed method outperforms a restricted variant of DeepLab V2 that does not include the MSC, COCO, and CRF improvements (that are also not used with the proposed method). However, the proposed method has worse accuracy than the full DeepLab V2 approach, with a particularly large accuracy gap on the Pascal VOC 2012 dataset. The authors should give results for the proposed method in combination with the MSC, COCO, and CRF improvements. - The proposed method is very similar to two prior works cited by the authors [3, 5]. The paper should more clearly compare the proposed method to these prior works, and include them for comparison in the tables of experimental results. - The proposed method seems to have lower accuracy than the two very similar prior works [3] and [5], and also seems to offer no advantage in terms of computational efficiency over [5]. - The paper would benefit from proofreading of the grammar. Response to author rebuttal: I thank the authors for clarifying the computational complexity of the diffusion step, specifically the fact that it is done with the features downsampled to 64x64; this addresses my confusion regarding the reported computation times. I appreciate the difficulty of reproducing and comparing against [3] and [5] without source code, and that the that they may also be more difficult to train than the proposed method. While a more robust method is a valuable contribution even if it does not provider better accuracy or speed, I think a more thorough investigation than is described in the paper would be needed to show that. Regarding the lack of evaluation with the DeepLab MSC, COCO and CRF steps, while the authors make the valid point that the MSC step and especially the CRF step increase the computational complexity, and that the COCO pretraining adds training data and therefore it may not be fair to compare COCO-pretrained models to those without it, evaluations including these steps are nonetheless important for properly comparing this method to other methods, given the existing similar work.","- The proposed method seems to have lower accuracy than the two very similar prior works [3] and [5], and also seems to offer no advantage in terms of computational efficiency over [5].",657,0
NIPS_2018_681,NIPS_2018,"Weakness: However, I'm not very convinced with experimental results and I a bit doubt that this method would work in general and is useful in any sense. 1. The authors propose a new classification network, but I a bit doubt that its classification error is universally as good as the standard softmax network. It is a bit dangerous to build a new model for better detecting out-of-distribution samples, while losing its classification accuracy. Could the authors report the classification accuracy of the proposed classifier on ImageNet data? Some theoretical justifications, if possible, would be great for the issue. 2. The detection procedure (i.e., measuring the norm of the predicted embedding) is not intuitive and I am not convinced why it is expected to work. Could the authors provide more detailed explanations about it? 3. The baselines to compare are not enough, e.g., compare the proposed method with LID [1] which is one of the state-of-the-art detection methods for detecting adversarial samples. [1] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J., Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018 4. Similar to Section 4.3, it is better to report AUROC and detection error when the authors evaluate their methods for detecting adversarial samples.","3. The baselines to compare are not enough, e.g., compare the proposed method with LID [1] which is one of the state-of-the-art detection methods for detecting adversarial samples. [1] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J., Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018 4. Similar to Section 4.3, it is better to report AUROC and detection error when the authors evaluate their methods for detecting adversarial samples.",658,0
NIPS_2018_120,NIPS_2018,"Writing - Primary MTL should be differentiated from alternative goals of MTL (such as improving the performance of all tasks or saving memory and computational cost by sharing computation in a single network) early on in the abstract and introduction - In the abstract, a point is made about residual connections in ROCK allowing auxiliary features to explicitly impact detection prediction. This needs to be contrasted with standard MTL where the impact is implicit. - The claim about ROCK handling missing annotations modalities is unclear. The MLT dataset needs to be described in Sec 4.3. - The introduction describes Transfer Learning (TL) and Fine-tuning (FT) as sequential MTL. I do not completely agree with this characterization. TL is a broader term for the phenomenon of learnings from one task benefitting another task. Fine-tuning is a sequential way of doing it. Standard MTL is a parallel means to the same end. - The first 2 paragraphs of introduction read like literature review instead of directly motivating ROCK and the problem that it solves. - Need to describe Flat MTL and include a diagram similar to Fig 2 that can be directly visually compared to ROCK. - Fig 1 is not consistent with Fig 2. Fig 2 shows one encoder-decoder per auxiliary task whereas Fig 1 shows a single shared encoder-decoder for multiple tasks. - L88-89 try to make the case that ROCK has similar complexity as the original model. I would be very surprised if this true because ROCK adds 8 conv layers, 3 pooling layers, and 1 fusion layer. Inference timings must be provided to make this claim. Weaknesses: Experiments - Ablation showing the contribution of each auxiliary task on object detection performance in ROCK as well as in standard MTL. This can be done by comparing the full model (DNS) with models where one task is dropped out (DS, NS, DN). - Cite MLT dataset in Table 2 caption and describe Geo, presumably some kind of geometric features. - More details are needed for reproducibility like backbone architecture, activations between conv layers, number of channels, etc. Weaknesses: Originality - While primary MTL setting considered in this work is very useful, especially in data-deficient domains, the main contribution of this work, the ROCK architecture, comes across as a little incremental. Summary My current assessment is that in spite of the weaknesses, the model is simple, reasonably well motivated and shows decent performance gains over appropriate baselines. Hence I am currently leaning towards an accept with a rating of ""6"". I am not providing a higher rating as of now because the writing can be improved in several places, an ablation is missing, and the novelty is limited. My confidence score if 4 because I have only briefly looked at [19] and [35] which seem to be the most relevant literature. --Final Review After Rebuttal-- The authors did present an ablation showing the effect of different tasks on multitask training as requested. A significant number of my comments were writing suggestions which I believe would improve the quality of the paper further. The authors have agreed to make appropriate modifications. In spite of some concern about novelty, I think this work is a valuable contribution towards an understanding of multitask learning. All reviewers seem to be more or less in agreement. Hence, I vote for an accept and have increased my rating to 7 (previously 6).","- Cite MLT dataset in Table 2 caption and describe Geo, presumably some kind of geometric features.",659,0
NIPS_2018_15,NIPS_2018,"weakness of this paper is its lack of clarity and aspects of the experimental evaluation. The ResNet baseline seems to be just as good, with no signs of overfitting. The complexity added to the hGRU model is not well motivated and better baselines could be chosen. What follows is a list 10 specific details that we would like to highlight, in no particular order: 1. Formatting: is this the original NIPS style? Spacing regarding sections titles, figures, and tables seem to deviate from the template. But we may be wrong. 2. The general process is still not 100% clear to us. The GRU, or RNNs in general, are applied to sequences. But unlike other RNNs applied to image classification which iterate over the pixels/spatial dimensions, the proposed model seems to iterate over a sequence of the same image. Is this correct? 2.1 Comment: The general high-level motivation seems to be map reading (see fig 1.c) but this is an inherently sequential problem to which we would apply sequential models so it seems odd that one would compare to pure CNNs in the first place. 3. Section 2 begins with a review of the GRU. But what follows doesn't seem to be the GRU of [17]. Compare eq.1 in the paper and eq.5 in [7]. a) there doesn't seem to be a trained transformation on the sequence input x_i and b) the model convolves the hidden state, which the standard GRU doesn't do (and afaik the convolution is usually done on the input stream, not on the hidden state). c) Since the authors extend the GRU we think it would make section 2 much more readable if they used the same/similar nomenclature and variable names. E.g., there are large variations of H which all mean different things. This makes it difficult to read. 4. It is not clear what horizontal connections are. One the one hand, it seems to be an essential part of the model, on the other hand, GRU is introduced as a method of learning horizontal connections. While the term certainly carries a lot of meaning in the neuroscience context, it is not clear to us what it means in the context of an RNN model. 5. What is a feed forward drive? The equations seem to indicate that is the input at every sequence step but the latter part of the sentence describes it as coming from a previous convolutional layer. 6. The dimensions of the tensors involved in the convolution don't seem to match. The convolution in a ConvNet is usually a 2D discrete convolution over the 2 spatial dimensions. If the image is WxHxC (width, height, and, e.g., the 3 colour channels), and one kernel is 1x1xC (line 77) then we believe the resulting volume should be WxHx1 and the bias is a scalar. The authors most certainly want to have several kernels and therefore several biases but we only found this hyper-parameter for the feed forward models that are described in section 3.4. The fact that they have C biases is confusing. 7. Looking very closely at the diagram, it seems that the ResNet architectures are as good if not even slightly better than the hGRU. Numerical measurements would probably help, but that is a minor issue. It's just that the authors claim that ""neural networks and their extensions"" struggle in those tasks. Since we may include ResNets in that definition, their own experiment would refute that claim. The fact that the hGRU is using many fewer parameters is indeed interesting but the ResNet is also a more general model and there is (surprisingly) no sign of overfitting due to a large model. So what is the motivation of the authors of having fewer parameters? 8. Given the fact that ResNets perform so well on this task, why didn't the authors consider the earlier and closely related highway (HW) networks [high1]? HWs use a gating mechanism which is inspired by the LSTM architecture, but for images. Resnets are a special case of HW, that is, HW might make an even stronger baseline as it would also allow for a mix and gain-like computation, unlike ResNets. 9. In general, the hGRU is quite a bit more complex than the GRU. How does it compare to a double layer GRU? Since the hGRU also introduces a two-layer like cell (inhibiton part is seperated by a nonlinearity from the exhibition part) it seems unfair to compare to the GRU with fewer layers (and therefore smaller model complexity) 10. Can the authors elaborate on the motivation behind using the scalars in eq 8-11? And why are they k-dimensional? What is k? 11. Related work: The authors focus on GRU, very similar to LSTM with recurrent forget gates [lstm2], but GRU cannot learn to count [gru2] or to solve context-free languages [gru2] and also does not work as well for translation [gru3]. So why not use ""horizontal LSTM"" instead of ""horizontal GRU""? Did the authors try? What is the difference to PyramidLSTM [lstm3], the basis of PixelRNNs? Why no comparison? Authors compare against ResNets, a special case of the earlier highway nets [high1]. What about comparing to highway nets? See point 8 above. [gru2] Weiss et al. On the Practical Computational Power of Finite Precision RNNs for Language Recognition. Preprint arXiv:1805.04908. [gru3] Britz et al (2017). Massive Exploration of Neural Machine Translation Architectures. Preprint arXiv:1703.03906 [lstm2] Gers et al. âLearning to Forget: Continual Prediction with LSTM.â Neural Computation, 12(10):2451-2471, 2000. [lstm3] Stollenga et al. Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation. NIPS 2015. Preprint: arxiv:1506.07452, June 2015. [high1] Srivastava et al. Highway networks. Preprints arXiv:1505.00387 (May 2015) and arXiv:1507.06228 (Jul 2015). Also at NIPS'2015. After the rebuttal phase, this review was edited by adding the following text: Thanks for the author feedback. However, we remain unconvinced. The baseline methods used for performance comparisons (on a problem on which few compete) are not the state of the art methods for such tasks - partially because they throw away spatial information the deeper they get, while shallower layers cannot connect the dots (literally) due to the restricted field of view. Why don't the authors compare to a state of the art baseline method that can deal with arbitrary distances between pixels - standard CNNs cannot, but the good old multi-dimensional (MD) RNN can (https://arxiv.org/abs/0705.2011). For each pixel, a 2D-RNN implicitly uses the entire image as a spatial context (and a 3D-RNN uses an entire video as context). A 2D-RNN should be a natural competitor on this simple long range 2D task. The RNN is usually LSTM (such as 2D-LSTM) but could be something else. See also MD-RNN speedups through parallelization (https://arxiv.org/abs/1506.07452). The submission, however, seems to indicate that the authors donât even fully understand multi-dimensional RNNs, writing instead about ""images transformed into one-dimensional sequencesâ in this context, although the point of MD-RNNs is exactly the opposite. Note that an MD-RNN in general does have local spatial organization, like the model of the authors. For any given pixel, a 2D-RNN sees this pixel plus the internal 2D-RNN states corresponding to neighbouring pixels (which already may represent info about lots of other pixels farther away). Thatâs how the 2D-RNN can recursively infer long range information despite its local 2D spatial neighbourhood wiring. So any good old MD-RNN is in fact strongly spatially organised, and in that sense even biologically plausible to some extent, AFAIK at least as plausible as the system in the present submission. The authors basically propose an alternative local 2D spatial neighbourhood wiring, which should be experimentally compared to older wirings of that type. And to our limited knowledge of biology, it is not possible to reject one of those 2D wirings based on evidence from neuroscience - as far as we can judge, the older 2D-RNN wiring is just as compatible with neurophysiological evidence as the new proposal. Since the authors talk about GRU: they could have used a 2D-GRU as a 2D-RNN baseline, instead of their more limited feedforward baseline methods. GRU, however, is a variant of the vanilla LSTM by Gers et al 2000, but lacking one gate, thatâs why it has those problems with counting and with recognising languages. Since the task might require counting, the best baseline method might be a 2D-LSTM, which was already shown to work on challenging related problems such as brain image segmentation where the long range context is important (https://arxiv.org/abs/1506.07452), while I donât know of similar 2D-GRU successes. We also agree with the AC regarding negative weights. Despite some motivation/wording that might appeal to neuroscientists, the proposed architecture is a standard ML model that has been tweaked to work on this specific problem. So it should be compared to the most appropriate alternative ML models (in that case 2D-RNNs). For now, this is a Machine Learning paper slightly disguised as a Computational Neuroscience paper. Anyway, the paper has even more important drawbacks than the baseline dispute. Lack of clarity still makes it hard to re-implement and reproduce, and a lot of complexity is added which is not well motivated or empirically evaluated through, say, an ablation study. Nevertheless, we encourage the authors to produce a major revision of this interesting work and re-submit again to the next conference!",5. What is a feed forward drive? The equations seem to indicate that is the input at every sequence step but the latter part of the sentence describes it as coming from a previous convolutional layer.,660,0
NIPS_2018_553,NIPS_2018,"- This paper misses a few details in model design and experiments: A major issue is the ""GTA"" / ""DET"" feature representation in Table 1. As stated in section 4.1, image regions are extracted from ground-truth / detection methods. But what is the feature extractor used on top of those image regions? Comparing resnet / densenet extracted features with vgg / googlenet feature is not fair. - The presentation of this paper can be further improved. E.g. paragraph 2 in intro section is a bit verbose. Also breaking down overly-long sentences into shorter but concise ones will improve fluency. Some additional comments: - Figure 3: class semantic feature should be labeled as ""s"" instead of ""c""? - equation 1: how v_G is fused from V_I? please specify. - equation 5: s is coming from textual representations (attribute / word to vec / PCA'ed TFIDF). It might have positive / negative values? However the first term h(W_{G,S}, v_G) is post ReLU and can only be non-negative? - line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help? - line 170: class semantic information is [not directly] embedded into the network? - Equation 11: v_s and u_G are both outputs from trained-network, and they are not normalized? So minimize L-2 loss could be simply reducing the magnitude of both vectors? - Line 201: the dimensionality of each region is 512: using which feature extractor? - Section 4.2.2: comparing number of attention layers is a good experiment. Another baseline could be not using Loss_G? So attention is only guided by global feature vector. - Table 4: what are the visual / textual representations used in each method? otherwise it is unclear whether the end-to-end performance gain is due to proposed attention model.",- equation 1: how v_G is fused from V_I? please specify.,661,0
NIPS_2018_761,NIPS_2018,"Weakness] * How to set the parameter S remains a problem. * Algorithm SMILE is interesting but their theoretical results on its performance is not easy to interpret. * No performance comparison with existing algorithms [Recommendation] I recommend this paper to be evaluated as ""a good submission; an accept"". Their problem formalization is clear, and SMILE algorithm and its theoretical results are interesting. All their analyses are asymptotically evaluated, so I worry about how large the constant factors are. It would make this manuscript more valuable if how good their algorithms (OOMM & SMILE) would be shown theoretically and empirically compared to other existing algorithms. [Detailed Comments] p.7 Th 3 & Cor 1: C^G and C^B look random variables. If it is true, then they should not be used as parameters of T's order. Maybe the authors want to use their upper bounds shown above instead of them. p.7 Sec. 5 : Write the values of parameter S. [Comments to Authorsâ Feedback] Setting parameter S: Asymptotic relation shown in Th 3 is a relation between two functions. It is impossible to estimate S from estimated M for a specific n using such a asymptotic functional relation.",* Algorithm SMILE is interesting but their theoretical results on its performance is not easy to interpret.,662,0
NIPS_2018_583,NIPS_2018,"Weakness: - (4) or (5) are nonconvex saddle point problems, there is no convergence guarantee for Alg 1. Moreover, as a subroutine for (7), it is not clear how many iterations (the hyperparameter n) should be taken to make sure (7) is convergent. Previously in structured SVM, people noticed that approximate inference could make the learning diverges. - Performance boost due to more parameters? In Tab 1,2,3, if we think carefully, LinearTop and NLTop adds additional parameters, while Unary performs much worse comparing to the numbers reported e.g. in [14], where they used a different and probably better neural network. This raises a question: if we use a better Unary baseline, is there still a performance boost? - In Table 1, the accuracies are extremely poor: testing accuracy = 0.0? Something must be wrong in this experiment. - Scalability: since H(x,c) outputs the whole potential vector with length O(K^m), where m is the cardinality of the largest factor, which could be extremely long to be an input for T. - The performance of NLTop is way behind the Oracle (which uses GT as input for T). Does this indicate (3) is poorly solved or because of the learning itself? [*] N Komodakis Efficient training for pairwise or higher order CRFs via dual decompositio. CVPR 2011. [**] D Sontag et al. Learning efficiently with approximate inference via dual losses. ICML 2010.","- (4) or (5) are nonconvex saddle point problems, there is no convergence guarantee for Alg 1. Moreover, as a subroutine for (7), it is not clear how many iterations (the hyperparameter n) should be taken to make sure (7) is convergent. Previously in structured SVM, people noticed that approximate inference could make the learning diverges.",663,0
NIPS_2018_821,NIPS_2018,"1. A few parts of this paper are not very clear or not sufficiently provided, such as the model details. Section 4.3 should be addressed more to make it clearer since some concepts/statements are misleading or confusing. 2. The trace to code model is complex, which requires as many LSTMs as the number of input/output pairs, and it may be hard to be applied to other program synthesis scenarios. 3. Apart from the DSL, more experiments on dominant PL (e.g., Python) would be appreciated by people in this research field. Details are described as below: 1. For a given program, how to generate diverse execution traces that can be captured by the I/O -> Trace model? Since execution traces are generated by running the program on N I/O pairs, it is possible that some execution traces have a large overlap. For example, in the extreme case, two execution traces may be the same (or very similar) given different I/O pairs. 2. The authors involve the program interpreter in their approach, which is a good trial and it should help enhance the performance. However, I am curious about is it easy to be integrated with the neural network during training and testing? 3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated. 4. Line 183-line 184, is it necessary to use embedding for only four conditionals (of Boolean type)? only 16 possible combinations. 5. As depicted in Figure 3, if more I/O pairs are provided, the Trace->Code should be very complex since each i/o example requires such an LSTM model. How to solve this issue? 6. In essence, the Trace->Code structure is a Sequence to Sequence model with attention, the only differences are the employment of I/O pair embedding and the max pooling on multiple LSTM. How are the I/O pair embeddings integrated into the computation? Some supplementary information should be provided. 7. It is interesting to find that model trained on gold traces perform poorly on inferred traces, the authors do not give a convincing explanation. More exploration should be conducted for this part. 8. It would be better if some synthesized program samples are introduced in an appendix or other supplementary documents.","1. For a given program, how to generate diverse execution traces that can be captured by the I/O -> Trace model? Since execution traces are generated by running the program on N I/O pairs, it is possible that some execution traces have a large overlap. For example, in the extreme case, two execution traces may be the same (or very similar) given different I/O pairs.",664,0
NIPS_2018_31,NIPS_2018,"- Since it is difficult to evaluate image generation tasks quantitatively, a human study could be performed where users need to select their preferred generated image. This evaluation could help showing the increased visual quality with respect to other methods. - Since Equation 5 is applied multiple times it should contain some notion of iterations (e.q. M_w^{(i)}. Otherwise M_w appears on both sides of the equation. - An interesting evaluation could be correlating reconstruction quality and RS measure. This could show that the loss actually helps finding the right correspondences between patches. Minor Comments - L. 82: {1, 2, â¦, n} - The type of upsampling for the feature maps before concatenation is not mentioned. - Since the ID-MRF is only used in training it would be interesting to report training times or even time per image during training compared to inference. - Potential typo in Equation (4). L_M(conv4_2) appears twice. - The intuitive example for L_M from the supplementary material could be included in the paper. I found it easier to follow than the explanations in lines 117-128. - L. 224: our method is still difficult to deal with large-scale -> still has difficulties dealing with [â¦] Rating & Evaluation Given the above strengths and weaknesses I am in favor of accepting this submission. A user study and a more in-depth analysis of the proposed MRF loss could make this paper even stronger. -- After reading the other reviews and the rebuttal I find most of my concerns addressed. The authors provide several additional quantitative results including a user study with convincing outcome. I vote in favor of accepting the paper.","- Since it is difficult to evaluate image generation tasks quantitatively, a human study could be performed where users need to select their preferred generated image. This evaluation could help showing the increased visual quality with respect to other methods.",665,0
NIPS_2018_100,NIPS_2018,"Weakness - We know the traditional KNN has obvious limitations â slow and memory inefficient. While this paperâs main novelty is introducing a KNN relaxation, could you also discuss on the computational cost incurred by using N3 block, and compare with KNN block? - In the ablation study, a more fair comparison to demonstrate the effectiveness of your individual components is to add 2 Ã DnCNN (d = 17), KNN block (k = 7) and 2 Ã DnCNN (d = 17), N3 block (k = 7). An alternative is to run a 1 Ã DnCNN (d = 6) as baseline, and then do 2 Ã DnCNN (d = 6), N3 block (k = 7). In this way, it is cleaner to see how much improvement that one KNN or N3 block brings. - Minor point: please give more clues on the choice of hyper-parameters, such as k (just like KNN is sensitive to K choice, is here the same?)","- Minor point: please give more clues on the choice of hyper-parameters, such as k (just like KNN is sensitive to K choice, is here the same?)",666,0
NIPS_2018_1005,NIPS_2018,"- It is obvious that there is a strong connection between PCA and linear regression (see for example [41]), thus the originality of the general idea is limited. - The spiked covariance model for PCA is quite restrictive (for example, Gaussian white noise) and unlikely to be met in practical applications. In fact Gaussianity is crucial for this approach, which makes the results less significant. - The assumption that the number of nonzero components of u are known is strong (though the authors argue that there are adaptive methods to adjust this). - It is not clear how easy it is to check / verify for a particular problem and SLR method that Condition 2.4 holds. It looks like a strong assumptions which is hard to verify. Ideally, the authors should have demonstrated verifying this condition on an example. - The fact that the underlying SLR method is treated as a black-box might hide the problem of selecting the appropriate SLR method. Which one should be used for various problems? Minor comments: - The authors argue that though random design matrices for linear regression can arise, it makes no difference to assume that it is deterministic (by conditioning). This argument is a bit misleading, as it is only true if the design matrix (\mathbb{X}) and the noise vector (w) affecting the observations are independent. This is not the case, for example, if the linear regression problem arises from a time-series problem, such as estimating the parameters of an autoregressive model (in which case the design matrix cannot be assumed to be deterministic). - It is not a good practice to cite works that are only available on arXiv (as they did not go through any review process, they could contain unsupported claims, etc.). Post-rebuttal comments: Thank you for your replies. It is a nice feature that the presented approach can turn a black-box SLR to a SPCA solver. Nevertheless, it requires strong assumptions, for example, the spiked covariance model, knowledge of sparsity, and RE, which limit its theoretical and practical relevance. I understand that these assumptions could be relaxed a bit, for example, the method could be extended to sub-Gaussian variables and unknown sparsity could be handled by binary search (for hypothesis testing). It would be good to discuss these issues in more detail in the revised paper.","- The authors argue that though random design matrices for linear regression can arise, it makes no difference to assume that it is deterministic (by conditioning). This argument is a bit misleading, as it is only true if the design matrix (\mathbb{X}) and the noise vector (w) affecting the observations are independent. This is not the case, for example, if the linear regression problem arises from a time-series problem, such as estimating the parameters of an autoregressive model (in which case the design matrix cannot be assumed to be deterministic).",667,0
NIPS_2018_917,NIPS_2018,"- Results on bAbI should be taken with a huge grain of salt and only serve as a unit-test. Specifically, since the bAbI corpus is generated from a simple grammar and sentence follow a strict triplet structure, it is not surprising to me that a model extracting three distinct symbol representations from a learned sentence representation (therefore reverse engineering the underlying symbolic nature of the data) would solve bAbI tasks. However, it is highly doubtful this method would perform well on actual natural language sentences. Hence, statements such as ""trained [...] on a variety of natural language tasks"" is misleading. The authors of the baseline model ""recurrent entity networks"" [12] have not stopped at bAbI, but also validated their models on more real-world data such as the Children's Book Test (CBT). Given that RENs solve all bAbI tasks and N2Ns solve all but one, it is not clear to me what the proposed method adds to a table other than a small reduction in mean error. Moreover, the N2N baseline in Table 2 is not introduced or referenced in this paper, so I am not sure which system the authors are referring to here. Minor Comments - L11: LSTMs have only achieved on some NLP tasks, whereas traditional methods still prevail on others, so stating they have achieved SotA in NLP is a bit too vague. - L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253). - Eq. 1: This seems like a very specific choice of combining the information from entity representations and their types. Why is this a good choice? Why not keep the concatenation of the kitty/cat outer product and the mary/person outer product? Why is instead the superposition of all bindings a good design choice? - I believe section four could benefit from a small overview figures illustrating the computation graph that is constructed by the method. - Eq. 7: At first, I found it surprising why three distinct relation representation are extracted from the sentence representation, but it became clearer later with the write, move and backling functions. Maybe already mention at this point why the three relation representations are going to be used for. - Eq. 15: s_question has not been introduced before. I imagine it is a sentence encoding of the question and calculated similarly to Eq. 5? - Eq. 20: A bit more details for readers unfamiliar with bAbI or question answering would be good here. ""valid words"" here means possible answer words for the given story and question, correct? - L192: ""glorot initalization"" -> ""Glorot initialization"". Also, there is a reference for that method: Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). - L195: Î±=0.008, Î²â=0.6 and Î²â=0.4 look like rather arbitrary choices. Where does the configuration for these hyper-parameters come from? Did you perform a grid search? - L236-244: If I understand it correctly, at test time stories with new entities (Alex etc.) are generated. How does your model support a growing set of vocabulary words given that MLPs have parameters dependent on the vocabulary size (L188-191) and are fixed at test time? - L265: If exploding gradients are a problem, why don't you perform gradient clipping with a high value for the gradient norm to avoid NaNs appearing? Simply reinitializing the model is quite hacky. - p.9: Recurrent entity networks (RENs) [12] is not just an arXiv paper but has been published at ICLR 2017.",- I believe section four could benefit from a small overview figures illustrating the computation graph that is constructed by the method.,668,0
NIPS_2018_884,NIPS_2018,". It's hard to judge impact in real-world settings when most of the quantitative evaluations are on datasets not representative of complex natural images (e.g. MNIST and NORB). On MNIST, the method shows clear advantages over competing methods. However, even on NORB, where a lot of the deformations can't easily be parameterized, this advantage has turned into being only on par with other leading methods. I think the inclusion of the faces dataset was important for this reason. I was confused for a while what the exact orbit was for each dataset. I kept scanning the text for this. A table of all three datasets and a short note on how orbits were defined and canonical samples selected would make things a lot clearer. Concurrent work. Similar ideas of representation learning through transformation priors have appeared in recent work. I don't think it takes away any novelty from this submission, since judging from the dates these is concurrent works. I just thought I would bring your attention to it: - https://openreview.net/pdf?id=S1v4N2l0- (ICLR 2018) - https://arxiv.org/pdf/1804.01552.pdf (CVPR 2018) Minor comments. - eq. 6: what connects the orbit with this loss? I don't see the connection just yet - eq. 7: ""x_q not in Oxq!=Oxi"" What is this notation ""set1 != set2"" that seems to imply it forms another set (and not a true/false value) line 136: Oxq \not= Oxi, again, I'm not sure about this notation. I understand what it means, but it looks odd to me. I have never seen this as part of set notation before. - eq. 8: where is x_p, x_q, x_c coming from? Shouldn't the summand be $(x_i, x_p, x_q) \in \mathcal{T}$? The canonical sample x_c is still unclear where it comes from. If x_c is the canonical instance for each orbit, then it also changes in the summation. This is not clear from the notation. - line 196: max unpooling transfers the argmax knowledge of maxpooling to the decoder. Do you use this behavior too? - Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11? - line 213: are all feature spaces well-suited for 1-NN? If a feature space is not close to a spherical Gaussian, it may perform poorly. If feature dimensions are individually standardized, it would avoid this issue. - It was a bit unclear how canonical samples were constructed on the face dataset (""least yaw displacement from a frontal pose""). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?",- Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11?,669,0
NIPS_2018_180,NIPS_2018,"- Although authors have proposed a set of modifications to their baseline DenseNet + SSD model, the novelty on each modification is limited. - Section 2.2.3. The experiment comparing PeleeNet with other network architecture is flawed. The author says ""we can evaluate a model pre-trained on ILSVRC 2012 on this dataset"", but the whole ImageNet has 1000 object classes but Stanford Dogs dataset only has 120 dog classes. Although some features can be shared in lower layers, comparing a model trained for a variety of categories with a model for dogs only is not fair. Also Table 3, PeleeNet shows big improvement on stanford dogs (~7% top-1 gain over mobilenet) but only 0.6% on the full Imagenet also suggests this experiment is not solid. Some additional comments: - It makes a lot of sense separable conv has an advantage in FLOP counts but doesn't run as fast in actual devices, due to bad memory access pattern. However the final Pelee model (which uses conventional conv) has similar MACs with mobilenet SSD and similar runtime too. Additional analysis / profiling could make this paper stronger. - Section 1, paragraph 3: ""the layer uses a small kernel size (3x3), which is good enough to capture small-size objects"": 3x3 is a standard practice for most modern NNs. It is arguable 3x3 is ""good enough"". Also smaller kernel is not very relevant with small objects? What matters more is better spatial resolution in conv feature maps, which is usually decided by the stride of conv kernels rather than its size. - Section 1, paragraph ""Composite Function"": ""post-activation"" is referring to BN before ReLU, this name is a bit confusing... better call it ""BN before activation"" / ""BN after activation""? - Same paragraph: experiment (Table 2) does show BN before ReLU hurt performance. The authors argue BN can be folded into conv this way, but BN after ReLU can be folded into next Conv op, because all activation in the same conv feature map shares the same mean / var / scale / offset in BN implementation? - The proposed architecture drops 38x38 feature map which would hurt on small objects? Usually small objects would cost more budget to capture and they don't give good improvement on overall mAP. It would be more convincing to analyze how the proposed model works for small / medium / large size objects. - [Minor] Section 2.1. The proposed model uses avg pool with stride 2. Conv with stride 2 could potentially be a good alternative? - [Minor] Section 3.1: SSD feature map used in this paper is very similar to [5]. Dropping 2x2 map has limited impact on speed / accuracy. Meanwhile, Mobilenet V1 / V2 + SSD lite (https://arxiv.org/pdf/1801.04381.pdf) presents similar performance but with smaller size / FLOPs. However it is reasonable to assume this paper to be concurrent work.","- Although authors have proposed a set of modifications to their baseline DenseNet + SSD model, the novelty on each modification is limited.",670,0
NIPS_2018_821,NIPS_2018,"1. A few parts of this paper are not very clear or not sufficiently provided, such as the model details. Section 4.3 should be addressed more to make it clearer since some concepts/statements are misleading or confusing. 2. The trace to code model is complex, which requires as many LSTMs as the number of input/output pairs, and it may be hard to be applied to other program synthesis scenarios. 3. Apart from the DSL, more experiments on dominant PL (e.g., Python) would be appreciated by people in this research field. Details are described as below: 1. For a given program, how to generate diverse execution traces that can be captured by the I/O -> Trace model? Since execution traces are generated by running the program on N I/O pairs, it is possible that some execution traces have a large overlap. For example, in the extreme case, two execution traces may be the same (or very similar) given different I/O pairs. 2. The authors involve the program interpreter in their approach, which is a good trial and it should help enhance the performance. However, I am curious about is it easy to be integrated with the neural network during training and testing? 3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated. 4. Line 183-line 184, is it necessary to use embedding for only four conditionals (of Boolean type)? only 16 possible combinations. 5. As depicted in Figure 3, if more I/O pairs are provided, the Trace->Code should be very complex since each i/o example requires such an LSTM model. How to solve this issue? 6. In essence, the Trace->Code structure is a Sequence to Sequence model with attention, the only differences are the employment of I/O pair embedding and the max pooling on multiple LSTM. How are the I/O pair embeddings integrated into the computation? Some supplementary information should be provided. 7. It is interesting to find that model trained on gold traces perform poorly on inferred traces, the authors do not give a convincing explanation. More exploration should be conducted for this part. 8. It would be better if some synthesized program samples are introduced in an appendix or other supplementary documents.","2. The authors involve the program interpreter in their approach, which is a good trial and it should help enhance the performance. However, I am curious about is it easy to be integrated with the neural network during training and testing?",671,0
NIPS_2018_26,NIPS_2018,"Weakness and questions: 1. It seems like the model is not capable of handling the interaction between compound objects since the model only merges the compound objects with the initial objects in Eq. (7). Does the model somehow handle such cases? 2. I am not convinced to call the procedure of computing R^{(t)} as relational reasoning because the model only elementwise-sums the gated object features. The resulting feature R^{(t)} can only contain the element-wise linear combination of the initial object features for any time t. 3. Reusing the gating networks for multiple time steps is not intuitive. Since both gating networks for computing R^{(t)} using the weights W_{l_1}, W_{l_2}, W_{r_1}, W_{r_2} takes the question as its only input, the same gating weights G_l and G_r would be obtained at every time step. In authors' rebuttal, I would like to see the answers to the above points in weakness. Especially, I believe that clear answers to the second and third points are necessary for the technical novelty of the paper. After the rebuttal phase: I thank the authors for the rebuttal answering all my questions. The rebuttal mainly presents empirical results to show that the proposed method performs better than the methods in the questions. However, I am still not fully convinced by their arguments as there are many possible ways to incorporate such concepts (eg., we can adopt a recurrent architecture to avoid the linearly increasing number of parameters while maintaining different gating values at different time steps.). I believe that these can be good directions to improve the method in their future work. Despite some of my concerns, I still think it is worth to accept this paper considering the contributions (written in the strength section of this review) with the reasonable empirical supports.",2. I am not convinced to call the procedure of computing R^{(t)} as relational reasoning because the model only elementwise-sums the gated object features. The resulting feature R^{(t)} can only contain the element-wise linear combination of the initial object features for any time t.,672,0
NIPS_2018_559,NIPS_2018,"- my only objection to the paper is that it packs up quite a lot of information, and because of the page-limits it doesnât include all the details necessary to reconstruct the model. This means cuts were made, some of which are not warranted. Sure, the appendix is there, but the reader needs to get all the necessary details in the main body of the paper. I quite enjoyed the paper, I think itâs definitely NIPS material, but it needs some additional polishing. I added my list of suggestions I think would help improve readability of the paper at the end of the review. Questions: - I might have missed the point of section 4.2 - I see it as a (spacewise-costly) way to say âprograms (as opposed to specs) are a better choice as they enable generalization/extrapolation via changing variable valuesâ? What is the experiment there? If itâs just to show that by changing variables, one can extrapolate to different images, I would save space on 1/2 of Figure 9 and focus on lacking parts of the paper (190 - extrapolations produced by our system - how did the system produce those extrapolations? was there a human that changed variable values or is there something in the system enabling this?) - What is + in Figure 3? If elementwise addition, please specify that - Figure 4 caption explains why the number N of particles is not the same across models. However, that still doesnât stop me from wondering whether there is a significant difference in performance in case all models are using the same number of particles. Do you have that information? - Line 54 mentions that the network can âderenderâ images with beam search. Is beam search used or not? What is the size of the beam? Is beam search used for each of the N particles? - From what I understood, the model does not have access to previously generated commands. Can you confirm that? - The order of (generated) specs is irrelevant for rendering, but it is for the generation process. How do you cope with that? Do you use a particular order when training the model or do you permute the specs? - Table 2 - â;â denotes OR, right? I would personally use the BNF notation here and use â|â - 153 - âminimized 3 using gradient descentâ - how did you treat the fact that min is not differentiable? - Table 5 - this is evaluated on which problems exactly? The same 100 on which the policy was trained? - Please provide some DeepCoder -style baseline details - the same MLP structure? Applied to which engine? A search algorithm or Sketch? - I find 152 - 153 unclear - how did you synthesize minimum cost programs for each \sigma ? \sigma represents a space of possible solutions, no? - Please provide more details on how you trained L_learned - what is the dataset you trained it on (randomly selected pairs of images, sampled from the same pool of randomly generated images, with a twist)? How did you evaluate its performance? What is the error of that model? Was it treated as a regression or as a classification task? - Figure 7 introduces IoU. Is that the same IoU used in segmentation? If so, how does that apply here? Do you count the union/intersection of pixels? Please provide a citation where a reader can quickly understand that measure. Suggestions: - full Table 3 is pretty, but it could easily be halved to save space for more important (missing!) details of the paper - the appendix is very bulky and not well structured. If you want to refer to the appendix, I would strongly suggest to refer to sections/subsections, otherwise a reader can easily get lost in finding the details - Section 2.1 starts strong, promising generalization to real hand drawings, but in the first sentence the reader realizes the model is trained on artificial data. Only in line 91 it says that the system is tested on hand-written figures. I would emphasize that from the beginning. - Line 108 - penalize using many different numerical constants - please provide a few examples before pointing to the supplement. - Line 154 - a bit more detail of the bilinear model would be necessary (how low-capacity?) - 175-176 - see supplement for details. You need to provide some details in the body of the paper! I want to get the idea how you model the prior from the paper and not the supplement - Table 5 - thereâs a figure in the appendix which seems much more informative than this Table, consider using that one instead The related work is well written, I would just suggest adding pix2code (https://arxiv.org/abs/1705.07962) and SPIRAL (https://arxiv.org/abs/1804.01118) for completeness. UPDATE: I've read the author feedback and the other reviews. We all agree that the paper is dense, but we seem to like it nevertheless. This paper should be accepted, even as is because it's a valuable contribution, but I really hope authors will invest additional effort into clarifying the parts we found lacking.","- Please provide more details on how you trained L_learned - what is the dataset you trained it on (randomly selected pairs of images, sampled from the same pool of randomly generated images, with a twist)? How did you evaluate its performance? What is the error of that model? Was it treated as a regression or as a classification task?",673,0
NIPS_2018_702,NIPS_2018,"Weakness: 1. This paper is a simple extension of capsule network, from 2D to 3D. It is interesting and straightforward, but novelty is limited. 2. Performance in table 1 is confusing. The f-mAP is quite inconsistent with v-mAP . I don't understand how your v-mAP is so high but your f-mAP is so low. It is counter-intuitive.","1. This paper is a simple extension of capsule network, from 2D to 3D. It is interesting and straightforward, but novelty is limited.",674,0
NIPS_2018_15,NIPS_2018,"- The hGRU architecture seems pretty ad-hoc and not very well motivated. - The comparison with state-of-the-art deep architectures may not be entirely fair. - Given the actual implementation, the link to biology and the interpretation in terms of excitatory and inhibitory connections seem a bit overstated. Conclusion: Overall, I think this is a really good paper. While some parts could be done a bit more principled and perhaps simpler, I think the paper makes a good contribution as it stands and may inspire a lot of interesting future work. My main concern is the comparison with state-of-the-art deep architectures, where I would like the authors to perform a better control (see below), the results of which may undermine their main claim to some extent. Details: - The comparison with state-of-the-art deep architectures seems a bit unfair. These architectures are designed for dealing with natural images and therefore have an order of magnitude more feature maps per layer, which are probably not necessary for the simple image statistics in the Pathfinder challenge. However, this difference alone increases the number of parameters by two orders of magnitude compared with hGRU or smaller CNNs. I suspect that using the same architectures with smaller number of feature maps per layer would bring the number of parameters much closer to the hGRU model without sacrificing performance on the Pathfinder task. In the author response, I would like to see the numbers for this control at least on the ResNet-152 or one of the image-to-image models. The hGRU architecture seems very ad-hoc. - It is not quite clear to me what is the feature that makes the difference between GRU and hGRU. Is it the two steps, the sharing of the weights W, the additional constants that are introduced everywhere and in each iteration (eta_t). I would have hoped for a more systematic exploration of these features. - Why are the gain and mix where they are? E.g. why is there no gain going from H^(1) to \tilde H^(2)? - I would have expected Eqs. (7) and (10) to be analogous, but instead one uses X and the other one H^(1). Why is that? - Why are both H^(1) and C^(2) multiplied by kappa in Eq. (10)? - Are alpha, mu, beta, kappa, omega constrained to be positive? Otherwise the minus and plus signs in Eqs. (7) and (10) are arbitrary, since some of these parameters could be negative and invert the sign. - The interpretation of excitatory and inhibitory horizontal connections is a bit odd. The same kernel (W) is applied twice (but on different hidden states). Once the result is subtracted and once it's added (but see the question above whether this interpretation even makes sense). Can the authors explain the logic behind this approach? Wouldn't it be much cleaner and make more sense to learn both an excitatory and an inhibitory kernel and enforce positive and negative weights, respectively? - The claim that the non-linear horizontal interactions are necessary does not appear to be supported by the experimental results: the nonlinear lesion performs only marginally worse than the full model. - I do not understand what insights the eigenconnectivity analysis provides. It shows a different model (trained on BSDS500 rather than Pathfinder) for which we have no clue how it performs on the task and the authors do not comment on what's the interpretation of the model trained on Pathfinder not showing these same patterns. Also, it's not clear to me where the authors see the ""association field, with collinear excitation and orthogonal suppression."" For that, we would have to know the preferred orientation of a feature and then look at its incoming horizontal weights. If that is what Fig. 4a shows, it needs to be explained better.",- The comparison with state-of-the-art deep architectures may not be entirely fair.,675,0
NIPS_2018_884,NIPS_2018,". It's hard to judge impact in real-world settings when most of the quantitative evaluations are on datasets not representative of complex natural images (e.g. MNIST and NORB). On MNIST, the method shows clear advantages over competing methods. However, even on NORB, where a lot of the deformations can't easily be parameterized, this advantage has turned into being only on par with other leading methods. I think the inclusion of the faces dataset was important for this reason. I was confused for a while what the exact orbit was for each dataset. I kept scanning the text for this. A table of all three datasets and a short note on how orbits were defined and canonical samples selected would make things a lot clearer. Concurrent work. Similar ideas of representation learning through transformation priors have appeared in recent work. I don't think it takes away any novelty from this submission, since judging from the dates these is concurrent works. I just thought I would bring your attention to it: - https://openreview.net/pdf?id=S1v4N2l0- (ICLR 2018) - https://arxiv.org/pdf/1804.01552.pdf (CVPR 2018) Minor comments. - eq. 6: what connects the orbit with this loss? I don't see the connection just yet - eq. 7: ""x_q not in Oxq!=Oxi"" What is this notation ""set1 != set2"" that seems to imply it forms another set (and not a true/false value) line 136: Oxq \not= Oxi, again, I'm not sure about this notation. I understand what it means, but it looks odd to me. I have never seen this as part of set notation before. - eq. 8: where is x_p, x_q, x_c coming from? Shouldn't the summand be $(x_i, x_p, x_q) \in \mathcal{T}$? The canonical sample x_c is still unclear where it comes from. If x_c is the canonical instance for each orbit, then it also changes in the summation. This is not clear from the notation. - line 196: max unpooling transfers the argmax knowledge of maxpooling to the decoder. Do you use this behavior too? - Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11? - line 213: are all feature spaces well-suited for 1-NN? If a feature space is not close to a spherical Gaussian, it may perform poorly. If feature dimensions are individually standardized, it would avoid this issue. - It was a bit unclear how canonical samples were constructed on the face dataset (""least yaw displacement from a frontal pose""). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?",- eq.6: what connects the orbit with this loss? I don't see the connection just yet - eq.,676,0
NIPS_2018_66,NIPS_2018,"of their proposed method for disentangling discrete features in different datasets. I think that the main of the paper lies in the relatively thorough experimentation. I thought the results in Figure 6 were particularly interesting in that they suggest that there is an ordering in features in terms of mutual information between data and latent variable (for which the KL is an upper bound), where higher mutual information features appear first as the capacity is increased. I also appreciate the explicit discussion of the robust of the degree of disentanglement across restarts, as well as the sensitivity to hyperparameters. Given the difficulties observed in Figure 4 in distinguishing between similar digits (such as 5s and 8s), it would be interesting to see results for this method on a dataset like dSprites, where the shapes are very similar in pixel space. The inferred chair rotations in Figure 7 are also a nice illustration of the ability of the method to generalize to the test set. The main thing that this paper lacks is a more quantitative evaluation. A number of recent papers have proposed metrics for evaluating disentangled representations. In addition the metrics proposed by Kim & Mnih (2018) and Chen et al. (2018), the work by Eastwood & Williams (2017) [1] is relevant in this context. All of these metrics presume that we have access to labels for true latent factors, which is not the case for any of the datasets considered in the experimentation. However, it would probably be worth evaluating one or more of these metrics on a dataset such as dSprites. A minor criticism is that details the training procedure and network architectures are somewhat scarce in the main text. It would be helpful to briefly describe the architectures and training setup in a bit more detail, and explicitly call out the relevant sections of the supplementary material. In particular, it would be good to list key parameters such as Î³ and the schedule for the capacities Cz and Cc, e.g., the figure captions. In Figure 6a, please mark the 25k iterations (e.g. with a vertical dashed line) to indicate that this is where the capacity is no longer increased further. Questions - How robust is the ordering on features Figure 6, given the noted variability across restarts in Section 4.3? I would hypothesize that the discrete variable always emerges first (given that this variable is in some sense given a âgreaterâ capacity than individual dimensions in the continuous variables). Is the ordering on the continuous variables always the same? What happens when you keep increasing the capacity beyond 25k iterations. Does the network eventually use all of the dimensions of the latent variables? - I would also appreciate some discussion of how the hyperparameters in the objective were chosen. In particular, one could imagine that the relative magnitude of Cc and Cz would matter, as well as Î³. This means that there are more parameter to tune than in, e.g., a vanilla Î²-VAE. Can the authors comment on how they chose the reported values, and perhaps discuss the sensitivity to these particular hyperparameters in more detail? - In Figure 2, what is the range of values over which traversal is performed? Related Work In addition to the work by Eastwood & Williams, there are a couple of related references that the authors should probably cite: - Kumar et. al [2] also proposed the total correlation term along with Kim & Mnih (2018) and Chen et al. (2018). - A recent paper by Esmaeli et al. [3] employs an objective based on the Total Correlation, related to the one in Kim & Mnih (2018) and Chen et. al (2018) to induce disentangled representations that can incorporate both discrete and continuous variables. Minor Comments - As the authors write in the introduction, one of the purported advantages of VAEs over GANs is stability of training. However, as mentioned by the author, including multiple variables of different types also makes the representation unstable. Given this observation, maybe it is worth qualifying these statements in the introduction. - I would say that section 3.2 can be eliminated - I think that at this point readers can be presumed to know about the Gumbel-Softmax/Concrete distribution. - Figure 1 could be optimized to use less whitespace. - I would recommend to replace instances of (\citet{key}) with \citep{key}. References [1] Eastwood, C. & Williams, C. K. I. A Framework for the Quantitative Evaluation of Disentangled Representations. (2018). [2] Kumar, A., Sattigeri, P. & Balakrishnan, A. Variational inference of disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848 (2017). [3] Esmaeili, B. et al. Structured Disentangled Representations. arXiv:1804.02086 [cs, stat] (2018).","- A recent paper by Esmaeli et al. [3] employs an objective based on the Total Correlation, related to the one in Kim & Mnih (2018) and Chen et. al (2018) to induce disentangled representations that can incorporate both discrete and continuous variables. Minor Comments - As the authors write in the introduction, one of the purported advantages of VAEs over GANs is stability of training. However, as mentioned by the author, including multiple variables of different types also makes the representation unstable. Given this observation, maybe it is worth qualifying these statements in the introduction.",677,0
NIPS_2018_799,NIPS_2018,"- It would help to have some element of theoretical analysis on the chosen definition of the importance measures Z (Sec. 3). For example, deriving this as an explicit regularizer on the layer weights. - The framework (proposed as a general feature selection method?), is restricted to Gaussian design and the use of an estimated precision matrix, for which there might not be any robustness guarantees. Some discussion (or paths for alternative designs?) might be helpful. - There is some disconnect, in my opinion, between motivating this work as a feature selection method for NN and using a NN as a way to measure importance (given a prediction task). - The empirical validation, even though convincing in some respect: the benefit of the novelty here (i.e. adding a pairwise connected layer vs. having a vanilla MLP) is only explored on the synthetic data. Specific comments: - The method is motivated by the need for interoperability in NN and error control in NN, though only the latter is explored (in a synthetic scenario). - Similarly, the title states âreproducibleâ feature selection and this might something that needs further explaining. In what sense reproducible? - Did the authors explore/consider different definitions of the importance measures on the knockoff statistics? - What is the dependency of some aspects of the performance on the knock-off feature generation (e.g., s for the Gaussian case)? Is there a way to make vanilla MLP + knockoff features better by changing (or tuning) s? - Is there a specific rationale for fixing the number of neurons in the hidden layer to be p (equal to the input dimension)? As long as the output layer is p-dimensional the reasoning should still hold with a different âfactorizationâ of the input to output mapping that defines w. The same holds for using L1 regularization (and with a specific regularization parameter). - How dependent on the specific parametrization of the MLP (or NN more general) is the performance? - The FDR target error level is fixed/set to q=0.2 everywhere. Would it be insightful to vary this and re-interpret, e.g. the results of Sec. 5.1? - It would be helpful to see some prediction results with real data, based on the selected features. E.g. a two-pass process where the NN is re-trained on the selected, error-control features to predict the target function values or class. - How much False Discovery errors are relevant for NN performance in practical applications (and in high-dimensions)? An open question that might be worth discussing. - Did the authors try the knockoffs+vanilla MLP approach on the real data in 5.1 (or the qualitative analysis in 5.2)? The pairwise-connected layer is the novelty of this paper and this should be further highlighted. ========================= Comments after author feedback: Thank you for the replies to all reviews and most of the comments. I agree with the points of your clarifications and can see the value of the points made in the response. I also agree with Reviewer 1 that this can inspire future work by the authors and others. I am changing my score to a 6 to reflect these. At the same time my main concern on general applicability remains, and in some cases has been strengthened by some of the responses. - âin the sense that small changes in the data can yield dramatic changes in the selected features. â If that is the case, perturbations in the data (and thus reproducibility of the framework) are not explored. What type of changes? - âWe consider the primary virtue of the robustness to be improved reproducibility of the selected features;â As above. I donât feel that this aspect of the authorsâ work is highlighted or supported experimentally in the paper. - âGiven the universal approximation property of DNNs, practitioners can now avoid having to handcraft feature importance measures with the help of DeepPINK.â This is a good point, thank you for making this connection explicit. Is the addition of DeepPINK layer however âalteringâ the network definition, i.e. are you selecting features for the original network or the modified one? How dependent is this selection on initialization, parametrization, and as the authors state, changes in the input data (e.g. transformations, subsampling, noise etc). - âusing interpretability alone as a measurement is far less reliable than using FDR and power with synthetic data as measurements.â This is also a good point, and provides statistical guarantees (as Reviewer 1 also points out). - âit is the fundamental architecture (instead of the tuning) of vanilla MLP that prevents the knockoff filter from performing well.â I am not sure what this claim implies. That MLP is not a good choice for being subjected to knockoff filters? This defies the purpose of the paper. Or that knockoffs can only add value coupled in the learning process (like for DeepPINK)? I wonder, and I made the same point in my initial review on which the authors replied, how much those findings are generally applicable to any âneural networkâ parametrization or just MLP-type. How about basic linear or (shallow) non-linear regression with the PINK coupling and solved through SGD? - âSince there is no existing method available for robust feature selection in DNNs, any effective methods would be significant.â I agree. However the idea of input features and feature representation (hidden layers) is coupled in NNs. As the authors point out, interpretability and selection is conditioned on the prediction algorithm. So this is not âfeature selectionâ, but input dimension selection. One could envision however a PINK layer attached on higher hidden layers for representation dimension selection, interpretability and transferability. - âThe proposed network architecture may seem simple, but our journey to finding it was nontrivial.â So then does the paper involve a single, fixed architecture? Or is it generally applicable for any parametrization that one can add on top of the pairwise linear layer?","- It would be helpful to see some prediction results with real data, based on the selected features. E.g. a two-pass process where the NN is re-trained on the selected, error-control features to predict the target function values or class.",678,0
NIPS_2018_686,NIPS_2018,"- It would have been nice to see experiments that showed the effect of using different numbers of particles. It is currently unclear how important this choice is. - The writing is occasionally uneven and could use additional editing (more from a language-usage and type perspective than a technical perspective). Also, in the equations, the notation is exact, but gets quite unwieldy at times. It may be worth considering dropping some of the sub/superscripts for convenience where possible (and noting that you are doing so). - This is a minor detail, but in section 2.2 when discussing the ill-posedness of IRL, the paper focuses on degenerate solutions. But the main ill-posedness problem comes more from the infinite number of reward functions that can describe identical policies / equally explain demonstrations. Overall, the author response addressed several of the concerns of myself and the other reviews, reinforcing my positive outlook on this work.",- It would have been nice to see experiments that showed the effect of using different numbers of particles. It is currently unclear how important this choice is.,679,0
NIPS_2018_464,NIPS_2018,"of the approach is the definition of the behavior characterization, which is domain-dependent, and may be difficult to set in some environments; however, the authors make this point clear in the paper and I understand that finding methods to define good behavior characterization functions is out of the scope of the submission. The article is properly motivated, review of related work is thorough and extensive experiments are conducted. The methods are novel and their limitations are appropriately stated and justified. The manuscript is carefully written and easy to follow. Source code is provided in order to replicate the results, which is very valuable to the community. Overall, I believe this is a high quality submission and should be accepted to NIPS. Please read more detailed comments below: - The idea of training M policies in parallel is somewhat related to [Liu et al., Stein Variational Policy Gradient], a method that optimizes for a distribution of M diverse and high performing policies. Please add this reference to the related work section. - The update of w in NSRA-ES somehow resembles the adaptation of parameter noise in [Plapper et al., âParameter Space Noise for Explorationâ, ICLR 2018]. The main difference is that the adaptation in Plappert et al. is multiplicative, thus yielding more aggressive changes. Although this is not directly compatible with the proposed method, where w is initialized to 0, I wonder whether the authors tried different policies for the adaptation of w. Given its similarity to ES (where parameter noise is used for structured exploration instead of policy optimization), I believe this is a relevant reference that should be included in the paper as well. - It seems that the authors report the best policy in plots and tables (i.e. if f(theta_t) > f(theta_{t+k}), the final policy weights are theta_t). This is different to the setup by Salimans et al. (c.f. Figure 3 in their paper). I understand that this is required for methods that rely on novelty only (i.e. NS-ES), but not for all of them. Please make this difference clear in the text. - Related to the previous point, I believe that section 3.1 lacks a sentence describing how the final policy is selected (I understand that the best performing one, in terms of episodic reward, is kept). - In the equation between lines 282 and 283, authors should state how they handle comparisons between episodes with different lengths. I checked the provided code and it seems that the authors pad the shorter sequence by replicating its last state in order to compare both trajectories. Also, the lack of a normalization factor of 1/T makes this distance increase with T and favors longer trajectories (which can be a good proxy for many Atari games, but not necessarily for other domains). These decisions should be understood by readers without needing to check the code. - There is no caption for Figure 3 (right). Despite it is mentioned in the text, all figures should have a caption. - The blue and purple color in the plots are very similar. Given the small size of some of these plots, it is hard to distinguish them -- especially in the legend, where the line is quite thin. Please use different colors (e.g. use some orangish color for one of those lines). - Something similar happens with the ES plot in Figure 2. The trajectory is quite hard to distinguish in a computer screen. - In SI 6.5, the authors should mention that despite the preprocessing is identical to that in Mnih et al. [7], the evaluation is slightly different as no human starts are used. - In SI 6.6, the network description in the second paragraph is highly overlapping with that in the first paragraph. ---------------------------------------------------------------------------------------------------------------------------------------------------------- Most of my comments had to do with minor modifications that the authors will adress. As stated in my initial review, I vote for accepting this submission.","- Related to the previous point, I believe that section 3.1 lacks a sentence describing how the final policy is selected (I understand that the best performing one, in terms of episodic reward, is kept).",680,0
NIPS_2018_849,NIPS_2018,"- The presented node count for the graphs is quite low. How is performance affected if the count is increased? In the example of semantic segmentation: how does it affect the number of predicted classes? - Ablation study: how much of the learned pixel to node association is responsible for the performance boost. Previous work has also shown in the past that super-pixel based prediction is powerful and fast, I.e. with fixed associations. # Typos - Line 36: and computes *an* adjacency matrix - Line 255: there seems to be *a weak* correlation # Further Questions - Is there an advantage in speed in replacing some of the intermediate layers with this type of convolutional blocks? - Any ideas on how to derive the number of nodes for the graph? Any intuition on how this number regularises the predictor? - As far as I can tell the projection and re-projection is using activations from the previous layer both as feature (the where it will be mapped) and as data (the what will be mapped). Have you thought about deriving different features based on the activations; maybe also changing the dimension of the features through a non-linearity? Also concatenating hand-crafted features (or a learned derived value thereof), e.g., location, might lead to a stronger notion of ""regions"" as pointed out in the discussion about the result of semantic segmentation. - The paper opens that learning long-range dependencies is important for powerful predictors. In the example of semantic segmentation I can see that this is actually happening, e.g., in the visualisations in table 3; but I am not sure if it is fully required. Probably the truth lies somewhere in between and I miss a discussion about this. If no form of locality with respect to the 2d image space is encoded in the graph structure, I suspect that prediction suddenly depends on the image size.",- The presented node count for the graphs is quite low. How is performance affected if the count is increased? In the example of semantic segmentation: how does it affect the number of predicted classes?,681,0
NIPS_2018_177,NIPS_2018,"weakness and issues which should be clarified: 1) The novelty of this paper is incremental. The proposed method is developed based on the MDNet framework. It seems that the only difference is that the proposed method further incorporate the attention regularization for backward propagation. 2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value. 3) The author claims that the proposed method can enable the classifier attend to temporal invariant motion patterns. It seems that no explanation is provided about what motion patterns mean in this paper. Although some figures show the evolvement of attention during training, no motion pattern is illustrated. In addition, some large variations may happen during the tracking process, such as out-plane-rotation, how can the proposed method ensure that the temporal motion invariant pattern can be found and the classifiers can attend to them? [POST-REBUTTAL COMMENTS] I have read the rebuttal and still have the concerns on the theoretical support for the regularization term. I keep my rating.",1) The novelty of this paper is incremental. The proposed method is developed based on the MDNet framework. It seems that the only difference is that the proposed method further incorporate the attention regularization for backward propagation.,682,0
NIPS_2018_234,NIPS_2018,"1. There are many unclear parts in the paper, lacking formalism in some places which leaves the reader confused. I will give some examples for that in the order in which they appear a. Introduction, 2nd paragraph â the example requires the authors to make some analogy between the chemical quantities of the systems and concepts in active learning. It is completely unclear to me how this example makes a case for active learning. b. The authors make first reference to a leaf in the setting and background in page 2 line 54. However, no definition of a tree has been made yet, or even a simple reference to such. c. Authors refer to n and d in line 93 and in algorithm 1 however it was never made clear what they are. ânâ, for example: Is ânâ the total size of data? Is it the test set size? Pool set size? d. Preposition 4.1 is not clear are we looking at the post selected leaf-set X after q_k proportion was selected? e. Most important: Key Corollary 4.6 proof is unclear, short, informal and confusing: what minimum and average are your referring to? This is a key result of the paper and it should be very clearly explained and proved! 2. Author should explain and motivate their decision to let $a_k=\frac{p_k}{\sgima^2_{Y,k}} in line 156. Why should the multiplicative factor have such a form? 3. Experimental results: Results are not convincing in demonstrating that the method is advantageous for real data. Authors should provide such examples or improve their algorithm. 4. In addition: a. Authors should provide basic data sizes for the datasets they experiment with in order for the reader to evaluate the efficiency of their method by, for example, comparing the size of the training set with the size of the full data set. b. Please provide reference for Breiman trees 5. Minor: many typos exist, authors should proof read their paper.",3. Experimental results: Results are not convincing in demonstrating that the method is advantageous for real data. Authors should provide such examples or improve their algorithm.,683,0
NIPS_2018_86,NIPS_2018,"weakness in this work is in the lack of experiments using the spike-and-slab theory. It would be very nice to see a) that the spike-and-slab priors discussed could be trained in practice, whether by some sort of Gibbs sampling, MCMC, or by a variant of standard SGD, and b) if an architecture could be adaptively trained using the prior on network width using Equation 18. - It is interesting that the composed deep network suggested by [1] (~180 parameters) performs so much better than the wide shallow network (~8000 parameters). Perhaps the authors can comment on if they would expect this result to hold in the limit of width as the single layer neural network converges to a Gaussian process? - The results seem to hinge on the true generative model, P_f0^n, being in the same class of functions as the model. Could the authors clarify if this is the case, and if not, what would happen to both sparsity and the posterior distribution? - Proof of Theorem 5.1: In Section 3, the authors point to the fact that they must verify all three conditions (Equations 11-13); however, in the proof, they only verify Equations 11 and 12. A quick sentence demonstrating why they do not need to verify Equation 13 would be quite helpful. - Corollary 5.1: In line 274, s_n = n \epsilon_n^2, which has been assumed to approach infinity in the limit (line 183). Thus, it seems like the corollary is just showing that the sparsity level is finite (and thus the number of non-zero parameters is finite), when it should be finite by construction of the neural network. Could the authors comment on this interpretation? Updates after Author Responses: I thank the authors for addressing my mathematical comments, questions about implementations, and comments about paper structuring. These resolve many of my concerns with the paper. Minor Comments/Typos: Line 69: Similarly as dropout, -> Similarly to dropout Line 79: Section 3 defined â¦ -> Section 3 defines (rest of paragraph is in present tense) Equation 7: A verification that this holds for all L>1, p>1 and N>1 would be nice. It is possible to find nonsensical parameter settings that do not satisfy this inequality under those conditions, but I could not find a counter-example to the inequality. Line 168: Potentially (T \choose s)^{-1} instead of 1/ (T \choose s) for clarity. Also applies to other locations. Line 178: The posterior distribution should also include dependence on X (the observed features) and not just Y (the observed classes). Line 201: A citation for âit is commonly agreed â¦â would be quite helpful. Line 297: We proofâ¦ -> We proveâ¦ Line 297: There seems to be a missing \Epsilon. log N (â¦) -> log N \Epsilon (â¦) Line 307: Where does \epsilon_n/2 come from? It seems like the triangle inequality would give \epsilon_n instead. References: [1] Mhaskar, H., Liao, Q., and Poggio, T. A. When and why are deep networks better than shallow ones? AAAI, 2017. [2] Zhang, C., et al. Understanding Deep Learning Requires Rethinking Generalization, ICLR, 2017. (see particularly Theorem 1 and Lemma 1 in Appendix C.)","- The results seem to hinge on the true generative model, P_f0^n, being in the same class of functions as the model. Could the authors clarify if this is the case, and if not, what would happen to both sparsity and the posterior distribution?",684,0
NIPS_2018_629,NIPS_2018,"I have a few points below which I hope can be addressed in the rebuttal. 1. Choice of certain hyperparameters â how is the size of the latent space chosen? 2. Experimental results: Ablation study: needed to understand the impact of different components of the loss function â reconstruction loss and the two adversarial losses â one for the latent space, other for the output as also the impact of the probabilistic model. The authors allude to the fact that adversarial training improves the model, this is not experimentally validated anywhere. Manifold projection and the residual: The authors model the two probability distributions â one on the manifold via the latent space distribution and the other for the residual of the projection onto the manifold. Independence and stationarity are assumed to compute the two and use the product as the probability for generating an inlier data point. Since outliers are expected to be away from the manifold, the projection residual is expected to play a bigger role in identifying outliers. This is not the case as shown in Figure 5. Why? Please discuss. Comparison with other methods: There is also no comparative discussion of their results vis-Ã -vis the state of the art.",1. Choice of certain hyperparameters â how is the size of the latent space chosen?,685,0
NIPS_2018_90,NIPS_2018,"weakness of previous work with regards to the variance of these generated examples. In particular, since the true distribution of the novel class is not known, itâs difficult to estimate its intra-class variance. Previous works have implicitly assumed that all classes have the same variance, which is an unrealistic assumption. This work tackles this problem by instead assuming that a novel class will have similar covariance to the covariance of its most related base classes. In every low-shot learning episode, comprised of a set of N novel classes, they sample a set of base classes that are relevant to those novel classes. The relevance between a novel and base classes is determined via a metric inspired by the prototypical network approach: a prototype is created for each base class, which is the average representation of its examples, and a prototype is created for each novel class similarly. The base class prototypes that are the nearest-neighbors of a novel class prototype correspond to its most related classes. In every such batch consisting of N novel classes and the set of most relevant base classes, an adversarial network is employed to âtransformâ each base class example into a novel class example. The GAN is trained as usual with the difference that the generatorâs classification task has (N+1) targets: each example will either belong to one of the N novel classes of the batch, or is a generated example (ie an example that is transformed from one of the related base classes into something that should look like an example of a novel class). Successful training of this GAN will make the generated examples of the novel classes indistinguishable from the real examples of the novel classes. They then introduce additional components to the loss function to match the covariance of the generated examples for a class to the covariance of the real class from which they were transformed. Pros - Using the intra-class variance of classes that are related to the novel class seems like a reasonable proxy for the intra-class variance of the novel class. Generating examples this way seems indeed like a promising way of sampling more diverse examples that are still plausible representatives of the novel class. - They progressively add layers of complication to their method. By evaluating each of these variants they justify that their final approach is well-motivated and all its components are necessary for achieving good performance. The simplest version is a standard conditional GAN whose purpose is to translate examples of base classes into examples of novel classes such that these generated examples are indistinguishable from the real examples of novel classes. Then, they take extra steps to ensure that the distribution of the base class that is most related to a novel class is faithfully maintained during translation. They do this using a cyclized GAN objective. Their final variant adds an additional component to the loss that explicitly penalizes differences in the covariance between the related base class and the translated examples for a given novel class. - They verify both quantitatively and qualitatively that their generated images are more diverse than competing approaches - Their method achieves state-of-the-art results on ImageNet Cons - An important component of the algorithm was left unclear: how the training progresses. One potential option would be to train episodically, through a series of training episodes each of which mimic a low-shot test episode. In that case, N base classes would be sampled for a training episode, and their most related other base classes would be included (as determined by class prototypes) would contribute the examples that would be âtranslatedâ into the N chosen base classes. Then a classifier for this episode would be learned using the k real examples of each of the N classes, together with the fake examples. Finally, based on the performance of this newly learned classifier on a disjoint set of examples originating from these N classes, the modelâs weights are updated (e.g. embedding network, generator parameters, etc). Though this seems natural, itâs not stated whether this is indeed how training proceeds. Itâs important to clarify this point. - Clarity is sometimes compromised due to using certain words incorrectly. The word âincurâ is used multiple times in a confusing way for example. - Experimentally it would be nice to compare to other low-shot learning methods that do not necessarily work by feature augmentation. For example metric learning and meta-learning methods present a different solution which is shown to be effective as well. It would be nice to compare against some representatives of these families, e.g. Prototypical Network, Matching Network, Siamese Network, MAML (Finn et al, 2017). Other comments: - It should be noted that this approach utilizes an additional piece of information than many of the competitors: it utilizes the data of the base classes not only at meta-training time but also during meta-testing. In particular, even at test time the real data of the base classes constitutes the input to the generator for translating those images into fake data of the novel classes. - A prototypical network is an episodic model: it averages the representations of the points *of a given episode* that belong to the same class to form the prototype of that class. Here, as I understand it, to form the prototype of a class, all examples in the entire base set that belong to that class are averaged. Therefore I wouldnât call this a Prototypical Network. This process can just be explained as representing each class as a cluster whose mean is the average of the features of the corresponding class points. - Prototypical Matching Network (PMN) is an entry in the table, without a reference. Also, no description is provided for that method. Please cite the appropriate work. - Similarly for P-FH: there is a short description provided but unclear which previous work this method comes from. Is this a new proposed baseline? If so it would be good to give more details. - Line 248: âits embedding technique is more complexâ, referring to the embedding technique of FI compared to this proposed model. It wasnât clear why itâs more complex. In a nutshell, the authors put forth a new model for low-shot learning by feature augmentation whose purpose is to generate as diverse as possible examples that still plausibly belong to each novel class. They experimentally demonstrate that their approach achieves state-of-the-art on ImageNet and generated more diverse examples than previous works. The down sides of the paper are mostly in terms of clarity, caused both by the inappropriate or confusing use of words as well as omitting details of the algorithm that I feel are important. Finally, it would be useful to compare experimentally against other approaches that tackle low-shot learning in ways other than feature augmentation.","- It should be noted that this approach utilizes an additional piece of information than many of the competitors: it utilizes the data of the base classes not only at meta-training time but also during meta-testing. In particular, even at test time the real data of the base classes constitutes the input to the generator for translating those images into fake data of the novel classes.",686,0
NIPS_2018_2,NIPS_2018,"Weakness: 1. Notation confusion. In equation (1) and theorem #1, x_i denotes a local input from (i-m)-th vertex to (i+m)-th vertex. However, in equation (6), a same x_i represents a single value (with multi-channel) at i-th vertex only. 2. The detailed information for graph construction is missing, e.g. the way to define edges and construct a graph for non-euclidean datasets. 3. Despite the proposed method shares a different idea with [17], the P_i matrix in theorem #1 mathematically transforms the input x_i into a high-dimensional space where a convolution weights v are applied. Can the authors briefly compares the proposed method with [17]? My feeling is that [17] should be a good baseline to compare on Euclidean datasets. 4. It seems that the baselines in Experiment 5.2 are not the original models, but the authors' own implementations, e.g. ActCNNs[18] has 93% accuracy on Cifar10 instead of 90%. Can the authors comment about this? 5. It would be good if the author can compare number of parameters between the proposed network and used baselines. Minor comments: 1. In figure 3(f), the improvement looks saturated when using 56 layers network. Will the proposed method also benefit deeper network (e.g. resnet110 in Cifar10)? 2. It would be interesting if the authors would comment on how their approach can be applied to point cloud dataset. Conclusion: I hope the authors will response the above questions in the rebuttal and include these in the final version. Given the novelty of the approach and extensive experiments, the submission would be stronger if the authors could fix these flaws.","3. Despite the proposed method shares a different idea with [17], the P_i matrix in theorem #1 mathematically transforms the input x_i into a high-dimensional space where a convolution weights v are applied. Can the authors briefly compares the proposed method with [17]? My feeling is that [17] should be a good baseline to compare on Euclidean datasets.",687,0
NIPS_2018_296,NIPS_2018,"weakness, and provides guarantees on the number of nodes to be expanded before reaching a goal state. First, LevineTS based on Levine Search performs best-first search enumeration: penalizes with depth of a node and performs state cuts using the policy. Second, LubyTS is based on existing work on scheduling for randomized algorithms. It takes advantage of large number of candidate solutions (goal states) in the form of known maximum depth at which these solutions are present. It samples trajectories of this length using the policy until solution is found. Experiments are performed on Sokoban domain using a neural network policy learned using a reinforcement learning algorithm and comparison is performed with a domain-independent planner with good results. Pros: - Very well-written paper. - The proposed tree search algorithms and guarantees on expected search time (in terms of expanded nodes) to reach goal state. - Technically solid work. Cons: - The practical significance of this problem and tree search algorithms is not clear. - Lack of experimental results on more practical real-world applications of this problem setting and algorithms. Detailed Comments: 1. Section 2 introduces too much notation that I don't think is necessary to explain the problem setting and algorithms. These are very simple search concepts. 2. I would have at least liked to see some potential real-world use-cases of the problem setting considering this paper is submitted to a NIPS conference. 3. Experimental results validate the algorithms and guarantees, but it would have been nice to see more empirical analysis on real-world use cases.",- Very well-written paper.- The proposed tree search algorithms and guarantees on expected search time (in terms of expanded nodes) to reach goal state.,688,1
NIPS_2018_342,NIPS_2018,"weakness. I didn't actually have a concern on practical use cases, as the response states. I just think it would be more interesting to consider noise that appears from the types of situations described, rather than Gaussian (and the authors acknowledged this point). Overall, this was an interesting submission. I am upping my score to 7. -------------------- This paper proposes to learn an unknown graph given a subset of the effective resistances of pairs of nodes. The proposed methods can substantially out-perform random guessing in network reconstruction on a couple synthetic networks and some Facebook egonets. The main idea of the paper is interesting and original, and the proposed methods do well on the datasets considered. However, I think the choice of egonets as real-world datasets is limiting. The fact that all nodes share a common neighbor implies substantial social structure (and possibly substantial graph structure, if the ego is included --- it is not clear if the ego is included in these datasets). I would suggest to instead use a variety of social networks, or if the authors are keen on Facebook, use some of the Facebook100 graphs. Another weakness in the experimental design is the following. The noise terms for the experiments are Gaussian, but I read the motivation for the noise terms to come from using approximation algorithms to deal with computational constraints or from partial measurements (last paragraph of introduction, lines 43--48). The usefulness of the Gaussian error seems to be that the solution to the optimization problem (Problem 2) is the MLE. The experimental results would be much more convincing if the noise was closer to the motivation. For example, the authors could compute approximations to the effective resistance with an approximation algorithm (the simplest case might be to run just use a few steps of an iterative solver for computing the effective resistances). From the presentation of the introduction, I expected more theoretical results. However, the only formal theoretical statements I see in the main text are Theorem 1, which was proved in another paper, and Proposition 1, which is a gradient calculation. Overall, I think this paper would greatly benefit by tightening up all of the language around the problem formulation and including more experiments and description of the actual methodology (e.g., what is the SDP formulation?). All of that being said, I enjoyed the main idea of the paper. The problem itself opens up some interesting avenues for both theoretical and empirical future work, and the numerical experiments do show that the proposed methods can be successful. I think the experiments (namely, choice of real-world datasets and types of noise) and presentation could be updated for an improved camera version. Thus, I lean towards accepting this paper. Fianlly, perhaps I am missing a compelling reason that we should be using egonets as the main example, so please point this out to me if it is the case. Some small issues: 1. In the explanation of privacy attacks (lines 36--42), it would be useful to know some scenarios under which the node similarities would be released, but the link information would not be released. 2. ""we show that, when G is a tree, exact recovery is possible for Problem 1.2 when S is a superset of G's edges"" Should this be subset of edges? 3. Please include a reference to [43] in the statement of Theorem 1. Right now, it seems like the theorem is a novel contribution of this paper, which, from my reading of the text, it is not. 4. ""This approach quickly converges to near global minimums for many networks."" I am not sure how I am supposed to draw this conclusion from the experiments. 5. ""This modified problem is convex and can be solved via an SDP."" Where is the SDP formulation? 6. Why is there no \sigma^2 = 0.1 case for FB Medium A and FB Large A. 7. Typos: Appendixx E (line 186), we learn via share (line 236)","4. ""This approach quickly converges to near global minimums for many networks."" I am not sure how I am supposed to draw this conclusion from the experiments.",689,0
NIPS_2018_975,NIPS_2018,"- Do all the baseline LSTM models in the paper use recurrent projection units (Sak et al. 2014, Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling)? It should be easy to implement and it is effective to improve performance with the same number of parameters. - LSTM + 1-D conv should be used as the baseline in Table 3 and 5 since it is shown to be better than LSTM alone in Table 1. - It is kind of surprising that SRU + 1-D conv achieves better accuracy than LSTM + 1-D conv. It is worth discussing the reason / hypothesis in the paper. - In Table 6, it would be useful to also measure the computation time of LSTM AM for comparison. - The proposed system should be compared to a standard LSTM AM + WFST decoder graph with a (pruned) n-gram LM, which is shown to be efficient for embedded speech recognition in previous works (e.g. McGraw et al. 2016, Personalized speech recognition on mobile devices), especially now that the RNN LM is the computation bottleneck of the entire system as is shown in Figure 2. In fact, the speed of RNN LM could be further improved by 8-bit quantization, which typically does not degrade the WER. Clarity: In general, the presentation in the paper is clear and well-organized, except for the following questions: - In Table 2, what is the system performance with HCLM? - Does the system in Table 6 use characters or word pieces? Both should be shown for comparison. - How much is the WER different between different beam width in Figure 2? Does using half-precision in LM hurt WER? - It would be clearer to mention in Table 2 that 3.60 (for Deep Speech 2) is WER (instead of CER). Originality: The problem is well-motivated: multi-time step parallel processing for AM would be very useful for embedded system recognition, but SRU alone does not work. It is novel to find that simply combining 1-D conv with SRU substantially improves the quality of SRU, leading to better performance than LSTM. Other tricks and analyses in the paper to improve speed and accuracy are also useful. Significance: The results are very encouraging in terms of both accuracy and speed. Both the idea and the experiments would be of interest to the field and motivate future work. It is a good submission. I recommend an accept, although the issues mentioned above should be addressed.","- In Table 6, it would be useful to also measure the computation time of LSTM AM for comparison.",690,0
NIPS_2018_888,NIPS_2018,"weakness of the paper is that some parts are a bit vague or unclear, and that it contains various typos or formatting issues. I shall provide details in the following list; typos I found will be listed afterwards. 1. In the section about related work, I was wondering whether there is no related literature from the robust scheduling literature. I am not overly familiar with the area, so I cannot give concrete references, but I am aware that robust scheduling is a very large field, so I would be surprised if there were no robust online scheduling problems/algorithms that are connected to the work at hand in one way or another. Could you please shed some light on this aspect (and provide some reference(s) if appropriate)? 2. From the description in the âmain resultsâ section of the Introduction, the precise connection between competitive ratio and robustness or consistency did not become entirely clear to me. Please provide some additional clarification on what is to be compared here â in particular, I also stumbled over why (as stated in several corresponding theorems) one ends up with competitive ratios âat most min{ robustness-ratio, consistency-ratio }â. At first glance, I had thought that if an algorithm does well for good predictions but the robustness bound is bad, that the latter should dominate. As this is apparently not the case, I ask for a brief explanation/clarification, preferably already in the introduction, as to why the minimum of the two values yields a bound on the competitive ratio. 3. In Algorithm 3, please clarify what âchosenâ means exactly. (Is it âdrawn uniformly at random from the k (or l, resp.) values q_i (r_i, resp.)â ? Or âchosen as index yielding maximal q_i/r_i-valueâ? Or something else?) Also, on a higher level, what makes q_i and r_i probability distributions? 4. On p.5, line 151 (multi-line inequality chain): I could not immediately verify the 3rd equation (in the middle line of the chain) â if it is based on a known formula, I do not recall it (maybe provide a reference?), otherwise please elaborate. 5. Please provide a reference for the statement that â... no algorithm can yield any non-trivial guarantees if preemptions are not allowedâ (lines 187-188, p. 6, 1st paragraph of Section 3). 6. At the very end of Section 3, you omit a proof for lack of space. If possible, please do include the proof in the revision (at least as supplementary material, but maybe it fits into the main paper after all). 7. Beginning of Section 2.2: should it not be â... must incur a competitive ratio of at most $b$.â (not âat leastâ) ? The worst case would be x=1, then the ratio would be b/1=b, but in all other cases (x>1), the ratio is either b/x or eventually b/b=1 (as soon as x>b). 8. In Section 2.4, perhaps add âas in Theorems 2.2 and 2.3, respectively.â to the sentence âBoth Algorithms 2 and 3 extend naturally to this setting to yield the same robustness and consistency guarantees.â ? Finally, here's a (sorry, quite pedantic) list of typos or formatting improvement suggestions for the authors' consideration: -- line 1: âmachine-learnedâ (hyphen is missing here, but used subsequently) -- l. 13: I think it should be âaimed at tacklingâ, not âaimed to tackleâ -- l. 18: missing comma between âHereâ and âthe effort...â -- The dollar sign (used in the context of the ski rental problem) somehow looks awkward; actually, could you not simply refrain from using a specific currency? -- Regarding preemptions, I think âresumeâ is more common than ârestartâ (ll. 54, 186) -- l. 76: I think it should be âused naivelyâ, not ânaively usedâ -- Proof of Lemma 2.1, last bullet point: to be precise, it should be â â¦ x < b+x-y ... â -- l. 119: comma is missing after âi.e.â -- Proof of Theorem 2.2: There is a âdâ that should probably be a âbâ (ll. 134 and 136) -- Throughout the paper, you use four different ways to typeset fractions: in-line, \frac, \tfrac, and that other one with a slanted âfrac-lineâ. I would appreciate a bit more consistency in that regard. In particular, please avoid using \frac when setting fractions in-line, as in Theorem 2.2, where it messes up the line spacing. -- l. 144 (1st sentence of Sect. 2.3): comma missing after âIn this sectionâ -- p. 5: the second inequality chain violates the textwidth-boundary â can probably easily be fixed be linebreaking before the last equality- rather than the first inequality-sign. Also, âsimilar toâ should be âsimilarly toâ (ll. 157 and 159). -- l. 216, comma missing after âi.e.â -- Lemma 3.2: Statement should start with a âTheâ (and avoid \frac in-line, cf. earlier comment) Given the end of the proof, the statement could also be made with the less-coarse bound, or writing âless thanâ instead of âat mostâ. -- l. 234: insert âtheâ between â...define d(i,j) asâ and âamount of jobâ (perhaps use âportionâ instead of âamountâ, too) -- l. 238: missing comma between âpredictedâ and âthe longer jobâ -- l. 243: suggest replacing âessentiallyâ by âasymptoticallyâ -- l. 249: comma missing after âFinallyâ -- Theorem 3.3: Statement should start with a âTheâ. -- l. 257: comma missing after âe.g.â -- l. 276: hyphenate âwell-modeledâ -- l. 279: comma missing before âwhereâ -- Ref. [11]: use math-mode for e/(e-1), and TCP should be upper-case -- Ref. [14]: âP.N.â, not âPNâ, for Puttaswamy's initials -- Ref.s [15, 16, 17]: Journal/Source ? Page numbers? -- Generally, the reference list is somewhat inconsistent regarding capitalization (in paper and journal titles) and abbreviations (compare, e.g., [11] Proceedings title with [21] Proceedings title).","1. In the section about related work, I was wondering whether there is no related literature from the robust scheduling literature. I am not overly familiar with the area, so I cannot give concrete references, but I am aware that robust scheduling is a very large field, so I would be surprised if there were no robust online scheduling problems/algorithms that are connected to the work at hand in one way or another. Could you please shed some light on this aspect (and provide some reference(s) if appropriate)?",691,0
NIPS_2018_884,NIPS_2018,". It's hard to judge impact in real-world settings when most of the quantitative evaluations are on datasets not representative of complex natural images (e.g. MNIST and NORB). On MNIST, the method shows clear advantages over competing methods. However, even on NORB, where a lot of the deformations can't easily be parameterized, this advantage has turned into being only on par with other leading methods. I think the inclusion of the faces dataset was important for this reason. I was confused for a while what the exact orbit was for each dataset. I kept scanning the text for this. A table of all three datasets and a short note on how orbits were defined and canonical samples selected would make things a lot clearer. Concurrent work. Similar ideas of representation learning through transformation priors have appeared in recent work. I don't think it takes away any novelty from this submission, since judging from the dates these is concurrent works. I just thought I would bring your attention to it: - https://openreview.net/pdf?id=S1v4N2l0- (ICLR 2018) - https://arxiv.org/pdf/1804.01552.pdf (CVPR 2018) Minor comments. - eq. 6: what connects the orbit with this loss? I don't see the connection just yet - eq. 7: ""x_q not in Oxq!=Oxi"" What is this notation ""set1 != set2"" that seems to imply it forms another set (and not a true/false value) line 136: Oxq \not= Oxi, again, I'm not sure about this notation. I understand what it means, but it looks odd to me. I have never seen this as part of set notation before. - eq. 8: where is x_p, x_q, x_c coming from? Shouldn't the summand be $(x_i, x_p, x_q) \in \mathcal{T}$? The canonical sample x_c is still unclear where it comes from. If x_c is the canonical instance for each orbit, then it also changes in the summation. This is not clear from the notation. - line 196: max unpooling transfers the argmax knowledge of maxpooling to the decoder. Do you use this behavior too? - Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11? - line 213: are all feature spaces well-suited for 1-NN? If a feature space is not close to a spherical Gaussian, it may perform poorly. If feature dimensions are individually standardized, it would avoid this issue. - It was a bit unclear how canonical samples were constructed on the face dataset (""least yaw displacement from a frontal pose""). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?","- It was a bit unclear how canonical samples were constructed on the face dataset (""least yaw displacement from a frontal pose""). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?",692,0
NIPS_2018_940,NIPS_2018,"1. Concerning K-L divergence, though it blows up with decreasing variance, it does behave like Euclidean distance between means scaled by the inverse of the variance. 2. Insufficient details for reproducing word embeddings experiments such as the data set used for training, parameter initialization, context window size, number epochs over data set, etc. 3. The experimental results on word embeddings in Tables 1 and 2 are not very compelling as compared to Gaussian embeddings, the chief baseline from the literature. 4. For the Hypernymy experiments some confirmation that negative examples (for training and eval) for a relation (u,v) include (v,u) and (u,u) would be reassure that an apples-to-apples comparison is being made wrt. Nickel and Kiela 2017. Also missing are implementation details like how parameters are initialized, number of negatives per positive, training epochs, etc. Also no explanation of choice behind have one embedding for context and target and RMSProp vs Adagrad in the word embedding experiments. 5. Given the significant improvement over Nickel-Kiela Hypernymy results, there should be some attempt to explain the source of these improvements and tie them back to properties of the proposed embedding. This is critical especially given that the Bures product is symmetric while Hypernym relations are not. That the Mean Rank drops to 1 is especially surprising given this. 6. Some discussion about difference between (6) in Nicekl Kiela and l. 303 loss would be nice. In N-K (6), the positive example contribution does not appear in the denominator whereas in l. 303 it does. Typos: l. 18 'entanglement' -> 'entailment' (I think) l. 155: repetition of text starting on line 146 l. 310: 'embedd' -> 'embed' l. 323: 'interesections' -> 'intersections'","4. For the Hypernymy experiments some confirmation that negative examples (for training and eval) for a relation (u,v) include (v,u) and (u,u) would be reassure that an apples-to-apples comparison is being made wrt. Nickel and Kiela 2017. Also missing are implementation details like how parameters are initialized, number of negatives per positive, training epochs, etc. Also no explanation of choice behind have one embedding for context and target and RMSProp vs Adagrad in the word embedding experiments.",693,0
NIPS_2018_415,NIPS_2018,"WEAKNESS - Some might say the approach (ensemble) may not be novel enough. - Empirical evaluation on other datasets may be desirable, such as WN18(RR) and NELL995. - Connection to complex embeddings may be desirable, which is known to be isomorphic to HolE. COMMENTS This is a solid piece of work that extends HolE, in somewhat straightforward manner in terms of approach, nonetheless it is quite effective. Moreover, theoretical analysis provided is very nice. I would like to see the discussion on its relationship to ComplEx, which has recently been shown to be isomorphic to HolE [Hayashi and Shimbo, ACL 2017], on which HolEx is based --- this is because ComplEx can be computed in linear time while HolE is not. Some minor suggestions: Since there is space on the left and right side of Table 1, please report Mean Reciprocal Rank and HITS@1, 5 as well, for convenience of comparison with existing work that only reports these numbers. Regarding Remark 1, could you clarify the situation of the numbers for ProjE in Table 1? I checked the GitHub page for ProjE but could not find the numbers on the table there (although there is a discussion in Issue tracker but no numbers are present.)","- Empirical evaluation on other datasets may be desirable, such as WN18(RR) and NELL995.",694,0
NIPS_2018_801,NIPS_2018,"- Even though the meta-learning procedure is strived to become as cheap as possible, it is still impossible to be tried by the research groups with small to moderate available computational resources: ""We deploy the resulting proxy task with our proposed architecture search space on Cityscapes to explore 28K DPC architectures across 370 GPUs over one week."" - The selected space search algorithm (random search) might not to be the most optimal one. Further improvements are possible by incorporating search algorithms that can achieve good solutions more efficiently (e.g. by better incorporating prior knowledge about the search process) Other (minor) remarks: - It would have been useful to get more insights on the objective function landscape. For instance, the distribution of fitness of the explored architectures could have given information about the amount of variations in the objective function, or a diagram of best objective value based on number of architectures explored, could have indicated how far the search needed to continue. - As mentioned in sections 4.4 and 4.5, the same best found DPC on the cityscapes dataset is used for the PASCAL VOC and PASCAL person part datasets and so no dataset-specific meta-learning is performed. However, it is interesting to assess how well the (close-to-)optimal structure in one dataset can generalize to be a (close-to-)optimal structure in the other ones. This assessment becomes relatively important given the intractability of meta-learning for new dataset for majority of researchers.","- It would have been useful to get more insights on the objective function landscape. For instance, the distribution of fitness of the explored architectures could have given information about the amount of variations in the objective function, or a diagram of best objective value based on number of architectures explored, could have indicated how far the search needed to continue.",695,0
NIPS_2018_917,NIPS_2018,"- Results on bAbI should be taken with a huge grain of salt and only serve as a unit-test. Specifically, since the bAbI corpus is generated from a simple grammar and sentence follow a strict triplet structure, it is not surprising to me that a model extracting three distinct symbol representations from a learned sentence representation (therefore reverse engineering the underlying symbolic nature of the data) would solve bAbI tasks. However, it is highly doubtful this method would perform well on actual natural language sentences. Hence, statements such as ""trained [...] on a variety of natural language tasks"" is misleading. The authors of the baseline model ""recurrent entity networks"" [12] have not stopped at bAbI, but also validated their models on more real-world data such as the Children's Book Test (CBT). Given that RENs solve all bAbI tasks and N2Ns solve all but one, it is not clear to me what the proposed method adds to a table other than a small reduction in mean error. Moreover, the N2N baseline in Table 2 is not introduced or referenced in this paper, so I am not sure which system the authors are referring to here. Minor Comments - L11: LSTMs have only achieved on some NLP tasks, whereas traditional methods still prevail on others, so stating they have achieved SotA in NLP is a bit too vague. - L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253). - Eq. 1: This seems like a very specific choice of combining the information from entity representations and their types. Why is this a good choice? Why not keep the concatenation of the kitty/cat outer product and the mary/person outer product? Why is instead the superposition of all bindings a good design choice? - I believe section four could benefit from a small overview figures illustrating the computation graph that is constructed by the method. - Eq. 7: At first, I found it surprising why three distinct relation representation are extracted from the sentence representation, but it became clearer later with the write, move and backling functions. Maybe already mention at this point why the three relation representations are going to be used for. - Eq. 15: s_question has not been introduced before. I imagine it is a sentence encoding of the question and calculated similarly to Eq. 5? - Eq. 20: A bit more details for readers unfamiliar with bAbI or question answering would be good here. ""valid words"" here means possible answer words for the given story and question, correct? - L192: ""glorot initalization"" -> ""Glorot initialization"". Also, there is a reference for that method: Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). - L195: Î±=0.008, Î²â=0.6 and Î²â=0.4 look like rather arbitrary choices. Where does the configuration for these hyper-parameters come from? Did you perform a grid search? - L236-244: If I understand it correctly, at test time stories with new entities (Alex etc.) are generated. How does your model support a growing set of vocabulary words given that MLPs have parameters dependent on the vocabulary size (L188-191) and are fixed at test time? - L265: If exploding gradients are a problem, why don't you perform gradient clipping with a high value for the gradient norm to avoid NaNs appearing? Simply reinitializing the model is quite hacky. - p.9: Recurrent entity networks (RENs) [12] is not just an arXiv paper but has been published at ICLR 2017.",- Eq.15: s_question has not been introduced before. I imagine it is a sentence encoding of the question and calculated similarly to Eq. 5?,696,0
NIPS_2018_612,NIPS_2018,"weakness is not including baselines that address the overfitting in boosting with heuristics. Ordered boosting is non-trivial, and it would be good to know how far simpler (heuristic) fixes go towards mitigating the problem. Overall, I think this paper will spur new research. As I read it, I easily came up with variations and alternatives that I wanted to see tried and compared. DETAILED COMMENTS The paper is already full of content, so the ideas for additional comparisons are really suggestions to consider. * For both model estimations, why start at example 1? Why not start at an example that is 1% of the way into the training data, to help reduce the risk of high variance estimates for early examples? * The best alternative I've seen for fixing TS leakage, while reusing the data sample, uses tools from differential privacy [1, 2]. How does this compare to Ordered TS? * Does importance-sampled voting [3] have the same target leakage problem as gradient boosting? This algorithm has a similar property of only using part of the sequence of examples for a given model. (I was very impressed by this algorithm when I used it; beat random forests hands down for our situation.) * How does ordered boosting compare to the subsampling trick mentioned in l. 150? * Yes, fixes that involve bagging (e.g., BagBoo [4]) add computational time, but so does having multiple permuted sequences. Seems worth a (future?) comparison. * Why not consider multiple permutations, and for each, split into required data subsets to avoid or mitigate leakage? Seems like it would have the same computational cost as ordered boosting. * Recommend checking out the Wilcoxon signed rank test for testing if two algorithms are significantly different over a range of data sets. See [6]. * l. 61: ""A categorical feature..."" * l. 73: ""for each categorical *value*"" ? * l. 97: For clarity, consider explaining a bit more how novel values in the test set are handled. * The approach here reminds me a bit of Dawid's prequential analysis, e.g., [5]. Could be worth checking those old papers to see if there is a useful connection. * l. 129: ""we reveal"" => ""we describe"" ? * l. 131: ""called ordered boosting"" * l. 135-137: The ""shift"" terminology seems less understandable than talking about biased estimates. * l. 174: ""remind"" => ""recall"" ? * l. 203-204: ""using one tree structure""; do you mean shared \sigma? * Algorithm 1: only one random permutation? * l. 237: Don't really understand what is meant by right hand side of equality. What is 2^j subscript denoting? * l. 257: ""tunning"" => ""tuning"" * l. 268: "", what is expected."" This reads awkwardly. * l. 311: This reference is incomplete. REFERENCES [1] https://www.slideshare.net/SessionsEvents/misha-bilenko-principal-researcher-microsoft [2] https://www.youtube.com/watch?v=7sZeTxIrnxs [3] Breiman (1999). Pasting small votes for classification in large databases and on-line. Machine Learning 36(1):85--103. [4] Pavlov et al. (2010). BagBoo: A scalable hybrid bagging-the-boosting model. In CIKM. [5] Dawid (1984). Present position and potential developments: Some personal views: Statistical Theory: The Prequential Approach. Journal of the Royal Stastical Society, Series A, 147(2). [6] Demsar (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1--30.","* Why not consider multiple permutations, and for each, split into required data subsets to avoid or mitigate leakage? Seems like it would have the same computational cost as ordered boosting.",697,0
NIPS_2018_30,NIPS_2018,"- As mentioned in section 3.2, MetaAnchor does not show significant improvement for two-stage anchor-based object detection. - The experiment evaluation is only done for one method and on one dataset. It is not very convincing that MetaAnchor is able to work with most of the anchor-based object detection system. Rebuttal Response: Overall, I think this approach has value on one-stage frameworks. The experiment is only on COCO, but performed in an extensive way. It would be great to have more consistently good results on another benchmark dataset such as OpenImage. But for this submission now, I am willing the upscale my score from 4 to 6.","- The experiment evaluation is only done for one method and on one dataset. It is not very convincing that MetaAnchor is able to work with most of the anchor-based object detection system. Rebuttal Response: Overall, I think this approach has value on one-stage frameworks. The experiment is only on COCO, but performed in an extensive way. It would be great to have more consistently good results on another benchmark dataset such as OpenImage. But for this submission now, I am willing the upscale my score from 4 to 6.",698,0
NIPS_2018_25,NIPS_2018,"- My understanding is that R,t and K (the extrinsic and intrinsic parameters of the camera) are provided to the model at test time for the re-projection layer. Correct me in the rebuttal if I am wrong. If that is the case, the model will be very limited and it cannot be applied to general settings. If that is not the case and these parameters are learned, what is the loss function? - Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned. - ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper. - During evaluation at test time, how is the 3D alignment between the prediction and the groundtruth found? - Please comment on why the performance of GTSeeNet is lower than that of SeeNetFuse and ThinkNetFuse. The expectation is that groundtruth 2D segmentation should improve the results. - line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD? - What does NoSeeNet mean? Does it mean D=1 in line 96? - I cannot parse lines 113-114. Please clarify.",- line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD?,699,0
NIPS_2018_888,NIPS_2018,"weakness of the paper is that some parts are a bit vague or unclear, and that it contains various typos or formatting issues. I shall provide details in the following list; typos I found will be listed afterwards. 1. In the section about related work, I was wondering whether there is no related literature from the robust scheduling literature. I am not overly familiar with the area, so I cannot give concrete references, but I am aware that robust scheduling is a very large field, so I would be surprised if there were no robust online scheduling problems/algorithms that are connected to the work at hand in one way or another. Could you please shed some light on this aspect (and provide some reference(s) if appropriate)? 2. From the description in the âmain resultsâ section of the Introduction, the precise connection between competitive ratio and robustness or consistency did not become entirely clear to me. Please provide some additional clarification on what is to be compared here â in particular, I also stumbled over why (as stated in several corresponding theorems) one ends up with competitive ratios âat most min{ robustness-ratio, consistency-ratio }â. At first glance, I had thought that if an algorithm does well for good predictions but the robustness bound is bad, that the latter should dominate. As this is apparently not the case, I ask for a brief explanation/clarification, preferably already in the introduction, as to why the minimum of the two values yields a bound on the competitive ratio. 3. In Algorithm 3, please clarify what âchosenâ means exactly. (Is it âdrawn uniformly at random from the k (or l, resp.) values q_i (r_i, resp.)â ? Or âchosen as index yielding maximal q_i/r_i-valueâ? Or something else?) Also, on a higher level, what makes q_i and r_i probability distributions? 4. On p.5, line 151 (multi-line inequality chain): I could not immediately verify the 3rd equation (in the middle line of the chain) â if it is based on a known formula, I do not recall it (maybe provide a reference?), otherwise please elaborate. 5. Please provide a reference for the statement that â... no algorithm can yield any non-trivial guarantees if preemptions are not allowedâ (lines 187-188, p. 6, 1st paragraph of Section 3). 6. At the very end of Section 3, you omit a proof for lack of space. If possible, please do include the proof in the revision (at least as supplementary material, but maybe it fits into the main paper after all). 7. Beginning of Section 2.2: should it not be â... must incur a competitive ratio of at most $b$.â (not âat leastâ) ? The worst case would be x=1, then the ratio would be b/1=b, but in all other cases (x>1), the ratio is either b/x or eventually b/b=1 (as soon as x>b). 8. In Section 2.4, perhaps add âas in Theorems 2.2 and 2.3, respectively.â to the sentence âBoth Algorithms 2 and 3 extend naturally to this setting to yield the same robustness and consistency guarantees.â ? Finally, here's a (sorry, quite pedantic) list of typos or formatting improvement suggestions for the authors' consideration: -- line 1: âmachine-learnedâ (hyphen is missing here, but used subsequently) -- l. 13: I think it should be âaimed at tacklingâ, not âaimed to tackleâ -- l. 18: missing comma between âHereâ and âthe effort...â -- The dollar sign (used in the context of the ski rental problem) somehow looks awkward; actually, could you not simply refrain from using a specific currency? -- Regarding preemptions, I think âresumeâ is more common than ârestartâ (ll. 54, 186) -- l. 76: I think it should be âused naivelyâ, not ânaively usedâ -- Proof of Lemma 2.1, last bullet point: to be precise, it should be â â¦ x < b+x-y ... â -- l. 119: comma is missing after âi.e.â -- Proof of Theorem 2.2: There is a âdâ that should probably be a âbâ (ll. 134 and 136) -- Throughout the paper, you use four different ways to typeset fractions: in-line, \frac, \tfrac, and that other one with a slanted âfrac-lineâ. I would appreciate a bit more consistency in that regard. In particular, please avoid using \frac when setting fractions in-line, as in Theorem 2.2, where it messes up the line spacing. -- l. 144 (1st sentence of Sect. 2.3): comma missing after âIn this sectionâ -- p. 5: the second inequality chain violates the textwidth-boundary â can probably easily be fixed be linebreaking before the last equality- rather than the first inequality-sign. Also, âsimilar toâ should be âsimilarly toâ (ll. 157 and 159). -- l. 216, comma missing after âi.e.â -- Lemma 3.2: Statement should start with a âTheâ (and avoid \frac in-line, cf. earlier comment) Given the end of the proof, the statement could also be made with the less-coarse bound, or writing âless thanâ instead of âat mostâ. -- l. 234: insert âtheâ between â...define d(i,j) asâ and âamount of jobâ (perhaps use âportionâ instead of âamountâ, too) -- l. 238: missing comma between âpredictedâ and âthe longer jobâ -- l. 243: suggest replacing âessentiallyâ by âasymptoticallyâ -- l. 249: comma missing after âFinallyâ -- Theorem 3.3: Statement should start with a âTheâ. -- l. 257: comma missing after âe.g.â -- l. 276: hyphenate âwell-modeledâ -- l. 279: comma missing before âwhereâ -- Ref. [11]: use math-mode for e/(e-1), and TCP should be upper-case -- Ref. [14]: âP.N.â, not âPNâ, for Puttaswamy's initials -- Ref.s [15, 16, 17]: Journal/Source ? Page numbers? -- Generally, the reference list is somewhat inconsistent regarding capitalization (in paper and journal titles) and abbreviations (compare, e.g., [11] Proceedings title with [21] Proceedings title).","3. In Algorithm 3, please clarify what âchosenâ means exactly. (Is it âdrawn uniformly at random from the k (or l, resp.) values q_i (r_i, resp.)â ? Or âchosen as index yielding maximal q_i/r_i-valueâ? Or something else?) Also, on a higher level, what makes q_i and r_i probability distributions?",700,0
NIPS_2018_2,NIPS_2018,"Weakness: 1. Notation confusion. In equation (1) and theorem #1, x_i denotes a local input from (i-m)-th vertex to (i+m)-th vertex. However, in equation (6), a same x_i represents a single value (with multi-channel) at i-th vertex only. 2. The detailed information for graph construction is missing, e.g. the way to define edges and construct a graph for non-euclidean datasets. 3. Despite the proposed method shares a different idea with [17], the P_i matrix in theorem #1 mathematically transforms the input x_i into a high-dimensional space where a convolution weights v are applied. Can the authors briefly compares the proposed method with [17]? My feeling is that [17] should be a good baseline to compare on Euclidean datasets. 4. It seems that the baselines in Experiment 5.2 are not the original models, but the authors' own implementations, e.g. ActCNNs[18] has 93% accuracy on Cifar10 instead of 90%. Can the authors comment about this? 5. It would be good if the author can compare number of parameters between the proposed network and used baselines. Minor comments: 1. In figure 3(f), the improvement looks saturated when using 56 layers network. Will the proposed method also benefit deeper network (e.g. resnet110 in Cifar10)? 2. It would be interesting if the authors would comment on how their approach can be applied to point cloud dataset. Conclusion: I hope the authors will response the above questions in the rebuttal and include these in the final version. Given the novelty of the approach and extensive experiments, the submission would be stronger if the authors could fix these flaws.","1. In figure 3(f), the improvement looks saturated when using 56 layers network. Will the proposed method also benefit deeper network (e.g. resnet110 in Cifar10)?",701,0
NIPS_2018_799,NIPS_2018,"- It would help to have some element of theoretical analysis on the chosen definition of the importance measures Z (Sec. 3). For example, deriving this as an explicit regularizer on the layer weights. - The framework (proposed as a general feature selection method?), is restricted to Gaussian design and the use of an estimated precision matrix, for which there might not be any robustness guarantees. Some discussion (or paths for alternative designs?) might be helpful. - There is some disconnect, in my opinion, between motivating this work as a feature selection method for NN and using a NN as a way to measure importance (given a prediction task). - The empirical validation, even though convincing in some respect: the benefit of the novelty here (i.e. adding a pairwise connected layer vs. having a vanilla MLP) is only explored on the synthetic data. Specific comments: - The method is motivated by the need for interoperability in NN and error control in NN, though only the latter is explored (in a synthetic scenario). - Similarly, the title states âreproducibleâ feature selection and this might something that needs further explaining. In what sense reproducible? - Did the authors explore/consider different definitions of the importance measures on the knockoff statistics? - What is the dependency of some aspects of the performance on the knock-off feature generation (e.g., s for the Gaussian case)? Is there a way to make vanilla MLP + knockoff features better by changing (or tuning) s? - Is there a specific rationale for fixing the number of neurons in the hidden layer to be p (equal to the input dimension)? As long as the output layer is p-dimensional the reasoning should still hold with a different âfactorizationâ of the input to output mapping that defines w. The same holds for using L1 regularization (and with a specific regularization parameter). - How dependent on the specific parametrization of the MLP (or NN more general) is the performance? - The FDR target error level is fixed/set to q=0.2 everywhere. Would it be insightful to vary this and re-interpret, e.g. the results of Sec. 5.1? - It would be helpful to see some prediction results with real data, based on the selected features. E.g. a two-pass process where the NN is re-trained on the selected, error-control features to predict the target function values or class. - How much False Discovery errors are relevant for NN performance in practical applications (and in high-dimensions)? An open question that might be worth discussing. - Did the authors try the knockoffs+vanilla MLP approach on the real data in 5.1 (or the qualitative analysis in 5.2)? The pairwise-connected layer is the novelty of this paper and this should be further highlighted. ========================= Comments after author feedback: Thank you for the replies to all reviews and most of the comments. I agree with the points of your clarifications and can see the value of the points made in the response. I also agree with Reviewer 1 that this can inspire future work by the authors and others. I am changing my score to a 6 to reflect these. At the same time my main concern on general applicability remains, and in some cases has been strengthened by some of the responses. - âin the sense that small changes in the data can yield dramatic changes in the selected features. â If that is the case, perturbations in the data (and thus reproducibility of the framework) are not explored. What type of changes? - âWe consider the primary virtue of the robustness to be improved reproducibility of the selected features;â As above. I donât feel that this aspect of the authorsâ work is highlighted or supported experimentally in the paper. - âGiven the universal approximation property of DNNs, practitioners can now avoid having to handcraft feature importance measures with the help of DeepPINK.â This is a good point, thank you for making this connection explicit. Is the addition of DeepPINK layer however âalteringâ the network definition, i.e. are you selecting features for the original network or the modified one? How dependent is this selection on initialization, parametrization, and as the authors state, changes in the input data (e.g. transformations, subsampling, noise etc). - âusing interpretability alone as a measurement is far less reliable than using FDR and power with synthetic data as measurements.â This is also a good point, and provides statistical guarantees (as Reviewer 1 also points out). - âit is the fundamental architecture (instead of the tuning) of vanilla MLP that prevents the knockoff filter from performing well.â I am not sure what this claim implies. That MLP is not a good choice for being subjected to knockoff filters? This defies the purpose of the paper. Or that knockoffs can only add value coupled in the learning process (like for DeepPINK)? I wonder, and I made the same point in my initial review on which the authors replied, how much those findings are generally applicable to any âneural networkâ parametrization or just MLP-type. How about basic linear or (shallow) non-linear regression with the PINK coupling and solved through SGD? - âSince there is no existing method available for robust feature selection in DNNs, any effective methods would be significant.â I agree. However the idea of input features and feature representation (hidden layers) is coupled in NNs. As the authors point out, interpretability and selection is conditioned on the prediction algorithm. So this is not âfeature selectionâ, but input dimension selection. One could envision however a PINK layer attached on higher hidden layers for representation dimension selection, interpretability and transferability. - âThe proposed network architecture may seem simple, but our journey to finding it was nontrivial.â So then does the paper involve a single, fixed architecture? Or is it generally applicable for any parametrization that one can add on top of the pairwise linear layer?","- There is some disconnect, in my opinion, between motivating this work as a feature selection method for NN and using a NN as a way to measure importance (given a prediction task).",702,0
NIPS_2018_109,NIPS_2018,"that limit the contribution. In particular: 1) the method does not seem to improve on the robustness and sensitivity as claimed in the motivation of using evolutionary methods in the first place. In fig 3 the new method is noisier in 2 domains and equally noisy as the competitors in the rest. 2) The paper claims SOTA in these domains compared to literature results. The baseline results reported in the paper under review in HalfCheetah, Swimmer and Hopper are worse the state of the art reported in the literature [1,2]. Either because the methods used achieved better results in [1,2] or because the SOTA in the domain was TRPO which was not reported in the paper under review. SOTA is a big claim; support it carefully. 3) There is significant over-claiming throughout the paper. E.g line 275 ""best of both approaches"", line 87 ""maximal information extraction"" 4) It is not clear why async methods like A3C were not discussed or compared against. This is critical given the SOTA claim 5) The paper did not really attempt to tease apart what was going on in the system. When evaluating how often was DDPG agent chosen for evaluation trials or did you prohibit this? What does the evaluation curve look like for the DDPG agent? That is do everything you have done, but evaluate the DDPG agent as the candidate from the population. Or allowing the population to produce the data for the DDPG and training it totally off-policy to see how well the DDPG learnings (breaking the bottom right link in fig 1). Does adding more RL agents help training? The paper is relatively clear and the is certainly original. However my concerns above highlight potential issues with quality and significance of the work. [1] https://arxiv.org/abs/1709.06560 [2] https://arxiv.org/pdf/1708.04133.pdf ++++++++++++ Ways to improve the paper that did not impact the scoring above: - you have listed some of the limitations of evolutionary methods, but I think there are much deeper things to say regarding leveraging state, reactiveness, and learning during an episode. Being honest and direct would work well for this work - the title is way to generic and vague - be precise when being critical. What does ""brittle convergence properties mean"" - I would say DeepRL methods are widely adopted. Consider the landscape 10 years ago. - claim V-trace is too expensive. I have no idea why - its important to note that evolutionary methods can be competitive but not better than RL methods - discussion starting on line 70 is unclear and seems not well supported by data. Say something more plain and provide data to back it up - definition of policy suggests deterministic actions - not sure what state space s = 11 means? typo - section at line 195 seems repetitive. omit","3) There is significant over-claiming throughout the paper. E.g line 275 ""best of both approaches"", line 87 ""maximal information extraction""",703,0
NIPS_2018_112,NIPS_2018,"I have some questions on the experiments. - Missing baseline: Performances when removing L_id, L_pd, L_r and performances when only removing one of the not share E, L_sp, L_v are not evaluated. - How the experimental comparison with DR-GAN was done is not clear enough since DR-GAN does not use the pose map for input data. - The proposed method does not require the pose estimation in the inference, so test image does not have pose maps. It is not clear if the visual analysis on Sec.4.4 is the result on training images or test images. - Why the generated images of Figure 4 (a) shows only one of the [18],[19] for each input image? - Why the [18][19] fails and the proposed method succeed? Several parts of this paper are hard to understand. -The names âIdentity discriminatorâ and âpose discriminatorâ are confusing. When reading Fig.2, it looks like these losses are to distinguish different identities and poses. However, Eq.(2) seems to be the loss to distinguish real vs. fake images (of the same person), not to distinguish between different identities. Similarly, the âpose discriminatorâ is not to distinguish between different poses. - Line29: What is LOSO regularization? - I suggest replacing some arXiv papers with actual published works. Response to Rebuttal: The rebuttal addressed my concerns about the experiments. However, the additional ablation study was only on Market-1501 dataset, and DukeMTMC-reID dataset should be included in the final version. Also, the explanation of identity/pose discriminators should be carefully revised according to the response.",- How the experimental comparison with DR-GAN was done is not clear enough since DR-GAN does not use the pose map for input data.,704,0
NIPS_2018_630,NIPS_2018,"- While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score.",- I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize?,705,0
NIPS_2018_283,NIPS_2018,"1) The implementation of l_Î¸1 and g_Î¸2 are stacked by multiple neural layers, which not naturally satisfy the local-Lipschitz-continuity in Proposition 1 (As it mentioned in line 136-137). The author proposed to smooth the mapping by add Gaussian noise to training data. But I donât see how this trick can guarantee the satisfaction. Thus, thereâs risk of non-converging iteration. 2) Why l_Î¸1 and g_Î¸2 satisfy Lipschitz constant L<1? The sentence âif the initial start S(0) surrounds â¦ sufficientlyâ in Proposition 1 is a little bit vague. 3) The proposed method stands on an important assumption that thereâs a one-to-one correspondence of caption sentence and temporal segment in video. This mainly relates to what data is used during experiments. Thus, it would be worthwhile to show some evidence why this assumption is valid. 4) During test time, a random initial point is needed.","3) The proposed method stands on an important assumption that thereâs a one-to-one correspondence of caption sentence and temporal segment in video. This mainly relates to what data is used during experiments. Thus, it would be worthwhile to show some evidence why this assumption is valid.",706,0
NIPS_2018_433,NIPS_2018,"- This is not actually an OMP algorithm as the authors claim, because their algorithm selects for multiple orthogonal bases, instead of just one. This is worth mentioning since it means the intuition that we have from the analysis of OMP, that it will converge to a minimum, does not hold here - The authors analyze their algorithm assuming that the basis functions are orthonormal, but it is not clear that the experiments conformed to this assumption: are the basis functions obtained from the Taylor Series expansion of the gaussian kernel orthonormal? The authors should make it clear either way. - The theory is provided for hard constraints on the mixture coefficients, while the experimental results are given for a lagrangian relaxation. It is possible to obtain theory that applies directly to the lagrangian relaxation - I would like to have seen a comparison with a fast randomized feature map such as FastFood: the authors compare to the basic random Fourier Feature map, which is much slower because it uses more iid random variables. I believe the practice is to use FastFood. - The graphs are illegible when the paper is printed in black and white. It would be preferable to use markers and different line styles to help avoid this issue.",- The graphs are illegible when the paper is printed in black and white. It would be preferable to use markers and different line styles to help avoid this issue.,707,0
NIPS_2018_296,NIPS_2018,"weakness of the proposed approach. Model-based algorithms (LevinTS is model-based) for planning do not have such requirements. On the other hand, if the goal is to refine a policy at the end of some optimization procedure I understand the choice of using a policy-guided heuristic. - Concerning LubyTS it is hard to quantify the meaning of the bound in Thm. 6 (the easy part is to see when it fails, as mentioned before). There are other approaches based on generative models with guarantees, e.g., (Grill, Valko, Munos. Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning, NIPS 2016). How do you perform compared to them? - Your setting is very specific: you need to know the model or/and have access to a generative model (for expanding or generating trajectories), the problem should be episodic and the reward should be given just at the end of a task (i.e., reaching the target goal). Can you extend this approach to more general settings? - In this paper, you perform planning offline since you use a model-based approach (e.g., generative model). Is it possible to remove the assumption of the knowledge of the model? In that case, you would have to interact with the environment trying to minimize the number of times a ""bad"" state is visited. Since the expansion of a node can be seen as an exploratory step, this approach seems to be related to the exploration-exploitation dilemma. Bounding the number of expansions does not correspond to having small regret in general settings. Can you integrate a concept related to the long-term performance in the search strategy? This is what is often done in MCTS. All the proposed approaches have weaknesses that are partially acknowledged by the authors. A few points in the paper can be discussed in more details and clarified but I believe it is a nice contribution overall. -------- after feedback I thank you for the feedback. In particular, I appreciated the way you addressed the zero probability issue. I think that this is a relevant aspect of the proposed approaches and should be addressed in the paper. Another topic should be added to the paper is the comparison with Trailblazer and other MCTS algorithms. Despite that, I personally believe that the main limitation is the assumption of deterministic transitions. While the approach can be extended to stochastic (known) models I would like to know how the theoretical results become.",- Concerning LubyTS it is hard to quantify the meaning of the bound in Thm.,708,0
NIPS_2018_559,NIPS_2018,"- my only objection to the paper is that it packs up quite a lot of information, and because of the page-limits it doesnât include all the details necessary to reconstruct the model. This means cuts were made, some of which are not warranted. Sure, the appendix is there, but the reader needs to get all the necessary details in the main body of the paper. I quite enjoyed the paper, I think itâs definitely NIPS material, but it needs some additional polishing. I added my list of suggestions I think would help improve readability of the paper at the end of the review. Questions: - I might have missed the point of section 4.2 - I see it as a (spacewise-costly) way to say âprograms (as opposed to specs) are a better choice as they enable generalization/extrapolation via changing variable valuesâ? What is the experiment there? If itâs just to show that by changing variables, one can extrapolate to different images, I would save space on 1/2 of Figure 9 and focus on lacking parts of the paper (190 - extrapolations produced by our system - how did the system produce those extrapolations? was there a human that changed variable values or is there something in the system enabling this?) - What is + in Figure 3? If elementwise addition, please specify that - Figure 4 caption explains why the number N of particles is not the same across models. However, that still doesnât stop me from wondering whether there is a significant difference in performance in case all models are using the same number of particles. Do you have that information? - Line 54 mentions that the network can âderenderâ images with beam search. Is beam search used or not? What is the size of the beam? Is beam search used for each of the N particles? - From what I understood, the model does not have access to previously generated commands. Can you confirm that? - The order of (generated) specs is irrelevant for rendering, but it is for the generation process. How do you cope with that? Do you use a particular order when training the model or do you permute the specs? - Table 2 - â;â denotes OR, right? I would personally use the BNF notation here and use â|â - 153 - âminimized 3 using gradient descentâ - how did you treat the fact that min is not differentiable? - Table 5 - this is evaluated on which problems exactly? The same 100 on which the policy was trained? - Please provide some DeepCoder -style baseline details - the same MLP structure? Applied to which engine? A search algorithm or Sketch? - I find 152 - 153 unclear - how did you synthesize minimum cost programs for each \sigma ? \sigma represents a space of possible solutions, no? - Please provide more details on how you trained L_learned - what is the dataset you trained it on (randomly selected pairs of images, sampled from the same pool of randomly generated images, with a twist)? How did you evaluate its performance? What is the error of that model? Was it treated as a regression or as a classification task? - Figure 7 introduces IoU. Is that the same IoU used in segmentation? If so, how does that apply here? Do you count the union/intersection of pixels? Please provide a citation where a reader can quickly understand that measure. Suggestions: - full Table 3 is pretty, but it could easily be halved to save space for more important (missing!) details of the paper - the appendix is very bulky and not well structured. If you want to refer to the appendix, I would strongly suggest to refer to sections/subsections, otherwise a reader can easily get lost in finding the details - Section 2.1 starts strong, promising generalization to real hand drawings, but in the first sentence the reader realizes the model is trained on artificial data. Only in line 91 it says that the system is tested on hand-written figures. I would emphasize that from the beginning. - Line 108 - penalize using many different numerical constants - please provide a few examples before pointing to the supplement. - Line 154 - a bit more detail of the bilinear model would be necessary (how low-capacity?) - 175-176 - see supplement for details. You need to provide some details in the body of the paper! I want to get the idea how you model the prior from the paper and not the supplement - Table 5 - thereâs a figure in the appendix which seems much more informative than this Table, consider using that one instead The related work is well written, I would just suggest adding pix2code (https://arxiv.org/abs/1705.07962) and SPIRAL (https://arxiv.org/abs/1804.01118) for completeness. UPDATE: I've read the author feedback and the other reviews. We all agree that the paper is dense, but we seem to like it nevertheless. This paper should be accepted, even as is because it's a valuable contribution, but I really hope authors will invest additional effort into clarifying the parts we found lacking.",- Please provide some DeepCoder -style baseline details - the same MLP structure? Applied to which engine? A search algorithm or Sketch?,709,0
NIPS_2018_539,NIPS_2018,"Weakness: - Although authors try to explain the STABLE model in a Bayesian framework using variational lower bound, as we see in the final form of the algorithm, it doesn't seem different from a simple regularization method illustrated in Section 3.2.1 when discriminator in Figure 3 corresponds to L2 distance. - The experimental protocol seems favorable to the proposed method and unfair to previous methods such as loss correction or S-adaptation. For example, one may inject noise transition prior on S-adaptation by setting corresponding \theta values to zero and renormalize after each update. - In addition, as illustrated in 3.2.1, one can easily include regularization to train noise transition matrix to follow the prior. It doesn't seem like a valid excuse to exclude regularization method from comparison due to the difficulty of experimentation when the proposed method may also involve some hyperparameters that balances training between reconstructor, generator and discriminator. - Experimental validation is not satisfactory as it is only evaluated with artificially generated noise transition constraints.",- Experimental validation is not satisfactory as it is only evaluated with artificially generated noise transition constraints.,710,0
NIPS_2019_703,NIPS_2019,"of their work? The submission is of very high quality. Max-value entropy search is well motivated, demonstrably works well in practice. The authors deserve credit for the careful, high quality experiments. Furthermore, the paper provides a theoretical guarantee on the performance, although I lack the expertise to properly judge the significance of this. (2) Clarity: Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.) The paper is an excellent read. It presents the algorithm in a structured way, with sufficient detail for reproduction. I would recommend it for reading to anyone interested in the general topic. (3) Originality: Are the tasks or methods new? Is the work a novel combination of well-known techniques? Is it clear how this work differs from previous contributions? Is related work adequately cited? As mentioned above, it is clear that this work was heavily inspired by MES (Wang and Jegelka, 2017). This work does not adequately cite related works. My problem is that the way this submission puts it, 'our work is inspired by the recent success of single objective BO algorithms based on the idea of optimizing output-space information gain', might give the wrong impression to the reader. MESMO is the natural extension of MES (Wang and Jegelka, 2017) to the multiobjective domain. It also positions MESMO against PESMO and it compares/contrasts the two. This comparison might make it seem like the contribution is greater than it is in reality. On multiple occasion, the paper presents work without disclosing the strong connection to (Wang and Jegelka, 2017). Examples: - Section 4, Equations 4.4-4.6. These equations literally come from (Wang and Jegelka, 2017). - Section 4 1) and 2), The algorithm this paper uses to approximate MESMO is the same algorithm as in (Wang and Jegelka, 2017). - Section 4.1 The theoretical result is analogous to Section 3.4 of (Wang and Jegelka, 2017). Based on the arguments above, I will rephrase the question: Are the ideas in this paper novel enough to warrant a new publication over (Wang and Jegelka, 2017)? I would argue that they are not. The reason is that there does not seem to be a significant hurdle that this work needed to overcome in order to extend MES to the multiobjective domain. Both the formula for MESMO and the algorithm to approximate it extend to multiobjective problems. I cannot comment on Theorem 1. (4) Significance: Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach? The method is certainly interesting to practitioners. The work demonstrates the performance of MESMO on a variety of benchmarks and it consistently outperforms the baselines. _________________________________________________________________________ After reading the author's reply, I decided to raise my score. I expect that the connection to (Wang and Jegelka, 2017) is properly disclosed in the revised version. The paper is very well written but it has shortcomings in originality. The explanation for my score is that I think a practitioner might find this paper useful, but I don't expect it to have a large impact in the research community. I think this paper is truly borderline. I do not have strong arguments for or against acceptance.","- Section 41) and2), The algorithm this paper uses to approximate MESMO is the same algorithm as in (Wang and Jegelka, 2017).",711,0
NIPS_2019_483,NIPS_2019,"weakness of this work is in its experiments. The results shown in Figs. 2 & 5 seem unimpressive, and this work contains zero comparisons to any other methods or variants of the model. It is unacceptable that it does not include a comparison to explicit model learning. The results in Figs. 4 & 6 are qualitatively interesting but somewhat hard to interpret; they seem to indicate that the model which is learned is only vaguely related to ground-truth prediction. The comparison between architectures in Fig. 5 shows no significant difference; furthermore, without sharing the amount of data used to train the two architectures it is impossible to evaluate (as inductive biases will be washed out with sufficient data). The number of environment steps used may be computable from the number of training generations, etc in the Appendix but should be explicitly stated in the main body. It is also clear that with an expressive policy which is able to distinguish between real and generated observations, there is no reason at all that the implicit ""model"" should need to make forward predictions at all. In that case the policy as a whole reduces to a recurrent neural network policy. It would be important to include a discussion of this limitation. Clarity The writing quality in this work is high and I enjoyed reading it. However, there are a few details which could use elaboration: - All experiments should include the number of environment samples used. - The observation space for the cartpole example is not explicitly stated. Significance Currently the significance of this paper is low-medium. It has a clever idea but it does not establish it well enough to motivate follow-on work by others.",- The observation space for the cartpole example is not explicitly stated. Significance Currently the significance of this paper is low-medium. It has a clever idea but it does not establish it well enough to motivate follow-on work by others.,712,0
NIPS_2019_899,NIPS_2019,"Weakness: - Latent language seems to add a more complex intermediate problem. You are now introducing text understanding which might be a harder problem. Do we really need text? why not use a program for guidance? Surely, a program is more expressive than macro-action and interpretable and you dont have language understanding challenges. Maybe a clever data collection strategy can collect programs. - The problem is solved using supervised learning without any exploration based learning. This makes me wonder how easy the setup is. Did you try comparing against agents that are trained without text but use reinforcement learning? The environment must have some reward (score, number of enemies killed etc.). Of course, once you consider exploration it is not clear how accurate your instruction model would be. Maybe this is a limitation of this direction? Questions and Other Comments - Why would any instruction be repeated at all? I am trying to understand the purpose of using one-hot vector encoding for instructions. How many instructions occur more than once? - Did you evaluate against rule-based bots? How is the performance against another model trained using the same strategy (i.e. self play). - I believe authors first generate a text using the instruction model and then re-encode the text using an encoder. Why not do something like this: create an instruction embedding g(f(s)) from state encoding f(s). Pass this instruction encoding directly to the executor (as opposed to generated text). Then you can add an auxiliary objective which will try to bring g(f(s)) closer to the gold instruction encoding. This might work better as you are not adding discretization in between which can fail due to a single wrong decoding (say as based on a tie). - Equation on line 214 should have different state s for each time ""i"". Otherwise your state representation is not changing while you take new actions. - Many missing citations (see Tellex et al., AAAI 2011, Tellex et al., RSS 2014, Chaplot et al., AAAI 2017, Bahdanau et al., ICLR 2017, Misra et al., EMNLP 2018, Mirowski et al., 2019, Chen et al., CVPR 2019) etc.","- Equation on line 214 should have different state s for each time ""i"". Otherwise your state representation is not changing while you take new actions.",713,0
NIPS_2019_1348,NIPS_2019,"0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent.","3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that.",714,0
NIPS_2019_894,NIPS_2019,"1. Experimental evaluation 1.1. It would be great to also include zero shot performance on ImageNet (this is most likely missing as there are not attribute annotations for ImageNet, but the approach does not seem to be limited to attributes for transfer) 1.2. It would be interesting to quantitatively compare to [31] and [34] as ablations of the authorâs appraoch from which authors took inspiration. 1.3. The authors claim in the reproducibility checklist to have âClearly defined error barsâ and âA description of results with central tendency (e.g. mean) & variation (e.g. stddev)â, but they donât, although it would be good if they had. 2. Related work 2.1. The paper misses to discuss (qualitatively and quantitatively) recent related work including [A]. [A] achieves higher performance on SUN. Similarly, DCN [18] is not discussed and misses in Table 3, although it is better than other prior work on CUB. 2.2. Not w.r.t. performance, but with w.r.t. transductive label propagation, [B] is relevant. 3. Clarity: 3.1. Calling splits âProposed Splitsâ, although they have been proposed in [28] is a bit confusing. Better might be to refer e.g. as âpureâ as in [B]. Summary and Conclusion: While the paper includes an interesting novel transductive approach for zero shot learning, with an overall solid evaluation setup and several datasets, the paper misses to compare (discussion and quantitative) to Neurips 2018 papers ([A], [18]). References: [A] Zhao, An, et al. ""Domain-invariant projection learning for zero-shot recognition."" Advances in Neural Information Processing Systems. 2018. [B] Transfer Learning in a Transductive Setting; Rohrbach et al. Neurips 2013 == Post rebuttal == The authors provided additional convincing results on ImageNet (and Sun) and promising to add missing comparison in the final version. Furthermore, the authors included more comparisons to prior work in the author response. While I think this paper is sufficiently different with [34], I think the discussion on [34] could be further improved. I agree with R3 that the ""new setting"" is somewhat adhoc, and it would probably be good to compare it to the open world setting but it is still interesting that this paper studies it, especially in the transductive setting. I expect the authors include the results from the author response and I strongly recommend that the author release code (which is missing for [34]) to allow future work to build on and compare to this work. Overall, I think the paper provides sufficient methodological and significant experimental contribution, including reporting Generalized ZSL Results. I recommend accepting this paper.","2. Related work 2.1. The paper misses to discuss (qualitatively and quantitatively) recent related work including [A]. [A] achieves higher performance on SUN. Similarly, DCN [18] is not discussed and misses in Table 3, although it is better than other prior work on CUB. 2.2. Not w.r.t. performance, but with w.r.t. transductive label propagation, [B] is relevant.",715,0
NIPS_2019_504,NIPS_2019,"weakness of the proposed method is that its effectiveness still depends critically on the availability of good features \phi. The paper could be strengthened if ideas of how interpolation can be applied to other value function approximators were provided. The experimental evaluation is insufficient as only simple problems are considered and not nearly enough experimentation is provided to show empirically the error scaling, the effect of the anchor point selection, the advantage over averagers, or the dependence on the choice of \phi. More results should have been provided on the dependence between the anchor number and location optimization and the choice of features \phi. The theoretical derivation of the error bounds is illuminating in terms of the structure of error scaling but the bounds themselves do not seem practically useful. ## Major Comments + The introduction and preliminaries sections are very well written, giving the right context and a formal problem definition. + The specification of the necessary number of samples to minimize the error between the interpolant and the best linear Q-function approximation in Lemma 2 is nice. The greedy approach to adding anchor points to meet a specified amplification target is a nice contribution. - The greedy heuristic to construct a good set of anchor points before the learning process is a good addition to the proposed algorithm but is not explored in sufficient detail. A discussion on how complex the optimization problem max_{s,a} ||\theta^{\phi(s,a)}||_1 is should be added. Examples of the numbers, positions of anchor points, and the resulting extrapolation coefficient for common choices of \phi would have been nice to see. - The experiments are similar and somewhat simple. It would have been interesting to evaluate LAVIER on a real scale problem in addition to the toy problems vs LS-AVI. For example, it would have been great to see how LAVIER performs on a reinforcement learning problem where a good choice of features is not known but polynomial, RBF, or kitchen sink features are used. A comparison versus averagers and/or OPPQ-Learning [15] would have been interesting as well. It would be have been good to provide more variations on the Chain MDP evaluation, in terms of different feature choices and different planning horizon choices. ## Minor Comments: - Typos: ""Whenever the ineherent Bellman error is unbounded,"" - It would be cleaner to explicitly write the dependence on s_i and a_i in eq. (1) - The meaning of \delta should be introduced more clearly in Lemma 2 - In Fig. 3 middle, what value of C is achieved by the shown anchor points? - The notation is sloppy at places, switching from arguments in parentheses to subscripts for functions or from having two to one subscript (e.g., Proof of Proposition 3, around eq. (2))","- The notation is sloppy at places, switching from arguments in parentheses to subscripts for functions or from having two to one subscript (e.g., Proof of Proposition 3, around eq. (2))",716,0
NIPS_2019_1026,NIPS_2019,"that the authors should address: 1. Even though the analysis contains sufficiently novel elements, it is quite similar in structure with some of the papers already appearing in the references. I believe it would be fruitful for the community if the authors actually acknowledge this and in fact describe very carefully what is standard methodology and what is novel. In that spirit, there is a lot of room for improvement in section 3.3. 2. Along the same lines, i find the writing in section 3.4 unsatisfactory. It is too cryptic and it is not essential for a short paper: this type of initialization has already been discussed in references appearing in the paper and a short mention that this can be applied should be enough. Hence i suggest reducing section 3.4 to a short sentence. 3. Instead, more experiments are needed: a) please show the convergence rates for sigmoid and tanh. b) please compare results using âgoodâ and random initialization and c) please show some more realistic experiments with higher values of d,k,n and real data. 4. Please clarify: removing the redundancy from the objective function requires some knowledge about the optimal solution, correct? If so, what is the applicability of this result?","3. Instead, more experiments are needed: a) please show the convergence rates for sigmoid and tanh. b) please compare results using âgoodâ and random initialization and c) please show some more realistic experiments with higher values of d,k,n and real data.",717,0
NIPS_2019_1350,NIPS_2019,"of the method. CLARITY: The paper is well organized, partially well written and easy to follow, in other parts with quite some potential for improvement, specifically in the experiments section. Suggestions for more clarity below. SIGNIFICANCE: I consider the work significant, because there might be many settings in which integrated data about the same quantity (or related quantities) may come at different cost. There is no earlier method that allows to take several sources of data into account, and even though it is a fairly straightforward extension of multi-task models and inference on aggregated data, it is relevant. MORE DETAILED COMMENTS: --INTRO & RELATED WORK: * Could you state somewhere early in the introduction that by ""task"" you mean ""output""? * Regarding the 3rd paragraph of the introduction and the related work section: They read unnaturally separated. The paragraph in the introduction reads very technical and it would be great if the authors could put more emphasis there in how their work differs from previous work and introduce just the main concepts (e.g. in what way multi-task learning differs from multiple instance learning). Much of the more technical assessment could go into the related work section (or partially be condensed). --SECTION 2.3: Section 2 was straightforward to follow up to 2.3 (SVI). From there on, it would be helpful if a bit more explanation was available (at the expense of parts of the related work section, for example). More concretely: * l.145ff: $N_d$ is not defined. It would be good to state explicitely that there could be a different number of observations per task. * l.145ff: The notation has confused me when first reading, e.g. $\mathbb{y}$ has been used in l.132 for a data vector with one observation per task, and in l.145 for the collection of all observations. I am aware that the setting (multi-task, multiple supports, different number of observations per task) is inherently complex, but it would help to better guide the reader through this by adding some more explanation and changing notation. Also l.155: do you mean the process f as in l.126 or do you refer to the object introduced in l.147? * l.150ff: How are the inducing inputs Z chosen? Is there any effect of the integration on the choice of inducing inputs? l.170: What is z' here? Is that where the inducing inputs go? * l.166ff: It would be very helpful for the reader to be reminded of the dimensions of the matrices involved. * l.174 Could you explicitly state the computational complexity? * Could you comment on the performance of this approximate inference scheme based on inducing inputs and SVI? --EXPERIMENTS: * synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by ""support data"" and what by ""predicted training count data""? Could you write down the model used here explicitly, e.g. add it to the appendix? * Fertility rates: - It is unclear to me how the training data is aggregated and over which inputs, i.e. what you mean by 5x5. - Now that the likelihood is Gaussian, why not go for exact inference? * Sensor network: - l.283/4 You might want to emphasize here that CI give high accuracy but low time resolution results, e.g. ""...a cheaper method for __accurately__ assessing the mass..."" - Again, given a Gaussian likelihood, why do you use inducing inputs? What is the trade-off (computational and quality) between using the full model and SVI? - l.304ff: What do you mean by ""additional training data""? - Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth? - Now the sensors are co-located. Ideally, you would want to have more low-cost sensors that high-cost (high accuracy) sensors in different locations. Do you have a thought on how you would account for spatial distribution of sensors? --REFERENCES: * please make the style of your references consistent, and start with the last name. Typos etc: ------------- * l.25 types of datasets * l.113 should be $f_{d'}(v')$, i.e. $d'$ instead of $d$ * l.282 ""... but are badly bias"" should be ""is(?) badly biased"" (does the verb refer to measurement or the sensor? Maybe rephrase.) * l.292 biased * Figure 3: biased, higher peaks, 500 with unit. * l.285 consisting of? Or just ""...as observations of integrals"" * l.293 these variables","* Could you state somewhere early in the introduction that by ""task"" you mean ""output""?",718,0
NIPS_2019_1225,NIPS_2019,"1. Determining hyperparameters and reporting complexity 1.1. The paper requires setting âaccuracy goalsâ when encountering a new task. However, it might be unclear which accuracy can be reached and the paper is opaque how these accuracy goals are determined e.g. when comparing to prior work. To reach optimal performance algorithm 1 might need significant manual intervention. 1.1.1. How are the âaccuracy goalsâ determined (especially for Table 6,7)? 1.1.2. What happens if growing the network does not lead to achieving the accuracy goal? E.g. increasing the network capacity might lead to stronger overfitting and a reduced accuracy? 1.2. The approach may need many iterations to retrain the model to meet the âaccuracy goalâ (both w.r.t. growing and compressing) 1.3. How much is the model grown, how much is picked, how much is compressed? It would be interesting to see this for the different models in Table 6, as well as the accuracy targets. 1.4. It would be good to report the memory overhead from the binary masks and relate this to memory-based approached such as GEM, A-GEM, and generative replay. 2. Experimental Evaluation 2.1. Ablations 2.1.1. The paper claims that âAnother distinction of our approach is the âpickingâ step â. However, this aspect is not ablated. 2.2. Experiments on CIFAR. The comparison on CIFAR is not convincing 2.2.1. The continual learning literature has extensive experiments on this dataset and the paper only compares to one approach (DEN). 2.2.2. It is unclear if DEN is correctly used/evaluated. It would have been more convincing if the authors used the same setup as in the DEN paper to make sure the comparison is fair/correct. 3. Motivation 3.1. The paper claims forgetting is fully avoided due to the usage of a mask. While it is true that *after* model compression no further forgetting happens, but there is an accuracy drop during pruning, in contrast to e.g. regularization-based methods. Specifically, the original value (before pruning) is not recoverable and hence should be reported as forgetting. 4. The checklist is not fully accurate. The paper does not provide error bars and std-deviation for experiments. 5. Minor: 5.1. Grammar issue in word âdeterminingâ in the 4th paragraph on page 3. 5.2. On page 3, in âMethod overviewâ it says âAn overview of our method is depicted belowâ whereas it should directly refer to Figure 1 because Figure 1 is on page 2 5.3. On page 6, right below Figure 2, it says âin all experiments, but realize DENâ. Word ârealizeâ does not fit into the context. 5.4. In future, please use the submission template (not the camera-ready version) so that line numbers on the margins can be used to easily refer to the text. I lean more towards accept: The overall convincing results (especially Table 6) and overall novel model outweigh the limitations discussed above.",4. The checklist is not fully accurate. The paper does not provide error bars and std-deviation for experiments.,719,0
NIPS_2019_175,NIPS_2019,"1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.","1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it.",720,0
NIPS_2019_933,NIPS_2019,"+ I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.","- p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.",721,0
NIPS_2019_1008,NIPS_2019,"of different methods. - The paper is well-written. - The proposed method is clearly superior in all tasks. Potential weaknesses - Motivation: how does modeling these time series vary or similar to modeling audio, images, or text? The paper mentions time series are more dynamic, involving temporal irregularities such as discontinuities and varying levels of rate of change in different regions, perhaps motivate a bit more why a fixed âsamplingâ scheme of ""halving"" the sequence addresses the challenges in the domain or other considerations for future work. - The method assumes fixed-length sequences. Is this common in time series problems and datasets? - How much are the given anchor points (distributed across the sequence) giving the long-term structure versus the model learning them? - [masking] The masking scheme is not described. The figures seem to show that the unmasked positions are those in order with how the divide-and-conquer scheme would proceed. Does make it harder for certain baselines to cope with? Is the masking per time step, that is all dimensions within a time step is masked if a tilmestep is masked? One of the baselines, the GRUI paper, uses random masking across both time steps and dimensions. Given the divide-and-conquer scheme it might not be directly applicable, but would a variation of it be? One question the reader might have is that if for example if the unmasked positions are less well distributed throughout the sequence, how would it affect the performance? In line 235 âRobustness to percentage of missing valuesâ, the word âpercentageâ could be a bit misleading because itâs not a random percentage but portion in a pre-determined ordering. - [baseline] Would another baseline be generating auto-regressively but using the divide and conquer ordering, without adversarial loss? - Out of scope for this paper, perhaps question for future work, would some modification of the Transformer architectures be a competitive candidate for capturing these long-term dependencies? Clarifying questions: - In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also. - From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped? - In Figure 2, would g_1 and g_2 be reversed since the subscript r refers to the resolution (size of gap)?","- In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also.",722,0
NIPS_2019_1402,NIPS_2019,"1. Some experimental results are not convincing enough. For example, Bellman and consistent Bellman completely fail in Fig 1. That usually is due to lack of exploration. It is possible that RSO works better simply because it has more randomness. In addition, section 4.5 claims that the uniform \beta distribution is better than constant, but no numerical results are provided. Also in Fig 2 (b), the difference between RSO and consistent Bellman seems very small compared to the scale of rewards. 2. It is perhaps better to explain intuitively why RSO is optimality-preserving and action-gap-increasing based on equation (4). Then the reader wouldn't have to read through the proofs in the appendix before getting a rough understanding.","1. Some experimental results are not convincing enough. For example, Bellman and consistent Bellman completely fail in Fig 1. That usually is due to lack of exploration. It is possible that RSO works better simply because it has more randomness. In addition, section 4.5 claims that the uniform \beta distribution is better than constant, but no numerical results are provided. Also in Fig 2 (b), the difference between RSO and consistent Bellman seems very small compared to the scale of rewards.",723,0
NIPS_2019_873,NIPS_2019,"--- I think human studies in interpretability research are mis-represented at L59. * These approaches don't just ask people whether they think an approach is trustworthy. They also ask humans to do things with explanations and that seems to have a better connection to whether or not an explanation really explains model behavior. This follows the version of interpretability from [1]. This paper laments a lack of theoretical foundation to interpretability approaches (e.g., at L241,L275-277) and it acknowledges at multiple points that we don't know what ground truth for feature importance estimates should look like. Doesn't a person have to interpret an explanation at some point a model for it to be called interpretable? It seems like human studies may offer a way to philosophically ground interpretability, but this part of the paper mis-represents that research direction in contrast with its treatment of the rest of the related work. Minor evaluation problems: * Given that there are already multiple samples for all these experiments, what is the variance? How significant are the differences between rankings? I only see this as a minor problem because the differences on the right of figure 4 are quite large and those are what matter most. * I understand why more baseline estimators weren't included: it's expensive. It would be interesting to incorporate lower frequency visualizations like Grad-CAM. These can sometimes give significantly different performance (e.g., as in [3]). I expect it may have significant impact here because a more coarse explanation (e.g., 14x14 heatmap) may help avoid noise that comes from the non-smooth, high frequency, per-pixel importance of the explanations investigated. This seems further confirmed by the visualizations in figure 1 which remove whole objects as pointed out at L264. The smoothness of coarse visualization method seems like it should do something similar, so it would further confirm the hypothesis about whole objects implied at L264. * It would be nice to summarize ROAR into one number. It would probably have much more impact that way. One way to do so would be to look at the area under the test accuracy curves of figure 4. Doing so would obscure richer insights that ROAR would provide, but this is a tradeoff made by any aggregate statistic. Presentation: * L106: This seems to carelessly resolve a debate that the paper was previously careful to leave open (L29). Why can't it be that the distribution has changed? Do any experiments disentangle changes in distribution from removal of information? Things I didn't understand: * L29: I didn't get this till later in the paper. I think I do now, but my understanding might change again after the rebuttal. More detail here would be useful. * L85: Wouldn't L1 regularization be applied to the weights? Is that feature selection? What steps were actually taken in the experiments used in this paper? Did the ResNet50 used have L1 regularization? * L122: What makes this a bit unclear is that I don't know what is and what is not a random variable. Normally I would expect some of these (epsilon, eta) to be constants. Suggestions --- * It would be nice to know a bit more about how ROAR is implemented. Were the new datasets dynamically generated? Were they pre-processed and stored? * Say you start re-training from the same point. Train two identical networks with different random seeds. How similar are the importance estimates from these networks (e.g. using rank correlation similarity)? How similar are the sets of the final 10% of important pixels identified by ROAR across different random seeds? If they're not similar then whatever importance estimator isn't even consistent with itself in some sense. This could be thought of as an additional sanity check and it might help understand why the baseline estimators considered don't do well. [1]: Doshi-Velez, F., & Kim, B. (2017). A Roadmap for a Rigorous Science of Interpretability. ArXiv, abs/1702.08608. Final Evaluation --- Quality: The experiments were thorough and appropriately supported the conclusions. The paper really only evaluate importance estimators using ROAR. It doesn't really evaluate ROAR itself. I think this is appropriate given the strong motivation the paper has and the lack of concensus about what methods like ROAR should be doing. Clarity: The paper could be clearer in multiple places, but it ultimately gets the point across. Originality: The idea is similar to [30] as cited. ROAR uses a similar principle with re-training and this makes it new enough. Significance: This evaluation could become popular, inspire future metrics, and inspire better importance estimators. Overall, this makes a solid contribution. Post-rebuttal Update --- After reading the author feedback, reading the other reviews, and participating in a somewhat in-depth discussion I think we reached some agreement, though not everyone agreed about everything. In particular, I agree with R4's two recommendations for the final version. These changes would address burning questions about ROAR. I still think the existing contribution is a pretty good contribution to NeurIPS (7 is a good rating), though I'm not quite as enthusiastic as before. I disagree somewhat with R4's stated main concern, that ROAR does not distinguish enough between saliency methods. While it would be nice to have more analysis about the differences between these methods, ROAR is only one way to analyze these explanations and one analysis needn't be responsible for identifying differences between all the approaches it analyzes.",* L85: Wouldn't L1 regularization be applied to the weights? Is that feature selection? What steps were actually taken in the experiments used in this paper? Did the ResNet50 used have L1 regularization?,724,0
NIPS_2019_360,NIPS_2019,"weakness (since unfortunately there are other confounding factors). Further, orthogonally to the accuracy results, it is an interesting finding if standard approaches indeed suffer from this and the proposed method provides a remedy. I would therefore focus on these qualitative results more, and explain in the main text (not just the appendix) exactly how those visualization are created, and show those results for various models. 2) Somewhat related to the previous point: Pure metric-based models like Prototypical Networks lack an explicit mechanism for adaptation to each task at hand and it therefore seems plausible that they indeed suffer from the identified issue. However, it is less clear whether (or to what extent) models that do perform task-specific adaptation run the same danger. Intuitively, it seems that task adaptation also constitutes a mechanism for modifying the embedding function so that it favours the identification of objects that are targets of the associated classification task. By task adaptation here Iâm referring either to gradient-based adaptation (as in MAML and variants) or amortized conditioning-based adaptation (as in TADAM for example). Therefore, it would be very interesting to empirically compare the proposed method to these other ones not only in terms of classification accuracy but also qualitatively via visualizations as in Figure 1 that show the areas of the image that a model focuses more for making classification decisions. 3) Suggestion for the transductive framework: In Equation 8, it might be useful to incorporate the unlabeled examples in a weighted fashion instead of trusting that every example whose confidence surpasses a manually-set threshold can safely contribute to the prototype of the class that it is predicted to belong to. Specifically, the contribution of an unlabeled example to the updated class prototype can be weighted by the cosine similarity between that unlabeled example and that prototype (normalized across classes) and maybe additionally by the confidence c_b^q. This might slightly relieve the need to find the perfect threshold, since even if it is not conservative enough, a query example will be prohibited by modifying a prototype too much. An example of this is in Ren et al. [1] when computing refined prototypes by including unlabeled examples. 4) It seems that the weakness that this method is addressing would be more prominent in images comprised of multiple objects, or cluttered scenes. It would be very interesting to compare this approach to previous ones on few-shot classification on such a dataset! 5) For more easily assessing the degree of apples-to-applesness of the various comparisons in the tables, it would be useful to note which of the listed methods use data augmentations (as until recently this was not common practice for few-shot classification), what architecture they use, and what objective (most are episodic only but I think TADAM also performs joint training as the proposed method). 6) Another difference between the proposed approach and previous Prototypical Network-like methods is that the distance comparisons that inform the classification decisions are done in a feature-wise manner in this work. Specifically, when comparing embeddings a and b, for each spatial location, the distance between the feature vectors of a and b at that location is computed. The final estimate of the distance between a and b is obtained by aggregating those feature-wise distance estimates over all spatial locations. In contrast, usually the output of the last embedding layer is reshaped into a single vector (of shape channels x height x width) and distance comparisons of examples are made by directly comparing these vectors. It would therefore be useful to perform another ablation where a standard Prototypical Network is modified to perform the same type of distance comparison as their method. 7) Similarly to how the proposed transductive method was applied to other models, it would be nice to see results where the proposed joint training is also applied to other models, since this is orthogonal to the choice of the meta-learner too. References [1] Meta-Learning for Semi-Supervised Few-shot Classification. Ren et al. ICLR 2018.","7) Similarly to how the proposed transductive method was applied to other models, it would be nice to see results where the proposed joint training is also applied to other models, since this is orthogonal to the choice of the meta-learner too. References [1] Meta-Learning for Semi-Supervised Few-shot Classification. Ren et al. ICLR 2018.",725,0
NIPS_2019_1382,NIPS_2019,"of their work? A: It seems so, except there's no clear attempt made to analyze weaknesses of the approach. >Clarity: Q: Is the submission clearly written? A: The submission is mostly clear, although there are some technical ambiguities. Q: Is it well organized? (If not, please make constructive suggestions for improving its clarity.) A: Yes. Q: Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.) A: I think some readers might be able to implement a version of this approach. The training algorithm is a complex mixture of many different components. >Significance: Q: Are the results important? A: The results indicate the model is a feasible approach for modeling multi-agent vehicle motion. Q: Are others (researchers or practitioners) likely to use the ideas or build on them? A: I think the graphical model presented holds promise for others to extend. Q: Does the submission address a difficult task in a better way than previous work? A: The approach has strengths with respect to previous work (tractable graphical model allows for exact inference) but they're accompanied by weaknesses (training algorithm is complex). Q: Does it advance the state of the art in a demonstrable way? A: It demonstrably advances the state-of-the-art on the NGSIM dataset, which is somewhat simplistic given its simple context. Q: Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach? A: It sounds like new data (on CARLA) will be released. Below are my major comments / concerns (unordered): -- It's unclear what some of the columns in Table 1 mean. Does context mean including past trajectories, image/LIDAR information, or both? (After reading further beyond the table, it appears to be both, but the Table should be clear when it is presented) Isn't DESIRE [21] variational, since it uses a cVAE? (Related: why is ""variational"" an important attribute of the model to list in the table?). Does ""inter-rollouts"" mean ""interacting rollouts""? What about ""inter. encodings""? What is ""hypothetical""? This table is almost more confusing than enlightening. The meaning of the attributes should be clear, and each attribute should be meaningful. -- Equation 5, 6 derivations are unclear (although I am fairly sure they're correct). I didn't follow the derivation of (5), at least not by using the mentioned log derivative trick. The derivation of Eq6 RHS was cryptic as well; I didn't see where Jensen's could be applied. However (6) is derived, please make sure the derivation is clear, and put in appendix if space doesn't allow. Here's my derivation of the RHS of (6) directly, which might be simpler (dropping X, \theta dependence), as it doesn't rely on Jensen's or the log-derivative trick. log p(y) = sum_z q(z|y) log p(y) = sum_z q(z|y) log [ p(y,z)/p(z|y) ] = sum_z q(z|y) log [ p(y,z) ] + H(q,p) = sum_z q(z|y) log [ p(y,z) ] + KL(q,p) + H(q) >= sum_z q(z|y) log [ p(y,z) ] + H(q) (since KL >= 0) -- L193 ""possible to evaluate the exact log-likelihoods of trajectories"" It would be helpful here to clarify that [if I'm correct] something like Eq(4) is used to evaluate log-likelihoods. It's not clear what precise units and coordinate system NLL is calculated in (cross-entropy depends on the coordinate system of the trajectories). Can the exact NLL metric be given analytically? Is the NLL computed for each agent's trajectory individually, or the joint (I'm assuming the joint)? Is there some normalization going on in the metric (e.g. by number of agents or by timesteps)? -- There's some related uncited work that I think the authors should be aware of [A]. Although [A] is too recent to expect quantitative comparison (since it appeared within a month of the beginning of the NeurIPS submission deadline), the work shares many similarities that merit discussion, as it overlaps with the submission's stated contributions: 1) interative and parallel step-wise rollouts for all agents 2) ability to perform ""hypothetical inference"" / planning. [A] demonstrated using their graphical model and likelihood function to condition on goal states of one agent, and plan the latent variables to infer how other agents might respond, and showed that this inference can improve forecasting performance. -- There should be some sample quality metric on the CARLA data. The common one (referred to as minADE or minMSD) is computed by fixing a budge of samples, e.g. 10 or 20, and reporting the smalled MSD to the ground-truth sample. -- There's no comparison to any other state-of-the-art methods on the collected CARLA dataset. That makes the CARLA experiments much less informative w.r.t. to prior work. -- ""Hypothetical inference"" was stated as a capability of the model, but there's no qualitative or quantitative results that provide evidence of this capability. -- Are the latent modes for each agent fairly robust to variation in scene context and the latent modes for the other agents? There's only a single qualitative example that the modes are semantically meaningful (Fig4c). Is there a metric (aside from human verification) that could be used to roughly measure whether the meaning of the modes retains semantic meaning across scenes and variation in other agents? If this claim isn't that integral to the paper, it could be removed. -- It's unclear what the context is in the NGSIM experiments. Is the visual context used at all? Is it just past trajectory context? Below are my minor comments / concerns (unordered): The title is not very informative -- it's hard to predict much about the contents of the paper (even the data domain!) from the title. Please make it more informative. It's not clear 1) that the problem is multi-agent 2) the domain is contextual vehicle motion prediction. L105 I'm not familiar with a ""Discrete(K)"" distribution. Perhaps the authors meant ""Categorical(K)"". Minor suggestion, use ""\big("" and ""\big)"" for outer parentheses, e.g. after the second equality in (5). L171 Is it really a KL? The non-RNN terms of Eq(7) RHS look like a cross-entropy -- isn't it d/\theta_z H(p(z|y,x;\theta') , p(z|x;\theta_z) ? The T and Hz of the modelled trajectories in CARLA are not clear. [A] ""PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings."" Rhinehart et al. arXiv preprint arXiv:1905.01296","2) ability to perform ""hypothetical inference"" / planning. [A] demonstrated using their graphical model and likelihood function to condition on goal states of one agent, and plan the latent variables to infer how other agents might respond, and showed that this inference can improve forecasting performance. -- There should be some sample quality metric on the CARLA data. The common one (referred to as minADE or minMSD) is computed by fixing a budge of samples, e.g.",726,0
NIPS_2019_95,NIPS_2019,"of the submission. * originality: I enjoyed reading this paper. It introduces a new and interesting twist on the secretary problem, thereby providing a stylized theoretical version capturing the main essence of the task of ranking in many online settings. Some part of the analysis also provides some novel techniques that may be independently useful for other purpose (e.g. the new anti-concentration inequality). The proposed randomized algorithm is natural and somewhat unsurprising, but its analysis building upon connections to linear probing is interesting. * quality: By in-large the paper seems to be technically sound. I have gone through most proofs in detail, and although some would benefit with added clarity (see some examples below), I haven't found any main flaws. * clarity: The paper is in general well written, although there is room for some improvement. I list some examples below. * significance: In terms of significance, I believe that this work would be of interest to a small fraction of researchers within NIPS. In fact, in terms of fit, this looks like more a submission to be found within SODA. * other details/comments: - p.2, line 42: the optimal => an optimal \- p.4, line 155 to 159: this is a bit of a repeat, and somewhat ill-placed. Should appear before line 148 \- p.4, measure of sortedness: perhaps indicate whether there are other interesting measures one could consider (besides Kendall' tau and Spearman's footrule) \- p.5, line 192,1: $R$ should be defined (set of available rank); $r_1$ should be defined as 0 \- p.5, line 192,5: how are ties within the argmin dealt with? \- p.5, line 203: this equation is not numbered, but seems to be referred to as Eq. (3.2) later on; so it should be numbered. Same elsewhere where you have an equation which you want to refer to later on ... \- p.5, proof of Proposition 3.3: in the introduction of the event ${\cal O}_\sigma$ 5 lines above, it was for a permutation $\sigma$ on $t$ elements. Now in the proof, $\sigma$ is a permutation over $t-1$ elements. Which conditional event do we have in the conditional probability. By the way that formula indicates that the relative rank $r_t$ is {\em conditionally} uniformly distributed ... \- p.5, line 227: $t$-the should be $t^{th}$. Also what bound do you refer to at the end of the sentence? \- p.7, line 247: shouldn't we say: is popular at time $t$ w.p. at most $e^{-\Omega(\alpha)}$, that would help in the proof of Lemma 3.5, otherwise I don't see how the last equality in 263 can be true as the integral could be infinite? \- p.8, line 277: should $O(n\sqrt{n})$ be $\Omega(n\sqrt{n})$?","* clarity: The paper is in general well written, although there is room for some improvement. I list some examples below.",727,1
NIPS_2019_1301,NIPS_2019,"of the submission. * originality: Sequential resource allocation problems with censored feedback is an important class of problems and this paper looks at a stylized version and show its connection to a multiple-play multi-armed bandits problem in the most simple setting, and to a combinatorial semi-bandits problem otherwise. This is an interesting contribution. * quality: The paper seems to be technically sound. I have gone through most proofs in detail, and although some would benefit with added clarity (see some examples below), I haven't found any main flaws. * clarity: The paper would benefit greatly with a rigorous and systematic editing so as to improve its readability. See below some examples. * significance: In terms of significance, I believe that the main contribution of this work is in establishing the connections to well-known bandits problems. Proofs and techniques are otherwise quite standard. The work may be useful for researchers interested in modeling such sequential resource allocations problems in applications linked to security and defense. * minor details/comments: - p.1, line 9: their => its \- p.1, line 34: the use of the term locations here (for arms) should be clarified as early as possible \- p.1, line 35: armed played => armed is played \- p.2, line 39: their => its \- p.2, line 67-68: the experimental evaluation $\Rightarrow$ experimental evaluations \- p.2, line 71: since we are dealing with divisible resources, number => amount. I would also replace $N$ with another notation such as $Q$ so as to make it clear that this quantity is not necessarily an integer \- p.2, line 73: you mean smaller, right? \- p.2, line 81: may or not => may not be \- p.2, line 84: technically shouldn't you include $K$ as identifying an instance of CSB? \- p.2, line 86: please briefly indicate why you don't consider cases when some arms have identical rates, i.e., $\mu_i=\mu_j$, $i \neq j$ \- p.3, Algorithm 1, line 1: the expectation operator is missing \- p.3, Definition 1: a bit ambiguous as written. I would say: For a given loss vector $\mu$ and resource $N$, we say that thresholds ... \- p.3, Definition 1: is the equality between the argmin a set equality? \- p.3, Lemma 1: do you assume that $N \geq 1$ (note that otherwise $M$ could be 0)?. Assuming $M \geq 1$, $\hat{\theta}_c$ should be defined as $\min \{N/M,1\}$, as $N/M$ can exceed 1 \- p. 3, line 106: the optimal $\Rightarrow$ an optimal (this is just one optimal allocation, there may be many others) \- p.3, line 115: leaner => learner \- p.4, line 119: a playing => playing \- p.4, line 125: denote set => denote the \- p.4, line 132: is denoted => be denoted \- p.4, line 134, last term of the information set: $S_{t-1}$ => $Y_{t-1}$ \- p.4, line 152: shown achieve => shown to achieve \- p.5, Algorithm 2: I understand the saving space of putting the algorithm in such a format but it doesnt help readability. Same for Algorithm 3 - p.5, line 180: the assumption that $\mu_1 \geq \epsilon > 0$ is obviously restrictive. It would have been nice to add it on p.2 when discussions and restrictions about the $\mu's$ were introduced \- p.6, line 230: the optimal => an optimal \- p.6, line 236: optimal => an optimal \- p.6, line 254: give variables ?",* clarity: The paper would benefit greatly with a rigorous and systematic editing so as to improve its readability. See below some examples.,728,0
NIPS_2019_1008,NIPS_2019,"of different methods. - The paper is well-written. - The proposed method is clearly superior in all tasks. Potential weaknesses - Motivation: how does modeling these time series vary or similar to modeling audio, images, or text? The paper mentions time series are more dynamic, involving temporal irregularities such as discontinuities and varying levels of rate of change in different regions, perhaps motivate a bit more why a fixed âsamplingâ scheme of ""halving"" the sequence addresses the challenges in the domain or other considerations for future work. - The method assumes fixed-length sequences. Is this common in time series problems and datasets? - How much are the given anchor points (distributed across the sequence) giving the long-term structure versus the model learning them? - [masking] The masking scheme is not described. The figures seem to show that the unmasked positions are those in order with how the divide-and-conquer scheme would proceed. Does make it harder for certain baselines to cope with? Is the masking per time step, that is all dimensions within a time step is masked if a tilmestep is masked? One of the baselines, the GRUI paper, uses random masking across both time steps and dimensions. Given the divide-and-conquer scheme it might not be directly applicable, but would a variation of it be? One question the reader might have is that if for example if the unmasked positions are less well distributed throughout the sequence, how would it affect the performance? In line 235 âRobustness to percentage of missing valuesâ, the word âpercentageâ could be a bit misleading because itâs not a random percentage but portion in a pre-determined ordering. - [baseline] Would another baseline be generating auto-regressively but using the divide and conquer ordering, without adversarial loss? - Out of scope for this paper, perhaps question for future work, would some modification of the Transformer architectures be a competitive candidate for capturing these long-term dependencies? Clarifying questions: - In the complexity section (line 139), why is only the backward hidden states updated when the step is imputed and not he forward states also. - From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped? - In Figure 2, would g_1 and g_2 be reversed since the subscript r refers to the resolution (size of gap)?","- From the description of the text (line 131 to 133), would line 10 (would include how g^{(r)} is obtained?) and 11 be swapped?",729,0
NIPS_2019_629,NIPS_2019,"- To my opinion, the setting and the algorithm lack a bit of originality and might seem as incremental combinations of methods of graph labelings prediction and online learning in a switching environment. Yet, the algorithm for graph labelings is efficient, new and seem different from the existing ones. - Lower bounds and optimality of the results are not discussed. In the conclusion section, it is asked whether the loglog(T) can be removed. Does this mean that up to this term the bounds are tight? I would like more discussions on this. More comparison with existing upper-bounds and lower-bound without switches could be made for instance. In addition, this could be interesting to plot the upper-bound on the experiments, to see how tight is the analysis. Other comments: - Only bounds in expectation are provided. Would it be possible to get high-probability bounds? For instance by using ensemble methods as performed in the experiments. Some measure about the robustness could be added to the experiments (such as error bars or standard deviation) in addition to the mean error. - When reading the introduction, I thought that the labels were adversarially chosen by an adaptive adversary. It seems that the analysis is only valid when all labels are chosen in advance by an oblivious adversary. Am I right? This should maybe be clarified. - This paper deals with many graph notions and it is a bit hard to get into it but the writing is generally good though more details could sometimes be provided (definition of the resistance distance, more explanations on Alg. 1 with brief sentences defining A_t, Y_t,...). - How was alpha tuned in the experiments (as 1/(t+1) or optimally)? - Some possible extensions could be discussed (are they straightforward?): directed or weighted graph, regression problem (e.g, to predict the number of bikes in your experiment)... Typo: l 268: the sum should start at 1","- To my opinion, the setting and the algorithm lack a bit of originality and might seem as incremental combinations of methods of graph labelings prediction and online learning in a switching environment. Yet, the algorithm for graph labelings is efficient, new and seem different from the existing ones.",730,1
NIPS_2019_933,NIPS_2019,"+ I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.",- A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning.,731,0
NIPS_2019_1200,NIPS_2019,"of the proposed method would be highly appreciated and useful. Clarity: The paper is clearly written and easy to read. Most of the notions introduced are well explained and the paper is well organized. Significance: Extending RPCA to nonlinear settings is an important research topic and could prove to have a high impact in practical applications. However I am not fully convinced by the experiments and some more detailed comments are presented below. General comments/questions: â¢ A short introduction to RPCA would be useful for readers who are not very familiar with this technique and to see how NRPCA connects to RPCA. â¢ There is some work on nonlinear settings connected to RPCA that the paper would need to make the connection to and possibly compare with: o B. Dai et al 2018-Connections with robust PCA and the role of emergent sparsity in variational autoencoder models o Y. Wang et al â Green generative models â¢ Overall, I would have liked to see a more detailed description of the optimization problem from Sect. 6 and maybe a bit less on the curvature estimation especially sect. 5.1 which introduces a lot of notation that is not used afterwards (could maybe go into the supplementary material). The closed-form in eq (13) is not straightforward, especially introducing the soft-thresholding operator. â¢ The paper seems to take the approach that outliers need to be corrected, but I believe this depends on the application, and a discussion on correcting vs. identifying outliers would be relevant. How can we distinguish between these two very different tasks? Detailed comments: â¢ Line 19: âfinding the nodes of the graphâ â not sure I understand â¢ Line 20: The references cited, as least 16 (LLE) and 17 (Isomap) (I am not familiar with 14) do not use the graph Laplacian. Isomap uses multidimensional scaling to do the embedding, not the Laplacian. â¢ Line 21: the literature on outlier detection seems quite old. â¢ Does the methodology presented in Sect 2 work for non-Gaussian noise too? â¢ Lines 55-61: is the neighbourhood patch defined with respect to X_i or \tilde{X}_i? I would believe it should be the noisy data \tilde{X}_i, but maybe I am missing something? â¢ Line 57: consisting â¢ Sect 5.2: I am not sure I understand the need to use \epsNN instead of kNN if anyway afterwards we have to pick randomly m points within the \epsNN? This is related to a more general comment: \epsNN is known to be difficult in very high dimensions where because of the curse of dimensionality almost all points tend to fall within a small neighbourhood and points tend to be equidistant. Why do the authors choose randomly m points within \epsNN instead of either using directly kNN with k=m or use all the points within \epsNN? â¢ Is the neighbourhood size \eta the same for all points? Could this be a problem if the manifold is not sampled uniformly and the density varies? â¢ The notations for the radius and the residual term â would help if they were different instead of having both as R. Maybe small r for the radius? â¢ Eq (11) is a bit confusing as it uses both the X_i, X_i_j and p,q notations in the same equation and even the same term. Does the right term have \Sigma^2 (similar in eq (12))? â¢ Would be good to have a derivation of (12) in the supplementary material. â¢ The approximation in (11) seems to work for k sufficiently large, but that would include all the points in the limit. Also, this brings me back to the discussion on \epsNN vs kNN: if we need a very large k, why first do an \epsNN and then pick randomly m points? â¢ Sect 6: writing L^i as a function of S is not straightforward and a more detailed derivation would be useful. If I understand correctly from Alg 1 the procedure in Sect 6 is iterative. If so, this should be mentioned and also explain how you choose the number of steps T or if there is a stopping criterion. â¢ Lines 208-210: How could we know when the neighborhood is wrong because of the strong sparse noise in order to apply the solution outlined? â¢ Line 216: is p=3? â¢ In Fig. 1 I would start with the noisy data which is the input as in Fig. 4. Adding it at the very end is a bit counterintuitive. If I understand well the authors apply Alg 1 either with one iteration T=1 or with two iterations T=2? What happens for larger T? Usually iterative algorithms run until some criterion is fulfilled, with T >> 2. â¢ Line 224: no reference to Laplacian eigenmaps, and was not cited either in the introduction. â¢ Fig. 3: t-SNE is known to work very well on MNIST. A comparison would be useful. How do you explain the gaps in the denoised versions? Maybe a classification task would help to reveal how much the proposed method works better compared to existing approaches. What is the dimension of the NRPCA space, is it two? Could you also show those results if dim(NRPCA) = 2 like in Fig. 1? Could you please provide the parameters used for Laplacian eigenmaps and Isomap for the original vs. denoised versions? â¢ Fig. 4: LLE is applied in the space of NRPCA if I understand correctly. What is the dimension of the NRPCA space, 2D? â¢ It seems that NRPCA can be used either independently (just like any dimension reduction method) or as a preprocessing to other methods (LLE, Isomap etc). Would be useful to state this somewhere in the paper. How many iterations are used here for each figure? If I understand correctly, T=1 for second plot, and T=2 for the two rightmost plots. Iâm still wondering what happens for larger T as in a previous comment? â¢ Line 258: equally","4: LLE is applied in the space of NRPCA if I understand correctly. What is the dimension of the NRPCA space, 2D? â¢ It seems that NRPCA can be used either independently (just like any dimension reduction method) or as a preprocessing to other methods (LLE, Isomap etc). Would be useful to state this somewhere in the paper. How many iterations are used here for each figure? If I understand correctly, T=1 for second plot, and T=2 for the two rightmost plots. Iâm still wondering what happens for larger T as in a previous comment? â¢ Line 258: equally",732,0
NIPS_2019_1049,NIPS_2019,"- While the types of interventions included in the paper are reasonable computationally, it would be important to think about whether they are practical and safe for querying in the real world. - The assumption of disentangled factors seems to be a strong one given factors are often dependent in the real world. The authors do include a way to disentangle observations though, which helps to address this limitation. Originality: The problem of causal misidentification is novel and interesting. First, identifying this phenomenon as an issue in imitation learning settings is an important step towards improved robustness in learned policies. Second, the authors provide a convincing solution as one way to address distributional shift by discovering the causal model underlying expert action behaviors. Quality: The quality of the work is high. Many details are not included in the main paper, but the appendices help to clarify some of the confusion. The authors evaluated the approach on multiple domains with several baselines. It was particularly helpful to see the motivating domains early on with an explanation of how the problem exists in these domains. This motivated the solution and experiments at the end. Clarity: The work was very well-written, but many parts of the paper relied on pointers to the appendices so it was necessary to go through them to understand the full details. There was a typo on page 3: Z_t â Z^t. Significance: The problem and approach can be of significant value to the community. Many current learning systems fail to identify important features relevant for a task due to limited data and due to the training environment not matching the real world. Since there will almost always be a gap between training and testing, developing approaches that learn the correct causal relationships between variables can be an important step towards building more robust models. Other comments: - What if the factors in the state are assumed to be disentangled but are not? What will the approach do/in what cases will it fail? - It seems unrealistic to query for expert actions at arbitrary states. One reason is because states might be dangerous, as the authors point out. But even if states are not dangerous, parachuting to a particular state would be hard practically. The expert could instead be simply presented a state and asked what they would do hypothetically (assuming the state representations of the imitator and expert match, which may not hold), but it could be challenging for an expert to hypothesize what he or she would do in this scenario. Basically, querying out of context can be challenging with real users. - In the policy execution mode, is it safe to execute the imitatorâs learned policy in the real world? The expert may be capable of acting safely in the world, but given that the imitator is a learning agent, deploying the agent and accumulating rewards in the real world can be unsafe. - On page 7, there is a reference to equation 3, which doesnât appear in the main submission, only in the appendix. - In the results section for intervention by policy execution, the authors indicate that the current model is updated after each episode. How long does this update take? - For the Atari game experiments, how is the number of disentangled factors chosen to be 30? In general, this might be hard to specify for an arbitrary domain. - Why is the performance for DAgger in Figure 7 evaluated at fewer intervals? The line is much sharper than the intervention performance curve. - The authors indicate that GAIL outperforms the expert query approach but that the number of episodes required are an order of magnitude higher. Is there a reason the authors did not plot a more equivalent baseline to show a fair comparison? - Why is the variance on Hopper so large? - On page 8, the authors state that the choice of the approach for learning the mixture of policies doesnât matter, but disc-intervention obtains clearly much higher reward than unif-intervention in Figures 6 and 7, so it seems like it does make a difference. ----------------------------- I read the author response and was happy with the answers. I especially appreciate the experiment on testing the assumption of disentanglement. It would be interesting to think about how the approach can be modified in the future to handle these settings. Overall, the work is of high quality and is relevant and valuable for the community.","- For the Atari game experiments, how is the number of disentangled factors chosen to be 30? In general, this might be hard to specify for an arbitrary domain.",733,0
NIPS_2019_656,NIPS_2019,"Despite the shown results and the details added in the appendix K, I think that the experimental part remains the weak part of this paper. The results displayed are convincing but I am disappointed that the authors did not tried their approach on more popular problems mentioned in the supplementary such as hierarchical classification. Even if this could be improved (in order to be at the level of the theoretical treatment), the proposed content is already solid and does not change my decision concerning the quality of this work. Remark: - The use of the sequence example at different step of the paper is really useful, however I'm a bit surprised that you mention in Example 2 a 'common' practice in the context of CRF corresponding to using as a scoring loss the Hamming distance over entire parts of the sequence. I've never seen this type of approach and am only aware of works reporting the hamming loss defined node wise. It would be great if you could point out some references there. - After reading the paper a few times, I still think that the notation $\Delta(z,y|x)$ is a bit strange and I would have preferred something of the form $\Delta(z,y)$ since in practice the losses you mention never takes into account the input data and $z$ is already a function of $x$. Maybe this is only personal taste and will be contradicted by the other reviewers. Minor remarks : missing brackets [ ] in theorem 4.","- After reading the paper a few times, I still think that the notation $\Delta(z,y|x)$ is a bit strange and I would have preferred something of the form $\Delta(z,y)$ since in practice the losses you mention never takes into account the input data and $z$ is already a function of $x$. Maybe this is only personal taste and will be contradicted by the other reviewers. Minor remarks : missing brackets [ ] in theorem 4.",734,0
NIPS_2019_387,NIPS_2019,"- The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples. - I find this statement in the supplemental section D.4 questionable: ""Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting"". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states? - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version. - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions : - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from ""scratch"". (Though Figure 3 makes it clear that pretrained embeddings have little impact). - I think the authors risk overclaiming when they write ""Existing language GANs... have shown little to no performance improvements over traditional language models"", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).","- The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples.",735,0
NIPS_2019_306,NIPS_2019,"Weakness: 1. There are many limitations of the proposed method. The proposed method assumes that the causal graphical is given. Also, the values must be discrete. 2. It would be good to show how to use the proposed method to achieve fair policy learning without ""severely damaging the performance of predictive model"". 3. It would be great to discuss why the fairness bound achieved by the proposed method is tighter compared with previous methods. Minor issues: line 17 irrespective their -> irrespective of their line 240 to find -> to finding Should the the numbers in Table 3 CE #of o 4 be bold? The bound of the proposed method is tighter than previous methods.",3. It would be great to discuss why the fairness bound achieved by the proposed method is tighter compared with previous methods. Minor issues: line 17 irrespective their -> irrespective of their line 240 to find -> to finding Should the the numbers in Table 3 CE #of o 4 be bold? The bound of the proposed method is tighter than previous methods.,736,1
NIPS_2019_1397,NIPS_2019,"weakness of the manuscript. Clarity: The manuscript is well-written in general. It does a good job in explaining many results and subtle points (e.g., blessing of dimensionality). On the other hand, I think there is still room for improvement in the structure of the manuscript. The methodology seems fully explainable by Theorem 2.2. Therefore, Theorem 2.1 doesn't seem necessary in the main paper, and can be move to the supplement as a lemma to save space. Furthermore, a few important results could be moved from the supplement back to the main paper (e.g., Algorithm 1 and Table 2). Originality: The main results seem innovative to me in general. Although optimizing information-theoretic objective functions is not new, I find the new objective function adequately novel, especially in the treatment of the Q_i's in relation to TC(Z|X_i). Relevant lines of research are also summarized well in the related work section. Significance: The proposed methodology has many favorable features, including low computational complexity, good performance under (near) modular latent factor models, and blessing of dimensionality. I believe these will make the new method very attractive to the community. Moreover, the formulation of the objective function itself would also be of great theoretical interest. Overall, I think the manuscript would make a fairly significant contribution. Itemized comments: 1. The number of latent factors m is assumed to be constant throughout the paper. I wonder if that's necessary. The blessing of dimensionality still seems to hold if m increases slowly with p, and computational complexity can be still advantageous compared to GLASSO. 2. Line 125: For completeness, please state the final objective function (empirical version of (3)) as a function of X_i and the parameters. 3. Section 4.1: The simulation is conducted under a joint Gaussian model. Therefore, ICA should be identical with PCA, and can be removed from the comparisons. Indeed, the ICA curve is almost identical with the PCA curve in Figure 2. 4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data). 5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z). 6. Line 429: It seems we don't need Gaussian assumption to obtain Cov(Z_j, Z_k | X_i) = 0. 7. Line 480: Why do we need to combine with law of total variance to obtain Cov(X_i, X_{l != i} | Z) = 0? 8. Lines 496 and 501: It seems the Z in the denominator should be p(z). 9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments: 10. The manuscript could be more reader-friendly if the mathematical definitions for H(X), I(X;Y), TC(X), and TC(X|Z) were state (in the supplementary material if no space in the main article). References to these are necessary when following the proofs/derivations. 11. Line 208: black -> block 12. Line 242: 50 real-world datasets -> 51 real-world datasets (according to Line 260 and Table 2) 13. References [7, 25, 29]: gaussian -> Gaussian Update: Thanks to the authors' for the response. A couple minor comments: - Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials. - Regarding the Gaussian evaluation metric, I think it would be helpful to include the comments as a note in the paper.","4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data).",737,0
NIPS_2019_772,NIPS_2019,"of this approach (e.g., it does not take into account language compositionally). I appreciate that the authors used different methods to extract influential objects: Human attention (in line with previous works), text explanation (to rely on another modality), and question parsing (to remove the need of extra annotation). As a complementary analysis, I would have compared object sets (Jaccard Distance) which are extracted with visual cues and text description. Indeed, the VQA-X dataset contains both information for each question/answer pairs. The method is more or less correctly explained. The training details seems complete and allow for reproducibility. The authors do not provide code source although they mentioned it in the reproducibility checklist. The empirical results are quite convincing and the necessary baselines and ablation studies are correctly provided. The formatting is simple and clear! It would have been perfect to provide the error bar as the number of experiments remains low (and over a small number of epochs) The cherry on the cake would be to run similar experiments on VQAv1 / VQA-CP1? To increase the impact of the paper, I would recommend extending the setting to either dense image captioning, or question answering (if possible). I feel that the discussion section raise some excellent points: - I really like table 4, that clearly show that the method perform as expected (I would have add HINT for being exhaustive) - the ablation study is convincing But, a lot of open-questions are still left open and could have been discussed. For instance, I would have appreciated a more in-depth analysis of model errors. What about the model complexity? Why only reweighting L_{crit}. How does evolve L_crit and L_infl at training time? On a more general note, I think the overall writing and paper architecture can be greatly improved. For instance, - the introduction and related work can be partially merged and summarized. - 4.2 starts by providing high-level intuition while 4.1 does not. - Training details incorporate some result discussion Generic questions (sorted by impact): - What is the impact of |I|, do you have the performance ration according to the number of top |I| influential objects - Eq1 is a modified version of GardCAM, however, the modifications are not highlighted (neither explained). For instance, why did the authors remove the ReLU - Even if the weight sensitivity in equation 5 is well motivated, it is not supported by previous works. Thus, did you perform an ablation study? It would be very have been nice in the discussion section. - What is the actual computation cost of the two losses? What is the relative additional time required? +5%, +20%, +200%? - As you used heuristics to retrieve influential objects, did you try to estimate the impact of false negatives in the loss. - How did you pick 0.6 for glove embedding similarity? Did you perform k-cross-validation? What is the potential impact - Have you tried other influential loss (Eq3)? For instance, replacing the min with a mean or NDCG? Remarks: - I would use a different notation for SV(.,.,.) as it is not symmetric. For instance SV_{a}(v_i || v_j) would avoid confusion (I am using KL notation here) - Non-formal expression should be avoided: Ex: ""l32 What's worse"" - The references section is full of format inconsistencies. Besides, some papers are published with proceeding but are referred to arxiv papers. - 3.1 introduces non-important notation, e.g., function h(.) or f(.) that are never used in the paper. - Several subsections could be gathered together, or define as a paragraph: 2.1/2.2/2.3 ; 5.1/5.2/5.3, etc. It would have save space for more experiments Conclusion: The paper introduces two losses to better tie influential objects and potential answers. The method is convincing, and the experimental results are diverse and good. However, I still think that the paper requires further polishing to improve the readability. I would also advocate providing more element to support the proposed method and to analyze the strengths and weaknesses. Although the current experiences are quite convincing, I would advocate adding more analysis to definitely conclude the efficiency of the method. ---------------------------------- The rebuttal was clearly written and insightful. it answered most of my questions, and the authors demonstrate their ability to update the paper accordingly. Therefore, I am happy to increase my score, and accept the paper",- 4.2 starts by providing high-level intuition while 4.1 does not.,738,0
NIPS_2019_962,NIPS_2019,"** * Clarity. There are parts of this paper that are a bit unclear. The diagram and caption for KeyQN section are very helpful, but the actual text section could be fleshed out more. It would nice if the text could have a little more detail on how the outputs from the transporter are input to the KeyQN architecture and how the whole thing is trained. The exploration section was well explained for most part, but it took a bit of time to understand. Maybe would help to have an algorithm box. Also, the explanation of training process a bit confusing. Maybe a diagram of the architecture and how the transporter feeds into this would help. Also, I am confused a bit about whether the transporter is pretrained and frozen or fine-tuned. One quote from the paper in this regard confused me: âOur transporter model and all control policies simultaneuosly â so the weights of the Transporter network are not frozen during the downstream task like in KeyQN? * Experiments: They only show these results on a few games (and no error bars), so it would have been nice (but not a dealbreaker) to see results from more Atari games. They do partially justify this by saying they couldnât use a random policy on other games, but Iâd be curious just to see what happens when they try a couple more games. Would be nice to see comparisons to other exploration methods (they only show results compared to random exploration) Nitpicks/Questions * Makes sense to just refer the reader to the PointNet paper instead of re-explaining it, but a short explanation if possible of PointNet (couple sentences) might be helpful, so that one doesnât have to skim that paper to understand this paper * The diagram in figure 5 (h_psi) should show a heat map not keypoints superimposed on raw frame right? * In the appendix âK is handpicked for each game?â How? Validation loss? * The tracking experiments but the section is a bit unclear. I have a few questions on that front: * why is there a need to separating precision and recall? * why not just report overall mean average precision or F1 score? Might be a bit easier for reader to digest one number * Why bucket into diff sequence lengths? what do the different sequence lengths mean? There is no prediction-in-keypoint space model right? So there is no concept of the performance worsening as the trajectory gets longer. Arenât the keypoint guesses just the output of the PointNet at each frame, so why would the results from a 200 frame sequence be much different than 100 or something? Why not just report overall precision and recall on the test set? * In the KeyQN section What is the keypoint mask averaged feature vector? just multiply each feature map element wise by H_psi?",* why is there a need to separating precision and recall?,739,0
NIPS_2019_660,NIPS_2019,"that could be further improved: - The approach used in the theoretic analysis in the paper isn't able to give a necessary or a sufficient condition for the exploding gradient problem for LSTM since lambda_2 in Proposition 2.1 is typically not zero. The analyses of the various normalization approaches are also only made on the upper bound of the gradient magnitude, but on the gradient itself. It would be great to have a more direct proof for the effectiveness of the solutions. - In the analysis in Section 3, it only studies that the increased scaling of the weight matrices does not affect the upper bound of the gradient magnitude due to \sigma. It would be good to also analyze the effect of all the normalization scaling parameters 'g' w.r.t. the exploding gradient problem. - It would make the paper even stronger if it can show the normalization techniques still make 1-bit or 2-bit quantized LSTM achieve comparable performance to a full-precision counterpart on a much larger network consisting of multiple LSTM layers, e.g. for machine translation or speech recognition, where there is a strong practical need for compressing the models especially for the embedded/mobile use case.","- It would make the paper even stronger if it can show the normalization techniques still make 1-bit or 2-bit quantized LSTM achieve comparable performance to a full-precision counterpart on a much larger network consisting of multiple LSTM layers, e.g. for machine translation or speech recognition, where there is a strong practical need for compressing the models especially for the embedded/mobile use case.",740,1
NIPS_2019_978,NIPS_2019,"/ questions - First of all, I would like to know why is the assumption about asymptotic setting with infinite width needed. Where in the proof is it used? In terms of the parametrization proposed here it means that we work with n->infty. Eventually, in the experiments, the networks have rather small width compared to the depth. Following-up on that, could it be somehow empirically validated? Is this width assumption important? - What is the exact setup in the experiment presented in FIg. 2 (right). The text of the paper fails to exactly describe the experimental setup. What is the depth of the network considered here? - I find the experiments with resNets contestable. First of all the range of depths considered here is very surprising. Indeed, a resnet with 10000 blocks per stage is rather uncommon. Second, this experiment is ran for ""one epoch of training"" (l. 215) which in my opinion fails to show anything else than ""PyTorch default init fails completely with such deep WN resnets"". - The formulation on l. 243 is rather surprising: ""Unlike previous works which use the test set for hyperparameter tuning"". I do agree that proper hyper parameter tuning is essential in ML, but the wording is a bit hard in my opinion. Moreover, the ""smaller training set"" (l. 245) could be avoided by training the network with optimal parameters on the complete train+val. - In Table 1, I would expect a baseline WN model to be presented for other architectures than Resnet-110. Overall this is a good paper, coping with an interesting problem and is executed properly. I would like the authors to react to my negative comments / questions, and await the discussion period to make my final decision. # Rebuttal After reading the other reviews and the author's response, I decide to stick to my accept rating of 7. The author's response answered most of my concerns and doubts.","- In Table 1, I would expect a baseline WN model to be presented for other architectures than Resnet-110. Overall this is a good paper, coping with an interesting problem and is executed properly. I would like the authors to react to my negative comments / questions, and await the discussion period to make my final decision. # Rebuttal After reading the other reviews and the author's response, I decide to stick to my accept rating of 7. The author's response answered most of my concerns and doubts.",741,1
NIPS_2019_685,NIPS_2019,"of CGD. Main concerns: 1) CGD is very similar to Newton's method, but there is no rationale mentioned for dropping the diagonal terms of Jacobian. 2) No argument is presented for the specific bilinear form proposed as the fundamental local approximation to the game. 3) Complexity of inverting a matrix is dealt with using conjugate gradient but comparisons are done using ""simple"" GANs. Would this method scale well to higher dimensions? Do you believe there is a threshold at which the matrix inversion method becomes intractable? Have you tried CGD on more standard GAN tasks, e.g., MNIST, Celeb-A, CIFAR10? Do you expect problems at that scale? Also, SGA [Balduzzi et al, 2018] was independently derived in ""Global Convergence to the Equilibrium of GANs using Variational Inequalities"" [Gemp et al, 2018]. This work provides a comparison similar to Figure 1 in this paper. It should probably be cited along with Balduzzi '18 and Letcher '19.",2) No argument is presented for the specific bilinear form proposed as the fundamental local approximation to the game.,742,0
NIPS_2019_663,NIPS_2019,"of their work?""] The submission is overall reasonably sound, although I have some comments and questions: * Regarding the model itself, I am confused by the GRU-Bayes component. I must be missing something, but why is it not possible to ingest observed data using the GRU itself, as in equation 2? This confusion would perhaps be clarified by an explanation in line 89 of why continuous observations are required. As it is written, I am not sure why it you couldn't just forecast (by solving the ODE defined by equation 3) the hidden state until the next measurement arrives, at which point g(t) and z(t) can be updated to define a new evolution equation for the hidden state. I am guessing the issue here is that this update only changes the derivative of the hidden state and not its value itself, but since the absolute value of the hidden state is not necessarily meaningful, the problem with this approach isn't very clear to me. I imagine the authors have considered such a model, so I would like to understand why it wouldn't be feasible here. * In lines 143-156, it is mentioned that the KL term of the loss can be computed empirically for binomial and Gaussian distributions. I understand that in the case of an Ornstein-Uhlenbeck SDE, the distribution of the observations are known to be (conditionally) Gaussian, but in the case of arbitrary data (e.g. health data), as far as I'm aware, few assumptions can be made of the underlying process. In this case, how is the KL term managed? Is a Gaussian distribution assumption made? Line 291 indicates this is the case, but it should be made clear that this is an assumption imposed on the data. For example, in the case of lab test results as in MIMIC, these values are rarely Gaussian-distributed and may not have Gaussian-distributed observation noise. On a similar note, it's mentioned in line 154 that many real-world cases have very little observation noise relative to the predicted distribution - I assume this is because the predicted distribution has high variance, but this statement could be better qualified (e.g. which real-world cases?). * It is mentioned several times (lines 203, 215) that the GRU (and by extension GRU-ODE-Bayes) excels at long-term forecasting problems, however in both experiments (sections 5.2 and 5.3) only near-term forecasting is explored - in both cases only the next 3 observations are predicted. To support this claim, longer prediction horizons should be considered. * I find it interesting that the experiments on MIMIC do not use any regularly-measured vital signs. I assume this was done to increase the ""sporadicity"" of the data, but it makes the application setting very unrealistic. It would be very unusual for values such as heart rate, respiratory rate, blood pressure and temperature not to be available in a forecasting problem in the ICU. I also think it's a missed opportunity to potentially highlight the ability of the proposed model to use the relationship between the time series to refine the hidden state. I would like to know why these variables were left out, and ideally how the model would perform in their presence. * I think the experiment in Section 5.5 is quite interesting, but I think a more direct test of the ""continuity prior"" would be to explicitly test how the model performs (in the low v. high data cases) on data which is explicitly continuous and *not* continuous (or at least, not 2-Lipschitz). The hypothesis that this continuity prior is useful *because* it encodes prior information about the data would be more directly tested by such a setup. At present, we can see that the model outperforms the discretised version in the low data regime, but I fear this discretisation process may introduce other factors which could explain this difference. It is slightly hard to evaluate because I'm not entirely sure what the discretised version consists of , however - this should be explained (perhaps in the appendix). Furthermore, at present there is no particular reason to believe that the data in MIMIC *is* Lipschitz-2 - indeed, in the case of inputs and outputs (Table 4, Appendix), many of these values can be quite non-smooth (e.g. a patient receiving aspirin). * It is mentioned (lines 240-242, section H.1.3) that this approach can handle ""non-aligned"" time series well. As mentioned, this is quite a challenging problem in the healthcare setting, so I read this with some interest. Do these statements imply that this ability is unique to GRU-ODE-Bayes, and is there a way to experimentally test this claim? My intuition is that any latent-variable model could in theory capture the unobserved ""stage"" of a patient's disease process, but if GRU-ODE-Bayes has some unique advantage in this setting it would be a valuable contribution. At present it is not clearly demonstrated - the superior performance shown in Table 1 could arise from any number of differences between this model and the baselines. 2.c Clarity: [""Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.)""] While I quite like the layout of the paper (specifically placing related work after a description of the methodology, which is somewhat unusual but makes sense here) and think it is overall well written, I have some minor comments: * Section 4 is placed quite far away from the Figure it refers to (Figure 1). I realise this is because Figure 1 is mentioned in the introduction of the paper, but it makes section 4 somewhat hard to follow. A possible solution would be to place section 4 before the related research, since the only related work it draws on is the NeuralODE-VAE, which is already mentioned in the Introduction. * I appreciate the clear description of baseline methods in Section 5.1. * The comprehensive Appendix is appreciated to provide additional detail about parts of the paper. I did not carefully read additional experiments described in the Appendix (e.g. the Brusselator) out of time consideration. * How are negative log-likelihoods computed for non-probabilistic models in this paper? * Typo on line 426 (""me"" instead of ""we""). * It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated. 2.d Significance: [""Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?""] This paper describes quite an interesting approach to the modelling of sporadically-measured time series. I think this will be of interest to the community, and appears to advance state of the art even if it is not explicitly clear where these gains come from.",* I appreciate the clear description of baseline methods in Section 5.1.,743,1
NIPS_2019_1277,NIPS_2019,"---------- - a bit heavy on constraint-based causal discovery jargon, some explanation for non-experts would help - computational complexity not sufficiently discussed Quality ------- The algorithm and theory is technically sound. What I miss is a more thorough discussion of the increase in compational complexity since this may be a major impediment in adopting the algorithm. The introduction could also mention other structure learning approaches that, I believe, don't have the inconsistency issue, e.g., SAT-solver based approaches [2]. One question to clarify for non-experts is whether the presented modification is relevant also for the graph class of DAGs instead of CPDAGs (no Markov ambiguity)? I.e., if only the skeleton phase is needed since all links are oriented due to time-order or other side knowledge. Also, since this is about finite sample consistency, the authors could discuss how the new algorithm relates to the uniform consistency proof of the PC algorithm [1]? Clarity ------- Overall the article well-written, but a bit heavy on constraint-based causal discovery jargon. I have listed a number of points below that could be immproved upon. What would help is a figure and intuitive example of sepset inconsistency. I find a paper much more readable that illustrates a problem and solution on a simple example. Originality ----------- As far as I know the work is new and related work is cited. Significance ------------ The new algorithm can be of significant impact since, as the authors also mention, it makes PC-based methods more robust and better interpretable. The question is just whether the increase in computational complexity makes this prohibitive. That's not clear to me from the paper. Minor further points -------------------- Abstract: - explain separating set for non-experts - ""uncover spurious conditional independences"" --> ""uncover false negatives"" ? Introduction ""sampling noise"" --> finite sample sizes - could cite some more related work, what about SAT solver approaches to causal discovery [2]? - ""invoked to remove"" --> sounds odd - ""achieve a better balance"" --> ""achieves a better balance"" Section 2.2.1 first paragraph is unclear, could phrase ""slicing the available data"" better... Definition 1: Here I would like to see an example of a sepset inconsistency with the final that gives a non-expert an intuition Section 2.2.2 S, S', S_0, etc: all a bit confusing. Maybe a different notation would help. ""(i.e. relaxed, majority or conservative rules)"" --> difficult to understand for non-experts Proof of Theorem 4 ""discard all"" --> ""discards all"" ""less stringent separating set consistency"" --> what does this imply theoretically? The theorem 4 still holds, right? ""the the edges"" --> ""the edges"" Section 2.3 Here one could summarize the difference in complexity between PC-stable and the proposed algorithm Section 2.3.3 - ""covariance ranges"" --> Covariance between what? The SCM is run with the given coefficient ranges and gaussian unit variance errors, no? - Which specific alpha levels were chosen? References ---------- [1] Kalisch, Markus. 2007. âEstimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm.â The Journal of Machine Learning Research 8: 613â36. [2] Hyttinen, Antti, Patrik O Hoyer, Frederick Eberhardt, and Matti JÃ¤rvisalo. 2013. âDiscovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure.â In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 301â10. ---------------------- Update after author feedback: The authors convincingly addressed my question regarding computational complexity and they have also found a better definition of orientation-consistency. I also acknowledge that the authors will explain the problem better with an example figure. With these, I would still stick to my previous score ""a good submission"" (7). I concur with the other reviewers evaluation that the work is more incremental, but in a very solid way.","- explain separating set for non-experts - ""uncover spurious conditional independences"" --> ""uncover false negatives"" ? Introduction ""sampling noise"" --> finite sample sizes - could cite some more related work, what about SAT solver approaches to causal discovery [2]?",744,0
NIPS_2019_1026,NIPS_2019,"that the authors should address: 1. Even though the analysis contains sufficiently novel elements, it is quite similar in structure with some of the papers already appearing in the references. I believe it would be fruitful for the community if the authors actually acknowledge this and in fact describe very carefully what is standard methodology and what is novel. In that spirit, there is a lot of room for improvement in section 3.3. 2. Along the same lines, i find the writing in section 3.4 unsatisfactory. It is too cryptic and it is not essential for a short paper: this type of initialization has already been discussed in references appearing in the paper and a short mention that this can be applied should be enough. Hence i suggest reducing section 3.4 to a short sentence. 3. Instead, more experiments are needed: a) please show the convergence rates for sigmoid and tanh. b) please compare results using âgoodâ and random initialization and c) please show some more realistic experiments with higher values of d,k,n and real data. 4. Please clarify: removing the redundancy from the objective function requires some knowledge about the optimal solution, correct? If so, what is the applicability of this result?","4. Please clarify: removing the redundancy from the objective function requires some knowledge about the optimal solution, correct? If so, what is the applicability of this result?",745,0
NIPS_2019_1158,NIPS_2019,"1. The proposed method only gets convergence rate in expectation (i.e. only variance bound), not with high probability. Though Chebyshev's inequality gives bound in probability from the variance bound, this is still weaker than that of Bach [3]. 2. The method description lacks necessary details and intuition: - It's not clear how to get/estimate the mean element mu_g for different kernel spaces. - It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score. - There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes. 3. The empirical results are not presented clearly: - In Figure 1: what is ""quadrature error""? Is it the sup of error over all possible integrand f in the RKHS, or for a specific f? If it's the sup over all f, how does one get that quantity for other methods such as Bayesian quadrature (which doesn't have theoretical guarantee). If it's for a specific f, which function is it, and why is the error on that specific f representative of other functions? Other comment: - Eq (18), definition of principal angle: seems to be missing absolute value on the right hand side, as it could be negative. Minor: - Reference for Kernel herding is missing [?] - Line 205: Getting of the product -> Getting rid of the product - Please ensure correct capitalization in the references (e.g., [1] tsp -> TSP, [39] rkhss -> RKHSs) [3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of Machine Learning Research, 18(1):714â751, 2017. ===== Update after rebuttal: My questions have been adequately addressed. The main comparison in the paper seems to be the results of F. Bach [3]. Compared to [3], I do think the theoretical contribution (better convergence rate) is significant. However, as the other reviews pointed out, the theoretical comparison with Bayesian quadrature is lacking. The authors have agreed to address this. Therefore, I'm increasing my score.",- There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes.,746,0
NIPS_2019_524,NIPS_2019,"weakness of the paper are as follows (from my perspective): * (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods. * (weakness) currently, the paper does not seem to have a detailed discussion on how their generative model was trained to produce images containing the goal information. The authors do clarify this on their feedback and it would be useful if they also add this discussion on their next version of the paper. More importantly, including this discussion is useful for the Deep RL community. * (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.","* (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.",747,0
NIPS_2019_220,NIPS_2019,"1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method. 2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference. 3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)","3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)",748,0
NIPS_2019_304,NIPS_2019,"of the proposed approach are not discussed. For instance, is the time complexity mentioned in Theorem 14 good or bad? To what extent does it have implications for practical application (e.g. Knowledge Bases with thousands, tens of thousands, hundreds of thousands of facts)? My main problem is gauging the significance of the presented work. I think this is in part me not able to understand the paper in detail, but also due to the fact that there is no empirical validation of the presented approach. Minor comments â¢ L65: I would add neuro-symbolic learning to the unification of statistical and logical representation here. See for example - Cohen, William W. ""Tensorlog: A differentiable deductive database."" arXiv preprint arXiv:1605.06523 (2016). - RocktÃ¤schel, Tim, and Sebastian Riedel. ""End-to-end differentiable proving."" Advances in Neural Information Processing Systems. 2017. - Evans, Richard, and Edward Grefenstette. ""Learning explanatory rules from noisy data."" Journal of Artificial Intelligence Research 61 (2018): 1-64. - Manhaeve, Robin, et al. ""Deepproblog: Neural probabilistic logic programming."" Advances in Neural Information Processing Systems. 2018. â¢ L81: I am not familiar with the last logical connective. â¢ L107: What does ""maximal"" mean here? â¢ L111: What does ""eÎ¸"" refer to? Applying substitutions Î¸ to e? â¢ L116: I am confused since as far as I understand z would be a rank and a name here? â¢ L155-7: I think this is a good summary of the goal of the paper and should be stated earlier. â¢ L175: Can you elaborate why summing Îµi is valid here? My intuition is that this only works if the clauses are independent. Maybe this is trivially the case, but it is not obvious to me. â¢ L220: This seems to depend on the distribution of test queries though. Are you assuming the follow the same/similar distribution as test queries? UPDATE: After considering the rebuttal by the authors and discussions with the other reviewers I am happy to increase my score. That said, I strongly encourage the authors to make the paper more accessible and improve its clarity. I particularly believe more running examples and intuitive explanations would make the paper stronger.","- Evans, Richard, and Edward Grefenstette. ""Learning explanatory rules from noisy data."" Journal of Artificial Intelligence Research 61 (2018): 1-64.",749,1
NIPS_2019_1397,NIPS_2019,"of their approach, i.e., it assumes a jointly Gaussian distribution for correctness, and it suffers convergence issues with nearly-deterministic distributions. Clarity: I found the paper hard to read. First, the paper suffers from a very dense content, in particular extensive mathematical derivations and experiments, which the authors have decided to place in the supplementary materials. Also, a lot of statements are directly referring to content in the supplementary materials for justification, which brings the actual total length of the paper to 21 pages rather than 8. Still, the main 8-pages body remains understandable on its own. A second point, which I believe the auhtors can fix, is that the paper is quite confusing and even a bit misleading in several aspects, such as the time complexity of the approach, the exact goal of the approach (structure learning ? regularized density estimation ?), or some divergences between the theoretical presentation of the method and its implementation, that is, the factorization p(z|x) = \prod_j p(z_j|x), which are not discussed at all. See the detailled list of things that have to be fixed below: l.8-9: The proposed method has linear time complexity w.r.t. the number of observed variables -> This is misleading. The proposed criterion to be minimized can be computed in linear time, however the whole learning procedure to minimize this criterion is most likely not. l.18-19: is a holy grail in many scientific domains, including neuroscience, computational biology, and finance -> any reference ? l.22-29: For instance, graphical LASSO, [...] have better time complexity but perform very poorly in undersampled regimes. -> What does it mean to perform poorly ? For which task ? Density estimation ? What does it mean quantitatively ? At this point the text is very vague. Please state from the start which tasks those methods or your proposed method intend to solve, and which criterion permits to compare them objectively. l.30-31: a novel latent factor modeling approach for uncovering the structure of a multivariate Gaussian random variable -> This is very confusing. Your approach not only learns the structure of a multivariate distribution, but also does density estimation. There exists a whole litterature in graphical model structure learning which learns the structure only (i.e., just the graph), see [Murphy 2012, Ch.26]. Please clarify this, e.g., ""a novel latent factor modeling approach for estimating multivariate Gaussian distributions with modular structures"". l.33-34: when each observed variable has a single latent variable as its only parent -> You are suddently talking about parents here, but at this point you didn't state yet that you consider directed graphical structures. There exists undirected structures as well, which can also be latent models. Please clarify. l.36: is block-diagonal with each block being a diagonal plus rank-one matrix -> Why would each block be rank-one ? It is not obvious to me, I first figured this was an assumption you were making. Is that statement important for the paper ? l.36-38: This modular inductive prior is appropriate for many real-world datasets, such as stock market, magnetic resonance imaging, and gene expression data -> Why would that be ? Any intuition behind that ? Evidence ? References ? Figure 1: (Thm. 2.2) / (for Gaussians) -> This is very misleading, since Theorem 2.2 assumes a Gaussian distribution. Or reformulate Theorem 2.2 as ""a modular latent factor implies TC(Z|Xi), and the reverse is true for Gaussian distributions"". l.40: gets easier -> in terms of what ? Time ? Log-likelihood ? Structural error ? l.44: unconstrained graphical models -> unconstrained latent factor models l.68: constrains -> constraints Theorem 2.1: This is trivial, and is more a definition than a theorem. Random variables X and Z define a latent factor model iff. p(x,z) = \prod_j p(z_j) \prod_i p(x_i|z) <=> TC(Z) + TC(X|Z) = 0. Definition 2.1: i=1,2,...,p is redundant with \prod_{i=1}^p l.89: sometimes called clustering of observed variables -> reference ? l.93: from [7] -> yhis is bad style. Please name the authors or the approach. l.98: jointly -> multivariate ? Please be consistent in the text. Theorem 2.2: Fig. 1b equivalence -> this is a very bad name for a Theorem. For the sake of readability, either do not give a name or give a proper name, e.g, Modular Latent Gaussian Characterization. l.108: jointly Gaussian -> This is not a general parameterization, but rather a constrained Gaussian distribution where TC(Z|X) = 0. How does that interplay with TC(Z) = 0, TC(X|Z) = 0 and TC(Z|Xi) = 0 ? Is this important for Linear CorEx to work ? Please mention at least that this additionnal constraint is compatible with a modular latent model. This was very confusing to me. Equation (1): minimize_{z=Wx+e} -> minimize_{W} l.145: by increasing m until the gain in modeling performance is insignificant -> How do you measure a gain ? Log-likelihood ? l.148: The computational complexity -> The stepwise computational complexity Figure 3: in both cases s=5 -> Either say what s is here (signal-to-noise ratio), or don't mention it. Knowing what s means requires digging the text. l.208: black -> block l.208: each black being a diagonal plus rank-one matrix -> Why is it rank-one ? Why is that important ? l.246-247: we chose the number of factors [...] -> using which criteria ? Log-likelihood ? l.252-253: We omitted empirical covariance estimation since all cases have n < p -> Why ? I would still be curious to have the numbers to compare. l.271: session 014 -> Why ? this sounds a lot like cherry-picking. l.272-273: We do spatial smoothing by applying a Gaussian filter with fwhm=8mm -> Why ? Your method doesn't work if you don't do that ? Significance: The presented results are important, as they provide 1) an efficient way of learning modular latent Gaussian models, and 2) empirical evidence that this modular prior results in a better density estimation than other state-of-the-art approaches on a wide range of problems. Some general comments which the authors may want to address: - Suppose the X distributions contains variables Xi which are independent of the rest, i.e., X_i \ind X\{X_i}. Then such variables may not be linked to any latent variable, and still your minimized criterion can reach 0. The learned structure is a modular latent factor model. How do you deal with that situation in variable clustering ? Your procedure picks the latent variable with highest correlation, yet all correlations with that particular variable are 0. An easy answer is to assign each such variable to its own cluster. You may want to discuss that in the text. - You can simplify the proof for Theorem 2.2, and at the same time make it more general than only Gaussian distributions. Indeed, Gaussian distributions, among others, support the Decomposition, the Composition and the Singleton-Transitivity properties (See, e.g., Sadeghi 2017, Faithfulness of Probability Distributions and Graphs, p.7). Then you have: Z_s \indep Z\Z_s for all Z_s \subseteq Z (<=> TC(Z) = 0) Z_s \indep Z\Z_s | X_i for all Z_s \subseteq Z and X_i \in X (<=> TC(Z|X_i) = 0) Then due to the Singleton-Transitivity: Z_s \indep X_i or Z\Z_s \indep X_i Either both sides are true, in which case X_i \indep Z due to the Composition, and Z_s \indep X_i for all Z_s \subseteq Z due to the Decomposition. Or one side is false, for all Z_s \subseteq Z, in which case the intersection of all such Z_s or Z\Z_s sets leaves out a single Z_j, the unique parent of X_i","- Suppose the X distributions contains variables Xi which are independent of the rest, i.e., X_i \ind X\{X_i}. Then such variables may not be linked to any latent variable, and still your minimized criterion can reach 0. The learned structure is a modular latent factor model. How do you deal with that situation in variable clustering ? Your procedure picks the latent variable with highest correlation, yet all correlations with that particular variable are 0. An easy answer is to assign each such variable to its own cluster. You may want to discuss that in the text.",750,0
NIPS_2019_445,NIPS_2019,"- Quality: Results of Section 2.1, which builds the main motivation of the paper, is demonstrated on a very limited settings and examples. It does not convince the reader that overfitting is the general reason for potential poor performance of the models under study. - Soundness: While expressiveness is useful, it does not mean that the optimal weights are learnable. The paper seem to not pay attention to this issue. - Clarity: Related work could be improved. Some related works are mainly named but their differences are not described enough. - Organization could be improved. Currently the paper is dependent on appendix (eg the algorithms). Also the contents of tables are too small. Overall, I do not think the quality of the paper is high enough and I vote for it to be rejected.","- Soundness: While expressiveness is useful, it does not mean that the optimal weights are learnable. The paper seem to not pay attention to this issue.",751,0
NIPS_2019_564,NIPS_2019,"Weakness: 1. The improvement of the proposed method over existing RL method is not impressive. 2. Compared to OR tools and RL baselines, the time and computational cost should be reported in detail to fairly compare different methods. Comment after feedback: The authors have addressed the concerns of running time. Since the applying RL in combinatorial optimization is not new, the lack of comparisons between existing RL methods makes it less convincing. Reinforcement Learning for Solving the Vehicle Routing Problem, Mohammadreza Nazari. ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!, Max Welling. Exact Combinatorial Optimization with Graph Convolutional Neural Networks, Maxime Gasse. Learning Combinatorial Optimization Algorithms over Graphs, Le Song.","2. Compared to OR tools and RL baselines, the time and computational cost should be reported in detail to fairly compare different methods. Comment after feedback: The authors have addressed the concerns of running time. Since the applying RL in combinatorial optimization is not new, the lack of comparisons between existing RL methods makes it less convincing. Reinforcement Learning for Solving the Vehicle Routing Problem, Mohammadreza Nazari. ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!, Max Welling. Exact Combinatorial Optimization with Graph Convolutional Neural Networks, Maxime Gasse. Learning Combinatorial Optimization Algorithms over Graphs, Le Song.",752,0
NIPS_2019_390,NIPS_2019,"1. The distinction between modeling uncertainty about the Q-values and modeling stochasticity of the reward (lines 119-121) makes some sense philosophically but the text should make clearer the practical distinction between this and distributional reinforcement learning. 2. It is not explained (Section 5) why the modifications made in Definition 5.1 aren't important in practice. 3. The Atari game result (Section 7.2) is limited to a single game and a single baseline. It is very hard to interpret this. Less major weaknesses: 1. The main text should make it more clear that there are additional experiments in the supplement (and preferably summarize their results). Questions: 1. You define a modified TD learning algorithm in Definition 5.1, for the purposes of theoretical analysis. Why should we use the original proposal (Algorithm 1) over this modified learning algorithm in practice? 2. Does this idea of propagating uncertainty not naturally combine with that of distributional RL, in that stochasticity of the reward might contribute to uncertainty about the Q-value? Typos, etc.: * Line 124, ""... when experimenting a transition ..."" ---- UPDATE: After reading the rebuttal, I have raised my score. I appreciate that the authors have included additional experiments and have explained further the difference between Definition 5.1 and the algorithm used in practice, as well as the distinction between the current work and distributional RL. I hope that all three of these additions will make their way into the final paper.","1. You define a modified TD learning algorithm in Definition 5.1, for the purposes of theoretical analysis. Why should we use the original proposal (Algorithm 1) over this modified learning algorithm in practice?",753,0
NIPS_2019_1130,NIPS_2019,"weakness is the lack of focus of the discussion. I feel that too many points are scattered and there lacks a central message on the insights gained. Below are some specific questions and concerns: 1. Line 100-101: The theoretical results in [36] and also [26] do not assume that the gradient noise $Z_k$ is Gaussian. The weak approximation results do not depend on the actual distribution of gradient noise, which only need to satisfy some moment conditions. These are always satisfied when the objective is of finite-sum form, as considered in this work. See also [B] below for more general statements. This part should be rephrased accordingly to properly represent the results of prior work. 2. Line 180: The assumption $H\sigma$ is quite restrictive, as even in the quadratic case, as long as the covariance of gradients are not constant you would expect there to be some growth. I suggest relaxing this condition by some localization arguments, since at the end your results only depend on $\sigma^*$. 3. Line 127-137: 1) The reference appears wrong, [37] does not talk about the convergence rate of SGD to SDE. 2) Note that in previous work, explicit bounds between expectations over arbitrary test functions (not just $||\nabla f||^2$) on SGD and SDE are established. These are not the same as the results presented in Appendix D, which are matching rates just on $||\nabla f||^2$ (not arbitrary test functions). Moreover, the presented results are not bounding the difference between the expectation iterates, but rather show them having similar rates. This is a weaker statement. In my opinion, this point should be better clarified to avoid confusion of what actualy is derived in this paper -- in fact, without looking at the appendix I thought that the authors obtained uniform-in-time approximation results for non-convex cases, which would certainly be interesting! As far as I know, so far only [C] provides such estimates, but require strong convexity. I suggest the authors make space for the statements of results in this section in the main paper, since you have mentioned this in your abstract as one of your main results. 4. Line 277-286: This is an interesting observation. However, I have some concerns on its validity in general settings. It is well-known that 1D SDEs with multiplicative noise can be written as a noisy gradient flow of a modified potential function, but this fails to hold in high dimensions. It appears to me that by assuming $H$ is diagonal and $\sigma$ is constant, we fall into the 1D scenario, but this analogy is not likely to generalize. Perhaps the authors can comment on this. 5. Minor typos: 1) Theorem B.2, assumption 1 should not have a square on the RHS. 2) line 194: know -> known References: [A] Smith, Samuel L., and Quoc V. Le. ""A bayesian perspective on generalization and stochastic gradient descent."" arXiv preprint arXiv:1710.06451 (2017). [B] Li et al. ""Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations."" Journal of Machine Learning Research 20.40 (2019): 1-40. [C] Feng, Yuanyuan, et al. ""Uniform-in-Time Weak Error Analysis for Stochastic Gradient Descent Algorithms via Diffusion Approximation."" arXiv preprint arXiv:1902.00635 (2019).","3. Line 127-137:1) The reference appears wrong, [37] does not talk about the convergence rate of SGD to SDE.",754,0
NIPS_2019_30,NIPS_2019,"1. The problem discussed in this paper is relatively straightforward, even it is targeted for black box variational inference. The overall discussed topic is still relying on the existence of affine mapping for location scale distributed latent variables. The more general reparameterization trick has been discussed in [12] and ""Implicit reparameterization gradients (NIPS 2018)"". I admit there is no error in this manuscript, but I think that a thorough analysis for more challenging task would meet the acceptance bar of NIPS venue. 2. In the experimental section, only two toy models (Baysian linear and logistic regression) are considered. I understand the experiments are supportive to verify the variance bound proposed in this paper. However, in order to recognize the value of this work, I would like to see at least one additional experiment with amortized variational inference, such as a simple VAE whose bound may be easy to calculate. Typos: In Appendix: Eq below Line 298: \epsilon -> u Eq below Line 301: t -> T","1. The problem discussed in this paper is relatively straightforward, even it is targeted for black box variational inference. The overall discussed topic is still relying on the existence of affine mapping for location scale distributed latent variables. The more general reparameterization trick has been discussed in [12] and ""Implicit reparameterization gradients (NIPS 2018)"". I admit there is no error in this manuscript, but I think that a thorough analysis for more challenging task would meet the acceptance bar of NIPS venue.",755,0
NIPS_2019_304,NIPS_2019,"of the proposed approach are not discussed. For instance, is the time complexity mentioned in Theorem 14 good or bad? To what extent does it have implications for practical application (e.g. Knowledge Bases with thousands, tens of thousands, hundreds of thousands of facts)? My main problem is gauging the significance of the presented work. I think this is in part me not able to understand the paper in detail, but also due to the fact that there is no empirical validation of the presented approach. Minor comments â¢ L65: I would add neuro-symbolic learning to the unification of statistical and logical representation here. See for example - Cohen, William W. ""Tensorlog: A differentiable deductive database."" arXiv preprint arXiv:1605.06523 (2016). - RocktÃ¤schel, Tim, and Sebastian Riedel. ""End-to-end differentiable proving."" Advances in Neural Information Processing Systems. 2017. - Evans, Richard, and Edward Grefenstette. ""Learning explanatory rules from noisy data."" Journal of Artificial Intelligence Research 61 (2018): 1-64. - Manhaeve, Robin, et al. ""Deepproblog: Neural probabilistic logic programming."" Advances in Neural Information Processing Systems. 2018. â¢ L81: I am not familiar with the last logical connective. â¢ L107: What does ""maximal"" mean here? â¢ L111: What does ""eÎ¸"" refer to? Applying substitutions Î¸ to e? â¢ L116: I am confused since as far as I understand z would be a rank and a name here? â¢ L155-7: I think this is a good summary of the goal of the paper and should be stated earlier. â¢ L175: Can you elaborate why summing Îµi is valid here? My intuition is that this only works if the clauses are independent. Maybe this is trivially the case, but it is not obvious to me. â¢ L220: This seems to depend on the distribution of test queries though. Are you assuming the follow the same/similar distribution as test queries? UPDATE: After considering the rebuttal by the authors and discussions with the other reviewers I am happy to increase my score. That said, I strongly encourage the authors to make the paper more accessible and improve its clarity. I particularly believe more running examples and intuitive explanations would make the paper stronger.","- RocktÃ¤schel, Tim, and Sebastian Riedel. ""End-to-end differentiable proving."" Advances in Neural Information Processing Systems. 2017.",756,1
NIPS_2019_1180,NIPS_2019,"--- There are two somewhat minor weakness: presentation and some missing related work. The main points in this paper can be understood with a bit of work, but there are lots of minor missing details and points of confusion. I've listed them roughly in order, with the most important first: * What factors varied in order to compute the error bars in figure 2? Were different random initializations used? Were different splits of the dataset used? How many samples do the error bars include? Do they indicate standard deviation or standard error? * L174: How exactly does the reactive baseline work? * L185: What does ""without agent embeddings"" mean precisely? * L201: More details about this metric are needed. I don't know exactly what is plotted on the y axis without reading the paper. Before looking into the details I'm not even sure whether higher or lower is good without looking into the details. (Does higher mean more information or does lower mean more information?) * Section 3: This would be much clearer if an example were used to illustrate the problem from the beginning of the section. * Will code be released? * L162: Since most experiments share perception between speaker and listener it would be much clearer to introduce this as a shared module and then present section 4.3 as a change to that norm. * L118: To what degree is this actually realized? * L84: It's not information content itself that will suffer, right? * L77: This is unnecessary and a bit distracting. * L144: Define M and N here. * L167: What is a ""sqeuence of episodes"" here? Are practice and evaluation the two types of this kind of sequence? Missing related work (seems very related, but does not negate this work's novelty): * Existing work has tried to model human minds, especially in robotics. It looks like [2] and [3] are good examples. The beginning of the related work in [1] has more references along these lines. This literature seems significantly different from what is proposed in this paper because the goals and settings are different. Only the high level motivation appears to be similar. Still, the literature seems significant enough (on brief inspection) to warrent a section in the related work. I'm not very familiar with this literature, so I'm not confident about how it relates to the current paper. [1]: Chandrasekaran, Arjun et al. âIt Takes Two to Tango: Towards Theory of AI's Mind.â CVPR 2017 [2]: Butterfield, Jesse et al. âModeling Aspects of Theory of Mind with Markov Random Fields.â International Journal of Social Robotics 1 (2009): 41-51. [3]: Warnier, Matthieu et al. âWhen the robot puts itself in your shoes. Managing and exploiting human and robot beliefs.â 2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication (2012): 948-954. Suggestions --- * L216: It would be interesting to realize this by having the speaker interact with humans since the listeners are analogous to role humans take in the high level motivation. That would be a great addition to this or future work. Final Justification --- Clarity - This work could significantly improve its presentation and add more detail, but it currently is clear enough to get the main idea. Quality - Despite the missing details, the experiments seem to be measuring the right things and support very clear conclusions. Novelty - Lots of work uses reference games with multiple agents, but I'm not aware of attempts to specifically measure and model other agents' minds. Significance - The work is a useful step toward agents with a theory of mind because it presents interesting research directions that didn't exist before. Overall, this is a pretty good paper and should be accepted. Post-rebuttal Updates --- After reading the reviews and the rebuttal this paper seems like a clear accept. After discussion with R3 I think we all roughly agree. The rebuttal addressed all my concerns except the minor one listed below satisfactorily. There is one piece R3 and I touched on which is still missing. I asked about the relation to meta-learning and there was no response. More importantly, R3 asked about a comparison to a practice-stage only reward, which would show the importance of the meta-learning aspect of the reward. This was also not addressed satisfactorily, so it's still hard to understand the role of practice/evaluation stages in this work. This would be nice to have, but rest of the paper provides a valuable contribution without it. Though it's hard to tell how presentation and related work will ultimately be addressed in the final version, the rebuttal goes in the right direction so I'll increase my score as indicated in the Improvements section of my initial review.",* L201: More details about this metric are needed. I don't know exactly what is plotted on the y axis without reading the paper. Before looking into the details I'm not even sure whether higher or lower is good without looking into the details. (Does higher mean more information or does lower mean more information?) * Section 3: This would be much clearer if an example were used to illustrate the problem from the beginning of the section.,757,0
NIPS_2019_1276,NIPS_2019,"* Really only one real takeaway/useful experiment from the paper, which is that disentangling is sample efficient for this strange set of upstream tasks. * I have a lot of problems with these abstract visual reasoning tasks. They seem a bit unintuitive and overly difficult (I have a lot of trouble solving them). Having multiple rows and having multiple and different factors changing between each frame is very confusing and it seems like it would be hard to interpret how much these models actually learn the pattern or just exploit some artifacts. Do we have any proof that more simpler visual reasoning tasks wouldnât do and this formulation in the paper is the way to go? * It seems weird the authors didnât just consider a task with one row and one panel missing and the same one factor changing between panels. Is there any empirical evidence that this is too easy or uninformative? Why not a row where there are a few panels of the ellipse getting bigger and then for the missing frame the model chooses between a smaller ellipse, same size ellipse, *bigger ellipse*, bigger ellipse but at the wrong angle, bigger ellipse, but translated, bigger ellipse but different color, etc. or at least some progression of difficulty starting from the easiest and working up to the tasks in the paper?","* Really only one real takeaway/useful experiment from the paper, which is that disentangling is sample efficient for this strange set of upstream tasks.",758,0
NIPS_2019_1309,NIPS_2019,"Weakness** * The title is elegant, but it seems a bit general/all-encompassing. Maybe mention fairness in the title? Arenât there other benefits of disentangled representations not covered here (sample efficiency, transfer/generalization, etc.) ? * Section 4.2 is quite clear and well-written for the most part, but there are a couple confusing parts. For me, in the adjusted score part it was a bit confusing to understand how subtracting the disentangling score from the nearest neighbors (in terms of classification accuracy) achieves the effect of âremovingâ the effect of accuracy. * In the ""How do we identify fair models?"" section, I am a bit confused at how the chain of logic concluded that classification accuracy could be a proxy for fairness. Perhaps a causal graph between three nodes (disentangling score, GBT10000 accuracy, fairness) could be drawn and explained. I would be curious to see if my understanding is correct: ââââ accuracy does not always lead to fairness (thm 1), disentangling correlates with accuracy (Locatello, et al. ), disentangling correlates with fairness (section 4.1), disentangling correlates with fairness even if we adjust for accuracy (section 4.2), accuracy correlates with fairness in disentangling models (figure 4), so then disentangling is maybe a âcommon causeâ between accuracy and fairness aka fairness <- disentangling -> accuracy. So then if this is true (if disentangling is the only confounder), the model with highest accuracy should be the most disentangled, which means the fairest?ââââ Overall, a nice thoughtful paper with thorough experiments and even though I don't know much about fairness, it seems it could have some significance for the fairness community","* The title is elegant, but it seems a bit general/all-encompassing. Maybe mention fairness in the title? Arenât there other benefits of disentangled representations not covered here (sample efficiency, transfer/generalization, etc.) ?",759,0
NIPS_2019_387,NIPS_2019,"- The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples. - I find this statement in the supplemental section D.4 questionable: ""Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting"". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states? - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version. - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions : - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from ""scratch"". (Though Figure 3 makes it clear that pretrained embeddings have little impact). - I think the authors risk overclaiming when they write ""Existing language GANs... have shown little to no performance improvements over traditional language models"", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).","- Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from ""scratch"". (Though Figure 3 makes it clear that pretrained embeddings have little impact).",760,0
NIPS_2019_1397,NIPS_2019,"weakness of the manuscript. Clarity: The manuscript is well-written in general. It does a good job in explaining many results and subtle points (e.g., blessing of dimensionality). On the other hand, I think there is still room for improvement in the structure of the manuscript. The methodology seems fully explainable by Theorem 2.2. Therefore, Theorem 2.1 doesn't seem necessary in the main paper, and can be move to the supplement as a lemma to save space. Furthermore, a few important results could be moved from the supplement back to the main paper (e.g., Algorithm 1 and Table 2). Originality: The main results seem innovative to me in general. Although optimizing information-theoretic objective functions is not new, I find the new objective function adequately novel, especially in the treatment of the Q_i's in relation to TC(Z|X_i). Relevant lines of research are also summarized well in the related work section. Significance: The proposed methodology has many favorable features, including low computational complexity, good performance under (near) modular latent factor models, and blessing of dimensionality. I believe these will make the new method very attractive to the community. Moreover, the formulation of the objective function itself would also be of great theoretical interest. Overall, I think the manuscript would make a fairly significant contribution. Itemized comments: 1. The number of latent factors m is assumed to be constant throughout the paper. I wonder if that's necessary. The blessing of dimensionality still seems to hold if m increases slowly with p, and computational complexity can be still advantageous compared to GLASSO. 2. Line 125: For completeness, please state the final objective function (empirical version of (3)) as a function of X_i and the parameters. 3. Section 4.1: The simulation is conducted under a joint Gaussian model. Therefore, ICA should be identical with PCA, and can be removed from the comparisons. Indeed, the ICA curve is almost identical with the PCA curve in Figure 2. 4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data). 5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z). 6. Line 429: It seems we don't need Gaussian assumption to obtain Cov(Z_j, Z_k | X_i) = 0. 7. Line 480: Why do we need to combine with law of total variance to obtain Cov(X_i, X_{l != i} | Z) = 0? 8. Lines 496 and 501: It seems the Z in the denominator should be p(z). 9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments: 10. The manuscript could be more reader-friendly if the mathematical definitions for H(X), I(X;Y), TC(X), and TC(X|Z) were state (in the supplementary material if no space in the main article). References to these are necessary when following the proofs/derivations. 11. Line 208: black -> block 12. Line 242: 50 real-world datasets -> 51 real-world datasets (according to Line 260 and Table 2) 13. References [7, 25, 29]: gaussian -> Gaussian Update: Thanks to the authors' for the response. A couple minor comments: - Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials. - Regarding the Gaussian evaluation metric, I think it would be helpful to include the comments as a note in the paper.",5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z).,761,0
NIPS_2019_1241,NIPS_2019,"- the paper is missing some references to other papers addressing fairness in MARL such as Hughes et al. 2018, Freire et al. 2019, and other related work on prosocial artificial agents such as Peysakhovich et al. 2018 etc. - the paper could benefit from comparisons against other baselines using fairness and prosocial ideas such as the ones proposed by Hughes et al. 2018 and Peysakhovich et al 2018 - I've find the use of ""decentralized training"" to not be entirely correct given that the agents need access to the utility of all (or at least some, in the Gossip version) agents in order to compute their own rewards. this is generally private information and, so I wouldn't consider it fully decentralized. while the Gossipy version of the model that only uses the utilities of neighboring agents helps to relax some of these constraints, the question of how these rewards can be obtains in a setting with truly independent learners remains. Please clarify if there is a misunderstanding on my part. Other Comments: - it wasn't very clear to me from the paper what happens at test time. is it the same as during training in that the meta-controller picks one of the policies to act? - it would be interesting to look at the behavior of the controller for gaining a better understanding of when it decides to pick the sub-policy that maximizes external reward and when it picks the sub-policies that maximize the fairness reward. - in particular, it would be valuable how the different types of policies are balanced and what factors influence the trade-off between the sub-policy maximizing external reward and those with diverse fair behavior (i.e. current reward, observation, training stage, environment etc.)","- in particular, it would be valuable how the different types of policies are balanced and what factors influence the trade-off between the sub-policy maximizing external reward and those with diverse fair behavior (i.e. current reward, observation, training stage, environment etc.)",762,0
NIPS_2019_274,NIPS_2019,"- Limited novelty of methods / theory Major Comments/Questions: 1. Novelty/Contributions. While GW has been used for graph matching repeatedly in previous work (albeit for small tasks - see below), I am not aware of other work that uses it for graph partitioning, so I would consider this an important contribution of this paper. It should be noted that most of the individual components used in this work are not novel (the GW itself, its application to graph matching, the proximal gradient method). However, I see consider its main contribution combining those components in a coherent and practical way, and producing as a consequence a promising and well-founded approach to two important tasks. 2. The Scalable method. While recursive partitioning methods are a common fixture of discrete optimization (and thus its use here provides limited novelty), it is satisfactorily applied in this case, and it seems to work well. My main concern/question about this component its is robustness/reliability. Recursive partitioning methods are prone to be unforgiving: I a wrong decision is made in the early stages, it can have disastrous consequences downstream as there is no possibility to revise poor early decisions. This seems to be the case here, so I would be interested to see if and whether the authors observed those catastrophic early mistakes in their experiments, and whether a best-of-k version of their method (e.g., like beam search for sequence decoding) would be able to lessen these. 3. Adjacency matrices. The paper continuously refers to the C matrices as adjacency matrices, yet they are defined over the reals (ie., not binary, as adjacency matrices usually are). I assume they are using soft edges or distances for the entries of these matrices, and that's why they are continuous, but the authors should clarify this. Also on this note, I would have liked to see some intuitive explanation of Eq (4), e.g., I understand it as a soft-averaged (barycenter-weighted) extrapolated similarity matrix. I would suggest the authors to discuss its interpretation, even if briefly. 4. The paper mentions node measures being estimated from node degrees (e.g. L58). How exactly is this done? 5. Initialization. I might be missing something, but I just don't see how the approaches described in the ""Optimal Transports"" and ""Barycenter Graphs"" paragraphs are initialization schemes. In particular, the former looks like a regularization scheme applied to every iteration. Could the authors please provide more details about these? 6. Complexity Analysis. I appreciate the thorough yet concise complexity analysis of related methods. This is sadly less and less common in ML papers, so it's encouraging to see it here. One comment: I would suggest reminding the reader what d is in Table 1. 7. Results. I have various minor comments about the experimental results: * The adjusted mutual information should probably be spelled out explicitly (perhaps in the Appendix). I suspect many readers are not familiar with it or don't remember its exact form (I didn't). * Fig 1(e) would be more meaningful with two-sided error bars * What is q%|V| in L273? Is this a product? * Why are there no results for MultiAlign for > 3 graphs? Was it because of timeout? Please mention this in the paper. * NC@1 and NC@all could be better explained - it took me a while to understand what was meant by these Minor Comments/Typos: - L83. ""discrepancy"" repetition - L107. Two arbitrary nodes - L138. Convergence is not properly linear, but nearly-linear - a detail, yes, but an important one.","6. Complexity Analysis. I appreciate the thorough yet concise complexity analysis of related methods. This is sadly less and less common in ML papers, so it's encouraging to see it here. One comment: I would suggest reminding the reader what d is in Table 1.",763,1
NIPS_2019_663,NIPS_2019,"in the proposed methods themselves (once again, I will note that while I know RNNs quite well, I am rusty on ODEs and only superficially familiar with recent neural ODE work). Most of the questions I have are more related to how the proposed approach is motivated and described, which I will address under Clarity. The experimental design seems sound for the most part; it is perhaps a little frustrating that critical experimental design details are sometimes deferred to the appendix, but this is unavoidable given the NeurIPS page limit, and ultimately, it is not a barrier to reproducibility -- especially since the authors use public datasets and have made the code available. One relatively big concern I have is the train/test split procedure for each experiment and how hyperparameters were tuned. The only mention of a test set is buried in lines 592-594 of the appendix in this ominous sentence: ""Best model was selected using early stopping and performance were assessed by applying the best model on a held out test set (10% of the total data). We performed 5-fold cross validation and present the test performance average and standard deviation in all tables."" A straightforward read suggests that k-fold cross validation with five different test sets was used instead of a fully independent test set and hyperparameters were tuned on test set performance (!). I'd like the authors to address this and the below questions in the response and to make it crystal clear in the MAIN BODY of the manuscript: * Am I correct that they used five-fold cross validation with no fully independent test set? * Are the folds fixed across runs or generated randomly at the start of each run? * Assuming so, can the authors estimate how many times they examined test set performance (or individual test examples) during experimentation before reporting their final numbers? With this number on hand, can they discuss the risk of test set leakage (direct or indirect) and steps that they took (or could take, in the future) to reduce this risk? Can they also discuss the pros and cons of this approach vs. having an independent test set that they use only at the very end? * How exactly did the authors tune hyperparameters? During each k-fold split, do they set aside some data as a validation set, or are they tuning on test set accuracy? * Did the authors invest the same amount of effort in tuning the hyperparameters of baselines as they did the proposed model? CLARITY: The presentation of the idea and experiments could use some work. As noted above in Quality, some critical aspects of experimental design were omitted. It is not necessary for the reader to be able to reproduce exactly the methods and experiments from just reading the main body -- especially if the details are clearly stated in the appendix and code is shared -- but things like, e.g., train/test procedure split are essential for the reader to be able to interpret the results. As for the proposed methods themselves, the GRU-Bayes component did not feel clearly motivated. Admittedly, my experience with RNNs has focused on predicting an independent target, e.g., classification, vs. next step ahead forecasting, but the RNN forecasting work I've seen I think usually uses a simpler input-output problem formulation, akin to language modeling, without casting it as Bayesian filtering. Is this a necessary component for adding the ODE piece, or is it just an additional improvement? On that note, if the ODE and Bayes components are independent, I'd love to see an additional row in Table 2 for a plain ""GRU-ODE."" Another detail: my understanding from the Chen paper is that minibataching with the ODE architecture is non-trivial. I didn't fully understand the description on lines 159-162 of how the authors dealt with these challenges. Also, it's not clear to me at all (from this paper or Chen) how backpropagation-through-time works in this setting. Since this paper is an extension of the NODE idea to a recurrent setting, I think this would be a very valuable discussion to include in the paper's main body. As far as I can tell, the only prior work on neural ODEs that the paper cites is the Chen paper, but a quick Google Scholar search and some citation following yields a number of papers published on the topic over the last year, including several by Ruthotto and Haber. Can the authors place their work in the larger context of research on the intersection of neural nets and ODEs, not just the Chen paper? Also, is there any connection to continuous time Bayes nets or piecewise-constant conditional intensity models? SIGNIFICANCE: Setting aside my concerns about the risk of test set leakage, this paper seems like an important advance in time series modeling with RNNs. Continuous time dynamics and irregular sampling present fundamental challenges in the field, particularly in key application areas like healthcare, and solutions that rely on discretization and imputation or heuristics (like passing in time deltas) feel unsatisfactory. Ideas from dynamical systems and control have been under-explored, at least in deep learning, and seem to offer very promising directions of inquiry. This paper seems like a nice (albeit arguably incremental) extension of NODE to recurrent architectures and a valuable empirical demonstration of its effectiveness for real world time series modeling. My suspicion is that in the big picture, this paper will be dwarfed by previous work on neural ODEs, but it is nonetheless valuable follow-up work and should be of interest to practitioners.",* Am I correct that they used five-fold cross validation with no fully independent test set?,764,0
NIPS_2019_827,NIPS_2019,"- There is no theoretical analysis. - The numerical evaluation might be limited since the tasks used in the experiments seems to be biased. While these tasks have a large number of states, they might have small impact on the technical assumptions, especially the factorization over the representation of belief state, compared to the classical POMDP tasks like the T-maze task [Bakker, 2002]. - Since the proposed method employs several heuristics, which are used in SOGBOFA [2], it is unclear which heuristics contributed significantly to performance in the numerical experiments. Minor issue: - Line 96, p(x=1) should be p(x=T), p(i=T), or something? Also, the notation of T will be missing. At first, I misunderstood it as the time step T. Bakker, B.: Reinforcement learning with long short-term memory. In: Advances in Neural Information Processing Systems, vol. 14. MIT Press (2002) === Update after the rebuttal === I would like to thank the authors for submitting a response to the reviews and addressing my concerns. The explanation of limitations of the proposed approach in the response is interesting and important. I would like to recommend to include the explanation in the main body of the final version.","- The numerical evaluation might be limited since the tasks used in the experiments seems to be biased. While these tasks have a large number of states, they might have small impact on the technical assumptions, especially the factorization over the representation of belief state, compared to the classical POMDP tasks like the T-maze task [Bakker, 2002].",765,0
NIPS_2019_150,NIPS_2019,"weakness: It would strengthen the paper to show an experiment that analyzes the weighting coefficient lambda on the entropy term. The authors state the collapsing class problem, but do not show an experiment highlighting why the problem is important. The number of auxiliary classes per primary class seems to be a hyperparameter; it would be informative if the authors could provide an analysis for how to choose this hyperparameter, as according to Figure 3 the choice of hyperparameter has a non-trivial effect on generalization performance. Clarity - strengths: the paper is very well written and motivated Originality - strengths: the proposed method seems to be novel Significance - strengths: MAXL can be in principled be applied to any classification task as long as the number (but not the identity) of auxiliary tasks is pre-defined. - weakness: While MAXL provides an improvement over single task learning as shown in Table 1, the improvement seems marginal. It would be informative for the authors to include a discussion for why MAXL could not improve generalization performance beyond one percentage point in all of the classification tasks.","- weakness: While MAXL provides an improvement over single task learning as shown in Table 1, the improvement seems marginal. It would be informative for the authors to include a discussion for why MAXL could not improve generalization performance beyond one percentage point in all of the classification tasks.",766,0
NIPS_2019_1366,NIPS_2019,"Weakness: - Although the method discussed by the paper can be applied in general MDP, the paper is limited in navigation problems. Combining RL and planning has already been discussed in PRM-RL~[1]. It would be interesting whether we can apply such algorithms in more general tasks. - The paper has shown that pure RL algorithm (HER) failed to generalize to distance goals but the paper doesn't discuss why it failed and why planning can solve the problem that HER can't solve. Ideally, if the neural networks are large enough and are trained with enough time, Q-Learning should converge to not so bad policy. It will be better if the authors can discuss the advantages of planning over pure Q-learning. - The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning","- The paper has shown that pure RL algorithm (HER) failed to generalize to distance goals but the paper doesn't discuss why it failed and why planning can solve the problem that HER can't solve. Ideally, if the neural networks are large enough and are trained with enough time, Q-Learning should converge to not so bad policy. It will be better if the authors can discuss the advantages of planning over pure Q-learning.",767,0
NIPS_2019_82,NIPS_2019,"1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking. 2. The language in the paper is sometimes conversational and not scientific (use of terms like massive), and there are several opinions and claims that are not substantiated (e.g. ""... facial landmarks, which are helpful for the recognition of AUs defined in small regions""), the paper could benefit from copy-editing 3. Why are two instances of the same network (resnet) are used as different views? Would using a different architecture instead be considered a more differing view? Would be great to see a justification for using two resnet networks. 4. Why is the approach limited to two views, it feels like the system should be able to generalize to more views without too much difficulty? Minor comments: - What is PCA style guarantee? - What is v in equation 2? - why are dfferent numbers of unlabeled images using in training BP4D and EmotioNet models? Trivia: massive face images -> large datasets donates -> denotes (x2) adjacent -> adjacency","2. The language in the paper is sometimes conversational and not scientific (use of terms like massive), and there are several opinions and claims that are not substantiated (e.g. ""... facial landmarks, which are helpful for the recognition of AUs defined in small regions""), the paper could benefit from copy-editing 3. Why are two instances of the same network (resnet) are used as different views? Would using a different architecture instead be considered a more differing view? Would be great to see a justification for using two resnet networks.",768,0
NIPS_2019_1119,NIPS_2019,"weakness of this paper, and the reason my overall score is ""clear reject"", is the lack of strong motivation and justification for the problem formulation and assumptions. When is it useful to have an estimate of the distribution of values along a path (i.e., a concrete motivating application)? For that application, what is an appropriate metric to use for measuring the accuracy of a method? Why is it reasonable to only assume that the field is Lipschitz? Why not something stronger (e.g., C^2)? Why is it reasonable to assume that the observations are not corrupted by any noise (so the only randomness is due to uncertainty about the position)? In addition, there were some aspects of the problem formulation that were not clear to me upon reaching the end of Sec 2. - What is the goal? (Distribution learning, but no specific metric or way of measuring performance was described, nor were baselines or fundamental performance limits discussed) - It also wasn't clear that a ""path"" really means a closed loop with known the starting and ending point always being the same. The example signal simulated in Sec 4 isn't time-varying. Is that intentional? There is some mismatch between the title (which would imply that the aim is to estimate a random field) and the actual setting of the paper (the field is deterministic, the sampling locations are random).","- What is the goal? (Distribution learning, but no specific metric or way of measuring performance was described, nor were baselines or fundamental performance limits discussed) - It also wasn't clear that a ""path"" really means a closed loop with known the starting and ending point always being the same. The example signal simulated in Sec 4 isn't time-varying. Is that intentional? There is some mismatch between the title (which would imply that the aim is to estimate a random field) and the actual setting of the paper (the field is deterministic, the sampling locations are random).",769,0
NIPS_2019_275,NIPS_2019,"1) The proposed approach in this paper can be viewed as a straightforward combination of an off-the-shelf GAN and [7] that learns to linearly fuse two images for data augmentation. The novelty of the proposed approach is somewhat limited. In addition, the connection with [7] is not fully discussed. 2) Since [7] is directly relevant to the proposed approach, it would be more convicting to show the experimental comparison with [7]. 3) The technique in the proposed approach seems not restricted to fine-grained recognition. It would be interesting to evaluate the approach on standard generic image few-shot benchmark such as miniImageNet. 4) How is the performance if we do not use a pre-trained GAN generator but train the generator also in an end-to-end manner? 5) How is the performance with respect to the number of generated examples? 6) Why does the proposed approach work for few-shot learning? It looks like that the GAN image is similar to the original real image. Hence, their combination does not introduce diverse examples beyond the given real images.","1) The proposed approach in this paper can be viewed as a straightforward combination of an off-the-shelf GAN and [7] that learns to linearly fuse two images for data augmentation. The novelty of the proposed approach is somewhat limited. In addition, the connection with [7] is not fully discussed.",770,0
NIPS_2019_281,NIPS_2019,"weakness of the experiments. Originality: 5/10 This paper seems mostly to be about transferring the more general result of [34] to the specific setting of constrained MDPs. So I wish the authors gave more attention to [34], specifically: - reviewing the contribution of [34] in more detail - clarifying the novelty of this work (Is it in the specific design choices? The actor-critic algorithm? The set-up in lines 532-542 (Appendix)? Is it just in noticing the suitability of [34]'s results for CMDPs?) Without this help from the authors, it's difficult for me assess the originality and significance of their work. At the moment, it looks to me like a pretty straightforward application of [34]s results to CMDPs. Quality: 4/10 Overall the paper seems technically sound and well-done. But the experiments seem like an after-thought and/or purely illustrative, since: 1) they aren't included in the main paper and 2) they don't include multiple trials for significance. I also didn't find the content in section 5 particularly valuable; I think experiments would have been better. I'm also not familiar with the setting (LQR), or previous work using RL for LQR (with or without constraints), and the authors' summary didn't give me enough context to interpret the strength or meaning of their results. The abstract and introduction also over-claim, stating that they ""apply"" the algorithm to multi-agent RL; but there are no experiments for that setting, only description of how it could be applied. Clarity: 7/10 The paper is mostly quite clearly written, with a few typos. The following should be clarified: - the novelty compared with [34] - the significance of the experimental results - the main baseline being methods with Lagrange-multipliers - the motivations from safety: i.e. this doesn't tackle safe exploration, right? It's for safe execution? How exactly would this kind of method be used to make a safe real-world system? What problems does(n't) it solve? For instance, is it possible to verify that constraints will be satisfied? In practice, do CMDP approaches out-perform incorporating constraints as large negative rewards (when)? Also, *some* intuition for the proof should be given. This should include both the intuition underlying the proof strategy from [34], and an explanation highlighting the differences and the explaining what the issues are the require modifications to their proof. Significance: 7/10 The introduction presents a strong case that (in some theoretical aspects) this work represents a significant step up from the standard approach to CMDPs, based on Lagrangian multipliers. A table summarizing different approaches to CMDPs and highlighting the advantages of this work over previous approaches might be a nice addition. However, without stronger experiments, I don't think the paper presents a strong case that this method will be superior to existing methods in practice. Some detailed suggestions: - Line 85: ""minimize (1)"" --> ""maximize (1)"" - Line 109-110 and line 83 are redundant - Algorithm 2: beta_w/v are not defined; initial values of delta^J/D are not defined - line 201: ""and their are"" --> ""and their derivatives are"" (I think?) - line 261 (right hand side): PF_k --> P_{F_k} - I suggest renaming Q1, R1, Q2, R2 as Q_J, R_J, Q_D, R_D Some Questions: - Appendix: what's the difference between figure 1a/b and figure 2a/b? - Line 152-153: why can a solution be written in closed form? (explain or reference). - Line 186: how/why does actor-critic ""improve the performance further""? And is the improvement in terms of performance, rate of convergence, both, neither? And how is the claim (of improved performance) supported? - is finding a stationary point good enough? why can't we do better? (e.g. local minima?) - What is the state of RL research on LQR? How does it compare with other approaches? Is studying RL algorithms in LQR interesting practically, scientifically, or both? Some high level questions: - what are the key contributions of this work compared to [34]? - what are the key challenges for the proof? - what is the point of section 5? [34] An Liu, Vincent Lau, and Borna Kananian. Stochastic successive convex approximation for non-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266, 2018","- what is the point of section 5? [34] An Liu, Vincent Lau, and Borna Kananian. Stochastic successive convex approximation for non-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266, 2018",771,0
NIPS_2019_596,NIPS_2019,"1. The comparison between DetNAS and ResNet is unfair since ShuffleNetv2 block is more efficient than the Residual block in ResNet. I suggest the authors provide more comparison between DetNas and other networks constructed by effitive block. For example, the author should provide the result of a 3.8G FLOPs ShuffleNetv2 in Table 2. 2. The novelty of this paper is limited. The one-shot search mothod is first proposed by [1]. An interesting result is that the improvement between DetNAS and Random structure is minor. It seems a carefully designed search space is more important than the search method. The authors shold provide the mmAP curve of parents network during EA search. Ref: [1] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. abs/1904.00420, 2019.","2. The novelty of this paper is limited. The one-shot search mothod is first proposed by [1]. An interesting result is that the improvement between DetNAS and Random structure is minor. It seems a carefully designed search space is more important than the search method. The authors shold provide the mmAP curve of parents network during EA search. Ref: [1] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. abs/1904.00420, 2019.",772,0
NIPS_2019_229,NIPS_2019,"It is hard to assess what is the contribution of the paper from a practical point of view as SGD automatically avoids possible training problems related to AV. On the theoretical side, the paper does not investigate deeply the relationship between AV and usual flat or sharp minima. For example, how are AV connected to each others and which properties of sharp/flat minima generalize to AV? Questions: - Under what conditions (on the network architecture) do AV appear? Is there an intuitive interpretation of why they can be found in the loss function associated with many `modern' neural networks? - Is the presence of AV restricted to the over-parameterized case? If yes, what happens in the under-parameterized situation? Which of the given theoretical properties extend to that case (as local minima cannot be expected to be equivalent in the under-parameterized case). - Do the structure of AV depend on the type of objective function used for training? What happens if a L-2 penalty term on the weights is added to the loss function? - Would it be possible to built a 2-dimensional analytical example of AV? - The averaged SGD performs well also in the case of convex loss. Is its good behaviour around a AV related to this? - In Section 3.2, it is claimed that AV can be found with `decent probability'. What is the order of magnitude of such probability? Does it depend on the complexity of the model? Does this mean that most of the minima are AV?",- Would it be possible to built a 2-dimensional analytical example of AV?,773,0
NIPS_2019_819,NIPS_2019,"Weakness: Due to the intractbility of the MMD DRO problem, the submission did not find an exact reformulation as much other literature in DRO did for other probability metrics. Instead, the author provides several layers of approximation. The reason why I emphasize the importance of a tight bound, if not an exact reformulation, is that one of the major criticism about (distributionally) robust optimization is that it is sometimes too conservative, and thus a loose upper bound might not be sufficient to mitigate the over-conservativeness and demonstrate the power of distributionally robust optimization. When a new distance is introduced into the DRO framework, a natural question is why it should be used compared with other existing approaches. I hope there will be a more fair comparision in the camera-ready version. =============== 1. The study of MMD DRO is mostly motivated by the poor out-of-sample performance of existing phi-divergence and Wasserstein uncertainty sets. However, I don't believe this is indeed the case. For example, Namkoong and Duchi (2016), and Blanchet, Kang, and Murthy (2016) show the dimension-independent bound 1/\sqrt{n} for a broad class of objective functions in the case of phi-divergence and Wasserstein metric respectively. They didn't require the population distribution to be within the uncertainty set, and in fact, such a requirement is way too conservative and it is exactly what they wanted to avoid. 2. Unlike phi-divergence or Wasserstein uncertainty sets, MMD DRO seems not enjoy a tractable exact equivalent reformulation, which seems to be a severe drawback to me. The upper bound provided in Theorem 3.1 is crude especially because it drops the nonnegative constraint on the distribution, and further approximation is still needed even applied to a simple kernel ridge regression problem. Moreover, it seems restrictive to assume the loss \ell_f belongs to the RKHS as already pointed out by the authors. 3. I am confused about the statement in Theorem 5.1, as it might indicate some disadvantage of MMD DRO, as it provides a more conservative upper bound than the variance regularized problem. 4. Given the intractability of the MMD DRO and several layers of approximation, the numerical experiment in Section 6 is insufficient to demonstrate the usefulness of the new framework. References: Namkoong, H. and Duchi, J.C., 2017. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980). Blanchet, J., Kang, Y. and Murthy, K., 2016. Robust wasserstein profile inference and applications to machine learning. arXiv preprint arXiv:1610.05627.","4. Given the intractability of the MMD DRO and several layers of approximation, the numerical experiment in Section 6 is insufficient to demonstrate the usefulness of the new framework. References: Namkoong, H. and Duchi, J.C., 2017. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980). Blanchet, J., Kang, Y. and Murthy, K., 2016. Robust wasserstein profile inference and applications to machine learning. arXiv preprint arXiv:1610.05627.",774,0
NIPS_2019_565,NIPS_2019,"- some experimental & analysis details are unclear or omitted - corpus baselines (and the status of ""referents"" in natural language generally) are not super well aligned with the main task in the paper While I have a few questions about the experimental setup and some of the linguistic claims, I think this is a thorough and well-executed empirical paper and a useful contribution to the emergent communication literature. I think it should be accepted. REFERENT DISTRIBUTIONS This is my main substantive complaint: line 43 says ""[referents are] randomly drawn from a power-law distribution (so that referent frequency is extremely skewed, as in natural language)"". This conflates the distribution of *words* with the distribution of *referents*. ZLA says nothing about the distribution of referents. A claim that referents in natural language are also distributed as a power law is non-trivial and requires citation, given that most words (especially the frequent ones!) have no referential function on their own. For the same reason, I'm a little uncomfortable with the models of natural language built from corpus word frequency data, since the processes determining word frequencies in these corpora are fundamentally different from those producing the agent behavior in this paper. I don't think any of this changes the bottom line, and you should definitely keep all the current experiments, but I would like to see this paper be a little more careful about the difference between words, referents, and their associated frequencies in both the motivation section and in comparisons to real-world language data. EXPERIMENTS AND ANALYSIS The central claim in this paper is that message length is anticorrelated with frequency in the base model, but this is only backed up visually (i.e. ""the line goes down in Figure 1""). We really need to see a correlation coefficient and a hypothesis test somewhere.... There are a couple of cases where numbers get averaged across training runs, and I'm unclear about what's being averaged. In Fig 2, are we looking at the average length of all rank-i messages? Or the average rank of all length-l messages? Similarly, in Fig 3, are we looking at (average pairwise distance) averaged across training runs? Or (average distance across training runs) averaged pairwise? There are a couple of empirical claims for which no quantitative statement (or even appendix pointer) is provided: ""The higher D is, the more accurate the agents become"" on 179; ""the patterns are general"" on 256. MESSAGE DISTRIBUTIONS 31: ""There is an inverse (non-linear) correlation between word frequency and length"". The precise nature of this non-linearity is one of the most interesting underlying computational-linguistic issues here, and I think the paper would benefit from exploring it a bit more. Of the citations provided for this line, one argues for a power-law distribution of word frequencies, and one argues for a Gamma distribution. The ""optimal code"" model in this paper has an exponential word frequency distribution, and I think monkey typing probably does as well. What about the learned agents? In addition to plotting things on an absolute message length / frequency scale, it would be extremely interesting to normalize them in some way and try to make a claim about the functional form of the length distribution (esp. as it appears in the penalized vs unpenalized experiment) with an appropriate statistical test (K-S etc.). As a related presentation issue, I think all of the figures in this paper would be clearer on log or log-log scales. MISCELLANEOUS - 15: s/strand/stray? - 21: Nitpick---I find this use of ""AIs"" needlessly imprecise. Perhaps ""automated agents"" or ""computational agents"" instead? - 57: cite Lewis 69 ""Convention"" for the signaling game. - Eq 1 is Williams 92, right? What's been added from Schulman 15? - 157: ""xdistributions"" - What does distribution of lengths look like w/r/t a uniform distribution over referents?","- Eq 1 is Williams 92, right? What's been added from Schulman 15?",775,0
NIPS_2019_346,NIPS_2019,"weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.","3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general.",776,0
NIPS_2019_369,NIPS_2019,"weakness is the experiment. # Weaknesses - Some important related works are discussed in Sec.5 but not compared directly in the experiments. What is gained by TRIP vs autoregressive priors [12,13] or flow-based priors [15]? There are no quantitative comparisons between training the generative models with TRIP and with other advanced parametrized priors. - What is the computational cost of TRIP? Since TRIP introduces additional parameters for the prior and brings extra computation, it is worth knowing that how much it slows down the training.","- What is the computational cost of TRIP? Since TRIP introduces additional parameters for the prior and brings extra computation, it is worth knowing that how much it slows down the training.",777,0
NIPS_2019_387,NIPS_2019,"- The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples. - I find this statement in the supplemental section D.4 questionable: ""Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting"". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states? - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version. - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions : - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from ""scratch"". (Though Figure 3 makes it clear that pretrained embeddings have little impact). - I think the authors risk overclaiming when they write ""Existing language GANs... have shown little to no performance improvements over traditional language models"", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).","- I think the authors risk overclaiming when they write ""Existing language GANs... have shown little to no performance improvements over traditional language models"", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).",778,0
NIPS_2019_274,NIPS_2019,"- Limited novelty of methods / theory Major Comments/Questions: 1. Novelty/Contributions. While GW has been used for graph matching repeatedly in previous work (albeit for small tasks - see below), I am not aware of other work that uses it for graph partitioning, so I would consider this an important contribution of this paper. It should be noted that most of the individual components used in this work are not novel (the GW itself, its application to graph matching, the proximal gradient method). However, I see consider its main contribution combining those components in a coherent and practical way, and producing as a consequence a promising and well-founded approach to two important tasks. 2. The Scalable method. While recursive partitioning methods are a common fixture of discrete optimization (and thus its use here provides limited novelty), it is satisfactorily applied in this case, and it seems to work well. My main concern/question about this component its is robustness/reliability. Recursive partitioning methods are prone to be unforgiving: I a wrong decision is made in the early stages, it can have disastrous consequences downstream as there is no possibility to revise poor early decisions. This seems to be the case here, so I would be interested to see if and whether the authors observed those catastrophic early mistakes in their experiments, and whether a best-of-k version of their method (e.g., like beam search for sequence decoding) would be able to lessen these. 3. Adjacency matrices. The paper continuously refers to the C matrices as adjacency matrices, yet they are defined over the reals (ie., not binary, as adjacency matrices usually are). I assume they are using soft edges or distances for the entries of these matrices, and that's why they are continuous, but the authors should clarify this. Also on this note, I would have liked to see some intuitive explanation of Eq (4), e.g., I understand it as a soft-averaged (barycenter-weighted) extrapolated similarity matrix. I would suggest the authors to discuss its interpretation, even if briefly. 4. The paper mentions node measures being estimated from node degrees (e.g. L58). How exactly is this done? 5. Initialization. I might be missing something, but I just don't see how the approaches described in the ""Optimal Transports"" and ""Barycenter Graphs"" paragraphs are initialization schemes. In particular, the former looks like a regularization scheme applied to every iteration. Could the authors please provide more details about these? 6. Complexity Analysis. I appreciate the thorough yet concise complexity analysis of related methods. This is sadly less and less common in ML papers, so it's encouraging to see it here. One comment: I would suggest reminding the reader what d is in Table 1. 7. Results. I have various minor comments about the experimental results: * The adjusted mutual information should probably be spelled out explicitly (perhaps in the Appendix). I suspect many readers are not familiar with it or don't remember its exact form (I didn't). * Fig 1(e) would be more meaningful with two-sided error bars * What is q%|V| in L273? Is this a product? * Why are there no results for MultiAlign for > 3 graphs? Was it because of timeout? Please mention this in the paper. * NC@1 and NC@all could be better explained - it took me a while to understand what was meant by these Minor Comments/Typos: - L83. ""discrepancy"" repetition - L107. Two arbitrary nodes - L138. Convergence is not properly linear, but nearly-linear - a detail, yes, but an important one.",* Fig 1(e) would be more meaningful with two-sided error bars * What is q%|V| in L273? Is this a product?,779,0
NIPS_2019_1348,NIPS_2019,"0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent.",1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used.,780,0
NIPS_2019_1397,NIPS_2019,"of their approach, i.e., it assumes a jointly Gaussian distribution for correctness, and it suffers convergence issues with nearly-deterministic distributions. Clarity: I found the paper hard to read. First, the paper suffers from a very dense content, in particular extensive mathematical derivations and experiments, which the authors have decided to place in the supplementary materials. Also, a lot of statements are directly referring to content in the supplementary materials for justification, which brings the actual total length of the paper to 21 pages rather than 8. Still, the main 8-pages body remains understandable on its own. A second point, which I believe the auhtors can fix, is that the paper is quite confusing and even a bit misleading in several aspects, such as the time complexity of the approach, the exact goal of the approach (structure learning ? regularized density estimation ?), or some divergences between the theoretical presentation of the method and its implementation, that is, the factorization p(z|x) = \prod_j p(z_j|x), which are not discussed at all. See the detailled list of things that have to be fixed below: l.8-9: The proposed method has linear time complexity w.r.t. the number of observed variables -> This is misleading. The proposed criterion to be minimized can be computed in linear time, however the whole learning procedure to minimize this criterion is most likely not. l.18-19: is a holy grail in many scientific domains, including neuroscience, computational biology, and finance -> any reference ? l.22-29: For instance, graphical LASSO, [...] have better time complexity but perform very poorly in undersampled regimes. -> What does it mean to perform poorly ? For which task ? Density estimation ? What does it mean quantitatively ? At this point the text is very vague. Please state from the start which tasks those methods or your proposed method intend to solve, and which criterion permits to compare them objectively. l.30-31: a novel latent factor modeling approach for uncovering the structure of a multivariate Gaussian random variable -> This is very confusing. Your approach not only learns the structure of a multivariate distribution, but also does density estimation. There exists a whole litterature in graphical model structure learning which learns the structure only (i.e., just the graph), see [Murphy 2012, Ch.26]. Please clarify this, e.g., ""a novel latent factor modeling approach for estimating multivariate Gaussian distributions with modular structures"". l.33-34: when each observed variable has a single latent variable as its only parent -> You are suddently talking about parents here, but at this point you didn't state yet that you consider directed graphical structures. There exists undirected structures as well, which can also be latent models. Please clarify. l.36: is block-diagonal with each block being a diagonal plus rank-one matrix -> Why would each block be rank-one ? It is not obvious to me, I first figured this was an assumption you were making. Is that statement important for the paper ? l.36-38: This modular inductive prior is appropriate for many real-world datasets, such as stock market, magnetic resonance imaging, and gene expression data -> Why would that be ? Any intuition behind that ? Evidence ? References ? Figure 1: (Thm. 2.2) / (for Gaussians) -> This is very misleading, since Theorem 2.2 assumes a Gaussian distribution. Or reformulate Theorem 2.2 as ""a modular latent factor implies TC(Z|Xi), and the reverse is true for Gaussian distributions"". l.40: gets easier -> in terms of what ? Time ? Log-likelihood ? Structural error ? l.44: unconstrained graphical models -> unconstrained latent factor models l.68: constrains -> constraints Theorem 2.1: This is trivial, and is more a definition than a theorem. Random variables X and Z define a latent factor model iff. p(x,z) = \prod_j p(z_j) \prod_i p(x_i|z) <=> TC(Z) + TC(X|Z) = 0. Definition 2.1: i=1,2,...,p is redundant with \prod_{i=1}^p l.89: sometimes called clustering of observed variables -> reference ? l.93: from [7] -> yhis is bad style. Please name the authors or the approach. l.98: jointly -> multivariate ? Please be consistent in the text. Theorem 2.2: Fig. 1b equivalence -> this is a very bad name for a Theorem. For the sake of readability, either do not give a name or give a proper name, e.g, Modular Latent Gaussian Characterization. l.108: jointly Gaussian -> This is not a general parameterization, but rather a constrained Gaussian distribution where TC(Z|X) = 0. How does that interplay with TC(Z) = 0, TC(X|Z) = 0 and TC(Z|Xi) = 0 ? Is this important for Linear CorEx to work ? Please mention at least that this additionnal constraint is compatible with a modular latent model. This was very confusing to me. Equation (1): minimize_{z=Wx+e} -> minimize_{W} l.145: by increasing m until the gain in modeling performance is insignificant -> How do you measure a gain ? Log-likelihood ? l.148: The computational complexity -> The stepwise computational complexity Figure 3: in both cases s=5 -> Either say what s is here (signal-to-noise ratio), or don't mention it. Knowing what s means requires digging the text. l.208: black -> block l.208: each black being a diagonal plus rank-one matrix -> Why is it rank-one ? Why is that important ? l.246-247: we chose the number of factors [...] -> using which criteria ? Log-likelihood ? l.252-253: We omitted empirical covariance estimation since all cases have n < p -> Why ? I would still be curious to have the numbers to compare. l.271: session 014 -> Why ? this sounds a lot like cherry-picking. l.272-273: We do spatial smoothing by applying a Gaussian filter with fwhm=8mm -> Why ? Your method doesn't work if you don't do that ? Significance: The presented results are important, as they provide 1) an efficient way of learning modular latent Gaussian models, and 2) empirical evidence that this modular prior results in a better density estimation than other state-of-the-art approaches on a wide range of problems. Some general comments which the authors may want to address: - Suppose the X distributions contains variables Xi which are independent of the rest, i.e., X_i \ind X\{X_i}. Then such variables may not be linked to any latent variable, and still your minimized criterion can reach 0. The learned structure is a modular latent factor model. How do you deal with that situation in variable clustering ? Your procedure picks the latent variable with highest correlation, yet all correlations with that particular variable are 0. An easy answer is to assign each such variable to its own cluster. You may want to discuss that in the text. - You can simplify the proof for Theorem 2.2, and at the same time make it more general than only Gaussian distributions. Indeed, Gaussian distributions, among others, support the Decomposition, the Composition and the Singleton-Transitivity properties (See, e.g., Sadeghi 2017, Faithfulness of Probability Distributions and Graphs, p.7). Then you have: Z_s \indep Z\Z_s for all Z_s \subseteq Z (<=> TC(Z) = 0) Z_s \indep Z\Z_s | X_i for all Z_s \subseteq Z and X_i \in X (<=> TC(Z|X_i) = 0) Then due to the Singleton-Transitivity: Z_s \indep X_i or Z\Z_s \indep X_i Either both sides are true, in which case X_i \indep Z due to the Composition, and Z_s \indep X_i for all Z_s \subseteq Z due to the Decomposition. Or one side is false, for all Z_s \subseteq Z, in which case the intersection of all such Z_s or Z\Z_s sets leaves out a single Z_j, the unique parent of X_i","1) an efficient way of learning modular latent Gaussian models, and",781,1
NIPS_2020_281,NIPS_2020,"1. Unclear what argument some sections are making. Section 3.1, 3.2 lower bound the number of rounds of self distillation but it is not clear what insight does it give. There seems to lack motivation for this. Section 3.5 similarly bounds the sparsity level S_B, but I'm not sure for what purpose. 2. The objective function does not correspond to distillation as done in practice. In particular, in all steps, one has both the loss with respect to the original labels y and the teacher labels. The paper only analyzes the objective with the teacher labels. If one uses the original labesl as well, I'm not sure the same phenomena (solution collapse to zero, performance decline) will happen.","1. Unclear what argument some sections are making. Section 3.1, 3.2 lower bound the number of rounds of self distillation but it is not clear what insight does it give. There seems to lack motivation for this. Section 3.5 similarly bounds the sparsity level S_B, but I'm not sure for what purpose.",782,0
NIPS_2020_496,NIPS_2020,"The idea of applying Shapley values for the understanding of deep neural networks is not new. Several works, such as Lundberg et al., 2017, have already discussed the theoretical motivation for using Shapley values as an attribution method to rank the importance of the input features. Lundberg et al., 2017 also proposed approximations like KernelSHAP and DeepSHAP, which are not compared to TMAB-Shapley. Besides this line of works, the idea of using Shapley values to rank the internal neurons has been proposed by the Stier et al., 2018 (cited) and Florin Leon, 2014 (not cited) in the context of pruning. Finally, Ancona et al., 2019 (not cited) proposed an approximation technique for Shapley values tailored for deep neural networks. How does their method compare with TMAB-Shapley in terms of accuracy and performance? At least a theoretical comparison with Ancona et al. and Lundberg et al. would be needed to justify the authors' claim ""this algorithm enables us, for the ﬁrst time, to compute Neuron Shapley values in large-scale state-of-the-art convolutional networks"". I also believe that the experimental section is somewhat superficial. While providing several experiments, the paper does not go in-depth in any of them. This leaves several open questions: - [Neuron Shapley identiﬁes a small number of critical neuron] While it is interesting to see that the network mostly relies on a few very important neurons, what are the practical implications of that? Does it mean that it is possible to use this technique the other way around, and find the least important neurons that can be pruned? This would be a more relevant use case I believe, but it would also require a comparison with other structured pruning techniques. - [Class speciﬁc critical neurons] the baselines of comparison are rather weak as one is random and two do not consider how the information is processed after the activation (l2 norm of the weights and l2 norm of the response). I would expect Neural Conductance to be a comparison benchmark for each experiment, in this case, to be shown in Fig 2a. - [Discovering unfair ﬁlters] Does the inspection of the ""unfair"" filters and their activating samples from the training set shed any light on what these filters are capturing? - [Identifying ﬁlters vulnerable to adversaries] I did not quite understand the sentence ""We note that while the modiﬁed network is robust to the original adversary, it is still vulnerable to a new adversary speciﬁcally designed to attack the modiﬁed network. This requires a white-box adversary who knows exactly which few neurons are zeroed out."" Does it mean that the accuracy was tested using the same adversarial images pre-computed for the original network? If it is the case, and if the new network can still be fooled by running PGD on it, how can the method ""offer a fast mechanism to repair the model""? The authors would need to show that at least for some attacks (either white or black box) the robustness is improved when the new model is attacked from scratch. The comparison with Neuron Conductance, which is the only strong baseline, seems to be unfair as 25000 images are used to compute Neuron Shapley and only 100 to compute conductance (according to Appendix C). Why this is the case? This means that conductance is computed without providing not even 1 example per class. The reported computational time is also misleading, as conductance can be computed much faster (https://arxiv.org/pdf/1807.09946.pdf, a possible implementation https://captum.ai/docs/algorithms). Since the authors are using a quite unique setup (a large CPU cluster), I would suggest reporting the running times in terms on the number of network evaluations needed (either forward or backward). - [1] Ancona et al., 2019, Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Values Approximation, ICML 2019 - [2] Florin Leon, Optimizing Neural Network Topology Using Shapley Value, 2014, ICSTCC","- [Class speciﬁc critical neurons] the baselines of comparison are rather weak as one is random and two do not consider how the information is processed after the activation (l2 norm of the weights and l2 norm of the response). I would expect Neural Conductance to be a comparison benchmark for each experiment, in this case, to be shown in Fig 2a.",783,0
NIPS_2020_663,NIPS_2020,"- The contribution of the paper is too marginal. DRO with KL ambiguity sets has been extensively studied. For example, the nonparametric case, see Faury et al ""Distributionally Robust Counterfactual Risk Minimization"" (AAAI 2020) and Si et al ""Distributionally Robust Policy Evaluation and Learning in Offline Contextual Bandits"" (ICML 2020). These papers propose explicit wort-case distributions, tractable algorithms for the modified robust ERM (via reweighting), and complete theoretical analysis thereof (consistency, sample complexity, etc.). - Moreover, in the particular parametric case of DRO on exponential families (the subject of this manuscript) has already been studied in Hu et al. ""Kullback-Leibler Divergence Constrained Distributionally Robust Optimization"" - The paper lacks solid theoretical background. Since everything is parametric, I'd expect explicit rates of convergence involvind all probalem complexity parameters (n, m, p, etc.) To make the rest of my points clear, let me recall the following notations are used in the paper: - n: the dimensionality of the covariate (i.e feature vector) X. Thus X is random vector in R^n. BTW, in the context of ML or stats, I'd use another notation here, as n conventionally stands for ""sample size"". - m: the dimensionality of the output variable Y. Thus X is a random vector in R^m. - p: the dimensionality of the ambient space in which the model parameter theta lives. - N: the number of samples in the training dataset - (x_1,y_1),...,(x_N,y_N): and iid sample of size N from the unknown joint distribution of X and Y. - C the number of distinct feature vectors x_i in the sample. Thus 1 <= C <= N. WLOG, tet the distinct feature vectors be x_1,...,x_C. - N_c (with 1 <= c <= C): #{i | x_i = x_c}, i.e number of examples whose features vector equals x_c. - Line 95 to 98: Since the covariates (i.e features) are continuous, we are certain to have C = N and N_c = 1 for all c. - Line 99 to 102: The model in (5) has C + dim(Theta)^C, which is rougly N + dim(Theta)^N parameters in case of continuous covariates (see previous comment). This cannot possibly work as soon as dim(Theta) > 1. - Proposition 4.2: This should be rewritten to clearly outline the dependence on the sample size N. Also, the third term on the right is mysterious. Also, at what rates do kappa_1 and kappa_2 go to zero (if they do...). - Line 227: I don't see how you let ""N_c tend to infinity"" in view of my above comment on N_c = 1 almost certainly (see my previous comments). The rest of the analysis (Lemma 4.4) is therefore awkward. - Why are these experimental setups presented in section 5 relevant to the subject ? - Documentation on the datasets in Table 2 should be provided (n = ?, m = ?, etc.).","- C the number of distinct feature vectors x_i in the sample. Thus 1 <= C <= N. WLOG, tet the distinct feature vectors be x_1,...,x_C.",784,1
NIPS_2020_544,NIPS_2020,"- The introduction would benefit from stating the optimization problem the approach solves and the assumptions on the objective (currently, line 32 abruptly mentions “f” without even defining it). The introduction should also include an informal definition of the approximation error used in Table 1. - Experiments: benchmarking only against BR seems restrictive. Adding the comparison against ME (in terms of accuracy and timing) would have been helpful. - Experiments: it would have been great to see more realistic experiments to support the claims that BMR can benefit optimization for ML and deep learning. Currently, seeing a 10-dimensional problem with a toy objective is a bit disappointing. In particular, since the approximation error in RS increases with the dimension, it would have been great to at least see tests with increasing dimension “d”. - after eq (10), the paper says that the bias is almost in O(sqrt(K)), which seems incorrect. Am I missing something? - BMR seems to have a lot of oscillations in Fig 3. Is it expected?","- Experiments: it would have been great to see more realistic experiments to support the claims that BMR can benefit optimization for ML and deep learning. Currently, seeing a 10-dimensional problem with a toy objective is a bit disappointing. In particular, since the approximation error in RS increases with the dimension, it would have been great to at least see tests with increasing dimension “d”.",785,0
NIPS_2020_1104,NIPS_2020,"1. While the exact architecture may not have been tried on this problem (or even otherwise), the architecture builds upon existing ideas in the field (casting grasp prediction as grasp proposal classification [24]), sim2real for grasping using depth data [17], uses of anchors [30], PointNet++[27]). Thus, I won't consider the novelty of the proposed architecture super high. 2. At the same time, experimental evaluation could be stronger. Given limited novelty, I would expect a through empirical evaluation of the different aspects of the proposed model, eg: is PointNet++ style architecture necessary, or would a more natural choice 3D CNN (or 2D Depth CNN as in DexNet) work better. 3. Why GPNet-Naive only report r = 10, b = 22, while a more through validation has been done for GPNet? 4. Use of non-standard evaluation metrics. Past work in [23] seems to use plots of success vs coverage. Current paper only reports 4 points on this curve. Why not include plots similar to ones in [23]? 5.Past papers in learning based grasping (albiet in 2D) employ a more thorough real experimental evaluation. By that measure the real world evaluation is weak.","2. At the same time, experimental evaluation could be stronger. Given limited novelty, I would expect a through empirical evaluation of the different aspects of the proposed model, eg: is PointNet++ style architecture necessary, or would a more natural choice 3D CNN (or 2D Depth CNN as in DexNet) work better.",786,0
NIPS_2020_706,NIPS_2020,"1) The experiments are simple toy simulations. I don’t know if this is standard practice in IV regression but it would be desirable to use benchmarks if available. However, the high-dimensional experiment, taken from prior work, is a more challenging/convincing test. The slave export dataset is unconvincing since it only shows that the method gives a similar estimate as another method which requires stronger assumptions but we do not know if that estimate is correct. 2) The methods section and introduction could be written more clearly. Before publishing, I would recommend to give them a thorough re-write. 3) A1 and A2 are not clearly explained. Furthermore, it should be discussed how realistic these assumptions are. Although its formulation should be improved, in my view, theorem 1 is a useful result and leads to a well-justified algorithm VDE. As far as I can see these are novel insights. So I lean towards acceptance.","2) The methods section and introduction could be written more clearly. Before publishing, I would recommend to give them a thorough re-write.",787,0
NIPS_2020_240,NIPS_2020,"1. How do you calculate $Y_i(T_E^{(r)})$ for a single member given Pa(i) and Ch(i)? Are p_{ij}^{(r)} and p_{ij}^{base} given or calculated from real data? How to do it? 2 Please check line 152. You have {\bf Pa}(i). Have you ever define it? 3. The proposed method depends on the existence of a set of ""intervening variables"". My question would be how to ensure that such existence is a reasonable one. Is there a way to check these two assumptions in practice. 4. In Line 166, where is $\Omage_r$ in Figure 1b? Please clarify. 5. In line 5 of Algorithm 1, what is (4)? Please clarify. 6. The authors claimed that the bootstrap method works in this case. Is there any theoretical result to ensure this?","3. The proposed method depends on the existence of a set of ""intervening variables"". My question would be how to ensure that such existence is a reasonable one. Is there a way to check these two assumptions in practice.",788,0
NIPS_2020_971,NIPS_2020,"- It would be better to also evaluate how the model's performances change as it sees more tasks during training. - Other types of more recent meta-learning baselines such as PEARL should be also included in the experiments. - I am curious to see if this kind of methods can be applied to reinforcement learning problems and achieves reasonable performance as well, where the training can be less robust and stable. - In Table 3, the evaluation is conducted only for 3 different seeds. It would be better to have experiments of larger scales.",- Other types of more recent meta-learning baselines such as PEARL should be also included in the experiments.,789,0
NIPS_2020_704,NIPS_2020,"1. This work is not well-motivated. It seems estimating the distribution is not necessary since the variance of the learned parameters for each item (teams, fund managers, etc.) already indicates whether the parameters of all items closely center somewhere or spread uniformly. So I don't see the point of a sophisticated second step in the algorithm. This paper does not provide a comparison in the experiments between their approach and the naive approach mentioned above either. 2. Most of this paper seems very technical but the theorems do not qualify as theorems. ""sufficiently large constants"" and ""sufficiently large n"" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.","2. Most of this paper seems very technical but the theorems do not qualify as theorems. ""sufficiently large constants"" and ""sufficiently large n"" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.",790,0
NIPS_2020_545,NIPS_2020,"I do have several questions: - Did the hyper-ensemble paper forced the networks to start form the same initialization point? I looked into the paper in ref[12] for this information but couldn't tell. If so, then the work will need a more justification with the difference w.r.t hyper-ensemble. - In page 5, starting from line 172 it was not clear why o(mk) became o(k^2), can you elaborate more on this? - From table#1 and table#2 it seems that hyper-ens, str hyper ens and deep ens are quite close to each other in nll,acc,ece ranges. What's exactly the range if improvement of using str-hyper-ens over the others? - In tables#1,2 what is the meaning of the numbers in brackets (1),(4)? - I understand that the empirical evaluation is expensive, but reporting results on other deep models such as VGG, ResNet, DenseNet for a small subset of the settings will clear any doubts regards that the method only works best for wide-resnet. - On the same point of wide-resnet as in lines 269-272 for using two deep ensembles, what are the results of this comparison for wideresnet?","- I understand that the empirical evaluation is expensive, but reporting results on other deep models such as VGG, ResNet, DenseNet for a small subset of the settings will clear any doubts regards that the method only works best for wide-resnet.",791,0
NIPS_2020_56,NIPS_2020,"While there are nice technical contributions here for a narrow set of models, a few factors may limit impact: * For the free energy prediction of Eq. (7), the ""brute force"" sum over the factorial number of permutations of the variables in each factor will limit application to low-degree models, while BP can be applied to some high-degree graphs whose factors have tractable structure. * Experiments focus exclusively on constraint satisfaction models, or models (like stochastic block models) which are defined by ""soft"" constraints. For such models, failure to converge is known to be a major issue for BP, and the BPNN-D dynamics have room for improvement. But for many other types of models, the larger issues are around accuracy of BP fixed points rather than convergence dynamics, and this approach may not be provide benefits. * To train the BPNN algorithm, a family of related models (about 50 instances) with true partition functions is required. This means the method is only useful when many closely-related problems need to be solved. The ""generalization"" demonstrated in the experiments in the paper is underwhelming (e.g., from 10x10 to 14x14 Ising models). * The training and experiments focus on the prediction of partition functions, which is indeed sometimes useful. But in practice, BP is more frequently used to compute approximate marginals of variables, and this is not addressed by the work here. For the variant of BPNN that modifies BP fixed points, there seem to be unexplored connections to other generalized BP algorithms that modify message passing updates to boost accuracy, like fractional/reweighted BP: * A new class of upper bounds on the log partition function, M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky, IEEE Trans. Info. Theory, July, 2005. * Fractional Belief Propagation, Wim Wiegerinck, Tom Heskes, Neural Information Processing Systems 2002.","* For the free energy prediction of Eq. (7), the ""brute force"" sum over the factorial number of permutations of the variables in each factor will limit application to low-degree models, while BP can be applied to some high-degree graphs whose factors have tractable structure.",792,0
NIPS_2020_183,NIPS_2020,"- The paper didn't cite a very relevant line of work on model compression (by for instance Caruana et al) but only focuses on the more modern take that goes under the brand of ""distillation"". Distilling ensembles of teachers given by previous generations of students is in fact a setting that is arguably very related to multi-generational self-distillation. Moreover, the conceptual framework provided by ensemble learning could have given rise to some interesting hypothesis as to how diversity in prediction uncertainty comes about and how it is useful at test time. - The presentation could be slightly improved. In particular, the final regularized loss that is being optimized is never written down explicitly, which hinders a little bit the clarity of the exposition.","- The paper didn't cite a very relevant line of work on model compression (by for instance Caruana et al) but only focuses on the more modern take that goes under the brand of ""distillation"". Distilling ensembles of teachers given by previous generations of students is in fact a setting that is arguably very related to multi-generational self-distillation. Moreover, the conceptual framework provided by ensemble learning could have given rise to some interesting hypothesis as to how diversity in prediction uncertainty comes about and how it is useful at test time.",793,0
NIPS_2020_1665,NIPS_2020,"-The paper suffers from several weaknesses, in particular with regards to its clarity. The method is described in a very confusing way, making it challenging to understand the building blocks and how they are arranged. Providing e.g. pseudo code describing the whole process, and more specifically the training process would greatly help comprehension. For example: the sampling process for S or where Q(z) computation fits in the whole training process. Figures also lack clarity, in particular Figure 1 which doesn't provide a clear distinction between the 2 compared strategies. - Authors miss an important category of related work relying on class similarity graphs (or knowledge bases) to generate unseen features. Such methods learn to generate features by implicitly exploiting similarities between classes. This contradicts the claim in the introduction l23 stating that there is no existing method leveraging shared concepts across classes. See relation to prior works for more details. - It is mentioned multiple times that the approach uniquely concatenates features instead of averaging them, yet use the classification strategy of DAZLE, which compares attribute specific features to each attribute. Therefore, the feature construction and sampling stage isn't clear. Is a feature vector constructed per attribute? What happens if an attribute is missing? Is S sampled such that all attributes are present?","- It is mentioned multiple times that the approach uniquely concatenates features instead of averaging them, yet use the classification strategy of DAZLE, which compares attribute specific features to each attribute. Therefore, the feature construction and sampling stage isn't clear. Is a feature vector constructed per attribute? What happens if an attribute is missing? Is S sampled such that all attributes are present?",794,0
NIPS_2020_1628,NIPS_2020,"1. As the Taylor estimator is a keystone in the proposed model, and the motivation is to improve the estimator for discrete random variable. This paper lacks the essential analysis and comparation between Taylor estimator and traditional methods, such as the Gumbel-Softmax and Gaussian-Softmax. 2. This paper lacks the data or experiment to verify the outperformance of Taylor estimator than other simplex on discrete random variable. I suggest authors to use the synthetic data to verify whether Taylor estimator has a more continuous modality than Gumbel-Softmax and others. 3. Authors didn’t make any case study to illustrate the generated text from TaylorGAN, even though they had compared the FED and LM scored in Table.1.","1. As the Taylor estimator is a keystone in the proposed model, and the motivation is to improve the estimator for discrete random variable. This paper lacks the essential analysis and comparation between Taylor estimator and traditional methods, such as the Gumbel-Softmax and Gaussian-Softmax.",795,0
NIPS_2020_1749,NIPS_2020,"(-): The proposed method does not precisely correspond to the motivation. (-): Some technical details and experimental settings are not clear. The negative points are explained more specifically as follows. (1) The authors claimed the main attack strategy as “a kernel density estimation approach to push attacked nodes to dense regions in two graphs, such that they are indistinguishable from many neighbors” and focus on deriving the KDE in Section 3. However, in the objective function Eq. 9, KDE is only adopted as a sort of regularization. I feel it is the first term, which explicitly pushes away ground-truth matching points, that really matters and it does not depend on the sophisticated KDE. The authors should prove that the KDE term is indeed helpful, e.g., by an explicit theorem or conducting an ablation study. (2) I am also not sure why the authors claimed using KDE “reduces the possibility of perturbation detection by humans or defender programs” since the level of attack, i.e., whether the permutation is perceptible or not, is only determined by the budget. (3) As for the experimental setting, it seems that most baselines are designed for GNN-based node classification tasks and how to adapt them in the graph matching problem remains unexplained (e.g., do you still use misclassifying node labels as the objective function?). In addition, following (1), I think the authors should also compare with the most intuitive method of directly maximizing the first term in Eq. 9. (4) Also, what’s the adopted projection M function? Do you use a surrogate model as [98] or do you need the actual graph matching model?","9. (4) Also, what’s the adopted projection M function? Do you use a surrogate model as [98] or do you need the actual graph matching model?",796,0
NIPS_2020_916,NIPS_2020,"1. I think the conclusion ""the Lipschitz constant is tightly and inversely related to the loss"" is somehow trivial. Could the authors explain to us whether there are no theoretical explanations before, or you just use some novel methods to reach this conclusion? What are the differences between your conclusion and the others (e.g. different settings)? 2. Since two types of problems are considered here, could the authors further explain some more connections between these two problems except for their similar forms? 3. I think the problems this paper tries to solve are not motivated very well. Some more discussions could be added to better show the importance of this problem.",3. I think the problems this paper tries to solve are not motivated very well. Some more discussions could be added to better show the importance of this problem.,797,0
NIPS_2020_1067,NIPS_2020,"1. My main concern is that the authors did not compare the proposed method with any latent baselines. For example, we can also use to the uncertainty (the loss value or the entropy of the predicted distribution) of the model as an indicator to identify those problematic examples in the pre-training data. As the proposed method in this paper does not show very impressive results in the experiments (the Pearson’s correlation is only 0.4~0.6 in Fig. 1), it may not outperform this simple baseline. 2. In the experiments, the transfer tasks come too artificially. “At the pretraining stage, we train the models with examples from two classes (“bird"" vs. “frog"") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST”. The transfer tasks in these settings may be too easy. In addition, the experiments are limited to small datasets like MNIST and CIFAR-10. I wonder how the proposed method scale to larger datasets or more challenging transfer tasks.","2. In the experiments, the transfer tasks come too artificially. “At the pretraining stage, we train the models with examples from two classes (“bird"" vs. “frog"") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST”. The transfer tasks in these settings may be too easy. In addition, the experiments are limited to small datasets like MNIST and CIFAR-10. I wonder how the proposed method scale to larger datasets or more challenging transfer tasks.",798,0
NIPS_2020_1831,NIPS_2020,"- One coid model assumption may not fit well with crowdsourcing tasks that have dramatically different item difficulties, which could be a potential issue. - A few key refererences that are highly related are not compared and explained. - Experiments needs further improvement to show the full effectiveness of the proposed framework.","- One coid model assumption may not fit well with crowdsourcing tasks that have dramatically different item difficulties, which could be a potential issue.",799,0
NIPS_2020_964,NIPS_2020,"A. Major concerns 1. Lines 277-282: ""We see that s for each channel and layer are generated differently for each dataset according to what has been learned in the meta-training stage."". If the weights of the affine transformation that produces s (lines 181-182) is a meta-parameter, then they are shared across all datasets/tasks. It would have been interesting to have a visualization of these weights, in addition to the values of s (since it is hard to understand what are the differences in image statistics from one dataset to another). B. Moderate concerns 1. Lines 181-182: ""We finally generate the scales s_1, ..., s_C with a shared affine transformation and a sigmoid function, and collect them into a single vector s = [s_1, ..., s_C] \in [0, 1]^{C}."". Can you comment on the difference between this transformation and (Perez et al., 2017)? 2. Lines 286-287: ""It is known that flatter loss surface is closely related to generalization performance, which partly explains why our model generalize well."". I believe this is an unsolved issue (and possibly controversial) in the Deep Learning community. See e.g. (Dinh et al., 2017). C. Minor concerns 1. Line 205 says that \phi is 84 dimensional, whereas line 192 says 82. Is this a typo? 2. Line 205: ""we only need to share the low-dimensional (e.g. 84) meta parameter \phi between the GPUs without sequential alternating training over the tasks"". Are the gradient wrt. \phi also shared (they have the same dimensionality though)? Are these updates of \phi asynchronous (i.e. some datasets/GPUs are using different versions of the meta-parameter \phi)? (Dinh et al., 2017) Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio. Sharp minima can generalize for deep nets (Perez et al., 2017) Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer","2. Line 205: ""we only need to share the low-dimensional (e.g. 84) meta parameter \phi between the GPUs without sequential alternating training over the tasks"". Are the gradient wrt. \phi also shared (they have the same dimensionality though)? Are these updates of \phi asynchronous (i.e. some datasets/GPUs are using different versions of the meta-parameter \phi)? (Dinh et al., 2017) Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio. Sharp minima can generalize for deep nets (Perez et al., 2017) Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer",800,0
NIPS_2020_22,NIPS_2020,"1. High level comment: Since the proposed solutions are both based on a counterfactual as well as interventional query, it seems important to justify which framing is more appropriate for the recourse task. On the face of it, recourse is a counterfactual query rather than an interventional one. The justification and motivation for each potential formulation is unclear and it would be good if the authors incorporate that in their motivation. 2. Authors do not address the issue of feasibility in the level of detail as is warranted for recourse. This makes the formulation a little impractical for actual practice. Can the authors clarify the details of feasibility or enumeration of feasible actions? All comments in the paper allude to searching over potential intervention sets \mathcal{I}. However, there are causal dependencies in the graph which can determine allowable feasible sets. It is unclear how to address this challenge here. 4. Is the learned CVAE completely respect the causal graph? The procedure of training CVAEs for interventional recourse should be clarified in further detail. 5. In experimental evaluation, I did not see a comparison to existing baselines in recourse, neither a qualitative assessment of the type of recourses that the model learns.","5. In experimental evaluation, I did not see a comparison to existing baselines in recourse, neither a qualitative assessment of the type of recourses that the model learns.",801,0
NIPS_2020_495,NIPS_2020,"- The paper mostly focuses on quite small experiments. This is okay as the paper is mostly concerned with analysing the higher order behaviour of neural ODEs and getting a detailed understanding of this. However, it would be nice to see larger scale experiments if possible. Are there any larger scale physics (or other second order) datasets that could be tested? What about modeling e.g. n-particle dynamics? - The proposed model is limited to second order behaviour even though higher order behaviour is not uncommon in real life physical systems.",- The proposed model is limited to second order behaviour even though higher order behaviour is not uncommon in real life physical systems.,802,0
NIPS_2020_361,NIPS_2020,- The definition of hard and easy examples is limited to their respective confidence scores or losses. - The paper partially illustrates a potential challenge in the current state-of-the-art technique but does not elaborate on it. - The model does not considerably improve performance metrics and is often on par with other approaches. - The paper lacks experiments illustrating turnaround training times of competing models.,- The paper lacks experiments illustrating turnaround training times of competing models.,803,0
NIPS_2020_765,NIPS_2020,FOR IMPROVING SCORE -------------------------------- - Develop and motivate a bit further the construction of the revenue function at the bottom of page 1 and if surrogates could be used. - Flesh out the literature review and spend more time contextualizing the results. - Share how the NP-hardness result is surprising if it is. - The broader impacts mentions pricing for online marketplaces; mentioning this in the motivation for *why* one might want to learn the Myerson price would strength the motivation of the paper. ================= AFTER RESPONSE ================= - Thank you for the elaboration re: the contextualization of the hardness result.,- Flesh out the literature review and spend more time contextualizing the results.,804,0
NIPS_2020_1161,NIPS_2020,"My main objection is that the generalization bound in Equation (8) does not easily translate to a bound on the excess of risk. It results that there is a clear lack of statistical evidence that the proposed estimator does minimize the population version of the risk. Thus, except empirically, what proves that the method is sound from a statistical point of view? That is, that the estimator achieves low error on average? As a more general question: is empirical risk stabilization as sound as empirical risk minimization? Minor comments: - Page 2, Line 86: ""operaor"" is a typo. - Page 3, Line 97: use \citep and \citet respectively. - Page 3, Line 126: sentence has to be refactored. - Page 4, Line 149: inequality for kernels has to be defined. - Page 4, Line 159: ""a an"" is a typo. - Page 5, Line 186: symbol \oplus has to be defined. - Page 7, Line 287: ""for \forall"" is a typo.","- Page 3, Line 97: use \citep and \citet respectively.",805,1
NIPS_2020_171,NIPS_2020,"- The proposed ‘grad drop’ seems been the main idea of this work, which seems not adequate to be total contributions of a NeurIPS paper. - The previous paper ‘Tseng, et al: Regularizing Meta-Learning via Gradient Dropout, arXivPrePrint, 2004.05859’ proposed similar idea for gradient dropping but mainly for meta-learning and with manually adjust distribution parameters, which reduces novelty. Also, this derives another consideration: Is the determination of probability in Equation 1 generally better than manually selected constants? Ablation study on this part is needed. - The method can surely guarantee the consistency since Bernoulli distribution will certainly not have parameters $p=1-p=0$, which makes proof of proposition 1 somewhat trivial. - ‘The key’ described in L136 and L137 is correct but the extension on ‘stable points under joint minima’ needs more concerns: Is the joint minima always existing in multitask learning? L27 introduced the definition of joint minima but failed to mention the evaluation of ‘near the local minima’. For instance, the combination of function $(x-2)^2$ and $(x-1)^2+3$ has minima point at $x=1.25$ but their own minima respectively lay on $x=1$ and $x=2$. In this condition, is the $x=1.25$ ‘near’ enough to be a joint minima? I call for possible theoretical or empirical explanations. - Graddrop actually selects part of loss functions but not total loss to optimize targets. It preserves consistent gradient to pursue joint minimum but the convergence of this algorithm is not proved. If joint minima do exist in a task, how does Graddrop ensure weights converging at ‘stable points’ but not ‘jumping across’ minima of different loss functions? If theoretical analysis is difficult, experiments on convergence are also acceptable. Figure 3 (c) failed to show conspicuous convergence of GradDrop. - The pre-multiplication for extension to batch-separated gradients in 3.2 is hard to be understood. See details in Correctness below. - SGD and PCGrad are trapped in local minima in experiment of section 4.1 regarding Figure 2 (a) and (b). Would reset the learning rate schedule be helpful for these methods? - From Figure 3 (a), the GradDrop achieves the highest maximum F1 in initial stage of training but all of other methods actually HARM the baseline in this stage. In later period, the GradNorm outperforms GradDrop for the reason explained in L189. These two situations weaken persuasion of GradDrop’s effectiveness and universality. Similar problems occurs in ‘Error Rate’ and “Max F1 Score’ columns in Table 1. - The Err Rate and Max F1 Score on CelebA combining GradNorm and GradDrop benefits the former but actually harms the GradDrop itself (8.52->8.57, 29.57->29.50), comparing Table 1 and 4. The performances on Waymo meets similar problem. Although the synergy certainly improves performances of GradNorm as a modular part, the synergy’s harmful impact on GradDrop itself weakens this property. Also, the limited success of GradDrop+MGDA in L239 and L240 makes the experiments on synergy with ‘Other Methods’ either not persuasive or not enough. - Most of experiments are taken with GradDrop added on the final layer between prediction heads in section 4. Providing experiments results of adding on other layer will be favorable.","- From Figure 3 (a), the GradDrop achieves the highest maximum F1 in initial stage of training but all of other methods actually HARM the baseline in this stage. In later period, the GradNorm outperforms GradDrop for the reason explained in L189. These two situations weaken persuasion of GradDrop’s effectiveness and universality. Similar problems occurs in ‘Error Rate’ and “Max F1 Score’ columns in Table 1.",806,0
NIPS_2020_1875,NIPS_2020,"The writing quality and missing details degrade my rating of the paper: 1. The mismatch between math equations and intentions: 1.1. In Line 90, I bet f_t (x) should be the final output (a vector/scalar) of a net, but the sum is just an input to a neuron in the next layer; 1.2. The use of \epsilon and \varepsilon is ambiguous. I bet \epsilon is a small threshold such that new neurons don't change the loss much. However, in Line 101 & 105, I bet \varepsilon should be used. 1.3. in Step Two between Line 121 and Line 122, for a standard Taylor approximation, I bet s_i should be just a \Delta L. Please explain why it is an integration. 1.4. between Line 166 and Line 167, readers can be confused if f_{1:t} (x) is a sequence of functions from step 1 to step t, or just a function at step t. If the latter, what's its difference from f_t (x)? 1.5. ""the candidate set of f_t+1 should consist of"" is confusing. Why a set is just a function? 2. some important but missing details (see comments in the ""Clarity"")","2. some important but missing details (see comments in the ""Clarity"")",807,0
NIPS_2020_1064,NIPS_2020,"1. Under the identification assumption, if other causal functionals exist, such as regression adjustment and IPTW, what is the advantage of the WERM-ID? 2. In the empirical simulation, there proposed method is only compared with plug-in estimands. What about other weighting methods? 3. The paper contains many technical details but lacks a detailed explanation. If the page limit is a major concern, the author may need to decide what are the essential messages to express and explain them very clearly, instead of thrusting and skipping over many materials. See the Clarity section for more details. ###### post rebuttal comment: The author's feedback addresses the major concerns of the reviewer. I modified the evaluation accordingly to marginal above acceptance.","3. The paper contains many technical details but lacks a detailed explanation. If the page limit is a major concern, the author may need to decide what are the essential messages to express and explain them very clearly, instead of thrusting and skipping over many materials. See the Clarity section for more details. ######",808,0
NIPS_2020_272,NIPS_2020,"- The method requires images acquired by a calibrated camera to backproject the 2D features to the 3D grid. Most competing methods do not have this assumption. - The subdivision of tasks between the creation of the occupancy map and the regression of the pointcloud in the decoder is quite interesting and seems effective. However it exposes the method to an obvious possible failure: if the occupancy map is not regressed correctly the point cloud part of the architecture has no way of fixing the prediction. To my understanding, in the case of an occupied voxel being predicted as empty the local folded patch of point will be completely ignored, while for an empty voxel predicted as full the predicted point will be inevitably considered and will add unwanted points to the final prediction. - The main advantage of the method seems to come from reasoning locally in the voxel grid space and creating the final point cloud as the stitch of many local patches (way more than the main competitors were able to do before). The focus on local regions however can also be one of the drawbacks of the method since contiguity between predicted nearby patches is not enforced in any way. The method might easily generate shapes that are not properly “closed” at the intersection between regressed patches. A comment by the authors on this point will be interesting. - Some additional ablation studies would have been nice to motivate the design choices made by the authors. For example what happens experimentally if you clamp projected features using depth? The explanation between line 136 and 142 it’s reasonable but I would have liked to see it supported by experimental results. Moreover what is the impact of using multiple scales for the elaboration? How much would the performance degrade using a single scale? - On the real Pix3D dataset the method is tested only on the chair class, is there any reason to not test on all classes? - Some implementation details of the method should be made more clear, but I think that this weakness can be addressed in a revised version of the paper, see my questions to the authors below.",- Some additional ablation studies would have been nice to motivate the design choices made by the authors. For example what happens experimentally if you clamp projected features using depth? The explanation between line 136 and 142 it’s reasonable but I would have liked to see it supported by experimental results. Moreover what is the impact of using multiple scales for the elaboration? How much would the performance degrade using a single scale?,809,0
NIPS_2020_450,NIPS_2020,"- The setting is restrictive, as it requires a setting where the barycenter support must be pre-specified. In general, the ideas for solving the fixed support problem do not seem to generalize to the general Wasserstein barycenter problem. - The algorithm they propose does not achieve the best complexity bound in all regimes. There seems to be a tradeoff between approximation factor and size of the empirical distributions, since it is obviously faster than IBP (it is an accelerated version), but it does not achieve a better dependence on approximation factor when compared with accelerated IBP and APDAGD. However, it does have a better dependence on the size of the empirical distributions and barycenter, n, than these two methods. This tradeoff is not explored in the paper. Not hints as to interesting theory from the analysis of FastIBP are given. -The explanation of the algorithm is short, and instead of giving heuristic ideas they instead just give the steps of the algorithm. Can anything be learned from this method or is it actually just taking some past work and applying it to this problem? Why is it able to achieve the better dependencies over other methods? - The authors don't compare with some mentioned methods, accelerated IBP and APDAGD, in the experiments. - It is unclear whether the assumption that all measures are supported on the same number of points necessary for these results - is the extension easy based on these results? Does the algorithmic complexity result extend?",- It is unclear whether the assumption that all measures are supported on the same number of points necessary for these results - is the extension easy based on these results? Does the algorithmic complexity result extend?,810,0
NIPS_2020_651,NIPS_2020,"Despite the interesting theoretical and experimental results, there are several caveats. * In lines (131-139) the paper claims that the adversarial perturbations generated with feature normalization technique are more effective. This claim was never confirmed later. Although models trained adversarially with Hypersphere Embedding (HE) seems to enjoy more robustness characteristics, this could be due to very different reasons, other than the aforementioned, e.g. the learnt representation is more clustered under HE. To demonstrate such a claim, an experiment like the following would be required: generate adversarial attacks against models trained with HE (e.g. TRADES + HE) to fool models trained without HE (e.g. TRADES). If the attack success rate is larger with such attacks, then the claim is confirmed. * It would be sufficient to replace Figure 1 with an actual synthetic experiment on small 2-D dataset to confirm. * The idea of feature normalization for better learning on hard adversarial examples is elegant. However, even normalizing the values to the range [-1,1] might not solve the problem. Still, the larger values will have larger contribution, right? The paper lacks some discussion on that aspect. * The modified version of HE in section 3.5 does not co-op with the idea of the paper. The dependence on the value of the angle rather than the cosine will alleviate what was mentioned earlier about the weak and strong examples. Moreover, the method seems to have negligible improvement only with adversarial training, and not with either ALP nor TRADES (Table 2 in the appendix). The paper does not elaborate on the idea behind such phenomenon. Perhaps moving the section with these experiments (along with results) to the appendix would suffice. * The paper reports the results against adaptive attacks when HE was combined with adversarial training only. The paper would improve by reporting more extensive results studying this aspect (similar to Table 2). * It is necessary to have at least a brief discussion about the related work in the main paper.",* It is necessary to have at least a brief discussion about the related work in the main paper.,811,0
NIPS_2020_357,NIPS_2020,"- Experiments don't really compare against anything but AIS (which is a reasonable baseline). There are lots of other (approximate) inference techniques that should be compared against. - For image segmentation, it might be good to place this in the context of the state-of-the-art for this task (even though, of course the goals isn't necessarily to beat this). - The partition function approximation scheme only really makes sense to me very close to the zero temperature limit. I don't think that this approach would really be super interesting beyond that. For comparison, how does importance sampling with say a naive mean field (fit) + plus uniform as a proposal do for these problems. I'd be surprised if it is too different. Again, I think it is important to have more baselines here.","- For image segmentation, it might be good to place this in the context of the state-of-the-art for this task (even though, of course the goals isn't necessarily to beat this).",812,0
NIPS_2020_1178,NIPS_2020,"1. Although weight sharing across different flows reduces the model size, it does not improve inference speed. The proposed model still computes same number of flows. 2. Weight sharing with embedding and projection layer is not novel. The authors applies the method on flow based models. The paper is an incremental work.","1. Although weight sharing across different flows reduces the model size, it does not improve inference speed. The proposed model still computes same number of flows.",813,0
NIPS_2020_207,NIPS_2020,"- I think this paper is a natural extension/combination of previous works [1] and [2], especially the bayesian formulation and the resulting algorithm. And the paper does not give enough insights/analysis of why such a bayesian framework provides special advantages in the context of multi-agent learning. For example, the authors may try to create some toy examples/experiments to demonstrate why the bayesian formulation is necessary and why the previous methods failed. Therefore, I'm not sure if the paper provides new insights/substantial technical contributions. - Following the same line, I think the paper should be better motivated in the next version. Before delving into the derivations (how to do bayesian MAIL), I don't quite understand why we should do bayesian MAIL. - It seems the theoretical analysis mainly discuss what difference will the approximation make when we do Nash Q learning. Although this is useful, this seems irrelevant to the main topic, i.e. multi-agent imitation learning (hence a gap between theory and practice). So the paper should also discuss more on what difference will it make in multi-agent imitation learning. Otherwise, the main claim should be the mean-field approximation. - I think a detailed experiment setup (environments illustration, tasks description, evaluation metrics, etc) is missing at the beginning of the experiments section. So I feel this part is a bit hard to follow. [1] A bayesian approach to generative adversarial imitation learning. [2] Multi-agent generative adversarial imitation learning.","- Following the same line, I think the paper should be better motivated in the next version. Before delving into the derivations (how to do bayesian MAIL), I don't quite understand why we should do bayesian MAIL.",814,0
NIPS_2020_674,NIPS_2020,"1. The major concern of the reviewer is on the novelty. All the adopted techniques are well-known, this paper just combines previous techniques into one framework. 2. FL with MPC is especially susceptible to poisoning attacks as the individual updates cannot be inspected. 3. Lack of theoretic support on the statement ""Our framework can provide strong privacy guarantees against colluding parties with unbounded computational power"". 4. Lack of comparison with the most recent MPC protocols, all the compared protocols were proposed 10 years ago. 5. Figure 3 is weird, training time of CodedPrivateML even decreases with the increasing number of of clients N, which seems counterfactual, any explanation here? 6. Implementation on simple logistic regression model cannot fully validate the scalability of CodedPrivateML. It is expected to see how CodedPrivateML performs on more complex DNN models.",2. FL with MPC is especially susceptible to poisoning attacks as the individual updates cannot be inspected.,815,0
NIPS_2020_1220,NIPS_2020,"1. Algorithm 1 is proposed to solve Eq. (1), but the derivation process is not very clear and the notations are somewhat confused (e.g. X denotes the input, does it mean the same thing in Algorithm 1?). 2. It is declared in Sec. 3.2 that ""DARP increases at most 20% of the overall training time of an existing SSL scheme"". I don't quite understand. 3. The experiments show that DARP could improve the performance of the SLL methods. But I observe that the results of SLL methods which are not specifically designed for imbalanced SLL (VAT, Mean-Teacher...) are comparable with that of imbalanced SLL methods. This may seem a little strange, so I wonder the reason.","3. The experiments show that DARP could improve the performance of the SLL methods. But I observe that the results of SLL methods which are not specifically designed for imbalanced SLL (VAT, Mean-Teacher...) are comparable with that of imbalanced SLL methods. This may seem a little strange, so I wonder the reason.",816,0
NIPS_2020_632,NIPS_2020,"- My main concern is regarding the reproducibility of this method. This is a heavy black box model. Implementation details are almost entirely missing (even the number of layers is not specified). No code is currently published. Will the authors publish the full code and evaluation settings that would allow full reproducibility of these results ? I will be willing to increase my current rating if this issue would be addressed. - Claimed contribution in lines 77-78: the authors do not mention, cite or use as baselines any of the recent works on products of constant curvature spaces, e.g. [1,2,3]. Technically, these methods could jointly learn both Euclidean and hyperbolic representations. How does the proposed model compare against GNNs that embed graphs in products of constant curvature spaces with fixed or learnable curvatures, e.g. [2] ? - Line 32: ""although they may suffer the problem of permutation variance "" --> as far as I am aware, none of the hyperbolic GNN models suffers from this problem. Can the authors detail this statement? - line 119: another option is to use gyro-midpoints or Riemannian Frechet means, e.g. see [2,4]. - lines 158-160: ""It is worth mentioning that after the feature fusion, the node embeddings [...] maintain properties and structures of the original space."" -----> This reads like a vague statement to me. Can you please clarify why does it preserve those ? - line 252: ""We evaluated all methods over the same random seed for 10 runs"" --> why do you observe any differences if you use the same random seed ? [1] Learning mixed-curvature representations in product spaces, Gu et al, ICLR 2019 [2] Constant Curvature Graph Convolutional Networks, Bachmann et al, ICML 2020 [3] Mixed-curvature Variational Autoencoders, Skopek et al, ICLR 2020 [4] Differentiating through the Fr\'echet Mean, Lou et al, ICML 2020 ================================ LE (after rebuttal): the authors partially addressed my comments. I will increase my score. I am still not sure how this work compares to embeddings of graphs in products of constant curvature spaces that have an overall variable curvature.","- line 119: another option is to use gyro-midpoints or Riemannian Frechet means, e.g. see [2,4].",817,0
NIPS_2020_1552,NIPS_2020,"* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs. * Relation to relevant prior work is not mentioned and elaborated. For example, Rajasegaran, et al. ""Random path selection for continual learning."" NeurIPS'19 also propose a similar masking based approach to learn non-overlapping paths for dissimilar tasks. Similarly, PathNet (Evolution Channels Gradient Descent in Super Neural Networks) selectively masks out irrelavent model paramters. These papers should be cited and disucssed (preferably compared against) in this manuscript. * To my understanding, the notion of similar and dissimilar tasks is not accurate. E.g., the prior works on task incremental learning have both sets of similar and dissimilar tasks. (E.g., consider CIFAR100 classes in GEM - NeurIPS'17). In fact the considered set of similar and dissimilar tasks is not too different from the ones considered in earlier works. Specifically, consider a seminal work from Li & Hoeim, ""Learning without forgetting"" (TPAMI), where different datasets such as ImageNet/Places365/VOC/CUB/Scenes/MNIST are considered in continual learning experiments). Nevertheless, the proposed splits and dataset choices should be properly motivated and the authors should also report some experiments on previously considred protocols for fair benchmarking against existing methods. * The annealing strategy is somewhat similar to controller proposed in iTAML (iTAML : An Incremental Task-Agnostic Meta-learning Approach - CVPR'20). * The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers. * Equation 3 is wrong, it should be explicitly written. * The caption of Figure 1 should have some description for the MTCL architecture (a) as well.","* The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers.",818,0
NIPS_2020_394,NIPS_2020,"1. A key limitation is that the proposed method is much more complex than the sota methods for unsupervised keypoint discovery. Further, the gains in performance are not commensurate with the additional complexity, which might lead to limited impact of this work. 2. Another limitation is that on ""standard"" inter-occular normalized error metrics used in related works for unsupervised facial landmark detection, the method seems to underperform on both MAFL and AFLW as compared to the method of Jakab et al. [13] and Zhang et al. [48]. 3. While the method does improve on sota on the ""backward"" error metric (measured by training a regressor from ground-truth to unsupervised discovered keypoints), comparisons against the method which they cited for this metric (Sanchez et al. [31]) are missing. 4. They use much higher resolution (256x256) input images than previous work of Jakab et al. and Zhang et al. (~100-128 pixels squared). Hence, the accuracies may not be directly comparable. 5. It is not clear if the clustering in the 2nd phase of training (section 3.3) is required at all if a linear regressor is learnt on top of the discovered keypoints for evaluation anyway. A clear ablation on this should be presented. 6. SuperPoint itself is trained using equivariance. It would help if a compaison against more traditional interest point detectors (e.g. SIFT) is presented.",6. SuperPoint itself is trained using equivariance. It would help if a compaison against more traditional interest point detectors (e.g. SIFT) is presented.,819,0
NIPS_2020_263,NIPS_2020,"1) The major idea of selecting instances in this paper is not novel. It is natural to select tasks that are solvable but not too easy or too hard to the current agent during training. There exist several works in RL using the same idea, for example, Automatic Goal Generation for Reinforcement Learning Agents by Florensa et al. 2018. 2) The proposed bandits algorithm does not have any theoretical justification. The proposed reward is simply the deviation of the observed success comparing to the running mean of success over history, and the momentum parameter alpha controls the soft window width of the running mean. Is the performance sensitive to alpha? Does different instances need different alpha since the agent could make progress on them with different speeds. These questions are important to understanding how the selection criterion/reward helps to accelerate the learning. 3) The sparse reward problem is handled by MCTS instead of the proposed curriculum. But MCTS can be very expensive especially in early stages. 4) The empirical analysis of the experiments is insufficient. The main results are simply the final times and #solved instances. As a curriculum learning method for RL, it is necessary to have plots showing the improving curve for the success rate/average reward during training. In addition, it is important to have more visualizations and analysis of the selected tasks, graph neural nets learned features, and curiosity reward. 5) Basic comparison lacks in most results. There is only one baseline, which is FF, when reporting the running time. Except this, no baseline is presented and compared with. Hence, it is not convincing that the proposed method indeed outperforms most other possible RL methods on Sokoban tasks. 6) The proposed method (e.g., the generation of subtasks) is specifically designed for Sokoban and might be hard to be extended to other RL tasks.","1) The major idea of selecting instances in this paper is not novel. It is natural to select tasks that are solvable but not too easy or too hard to the current agent during training. There exist several works in RL using the same idea, for example, Automatic Goal Generation for Reinforcement Learning Agents by Florensa et al. 2018.",820,0
NIPS_2020_1846,NIPS_2020,"- The total training time is compared to the baseline to show that the proposed model is light. However, it is more convincing to compare the inference time or number of parameters of the generator. - The results preserve text conditions well, but it looks a bit unrealistic and blurry, especially on the COCO dataset. Human evaluation is required to claim the superiority of the results.","- The results preserve text conditions well, but it looks a bit unrealistic and blurry, especially on the COCO dataset. Human evaluation is required to claim the superiority of the results.",821,0
NIPS_2020_497,NIPS_2020,"- one of the motivations for stochastic NFs is that they overcome topological constraints. In the remainder of the paper, such constraints were not carefully formalized nor a reasoning provided for what makes NFs fail in such scenarios and how SNFs are particularly suited to fix these issues. Put differently, I wonder if the shortcomings of RNVPs in Figure 3 can be overcome with other architectures for normalizing flows, such as invertible resnets, iaf, maf etc. - the empirical evaluation in the context of existing works is largely restricted. For example, the schemes in [21, 35] could very well be applied here as well. Similarly, in the setup for MNIST/fashion datasets Table 3, I would have expected the default use implementation of flows for approximating intractable posteriors in a VAE is an inverse autoregressive flow (IAF) but instead the authors choose it to be a Real-NVP. - the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations. - related work such as A-NICE-MC [35] take special care to ensure that detailed balance is satisfied while using flows as proposals for MCMC. In the current work, it is unclear if such conditions are being satisfied in practice for SNFs.",- the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations.,822,0
NIPS_2020_911,NIPS_2020,"1. While the idea of jointly discovering, hallucinating, and adapting is interesting, there is a complete lack of discussing the impact of adding additional parameters and additional computational effort due to the multi-stage training and the multiple discriminators. The authors should provide this analysis for a fair comparison with the baseline [31, 33, *]. 2. Splitting the target data into easy and hard is already explored in the context of UDA. 3. Discovering the latent domain from the target domain is already proposed in [24]. 4. The problem of Open Compound Domain Adaptation is already presented in [**]. 5. Hallucinating the latent target domains is achieved through an image translation network adapted from [5]. 6. Style consistency loss to achieve diverse target styles has been used in previous works. 7. While the existing UDA methods [31,33] only use one discriminator, it is unclear to me why authors have applied multiple discriminators. 8. The details of the discriminator have not been discussed. 9. I was wondering why including the hallucination part reduces the performance in Table 1(b). It seems like the Discover module with [31] performs better than (Discover + Hallucinate + [31]). Also, the complex adapting stage where the authors used multiple discriminators mostly brings performance improvement. More importantly, did authors try to run the baseline models [17, 25, 31, 33, 39] with a similar longer training scheme? Otherwise, it is unfair to compare with the baselines. 10. Since the authors mentioned that splitting the training process helps to achieve better performance, It could be interesting to see the results of single-stage and multi-stage training. 11. It is not well explained why the adaptation performance drops when K > 3. Also, the procedure of finding the best K seems ad hoc and time-consuming. 12. I am just curious to see how the proposed method performs in a real domain adaptation scenario (GTA5->CityScapes). [*] Fei Pan, Inkyu Shin, François Rameau, Seokju Lee, In So Kweon. Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision. In CVPR 2020. [**] Liu, Ziwei and Miao, Zhongqi and Pan, Xingang and Zhan, Xiaohang and Lin, Dahua and Yu, Stella X. and Gong, Boqing. Open Compound Domain Adaptation. In CVPR 2020.",2. Splitting the target data into easy and hard is already explored in the context of UDA.,823,0
NIPS_2020_384,NIPS_2020,1. There are no statistics. It seems that the entire paper presents a single instance of a trained RNN. 2. The clarity of the paper – figures and explanations of methods – is lacking.,2. The clarity of the paper – figures and explanations of methods – is lacking.,824,0
NIPS_2020_62,NIPS_2020,"- A comparison to E2C would have been nice - See my general comments on additional feedbacks about separating the policy learning from the embedding / state transition learning. - The results of the proposed method appear a bit unstable e.g for Walker or Hopper. - By the results from cheetah or Ant v2 I get a bit the impression that the results can be divergent and ""early stopping"" may be necessary. Why this may be?",- A comparison to E2C would have been nice - See my general comments on additional feedbacks about separating the policy learning from the embedding / state transition learning.,825,0
NIPS_2020_10,NIPS_2020,"1. What is the adversary strength (adversary model) of this paper? Oblivious, adaptive online, or adaptive offline? [16] uses FPL, which only works for an oblivious adversary. I was wondering which adversary model is assumed in this work. This is important since one can make fair comparisons only if the adversary model matches. 2. I was confused by line 133. KL-divergence is defined for probability vectors but here x and y are in R^n. And what if yi = 0 for some i? The authors may want to clarify it here. 3. Is the first-order regret bound just a data-dependent regret bound? For example, is the equation in line 136 a first-order regret bound? It looks similar to guarantees in parameter-free online learning. Are they really related?","3. Is the first-order regret bound just a data-dependent regret bound? For example, is the equation in line 136 a first-order regret bound? It looks similar to guarantees in parameter-free online learning. Are they really related?",826,0
NIPS_2020_1849,NIPS_2020,"- A proper conclusion is missing. - The related work section can be improved. The authors can also better explain the difference with the work in [10] -Unfortunately the paper analyses only state-feedback control. It would be nice to see also an analysis of output feedback control. -Theoretically speaking, LQR is a good algorithm to analyse. Despite that, it is almost never used in practice (from my experience). It is hard to specify design requirements using LQR, especially in the frequency domain. It would be nice to analyse RARL in case there are some control requirements.","- The related work section can be improved. The authors can also better explain the difference with the work in [10] -Unfortunately the paper analyses only state-feedback control. It would be nice to see also an analysis of output feedback control. -Theoretically speaking, LQR is a good algorithm to analyse. Despite that, it is almost never used in practice (from my experience). It is hard to specify design requirements using LQR, especially in the frequency domain. It would be nice to analyse RARL in case there are some control requirements.",827,0
NIPS_2020_1180,NIPS_2020,"Overall, I enjoy reading this paper: simple but effective idea, clear presentation, and convincing qualitative and quantitative results. However, I have certain concerns as follows. Please address the concerns in the rebuttal and incorporate the feedback in the final version. - The proposed method seems in principle not effective to non-stationary textures, as well as non-repetitive texture patterns. The conclusion discusses this issue a little bit and the supplementary materials showed some examples. I would like more discussions about the shortcomings, for example a short section to illustrate the failure cases and tell possible improvements upon the current framework that can handle them. - The ablation study only gave quantitative comparisons. But as it is a synthesis work, I expect qualitative results as well to convince the readers that each proposed module is effective and solves the limitations of the baseline. - The proposed upsampling operation uses a transposed convolution layer. I wonder whether the transposed convs can be replaced by other upsampling modules? For example, nearest neighbor upsampling + regular convolutional layer and etc. I am afraid a simple transposed conv will introduce aliasing artifact.","- The ablation study only gave quantitative comparisons. But as it is a synthesis work, I expect qualitative results as well to convince the readers that each proposed module is effective and solves the limitations of the baseline.",828,0
NIPS_2020_1544,NIPS_2020,"- The statement of Thereom 1 seems imprecise. Is \Delta simply a parameter, or is it related to the reward gap for all models (I understand that it is linked to the reward gap for the MNL model, but for other models this seems unclear). If Delta is just a parameter, this would imply that the proposed algorithm cannot be claimed to be order optimal outside of the MNL model, contrary to what the authors seem to claim. - For this type of general structured bandit problems, general regret lower bounds are known (see for instance http://papers.nips.cc/paper/6773-minimal-exploration-in-structured-stochastic-bandits ), so that it is a pity that the authors did not investigate these. - The proposed algorithm has an input parameter C, which must be chosen ""large enough"" for the regret bound of Theorem 2 to hold. However the authors never specify what information is necessary to make this choice. Must C be greater than some universal constant ? Must it be chosen in a problem specific manner ? If so this greatly diminishes the value of the proposed algorithm, unless one has some sort of method to select C automatically.","- The statement of Thereom 1 seems imprecise. Is \Delta simply a parameter, or is it related to the reward gap for all models (I understand that it is linked to the reward gap for the MNL model, but for other models this seems unclear). If Delta is just a parameter, this would imply that the proposed algorithm cannot be claimed to be order optimal outside of the MNL model, contrary to what the authors seem to claim.",829,0
NIPS_2020_453,NIPS_2020,"- Weaknesses and limitations of the contributions: A ) Contribution 1: Empirical evidence of the observed convex parabolic shape is only presented for classification tasks on one dataset CIFAR-10. Since this is the main contribution of the paper, other deep learning tasks and loss functions should be explored. Other datasets should also be studied to support the claim. (The author's feedback has adequately addressed this issue.) B ) Contribution 2: The method introduces new hyperparameters (update step adaptation, measuring step size, maximal step size). The sensitivity study in Figure 14 is not convincing for the following reasons: - it's unclear to me how different combinations of these parameters would perform based on that figure. - Although the gradient is normalized, I suspect the measuring step size value will still depend on the scale of the problem and I am concerned that the proposed range in Figure 14 is only adapted to ResNet32 trained on the CIFAR-10 problem. The method is claimed to be generalizable to any step direction, but no empirical evidence is presented to back up this up. It would be interesting to see how the proposed step size procedure would perform on SGD, with or without momentum and Adam directions. (The author's feedback has addressed this issue.) B ) Contribution 3: The authors used a so-called conjugate gradient method to pick the search direction, making it difficult to access whether PAL picks good learning rates based on the figures. It would be more convincing to compare the learning rates obtained by PAL using SGD-with-momentum (or any other algorithm) directions to the optimal learning rate schedule for the same algorithm. - Validation accuracy is consistently lower than SGD across the presented problems. - No plots comparing CPU times are included in the experiments. Computing an additional evaluation of the loss function on every step requires an additional forward pass and how this effects the total run time should be presented. - Comparison with PLS is not included. (The author's feedback has adequately addressed this issue.) - Comparison with second-order optimizers is not included. B ) Contribution 4: The convergence proof is provided under strong assumptions (parabolic shape + same Q matrix for individual-loss) which are, as mentioned by the authors, not valid for general deep learning scenarios.","- Comparison with PLS is not included. (The author's feedback has adequately addressed this issue.) - Comparison with second-order optimizers is not included. B ) Contribution 4: The convergence proof is provided under strong assumptions (parabolic shape + same Q matrix for individual-loss) which are, as mentioned by the authors, not valid for general deep learning scenarios.",830,0
NIPS_2020_1631,NIPS_2020,1. It is not immediately clear to me what is the main technical challenge to extend the dual-free analysis in [33] and [16] into the decentralized setting. The authors may need to explain this clearly. 2. When is it the case that the evaluating the gradient of conjugate function is expensive or infeasible? It is better to include more examples in machine learning to motivate this dual-free approach.,2. When is it the case that the evaluating the gradient of conjugate function is expensive or infeasible? It is better to include more examples in machine learning to motivate this dual-free approach.,831,0
NIPS_2020_1331,NIPS_2020,"The quality of the paper is already great, but there are a few comments. 1. In equation 3 (page 4), it is not clear whether you compute F of the generated source or target data. Also, I don't quite understand why the FI is computed for the difference between the pretrained and finetuned parameters, and not just for the pretrained parameters. Finally, I assume i in this equation is the layer index, but this should be clearly stated. Update: In the rebuttal, the authors kindly explained that the F is computed for each individual parameter in the network rather than for the entire layer. I suggest adding this clarification to the main paper as well. 2. In Figure 3, it would be very illustrative to show how each layer affects the generation. You can do this by regularizing everything but the layer and look at the generation result. This would further convince the reader that some layers are more important than others for the diversity preservation. 3. In Table 3, it is unclear from the caption what source dataset is used. Update: Please add some information about the source dataset in the caption. The image and table captions should contain all the necessary information about the experiment so that the reader doesn't have to search for in the main text. Apart from that, great job!","2. In Figure 3, it would be very illustrative to show how each layer affects the generation. You can do this by regularizing everything but the layer and look at the generation result. This would further convince the reader that some layers are more important than others for the diversity preservation.",832,0
NIPS_2020_94,NIPS_2020,"1. Compared to Ge et al. [10] which provides the rate of escaping saddle points of SGD, there is no explicit rate of escaping saddle points provided in this work. 2. Theorem 4 provides the rate of convergence to local minimum, but (1) it only works for strict local minimizers whose Hessian are positive definite, (2) it only provides the rate once the iterate is close to the strict local minimizer, but there is no guarantee if this happens, or how fast it happens. 3. The following work is very related and is missing: Daneshmand, Hadi, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. ""Escaping Saddles with Stochastic Gradients."" In International Conference on Machine Learning, pp. 1155-1164. 2018.","3. The following work is very related and is missing: Daneshmand, Hadi, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. ""Escaping Saddles with Stochastic Gradients."" In International Conference on Machine Learning, pp. 1155-1164. 2018.",833,0
NIPS_2020_1666,NIPS_2020,"1. Although the author has done a comparative experiment between the multi-modal and the GECA methods on the proposed benchmark, the design and analysis of the experiment are confusing and difficult to understand. 2. Besides, there are lots of inconsistencies in the experiment section, such as the title of Table1 claim that “Models fail on all splits except C and F”, which is inconsistent with the data in rows C and F. The poor performance of the two methods on most split datasets could explain that advances are needed in neural architectures for compositional learning. 3. However, I am not quite sure why the author can draw the following conclusions based on the experiment and analysis of split A-I: artifacts in SCAN are not central to the nature of compositional generalization, gSCAN removes these artifacts by introducing more sophisticated semantics through grounding. I think that seems a little bit unconvincing.","2. Besides, there are lots of inconsistencies in the experiment section, such as the title of Table1 claim that “Models fail on all splits except C and F”, which is inconsistent with the data in rows C and F. The poor performance of the two methods on most split datasets could explain that advances are needed in neural architectures for compositional learning.",834,0
NIPS_2020_1604,NIPS_2020,"1. The results provided in this paper seem kind of incremental to me. The settings and final error bounds are similar to the ones in [23] and [24], as shown in table 1 in appendix A. The difference is just the assumption for signal set K. In [23][24], K satisfies GMW and in this paper, K is the range of a generative model. However, both settings imply that effective dimension is small, which is a key to the proof. Note the difference between the two settings is actually addressed by lemma 2 (this result is stronger than that the effective dimension is small since the input dimension of the G is small in nature), which is from [2] and proved by using a delta-net cover. It seems to me that the main result of this paper is a combination of the lemma in [2] and the results from [23][24]. 2. The paper also gives the uniform recovery guarantee in section 5, but the result relies on assumption 1, which seems too strong and hard to test. I would expect the authors can give a sufficient condition that is neat and informative. 3. It would be better if the paper also provides some empirical studies.",3. It would be better if the paper also provides some empirical studies.,835,0
NIPS_2020_302,NIPS_2020,"1. In Eq. (1) and Eq. (2), how are the upper bounds and lower bounds determined? Do you set the bound manually first and then calculate $p(x)$ and $n(x)$, or obtain bounds through the calculation of $p(x)$ and $n(x)$? If you obtain bounds through the calculation, then the definition in Eq. (1) and Eq. (2) is problematic. If the bounds are set manually, then what is the form of bounds? how do you determine the bound with high dimensions, and how can you guarantee that there exists pertinent negative vector within the bounds? 2. There is no rigorous formulation and justification for the proposed method. Besides, why not use the knowledge distillation to learn the transparent models? It seems that the knowledge distillation is more direct. 3. In Eq. (3), why do you require that the output of the transparent model for the PN is different from the output of the black-box for the original input? If you want to ensure the consistency between two models, for the PN, the output of two models should be the same. 4. In Eq. (5), only one pertinent positive data point is used. However, there are many data points that satisfy the Eq. (1) and Eq. (2). These data points contribute equally to the Eq. (3), but there perturbation directions are different. Eq. (5) doesn’t include the exhaustive test over the diversity of all these data points. 5. Authors are making a circular arguments when verifying the effectiveness of the proposed method. They use the pertinent positive and pertinent negative to define the metric for the consistency, as well as use them to train the transparent global model. They are all based on the same ideas, so the comparison in Table 1 is questionable. 6. In Experiments, the datasets used are in low dimension. Authors should conduct experiments on more general datasets, e.g. MNIST, to show their effectiveness. 7. In section 4, PP and PNs in Eq. (5) are defined as vectors (features) with high dimensions. However, in Listing 1, the condition bounds are scalars. What is the correct form of PP and PNs? 8. The results shown in the Listing 1 is still unclear enough. Authors should investigate further to obtain an explanation in the semantic level. 9. The paper is not well-organized and hard to understand. The symbol used in Figure 1 is not introduced specific, it is hard to understand the Figure 1 even after I read the paper.","9. The paper is not well-organized and hard to understand. The symbol used in Figure 1 is not introduced specific, it is hard to understand the Figure 1 even after I read the paper.",836,0
NIPS_2020_1497,NIPS_2020,"1. In terms of the techniques, what are the novel techniques for proof beyond TLR18? In terms of conclusion, what is the key change to obtain a better rate from Overdamped LD to ULD and NLD? 2. Is the goal to minimize $\overline{F}$ or $F$? It seems we need to minimize $\overline{F}$, but the discussions are all for $F$. 3. The assumptions in Assumption 1 seem very strong and limited. For example, is the loss function being twice continuously differentiable practical? 4. What is the choice of J in practice? 5. Based on theoretical analysis, can you provide some guidelines to use ULD and NLS in practice? Can you demonstrate some empirical studies using some simple examples?","3. The assumptions in Assumption 1 seem very strong and limited. For example, is the loss function being twice continuously differentiable practical?",837,0
NIPS_2020_847,NIPS_2020,"The main limitation is that the study only presents the results in an asymptotic regime. It is not directly clear to the referee that how much insight can be shedded into practically important issues, either in a finite data / model, or beyond the linear model. Nonetheless, I am convince that the work is still important enough. A few minor comments that can be improved. 1) Figure 1 is actually not referenced in the main text. 2) page 2, it would be nice to give a reference to the statement on line 41. 3) I find Definition 2 hard to understand. Especially what is ""the same order"" ? Does it refer to entrywise of the same order or the vector norm of the same order. It would be better to have a more precise definition, preferrably with a forma. 4) line 241, change ""conjuncture"" to ""conjecture"". One last point is that some numerical results on more complicated models would greatly strengthen the study, e.g., on simple two-layer neural networks.","3) I find Definition 2 hard to understand. Especially what is ""the same order"" ? Does it refer to entrywise of the same order or the vector norm of the same order. It would be better to have a more precise definition, preferrably with a forma.",838,0
NIPS_2020_848,NIPS_2020,"1. Asynchronization is introduced as a way to speed-up a parallel implementation of an algorithm with possibly compromizing sample efficiency (compared with synchronous version), rather than to improve the performance. Showing a performance improvement at a fixed wall-clock time or a fixed nubmer of interaction do not really show the goodness of asynchronization. What is the maximum time budget is higher or lower? Why not showing a performance graph? 2. Ablation study is missing. It is unclear where the performance improvement comes from. 3. As mentioned in the summary above, two difficulties in implementing asynchronous distribution update are stated in this paper. However, I can not agree with the first difficulty: ""in the case of the asynchronous update, it is impossible to use this method because the actors have different ending moments."" It is definitely not ""impossible"". The second difficulty is already addressed in asynchronous ES [25]. Therefore, it should be able to simply combine asyncronous ES with CEM-RL. I am not sure whether the proposed approach is really promising compared to this very simple baseline. It is also because of the lack of ablation study.",2. Ablation study is missing. It is unclear where the performance improvement comes from.,839,0
NIPS_2020_251,NIPS_2020,"* The NF assumption was not discussed as compared to a standard SSM which uses additive measurement noise. Placing the emission noise *before* the nonlinearity is a crucial move; otherwise filtering is not tractable. I would have appreciated further discussion of the impact of this. It's possible that this technique can be applied as a drop-in replacement in many models to avoid awkward approximations such as EKF, UKF and PF; however this conclusion is not immediate from the work presented in this paper. * As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning. * The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer. * NKF does not show markedly better performance than the GP-Copula model in the main experiments.","* The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer.",840,0
NIPS_2020_966,NIPS_2020,"1. The scenes in the experiments are relatively easy: backgrounds and the geometries of the objects are simple. As the slot attention module is essentially an instance segmentor, it may be necessary to see if it can perform instance segmentation for realistic images (e.g. a smaller and customized MS Coco dataset). 2. Although the module seems to have low computational overhead, it will be better if the authors provide quantitative results on the runtime. 3. Unlike some other papers, the model seems not being able to handle a large number of objects (e.g. more than 20) as it may require more iterations to perform reasoning. In addition, it will be good to see some results on runtime to reach certain AP vs. number of objects.","3. Unlike some other papers, the model seems not being able to handle a large number of objects (e.g. more than 20) as it may require more iterations to perform reasoning. In addition, it will be good to see some results on runtime to reach certain AP vs. number of objects.",841,0
NIPS_2020_205,NIPS_2020,"- The motivation of the method is weak. Especially in introduction, the reference to the mutual information is InfoMax in deep representation learning. While the paper uses a fully-supervised model (pretrained) and the proposed method has less correlation to the input and output of deep networks presented in DeepInfoMax. - Some citations are missing for the recent works relating to transductive few-shot learning and information maximization. It would be recommended to discuss [1] and [2]. - The method shares similarity to [3], especially Eq. 7 in [3]. Would the authors highlight and discuss the difference? - In the 1-shot case, is the one-hot encoded label directly used for $\pi$ in Eq. 3? - It is not described well how w is updated. In TIM-GD, is w updated always with all of the classes (using the loss in Eq. 3)? Because in some recent works for few-shot learning, the samples to update w are sampled (like mini-batch) from the support set and not necessarily containing all samples from all classes (K). - Is this proposed method limited only to the uniform distribution? It is still not very clear how this method can be applied to the iNat dataset with highly-imbalanced tasks. [1] Li et al., ""Learning to Self-Train for Semi-Supervised Few-Shot Classification,"" Neurips, 2019. [2] Guo and Cheng, ""Attentive Weights Generation for Few Shot Learning via Information Maximization,"" CVPR, 2020. [3] Hu et al., ""Empirical Bayes Transductive Meta-Learning with Synthetic Gradients,"" ICLR, 2020.","- The method shares similarity to [3], especially Eq. 7 in [3]. Would the authors highlight and discuss the difference?",842,0
NIPS_2020_1684,NIPS_2020,"1. More motivation might be needed to understand why we need regression with rejection. 2. For the semi-supervised estimation procedure, I am wondering what is the sample size requirement for ensuring a good performance. 3. Is it possible to establish rate of convergence for a class of predictors, not just limited to kNN? 4. It is probably better to add a conclusion section.","2. For the semi-supervised estimation procedure, I am wondering what is the sample size requirement for ensuring a good performance.",843,0
NIPS_2020_624,NIPS_2020,"This is a strong paper that I feel overall positive about. No particular weakness is required for authors to address during rebuttal; just some nitpicks and suggestions: - I am quite curious what will happen, if you feed in lambda outside [0,1] range, at test time? - For the role of dual BN in adversarial robustness, a better reference than [16] is “Intriguing properties of adversarial training at scale”, ICLR 2020 (from the same authors). - As one key building block, the introduction of FiLM layer is underwhelming, and more details should be included to make the paper self-contained. - As an experimental paper, it would be nicer to demonstrate results on larger datasets. This is just a suggestion afterwards; I understand adversarial training on ImageNet scale cannot be completed easily in short time. - (minor) You might want to discuss a recent paper of similar taste, “Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms”, although admittedly the two works solve different questions.","- I am quite curious what will happen, if you feed in lambda outside [0,1] range, at test time?",844,0
NIPS_2020_1601,NIPS_2020,"1. In Fig 1, it is used to validate the claim ""Specifically, the compromised evaluation process would distort the ranking for augmentation strategies since the model trained with too few iterations are unstable."" However, the Figure cannot support the claim: the change of rank cannot indicate that the model is unstable. It just shows that the improvements caused the three methods are different. The rank decreases but the accuracy may also get improved. Also, whether a model is stable or not needs more clarification. 2. Overall, the proposed method is an incremental improvement on the existing AutoAugment. But the performance improvements are not very significant when comparing to recent methods. On ImageNet, resnet-50 is only 0.3 better than [18]. Also, the std is very large, which leads to doubts about generalizability. 3. In table 1, it is said that ""We report Mean STD (standard deviation) of the test error rates wherever available."" But no std reported in the table.","1. In Fig 1, it is used to validate the claim ""Specifically, the compromised evaluation process would distort the ranking for augmentation strategies since the model trained with too few iterations are unstable."" However, the Figure cannot support the claim: the change of rank cannot indicate that the model is unstable. It just shows that the improvements caused the three methods are different. The rank decreases but the accuracy may also get improved. Also, whether a model is stable or not needs more clarification.",845,0
NIPS_2020_439,NIPS_2020,"- Considering that Table I and Figure 4(a) report results for CIFAR-10, it would be desirable to have results for the same group of explicit models in both of them. - The authors provided all hyperparameters in Table 4 (supplementary materials), but provide no intuition on how to set these values for different datasets (especially the parameters that are specific to the proposed method).","- Considering that Table I and Figure 4(a) report results for CIFAR-10, it would be desirable to have results for the same group of explicit models in both of them.",846,0
NIPS_2020_1507,NIPS_2020,"- As the authors mentioned, their experimental results don’t effectively show a difference between the serial and joint methods. The authors mention that this could be due to the small scale of the experiments. I agree that including an experiment with a larger dataset would make this experimental section stronger. - There are issues with clarity in the notation and technical results (see below comments on “Clarity”). - Can the authors provide more details on the “separate” baseline in the experiments? Currently, Line 292 just says, “each classifier is learned separately on the training data.” However, can the authors specify which features each separate classifier uses? Does each classifier only use the direct parents of each label in the causal graph, or does each classifier use all available features? The same question applies to the Serial comparison. I could not find this information in the attached supplemental pdf. - The empirical results reported from Lines 300-304 are not at all clear to me. I’m not sure what point is being made here, what takeaways I should have, or what the 71.43% number really means. Can the authors describe these results more clearly, perhaps by further describing what they mean when they say multiple classifiers are “produced by” a given method? - The authors briefly mention that they perform “oversampling” on the Adult dataset to handle imbalanced data. Somehow, they obtain a dataset with 101,472 examples, when the original Adult dataset contains 48842 examples. However, they do not explicitly define what they mean by “imbalanced,” nor do they specifically outline what oversampling procedure they use (other than referencing a toolbox by Lemaitre et al.). Can the authors more explicitly outline their oversampling procedure?","- As the authors mentioned, their experimental results don’t effectively show a difference between the serial and joint methods. The authors mention that this could be due to the small scale of the experiments. I agree that including an experiment with a larger dataset would make this experimental section stronger.",847,0
NIPS_2020_1087,NIPS_2020,"Overall I find this submission makes no major mistakes and perhaps should be considered as a borderline one, given their contributions are more like some engineering endeavor but are evaluated properly, if there is no other reviewer points out some significant issues. I only have a few comments on this paper at this point: - If my understanding is correct, I feels the horizon of traj-wise loss (M) adaptive planning (N) and number of heads (H) are major hyper-parameters in the proposed improvement, but I can't find any illustrations on how can these three be selected during their evaluations. The authors are expected to provide a clarification on how the parameter search is conducted. A comprehensive ablations on how do these parameters affect the results is also expected. - Since the proposed improvement is agnostic to the concrete MBRL algorithm, experiments with different planning backbone should be included, e.g. Dreamer (https://openreview.net/forum?id=S1lOTC4tDS), MBPO(https://arxiv.org/abs/1906.08253), etc.","- If my understanding is correct, I feels the horizon of traj-wise loss (M) adaptive planning (N) and number of heads (H) are major hyper-parameters in the proposed improvement, but I can't find any illustrations on how can these three be selected during their evaluations. The authors are expected to provide a clarification on how the parameter search is conducted. A comprehensive ablations on how do these parameters affect the results is also expected.",848,0
NIPS_2020_131,NIPS_2020,"- The pros and cons of different methods are not comprehensively analyzed. In section 2, Related Work - Single-image HDR reconstruction, this paper just listed some state-of-the-art methods without specifically stating the advantage and disadvantage of them. - This paper claims that the proposed method runs 120 times faster than the previous MRF-based algorithm. However, it does not report the run-time of the comparing methods. - Minor issues: -- Eqn. 6 and Fig. 2 do not match. -- The dashed-line box of “UnModNet” should not include the \plusdot on the right hand side of the “Rollover mask predictor”.","- The pros and cons of different methods are not comprehensively analyzed. In section 2, Related Work - Single-image HDR reconstruction, this paper just listed some state-of-the-art methods without specifically stating the advantage and disadvantage of them.",849,0
NIPS_2020_1801,NIPS_2020,"1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?","1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers).",850,0
NIPS_2020_1854,NIPS_2020,"- The contribution goes in several directions which makes the paper hard to evaluate; is the main contribution the selection of existing datasets, introducing new datasets or new versions of datasets, the empirical evaluation or the software tooling? - The dataset does not describe existing datasets and benchmarks, and so it is hard to judge the exact differences between the proposed datasets and currently used datasets. A more direct comparison might be useful, and it's not clear why existing, smaller datasets are not included in the collection. - For some of the datasets, it's unclear if or how they have been used or published before. In particular, the datasets from Moleculenet seem to be mostly reproduced, using the splitting strategy that was suggested in their paper, with the modification potentially being addition of new features. - If the selection of the datasets is a main contribution, the selection process should be made more clear. What was the pool of datasets that was drawn from, and how were datasets selected? An example of such a work is the OpenML100 and OpenML CC-18 for classification, see Bischl et. al. ""OpenML Benchmarking Suites"". or Gijsbers et al ""An Open Source AutoML Benchmark"" In addition to selection of the datasets, the selection of the splitting procedure and split ratios also seems ad-hoc and is not detailed. - Neither the software package, nor the datasets, nor the code for the experiments has been submitted as supplementary material, and the details in the paper are unlikely to be enough to reproduce the creation of the datasets or the experiments given the datasets. - Given that many methods aim at one of the three tasks, having 5, 6 and 4 datasets for the tasks respectively, might not be enough for a very rigorous evaluation, in particular if some of the datasets are so large that not all algorithms can be used on them. Addendum: Thank you to the authors for their detailed reply. A repository and online platform for reproducing the experiments was provided, and it was clarified that the datasets are substantially novel. Motivations for the number and choice of datasets were given and I updated my assessment to reflect that.","- For some of the datasets, it's unclear if or how they have been used or published before. In particular, the datasets from Moleculenet seem to be mostly reproduced, using the splitting strategy that was suggested in their paper, with the modification potentially being addition of new features.",851,0
NIPS_2020_1418,NIPS_2020,"1. This work study several toy examples in purely optimization problems. It may be interesting to study machine learning optimization problems, or Bayesian sampling problem. 2. Is there a theoretical justification that shows the realistic Hamitlonian is better than Euclidean Hamiltonian? Does it provide a new convexity which helps the optimization convergence for some particular functions or non-convex functions?","1. This work study several toy examples in purely optimization problems. It may be interesting to study machine learning optimization problems, or Bayesian sampling problem.",852,0
NIPS_2021_812,NIPS_2021,"I see two primary weaknesses in this paper: 1) numerous sweeping claims are made regarding their superiority over previously published results without sufficient support. 2) The empirical demonstration of the model compares against control models that are not well fit to the tasks. Both of these weaknesses are easily addressable, by 1) either supporting their claims more carefully or dialing them back and 2) choosing a more reasonable control or experimental task.
The authors make a large number of bold claims, for example in lines 9-10; 44-47; 52-53; 67; 96-99; 110; 123-125; 139; 141-142; 166; 300-301; 339-341. While I do not know the literature well enough to directly refute all of them, I will note a few examples where they appear to be wrong. I believe that this weakness can be resolved by either softening the claims themselves (which, in my opinion, would not reduce the novelty or significance of the submission) or providing a more detailed review of how previous works compare.
a) On lines 44-47 the authors note that “all models cited above, with the exception of Khemakhem et al. (2020a), assume that the data are fully observed and noise-free”. While that might be true, I immediately thought of the work of Locatello et al. {1}, which was not cited by the authors, but is an identifiable disentanglement method and comments on their ability to handle noise: “The generative model does not model additive noise; we assume that the noise is explicitly modeled as a latent variable and its effect is manifested through [the ground truth generator], as is done by [several citations].” (last paragraph of section 3) I admit that this might not necessarily be a contradictory claim, but I nonetheless would appreciate it if the authors clarified how their method of explicitly modeling additive noise is beneficial over the type of setup in {1} and the citations in Locatello et al.’s quoted sentence.
1.b) On lines 96-97 the authors note that “the mixing function f is assumed bijective and thus dimension reduction is not possible in most of the above models. The only exception is Khemakhem et al. (2020a) who…” I find the second part of this quote (“dimension reduction is not possible”) surprising in general, considering that any publication attempting to perform disentanglement on the DisentanglementLib dataset necessarily has to assume dimensionality reduction. I thought maybe the authors intended to say that the theory does not allow dimensionality reduction, even if the practical implementations do. However, Klindt et al., which is cited earlier, presents an identifiable disentanglement framework that only assumes an injective mixing function in their theory and therefore allows for dimensionality reduction.
1.c) The authors claim that “under the conditions given in the next section, we can now guarantee identifiability for a very broad and rich class of models. First, notice that all previous Nonlinear ICA time-series models can be recast and often improved upon when viewed through this new unifying framework.” I do not understand how this could possibly be true given that not all previous Nonlinear ICA work abides by the conditions given in sections 3 and 4. Just as two examples, the requirement for unconditional independence on lines 114-115, and tail behavior in assumption A1 are not ubiquitous in the literature. I could definitely be misunderstanding what the authors intended with their statement, in which case I would be happy with a clarification.
I understand that it is not reasonable for the authors to precisely state how every previous contribution fails to meet their claimed advantages. However, given the examples above, I found myself unconvinced in general. I further found myself wondering if it was necessary to make so many sweeping claims in the first place.
The authors set up a simulated example problem in section 5.1 to understand how their model works with a restricted experiment. I agree that this is an important step for understanding how the model behaves. They choose to compare against what they claim to be the state of the art, IIA-HMM. However, they state themselves that “IIA-HMM has a much simpler model of dynamics and no noise model, and likely lost information due to PCA pre-processing.” This leaves me wondering why they felt that this was a fair comparison, given that the problem setup is not at all matched to what IIA-HMM was designed to solve. I would be curious to see how they do for a non-dimensionality-reduced setup, since as far as I know their framework does not require the number of generating latents to be fewer than the data dimensionality. Or, a better solution would be for them to compare their model against an alternative that is appropriately matched to the dimensionality reduction task. Furthermore, the authors do not provide a baseline comparison for their denoising task. This might be because the authors wished to focus on identifiable models, which restricts the otherwise large set of denoising methods. However, again given the work cited in {1}, I am unconvinced that there was not a suitable comparison to be made. Without appropriate comparisons, we are left to rely solely on the scalar MCC metric in an unfamiliar simulated example, which I find insufficient.
Additional minor complaints:
The citations need to be revised and edited. Many are listed as arxiv prints that are now published in peer-reviewed venues. A few are missing the venue entirely. Morioka 2020b appears to be listed twice.
I noticed typos on line 243 (“a complete statistics”), fig 2 caption (“ground true independent”)
I think it would be helpful for practitioners if the authors included some justification for their decision on line 318, or at least a discussion on the tradeoffs for the number of independent components chosen.
I would appreciate it if the authors spent more time discussing trade-offs made in their framework. As of now it is limited to the first two sentences of the “Limitations” section starting on line 367. I found this to be rather terse given the space allocated for stating the failures of prior work. For example, if I understand the unconditional independence assumption on lines 114-115 correctly, then it seems highly unlikely to be met in real-world data, including their MEG experiment. The constraint on the distribution tails and 2nd order nature of the generator also seem restricting with respect to real data. Perhaps the authors could note alternative work that does not require such assumptions for identifiability (in exchange for other restrictions), which would assist future researchers seeking a more general solution.
{1} http://proceedings.mlr.press/v119/locatello20a.html",1) either supporting their claims more carefully or dialing them back and,853,1
NIPS_2021_1788,NIPS_2021,"- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution.",854,0
NIPS_2021_589,NIPS_2021,"This paper does not discuss its limitation. Here are some of my questions and suggestions: 1. Does the proposed method perform better in pure combinational logic (without register), it seems it may be much easier to model without state related registers, it would be interesting to see a comparison between sequential design and combinational design. 2. How does it scale? What would be the sweet spot in terms of design complexity to train it on? 3. Is possible to extend this to analog model, probably at least system-verilog. 4. For related work, it would be great to separate them into non-ML based and ML-based, and have a section to state the novelty of the proposed method.","3. Is possible to extend this to analog model, probably at least system-verilog.",855,0
NIPS_2021_1737,NIPS_2021,"1. The paper lacks comparison with other baselines. For example, in [1], the authors proposed to use texture randomization in the augmentations which can also learn texture-invariant features. Also, in [1] the authors reported higher accuracy of MoCo-v2 on ImageNet-100 (81.0%), while the MoCo-v2 accuracy in this paper is less than 78%. 2. The paper proposed to learn less about the texture semantics by adding negative samples that only share the texture sematic with the anchor images. However, the texture semantics can still be helpful in some cases. Similar to [1], it would be good to try to ensemble models trained with the proposed negative augmentations and without it and see whether this could improve the performance. 3. The proposed method is restricted to removing texture semantics and lack of extension to other semantics. One interesting experiment to do is to remove color distortion in the augmentations of positive samples and add negative samples that share the same color distribution of the anchor image to see whether the proposed method could be helpful to avoid the color distribution shortcut.
Clarity: The paper is well written and easy to follow.
Significance: the paper proposes a simple method to construct negative samples to prevent contrastive learning from learning texture information which could affect its generalization ability. However, it is not always desirable to remove the texture semantics, since it is also an important semantics in image classification task. Moreover, the proposed method is restricted to the texture semantics and the authors do not show that it could be extended to other semantics. Therefore, the significance of the paper is limited.","2. The paper proposed to learn less about the texture semantics by adding negative samples that only share the texture sematic with the anchor images. However, the texture semantics can still be helpful in some cases. Similar to [1], it would be good to try to ensemble models trained with the proposed negative augmentations and without it and see whether this could improve the performance.",856,0
NIPS_2021_2074,NIPS_2021,"The main concerns are the following: 1. The explanation of using soft assignment instead of har-assignment is discussed in the description of the methodology, but it is unclear to me whether the authors have reported an experiment allowing to prove their conjecture. 2. The authors should discuss the fact that CP CROCS seem to be underperforming the other clustering approaches based on the age attribute. This result is quite strange and an outlier, but it would be interesting to have a discussion on a possible explanation of this phenomenon. 3. Could the authors discuss the main difference of the tow datasets, and especially as depicted in figure 4 why the separability of the classes is so much lower on PTB-XL compared to Chapman 4. IN terms of quality of the information retrieval, cold the authors comment why evaluating soft assignment (and performance if only one attribute is correct) is clinical significant? It seems to me quite important that pathology and should be well retrieved, extracting an example of AF when asking for Normal rhythm, would have higher impact than a mistake on age or sex 5. Can the authors discuss whether age and sex are good attributes for ECG signals? Heart Rate variability is affected by age, but I am not aware of morphological changes in the ECG signals due to age (or even sex)? Figure 3 seem to be indicating that the representation of the ECG signal is not that influenced by either age or sec, although there seem to be a trend or evolution with age, and separation of sex on the proposed full framework. 6. The order in sections 2 related work and 3 background should be consistent clustering then IR. 7. The acronym HER is not introduced properly, that is at the first appearance of the term",6. The order in sections 2 related work and 3 background should be consistent clustering then IR.,857,0
NIPS_2021_1029,NIPS_2021,"1 What's the meaning of Position 2 in the sparse graph attention message-passing towards social posterior collapse detection?
2 The inferred sparse attention is only used in e2v process. The v2e process still causes large storage and complexity.
3 What is the effects of the sparse graph attention message-passing, compared to the ordinary neural-message-passing without sparse attention? From the aspects of error and efficiency?
4 Many excellent previous works are not introduced for performance comparison, some of which are based on CVAE or neural-message-passing, including:
[1] Social lstm: Human trajectory prediction in crowded spaces
[2] Social attention: Modeling attention in human crowds
[3] Social gan: Socially acceptable trajectories with generative adversarial networks.
[4] Stgat: Modeling spatial-temporal interactions for human trajectory prediction
[5] Sophie: An attentive gan for predicting paths compliant to social and physical constraints
[6] Multi-agent tensor fusion for contextual trajectory prediction
[7] Encoding crowd interaction with deep neural network for pedestrian trajectory prediction
[8] Collaborative motion prediction via neural motion message passing
[9] Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control
[10] Conditional flow variational autoencoders for structured sequence prediction.
[11] Neural relational inference for interacting systems.
[12] Social-stgcnn: A social414spatio-temporal graph convolutional neural network for human trajectory prediction.
The results in this paper still need to be improved",2 The inferred sparse attention is only used in e2v process. The v2e process still causes large storage and complexity.,858,0
NIPS_2021_2326,NIPS_2021,"Weakness
Overview of the main concerns, which are detailed in the paragraphs below:
Improper evaluation of the universal controller.
Missing insights into the data and the strong performance of the pose estimation, which perform better than state-of-the-art pose estimation from third-person view on established benchmarks, despite the much harder task.
Generalization of the learned tasks is not properly evaluated.
1. Universal controller
First of all, learning robust universal controllers is a challenging task and is known to suffer from instabilities, especially if it has to replicate unseen motions. There is a branch of research [4][5][6] with the sole focus on building controllers that can scale to larger datasets, but have only achieved learning on a subset of AMASS (CMU ~ 2k motion sequences). The proposed method builds on a universal controller that is trained on AMASS (~11k sequences). Hence, the claim of a universal controller that can scale to an order of magnitude larger motion database than state-of-the-art needs to be properly evaluated on its own, since it would mark a substantial improvement in the direction of imitation learning from motion capture. As stated by the authors, the policy is able to execute a wide variety of motion “ranging from dancing to kickboxing” (cf. L.40) and therefore needs to be empirically evaluated on the full AMASS dataset to substantiate this claim. The current controller is only tested on the relatively simple motions contained in H36M (walking, standing, etc.). Furthermore, it is important to see how well it adapts to noisy estimates, because at the beginning of training the dynamic-regulated model, the kinematics policy will likely produce random residuals and hence noisy target poses.
2. Missing insights into data and performance
The universal controller is then utilized in the downstream task of egocentric pose estimation. Since the task is ill-posed, estimating pose from egocentric video without top-down view is extremely difficult. The results, namely the mean per joint position error, which is very low (~30-40mm), may indicate that there is very little variation between different motion sequences and overfitting to the training data (which is likely similar to the test data). For instance, since there is no way to tell how the occluded arms are moving, such a small error is only possible if the deviation of motion between the sequences in the data is very small. The authors should provide: 1) statistical data on their dataset to be able to better assess their quantitative results (e.g., data on per joint trajectories, pose diversity against available 3D datasets), 2) a proper discussion of the very small reported errors on the test data and 2) the per joint error to be able to get better insights into the performance of the model.
This seems even more evident when looking at the supplementary video, where it appears that the agent is always moving in the same way (movement of arms, gait, speed, etc.). Since the authors already provide the statistical data of the speed in the datasets, it would be good to see whether the agent actually learns to adapt to different speeds or just overfits to one single motion.
3. Generalization
As illustrated in the details of the dataset (Appendix D), the interaction objects are always positioned in the same location for both datasets, and hence it would be important to conduct experiments on generalization. For instance, how would the method fare if the objects’ locations were slightly different? Otherwise, the learned policy is likely to overfit to single object instances and not be useful for downstream applications. This should ideally be tested and at least be discussed in the paper. Furthermore, although the trajectory analysis provided in Figure 5 indicates a slight variation in the facing direction towards the object, it seems that the agent is mostly facing the interaction objects and needs to walk straight to reach it (cf. supplementary video). It would be interesting to see whether the agent can act in the scene without being right in front of the object of interest and facing towards it.
Other Comments and Technical Questions
The training procedure of the universal controller proposed for sampling sequences likely impose quite a large computational overhead since it has to run all frames of AMASS (4000k) through the value function. How often is this distribution recomputed? Moreover, is there a mistake in the notation of the initialization states, or what is the reason that the target state is the same as the input state and not the next state?
Did the authors run experiments without the redundancy of information in the state space of the universal controller (e.g. to have joint angles in axis angle and quaternion representation or the difference between joint position and target joint position in world and agent-centric coordinates) or what was the incentive behind overloading the state space?
Do the authors employ any technique, such as early stopping, when training the universal controller and the dynamics-regulated kinematic policy to avoid learning from failure cases?
The notation in Appendix C Policy Network architecture seems to be inconsistent. The quaternion difference ⊖
and the minus seem to be used for angle-axis difference and vice-versa, at least according to the dimensions provided.
Will the authors release the dataset in case of publication to foster further research?
A potential missed citation is [7]. The authors use a differentiable physics model to correct their kinematics model for pose reconstruction.
[1] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih, “Simpoe: Simulated character control for 3d human pose estimation”, CVPR, 2021
[2] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. 2020, “PhysCap: physically plausible monocular 3D motion capture in real time”, ACM Transactions on Grap., 2020.
[3] Davis Rempe, Leonidas J Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, and Jimei Yang, “Contact and human dynamics from monocular video”, ECCV 2020
[4] Jungdam Won, Deepak Gopinath, and Jessica Hodgins, “A scalable approach to control diverse behaviors for physically simulated characters”, ACM Trans. Graph., 2020.
[5] ] Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler, “Unicon: Universal neural controller for physics-based character motion”, arxiv, abs/2011.15119, 2020.
[6] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess, “Neural probabilistic motor primitives for humanoid control”, ICLR, 2019
[7] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick Pérez, and Christian Theobalt, “""Neural PhysCap"" Neural Monocular 3D Human Motion Capture with Physical Awareness”, ACM Trans. Graph., 2021.
Post rebuttal
I appreciate the author's extensive answer to my concerns and questions. In the light of how the concerns were addressed, I'm willing to raise my score to 6. What I would like to see in the final version of the paper in case of acceptance is 1) a thorough discussion of the limitations (which is missing in the current main part of the manuscript) 2) the requested evaluations of the universal controller and the dataset statistics. 3) Clarification of how the pose metrics were obtained with respect to failing sequences. It seems that such good numbers for the pose can only be achieved if the sequences are ended for episodes deemed unsuccessful.",1) a thorough discussion of the limitations (which is missing in the current main part of the manuscript),859,0
NIPS_2021_908,NIPS_2021,"The checklist claims that this work discusses its limitations, but the referenced limitation is “mechanisms in the RNNs should be interpreted as a hypothesis for biological mechanisms”. I wouldn’t consider this a discussion of limitations of this work. I’m not even sure how it’s a limitation at all, but if the idea is that RNNs aren’t a perfect model for biological intelligence and are primarily theorized as one, then that’s a critique of the whole field (in fact, one that this paper is trying to rectify). The one limitation that this work does discuss in the main text is that experiments should extend to more complex tasks, but this is given half a sentence and is posed as a future work. I totally agree and think the paper is complete enough to leave that for future work, but that’s not a sufficient discussion of limitations as per the NeurIPS checklist guidelines. I think the paper should discuss the limited space of tasks (even in the range of complexity of RDM and (N-)CDI) and learning rules, or biological implausibility of some of the learning rules. It’s certainly possible that I misinterpreted the note in the checklist, but I don’t think this paper really discusses the limitations. I also don’t think it would be hurt by a discussion - it’s a good paper that does enough to survive an honest discussion of limitations.
I agree that this paper is theoretical enough to not need a discussion of societal impacts. Since the work “involves understanding deep neural networks”, I’m not sure that it “inherits the broader positive and negative societal impacts of deep learning”. It does very indirectly (which is what I think the authors are saying), but (also indirectly, but less so) it inherits more of the impacts of neural network interpretability than neural network application. One thing that isn’t touched is the societal impact of having good models of biological intelligence, which are huge and rarely discussed. I don’t think this warrants a flag for ethical review, since 1) there is so much ground between the state of the field and a societally impactful model and 2) I rarely see such discussion in comp neuro papers, but I do believe it warrants consideration if it is the ultimate goal.","2) I rarely see such discussion in comp neuro papers, but I do believe it warrants consideration if it is the ultimate goal.",860,1
NIPS_2021_998,NIPS_2021,"• Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear. Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work",• 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes.,861,0
NIPS_2021_291,NIPS_2021,"The writing is clear and the motivation is clarified clearly. Besides, the theoretical grounding and experimental evaluation are not sufficient to show their originality and significance. Here are some of the suggestions: 1) I would like to see ablation studies for the proposed training method, the traditional backpropagation framework refers as the baseline. 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.","1) I would like to see ablation studies for the proposed training method, the traditional backpropagation framework refers as the baseline.",862,0
NIPS_2021_311,NIPS_2021,"- The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- Line 50: An example of when a prototype model would be unsuitable would strengthen your argument.,863,0
NIPS_2021_1852,NIPS_2021,"W1: The design of extending SGC (from Equation 1) to EIGNN (from Equation 3) is somehow implicit and ad-hoc without clear justifications. The authors should explain this more in details for better understanding by general audiences that not very familiar with implicit models.
W2: During the time complexity analysis, only the complexity of training is analyzed, but it seems like the computation of eigendecomposition of S, the normalized adjacency matrix with self-loops, (Line 176) is not added, which usually requires the cost of O ( n 3 )
. If this is true, a full eigendecomposition of a large sparse S could make EIGNN an impractical approach for prohibiting the scalability in terms of large number of nodes n
for huge real-world graphs.
W3: Several concerns upon experiments include: 1) The discussion on arbitrary hyperparameter γ
is missing, including how to set it in practice for a given graph and analyzing on the sensitivity of this hyperparameter， otherwise it will be hard for the researchers to follow. 2) As the weakness on the analysis of complexity, why the author chooses not to evaluate the long-range dependency on the standard dataset Amazon Co-purchase as used in IGNN. Amazon Co-purchase dataset has another benefit that it can also reflect the scalability of proposed method since it is a large dataset with ~33k nodes, while the experiments on real-world dataset are all conducted on graphs that less than 10k. 3) For the evaluation on over-smoothing, it would be interesting to see how the EIGNN performs with respect to over-smoothing under standard setting on real-world datasets, especially in comparison with variants focusing on dealing with over-smoothing, such as the setting used in GCNII. 4) The evaluation on robustness is not very convincing since structural attack is known to be more powerful and appreciative when we attack on graph-structured data. Thus, the authors are suggested to defend their proposed model against several popular structural attack methods such as Nettack for better demonstration rather than attacks on features used in experiments.","2) As the weakness on the analysis of complexity, why the author chooses not to evaluate the long-range dependency on the standard dataset Amazon Co-purchase as used in IGNN. Amazon Co-purchase dataset has another benefit that it can also reflect the scalability of proposed method since it is a large dataset with ~33k nodes, while the experiments on real-world dataset are all conducted on graphs that less than 10k.",864,0
NIPS_2021_1532,NIPS_2021,"1 There are some related methods missed, such as [1][2][3]. 2 The proposed method employs three strategies to alleviate catastrophic forgetting. These strategies are often employed in other incremental methods. Thus, the contributions are limited in my opinion. 3 Compared with the ground truth, the visualization results are terrible. Thus, I want to see some analysis of satisfied cases and terrible cases. [1] Class-incremental learning for semantic segmentation re-using neither old data nor old labels. ITSC2020. [2] Modeling the background for incremental learning in semantic segmentation. CVPR2020. [3] Incremental Learning Techniques for Semantic Segmentation. ICCVW2019","1 There are some related methods missed, such as [1][2][3].",865,0
NIPS_2021_312,NIPS_2021,"1. The proposed framework can be seen as a box-free simplification of DETR. Hence, the advantages and novel contributions of the proposed approach compared to DETR needs to be further clarified. 2. For the panoptic segmentation task, DETR predicts boxes around both stuff and thing classes, while the proposed approach directly predicts masks for both stuff and things classes. Lines 290-293: Please clarify why predicting masks based on boxes is inferior to predicting masks directly. In many scenarios, the box prediction may provide some prior information indicating possible image regions containing the desired category, which may be helpful even for the stuff classes. Hence, it will be interesting to investigate if the superior PQ^stuff performance over DETR in Table 3 is really due to this direct prediction of mask, or due to the encoder-decoder structure? 3. Why the MaskFormer operation (matrix multiplication operation) is superior to the panoptic head in DETR for the stuff segmentation task. It will be interesting to perform an ablation study that replaces the matrix operation in MaskFormer with the DETR panoptic head module.
There is no major concerns regarding the negative societal impact of this work.","1. The proposed framework can be seen as a box-free simplification of DETR. Hence, the advantages and novel contributions of the proposed approach compared to DETR needs to be further clarified.",866,0
NIPS_2021_1318,NIPS_2021,"weakness in clarity alone, I'll reduce my score down to a 7 with the hopes that the authors incorporate suggestions from the other reviewers to boost the presentation for a broader RL audience in a camera-ready version.
Given the novelty of the problem formulation and the wide range of possibilities that future work might explore, I'm far less concerned by the significance issues raised by the other reviewers (performance relative to backprop and more parameters overall for the critic) and continue to advocate for accepting the paper.
[1] Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. ""On the theory of policy gradient methods: Optimality, approximation, and distribution shift."" Journal of Machine Learning Research 22, no. 98 (2021): 1-76.
[2] Agarwal, Alekh, Mikael Henaff, Sham Kakade, and Wen Sun. ""Pc-pg: Policy cover directed exploration for provable policy gradient learning."" arXiv preprint arXiv:2007.08459 (2020).
[3] Osband, Ian, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. ""Deep Exploration via Randomized Value Functions."" J. Mach. Learn. Res. 20, no. 124 (2019): 1-62.
[4] Russo, Daniel, and Benjamin Van Roy. ""Learning to optimize via information-directed sampling."" Advances in Neural Information Processing Systems 27 (2014): 1583-1591.
[5] Schaul, Tom, Daniel Horgan, Karol Gregor, and David Silver. ""Universal value function approximators."" In International conference on machine learning, pp. 1312-1320. PMLR, 2015.
The authors are quite upfront in their discussion of the limitations in the proposed coagent network approach and adequately highlight paths forward for future work to consider in bypassing these obstacles.","124 (2019): 1-62. [4] Russo, Daniel, and Benjamin Van Roy. ""Learning to optimize via information-directed sampling."" Advances in Neural Information Processing Systems 27 (2014): 1583-1591. [5] Schaul, Tom, Daniel Horgan, Karol Gregor, and David Silver. ""Universal value function approximators."" In International conference on machine learning, pp. 1312-1320. PMLR, 2015. The authors are quite upfront in their discussion of the limitations in the proposed coagent network approach and adequately highlight paths forward for future work to consider in bypassing these obstacles.",867,1
NIPS_2021_2186,NIPS_2021,"Weakness
The novelty is incremental in engineering. The design of noisy labels' uncertainty can be only applied into the scenario of two-stage object detectors' training. For other one-stage detectors or anchor-free detectors, there may be no Δ b
calculated from IOU, thus limited the scope of the novelty.
The experiment results are not significant to show the effectiveness of the design. The compared the method is not the state-of-the-art performance. Before the NeuRIPS submission deadline, there is an ICLR 2021 paper [R1]. The reported performance on Table 1 VOC+coco-20cls is 50.34 A P 50 : 95
, which is higher than the proposed model's performance. In Table 2, the reported performance on coco-115 + coco-120 is 41.3 A P 50 : 95
, with only 0.4 decrease compared to that from the proposed model.
[R1]: Unbiased Teacher For Semi-supervised Object Detection, ICLR 2021.","95 , with only 0.4 decrease compared to that from the proposed model. [R1]: Unbiased Teacher For Semi-supervised Object Detection, ICLR 2021.",868,1
NIPS_2021_952,NIPS_2021,"- Some important points about the method and the experiments are left unclear (see also questions below). - The writing could be improved (see also Typos & Additional Questions below) - Multiple runs and significance tests are missing. This makes it hard to judge the improvements (Table 2 & 3).
Most Important Questions - Line 156: What is q_ij^k here exactly? I thought q_ij was a state flag, such as “2” or “0”. But you tokenize it and encode it, so it sounds more like it is something like “Copy(snow)”? (If it is the latter, then what is the meaning of tokenizing and encoding something like “Len(9)”?) - 192: What exactly is storyline and what do you need it for? - The baseline takes the predicate logic constraints as input: How does T6 know what to do with these inputs? Was the model trained on this but without the NRETM module? Can you give an example of what the input looks likes? How do these inputs guide which sentences should be generated? Looking at the datsset, it feels like one would need at least the first 2 sentences or so to know how to continue. Maybe this information is now in your constraints but it would be important to understand what they look like and how they were created. Is there no other suitable baseline for this experiment? - What is the overhead of your method compared to standard decoding approaches? (you mention GBS can only be used with T5-Base, so your method is more efficient? That would be important to point out) - What happens if the decoding process cannot find a sequence that satisfies all constraint? - Document-level MT: How do you know at test time whether the system translates a particular sentence or not? - How many sentences are misaligned by Doc-mBART25? What are the s-BLEU and d-BLEU values on the subset that NRETM aligns correctly and Doc does not? - Why was NEUROLOGIC not used as a comparison baseline? - What is dynamic vs static strategy? In which experiment did you show that dynamic works better than static (from conclusion)?
Typos & Additional Questions - Line 40: you could mention here that the examples will be translated into logic forms in the next section. - Paragraph starting at line 53: Why did you choose these datasets? How will they help evaluate the proposed approach? - Line 75: a and b should be bold faced? - 83: “that used” -> “that are used” - 83: “details” -> “for details” - Paragraph at line 86: At this point, the state matrix is unclear. What are the initial values? How can the state matrix be used to understand if a constraint is satisfied or not? - 98: “take[s]” & “generate[s]” - 108: “be all” -> “all be” - Paragraph at line 101: What is dynamic vs static strategy? - Paragraph at line 109: The state flag explanation would greatly benefit from an example. Does q_i refer to whether a particular U_i is satisfied? - Eq 2: What is the meaning of N? Can it change depending on the definition of U_k? Does it mean this constraint is not relevant for x_i? - 133: Figure 1 should be Figure 2 - Figure 2: What exactly do the “&” rows track? - Figure 2: Is the state flag matrix equal to the state matrix? If not, how do you go from one to the other? - Line 146: What does the inf in the superscript signify? - 177: What is the symbolic operator? - Paragraph at line 194: Without understanding what a storyline is, it is not clear what the constraints are. An example might be helpful here. - Line 204: what is the ROUGH-L metric? Do you mean ROUGE-L? - Line 223: How do you obtain the morphological inflections for the concepts? - 237: @necessity [of] integrating” - 3.3: How exactly is the document-level MT done? Is the entire input document the input to T5? - 293: “because” typo - 3.4 where/how exactly is the sentence index used?
The paper's broader impact section discusses general potential benefits and issues of text generation (from large language models). It could maybe be tailored a bit better by discussing what effect this proposed work would have on the potential benefits and issues.",- Some important points about the method and the experiments are left unclear (see also questions below).,869,0
NIPS_2021_1743,NIPS_2021,"1. While the paper claim the importance of language modeling capability of pre-trained models, the authors did not conduct experments on generation tasks that are more likely to require a well-performing language model. Experiments on word similarity and SquAD in section 5.3 cannot really reflect the capability of language modeling. The authors may consider to include tasks like language modeling, machine translation or text sumarization to strenghen this part, as this is one of the main motivations of COCO-LM. 2. Analysis of SCL in section 5.2 regarding few-shot abaility looks not convincing. The paper claims that a more regularized representation space by SCL may result in better generalization ability in few-shot scenarios. However, results in Figure 7(c) and (d) do not meet our expectation such that COCO-LM achieves much more improvements with less labels and the improvements will gradually disappear with more labels. Besides, the authors may check if COCO-LM brings benefits to sentence retrieval tasks with the learned anisotropy text representations. 3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works.
Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance? 2. In Table 2, it looks like COCO-LM especially affects the performance on CoLA and RTE hence the final performance. Can the authors provide some explanation on how the proposed pre-training tasks affect the two different GLEU tasks? 3. In section 5.1, the authors say that the benefits of the stop gradient operation are more on stability. What stability, the training process? If so, are there any learning curves of COCO-LM with and without stop gradient during pre-training to support this claim? 4. In section 5.2, the term “Data Argumentation” seems wrong. Did the authors mean data augmentation?
Typos 1. Check the term “Argumentation” in line 164, 252, and 314. 2. Line 283, “a unbalanced task”, should be “an unbalanced task”. 3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?","2. Line 283, “a unbalanced task”, should be “an unbalanced task”.",870,1
NIPS_2021_597,NIPS_2021,". As long as it is true that constant sensing on mobile devices is energy consuming, the energy requirements of running a POMDP policy itself can be much higher when a POMDP is solved using Monte Carlo planning or a deep neural network is involved. Over the last 10 years multiple research groups have observed that finite-state controllers could be more appropriate for executing POMDP policies on mobile devices. I would say that combining ACNO-MDPs with finite state controllers or equivalent could make the user-experience example more realistic. I cannot buy it in this form.
On page 5, the authors said ""While in our analysis we leverage algorithms that guarantee epsilon-optimal performance in POMDP planning [46, 10], these approaches are often impractical for any reasonably-sized POMDP."". I think that this is a bold statement or it should be clarified. Does it mean that all the existing research on PAC-bounds is useless? Do the authors want to say that PAC bounds are impractical? This statement can be aggravating for some researchers.
The paragraph that is between lines 197-209 is a bit unclear for someone who does not know [33]. Could you please check it? For example, after reading the paragraph I am not sure why I see GRU units in algorithm 2. GRU was not mentioned in the description of the method in the text above the algorithm.
The paper mentions ethical concerns in line 362. The authors should explain what the reason for those concerns could be. I would assume that in contrast to what the authors said, refusing to use advanced technology to help people would be unethical.
Many acronyms in the references section are not capitalised, e.g., POMDP is always pomdp, Bayesian should start with a capital B, and the ""US"" shouldn't be ""us"". Every letter that should be capitalised could be simply put in curly braces in Bibtex. For example, if you type {B}ayesian in Bibtex, you will get Bayesian in your list of references in Latex.
Equations on pp. 22 and 23 have some formatting issues. Also, the table on p. 28 is too wide.
Time in line 1019 is in seconds whereas in line 1024 in days and hours. Please unify the time units. Note that tens of thousands of seconds are not easy to comprehend.
Assuming that the PAC proves are correct, this could be a nice NeurIPS contribution.","22 and 23 have some formatting issues. Also, the table on p.",871,1
NIPS_2021_37,NIPS_2021,", * Typos/Comments)
Overall, I like and value the research topic and motivation of this paper and lean positive. However, some details are not clear enough. I would update my rating depending on the authors' feedback. The details are as follows.
+ Interesting and important research problem. This paper focuses on how to obtain disentangle representations for feature-level augmentation. This topic is interesting and important, and will attract many interests of the NeurIPS community.
+ Good quality of writing and organization. Overall, the writing quality is good and the paper is well organized. It is comfortable to read this paper, although some details are not clear.
+ Comprehensive experiments. Experiments are conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ).
- Relative difficulty score and generalized cross-entropy (GCE) loss. It is not clear how the relative difficulty score W ( x )
in Eq. (1) is used in the pipeline. W(x) is not mentioned again in both the overall objective functions Eq. (2) or Algorithm 1. Since readers may not be familiar with the generalized cross-entropy (GCE) loss, it is encouraged to briefly introduce the formulation and key points of the GCE loss to make this paper more self-contained.
- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.
- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.
- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?
- Figure 1 is not clear. First, it seems that the two y towards L CE
are the outputs of C i
, but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?
- Table 4 reported a much lower performance of ""swapping"" on BAR compared to the other three datasets. Is there any explanation for this, like the difference of datasets?
- Sensitivity to hyperparameters. The proposed framework consists of three important hyperparameters, ( λ dis , λ s w a p b , λ swap )
. It is not clear whether the framework is sensitive to these hyperparameters and how these hyperparameters are determined.
* (Suggestion) Illustration of backpropagation. As introduced in Line 167-168, the loss from C i
is not backpropagated to E b
. It would be clearer if this can be added in Figure 1.
* Line 280. Is ""the first row and column ... respectively"" a typo? It is a little confusing for me to understand this.
* Typos in Algorithm 1. Are λ dis and λ s w a p b
missed in L dis and L swap ?
* Typo in Line 209. Corrputed -> Corrupted.
============================= After rebuttal ===================================
After reading the authors' response to my questions and concerns, I would like to vote for acceptance.
The major strengths of this paper are:
The research problem, unbiased classification via learning debiased representation, is interesting and would attract the NeurIPS audience's attention.
The proposed method is simple but effective. The method is built on top of LfF [12] and further considers (1) intrinsic and bias feature disentanglement and (2) data augmentation by swapping the bias features among training samples.
The paper is clearly written and well organized.
These strengths and contributions are also pointed out by other colleague reviewers.
My main concerns were:
Unclear technical details of the GCE loss and the relative difficulty score. This concern was also shared with Reviewer 8Ai1 and iKKw. The authors' response clearly introduced the details and addressed my concern well.
Sensitivity to hyper-parameters. The authors' response provided adequate results to show the sensitivity to hyper-parameters. Other details of implementation and analysis of experimental results. The authors' responses clearly answered my questions.
Considering both strengths and the weakness, I am happy to accept this paper.
The authors have adequately addressed the limitations and potential negative societal impact of their work.","- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.",872,0
NIPS_2021_37,NIPS_2021,", * Typos/Comments)
Overall, I like and value the research topic and motivation of this paper and lean positive. However, some details are not clear enough. I would update my rating depending on the authors' feedback. The details are as follows.
+ Interesting and important research problem. This paper focuses on how to obtain disentangle representations for feature-level augmentation. This topic is interesting and important, and will attract many interests of the NeurIPS community.
+ Good quality of writing and organization. Overall, the writing quality is good and the paper is well organized. It is comfortable to read this paper, although some details are not clear.
+ Comprehensive experiments. Experiments are conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ).
- Relative difficulty score and generalized cross-entropy (GCE) loss. It is not clear how the relative difficulty score W ( x )
in Eq. (1) is used in the pipeline. W(x) is not mentioned again in both the overall objective functions Eq. (2) or Algorithm 1. Since readers may not be familiar with the generalized cross-entropy (GCE) loss, it is encouraged to briefly introduce the formulation and key points of the GCE loss to make this paper more self-contained.
- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.
- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.
- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?
- Figure 1 is not clear. First, it seems that the two y towards L CE
are the outputs of C i
, but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?
- Table 4 reported a much lower performance of ""swapping"" on BAR compared to the other three datasets. Is there any explanation for this, like the difference of datasets?
- Sensitivity to hyperparameters. The proposed framework consists of three important hyperparameters, ( λ dis , λ s w a p b , λ swap )
. It is not clear whether the framework is sensitive to these hyperparameters and how these hyperparameters are determined.
* (Suggestion) Illustration of backpropagation. As introduced in Line 167-168, the loss from C i
is not backpropagated to E b
. It would be clearer if this can be added in Figure 1.
* Line 280. Is ""the first row and column ... respectively"" a typo? It is a little confusing for me to understand this.
* Typos in Algorithm 1. Are λ dis and λ s w a p b
missed in L dis and L swap ?
* Typo in Line 209. Corrputed -> Corrupted.
============================= After rebuttal ===================================
After reading the authors' response to my questions and concerns, I would like to vote for acceptance.
The major strengths of this paper are:
The research problem, unbiased classification via learning debiased representation, is interesting and would attract the NeurIPS audience's attention.
The proposed method is simple but effective. The method is built on top of LfF [12] and further considers (1) intrinsic and bias feature disentanglement and (2) data augmentation by swapping the bias features among training samples.
The paper is clearly written and well organized.
These strengths and contributions are also pointed out by other colleague reviewers.
My main concerns were:
Unclear technical details of the GCE loss and the relative difficulty score. This concern was also shared with Reviewer 8Ai1 and iKKw. The authors' response clearly introduced the details and addressed my concern well.
Sensitivity to hyper-parameters. The authors' response provided adequate results to show the sensitivity to hyper-parameters. Other details of implementation and analysis of experimental results. The authors' responses clearly answered my questions.
Considering both strengths and the weakness, I am happy to accept this paper.
The authors have adequately addressed the limitations and potential negative societal impact of their work.","- Figure 1 is not clear. First, it seems that the two y towards L CE are the outputs of C i , but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?",873,0
NIPS_2021_953,NIPS_2021,"Although the paper gives detailed theoretical proof, the experiments are somewhat weak. I still have some concerns: 1）The most related works SwaV and Barlow Twins outperform the proposed method in some experimental results, as shown in Table 1,2,5. What are the main advantages of this method compared with SwaV and Barlow Twins? 2) HSIC(Z, Y) can be seen as a distance metric in the kernel space, where the cluster structure is defined by the identity. Although this paper maps identity labels into the kernel space, the information of one-hot label is somewhat limited compared with views embeddings in Barlow Twins. 3)Since the cluster structure is defined by the identity. How does the number of images impact the model performance? Do more training images make the performance worse or better ?
BYOL in the abstract should be explained for its first appearance.","2) HSIC(Z, Y) can be seen as a distance metric in the kernel space, where the cluster structure is defined by the identity. Although this paper maps identity labels into the kernel space, the information of one-hot label is somewhat limited compared with views embeddings in Barlow Twins.",874,0
NIPS_2021_2326,NIPS_2021,"Weakness
Overview of the main concerns, which are detailed in the paragraphs below:
Improper evaluation of the universal controller.
Missing insights into the data and the strong performance of the pose estimation, which perform better than state-of-the-art pose estimation from third-person view on established benchmarks, despite the much harder task.
Generalization of the learned tasks is not properly evaluated.
1. Universal controller
First of all, learning robust universal controllers is a challenging task and is known to suffer from instabilities, especially if it has to replicate unseen motions. There is a branch of research [4][5][6] with the sole focus on building controllers that can scale to larger datasets, but have only achieved learning on a subset of AMASS (CMU ~ 2k motion sequences). The proposed method builds on a universal controller that is trained on AMASS (~11k sequences). Hence, the claim of a universal controller that can scale to an order of magnitude larger motion database than state-of-the-art needs to be properly evaluated on its own, since it would mark a substantial improvement in the direction of imitation learning from motion capture. As stated by the authors, the policy is able to execute a wide variety of motion “ranging from dancing to kickboxing” (cf. L.40) and therefore needs to be empirically evaluated on the full AMASS dataset to substantiate this claim. The current controller is only tested on the relatively simple motions contained in H36M (walking, standing, etc.). Furthermore, it is important to see how well it adapts to noisy estimates, because at the beginning of training the dynamic-regulated model, the kinematics policy will likely produce random residuals and hence noisy target poses.
2. Missing insights into data and performance
The universal controller is then utilized in the downstream task of egocentric pose estimation. Since the task is ill-posed, estimating pose from egocentric video without top-down view is extremely difficult. The results, namely the mean per joint position error, which is very low (~30-40mm), may indicate that there is very little variation between different motion sequences and overfitting to the training data (which is likely similar to the test data). For instance, since there is no way to tell how the occluded arms are moving, such a small error is only possible if the deviation of motion between the sequences in the data is very small. The authors should provide: 1) statistical data on their dataset to be able to better assess their quantitative results (e.g., data on per joint trajectories, pose diversity against available 3D datasets), 2) a proper discussion of the very small reported errors on the test data and 2) the per joint error to be able to get better insights into the performance of the model.
This seems even more evident when looking at the supplementary video, where it appears that the agent is always moving in the same way (movement of arms, gait, speed, etc.). Since the authors already provide the statistical data of the speed in the datasets, it would be good to see whether the agent actually learns to adapt to different speeds or just overfits to one single motion.
3. Generalization
As illustrated in the details of the dataset (Appendix D), the interaction objects are always positioned in the same location for both datasets, and hence it would be important to conduct experiments on generalization. For instance, how would the method fare if the objects’ locations were slightly different? Otherwise, the learned policy is likely to overfit to single object instances and not be useful for downstream applications. This should ideally be tested and at least be discussed in the paper. Furthermore, although the trajectory analysis provided in Figure 5 indicates a slight variation in the facing direction towards the object, it seems that the agent is mostly facing the interaction objects and needs to walk straight to reach it (cf. supplementary video). It would be interesting to see whether the agent can act in the scene without being right in front of the object of interest and facing towards it.
Other Comments and Technical Questions
The training procedure of the universal controller proposed for sampling sequences likely impose quite a large computational overhead since it has to run all frames of AMASS (4000k) through the value function. How often is this distribution recomputed? Moreover, is there a mistake in the notation of the initialization states, or what is the reason that the target state is the same as the input state and not the next state?
Did the authors run experiments without the redundancy of information in the state space of the universal controller (e.g. to have joint angles in axis angle and quaternion representation or the difference between joint position and target joint position in world and agent-centric coordinates) or what was the incentive behind overloading the state space?
Do the authors employ any technique, such as early stopping, when training the universal controller and the dynamics-regulated kinematic policy to avoid learning from failure cases?
The notation in Appendix C Policy Network architecture seems to be inconsistent. The quaternion difference ⊖
and the minus seem to be used for angle-axis difference and vice-versa, at least according to the dimensions provided.
Will the authors release the dataset in case of publication to foster further research?
A potential missed citation is [7]. The authors use a differentiable physics model to correct their kinematics model for pose reconstruction.
[1] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih, “Simpoe: Simulated character control for 3d human pose estimation”, CVPR, 2021
[2] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. 2020, “PhysCap: physically plausible monocular 3D motion capture in real time”, ACM Transactions on Grap., 2020.
[3] Davis Rempe, Leonidas J Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, and Jimei Yang, “Contact and human dynamics from monocular video”, ECCV 2020
[4] Jungdam Won, Deepak Gopinath, and Jessica Hodgins, “A scalable approach to control diverse behaviors for physically simulated characters”, ACM Trans. Graph., 2020.
[5] ] Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler, “Unicon: Universal neural controller for physics-based character motion”, arxiv, abs/2011.15119, 2020.
[6] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess, “Neural probabilistic motor primitives for humanoid control”, ICLR, 2019
[7] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick Pérez, and Christian Theobalt, “""Neural PhysCap"" Neural Monocular 3D Human Motion Capture with Physical Awareness”, ACM Trans. Graph., 2021.
Post rebuttal
I appreciate the author's extensive answer to my concerns and questions. In the light of how the concerns were addressed, I'm willing to raise my score to 6. What I would like to see in the final version of the paper in case of acceptance is 1) a thorough discussion of the limitations (which is missing in the current main part of the manuscript) 2) the requested evaluations of the universal controller and the dataset statistics. 3) Clarification of how the pose metrics were obtained with respect to failing sequences. It seems that such good numbers for the pose can only be achieved if the sequences are ended for episodes deemed unsuccessful.",3) Clarification of how the pose metrics were obtained with respect to failing sequences. It seems that such good numbers for the pose can only be achieved if the sequences are ended for episodes deemed unsuccessful.,875,0
NIPS_2021_1711,NIPS_2021,"Weakness]:
1 While the results in this paper are interesting, I think that they are somewhat incremental. The idea of utilizing semialgebraic of ReLU and ∂ ReLU
is not new, even for monDEQs, see Proposition 5 in [c1], please. The approaches used are standard techniques. Moreover, as the authors point out, the size of the corresponding PSD matrix seems an unavoidable limiting factor. Thus, the proposed method appears troublesome to be extended to large models.
2 The relationship between the three models is unclear. In line 142, the authors say that “each model can be eventually used to certify robustness but they also have their own independent interest”. However, I do not find more description of “their own independent interest”. Did I miss something? There is a lack of discussion on the motivation of the three models.
3 The experiment is relatively weak. The empirical results are still on very small scales and not quite convincing. The authors conduct detailed comparison with [24], but the method are only evaluated on MNIST.
4 In line 43-45, “... DEQs have the definite advantage of being relatively small in size compared to DNNs and therefore...”. I do not agree with this argument. DEQs can be very large in size. As illustrated in previous empirical results, DEQs requires comparable parameters to achieve performance comparable to DNNs.
5 In Conclusion and Future work, the authors admit the limitation of the method. I really appreciate it. However, after reading your future work, I cannot feel the potential of this paper. I just feel that this paper is not well ready.
[Additional questions]
The authors observe that ``DEQs are much less robust to L ∞
perturbations than L 2
perturbations, in contrast with classical DNNs.” To be honest, I think this experimental result is quite interesting. But is this only an empirical observation? Moreover, as I mentioned in Weakness 4, the experiments are on very small scales (MNIST), it seems that this observation is not quite convincing. I will appreciate it if you provide more explanations that are theoretical.
Overall, there is still a lot of room for improvement in this paper. I suggest weak acceptance for this paper, but I believe that with further refinement the authors could submit a better version to a future conference.
[c1] https://openreview.net/forum?id=bodgPrarPUJ.
Yes, the authors have adequately addressed the limitations and potential negative societal impact of their work.","4 In line 43-45, “... DEQs have the definite advantage of being relatively small in size compared to DNNs and therefore...”. I do not agree with this argument. DEQs can be very large in size. As illustrated in previous empirical results, DEQs requires comparable parameters to achieve performance comparable to DNNs.",876,0
NIPS_2021_2304,NIPS_2021,"There are four limitations: 1. In this experiment, single dataset training and single dataset testing cannot verify the generalizable ability of models, it should conduct experiments on large-scale datasets. 2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems. 3. I hope to see that you can compare your model with ResNet-IBN / ResNet of FastReID, which is practical work in the person Reid task. 4. I think the authors only use the transformer to achieve the local matching, therefore, the contribution is limited.","3. I hope to see that you can compare your model with ResNet-IBN / ResNet of FastReID, which is practical work in the person Reid task.",877,0
NIPS_2021_1615,NIPS_2021,"The contribution of this paper is incremental. First, the searching algorithm is based on the existing works (Evolutionary search). The main difference is this paper aims to obtain a trade-off between the training time and sample efficiency, but what is the optimal trade-off has not been discussed. Second, as a NAS method, the searching algorithm is more important than the searched model. However, the authors cost many columns for introducing the searched model.
The part of the introduction section is like a program instruction. I am not sure whether it is appropriate for a research paper.
Authors highlight that the multi-DConv-Head attention is important, but no ablation study has been conducted for analyzing the impact of this module. Questions:
Is Tensorflow Program necessary? If the core contributions of this paper are the searching algorithm and the searched models, I think it should not be sensitive to the code platform (like PyTorch or MXNet). In other words, I think it is unnecessary to highlight Tensorflow usage in a research paper.
This paper argues the improvements are mainly from Squared ReLU and Multi-DConv-Head Attention. So these modules are determined by neural architecture search, or just hand-crafted design? Comments:
Line 115, ""search search"" => ""search"" =======================
After read authors' response, I have raised my score to 5. But I still lean negative opinion on this paper. Here are my responses: 1. Just as aforementioned, your paper focuses more attention on the model (named Primer) rather than the searching algorithm. Therefore, I think your title needs to be changed, like ""Primer: xxx"" rather than ""searching for efficient transformer"". Otherwise, it is unfair for other NAS papers, let alone you have not compared with any other NAS-based methods. And your paper is mainly focused on the autoregressive language model, called ""for language modeling"", which is also a little over-claim. 2. Searching efficient transformer has been surveyed in many previous works [a][b][c]. The difference between these works is that they only focus on lightweight compressed transformers due to the limitation of resources. Considering no one has searched such a large-scale model previously, it can be viewed as a contribution and I have raised my score to 5 based on this merit. However, a question is if under the same computation, is it possible to obtain a better model than other NAS methods? 3. About Squared ReLU and Separable Conv plus multi-head attention, I have privately conduct translation experiments on the original Transformer via replacing ReLU with Squared ReLU, and do not observe any improvement in bleu (maybe it is more suitable for the searched architecture or requires different specific hyper-parameters). And for Separable Conv plus multi-head attention has been used in many previous works, including NAS-based methods [a][b][c] and some computer vision works [d][e] also point out that Separable Conv plus multi-head attention can bring additional benefits. Therefore, it is unnecessary to emphasize the improvement is from Squared ReLU and Separable Conv plus multi-head attention in the search space. And more importantly, a convincing approach to validate the effectiveness of Squared ReLU and Separable Conv plus multi-head attention is using a search space with the constraints that we can not use Squared ReLU and Separable Conv plus multi-head attention. In other words, under these constraints, maybe we can get other advanced architectures, rather than Squared ReLU and Separable Conv plus multi-head attention. However, the authors only tried to replace the corresponding module on the original Transformer and Primer in the ablation study, rather than replace the search space and re-search architecture. 4. Will the searching code be released?
[a] AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search.
[b] NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search
[c] AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models.
[d] CvT: Introducing Convolutions to Vision Transformers
[e] Co-Scale Conv-Attentional Image Transformers","2. Searching efficient transformer has been surveyed in many previous works [a][b][c]. The difference between these works is that they only focus on lightweight compressed transformers due to the limitation of resources. Considering no one has searched such a large-scale model previously, it can be viewed as a contribution and I have raised my score to 5 based on this merit. However, a question is if under the same computation, is it possible to obtain a better model than other NAS methods?",878,0
NIPS_2021_998,NIPS_2021,"• Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear. Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work","• Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective.",879,0
NIPS_2021_558,NIPS_2021,"9. The work only explores distillation in the context of image classification.
10. This work does not report the sensitivity of their method to dataset shift, which is very relevant in the context of distillation, because distillation is usually motivated as a means to deploy ML models to small edge devices where dataset shift is almost inevitable. Hence it is extremely important to at least quantify the sensitivity to shift, rather than simply identify it as an unknown risk in the broader impacts section.","10. This work does not report the sensitivity of their method to dataset shift, which is very relevant in the context of distillation, because distillation is usually motivated as a means to deploy ML models to small edge devices where dataset shift is almost inevitable. Hence it is extremely important to at least quantify the sensitivity to shift, rather than simply identify it as an unknown risk in the broader impacts section.",880,0
NIPS_2021_2247,NIPS_2021,"1). Lack of speed analysis, the experiments have compared GFLOPs of different segmentation networks. However, there is no comparisons of inference speed between the proposed network and prior work. The improvement on inference speed will be more interesting than reducing FLOPs. 2). For the detail of the proposed NRD, it is reasonable that the guidance maps are generated from the low-level feature maps. And the guidance maps can be predicted from the the first-stage feature maps or the second-stage feature maps. It is better to provide one ablation study about the effect for them. 3). Important references are missing. The GFF[1] and EfficientFCN[2] both aims to implement the fast semantic segmentation method in the encode-decoder architecture. I encourage the authors to have a comprehensive comparison with these work.
[1]. Gated Fully Fusion for Semantic Segmentation, AAAI'20. [2]. EfficientFCN: Holistically-guided Decoding for Semantic Segmentation, ECCV'20.
See above. The societal impact is shown one the last page of the manuscript.","2). For the detail of the proposed NRD, it is reasonable that the guidance maps are generated from the low-level feature maps. And the guidance maps can be predicted from the the first-stage feature maps or the second-stage feature maps. It is better to provide one ablation study about the effect for them.",881,0
NIPS_2021_597,NIPS_2021,". As long as it is true that constant sensing on mobile devices is energy consuming, the energy requirements of running a POMDP policy itself can be much higher when a POMDP is solved using Monte Carlo planning or a deep neural network is involved. Over the last 10 years multiple research groups have observed that finite-state controllers could be more appropriate for executing POMDP policies on mobile devices. I would say that combining ACNO-MDPs with finite state controllers or equivalent could make the user-experience example more realistic. I cannot buy it in this form.
On page 5, the authors said ""While in our analysis we leverage algorithms that guarantee epsilon-optimal performance in POMDP planning [46, 10], these approaches are often impractical for any reasonably-sized POMDP."". I think that this is a bold statement or it should be clarified. Does it mean that all the existing research on PAC-bounds is useless? Do the authors want to say that PAC bounds are impractical? This statement can be aggravating for some researchers.
The paragraph that is between lines 197-209 is a bit unclear for someone who does not know [33]. Could you please check it? For example, after reading the paragraph I am not sure why I see GRU units in algorithm 2. GRU was not mentioned in the description of the method in the text above the algorithm.
The paper mentions ethical concerns in line 362. The authors should explain what the reason for those concerns could be. I would assume that in contrast to what the authors said, refusing to use advanced technology to help people would be unethical.
Many acronyms in the references section are not capitalised, e.g., POMDP is always pomdp, Bayesian should start with a capital B, and the ""US"" shouldn't be ""us"". Every letter that should be capitalised could be simply put in curly braces in Bibtex. For example, if you type {B}ayesian in Bibtex, you will get Bayesian in your list of references in Latex.
Equations on pp. 22 and 23 have some formatting issues. Also, the table on p. 28 is too wide.
Time in line 1019 is in seconds whereas in line 1024 in days and hours. Please unify the time units. Note that tens of thousands of seconds are not easy to comprehend.
Assuming that the PAC proves are correct, this could be a nice NeurIPS contribution.","28 is too wide. Time in line 1019 is in seconds whereas in line 1024 in days and hours. Please unify the time units. Note that tens of thousands of seconds are not easy to comprehend. Assuming that the PAC proves are correct, this could be a nice NeurIPS contribution.",882,0
NIPS_2021_918,NIPS_2021,"weakness Originality
+ The paper presents a novel algorithm. The algorithms applies ideas from bilevel optimization [43, 46, 55, 26, 45] to fairness and class imbalance learning. Unlike previous work [41, 33, 16, 32, 8, 13, 50, 63], which employs fixed class balancing loss functions based on some training data statistics, the proposed method automatically guides the loss function design and learns a more optimal set of parameters for the balanced loss. Quality
+ Claims are well supported through experimental evidence.
+ The authors are careful and honest about reporting the paper's limitations.
+ The paper compares its method to recent state-of-the-art [8,50] and beats them.
- Lack of more baselines. The paper compares performance to only two baselines [8,50], and it is not clear to me why more baselines are not evaluated (e.g. [41, 33, 16, 32, 13, 63]).
- No error bars in the results. There are no error bars in the result tables, making it more challenging to assess the significance of the improvements. Clarity
+ The paper is written very well, and it is easy to follow. On the most part, the paper adequately informs the reader.
- Some clarifications about the experiments are needed. Table 1 does not have any P D A
component in Algo. 1, does this mean that no P D A
was applied in those experiments? (if P D A
was used, did the authors also apply it to the baselines?) Similarly, did experiments in Table 2 apply L A init
? And lastly, does Table 3 show results for ""Algo 1. α ← Δ & l , L A init , P D A ""?
- A possible inconsistency in the results but I may also be misunderstanding something (see above for my confusion). Specifically, if I am correct in understanding, the diagonal results in Table 3 show the performance for ""Algo 1. α ← Δ & l , L A init , P D A
"" which have a higher error rate than both ""Algo 1. α ← P D A , Δ & l
"" in Table 2 and ""Algo 1. α ← Δ & l , L A init
"" in Table 1. This suggests that, in fact, P D A and L A init
degrade performance when applied together. If my understanding is correct, this undermines the second contribution point (lines 73-77) and should be explicitly clarified in the paper. If my understanding is wrong, why are the results in the diagonal in Table 3 different from either the bottom row of Table 1 or Table 2? Significance
+ This work aims to address a timely problem of fairness-seeking optimization, and the paper will be relevant to the community.
Post rebuttal update
I have read other reviewers' comments and the authors' rebuttals, and I am inclined to keep my original score of 7 with increased confidence of 4, condioned on the additional baselines being included in Table 1 in the main paper. The rebuttal addresses the clarity issues, and the additional results show that the method is at least as good as other baselines [28][63][41] in the Cifar-10-LT setting. The performance advantage over [28][63] in the ImageNet-LT dataset is much more obvious. Although results for [41] in the ImageNet-LT setting were not provided, likely the method would also be beaten like in the Cifar-10-LT setting. I think this work is good, addressing a timely problem and it will be relevant for the NeurIPS conference.
The paper adequately and fairly outlines its limitations in a dedicated section.","- No error bars in the results. There are no error bars in the result tables, making it more challenging to assess the significance of the improvements. Clarity + The paper is written very well, and it is easy to follow. On the most part, the paper adequately informs the reader.",883,1
NIPS_2021_1546,NIPS_2021,"Weakness
1. Some scalability concerns: the experiments are demonstrated under somewhat small-scale datasets
If possible, can the author show experiments requiring a large number of INR layers, i.e., a complex signal? For instance, 3D shapeNet needs eight layers to be represented with a Fourier feature network [2]. Demonstrating experiments under such conditions will be interesting for ( Implicit ) 2 .
Alternatively, datasets that are used in Neural Radiance Fields [1] will be significant. I believe such experiments are the mainstream task of INR.
2. (Minor) Comparison of inference time
I recognize that direct comparison of inference time is somewhat difficult (due to CPU utilization for the solver). However, still believe the paper should include such part.
If possible, can the author compare the inference time of ( Implicit ) 2
with the baselines?
3. (Minor) The variance seems to be big in Table 1
Can the author evaluate the images for a large number than 16, e.g., 250 samples, by following [3]? (Of course, the author can use a smaller dataset, e.g., CelebA)
Justification of the rating
I recommend accept. I believe the idea of the implicit layer is well suited for INR and also shows significant efficiency. I do enjoy reading the manuscript.
I still have some concerns about the scalability issues; therefore, I politely request the authors to share the experimental results mentioned in the weakness part. Reference
[1] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Mildenhall et al., ECCV 2020
[2] Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. Tanick et al., NeurIPS 2020
[3] Learned Initializations for Optimizing Coordinate-Based Neural Representations. Tanick et al., CVPR 2021
The authors have addressed the limitations.
For societal impacts, I agree that this work may not have negative impacts.","2. (Minor) Comparison of inference time I recognize that direct comparison of inference time is somewhat difficult (due to CPU utilization for the solver). However, still believe the paper should include such part. If possible, can the author compare the inference time of ( Implicit ) 2 with the baselines?",884,0
NIPS_2021_812,NIPS_2021,"I see two primary weaknesses in this paper: 1) numerous sweeping claims are made regarding their superiority over previously published results without sufficient support. 2) The empirical demonstration of the model compares against control models that are not well fit to the tasks. Both of these weaknesses are easily addressable, by 1) either supporting their claims more carefully or dialing them back and 2) choosing a more reasonable control or experimental task.
The authors make a large number of bold claims, for example in lines 9-10; 44-47; 52-53; 67; 96-99; 110; 123-125; 139; 141-142; 166; 300-301; 339-341. While I do not know the literature well enough to directly refute all of them, I will note a few examples where they appear to be wrong. I believe that this weakness can be resolved by either softening the claims themselves (which, in my opinion, would not reduce the novelty or significance of the submission) or providing a more detailed review of how previous works compare.
a) On lines 44-47 the authors note that “all models cited above, with the exception of Khemakhem et al. (2020a), assume that the data are fully observed and noise-free”. While that might be true, I immediately thought of the work of Locatello et al. {1}, which was not cited by the authors, but is an identifiable disentanglement method and comments on their ability to handle noise: “The generative model does not model additive noise; we assume that the noise is explicitly modeled as a latent variable and its effect is manifested through [the ground truth generator], as is done by [several citations].” (last paragraph of section 3) I admit that this might not necessarily be a contradictory claim, but I nonetheless would appreciate it if the authors clarified how their method of explicitly modeling additive noise is beneficial over the type of setup in {1} and the citations in Locatello et al.’s quoted sentence.
1.b) On lines 96-97 the authors note that “the mixing function f is assumed bijective and thus dimension reduction is not possible in most of the above models. The only exception is Khemakhem et al. (2020a) who…” I find the second part of this quote (“dimension reduction is not possible”) surprising in general, considering that any publication attempting to perform disentanglement on the DisentanglementLib dataset necessarily has to assume dimensionality reduction. I thought maybe the authors intended to say that the theory does not allow dimensionality reduction, even if the practical implementations do. However, Klindt et al., which is cited earlier, presents an identifiable disentanglement framework that only assumes an injective mixing function in their theory and therefore allows for dimensionality reduction.
1.c) The authors claim that “under the conditions given in the next section, we can now guarantee identifiability for a very broad and rich class of models. First, notice that all previous Nonlinear ICA time-series models can be recast and often improved upon when viewed through this new unifying framework.” I do not understand how this could possibly be true given that not all previous Nonlinear ICA work abides by the conditions given in sections 3 and 4. Just as two examples, the requirement for unconditional independence on lines 114-115, and tail behavior in assumption A1 are not ubiquitous in the literature. I could definitely be misunderstanding what the authors intended with their statement, in which case I would be happy with a clarification.
I understand that it is not reasonable for the authors to precisely state how every previous contribution fails to meet their claimed advantages. However, given the examples above, I found myself unconvinced in general. I further found myself wondering if it was necessary to make so many sweeping claims in the first place.
The authors set up a simulated example problem in section 5.1 to understand how their model works with a restricted experiment. I agree that this is an important step for understanding how the model behaves. They choose to compare against what they claim to be the state of the art, IIA-HMM. However, they state themselves that “IIA-HMM has a much simpler model of dynamics and no noise model, and likely lost information due to PCA pre-processing.” This leaves me wondering why they felt that this was a fair comparison, given that the problem setup is not at all matched to what IIA-HMM was designed to solve. I would be curious to see how they do for a non-dimensionality-reduced setup, since as far as I know their framework does not require the number of generating latents to be fewer than the data dimensionality. Or, a better solution would be for them to compare their model against an alternative that is appropriately matched to the dimensionality reduction task. Furthermore, the authors do not provide a baseline comparison for their denoising task. This might be because the authors wished to focus on identifiable models, which restricts the otherwise large set of denoising methods. However, again given the work cited in {1}, I am unconvinced that there was not a suitable comparison to be made. Without appropriate comparisons, we are left to rely solely on the scalar MCC metric in an unfamiliar simulated example, which I find insufficient.
Additional minor complaints:
The citations need to be revised and edited. Many are listed as arxiv prints that are now published in peer-reviewed venues. A few are missing the venue entirely. Morioka 2020b appears to be listed twice.
I noticed typos on line 243 (“a complete statistics”), fig 2 caption (“ground true independent”)
I think it would be helpful for practitioners if the authors included some justification for their decision on line 318, or at least a discussion on the tradeoffs for the number of independent components chosen.
I would appreciate it if the authors spent more time discussing trade-offs made in their framework. As of now it is limited to the first two sentences of the “Limitations” section starting on line 367. I found this to be rather terse given the space allocated for stating the failures of prior work. For example, if I understand the unconditional independence assumption on lines 114-115 correctly, then it seems highly unlikely to be met in real-world data, including their MEG experiment. The constraint on the distribution tails and 2nd order nature of the generator also seem restricting with respect to real data. Perhaps the authors could note alternative work that does not require such assumptions for identifiability (in exchange for other restrictions), which would assist future researchers seeking a more general solution.
{1} http://proceedings.mlr.press/v119/locatello20a.html","3) I admit that this might not necessarily be a contradictory claim, but I nonetheless would appreciate it if the authors clarified how their method of explicitly modeling additive noise is beneficial over the type of setup in {1} and the citations in Locatello et al.’s quoted sentence.",885,0
NIPS_2021_1554,NIPS_2021,"1 Why does Theorem 2 only show a second-order Taylor expansion of the excessive risk for group a, rather than similar result showing in Theorem 1? Since the unfairness defined in line 107 is based on excessive risk gap ξ a
, it is more meaningful and consistant to see the theoretical results with respect to ξ a
for DP-SGD in section 6.
2 In Section 9, the paper proposes a mitigation solution with extra terms. So how to determin appropriate values for γ 1 and γ 2
? How do different values of γ 1 and γ 2
affect the performance of original tasks, such as the MSE or prediction accuracy?
3 Can authors explain more about the definition of excessive risk in line 103 and how to calculate in practice, in terms of expectation? Since the optimal solution θ ∗
is not the optimal solution for the loss function w.r.t. data of group a. It can be negative values, right? But I see all excessive risk values in Figure 3 and Figure 7 are positive. What's more, are values of excessive risk comparable among different groups? If not, can authors explain why excessive risk is a good representation for fairness?","1 Why does Theorem 2 only show a second-order Taylor expansion of the excessive risk for group a, rather than similar result showing in Theorem 1? Since the unfairness defined in line 107 is based on excessive risk gap ξ a , it is more meaningful and consistant to see the theoretical results with respect to ξ a for DP-SGD in section 6.",886,0
NIPS_2021_1721,NIPS_2021,"I was very confused at the beginning about the difference between this paper and Bayesian RL. Assuming that my understanding of this paper is correct, I think the writing could be improved. Here are my comments:
It might be helpful to have a clear paragraph about the problem definition of this paper. I have found it confusing between Bayesian RL and the problem that this paper is focusing on.
Line 29-45: the zoo example conveys the drawback of empirical risk minimization. I understand that the map represents empirical risk minimization. However, I am not sure what peeking through the window represents? The anti-empirical risk minimization approach?
Line 29-45: the zoo example is good. However, it is an analogy for supervised learning rather than an analogy for RL. Considering that this paper is doing RL, it might be better to use an analogy for RL.
Sec. 4 and Fig. 1: the authors try to explain why standard RL methods could fail to generalize. However, the example problem of shoe classification is a contextual bandit problem rather than an MDP problem. Since the authors mentioned MDP multiple times in Sec. 4, it might be better to give an example of an MDP rather than a bandit.
Sec. 4 and Fig. 1: When I first read Sec. 4 and Fig. 1, I was very confused.
Given that methods like UCB can solve bandit problems, when I first read it, I didn't understand why the author said more sophisticated MDPs that use uncertainty estimates in their construction. Later on, I understood that the authors are solving a different problem from the problem that UCB is trying to solve.
Therefore, I think this section could be clarified a bit to avoid this confusion.
Line 228-237: again, I think this is a bandit problem, not an MDP problem.
Line 228-237: in this paragraph, the authors use terms like Bayes-optimal, which is very confusing to me at first.
Bayes-optimal refers to optimally solving the trade-off between exploration and exploitation. However, in the problem that this paper is focusing on, there is no exploration.
It is very confusing to see bayes-optimal and a = argmax p(y | x, D); together. Because in Bayesian RL and bandits, a = argmax p(...) usually refers to a greedy heuristic to approximate the bayes-optimal policy, e.g., UCB. So a = argmax p(...) is not bayes-optimal.
I think maybe the author could define a new concept of optimality for the generalization problem formulated in this paper, rather than using terms in Bayesian RL.
Line 228-237: again, the term POMDP makes me confusing at first, too.
In a POMDP, the belief (or posterior) is updated at every iteration when a new observation comes. However, in the problem formulated in this paper, no exploration (active information gathering) is allowed, and the belief is not updated at all during testing. Thus, I am not sure whether POMDP is the correct term. Maybe the authors could use POMDP and clarify this point.
Line 236: the authors use the word memoryless without defining it. I figured it out by reading the appendix. It would be better to define it in advance.
Eq. 6: is the policy gradient in Eq. 6 solving the optimal problem? So after convergence, will we get the optimal solution to Eq. 5? It might be better to clarify. Minor
Line 78: but also on learning - on is unnecessary.
Line 132: dπ(s) = (1−γ)... - I don't understand why (1−γ) is there (I could be wrong).
Line 212: extra space in the beginning.
Line 216: extra space in the beginning.
Line 233: It might be better to use a larger symbol for the indicator function.
I didn't check the proofs in Appendix. Post-rebuttal
Thank you for your clarification! Now I think the paper is clearer in my mind and I appreciate it more! I have raised my score by 1 point.","1: the authors try to explain why standard RL methods could fail to generalize. However, the example problem of shoe classification is a contextual bandit problem rather than an MDP problem. Since the authors mentioned MDP multiple times in Sec. 4, it might be better to give an example of an MDP rather than a bandit. Sec. 4 and Fig.",887,0
NIPS_2021_2024,NIPS_2021,"below). Using the related literature on active interventions would require full identification of the underlying DAG. It is emphasized that matching only the means can be done with significantly smaller number of interventions, and this is the difference from previous works. - Identifiability in terms of Markov equivalence classes (MEC) is well discussed. Graphical characterization of the proposed shift-interventional (shift-I) MEC, and its refinement over the general interventional MEC is given clearly. Assumptions are reasonable within the given setting. - Extending the decomposition of intervention essential graphs to shift interventional essential graphs is sound. Both of the proposed approaches for solving the problem, clique tree and supermodular strategies are reasonable. Use of a lower bound surrogate function to enable supermodularity is clever. - The paper is organized clearly, and the theoretical claims are well supported.
Weaknesses: I have several concerns on the importance of the proposed settings and usefulness of the results. - Although the causal matching problem seems interesting and new, it is not well motivated. To the reviewer’s knowledge, interventions on a causal model are tied to inferring the underlying structure (it does not need to be the whole structure of the model). In this regard, it is not clear how exactly matching the means of a causal system is preferable to performing more relaxed cases of soft interventions. The authors are encouraged to further explain how this setting can be beneficial. - Deterministic shift interventions are useful to test the applicability of the proposed ideas. However, restricting the problem setting to only shift interventions is quite limited and leads to some rather trivial results. For instance, existence and uniqueness results of matching shift-intervention in Lemma 1, and the properties of source nodes in Lemma 2 are immediate observations in a DAG. - Clique tree approximation is just a minor modification of the cited central node algorithm (Greenewald et al., 2019). - Complexity of the submodularity approach subroutine uses SATURATE algorithm (Krause et al., 2008), and is said to scale with N 5
in appendix D.4. It is worth commenting on the feasibility of this approach. For instance, what are the runtimes of the simulations for large models in Section 6? - It is a nice result that the number of proposed interventions is only a logarithmic factor of the lower bound. However, the baselines in the simulations are not very strong to demonstrate the usefulness. Though coloring approach of Shanmugam et al., 2015 is a related active intervention design, the goal of it is broader than finding a matching intervention. For instance, a simple random upstream search, the other baseline, performs much better than coloring due to the simpler objective. That being said, the reviewer understands that the proposed task is new and fair comparisons may not be easy.
Although this paper has several nice properties, the overall contribution, constraints on the problem, and the importance of the results are not adequate for publication at NeurIPS.
Main limitations of the work, which are also stated in the above review, and potential impact of the work, which is not very imminent, are adequately addressed in the discussion section.","- Extending the decomposition of intervention essential graphs to shift interventional essential graphs is sound. Both of the proposed approaches for solving the problem, clique tree and supermodular strategies are reasonable. Use of a lower bound surrogate function to enable supermodularity is clever.",888,1
NIPS_2021_952,NIPS_2021,"- Some important points about the method and the experiments are left unclear (see also questions below). - The writing could be improved (see also Typos & Additional Questions below) - Multiple runs and significance tests are missing. This makes it hard to judge the improvements (Table 2 & 3).
Most Important Questions - Line 156: What is q_ij^k here exactly? I thought q_ij was a state flag, such as “2” or “0”. But you tokenize it and encode it, so it sounds more like it is something like “Copy(snow)”? (If it is the latter, then what is the meaning of tokenizing and encoding something like “Len(9)”?) - 192: What exactly is storyline and what do you need it for? - The baseline takes the predicate logic constraints as input: How does T6 know what to do with these inputs? Was the model trained on this but without the NRETM module? Can you give an example of what the input looks likes? How do these inputs guide which sentences should be generated? Looking at the datsset, it feels like one would need at least the first 2 sentences or so to know how to continue. Maybe this information is now in your constraints but it would be important to understand what they look like and how they were created. Is there no other suitable baseline for this experiment? - What is the overhead of your method compared to standard decoding approaches? (you mention GBS can only be used with T5-Base, so your method is more efficient? That would be important to point out) - What happens if the decoding process cannot find a sequence that satisfies all constraint? - Document-level MT: How do you know at test time whether the system translates a particular sentence or not? - How many sentences are misaligned by Doc-mBART25? What are the s-BLEU and d-BLEU values on the subset that NRETM aligns correctly and Doc does not? - Why was NEUROLOGIC not used as a comparison baseline? - What is dynamic vs static strategy? In which experiment did you show that dynamic works better than static (from conclusion)?
Typos & Additional Questions - Line 40: you could mention here that the examples will be translated into logic forms in the next section. - Paragraph starting at line 53: Why did you choose these datasets? How will they help evaluate the proposed approach? - Line 75: a and b should be bold faced? - 83: “that used” -> “that are used” - 83: “details” -> “for details” - Paragraph at line 86: At this point, the state matrix is unclear. What are the initial values? How can the state matrix be used to understand if a constraint is satisfied or not? - 98: “take[s]” & “generate[s]” - 108: “be all” -> “all be” - Paragraph at line 101: What is dynamic vs static strategy? - Paragraph at line 109: The state flag explanation would greatly benefit from an example. Does q_i refer to whether a particular U_i is satisfied? - Eq 2: What is the meaning of N? Can it change depending on the definition of U_k? Does it mean this constraint is not relevant for x_i? - 133: Figure 1 should be Figure 2 - Figure 2: What exactly do the “&” rows track? - Figure 2: Is the state flag matrix equal to the state matrix? If not, how do you go from one to the other? - Line 146: What does the inf in the superscript signify? - 177: What is the symbolic operator? - Paragraph at line 194: Without understanding what a storyline is, it is not clear what the constraints are. An example might be helpful here. - Line 204: what is the ROUGH-L metric? Do you mean ROUGE-L? - Line 223: How do you obtain the morphological inflections for the concepts? - 237: @necessity [of] integrating” - 3.3: How exactly is the document-level MT done? Is the entire input document the input to T5? - 293: “because” typo - 3.4 where/how exactly is the sentence index used?
The paper's broader impact section discusses general potential benefits and issues of text generation (from large language models). It could maybe be tailored a bit better by discussing what effect this proposed work would have on the potential benefits and issues.",- 237: @necessity [of] integrating” - 3.3: How exactly is the document-level MT done? Is the entire input document the input to T5?,889,0
NIPS_2021_806,NIPS_2021,"The main limitation of this work is the lack of any empirical analysis. It would be of high importance for the reader to be able to check that the theoretical findings are aligned with those of the experiments (even on toy environments).
Another open question is if the proposed algorithm is actually applicable in practice. As mentioned by the authors, the sampling of the posterior is not computationally tractable in its current form (Eq. 3).
To conclude, despite the theoretical guarantees of the proposed conditional posterior sampling algorithm, authors should make clear if this algorithm could be also applied in practice.
Minor comments/typos
l. 150 pairs. -> pair,
l. 160: in comparison the the
I think that the limitations of this work have been adequately addressed by the authors","150 pairs. -> pair, l. 160: in comparison the the I think that the limitations of this work have been adequately addressed by the authors",890,1
NIPS_2021_1374,NIPS_2021,"Weakness: Major:
Lack of many important technical details:
1.1. Some important symbols are not explained, which makes it very difficult to understand what has been done. For example, what is y j
? And how is Ω t
sampled (sparely sampled on the trajectory only, or randomly and densely covering the testing environment)? What is V, E in Figure 2?
1.2. The method did not mention at all how the map Φ
can handle variations in Ω s
caused by camera rotations, which is a key question on whether the map can localize the camera when it takes two photos at the same position but with different orientations.
1.3. How are the positive and negative samples in the triplets generated? What's the influence of the hyperparameters in this module (3.3.1) on the whole method? 1.4. There is no experiment/ablation study to justify the need/importance for equation (6) instead of Euclidean distance in a local area.
Lack of discussions of the failure cases. Would this method fail in some cases? For example, the experiments never showed the trajectory in symmetric environments/maps. If Ω t
is a square or rectangle, how can this method handle the rotational ambiguities? How does it know that an image is taken at the top-left corner instead of the top-right corner (because the optimal transport cost could be the same in this case)?
Lack of comparisons to baseline methods, and related work discussions on other weakly supervised training of the localization function Φ
(essentially a PoseNet [Kendall 2015]).
Lack of real-world image-based localization experiments. Minor:
5. Section 3.2 may be shortened since many equations are NOT proposed by this work, but are well-known in the community already. This would save space to include the above missing important details.
6. A topological map has a special meaning in SLAM and localization (no global metric information), which is different from the one used in this paper. It is suggested to change the term to a metric map, because it seems that you do need to make metric measurements (even if it could be noisy) to construct Ω t .
7. Is it stable to train the deep localization network with the differentiable Sinkhorn? I wish to see some training losses.
8. What if you use L2 instead of KL for L(.,.)? Any ablation study? Do you have any reference to support your claim that ""choosing Eucidean distances for L is ill-posed""?
I like the raw idea of the paper, and the problem being addressed is an important one in robotics. However, the lack of details and the quality of the presentation make it very difficult for me to recommend this paper for acceptance.","8. What if you use L2 instead of KL for L(.,.)? Any ablation study? Do you have any reference to support your claim that ""choosing Eucidean distances for L is ill-posed""? I like the raw idea of the paper, and the problem being addressed is an important one in robotics. However, the lack of details and the quality of the presentation make it very difficult for me to recommend this paper for acceptance.",891,0
NIPS_2021_456,NIPS_2021,"For completeness, the search objective (Eq. (1)) should take into account or discuss all the possibilities. Nevertheless, the objective does not consider the situation that there is only one patch that has higher confidence than the image. In this situation, according to the objective, N_{i} would be an empty set.
The link between the motivation ""one explanation is not enough"" and the proposed structured attention graphs (SAGs) is in question. As shown in Figure 1, there are three regions (i.e., d, e, f) of interest that lead to high confidence w.r.t. the ground-truth class. Can we merge them into one such that we have one comprehensive explanation?
The connection between the proposed method and conventional vision task(s) is unclear. Specifically, the related works, i.e., Grad-CAM, LayerCAM, Ablation-CAM, provide experimental results to demonstrate that their attention maps are helpful for class-specific localization. In contrast, this work does not either provide similar experiments or discuss this. Therefore, without convincing empirical evidence, I don't understand how the proposed method benefits the community for vision task(s).
To study the problem of finding multiple explanations, multi-label datasets, e.g., MS COCO, may be better than single-label datasets, e.g., ImageNet, which is used in this work. In other words, for the images that are with single labels and are probably object-centric, what is the reason to generate multiple explanations for understanding the classification results?
The writing can be improved. 1). Section 4.1 is not clear as X in line 236 is not self-contained. Does it represent a set of images, which is defined in line 121? However, according to the context, this section presents how to find multiple candidate minimal sufficient explanations on an image, instead of a set of images. 2). Symbol c is defined as the desired size in line 243, while it is also defined as a class in line 121. 3). The acronym MSE (Minimal sufficient explanation) may give rise to some confusion as it conventional stands for mean squared error.
This work misses a few analyses for better understanding the proposed methods. Specifically, what is the time complexity w.r.t. the number of patches? How do the number of patches and P_{h} influence the proposed method and the conclusion of the user study?
In the checklist, 1.d, the answer states ""We discuss a potential negative societal impact of our work in the appendix"", but I couldn't find such a discussion in the appendix.
Regarding the potential societal impact, the authors may want to discuss computational models' bias, which may be reflected on class-specific attention maps.","1). Section 4.1 is not clear as X in line 236 is not self-contained. Does it represent a set of images, which is defined in line 121? However, according to the context, this section presents how to find multiple candidate minimal sufficient explanations on an image, instead of a set of images.",892,0
NIPS_2021_1546,NIPS_2021,"Weakness
1. Some scalability concerns: the experiments are demonstrated under somewhat small-scale datasets
If possible, can the author show experiments requiring a large number of INR layers, i.e., a complex signal? For instance, 3D shapeNet needs eight layers to be represented with a Fourier feature network [2]. Demonstrating experiments under such conditions will be interesting for ( Implicit ) 2 .
Alternatively, datasets that are used in Neural Radiance Fields [1] will be significant. I believe such experiments are the mainstream task of INR.
2. (Minor) Comparison of inference time
I recognize that direct comparison of inference time is somewhat difficult (due to CPU utilization for the solver). However, still believe the paper should include such part.
If possible, can the author compare the inference time of ( Implicit ) 2
with the baselines?
3. (Minor) The variance seems to be big in Table 1
Can the author evaluate the images for a large number than 16, e.g., 250 samples, by following [3]? (Of course, the author can use a smaller dataset, e.g., CelebA)
Justification of the rating
I recommend accept. I believe the idea of the implicit layer is well suited for INR and also shows significant efficiency. I do enjoy reading the manuscript.
I still have some concerns about the scalability issues; therefore, I politely request the authors to share the experimental results mentioned in the weakness part. Reference
[1] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Mildenhall et al., ECCV 2020
[2] Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. Tanick et al., NeurIPS 2020
[3] Learned Initializations for Optimizing Coordinate-Based Neural Representations. Tanick et al., CVPR 2021
The authors have addressed the limitations.
For societal impacts, I agree that this work may not have negative impacts.","1. Some scalability concerns: the experiments are demonstrated under somewhat small-scale datasets If possible, can the author show experiments requiring a large number of INR layers, i.e., a complex signal? For instance, 3D shapeNet needs eight layers to be represented with a Fourier feature network [2]. Demonstrating experiments under such conditions will be interesting for ( Implicit ) 2 . Alternatively, datasets that are used in Neural Radiance Fields [1] will be significant. I believe such experiments are the mainstream task of INR.",893,0
NIPS_2021_1318,NIPS_2021,"weakness in clarity alone, I'll reduce my score down to a 7 with the hopes that the authors incorporate suggestions from the other reviewers to boost the presentation for a broader RL audience in a camera-ready version.
Given the novelty of the problem formulation and the wide range of possibilities that future work might explore, I'm far less concerned by the significance issues raised by the other reviewers (performance relative to backprop and more parameters overall for the critic) and continue to advocate for accepting the paper.
[1] Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. ""On the theory of policy gradient methods: Optimality, approximation, and distribution shift."" Journal of Machine Learning Research 22, no. 98 (2021): 1-76.
[2] Agarwal, Alekh, Mikael Henaff, Sham Kakade, and Wen Sun. ""Pc-pg: Policy cover directed exploration for provable policy gradient learning."" arXiv preprint arXiv:2007.08459 (2020).
[3] Osband, Ian, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. ""Deep Exploration via Randomized Value Functions."" J. Mach. Learn. Res. 20, no. 124 (2019): 1-62.
[4] Russo, Daniel, and Benjamin Van Roy. ""Learning to optimize via information-directed sampling."" Advances in Neural Information Processing Systems 27 (2014): 1583-1591.
[5] Schaul, Tom, Daniel Horgan, Karol Gregor, and David Silver. ""Universal value function approximators."" In International conference on machine learning, pp. 1312-1320. PMLR, 2015.
The authors are quite upfront in their discussion of the limitations in the proposed coagent network approach and adequately highlight paths forward for future work to consider in bypassing these obstacles.","98 (2021): 1-76. [2] Agarwal, Alekh, Mikael Henaff, Sham Kakade, and Wen Sun. ""Pc-pg: Policy cover directed exploration for provable policy gradient learning."" arXiv preprint arXiv:2007.08459 (2020). [3] Osband, Ian, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. ""Deep Exploration via Randomized Value Functions."" J. Mach. Learn. Res. 20, no.",894,1
NIPS_2021_2336,NIPS_2021,". The authors state that the VCAUSE was developed for causal inference, however, they also state that they assumed that the causal graph is known in the current contribution. If it is the case, why there is the need for the causal inference?
Although the paper is quite clearly written, there are some flows. First, the role of U (exogenous) variables is not clear: is is assumed that there is 1 latent variable per 1 observed variable, as shown on Figure 1. Then, these U variables are not mentioned further in the paper. The VCAUSE (Definition 4.1) is defined without them. It is stated that ""the latent variables Z play a similar role to the exogenous variables U"" but it is very vague, and I do not see a clear correspondence.
Propositions 1 and 2 are important but I guess no novel, it should be a known result in deep learning.
I would appreciate more explanations on the design condition 2. I see that it follows from Prop. 2 but it is unclear why to use neural networks without hidden layers.",2 but it is unclear why to use neural networks without hidden layers.,895,0
NIPS_2021_1743,NIPS_2021,"1. While the paper claim the importance of language modeling capability of pre-trained models, the authors did not conduct experments on generation tasks that are more likely to require a well-performing language model. Experiments on word similarity and SquAD in section 5.3 cannot really reflect the capability of language modeling. The authors may consider to include tasks like language modeling, machine translation or text sumarization to strenghen this part, as this is one of the main motivations of COCO-LM. 2. Analysis of SCL in section 5.2 regarding few-shot abaility looks not convincing. The paper claims that a more regularized representation space by SCL may result in better generalization ability in few-shot scenarios. However, results in Figure 7(c) and (d) do not meet our expectation such that COCO-LM achieves much more improvements with less labels and the improvements will gradually disappear with more labels. Besides, the authors may check if COCO-LM brings benefits to sentence retrieval tasks with the learned anisotropy text representations. 3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works.
Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance? 2. In Table 2, it looks like COCO-LM especially affects the performance on CoLA and RTE hence the final performance. Can the authors provide some explanation on how the proposed pre-training tasks affect the two different GLEU tasks? 3. In section 5.1, the authors say that the benefits of the stop gradient operation are more on stability. What stability, the training process? If so, are there any learning curves of COCO-LM with and without stop gradient during pre-training to support this claim? 4. In section 5.2, the term “Data Argumentation” seems wrong. Did the authors mean data augmentation?
Typos 1. Check the term “Argumentation” in line 164, 252, and 314. 2. Line 283, “a unbalanced task”, should be “an unbalanced task”. 3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?","3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?",896,0
NIPS_2021_57,NIPS_2021,"Limitations
The paper does not introduce limitations of the proposed method.
As mentioned in Cons. 2), I think a potential limitation is that GWDS cannot significantly improve throughput and adversarial robustness if a mobile CNN adopts DWS or similar techniques already.","2), I think a potential limitation is that GWDS cannot significantly improve throughput and adversarial robustness if a mobile CNN adopts DWS or similar techniques already.",897,0
NIPS_2021_1822,NIPS_2021,"of the paper. Organization could definitely be improved and I oftentimes had a bit of a hard time following the discussed steps. But in general, I think the included background is informative and well selected. Though, I could see people having trouble understanding the state-space GP-regression when coming from the more machine-learning like perspective of function/ weight space GP-regression.
Significance: I think there is definitely a significance in this work, as GP-Regression is usually a bit problematic because of the scaling, though it is still used extensively in certain areas, such as Bayesian Optimization or for modelling dynamical systems in robotics.
• Background: f
is a random function which describes a map e.g. f : R × R D s → R
and not a function of X ∈ R N t × N s × D s
as described in eq 1. At least, when one infers the inputs from the definitions of the kernel functions.
• In general, the definitions are confusing and should be checked,e.g. check if f n , k = f ( X n , k )
is correct and properly define X n , k .
• The operator L s
is not mentioned in the text
• 2.1: The description of the process f ¯
is confusing as the relationship to the original process f
is established just at the end.
• It would be helpful to add a bit more background on how the state space model is constructed from the kernel κ t ( ⋅ , ⋅ )
, e.g. why it induces the dimensionality d t
and also describe the limitations that a finite dimensional SDE can only be established, when a suitable spectral decomposition of the kernel exist.
• It should be mentioned that p ( y ∣ H f ¯ ( t n ) )
has to be chosen Gaussian, as otherwise Kalman Filtering and Smoothing and CVI is not possible. Later on in the ELBOs this is assumed anyway. • p ( u ) = N ( u ∣ 0 , K z z )
is a finite dimensional Gaussian not a Gaussian process and p(u) is not a random variable ( = not ∼ ).
• The notations for the covariances, e.g. K z z
are discussed in the appendix. I am fine with it; however, it should be referenced as I was confused in the beginning.
• 2.2: The log
is missing for the Fisher information matrix.
• The acronym CVI is used in the paragraph headline before the definition.
• Some figures and tables are not referenced in the text, such as figure 1.
• 3.1: In line 173 the integration should be carried over f ¯
and not s
, I guess?
• I had a bit of a hard time establishing the connection E p ( f ) [ N ( Y ~ ∣ f , V ~ ) ] = p ( Y ~ )
which is the whole point why one part of the ELBO can be calculated using the Kalman filter. Adding this to a sentence to the text would have helped me a lot.
• One question I had was that for computing the ELBO the matrix exponential is needed. When backpropagating the gradient for the hyper parameters, is this efficient? As I am used to using the adjoint method for computing gradients of the (linear) differential equation.
• Reference to Appendix for the RMSE and NLPD metrics is missing.","• One question I had was that for computing the ELBO the matrix exponential is needed. When backpropagating the gradient for the hyper parameters, is this efficient? As I am used to using the adjoint method for computing gradients of the (linear) differential equation.",899,0
NIPS_2021_1863,NIPS_2021,"Weakness - One issue I find with the paper writing is this paper is not self-contained. In Table 1, a minimal introduction to iCaRL and CCIL is not missing, which makes it difficult to get the idea in section 3.1 before reviewing prior papers. Similarly, L169, minimal introduction to LwF-MC is not included. - The proposed SemanticAug seems to be an incremental contribution as it resembles ISDA [24] a lot. While the discussions at L255 are valid, the adaptation of ISDA idea in incremental classification seems to be straight-forward. - Some technical exposition is incomplete. o Fig 2 (b), when data augmentation is used, such as mixup, cutmix, it often requires more training epochs to learn a better model. Can you clarify whether results with data augmentation in Fig 2 (b) are obtained with more training epochs? o L217, how many augmented classes (i.e. m) are added? Any data sampling technique is used to balance original new classes and augmented new classes? how many images are generated for each augmented class? o L239, for each old class, do you fix the M deep features once they are generated from a normal distribution with fixed mean/covariance? Since backbone is also updated as more new tasks are added, the mean/covariance will be outdated. o L292, what is “herd” selection technique?
The chance of having negative social impact is low. So it is ok authors do not address them.","- One issue I find with the paper writing is this paper is not self-contained. In Table 1, a minimal introduction to iCaRL and CCIL is not missing, which makes it difficult to get the idea in section 3.1 before reviewing prior papers. Similarly, L169, minimal introduction to LwF-MC is not included.",900,0
NIPS_2021_28,NIPS_2021,"The paper is overall interesting, well-written and makes a valuable contribution. I do, however, have some comments for the authors to consider (which in my mind, are potential limitations of the study):
- Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well. - For completeness, the authors should also report how the hyperparameters for the linear decoder were determined. Ideally, I would’ve liked to see error bars for decoding accuracies as well (maybe by bootstrapping training set for the decoder?) - In future, the authors could also consider replacing the acc metric for decoding with better evaluation metrics for circular data, like circular correlation. This would treat the reach direction as a continuous variable (which it is) rather than as a discrete unordered variable (which it theoretically isn’t).
- The authors consider swapping only the block of variables belonging to the `content’ group. What would happen if the reconstruction term in the BlockSwap method swapped both the content and style of the augmented views? Does swapping only the content block necessarily facilitate disentanglement? If the BlockSwap was essential, does the proposed method required knowing the number of latent factors in advance. The authors could discuss these aspects in their conclusion/discussion. - When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper. - The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?","- When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper.",901,0
NIPS_2021_947,NIPS_2021,"weakness is the assumption that the adversarial noise is bounded, which seems necessary for the proposed stability property to guarantee a high revenue. However, this assumption seems very strong, and it would be interesting to design variants of stability that are suitable for more realistic noise models.
Minor comments: - Parenthesis on last line of Def. 3.1 would help with clarity.
(*) Huang, Liu, and Wang. Learning Optimal Reserve Price against Non-myopic Bidders, 2018.
——- Update: Thanks to the authors for their detailed responses. Although investigating the stability of a richer class of mechanisms would provide a more comprehensive picture, the author response has convinced me that their results on multi-item auctions are sufficiently important for acceptance. I have thus decided to raised my score.","- Parenthesis on last line of Def. 3.1 would help with clarity. (*) Huang, Liu, and Wang. Learning Optimal Reserve Price against Non-myopic Bidders, 2018. ——- Update: Thanks to the authors for their detailed responses. Although investigating the stability of a richer class of mechanisms would provide a more comprehensive picture, the author response has convinced me that their results on multi-item auctions are sufficiently important for acceptance. I have thus decided to raised my score.",902,1
NIPS_2021_1886,NIPS_2021,"Weakness I think the overall experimental settings and results only show the improved performance on under-represented classes without evaluating the debiased performance between highly correlated classes, which is the main topic addressed in this paper. - Tables 1 and 2 do not contain the information on which classes suffer from the bias. I am concerned that the low class frequency does not necessarily cause the classes to be co-located with other classes. In other words, frequently appearing classes may also suffer from the performance degradation due to the highly correlated object classes. - Similarly, are Train-Truck, Truck-Bus, and Bicycle-Motorcycle highly co-located objects in the Cityscapes dataset? Does Vanilla fail because they have the correlation between the classes or because the frequency of such classes is low?
I would like the authors to 1) specify the classes vulnerable to the bias due to the unwanted correlation with other classes and 2) provide the quantitative comparison between classes with bias and without bias. (e.g., class A is often highly co-located with class B, and the segmentation results of A are xxx worse compared to the class C which does not have the unwanted correlation with other classes.) Additionally, the authors need to demonstrate the improved results of these classes by the proposed DropClass.
Yes. The authors included the limitations and negative societal impact in Section 5.",1) specify the classes vulnerable to the bias due to the unwanted correlation with other classes and,903,1
NIPS_2021_952,NIPS_2021,"- Some important points about the method and the experiments are left unclear (see also questions below). - The writing could be improved (see also Typos & Additional Questions below) - Multiple runs and significance tests are missing. This makes it hard to judge the improvements (Table 2 & 3).
Most Important Questions - Line 156: What is q_ij^k here exactly? I thought q_ij was a state flag, such as “2” or “0”. But you tokenize it and encode it, so it sounds more like it is something like “Copy(snow)”? (If it is the latter, then what is the meaning of tokenizing and encoding something like “Len(9)”?) - 192: What exactly is storyline and what do you need it for? - The baseline takes the predicate logic constraints as input: How does T6 know what to do with these inputs? Was the model trained on this but without the NRETM module? Can you give an example of what the input looks likes? How do these inputs guide which sentences should be generated? Looking at the datsset, it feels like one would need at least the first 2 sentences or so to know how to continue. Maybe this information is now in your constraints but it would be important to understand what they look like and how they were created. Is there no other suitable baseline for this experiment? - What is the overhead of your method compared to standard decoding approaches? (you mention GBS can only be used with T5-Base, so your method is more efficient? That would be important to point out) - What happens if the decoding process cannot find a sequence that satisfies all constraint? - Document-level MT: How do you know at test time whether the system translates a particular sentence or not? - How many sentences are misaligned by Doc-mBART25? What are the s-BLEU and d-BLEU values on the subset that NRETM aligns correctly and Doc does not? - Why was NEUROLOGIC not used as a comparison baseline? - What is dynamic vs static strategy? In which experiment did you show that dynamic works better than static (from conclusion)?
Typos & Additional Questions - Line 40: you could mention here that the examples will be translated into logic forms in the next section. - Paragraph starting at line 53: Why did you choose these datasets? How will they help evaluate the proposed approach? - Line 75: a and b should be bold faced? - 83: “that used” -> “that are used” - 83: “details” -> “for details” - Paragraph at line 86: At this point, the state matrix is unclear. What are the initial values? How can the state matrix be used to understand if a constraint is satisfied or not? - 98: “take[s]” & “generate[s]” - 108: “be all” -> “all be” - Paragraph at line 101: What is dynamic vs static strategy? - Paragraph at line 109: The state flag explanation would greatly benefit from an example. Does q_i refer to whether a particular U_i is satisfied? - Eq 2: What is the meaning of N? Can it change depending on the definition of U_k? Does it mean this constraint is not relevant for x_i? - 133: Figure 1 should be Figure 2 - Figure 2: What exactly do the “&” rows track? - Figure 2: Is the state flag matrix equal to the state matrix? If not, how do you go from one to the other? - Line 146: What does the inf in the superscript signify? - 177: What is the symbolic operator? - Paragraph at line 194: Without understanding what a storyline is, it is not clear what the constraints are. An example might be helpful here. - Line 204: what is the ROUGH-L metric? Do you mean ROUGE-L? - Line 223: How do you obtain the morphological inflections for the concepts? - 237: @necessity [of] integrating” - 3.3: How exactly is the document-level MT done? Is the entire input document the input to T5? - 293: “because” typo - 3.4 where/how exactly is the sentence index used?
The paper's broader impact section discusses general potential benefits and issues of text generation (from large language models). It could maybe be tailored a bit better by discussing what effect this proposed work would have on the potential benefits and issues.","- What is the overhead of your method compared to standard decoding approaches? (you mention GBS can only be used with T5-Base, so your method is more efficient? That would be important to point out) - What happens if the decoding process cannot find a sequence that satisfies all constraint?",904,0
NIPS_2021_1852,NIPS_2021,"W1: The design of extending SGC (from Equation 1) to EIGNN (from Equation 3) is somehow implicit and ad-hoc without clear justifications. The authors should explain this more in details for better understanding by general audiences that not very familiar with implicit models.
W2: During the time complexity analysis, only the complexity of training is analyzed, but it seems like the computation of eigendecomposition of S, the normalized adjacency matrix with self-loops, (Line 176) is not added, which usually requires the cost of O ( n 3 )
. If this is true, a full eigendecomposition of a large sparse S could make EIGNN an impractical approach for prohibiting the scalability in terms of large number of nodes n
for huge real-world graphs.
W3: Several concerns upon experiments include: 1) The discussion on arbitrary hyperparameter γ
is missing, including how to set it in practice for a given graph and analyzing on the sensitivity of this hyperparameter， otherwise it will be hard for the researchers to follow. 2) As the weakness on the analysis of complexity, why the author chooses not to evaluate the long-range dependency on the standard dataset Amazon Co-purchase as used in IGNN. Amazon Co-purchase dataset has another benefit that it can also reflect the scalability of proposed method since it is a large dataset with ~33k nodes, while the experiments on real-world dataset are all conducted on graphs that less than 10k. 3) For the evaluation on over-smoothing, it would be interesting to see how the EIGNN performs with respect to over-smoothing under standard setting on real-world datasets, especially in comparison with variants focusing on dealing with over-smoothing, such as the setting used in GCNII. 4) The evaluation on robustness is not very convincing since structural attack is known to be more powerful and appreciative when we attack on graph-structured data. Thus, the authors are suggested to defend their proposed model against several popular structural attack methods such as Nettack for better demonstration rather than attacks on features used in experiments.","4) The evaluation on robustness is not very convincing since structural attack is known to be more powerful and appreciative when we attack on graph-structured data. Thus, the authors are suggested to defend their proposed model against several popular structural attack methods such as Nettack for better demonstration rather than attacks on features used in experiments.",905,0
NIPS_2021_2291,NIPS_2021,"- As discussed in Line 108, one of the main difference between the proposed YOLOS and DETR is that DETR uses a randomly initialized transformer while YOLOS utilizes pre-trained ViT. However, there are some prior works that discuss the pre-training of DETR, such as UP-DETR [Dai et. al, CVPR 2021]. What is the difference between the proposed method and these DETR pre-training method? Why the proposed method might be better? - There are also some prior methods that discuss and conclude that the decoder is not necessary for transformer-based object detection [Rethinking Transformer-based Set Prediction for Object Detection, arXiv 2020]. What is the different between the proposed YOLOS and these prior encoder-only transformer-based detectors? - Though the authors claim that YOLOS is not designed to be yet another high-performance object detector, the performance is not quite competitive with state-of-the-arts detectors, especially compared with DETR. - I do not quite understand how the visualization in Figure 2 comes to the conclusion of “We observe that each [DET] token learns to specialize on certain regions and sizes.” - I am quite confused with the model scaling part. What does f(Lin.)/f(Att.) actually impact the scaling strategy? Can I conclude from the section “Towards Better Bigger Transformer” that the proposed scaling method is not suitable for YOLOS?
I have mixed evaluation for this manuscript and my initial rating is around borderline. On one hand, I do think that the topic discussed in this paper is quite interesting. But on the other hand, the proposed YOLOS does not present much novel design and the performance is also not quite strong.
The limitations have been discussed in the last section of the paper.","- As discussed in Line 108, one of the main difference between the proposed YOLOS and DETR is that DETR uses a randomly initialized transformer while YOLOS utilizes pre-trained ViT. However, there are some prior works that discuss the pre-training of DETR, such as UP-DETR [Dai et. al, CVPR 2021]. What is the difference between the proposed method and these DETR pre-training method? Why the proposed method might be better?",906,0
NIPS_2021_396,NIPS_2021,"Major points:
1- Confusing description: Authors claim to learn ""inter-relationship among clips"" (line 12) and state that other methods ""consider each clip-text pair separately"" (line 36), ""each clip-text pair is independently encoded"" (line 107). This sounds to me like they would combine several clip-text data points in some way and integrate knowledge between data points. However, the method samples multiple combinations of frames for the same data point, therefore I find these main claims confusing. E.g. ClipBERT simply states in its abstract that ""a single or a few sparsely sampled short clips from a video are used at each training step"" which I find to be a much more fitting description.
2- Effect of pre-training: Authors load pre-trained weights from ClipBERT and then pre-train again on TGIF-QA dataset. They claim that this has advantages (line 335), however, there is no ablation to support this claim: It would be helpful to see the results on MSRVTT-QA and MSVD-QA without pre-training on TGIF-QA dataset.
3- No comparisons to ClipBERT:
ClipBERT results are missing from Table 5 (Performance on TGIF-QA), while they are added in Table 3 (Performance on MSRVTT-QA and MSVD-QA) and no reason is given on why these results are missing. This is important as ClipBERT reports better performance on TGIF-QA than the paper and has a very similar setup (same model, no more than 1 clip during testing, see ClipBERT paper table 8 second-last row.)
The ablations in Table 1 should be additionally done either on TGIF-QA or on MSRVTT-QA and compared to the ClipBERT result described above.
4- Hyper-parameters: Authors use dataset-specific hyperparameters tuning (line 276) which could be another reason for some of the improvements over the ClipBERT method. What was the hyper-parameter tuning approach?
5- Table 1: This table reports the ablation study on the effect of SKG and SKR components. However, there is no result with SKG alone. Also, the authors don't report the mean and std of multiple experiments. Would be great if the authors report mean/std and also results with only SKG.
Given the above concerns, it seems that the main improvement in this paper comes from taking ClipBERT and pretraining it on TGIF-QA before applying it to other datasets. This is contradicting to the story which reasons that the improvements are due to the proposed new modules.
Minor points:
1- Motivation is missing as to why the method is evaluated exclusively on VideoQA and not on other tasks like e.g. Video Classification.
2- There are no details on pre-training and fine-tuning time complexity.
3- Architecture details are missing: ""multimodal transformer is applied to produce the clip-text feature"" (line 133). I would assume the output is a single feature vector obtained from the CLS token, however, this should be specified directly. Otherwise, it would be very hard to reproduce the paper.
Typos: The paper is difficult to follow. The structure of the paper needs to be improved and also there are lots of typos and grammar errors:
78 ""Not"" should be ""Note""
-Figure 3 sentence 1 ""to be extracted"" sentence is missing a proper verb.
129 ""all clips number"" is not a proper term, should be ""number of all clips"".
129: ""Each clip ... is uniformly sampled ... frames"" should be rephrased to ""frames are sampled per clip"" or similar.
136 ""dimension"" should be ""dimensions""
144 ""including"" is confusing here. Does this mean: siamese clips have semantics that are similar ""to"" the anchor clip?
153: ""predictions on their predicted class probabilities"": Duplicate ""predict"" is incorrect
225, 278: ""excavate"" means physically digging (e.g. with a shovel), would be better to replace it with ""use"" here.
364 ""Boarder"" should be ""Broader""
375 ""look"" should be ""looking""","136 ""dimension"" should be ""dimensions"" 144 ""including"" is confusing here. Does this mean: siamese clips have semantics that are similar ""to"" the anchor clip? 153: ""predictions on their predicted class probabilities"": Duplicate ""predict"" is incorrect 225, 278: ""excavate"" means physically digging (e.g. with a shovel), would be better to replace it with ""use"" here.",907,0
NIPS_2021_1343,NIPS_2021,"Weakness - I am not convinced that transformer free of locality-bias is indeed the best option. In fact, due to limited speed of information propagation, the neighborhood agents should naturally have more impacts on each other, compared to far away nodes. I hope the authors to explain more why transformer’s no-locality won’t make a concern here. - Due to the above, I feel graph networks seem to capture this better than the too-free transformer, and their lack of global context/ the “over-squashing” might be mitigated by adding non-local blocks (e.g., check “Non-Local Graph Neural Networks” or several other works proposing “global attention” for GNNs). - The authors also claimed “traditional GNNs” cannot handle direction-feature coupling: that is not true. See a latest work “MagNet: A Neural Network for Directed Graphs” and I am sure there were more prior arts. Authors are asked to consider whether those directional GNNs can possibly suit their task well too. - Transform is introduced as a centralized agent. Its computational overhead can become formidable when the network gets larger. Authors shall discuss how they prepare to address the scalability bottleneck.","- Due to the above, I feel graph networks seem to capture this better than the too-free transformer, and their lack of global context/ the “over-squashing” might be mitigated by adding non-local blocks (e.g., check “Non-Local Graph Neural Networks” or several other works proposing “global attention” for GNNs).",908,0
NIPS_2021_1532,NIPS_2021,"1 There are some related methods missed, such as [1][2][3]. 2 The proposed method employs three strategies to alleviate catastrophic forgetting. These strategies are often employed in other incremental methods. Thus, the contributions are limited in my opinion. 3 Compared with the ground truth, the visualization results are terrible. Thus, I want to see some analysis of satisfied cases and terrible cases. [1] Class-incremental learning for semantic segmentation re-using neither old data nor old labels. ITSC2020. [2] Modeling the background for incremental learning in semantic segmentation. CVPR2020. [3] Incremental Learning Techniques for Semantic Segmentation. ICCVW2019","2 The proposed method employs three strategies to alleviate catastrophic forgetting. These strategies are often employed in other incremental methods. Thus, the contributions are limited in my opinion.",909,0
NIPS_2021_2257,NIPS_2021,"- Missing supervised baselines. Since most experiments are done on datasets of scale ~100k images, it is reasonable to assume that full annotation is available for a dataset at this scale in practice. Even if it isn’t, it’s an informative baseline to show where these self-supervised methods are at comparing to a fully supervised pre-trained network. - The discussion in section 3 is interesting and insightful. The authors compared training datasets such as object-centric versus scene-centric ones, and observed different properties that the model exhibited. One natural question is then what would happen if a model is trained on \emph{combined} datasets. Can the SSL model make use of different kinds of data? - The authors compared two-crop and multi-crop augmentation in section 4, and observed that multi-crop augmentation yielded better performance. One important missing factor is the (possible) computation overhead of multi-crop strategies. My estimation is that it would increase the computation complexity (i.e., slowing the speed) of training. Therefore, one could argue that if we could train the two-crop baseline for a longer period of time it would yield better performance as well. To make the comparison fair, the computation overhead must be discussed. It can also be seen from Figure 7, for the KNN-MoCo, that the extra positive samples are fed into the network \emph{that takes the back-propagated gradients}. It will drastically increase training complexity as the network not only performs forward passing, but also the backward passing as well. - Section 4.2 experiments with AutoAugment as a stronger augmentation strategy. One possible trap is that AutoAugment’s policy is obtained by supervise training on ImageNet. Information leaking is likely.
Questions - In L114 the authors concluded that for linear classification the pretraining dataset should match the target dataset in terms of being object or-scene centric. If this is true, is it a setback for SSL algorithms that strive to learn more generic representations? Then it goes back again to whether by combining two datasets SSL model can learn better representations. - In L157 the authors discussed that for transfer learning potentially only low- and mid-level visual features are useful. My intuition is that low- and mid-level features are rather easy to learn. Then how does it explain the model’s transferability increasing when we scale up pre-training datasets? Or the recent success of CLIPs? Is it possible that \emph{only} MoCo learns low- and mid-level features?
Minor things that don’t play any role in my ratings. - “i.e.” -> “i.e.,”, “e.g.” -> “e.g.,” - In Eq.1, it’s better to write L_{contrastive}(x) = instead of L_{contrastive}. Also, should the equation be normalized by the number of positives? - L241 setup paragraph is overly complicated for an easy-to-explain procedure. L245/246, the use of x+ and x is very confusing. - It’s better to explain that “nearest neighbor mining” in the intro is to mine nearest neighbor in a moving embedding space in the same dataset.
Overall, I like the objective of the paper a lot and I think the paper is trying to answer some important questions in SSL. But I have some reservation to confidently recommend acceptance due to the concerns as written in the “weakness” section, because this is an analysis paper and analysis needs to be rigorous. I’ll be more than happy to increase the score if those concerns are properly addressed in the feedback.
The authors didn't discuss the limitations of the study. I find no potential negative societal impact.",- In L157 the authors discussed that for transfer learning potentially only low- and mid-level visual features are useful. My intuition is that low- and mid-level features are rather easy to learn. Then how does it explain the model’s transferability increasing when we scale up pre-training datasets? Or the recent success of CLIPs? Is it possible that \emph{only} MoCo learns low- and mid-level features? Minor things that don’t play any role in my ratings.,910,0
NIPS_2021_2082,NIPS_2021,"weakness of the existing works. 3. In Table 1 we can see that, besides the CAMELYON16 dataset, the baseline MIL-based methods showed much lower performances than max-pooling. Please give some discussions about the reason. 4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript. 5. For Fig.6, What is the purpose to show the zoom-in view of heatmap? I cannot see anything special in this area. 6. For Fig. 7, the initial accuracy of MIL-based baseline model were higher than the converged models, especially for the NSCLC dataset, why?","4. For ablation study, The Table 2 and Fig. 5 were not mentioned in the manuscript. What do the values stand for in Table 2 and Fig. 5? Why giving the detailed discussion in Appendix? I suggest move the discussion to the main manuscript.",911,0
NIPS_2021_28,NIPS_2021,"The paper is overall interesting, well-written and makes a valuable contribution. I do, however, have some comments for the authors to consider (which in my mind, are potential limitations of the study):
- Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well. - For completeness, the authors should also report how the hyperparameters for the linear decoder were determined. Ideally, I would’ve liked to see error bars for decoding accuracies as well (maybe by bootstrapping training set for the decoder?) - In future, the authors could also consider replacing the acc metric for decoding with better evaluation metrics for circular data, like circular correlation. This would treat the reach direction as a continuous variable (which it is) rather than as a discrete unordered variable (which it theoretically isn’t).
- The authors consider swapping only the block of variables belonging to the `content’ group. What would happen if the reconstruction term in the BlockSwap method swapped both the content and style of the augmented views? Does swapping only the content block necessarily facilitate disentanglement? If the BlockSwap was essential, does the proposed method required knowing the number of latent factors in advance. The authors could discuss these aspects in their conclusion/discussion. - When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper. - The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?",- Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well.,912,0
NIPS_2021_918,NIPS_2021,"weakness Originality
+ The paper presents a novel algorithm. The algorithms applies ideas from bilevel optimization [43, 46, 55, 26, 45] to fairness and class imbalance learning. Unlike previous work [41, 33, 16, 32, 8, 13, 50, 63], which employs fixed class balancing loss functions based on some training data statistics, the proposed method automatically guides the loss function design and learns a more optimal set of parameters for the balanced loss. Quality
+ Claims are well supported through experimental evidence.
+ The authors are careful and honest about reporting the paper's limitations.
+ The paper compares its method to recent state-of-the-art [8,50] and beats them.
- Lack of more baselines. The paper compares performance to only two baselines [8,50], and it is not clear to me why more baselines are not evaluated (e.g. [41, 33, 16, 32, 13, 63]).
- No error bars in the results. There are no error bars in the result tables, making it more challenging to assess the significance of the improvements. Clarity
+ The paper is written very well, and it is easy to follow. On the most part, the paper adequately informs the reader.
- Some clarifications about the experiments are needed. Table 1 does not have any P D A
component in Algo. 1, does this mean that no P D A
was applied in those experiments? (if P D A
was used, did the authors also apply it to the baselines?) Similarly, did experiments in Table 2 apply L A init
? And lastly, does Table 3 show results for ""Algo 1. α ← Δ & l , L A init , P D A ""?
- A possible inconsistency in the results but I may also be misunderstanding something (see above for my confusion). Specifically, if I am correct in understanding, the diagonal results in Table 3 show the performance for ""Algo 1. α ← Δ & l , L A init , P D A
"" which have a higher error rate than both ""Algo 1. α ← P D A , Δ & l
"" in Table 2 and ""Algo 1. α ← Δ & l , L A init
"" in Table 1. This suggests that, in fact, P D A and L A init
degrade performance when applied together. If my understanding is correct, this undermines the second contribution point (lines 73-77) and should be explicitly clarified in the paper. If my understanding is wrong, why are the results in the diagonal in Table 3 different from either the bottom row of Table 1 or Table 2? Significance
+ This work aims to address a timely problem of fairness-seeking optimization, and the paper will be relevant to the community.
Post rebuttal update
I have read other reviewers' comments and the authors' rebuttals, and I am inclined to keep my original score of 7 with increased confidence of 4, condioned on the additional baselines being included in Table 1 in the main paper. The rebuttal addresses the clarity issues, and the additional results show that the method is at least as good as other baselines [28][63][41] in the Cifar-10-LT setting. The performance advantage over [28][63] in the ImageNet-LT dataset is much more obvious. Although results for [41] in the ImageNet-LT setting were not provided, likely the method would also be beaten like in the Cifar-10-LT setting. I think this work is good, addressing a timely problem and it will be relevant for the NeurIPS conference.
The paper adequately and fairly outlines its limitations in a dedicated section.","- Lack of more baselines. The paper compares performance to only two baselines [8,50], and it is not clear to me why more baselines are not evaluated (e.g. [41, 33, 16, 32, 13, 63]).",913,0
NIPS_2021_37,NIPS_2021,", * Typos/Comments)
Overall, I like and value the research topic and motivation of this paper and lean positive. However, some details are not clear enough. I would update my rating depending on the authors' feedback. The details are as follows.
+ Interesting and important research problem. This paper focuses on how to obtain disentangle representations for feature-level augmentation. This topic is interesting and important, and will attract many interests of the NeurIPS community.
+ Good quality of writing and organization. Overall, the writing quality is good and the paper is well organized. It is comfortable to read this paper, although some details are not clear.
+ Comprehensive experiments. Experiments are conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ).
- Relative difficulty score and generalized cross-entropy (GCE) loss. It is not clear how the relative difficulty score W ( x )
in Eq. (1) is used in the pipeline. W(x) is not mentioned again in both the overall objective functions Eq. (2) or Algorithm 1. Since readers may not be familiar with the generalized cross-entropy (GCE) loss, it is encouraged to briefly introduce the formulation and key points of the GCE loss to make this paper more self-contained.
- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.
- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.
- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?
- Figure 1 is not clear. First, it seems that the two y towards L CE
are the outputs of C i
, but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?
- Table 4 reported a much lower performance of ""swapping"" on BAR compared to the other three datasets. Is there any explanation for this, like the difference of datasets?
- Sensitivity to hyperparameters. The proposed framework consists of three important hyperparameters, ( λ dis , λ s w a p b , λ swap )
. It is not clear whether the framework is sensitive to these hyperparameters and how these hyperparameters are determined.
* (Suggestion) Illustration of backpropagation. As introduced in Line 167-168, the loss from C i
is not backpropagated to E b
. It would be clearer if this can be added in Figure 1.
* Line 280. Is ""the first row and column ... respectively"" a typo? It is a little confusing for me to understand this.
* Typos in Algorithm 1. Are λ dis and λ s w a p b
missed in L dis and L swap ?
* Typo in Line 209. Corrputed -> Corrupted.
============================= After rebuttal ===================================
After reading the authors' response to my questions and concerns, I would like to vote for acceptance.
The major strengths of this paper are:
The research problem, unbiased classification via learning debiased representation, is interesting and would attract the NeurIPS audience's attention.
The proposed method is simple but effective. The method is built on top of LfF [12] and further considers (1) intrinsic and bias feature disentanglement and (2) data augmentation by swapping the bias features among training samples.
The paper is clearly written and well organized.
These strengths and contributions are also pointed out by other colleague reviewers.
My main concerns were:
Unclear technical details of the GCE loss and the relative difficulty score. This concern was also shared with Reviewer 8Ai1 and iKKw. The authors' response clearly introduced the details and addressed my concern well.
Sensitivity to hyper-parameters. The authors' response provided adequate results to show the sensitivity to hyper-parameters. Other details of implementation and analysis of experimental results. The authors' responses clearly answered my questions.
Considering both strengths and the weakness, I am happy to accept this paper.
The authors have adequately addressed the limitations and potential negative societal impact of their work.",- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?,914,0
NIPS_2021_1498,NIPS_2021,"The quantitative evaluation on real images is missing. In the paper, only visual examples of different image editing methods are provided, which I think could be biased and subjective given that the proposed method is not visually much better than the baselines (TediGAN and StyleCLIP). I would recommend using some metrics to measure the image editing quality (such as disentanglement, FID and controllability) or doing some human study evaluations.
For the proposed disentanglement model, I do not see much novelty on how to deal with partially labeled attributes (Eq. 3-5), since as far as I know, previous works have already used this similar idea, such as [1].
I am just curious about why Locatello et al. [29] has such low MIG scores on these benchmarks (say, 0.01 on dSprites). Is this really because of permuted-labels configuration or the hyperparameters in the baseline have not been well selected?
It looks like the highest resolution of real images in this paper is only 256x256, which seems to limit the generation quality of the proposed method. I wonder if the method can be scaled up to a larger resolution, such as 1024x1024 in the StyleGANs. Is there any constraint in doing so?
[1] Kingma et al., Semi-Supervised Learning with Deep Generative Models, NeurIPS 2014. =================
Post-rebuttal update:
I thank the authors for taking the time to respond to my reviews. But my major concerns still remain: 1) The qauntitative results of real images actually show the worse disentanglement performance measured by AD (though I agree that it only refects one aspect of disentanglement quality). 2) Although [1] only studies the single-attribute case, Eq. (3) and (4) look exactly the same with the second term of Eq. (7) and the second term of Eq. (9) in [1]. Thus, I still think the novelty of dealing with partially labeled attributes is limited. 3) The visual quality on real images (up to res 256x256) is not high, so I'm a little doubtful that the proposed method can be scaled to more complex high-res image. Therefore, I keep the initial rating unchanged and tend to reject. =================
Post-rebuttal update (2nd round):
I thank the reviewer for the further clarifications, in particular about the difference between this work and the reference [1]. I agree that disentanglement evaluation on real images is more challenging but I think solely reporting AD results (i.e., measuring how the modification affects other attributes) is problematic. I suggest the authors also consider other metrics to measure the ""editing strength"" (i.e., how large the modification changes the considered attribute). One implementation could be that we first train image attribute classifiers on CelebA and use them as oracle models to measure the predicted score differences of generated images before and after editing. Overall, I'm convinced by the authors regarding the novelty and happy to raise my initial score by 1.
The authors have adequately addressed the limitations and potential negative societal impact of their work",1) The qauntitative results of real images actually show the worse disentanglement performance measured by AD (though I agree that it only refects one aspect of disentanglement quality).,915,0
NIPS_2021_2145,NIPS_2021,"The paper mentions some limitations in the results section. 1. Because of the distribution estimation, the algorithm works well on balanced datasets but the performance degrades when the dataset is unbalanced. (shown in Table 1-2); 2. As mentioned in line 266-268, the method has a possible improving direction to benefit from class-conditional alignment techniques.","2. As mentioned in line 266-268, the method has a possible improving direction to benefit from class-conditional alignment techniques.",916,1
NIPS_2021_2074,NIPS_2021,"The authors acknowledge several key limitations, mostly the result of the data that were available to them. I think the authors should discuss several other key limitations: 1.) in the evaluation of the methods, particularly a.) the re-implementation of prior methods, and b.) clinical evaluation of the information retrieval task. It should be straightforward to test whether a clinician believes the information retrieval matches their training (by showing a matched ECG and a further ECG and testing whether they identify the matched ECG). If they do not, these examples can be further explored – either the clinician is wrong, or the model could be improved. 2.) the lack of external validation or transfer to alternate datasets (e.g. training on Chapman and application to PTB-XL - even if only for Age, Sex)
This work has a potential to contribute to important societal impacts, particularly around fair machine learning for health. It may allow researchers to study underrepresented groups that are not clearly defined by binary categorizations of race, sex etc. but will need to be further studied to demonstrate this.","2.) the lack of external validation or transfer to alternate datasets (e.g. training on Chapman and application to PTB-XL - even if only for Age, Sex) This work has a potential to contribute to important societal impacts, particularly around fair machine learning for health. It may allow researchers to study underrepresented groups that are not clearly defined by binary categorizations of race, sex etc. but will need to be further studied to demonstrate this.",917,0
NIPS_2021_2024,NIPS_2021,"below). Using the related literature on active interventions would require full identification of the underlying DAG. It is emphasized that matching only the means can be done with significantly smaller number of interventions, and this is the difference from previous works. - Identifiability in terms of Markov equivalence classes (MEC) is well discussed. Graphical characterization of the proposed shift-interventional (shift-I) MEC, and its refinement over the general interventional MEC is given clearly. Assumptions are reasonable within the given setting. - Extending the decomposition of intervention essential graphs to shift interventional essential graphs is sound. Both of the proposed approaches for solving the problem, clique tree and supermodular strategies are reasonable. Use of a lower bound surrogate function to enable supermodularity is clever. - The paper is organized clearly, and the theoretical claims are well supported.
Weaknesses: I have several concerns on the importance of the proposed settings and usefulness of the results. - Although the causal matching problem seems interesting and new, it is not well motivated. To the reviewer’s knowledge, interventions on a causal model are tied to inferring the underlying structure (it does not need to be the whole structure of the model). In this regard, it is not clear how exactly matching the means of a causal system is preferable to performing more relaxed cases of soft interventions. The authors are encouraged to further explain how this setting can be beneficial. - Deterministic shift interventions are useful to test the applicability of the proposed ideas. However, restricting the problem setting to only shift interventions is quite limited and leads to some rather trivial results. For instance, existence and uniqueness results of matching shift-intervention in Lemma 1, and the properties of source nodes in Lemma 2 are immediate observations in a DAG. - Clique tree approximation is just a minor modification of the cited central node algorithm (Greenewald et al., 2019). - Complexity of the submodularity approach subroutine uses SATURATE algorithm (Krause et al., 2008), and is said to scale with N 5
in appendix D.4. It is worth commenting on the feasibility of this approach. For instance, what are the runtimes of the simulations for large models in Section 6? - It is a nice result that the number of proposed interventions is only a logarithmic factor of the lower bound. However, the baselines in the simulations are not very strong to demonstrate the usefulness. Though coloring approach of Shanmugam et al., 2015 is a related active intervention design, the goal of it is broader than finding a matching intervention. For instance, a simple random upstream search, the other baseline, performs much better than coloring due to the simpler objective. That being said, the reviewer understands that the proposed task is new and fair comparisons may not be easy.
Although this paper has several nice properties, the overall contribution, constraints on the problem, and the importance of the results are not adequate for publication at NeurIPS.
Main limitations of the work, which are also stated in the above review, and potential impact of the work, which is not very imminent, are adequately addressed in the discussion section.","- Identifiability in terms of Markov equivalence classes (MEC) is well discussed. Graphical characterization of the proposed shift-interventional (shift-I) MEC, and its refinement over the general interventional MEC is given clearly. Assumptions are reasonable within the given setting.",918,1
NIPS_2021_1737,NIPS_2021,"1. The paper lacks comparison with other baselines. For example, in [1], the authors proposed to use texture randomization in the augmentations which can also learn texture-invariant features. Also, in [1] the authors reported higher accuracy of MoCo-v2 on ImageNet-100 (81.0%), while the MoCo-v2 accuracy in this paper is less than 78%. 2. The paper proposed to learn less about the texture semantics by adding negative samples that only share the texture sematic with the anchor images. However, the texture semantics can still be helpful in some cases. Similar to [1], it would be good to try to ensemble models trained with the proposed negative augmentations and without it and see whether this could improve the performance. 3. The proposed method is restricted to removing texture semantics and lack of extension to other semantics. One interesting experiment to do is to remove color distortion in the augmentations of positive samples and add negative samples that share the same color distribution of the anchor image to see whether the proposed method could be helpful to avoid the color distribution shortcut.
Clarity: The paper is well written and easy to follow.
Significance: the paper proposes a simple method to construct negative samples to prevent contrastive learning from learning texture information which could affect its generalization ability. However, it is not always desirable to remove the texture semantics, since it is also an important semantics in image classification task. Moreover, the proposed method is restricted to the texture semantics and the authors do not show that it could be extended to other semantics. Therefore, the significance of the paper is limited.","1. The paper lacks comparison with other baselines. For example, in [1], the authors proposed to use texture randomization in the augmentations which can also learn texture-invariant features. Also, in [1] the authors reported higher accuracy of MoCo-v2 on ImageNet-100 (81.0%), while the MoCo-v2 accuracy in this paper is less than 78%.",919,0
NIPS_2021_2168,NIPS_2021,"1.The motivation to investigate a graph structured model is to capture the global dependency structure in the sentence which different from existing sequence models that tend to focus on the dependency between each word and its close preceding words. However,the encoder and decoder is based on Transformer，which can draw global dependencies in sentence. Therefore, I am a bit confused about the complex approach of this article.
2.Table3 does not use the C3D feature to do the experiment, I think this paper should compare the result of C3D features in MSR-VTT with [1].On the MSR-VTT dataset, the performance of the method has not improved much on CIDEr, within 0.3%.
3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.
4.Constructing the generation tree, constructing the ground-truth dependency trees, calculating loss and reward all consume many computing resources. Is the time cost of the training process large?
[1] Zhang at al.Object Relational Graph with Teacher-Recommended Learning for Video Captioning.CVPR'2020 Typos:e.g.The last two rows of table3 are wrongly blackened on the METEOR on dataset MSR-VTT.","2.Table3 does not use the C3D feature to do the experiment, I think this paper should compare the result of C3D features in MSR-VTT with [1].On the MSR-VTT dataset, the performance of the method has not improved much on CIDEr, within 0.3%.",920,0
NIPS_2021_472,NIPS_2021,"Weakness
My main concerns are two folds.
1. The supports for the claims are not sufficient.
1.1 The coexistence of label noise and class imbalance and not explicitly handled in the experiment.
The authors repeatedly claim that one of the important problems in the real-world setting is the coexistence of label noise and class imbalance (L1, L24-25), and their GDW simultaneously mitigates these problems (L6, L300-301). However, these two issues are handled separately in the experiment, which is not good for supporting the above-mentioned claim.
1.2 The effectiveness of the proposed method in class-imbalance case seems marginal
Looking at the standard deviations, the proposed method is not significantly better than the existing methods. Is there any analysis on why the method works fine in label-nose setting whereas shows relatively lower performance in class-imbalance setting?
1.3 Figure 5 and L229-232
The meaning of “target” is not clear as I later point out in more detail. Therefore, it is not clear for me what w n t n
includes. L232 says it represents the importance of the “not bird”, but how about the “dog”? I think decreasing the weight of the “dog” gradient is as important as decreasing the weight of “not cat” in the case of Figure 1.
In addition, it is better to show w t t c
, too, for the completeness.
1.4 Figure 6
How about the ratio trend of the number of increased w 8
on C1-C7, and C8? I think C8 instances should receive lower weights not only in the shown case, but also when C1-C7 instances are used in the training. Similarly, they should receive larger weights when C8 instances are used in the training. How is the same analysis for w 9
on C1-C8 and C9? Showing only one example is not sufficient.
1.5 L248-249: “We have also tried training without the zero-mean constraint in Section 3.3 and got poor results.”
Just “got poor results” is not enough as an evidence of the importance of zero-mean constraint. Please show it quantitatively.
2. Some parts of the paper are not clear enough.
2.1 L131-132 `Specifically, GDW tries to downweight the gradient flows for ""dog"" and ""not cat"", and upweight the gradient flow for ""not bird"".’
I understand what the proposed method aims to do. However I do not think the reason why such a thing is possible is clearly explained. Please explain the intuition behind this.
2.2 Section 3.3 is not clear
L136: What does “retain the Softmax-CrossEntropy loss structure” mean? What is the motivation to do so?
L138: What does “the target (label) position” mean? Especially in noisy label case, does “target” mean a true category or wrongly labeled ground-truth category? For example in Figure 1, what does “target” represent? Without precisely understand what is “target”, I cannot judge the soundness of equation 3 and subsequent discussion. For now, I can only understand equation 3 holds true if w t = w j .
2.3 equation 11
I think it is better to explicitly write down what g i
exactly denotes in the equation. No.
It is recommended to discuss the limitation of the method more deeply in the main body of the paper. For example, the proposed method is tested only in label-noise case and in class-imbalance case separately. I think this is one of the limitation of the paper.
The authors did not discuss any potential negative societal impacts. Therefore, the answer to this question in the checklist should be [No] rather than [N/A]. Any kinds of works can be used in an undesirable manner regardless of the authors’ intention. For example, the method may be used to intentionally increase the bias since it can handle class-level information. It is important to discuss the possible negative aspects (and hopefully how to cope with them).","2. Some parts of the paper are not clear enough. 2.1 L131-132 `Specifically, GDW tries to downweight the gradient flows for ""dog"" and ""not cat"", and upweight the gradient flow for ""not bird"".’ I understand what the proposed method aims to do. However I do not think the reason why such a thing is possible is clearly explained. Please explain the intuition behind this. 2.2 Section 3.3 is not clear L136: What does “retain the Softmax-CrossEntropy loss structure” mean? What is the motivation to do so?",921,0
NIPS_2021_2291,NIPS_2021,"- As discussed in Line 108, one of the main difference between the proposed YOLOS and DETR is that DETR uses a randomly initialized transformer while YOLOS utilizes pre-trained ViT. However, there are some prior works that discuss the pre-training of DETR, such as UP-DETR [Dai et. al, CVPR 2021]. What is the difference between the proposed method and these DETR pre-training method? Why the proposed method might be better? - There are also some prior methods that discuss and conclude that the decoder is not necessary for transformer-based object detection [Rethinking Transformer-based Set Prediction for Object Detection, arXiv 2020]. What is the different between the proposed YOLOS and these prior encoder-only transformer-based detectors? - Though the authors claim that YOLOS is not designed to be yet another high-performance object detector, the performance is not quite competitive with state-of-the-arts detectors, especially compared with DETR. - I do not quite understand how the visualization in Figure 2 comes to the conclusion of “We observe that each [DET] token learns to specialize on certain regions and sizes.” - I am quite confused with the model scaling part. What does f(Lin.)/f(Att.) actually impact the scaling strategy? Can I conclude from the section “Towards Better Bigger Transformer” that the proposed scaling method is not suitable for YOLOS?
I have mixed evaluation for this manuscript and my initial rating is around borderline. On one hand, I do think that the topic discussed in this paper is quite interesting. But on the other hand, the proposed YOLOS does not present much novel design and the performance is also not quite strong.
The limitations have been discussed in the last section of the paper.","- There are also some prior methods that discuss and conclude that the decoder is not necessary for transformer-based object detection [Rethinking Transformer-based Set Prediction for Object Detection, arXiv 2020]. What is the different between the proposed YOLOS and these prior encoder-only transformer-based detectors?",922,0
NIPS_2021_2304,NIPS_2021,"There are four limitations: 1. In this experiment, single dataset training and single dataset testing cannot verify the generalizable ability of models, it should conduct experiments on large-scale datasets. 2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems. 3. I hope to see that you can compare your model with ResNet-IBN / ResNet of FastReID, which is practical work in the person Reid task. 4. I think the authors only use the transformer to achieve the local matching, therefore, the contribution is limited.","4. I think the authors only use the transformer to achieve the local matching, therefore, the contribution is limited.",923,0
NIPS_2022_1034,NIPS_2022,"1.The derivation process and the presentation need to be improved. Some important symbol annotations or explanation is missing during the algorithm description, which make readers hard to follow the derivation process. For example, in the equation (9), some important symbol annotations are missing, e.g. ‘s’, ‘R’. It is difficult for readers to catch up the derivation, and the derivation of p(k) is crucial to the following interpretation. 2.Some pitfalls in the paper: a) in line 100, “are i.i.d samples from.” ; b) in equation (6) one of /psi is written as /phi; c)line 185, the C’ in text are in wrong format.
The paper didn't address the limitation and potential negative societal impact of the work.","2.Some pitfalls in the paper: a) in line 100, “are i.i.d samples from.” ; b) in equation (6) one of /psi is written as /phi; c)line 185, the C’ in text are in wrong format. The paper didn't address the limitation and potential negative societal impact of the work.",924,0
NIPS_2022_2414,NIPS_2022,"- The explanation of the proposed method is poor. The authors have to make a better job at explaining their method. Eq. (3) and the update on p(v*) has to be better explained.
- Section 2.1 is poorly explained. It is not clear the connections with other methods.
- The experimental section is a bit weak. Strengths:
- Nice theoretical results are derived for the proposed method.
The authors have indicated that they have described the limitations of their method. However, they do not indicate where in the manuscript.",- Section 2.1 is poorly explained. It is not clear the connections with other methods.,925,0
NIPS_2022_1350,NIPS_2022,"1.The comparison objects selected in the comparison experiment are very old, and the performance has not been greatly improved.
2.Poor effect in deep network model.
3.Insufficient experiments in image segmentation.
The idea is very good, but I need better experimental results to support it.","1.The comparison objects selected in the comparison experiment are very old, and the performance has not been greatly improved.",926,0
NIPS_2022_2697,NIPS_2022,"Weakness
-Although the proposed sampling method and its corresponding backpropagation process is sound and promising, the paper lacks the experiments to validate its effectiveness:
1.The method is only evaluated on a single scene (Lego)
2.The method is only compared to the base NeRF. There are works on speeding up NeRF without sacrificing performance during both training and rendering (eg. Plenoxel NeRF). How does this method compare those methods? And more importantly, can the method be plugged into those methods for further speed-up? Without the corresponding experiment, those questions can hardly be answered.
3.Key experiment missing for trade-off between speed and reconstruction quality: How is reconstruction quality degrades with the decrease # of samples (ie. speed-up) by using the proposed method?
4.Related to the missing speed-quality trade-off: From Tab.2, the estimation time per iteration increases with the number of points in the splines and exceeds the baseline at around 16 points. So the question is how does the reconstruction quality compare with the baseline at around 16 sampling points. The paper does not include the quality comparison .
5.Comparison to other SOTA method missing: How is the proposed method compared to the SOTA NeRF methods in terms of reconstruction quality and computational and memory consumption
-Paper needs more refinement: There are multiple question marks in the paper due to error in reference.
Experiment is performed on only one scene. So it’s unknown whether this method is finetuned for that scene or more general.
From L131-133, the sampling grid t_0<...<t_m cannot be guaranteed to cover the non-empty space.",3.Key experiment missing for trade-off between speed and reconstruction quality: How is reconstruction quality degrades with the decrease # of samples (ie. speed-up) by using the proposed method?,927,0
NIPS_2022_1035,NIPS_2022,"] 1. The minimum patch size can be one to be added in Table 4, which can be the comparison in the per-pixel setting. 2. The experiment about time cost can be added to verify the superiority of the PAR. 3. This method is to reduce the size of the adversarial noise, but in practical applications, reducing the number of queries is a more important goal that needs to be optimized.",2. The experiment about time cost can be added to verify the superiority of the PAR.,928,0
NIPS_2022_2696,NIPS_2022,"Weakness: 1. In my view, training data and evaluation data should be isolated completely. That is, there should be a certain mount streams and items haven’t happened in the meta-tasks. Therefore, sampling training set and evaluating set from the same pool is not sufficient enough to prove the generalization. 2. The references and baselines are out of date, more researches of recent years should be covered. And it is easy to get a lot of results if you search items like “Item Frequencies of Data Streams”. 3. One advantage of meta-learning is that it could find better params of network faster and more efficiently. So I believe the efficiency is also a good sight of this model and it should be contained in the experiment.","2. The references and baselines are out of date, more researches of recent years should be covered. And it is easy to get a lot of results if you search items like “Item Frequencies of Data Streams”.",929,0
NIPS_2022_2452,NIPS_2022,"Weakness:
The proposed new methods appear to have marginal improvements over current results, although the current best one - Stochastic Mixup & CutMix also supports their claims.
Some of the parts could be written in a better manner, e.g., the explanation in Fig. 4.
This paper claims that Mixup & CutMix are good for different cases. But it only provides the results of their proposed methods when CutMix is the better choice. It would be better to provide the case when Mixup is alternatively the better choice to further support that the proposed methods combine the advantages of both these two methods.
The captions of figures could be more detailed.",4. This paper claims that Mixup & CutMix are good for different cases. But it only provides the results of their proposed methods when CutMix is the better choice. It would be better to provide the case when Mixup is alternatively the better choice to further support that the proposed methods combine the advantages of both these two methods. The captions of figures could be more detailed.,930,0
NIPS_2022_2390,NIPS_2022,"• The strong results come with a major caveat: the ViT-small and Swin-tiny architectures have 22M and 29M parameters respectively, while the compared baselines are almost entirely based on ResNet12, which by my recollection has only 12M parameters. While this is still less than the widely-used WRN-28-10 backbone (36.5M params), I worry that the comparisons presented in the paper are apples-to-oranges. The difference in model size should be discussed and addressed.
• Use of vision transformers for few shot classification deserves an empirical study all on its own. Understandably this is not provided here, but because of this it is unclear to what degree improvement is coming from the token reweighting scheme (in theory compatible with existing convolutional architectures) vs the vision transformer backbone (in theory compatible with existing few-shot classifiers). For example, how does the token reweighting scheme compare to simply training a linear classifier head on the support features from a vision transformer? Admittedly, a full comparison along both these axes would be clearly out of scope here.
• Similarly, there is no ablation study provided for the impact of token reweighting vs the logsumexp aggregation scheme. How much better is logsumexp aggregation than direct addition, for example, which would correspond to basic prototype comparison with reweighted averages on each prototype? More broadly, it appears that the token reweighting scheme is broadly compatible with many existing token-to-token classifiers such as CTX and FRN, and it is not clear how the logsumexp aggregator compares.
• More generally, the approach, while straightforward and sensible, does contain a few design choices that are not fully explained or empirically justified (for example, in addition to above, the choice of token similarity metric).
• A slightly relevant omitted citation: the masked inner token reweighting scheme might possibly owe some conceptual debt to Batch Folding from [Few Shot Learning with Localization in Realistic Settings, CVPR2019], which also models a support-to-support classification task with an identical image-masked leave-one-out scheme (though admittedly implemented quite differently).
The analysis of 1-shot effectiveness and discussion of smaller training datasets is insightful. The entanglement of vision transformer benefits with token reweighting benefits in presented results is not discussed. Societal impacts are not discussed, though do not extend beyond those of few-shot learning in general.","• Use of vision transformers for few shot classification deserves an empirical study all on its own. Understandably this is not provided here, but because of this it is unclear to what degree improvement is coming from the token reweighting scheme (in theory compatible with existing convolutional architectures) vs the vision transformer backbone (in theory compatible with existing few-shot classifiers). For example, how does the token reweighting scheme compare to simply training a linear classifier head on the support features from a vision transformer? Admittedly, a full comparison along both these axes would be clearly out of scope here.",931,0
NIPS_2022_2617,NIPS_2022,"The essentialness of using orthogonal matrix is not studied. The whole OSA process 1. connects tokens within local windows with local window token orthogonalization, is serves as MLP layer within local windows, except the weight matrix of MLP is naturally orthogonal. 2. Connects tokens beyond local windows by forming new groups across previous local window. 3. Token reverse as the inverse of orthogonal matrix is easy to get, just the transpose of the matrix. Step 2 can be done regardless of the weight matrix of this local window MLP is orthogonal or not. Step 3 is the vital part that only orthogonal matrix weight can perform, I believe this should be studied, which is not presented, for validating the essentialness of using orthogonal matrix rather than just following the form that connects local and connects beyond local windows.",2. Connects tokens beyond local windows by forming new groups across previous local window.,932,1
NIPS_2022_2049,NIPS_2022,"Technical originality: The technical originality of the paper is marginal. The proposed method is an extension to damping belief propagation. Leveraging attentions to weight messages has been widely explored in different tasks [a].
[a] Zhang, Li, et al. ""Dynamic graph message passing networks."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
Quality: The quality of the paper can be improved in two aspects: 1) there are missing related works; 2) the proposed method is overall lack of theoretical justifications.
Significance: The proposed DABP outperforms SOTA baselines only marginally while the computing time is significantly longer than SOTA baselines. In Table 1, on small-work networks with |X|=100, DABP achieves cost 25.65 which is slightly better than DBP-SCFG (cost = 26.13). However, DABP takes 7m31s while DBP-SCFG only takes 2m10s. The proposed training objective (Eq. 14) seems to be of high computational complexity.
I detail my comments on the weaknesses in the Question section below.
The authors provide discussions on possible limitations of the proposed method.","2) the proposed method is overall lack of theoretical justifications. Significance: The proposed DABP outperforms SOTA baselines only marginally while the computing time is significantly longer than SOTA baselines. In Table 1, on small-work networks with |X|=100, DABP achieves cost 25.65 which is slightly better than DBP-SCFG (cost = 26.13). However, DABP takes 7m31s while DBP-SCFG only takes 2m10s. The proposed training objective (Eq. 14) seems to be of high computational complexity. I detail my comments on the weaknesses in the Question section below. The authors provide discussions on possible limitations of the proposed method.",933,0
NIPS_2022_1479,NIPS_2022,"1.The framework is based on the assumption that (as in L192) Tu will have higher entropy than Tk, which might be too strong to hold.
2.Domain adversarial learning is known to be unstable during optimization and lacks interpretability.
The limitations stated in the supplementary materials, i.e., suppL281 – 289, are the limitations for the general DA problem, but not specifically for their own methods. It is the current paradigm of DA that cannot tackle streamlined data gathering and needs sufficient source domain categories, not just for this paper. More concrete limitations of the proposed methods in this paper are recommended included.","1.The framework is based on the assumption that (as in L192) Tu will have higher entropy than Tk, which might be too strong to hold.",934,0
NIPS_2022_1830,NIPS_2022,"- For me the main weakness of the paper is the partial information bound. While the generality is nice, it also does not seem to give the optimal rate in the standard bandit setting ($B=1$) and the best I can get is a $O(T^{2/3})$ bound, where I hide dependency on $N$. This also makes me believe that the approach does not give satisfactory results in the cases where $B>1$, although I do not have a point of reference for these cases.
- The authors show a lower bound only for the full feedback setting and as the authors acknowledge this result is weak. It would have been nice to have a stronger one, to better understand the nature of the problem.
- The authors provide a regret bound for \textit{OG} which uses \textit{Follow the Top Perturbed Leaders} as a subroutine in the full feedback setting, but they do not for the partial feedback setting. They do give a hint for the adaptation to the partial feedback setting and test its performance in the experiments, but it would have been a more complete work if they also provided the bound.
Minor comments
- In lines 156 to 165 I have trouble understanding the role of $S_t$ versus $S_t^*$. I think $S_t$ should be $S_t^*$, or can they be used interchangeably?
- Typo for the first character at line 363.
- The acronym for follow the top perturbed leader is a bit unfortunate as in literature FTL usually means Follow the Leader. Perhaps a change of acronym is in order.
- In line 160, it seems that the second strict inequality should be an equality
The authors have adequately addressed the limitations and potential negative societal impact of their work.",- The acronym for follow the top perturbed leader is a bit unfortunate as in literature FTL usually means Follow the Leader. Perhaps a change of acronym is in order.,935,0
NIPS_2022_1994,NIPS_2022,"Please note that below are only for questions and potential discussions. There is no need to rerun experiments during the rebuttal phase.
1. About experiments
It seems that, in some cases, improvements compared with C-learning are minor (Fig. 5). Can the authors give some explanation. Meanwhile, why not compare with other contrastive learning approaches in RL mentioned in the early sections?
2. About other common challenges in goal-conditioned RL
Can this approach tackle common challenges in goal-conditioned RL, especially for off-line datasets? e.g., the generalization ability to different distributions.
3. Minor typos
-> Line 107: π ( τ ∣ S t ) -> π ( τ ∣ S g ) ;
-> Eq. 4: consider using another color, the current v +
is hard to recognize when print out the paper to read.
The authors mentioned the limitations in the paper, mainly about how to generalize the work into other RL settings. I think this would be the potential direction for future works. I raise a few questions, which are more like vague points instead of limitations. I think this is a decent paper and I would vote for acceptance.","1. About experiments It seems that, in some cases, improvements compared with C-learning are minor (Fig. 5). Can the authors give some explanation. Meanwhile, why not compare with other contrastive learning approaches in RL mentioned in the early sections?",936,0
NIPS_2022_934,NIPS_2022,"] 1.Besides objects (or semantic parts), the background is an important and large part contained in images. This paper does not discuss how to deal with the background. If we regard the semantic parts as the visual analog of words, what is the background for? How should we treat it?
2.I am wondering why this paper chooses the masking training strategy to validate its idea (i.e., semantic parts of objects are the visual analogue of words). Are there any other tasks related to this idea? In addition, what is the relationship between the idea and the masking strategy (i.e., masking parts from intra-part patterns to inter-part)? These questions were not well explained?
Yes, the authors adequately discussed the limitations of this paper. Meanwhile, potential negative societal impacts are not found yet.","2.I am wondering why this paper chooses the masking training strategy to validate its idea (i.e., semantic parts of objects are the visual analogue of words). Are there any other tasks related to this idea? In addition, what is the relationship between the idea and the masking strategy (i.e., masking parts from intra-part patterns to inter-part)? These questions were not well explained? Yes, the authors adequately discussed the limitations of this paper. Meanwhile, potential negative societal impacts are not found yet.",937,0
NIPS_2022_1592,NIPS_2022,"1) It states the connections with the existing methods, while it is better to validate the connections by experiments. 2) It lacks comparisons, making it hard to judge its advancement. 3) It is better to provide the statistic for the relationship between the Lagrange multiplier and the perceptual quality. 4) The original performance of R in Table 1 is positive infinity, and the following values are not informative. Equation 6 can be modified by adding a regular term to the denominator to make the original performance a positive real number. 5) SSIM is a higher the better metric, and it should be stated in the text whether (1-SSIM) or -SSIM is used as the D","4) The original performance of R in Table 1 is positive infinity, and the following values are not informative. Equation 6 can be modified by adding a regular term to the denominator to make the original performance a positive real number.",938,0
NIPS_2022_1430,NIPS_2022,"Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x )
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, ""is is useful think ...""
Many issues in Table1: 4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?","4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean?",939,0
NIPS_2022_2367,NIPS_2022,"1) Since this method needs a forward-backward training process, does it require more time to train the network? How many extra parameters are introduced in the newly proposed method compared with the previously proposed method VolMinNet[13]? 2) This paper states that it tries to estimate the transition matrix under the sufficiently scattered assumption, what’s the difference between this assumption and the previous anchor point assumption?","2) This paper states that it tries to estimate the transition matrix under the sufficiently scattered assumption, what’s the difference between this assumption and the previous anchor point assumption?",940,0
NIPS_2022_2398,NIPS_2022,"Details are missing, especially about the baseline and it weakens the compelling results. 1) KPT is a method to expand the verbalizers, what verbalizers did you get? I don't understand why the results would be worse than LM-BFF as it seems to be LM-BFF + additional verbalizers? 2) For LM-BFF, how did you get the scores with demonstrations in a zero-shot setting?
Some design choices are not well justified or ablated: 1) Why specifically use FocalLoss? How does it compare to a linear combination of the original cross-entropy loss and a KNN loss similar to KNN-LM and it would be closer to the retrieval setup during test time? 2) Why does neural demonstration happen at the embedding layer?
Yes, the authors discuss about efficiency overhead of the proposed method in appendix.",1) Why specifically use FocalLoss? How does it compare to a linear combination of the original cross-entropy loss and a KNN loss similar to KNN-LM and it would be closer to the retrieval setup during test time?,941,0
NIPS_2022_1716,NIPS_2022,"I don’t see any potential negative societal impact in this work.
[1] Van de Panne, Michiel, Ryan Kim, and Eugene Fiume. ""Virtual wind-up toys for animation."" In Graphics Interface, pp. 208-208. CANADIAN INFORMATION PROCESSING SOCIETY, 1994. [2] Holden, Daniel, Taku Komura, and Jun Saito. ""Phase-functioned neural networks for character control."" ACM Transactions on Graphics (TOG) 36, no. 4 (2017): 1-13. [3] Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. ""Random synaptic feedback weights support error backpropagation for deep learning."" Nature communications 7, no. 1 (2016): 1-10. [4] Nøkland, Arild. ""Direct feedback alignment provides learning in deep neural networks."" Advances in neural information processing systems 29 (2016). [5] Frenkel, Charlotte, Martin Lefebvre, and David Bol. ""Learning without feedback: Fixed random learning signals allow for feedforward training of deep neural networks."" Frontiers in neuroscience 15 (2021): 629892.","1 (2016): 1-10. [4] Nøkland, Arild. ""Direct feedback alignment provides learning in deep neural networks."" Advances in neural information processing systems 29 (2016). [5] Frenkel, Charlotte, Martin Lefebvre, and David Bol. ""Learning without feedback: Fixed random learning signals allow for feedforward training of deep neural networks."" Frontiers in neuroscience 15 (2021): 629892.",942,1
NIPS_2022_2411,NIPS_2022,"Weakness:
It is great that the generated 3D textured meshes can be directly used in the production tools like Blender. I wonder how the texture is represented in the final output as I don't think these tools can utilize texture fields? Is it easy to get the UV map from the proposed model? Or does ""texture"" only mean ""colored vertices""?
Based on Table 1, [46] actually is closer to what the authors proposed in this paper (without the texture part though). Why [46] is excluded from the comparison?
What is the illumination model in the differentiable rendering part? If I understand correctly, it is a Phone model with only 1.0 ambient lighting.
Though in L255, the authors claimed that ""hence the subdivision cannot provide further improvements"", in Table 2, subdiv actually hurts the performance seriously. Any idea?
Shape interpolation is impressive. It would be great to see the texture interpolation as well (though the title of Fig 6 is shape interpolation, the texture has also changed...why...).
The authors listed two major limitations in their work: 1. the evaluation was done on the synthetic data only and 2. per category training. The authors also discussed the potential social impact of their work applying on privacy or biased data. All these sound sufficient to me.",1. the evaluation was done on the synthetic data only and 2. per category training. The authors also discussed the potential social impact of their work applying on privacy or biased data. All these sound sufficient to me.,943,1
NIPS_2022_2686,NIPS_2022,"Weakness:
1.The technical contribution is somewhat weak in my view. The authors leverage the discretized error types to incorporate the entire confusion matrix, but many steps of the proofs mainly follow existing results.
2.For the minor problems, there are some flaws in the paper, e.g., incorrect citations in L13 in the paper.","2.For the minor problems, there are some flaws in the paper, e.g., incorrect citations in L13 in the paper.",944,0
NIPS_2022_1200,NIPS_2022,"Weakness: Originality:
1.I want to know if this paper is the first time to study the problem of the robust collaborative inference, where there are both arbitrary agents and adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.
From Eq.(5), the framework aims to find a combined feature l
which is on the manifold and is near h
. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it? Writtings:
1.After so many times of reading, I guess I understand this paper. The authors introduce their method in Section 2.3, which is very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.
2.The notations are confusing. For example, h and l
both denote the feature. Why not use a letter (or with its variants)?
the citation format may be ICLR rather NeurIPS.
Theoretical analysis:
1.This paper provides an extensive theoretical analysis. In fact, I suggest the authors discuss more what the analysis means. Compared with baselines, why CoPur could do better.
2.Could the authors give an intuitive explanation about the effect of the sparsity α
on CoPur? Experiments:
1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c
and different Ω a d v ?
2.More analysis is helpful, for example, The comparison on optimization efficiency.
The authors discuss the limitations in Appendix. I have no other suggestions.","2.More analysis is helpful, for example, The comparison on optimization efficiency. The authors discuss the limitations in Appendix. I have no other suggestions.",945,1
NIPS_2022_1890,NIPS_2022,"I do not understand how MESA is only 15% more expensive than standard training. If I am understanding correctly, MESA requires a forward pass through both the EMA weights and the standard weights. Doesn't this require two forward passes where there was previously only one?
It seems like free should not be in the name of SAF because there is an additional memory overhead (minor)
I did not understand very well how SAF and MESA relate to SAM. In particular, Equation 8 is not well motivated and I did not see where the KL came from.
I have a few concerns with the ResNet baselines: 1) The ResNet baseline for SAM is below that reported in the SAM paper -- could this perhaps be because perturbations are synced? 2) Modern ResNets, e.g. with techniques from the timm library which is the library used in this paper can get to 80% accuracy (https://arxiv.org/pdf/2110.00476.pdf). Are the methods presented in this paper additive with these modern approaches?
The paper claims to address this in Section 3.3 but a revision could benefit from an explicit limitations section (minor).","2) Modern ResNets, e.g. with techniques from the timm library which is the library used in this paper can get to 80% accuracy (https://arxiv.org/pdf/2110.00476.pdf). Are the methods presented in this paper additive with these modern approaches? The paper claims to address this in Section 3.3 but a revision could benefit from an explicit limitations section (minor).",946,0
NIPS_2022_1948,NIPS_2022,"""Shaping representations"" and ""leverage from the learned representations"" is what loss-based approaches (IsoMax, Scaled Cosine, SNGP, DUQ, and IsoMax+) have been doing since 2019. The fact that these loss-based approaches for OOD detection were not even cited may explain why the paper understand ""shaping representations"" and ""leverage from the learned representations"" as a significant novelty. Unfortunately, it is clearly not the case.
Unfortunately, the paper entirely ignores the last three years of relevant advances in loss-based approaches that are completely reshaping the area after the Mahalanobis inference-based approach faded out. The paper insistently compares against Mahalanabos, a four years old approach that has been outperformed very easily in the last three years.
1. Making representation compatible with the OOD Detection procedure is at least three years old.
The Mahalanobis paper is from 2018. At that time, inference-based metric learning approaches for OOD were trending. Since 2019, we have observed that loss-based methods are now mainstream. Therefore, training the model using a loss to make training representation and test/inference distribution compatible, which is the central claim of the paper's novelty, is not novel at all.
Unfortunately, the paper lacks to realize that training the model to produce train and inference representations compatible with performing OOD has been the mainstream approach since 2019 and was proposed by IsoMax, Scaled Cosine, DUQ, SNGP, and more recently by DisMax. Kapa appears similar to essentially the entropic scale of IsoMax.
2. Forcing representation to the unit hypersphere is not novel.
Forcing representation to the unit hypersphere during training to improve OOD detection was done in Scaled Cosine and IsoMax+, and more recently by DisMax.
3. Very limited novelty.
Considering that point #1 above showed that leveraging representations/distributions learned during training to perform OOD detection has already been proposed. Moreover, noticing that point #2 above showed that unit hypersphere representations have also already been proposed, we conclude that the novelty of the proposed approach is very limited.
4. Neither cited nor compared with similar previous approaches.
Considering that the paper essentially proposes a loss to train the network to improve OOD detection, we believe that the paper should have mentioned and compared with (at least) IsoMax, Scaled Cosine, DUQ, IsoMax+, and SNGP. Considering that this did not happen, we do not know if using vMF helps in any way.
5. Results for CIFAR10 do not look impressive. Apparently, the results for CIFAR10 are not remarkable compared to the approaches mentioned above (a direct comparison is absolutely mandatory). We would also like to see results for CIFAR100.
6. Results for CIFAR10 need to show classification accuracy. We also would like to see classification accuracy results to evaluate a possible classification accuracy drop, which is very usual in loss-based approaches.
7. We need much more experiment results using CIFAR10. Furthermore, we need CIFAR100 results as well. CIFAR10 and CIFAR100 still are the de facto standard datasets on which major OOD detection approaches have published results. The paper requires much more experimentation using CIFAR10 and CIFAR100 datasets. Classification accuracy need also to be shown and analyzed, as classification accuracy drop is a significant issue when using loss-based approaches for OOD detection.
8. Unlike recent approaches, the proposed solutions do not allow regular pure end-to-end backpropagation-based training.
9. The proposed approach presents much more hyperparameters than current state-of-the-art approaches.
10. Results should show mean and standard deviations.
IsoMax: https://arxiv.org/abs/1908.05569
IsoMax (journal): https://arxiv.org/abs/2006.04005
Scaled Cosine: https://arxiv.org/abs/1905.10628
DUQ: https://arxiv.org/abs/2003.02037
SNGP: https://arxiv.org/abs/2006.10108
IsoMax+: https://arxiv.org/abs/2105.14399
DisMax: https://arxiv.org/abs/2205.05874
After rebuttal: I will keep my score because the differences between the proposed model and the cited training-based approaches are not significant. No direct comparison against the methods we presented was shown. Finally, the majority of the concerns we presented were not covered in the rebuttal. Neither citing nor comparing against the previous related work for the last four years is hard to accept.
The paper does not properly comment on the approach's limitations. Unlike many other cited approaches, it does not allow pure end-to-end backpropagation training. It also should better explain the hyperparameter validation procedures required for the alpha, beta, and the dimension of the last layer.","6. Results for CIFAR10 need to show classification accuracy. We also would like to see classification accuracy results to evaluate a possible classification accuracy drop, which is very usual in loss-based approaches.",947,0
NIPS_2022_787,NIPS_2022,"1. The input of the mentioned segmentation requires the object located at the center. I think this requirement is too hard to fulfill in the real world, and the proposed method didn’t consider this. 2. Although the paper is focusing on point clouds completion, statistics and comparison of completion performance are missing in the paper.
The authors have not addressed the limitations or potential negative societal impact. My suggestions are listed in the weaknesses parts.","2. Although the paper is focusing on point clouds completion, statistics and comparison of completion performance are missing in the paper. The authors have not addressed the limitations or potential negative societal impact. My suggestions are listed in the weaknesses parts.",948,0
NIPS_2022_2480,NIPS_2022,Weakness: - The proposed approach may introduce new parameters to the network. - The proposed Semantic Difference Convolution (SDC) is similar to Central Difference Convolution (CDC). - The effectiveness of the semantic difference term needs more clarification.,- The effectiveness of the semantic difference term needs more clarification.,949,0
NIPS_2022_31,NIPS_2022,"Some of the weaknesses of the paper are as follows:
• Maybe I'm picky but I think the idea is not too novel as it feels like it just fixes a particular issue of an existing BO approach. I do appreciate all the analysis conducted to show the issue of the approach in Muller et al [13] and all the derivations to find the most likely descent direction and to compute the new acquisition function. However, I normally expect the key idea to be more novel, and if it's not too novel, I expect the experiment results to be very impressive. In this case, the experiment results are not really that impressive. The proposed approach does have some very good performance on some objective functions; however, it also performs on par with other baselines in multiple objective functions, and it performs worse for some objective functions.
• The proposed approach is developed for the batch setting; however, only sequential setting (batch 1) is conducted in the experiments.
• The paper seems to lack of the discussion about some benchmarks on Bayesian optimization with gradient, for example the work Bayesian Optimization with Gradients by Wu et al (NeurIPS 2017).
• I expect more sensitivity analysis and discussions on some hyperparameters of the proposed approach such as p^*.
The paper has some discussions on the societal impact of their work.","• The proposed approach is developed for the batch setting; however, only sequential setting (batch 1) is conducted in the experiments.",950,0
NIPS_2022_2410,NIPS_2022,"1. The method section is a bit messy and becomes hard to follow. 2. Notations in 3. are definitely over complex, and could be simplified, e.g. by dropping ‘i’ in Eq. 1-6. 3. The method is not entirely free from additional resources contrary to what the authors claim, since it requires an additional model (momentum encoder- local + global) to be trained and stored, as well as the queue for the training. 4. Some typos and minor editing issues. 5. The conclusions on the m hyperparameter are unclear. 6. Both figures 1 are non-informative and confuse the reader. 7. Lack of clear motivation why CVML among other cross-modal tasks and evaluation only on one of them seems to be quite limited.
The authors very briefly mention the limitations of their work, with no discussion of potentially negative impact. In my opinion, the limitations of this work are two-fold. First, as the authors mention, they only tackle the CVML task, however, to fully address this task, the results of state-of-the-art approaches on particular datasets should also be included, showing that they indeed struggle with catastrophic forgetting. Otherwise, it would be beneficial to address other cross-modal tasks.","3. The method is not entirely free from additional resources contrary to what the authors claim, since it requires an additional model (momentum encoder- local + global) to be trained and stored, as well as the queue for the training.",951,0
NIPS_2022_767,NIPS_2022,"[Update]: Weaknesses have been addressed in the rebuttal and I've increased the score from 6 to 7.
(medium): Table 1 + 2 only compare Single with linear + nonlinear activation function to the MoE models and shows that the Single model isn't able to learn the problem at hand to the same extend as the MoE variants. Since effectively MoE models have more parameters to play with (albeit them being in parallel branches), I believe adding total + effective parameter counts and adding one more line where we extend the Single model to the total number of parameters in the MoE model would be beneficial. This would verify that MoE's aren't just finding winning lottery tickets in their parameter initialization which allows them to perform better.
(small): The paper does consider other non-linear activation functions in the Appendix but here the setting is different than in the main paper. I would like to see Table 6 for the settings in Table 1, i.e. with α ∈ ( 0.5 , 2 ) , β ∈ ( 1 , 2 ) , γ ∈ ( 0.5 , 3 ) , σ n = 1 and α ∈ ( 0.5 , 2 ) , β ∈ ( 1 , 2 ) , γ ∈ ( 0.5 , 3 ) , σ n = 2 .
(large): Comparing the results of CIFAR-10 and CIFAR-10-Rotate, I find it surprising that MoE's don't seem to benefit CIFAR-10 although it should also have intrinsic clusters (similar to CIFAR-10-Rotate) which different experts should be able to utilize. In a paper that tries to understand MoE's and states that the cluster structure is important, I was hoping for a better explanation (or at least a hypothesis) in l.340 -- l.345 that would help to assess when MoE's are beneficial and what properties to look for in the dataset. In its current state, I do not know how I would transfer some of the findings in the paper to other real-world datasets e.g. multilingual machine translation. Maybe adding additional datasets from other domains or other computer vision datasets would help to solve this problem.
(large): Continuing from the previous point, a nice analysis would be to visualise the latent embeddings on CIFAR-10 & CIFAR-10-Rotate similar to Figure 1 using t-SNE and see if we can distill any inherent characteristics about the clustering behaviour. My assumption would be that in CIFAR-10-Rotate the inter-cluster separation/distance is a lot higher than in regular CIFAR-10 which enables better expert learning. Thinking this further, maybe we could add additional regularization during the the training of MOE's on CIFAR-10 to enforce better separation and simplify the training for the different experts.
Minor Improvement Suggestions:
Related literature that seems relevant enough to cite:
""BASE Layers: Simplifying Training of Large, Sparse Models"" (Lewis et al., ICML 2021)
""Hash Layers For Large Sparse Models"" (Roller et al., NeurIPS 2021)
""Tricks for Training Sparse Translation Models"" (Dua et al., arXiv 2021)
Typos & Formatting:
Table 1 can be made more easy to read and parse by using the L A T E X
booktabs package with proper spacing. Similarly, Table 2--6 would benefit from it as well.
The definition the empirical loss after l. 175 & dispatch Entropy after l. 316 should have an Equation number
l. 121 Radamacher -> Rademacher
l. 165 refer to the experiments with other activation functions in Table 6 in the appendix where you consider more commonly used choices such as ReLU / GeLU .
l. 193 cite Adam paper
Generally refer more to the Appendix in the main paper. It is really hard to follow the reading flow without it.
Apart from the mentioned points, the paper is well written and presentation, structure, and general layout of thoughts are up to NeurIPS standard. Related work is mentioned and cited accordingly apart from the previously mentioned papers.
Limitations are appropriately addressed in the conclusion and future extensions are given.",165 refer to the experiments with other activation functions in Table 6 in the appendix where you consider more commonly used choices such as ReLU / GeLU . l.,952,1
NIPS_2022_1889,NIPS_2022,"Weakness
Several metrics are proposed to evaluate the realism and diversity of the generated images, e.g., precision and recall. Can the authors provide more intuitions why these metrics are not used in measuring the diversity in conditional generative modeling?
Kynkäänniemi et al., ""Improved Precision and Recall Metric for Assessing Generative Models.""
Recent data augmentations can significantly improve the performance of GAN-based approaches, e.g., ADA, especially in the limited training data case. Can the authors comment on 1) how the proposed IMLE-based performs against the GAN-based methods trained with ADA and 2) how the proposed algorithm performs in the limited training data case?
Karras et al., ""Training Generative Adversarial Networks with Limited Data.""","2) how the proposed algorithm performs in the limited training data case? Karras et al., ""Training Generative Adversarial Networks with Limited Data.""",953,0
NIPS_2022_857,NIPS_2022,"In multimodal temporal contrastive loss, is there an assumption that the clip content and sentence will not repeat in the video? Otherwise, the line 50-51 will not make sense.
Line 157-158, what is the sampling strategy here for K and V. If these two sets are far away from each other, samples in the positive pair may not match each other.
The effectiveness of each component is not well analysed. There are some ablation studies in the Table 5. However, it is not clear to see the effectiveness of L_time alone, L_mlm, L_vtm, \lambda_1 and \lambda_2. It will be great to show how each part contribute to the final performance.
Missing details in the comparison: Table 1-3 demonstrate the performance of this paper and previous works. In addition to the pretraining dataset, these details are also important: a). Computational cost. 2) Running Time. 3) Additional dataset (used in pretraining and pretrained model). 4) Input resolution. Combined with the weakness No.3, it is hard to distinguish which part of this paper is the key for the superior performance.",2) Running Time.3) Additional dataset (used in pretraining and pretrained model).,954,1
NIPS_2022_2403,NIPS_2022,"The authors suggest in the abstract and introduction of the paper that one of the main benefits of the proposed method is to reduce the complexity of the inference. However, there is no evidence to support this point. Detailed comparisons of runtime performance such as FLOPs and inference time should be reported.
The box detection pipeline can be replaced by a DETR-like architecture using object queries to further reduce the complexity of the model. I wonder if the authors have considered this design. If so, what’s the performance and why the current FPN-like architecture is preferred?
It is interesting to see in the supplementary that the proposed method outperforms two-stage models on CrowdPose by a quite notable margin. I’d like to see a more detailed justification/discussion on the results.
Some comments to improve the clarity of the paper: 1. In Figure 2, it is unclear to see how the results from the previous stage are used as the input of the current stage. 2. The results on CrowdPose are important and should be put into the main paper.",2. The results on CrowdPose are important and should be put into the main paper.,955,1
NIPS_2022_1830,NIPS_2022,"- For me the main weakness of the paper is the partial information bound. While the generality is nice, it also does not seem to give the optimal rate in the standard bandit setting ($B=1$) and the best I can get is a $O(T^{2/3})$ bound, where I hide dependency on $N$. This also makes me believe that the approach does not give satisfactory results in the cases where $B>1$, although I do not have a point of reference for these cases.
- The authors show a lower bound only for the full feedback setting and as the authors acknowledge this result is weak. It would have been nice to have a stronger one, to better understand the nature of the problem.
- The authors provide a regret bound for \textit{OG} which uses \textit{Follow the Top Perturbed Leaders} as a subroutine in the full feedback setting, but they do not for the partial feedback setting. They do give a hint for the adaptation to the partial feedback setting and test its performance in the experiments, but it would have been a more complete work if they also provided the bound.
Minor comments
- In lines 156 to 165 I have trouble understanding the role of $S_t$ versus $S_t^*$. I think $S_t$ should be $S_t^*$, or can they be used interchangeably?
- Typo for the first character at line 363.
- The acronym for follow the top perturbed leader is a bit unfortunate as in literature FTL usually means Follow the Leader. Perhaps a change of acronym is in order.
- In line 160, it seems that the second strict inequality should be an equality
The authors have adequately addressed the limitations and potential negative societal impact of their work.","- In line 160, it seems that the second strict inequality should be an equality The authors have adequately addressed the limitations and potential negative societal impact of their work.",956,1
NIPS_2022_721,NIPS_2022,"In general, to me, the experiments are a bit weak/limited. Please find the detailed comments as follows.
• Experiments are only conducted to compare the proposed Batch BORE method with existing baselines, and very limited experiments are conducted to compare the performance of BORE++ over baselines (e.g., BORE or GP-UCB).
• For the experiments that compare the proposed Batch BORE method with existing baselines, only synthetic objective functions are used, there are no real-world problems.","• For the experiments that compare the proposed Batch BORE method with existing baselines, only synthetic objective functions are used, there are no real-world problems.",957,0
NIPS_2022_2530,NIPS_2022,"The assumptions of the lemma and theorems are not specified clearly. Do they apply to all kinds of convolutional layers?
It seems that Theorem 2 requires n ≡ 0 ( mod s )
, which means the spatial size n
has to be divisible by the stride s
. Does it mean the stride equals the kernel size (i.e., s = k
)? Many convolutional layers may not satisfy such a requirement.
The experiments of computational complexity are too limited. A detailed comparison (in terms of, for example, running time) between the proposed method and prior methods should be added.
The correctness of the singular values (SVs) using the proposed method should be validated empirically. For example, the SVs computed by the proposed method can be compared to those from [1]. Note that the method from [1] may only apply to convolutional layers with a stride equaling 1. For other strides, an alternative comparison is [2] by using the inequalities of matrix norms: $r|A|\alpha\leq|A|\beta\leq s|A|\alpha , i . e . , b y e m p i r i c a l l y s h o w i n g t h a t t h e l a r g e s t s i n g u l a r v a l u e f r o m t h e p r o p o s e d m e t h o d c a n b e b o u n d e d b y t h e \ell_1 n o r m o r
\ell\infty$ norm from [2].
[1] Sedghi H, Gupta V, Long PM. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408. 2018 May 26.
[2] Liang Y, Huang D. Large norms of CNN layers do not hurt adversarial robustness. InProceedings of the AAAI Conference on Artificial Intelligence 2021 May 18 (Vol. 35, No. 10, pp. 8565-8573).
The limitations are not sufficiently discussed. For example, the assumptions of the lemma and theorems are not specified clearly.","2018 May 26. [2] Liang Y, Huang D. Large norms of CNN layers do not hurt adversarial robustness. InProceedings of the AAAI Conference on Artificial Intelligence 2021 May 18 (Vol. 35, No. 10, pp. 8565-8573). The limitations are not sufficiently discussed. For example, the assumptions of the lemma and theorems are not specified clearly.",958,0
NIPS_2022_522,NIPS_2022,"Weakness: 1. The settings of the theoretical analysis are far from the practice. This paper uses the forward propagation rule x_{t+1}=x_{t} +1/N f(x_t,\theta_t), where N is the depth of neural networks. However, in the latter analysis (e.g. Proposition 1,2), it is assumed that \theta_t has bounded norm (or f has bounded Lipschitz parameter with respect to theta_t). Therefore, if we absorb the 1/N factor into f (or more directly into \theta_t, as in neural networks, f is usually a piece-wise linear function of \theta_t), which is what we do in practice, the initialization scale of \theta can be rather small and decreases with the depth, while in practice the initialization scale usually depends on the width. Theorem 1 requires the parameters between adjacent layers to be close, which is also unrealistic. 2. The experiment results are not promising even compared to the current state of the arts. For example, in table 2, the test accuracy on Imagenet for Resnet using the adjoint method is not high even compared to the vanilla Resnet [1]. Also, the performance of the adjoint method in Figure 4 is worse than the traditional backpropagation.
[1]. https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=3","2. The experiment results are not promising even compared to the current state of the arts. For example, in table 2, the test accuracy on Imagenet for Resnet using the adjoint method is not high even compared to the vanilla Resnet [1]. Also, the performance of the adjoint method in Figure 4 is worse than the traditional backpropagation. [1]. https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=3",959,0
NIPS_2022_1770,NIPS_2022,"Weakness: There are still several concerns with the finding that the perplexity is highly correlated with the number of decoder parameters.
According to Figure 4, the correlation decreases as top-10% architectures are chosen instead of top-100%, which indicates that the training-free proxy is less accurate for parameter-heavy decoders.
The range of sampled architectures should also affect the correlation. For instance, once the sampled architectures are of similar sizes, it could be more challenging to differentiate their perplexity and thus the correlation can be lower.
Detailed Comments:
Some questions regarding Figure 4: 1) there is a drop of correlation after a short period of training, which goes up with more training iterations; 2) the title ""Top-x%"" should be further explained;
Though the proposed approach yields the Pareto frontier of perplexity, latency and memory, is there any systematic way to choose a single architecture given the target perplexity?","2) the title ""Top-x%"" should be further explained; Though the proposed approach yields the Pareto frontier of perplexity, latency and memory, is there any systematic way to choose a single architecture given the target perplexity?",960,0
NIPS_2022_864,NIPS_2022,"major issues:
The whole approach consists in using Reinforcement Learning (RL), and more specifically Q-learning (and its neural network variants). Howerver RL requires to work on a Markov Decision Process (MDP), the Markov property must hold : P(s_2 | s_1, a_1) = P( s_2 | s_1, a_1, s_0, a_0). However it seems that the object defined here is not a proper MDP. Indeed, since the state s belongs to {0, 1} ^n_k we only have access to the activations of the layer k : no information about the previous activations, not mentioning the input x itself. Here, the state s has no knowledge of how ""deep"" we are in the equation being built. It feels weird : how a neural network can take the right actions if it does not ""see"" the whole input ? Moreover, in MDP, the reward depends of the current state s and action a, or of the current state s and future state s' (depending of the literature). This is not fulfilled by the construction given by the authors in either case.
If the problem is not a MDP, the Q function has no reason to exist, and fixed point based algorithms like value iteration (and variants such as Deep Q learning) are not guaranteed to converge. Even if the optimal Q function exists, it has no reason to be convex in general. This is not a property to be enforced with ICNN, it is a property of the MDP itself, that must be defined as such (Markov property must be fufilled !).
As said by authors in literature review, the problem of SR is NP hard. Hence, an attempt to convexify the problem that would yield polynomial time algorithm would fail. So either the problem cannot be cheaply convexified, either it induces a bias that prevents to find the global optimum of the original problem.
Even if it was a MDP, there are other issues. For example, the complexity of value iteration is O(S² * A) with A and S the action and space states respectively. For S defined by the authors, there is only 2^n_s states and 2^(n_s * n_s) actions. The total complexity of value iteration would be O( 2^(n_s * n_s)). How does it compare to Deep Q learning in sample efficiency ? For dataset (1) the pool of activations is small, for (2) and (3) the pool of activations is only a singleton (!) the number of variables in input is also very small for (1) : only 3, while for (2) it is 5 nodes and for (3) it is 10 nodes. On those toy examples, the search space is so small that value iteration seems sufficient, no need for Deep Q learning
Theorem 1 is false. Since it is not a MDP, Q function is not defined. There is no reward in this MDP, only a convex surrogate function R() paramatrized by ICNN, that looks like the reward R received at each time step. But in reality, this R is not a Markvovian reward (that depends of state/action pair), it is a surrogate that depends of the final return of the trajectory.
Forcing the surrogate R and the network Q to be convex using ICNN induces a bias, but can not be used to prove anything about the underlying MDP.
l192 : ""Then, we 192 prove that the convexity of the negative optimal Q-function eventually yields the exact equations."" Well, then, why is there a non zero error in experiments Fig. 3 ? Such error is not negligible.
minor issues:
i) It it not clear what neural networks bring here. In general their power in combinatorics is their generalization capabilities - i.e their capacity to interpolate the value of unseen inputs. Paper lack details about this precise contribution of NN.
ii) Theorem 4 is very ad-hoc due to its complicated form. i,j and k variables are not defined. Even the appendix does not contain a clear statement o the theroem, only the proof. Hence, it is hard to understand what the objets clearly are.
l 202 : what ""exact equations"" mean ? In general there might be no unicity of the equations, so that true equation and the one returned by the network might be different.
iii) theorem 2 is assumes that the optimal W* is recovered (why ? How ?) and that the sequence of actions is selected according to the optimal policy Q*, assuming it exists, such that the optimum of Q* corresponds to exact equations (using the fact that the dataset is noiseless). Authors conclude the exacts equations can be recovered. This is essentially trivial : authors are concluding that under the hypothesis that the optimum exists and has been found, it has beend found...
iv) theorem 3: it seems that authors are saying that in the neighborhood of the minimum, if for one of the examples there is no plateau (assumption 2), the second orer derivative (assuming it exists, assumption 1) is positive. For unconstrained minimization of twice differentiable function, this is a well known property: https://math.stackexchange.com/a/19473 (second paragraph)
With my current understanding of the paper, the whole theoretical framework used in the paper is invalid, which seriously impedes its impact. Moreover the experiments seem to be very small, specially when there is only linear {x} symbol pools.","1) is positive. For unconstrained minimization of twice differentiable function, this is a well known property: https://math.stackexchange.com/a/19473 (second paragraph) With my current understanding of the paper, the whole theoretical framework used in the paper is invalid, which seriously impedes its impact. Moreover the experiments seem to be very small, specially when there is only linear {x} symbol pools.",961,0
NIPS_2022_948,NIPS_2022,"The main theoretical flaw is that the analysis of NEOLITHIC relies on a restrictive Assumption 5 (bounded dissimilarity): 1 n ∑ i = 1 n ‖ ∇ f i ( x ) − ∇ f ( x ) ‖ 2 ≤ b 2 , ∀ x ∈ R d
One can easily come up with an example for which this assumption does not generally hold. For example, let us consider f i ( x ) = x ⊤ A i x
, where A i ∈ R d × d
. Since ∇ f i ( x ) = B i x
, where B i = A i + A i ⊤
. The bounded dissimilarity assumption (Assumption 5), which can be written in the form 1 n ∑ i = 1 n | ( B i − 1 n ∑ j = 1 n B j ) x | 2 ≤ b 2
, also does not hold, unless B i = B j
for all i , j
, which reduces to the identical data regime, which is of limited interest.
Some details on the experimental setting are missing. See Question 1) in the next section.
Literature review ignores several papers that are seemed to be relevant [1], [2]. It seems VR-MARINA for online problems from [1] and DASHA-MVR from [2] both satisfy Assumption 2 and have a better rate than QSGD in the stochastic regime. See Question 2) in the next section.
Table 1 contains possible typos:
It mentions a paper on MEM-SGD that does not have a non-convex rate. Their rate is applicable to strongly convex functions. I would recommend here to mention another, more relevant work [3]. See Question 3) in the next section.
Similar problems with CSER, Double Squeeze, and QSGD. See corresponding Questions 4), 5) and 6) in the next section. References:
[1] Gorbunov, Eduard, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. 2021. “MARINA: Faster Non-Convex Distributed Learning with Compression.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.07845.
[2] Tyurin, Alexander, and Peter Richtárik. 2022. “DASHA: Distributed Nonconvex Optimization with Communication Compression, Optimal Oracle Complexity, and No Client Synchronization.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2202.01268.
[3] Koloskova, A., Lin, T., Stich, S. U., & Jaggi, M.. Decentralized deep learning with arbitrary communication compression. arXiv preprint arXiv:1907.09356, 2019
I would recommend making the y-axis in the right subfigure of Figure 1 logarithmic scale. Otherwise, it is hard to distinguish plots corresponding to different methods.
One more possibly relevant and missing citation is [7]. The Algorithm 3PCv3 (Appendix C.6, page 26) already employs a similar nested structure as proposed in Paper8019 by FCC.
I would recommend running least squares and logistic regression experiments for a longer period. It looks like the methods on the left subfigure of Figure 1 and Figure 2 were stopped quite early and did not reach the SGD-specific oscillation region.
Some minor notes:
(line 685): instead of Cauchy-Schwarz, one needs to refer to Young's inequality for product;
(line 693): instead of Cauchy-Schwarz, one needs to refer to Jensen's inequality
FINAL REMARKS:
I would be happy to rate this paper an 8 for its solid theoretical contributions and reliable experiments.
However, at this moment, I can not do so since the paper still contains several crucial issues that are needed to be clarified or fixed.
I am ready to reconsider my current rate during rebuttals once you respond to me on Weaknesses 2-4 and Questions 1 - 6.
UPDATE: After the Authors-Reviewers discussion, I decided to increase the score since my concerns were resolved.
References: [7] Richtárik, Peter, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li, and Eduard Gorbunov. 2022. “3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2202.00998.","3) in the next section. Similar problems with CSER, Double Squeeze, and QSGD. See corresponding Questions",962,0
NIPS_2022_2453,NIPS_2022,"Weakness: 1. Lack of discussion with previous multi-branched network structure. Since the current paper focus on the design of a multi-branched block structure, a detailed comparison and discussion should be included, like ResNeXt[1], GoogleNet Family and Inception Family. Although they did not include self-attention in their branches, some of the networks do have the mixed kernel size (1x1 Vs 5x5), which is also another form of low/high frequency mixer. Also, the multi-branched network seems to have better generalization and optimization properties [2] compared with the single-branched counter parts. A simple citation is insufficient to highlight your improvement over them. In addition, as the paper is largely driven by the low/high pass filter design in signal processing, I would encourage the author incudes some reference on this topic. 2. Problems on ablation study. To be scientific, ablation study means quantifying the influence of each modular design by removal/changing of one component from the entire system. But in table 5 row 1-4, the authors are accurately adding one component at each time. I know that a lot of paper take this style of ablation study, but scientifically, this is not a valid ablation study. Better fix this.
[1] Aggregated Residual Transformations for Deep Neural Networks (CVPR 2017) [2] Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex (ICML 2019)","2. Problems on ablation study. To be scientific, ablation study means quantifying the influence of each modular design by removal/changing of one component from the entire system. But in table 5 row 1-4, the authors are accurately adding one component at each time. I know that a lot of paper take this style of ablation study, but scientifically, this is not a valid ablation study. Better fix this. [1] Aggregated Residual Transformations for Deep Neural Networks (CVPR 2017) [2] Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less Non-Convex (ICML 2019)",963,0
NIPS_2022_2269,NIPS_2022,"Weakness: 1) The notations are a bit confusing, like Z, F and their subscripts. 2) Sparse variational inference might further improve the inference efficiency here.","1) The notations are a bit confusing, like Z, F and their subscripts.",964,0
NIPS_2022_2681,NIPS_2022,"1.The novelty and technical contributions of this work are quite limited. It simply assembles several existing algorithms, such as CogView2 [6] and Swin Transformer [14]. 2.The performance of the proposed CogVideo is not as strong as the authors claimed. As shown in Tab. 1, the metrics of CogVideo (IS and FVD) fall behind previous methods. It is not acceptable to only compare with publicly available models. 3.The authors trained the CogVideo on a large dataset of 5.4 million captioned videos, but did not give any detailed information regarding this dataset. What are the sources of the videos and captions? How are they collected? Will the dataset be released? 4.The authors claimed that they greatly enhance parallelism and accelerate inference, but no results about inference speed of the CogVideo model are provided. 5.In Sec. 5.1, the authors ""fine-tune CogVideo on the whole dataset (UCF-101) for 10,000 iterations"". Is it appropriate in this field?
Yes, the authors adequately addressed the limitations and potential negative societal impact of the work.","1.The novelty and technical contributions of this work are quite limited. It simply assembles several existing algorithms, such as CogView2 [6] and Swin Transformer [14].",965,0
NIPS_2022_1948,NIPS_2022,"""Shaping representations"" and ""leverage from the learned representations"" is what loss-based approaches (IsoMax, Scaled Cosine, SNGP, DUQ, and IsoMax+) have been doing since 2019. The fact that these loss-based approaches for OOD detection were not even cited may explain why the paper understand ""shaping representations"" and ""leverage from the learned representations"" as a significant novelty. Unfortunately, it is clearly not the case.
Unfortunately, the paper entirely ignores the last three years of relevant advances in loss-based approaches that are completely reshaping the area after the Mahalanobis inference-based approach faded out. The paper insistently compares against Mahalanabos, a four years old approach that has been outperformed very easily in the last three years.
1. Making representation compatible with the OOD Detection procedure is at least three years old.
The Mahalanobis paper is from 2018. At that time, inference-based metric learning approaches for OOD were trending. Since 2019, we have observed that loss-based methods are now mainstream. Therefore, training the model using a loss to make training representation and test/inference distribution compatible, which is the central claim of the paper's novelty, is not novel at all.
Unfortunately, the paper lacks to realize that training the model to produce train and inference representations compatible with performing OOD has been the mainstream approach since 2019 and was proposed by IsoMax, Scaled Cosine, DUQ, SNGP, and more recently by DisMax. Kapa appears similar to essentially the entropic scale of IsoMax.
2. Forcing representation to the unit hypersphere is not novel.
Forcing representation to the unit hypersphere during training to improve OOD detection was done in Scaled Cosine and IsoMax+, and more recently by DisMax.
3. Very limited novelty.
Considering that point #1 above showed that leveraging representations/distributions learned during training to perform OOD detection has already been proposed. Moreover, noticing that point #2 above showed that unit hypersphere representations have also already been proposed, we conclude that the novelty of the proposed approach is very limited.
4. Neither cited nor compared with similar previous approaches.
Considering that the paper essentially proposes a loss to train the network to improve OOD detection, we believe that the paper should have mentioned and compared with (at least) IsoMax, Scaled Cosine, DUQ, IsoMax+, and SNGP. Considering that this did not happen, we do not know if using vMF helps in any way.
5. Results for CIFAR10 do not look impressive. Apparently, the results for CIFAR10 are not remarkable compared to the approaches mentioned above (a direct comparison is absolutely mandatory). We would also like to see results for CIFAR100.
6. Results for CIFAR10 need to show classification accuracy. We also would like to see classification accuracy results to evaluate a possible classification accuracy drop, which is very usual in loss-based approaches.
7. We need much more experiment results using CIFAR10. Furthermore, we need CIFAR100 results as well. CIFAR10 and CIFAR100 still are the de facto standard datasets on which major OOD detection approaches have published results. The paper requires much more experimentation using CIFAR10 and CIFAR100 datasets. Classification accuracy need also to be shown and analyzed, as classification accuracy drop is a significant issue when using loss-based approaches for OOD detection.
8. Unlike recent approaches, the proposed solutions do not allow regular pure end-to-end backpropagation-based training.
9. The proposed approach presents much more hyperparameters than current state-of-the-art approaches.
10. Results should show mean and standard deviations.
IsoMax: https://arxiv.org/abs/1908.05569
IsoMax (journal): https://arxiv.org/abs/2006.04005
Scaled Cosine: https://arxiv.org/abs/1905.10628
DUQ: https://arxiv.org/abs/2003.02037
SNGP: https://arxiv.org/abs/2006.10108
IsoMax+: https://arxiv.org/abs/2105.14399
DisMax: https://arxiv.org/abs/2205.05874
After rebuttal: I will keep my score because the differences between the proposed model and the cited training-based approaches are not significant. No direct comparison against the methods we presented was shown. Finally, the majority of the concerns we presented were not covered in the rebuttal. Neither citing nor comparing against the previous related work for the last four years is hard to accept.
The paper does not properly comment on the approach's limitations. Unlike many other cited approaches, it does not allow pure end-to-end backpropagation training. It also should better explain the hyperparameter validation procedures required for the alpha, beta, and the dimension of the last layer.","2. Forcing representation to the unit hypersphere is not novel. Forcing representation to the unit hypersphere during training to improve OOD detection was done in Scaled Cosine and IsoMax+, and more recently by DisMax.",966,0
NIPS_2022_1078,NIPS_2022,"while this is a great step for the ""deep thinking"" models, I don't see a lot of comparison to/discussion of other (if any) architectures that could be applied to these tasks. E.g., why not compare to a universal transformer? In this sense the work is not well contextualized.
comments, including presentation comments and typos:
(general) this work uses 'recurrence', but in a different way from RNNs: RNNs recur alongside an input sequence, eating one token at a time and finishing at the end of the input, this architecture simply recurs on its own state (albeit with help of the input recall) for a given number of steps, independently of the input. Given the popularity of RNNs, for clarity, I suggest this distinction is clarified early in the paper, e.g. a sentence of the sort ""this work uses recurrence in a manner different from that commonly referred to in RNN literature, the architecture behaviors should not be confused""
line 8 ""because behavior degenerates"" I don't know if 'behavior degenerates' so much as iterations continue past correct stop (losing solution), or representation degenerates (also losing solution), or some other issue. Rephrase (unless you prove that indeed ""behavior degenerates"" and is the ""cause"" of the failure to scale).
Fig 1 caption description of 59x59 as ""center"" is confusing, consider just labeling the subfigures
line 20 ""lack the ability to solve complex reasoning tasks in a scalable, algorithmic, way"" again strong concrete statement, not sure I agree, soften/rephrase.
Eq 2 (description of f_recall, r_recall) - bad notation: r_recall takes two inputs, one of which is its own previous output, and the other the original input to the net. By construction such a function cannot be applied recurrently (r^2_recall(phi,x) unrolls to r_recall(r_recall(phi,x)), which makes no sense because the x has to be put in again). Consider instead notation similar to that used in RNN descriptions, e.g.: phi_t+1=r_recall(phi_t,x), phi_1=r(x), f_recall(x,m)=h(phi_m) (something like that - fill it out right according to your architecture. Please also make sure to properly distinguish between the original input x and the processed input P(x), so we can tell which one is being used where!).
line 163: ""re-initialize the network"" - the entire network or just the recurrent part? clarify
line 164: clarify here that gradient is being discarded, easier to follow that way
line 169-170: (too) strong statements! 1) the network may arbitrarily implement counting its iterations, it is not ""prevented"" from doing so, and 2) the network might still learn iteration specific behaviors, whether by luck or even directly through the SGD (given that the standard loss is also still used!). The only claim I would make here is that encouraging random iterations to be correct a) encourages a stabilizing behavior (i.e. not losing the solution once obtained) and b) encourages 'faster' (shallower) solutions (i.e. having the solution ready in the earliest iteration possible) - basically, it pushes the network to have the correct solution available in the maximum number of iterations possible.
Fig 3: would also be nice to see the stability of DT-recall (i.e. no prog loss) after reaching the solution, e.g. plot accuracy up to 1000 iterations (given that prog loss is supposed to encourage stability, and here we only just see DT-recall reach the solution but not whether it stabilizes there)
line 254-255: [s/of sampling/of randomly sampling], [s/We modify/To show this, we modify], [s/slighty to always/slightly, to always]
line 259 s/random/randomly
line 265 ""harder"" - harder than what?
line 293-294: explicitly mention that the best models had progressive loss (currently missing from this sentence)","1) the network may arbitrarily implement counting its iterations, it is not ""prevented"" from doing so, and",967,1
NIPS_2022_1323,NIPS_2022,"incremental contribution; GPU friendly attention appears quite related to previous work [13], as well as to Linformer and Nystromformer (see below).
hybrid convolutional-transformer models have been proposed before (eg DPT hybrid, see below)
the proposed improvements perform only slightly better than baselines in Fig.3
missing configuration in Fig.3a: EA + CA
large training footprint: it appears that only 3 crops 512x1024 can fit into a V100
incomplete related work in the field of efficient models for semantic segmentation (eg HardNet, SwiftNet, see below)
Missing related work:
René Ranftl, Alexey Bochkovskiy, Vladlen Koltun. Vision Transformers for Dense Prediction. ICCV 2021: 12159-12168
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh. Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention. AAAI 2021.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. Linformer: Self-Attention with Linear Complexity. CoRR abs/2006.04768 (2020).
Marin Orsic, Sinisa Segvic. Efficient semantic segmentation with pyramidal fusion. Pattern Recognit. 110: 107611 (2021).
Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin. HarDNet: A Low Memory Traffic Network. ICCV 2019
It appears that large memory footprint precludes training on single GPU systems.","107611 (2021). Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin. HarDNet: A Low Memory Traffic Network. ICCV 2019 It appears that large memory footprint precludes training on single GPU systems.",968,0
NIPS_2022_1948,NIPS_2022,"""Shaping representations"" and ""leverage from the learned representations"" is what loss-based approaches (IsoMax, Scaled Cosine, SNGP, DUQ, and IsoMax+) have been doing since 2019. The fact that these loss-based approaches for OOD detection were not even cited may explain why the paper understand ""shaping representations"" and ""leverage from the learned representations"" as a significant novelty. Unfortunately, it is clearly not the case.
Unfortunately, the paper entirely ignores the last three years of relevant advances in loss-based approaches that are completely reshaping the area after the Mahalanobis inference-based approach faded out. The paper insistently compares against Mahalanabos, a four years old approach that has been outperformed very easily in the last three years.
1. Making representation compatible with the OOD Detection procedure is at least three years old.
The Mahalanobis paper is from 2018. At that time, inference-based metric learning approaches for OOD were trending. Since 2019, we have observed that loss-based methods are now mainstream. Therefore, training the model using a loss to make training representation and test/inference distribution compatible, which is the central claim of the paper's novelty, is not novel at all.
Unfortunately, the paper lacks to realize that training the model to produce train and inference representations compatible with performing OOD has been the mainstream approach since 2019 and was proposed by IsoMax, Scaled Cosine, DUQ, SNGP, and more recently by DisMax. Kapa appears similar to essentially the entropic scale of IsoMax.
2. Forcing representation to the unit hypersphere is not novel.
Forcing representation to the unit hypersphere during training to improve OOD detection was done in Scaled Cosine and IsoMax+, and more recently by DisMax.
3. Very limited novelty.
Considering that point #1 above showed that leveraging representations/distributions learned during training to perform OOD detection has already been proposed. Moreover, noticing that point #2 above showed that unit hypersphere representations have also already been proposed, we conclude that the novelty of the proposed approach is very limited.
4. Neither cited nor compared with similar previous approaches.
Considering that the paper essentially proposes a loss to train the network to improve OOD detection, we believe that the paper should have mentioned and compared with (at least) IsoMax, Scaled Cosine, DUQ, IsoMax+, and SNGP. Considering that this did not happen, we do not know if using vMF helps in any way.
5. Results for CIFAR10 do not look impressive. Apparently, the results for CIFAR10 are not remarkable compared to the approaches mentioned above (a direct comparison is absolutely mandatory). We would also like to see results for CIFAR100.
6. Results for CIFAR10 need to show classification accuracy. We also would like to see classification accuracy results to evaluate a possible classification accuracy drop, which is very usual in loss-based approaches.
7. We need much more experiment results using CIFAR10. Furthermore, we need CIFAR100 results as well. CIFAR10 and CIFAR100 still are the de facto standard datasets on which major OOD detection approaches have published results. The paper requires much more experimentation using CIFAR10 and CIFAR100 datasets. Classification accuracy need also to be shown and analyzed, as classification accuracy drop is a significant issue when using loss-based approaches for OOD detection.
8. Unlike recent approaches, the proposed solutions do not allow regular pure end-to-end backpropagation-based training.
9. The proposed approach presents much more hyperparameters than current state-of-the-art approaches.
10. Results should show mean and standard deviations.
IsoMax: https://arxiv.org/abs/1908.05569
IsoMax (journal): https://arxiv.org/abs/2006.04005
Scaled Cosine: https://arxiv.org/abs/1905.10628
DUQ: https://arxiv.org/abs/2003.02037
SNGP: https://arxiv.org/abs/2006.10108
IsoMax+: https://arxiv.org/abs/2105.14399
DisMax: https://arxiv.org/abs/2205.05874
After rebuttal: I will keep my score because the differences between the proposed model and the cited training-based approaches are not significant. No direct comparison against the methods we presented was shown. Finally, the majority of the concerns we presented were not covered in the rebuttal. Neither citing nor comparing against the previous related work for the last four years is hard to accept.
The paper does not properly comment on the approach's limitations. Unlike many other cited approaches, it does not allow pure end-to-end backpropagation training. It also should better explain the hyperparameter validation procedures required for the alpha, beta, and the dimension of the last layer.","7. We need much more experiment results using CIFAR10. Furthermore, we need CIFAR100 results as well. CIFAR10 and CIFAR100 still are the de facto standard datasets on which major OOD detection approaches have published results. The paper requires much more experimentation using CIFAR10 and CIFAR100 datasets. Classification accuracy need also to be shown and analyzed, as classification accuracy drop is a significant issue when using loss-based approaches for OOD detection.",969,0
NIPS_2022_1622,NIPS_2022,"I am not sure if the empirical comparison is fair, as the different methods seem to have different runtime requirements; in particular, you claim that the inference runtime of FED scales as 1 + (# of epsilon samples) / 35, which for 120 samples (i.e. the number you use in the experiments) is roughly 4.5; in my understanding, this means that the inference time of FED is 4.5x slower than that of a single deterministic model (while in contrast, the vanilla ensemble is 120x slower; I requested clarification for this in the Questions below, so please let me know if my understanding is incorrect); if this assumption is correct, this would mean that FED is significantly slower than some of the baselines considered; e.g., in my understanding, EnDD should have a runtime similar to that of a single model (again, please correct me if I'm mistaken); all in all, my concern is that the performance comparison should be conducted more carefully, taking inference runtime into account, to make it more meaningful; to this end, one could e.g. plot the results in 2D with runtime on the x-axis and performance on the y-axis; different methods would then result in different trade-offs on this plot, yielding Pareto curves that would be much more insightful than just single performance numbers; at the end of the day, a practitioner is probably interested in one of two questions: 1) given a fixed runtime budget X, what’s the best performance Y that I can achieve?, or 2) given a desired performance Y, what’s the fastest runtime X to achieve it? to make the comparison even more interesting, it would be great to consider different settings for each individual method for trading off between runtime and performance; I'm not fully sure how to best do this, but e.g. for FED, one could try varying the number of epsilon samples (for a fixed number of ensemble members) to make the method cheaper; conversely, one could try to make the baselines more expensive in more way; this would allow you to compare performance of the methods for a given fixed inference budget.
The empirical evaluation could be significantly improved by considering a more diverse set of model architectures and benchmarks; the paper focuses on a single architecture (ResNet18) and three small-to-medium-sized image classification benchmarks (CIFAR-10, CIFAR-100, STL-10); it is not clear if the drawn conclusions would extend to other model families (e.g. transformers), data modalities (e.g. text) and benchmark sizes (e.g. ImageNet); to gain more space for reporting additional major empirical results, one could e.g. move Section 5.4 (on the OOB methods for inducing diversity without an auxiliary dataset) to the appendix -- while the experiment and conclusion in that section are certainly interesting, they do not seem to necessary to have in the main text of the paper due to the comparably poor results.
The paper emphasises that, in contrast to previous methods (which are e.g. based on fitting a Dirichlet distribution over predictions), FED is able to capture correlations between outputs, and claims that this is an important feature for many real-world applications (e.g. l. 9-11, l. 52-55, l.149-153); as this is mentioned already prominently in the abstract as a main selling point for the method (l. 9-11), I was expecting at least some empirical evidence for the benefit of this feature; unfortunately, no explicit evidence is provided beyond some vague intuitive written arguments (l. 52-55, l. 149-153). It would significantly strengthen the paper if capturing these covariances could be shown to improve performance in some application. (While I understand that FED is shown to improve upon previous methods, it is not clear to me how this gain can be directly attributed to the aforementioned property.)
Minor Issues
Table 3: bolding the best values would improve clarity
It would be useful to put a brief take-away message into the caption of each table/figure
The paper misses the Laplace approximation as another fundamental Bayesian inference approach for neural networks (in addition to VI and MCMC); for a recent overview, see e.g. Daxberger et al., Laplace Redux -- Effortless Bayesian Deep Learning, NeurIPS 2021
In addition to the runtime comparison requested above, it would also be useful to have a more clear/complete comparison of the memory requirements of all methods; you mention the memory burden of FED in Appendix C.2, and e.g. that of the Hydra baseline in Appendix A; it would be useful to have a table/plot comparing this for all methods considered","1) given a fixed runtime budget X, what’s the best performance Y that I can achieve?, or",970,1
NIPS_2022_2286,NIPS_2022,"Weakness 1. It is hard to understand what the axes are for Figure 1. 2. It is unclear what the major contributions of the paper are. Analyzing previous work does not constitute as a contribution. 3. It is unclear how the proposed method enables better results. For instance, Table 1 reports similar accuracies for this work compared to the previous ones. 4. The authors talk about advantages over the previous work in terms of efficiency however the paper does not report any metric that shows it is more efficient to train with this proposed method. 5. Does the proposed method converge faster compared to previous algorithms? 6. How does the proposed methods compare against surrogate gradient techniques? 7. The paper does not discuss how the datasets are converted to spike domain.
There are no potential negative societal impacts. One major limitation of this work is applicability to neuromorphic hardware and how will the work shown on GPU translate to neuromorphic cores.","3. It is unclear how the proposed method enables better results. For instance, Table 1 reports similar accuracies for this work compared to the previous ones.",971,0
NIPS_2022_1667,NIPS_2022,"1. The invariant learning module in the proposed RA2 is dependent on the previous environmental-based invariant learning methods. The connection between the two sub-modules seems weak. Could the whole framework be trained in end-to-end optimization? 2. The mask-based feature selection may be weak to more complicated data input, such as high-dimensional images.
The authors provide the discussion about negative societal impact. I suggest the authors make a further discussion about more powerful feature learning technologies with the proposed RA2 framework.",1. The invariant learning module in the proposed RA2 is dependent on the previous environmental-based invariant learning methods. The connection between the two sub-modules seems weak. Could the whole framework be trained in end-to-end optimization?,972,0
NIPS_2022_1994,NIPS_2022,"Please note that below are only for questions and potential discussions. There is no need to rerun experiments during the rebuttal phase.
1. About experiments
It seems that, in some cases, improvements compared with C-learning are minor (Fig. 5). Can the authors give some explanation. Meanwhile, why not compare with other contrastive learning approaches in RL mentioned in the early sections?
2. About other common challenges in goal-conditioned RL
Can this approach tackle common challenges in goal-conditioned RL, especially for off-line datasets? e.g., the generalization ability to different distributions.
3. Minor typos
-> Line 107: π ( τ ∣ S t ) -> π ( τ ∣ S g ) ;
-> Eq. 4: consider using another color, the current v +
is hard to recognize when print out the paper to read.
The authors mentioned the limitations in the paper, mainly about how to generalize the work into other RL settings. I think this would be the potential direction for future works. I raise a few questions, which are more like vague points instead of limitations. I think this is a decent paper and I would vote for acceptance.","4: consider using another color, the current v + is hard to recognize when print out the paper to read. The authors mentioned the limitations in the paper, mainly about how to generalize the work into other RL settings. I think this would be the potential direction for future works. I raise a few questions, which are more like vague points instead of limitations. I think this is a decent paper and I would vote for acceptance.",973,1
NIPS_2022_1034,NIPS_2022,"Regarding the background: the authors should consider adding a preliminary section to introduce the background knowledge on the nonparametric kernel regression, kernel density estimation, and the generalized Fourier Integral theorem, which could help the readers easily follow the derivation of Section 2 and understand the motivation to use the Fourier Integral theorem as a guide to developing a new self-attention mechanism.
Regarding the experimental evaluation: the issues are three-fold. 1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification. 2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive. 3) Since the FourierFormer need customized operators for implementation, the authors should also provide the memory/time cost profiling compared to popular Transformer architectures. Based on these issues, the efficiency and effectiveness of the FourierFormer are doubtful.
-------After Rebuttal------- Thank authors for the detailed response. Most of my concerns have been addressed. I have updated my scores to 6.","1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification.",974,0
NIPS_2022_955,NIPS_2022,"The biggest weakness of the paper (and the method proposed by the authors more generally) is its questionable pertinence to federated learning. In particular, as the authors state in their ""Broad Impact"" section, one of the primary tenets of federated learning is the ability to perform machine learning without compromising data privacy. In the original FedAvg paper, for example, the models trained by clients are never shared directly with one another or the server, but are instead all averaged together (which paves the way for things like differential privacy and secure aggregation, which have formal privacy guarantees).
However, Algorithm 1 repeatedly sends models that are held by a client (specifically an anchor client) to many other clients. While this is not the same as broadcasting data directly, it seems questionable as to whether this method is preserving data privacy. This is particularly concerning due to the linear structure of the problem, where clients could conceivably use an anchor client's model to learn about the client's data. Note that I do not believe that all FL algorithms need to incorporate privacy from the start. However, I think that a lack of any reasonable pathway to privacy integration is a drawback to a method that should be discussed and considered. As a naive reference point, an algorithm that had clients send all their data directly to one another would be effectively useless as an FL method.
This is all to say that I believe that the paper should 1) be up front about what kind of privacy model it is actually envisioning and 2) discuss possible pathways to improving the privacy of the algorithm (if there are any). If the authors believe that broadcasting cluster model estimates (as in Algorithm 1, Line 9) directly to other clients is acceptable, then I believe that this warrants discussion.
A more minor weakness of the work is that some of the notation becomes cumbersome, and relies on quantities/assumptions that could use extra explanation. For example, Theorem 2 makes an assumption on the ""clustering error"" term ν
(namely, k ν log ⁡ ( e / ν ) ≤ ρ / κ
). It is not immediately clear how strong or weak this assumption is. If I understand correctly, then this actually imposes a restriction on the dimension d
, which seems like an important thing to note. Summary
I think this is a theoretically strong paper that studies an important topic in federated learning. However, I would prefer that the authors are more up front about the actual privacy model underlying their method, and whether or not that has a pathway to integration with privacy mechanisms like differential privacy or secure aggregation.
Review Score
I have opted to only give the work a weak accept (6) at the time of writing this review. While I believe the theory is strong, this score is tied to the privacy concerns above. I would be happy to revisit this score, contingent upon the work including a useful, grounded discussion of the privacy risks of the method and potential pathways forward.
Concerning the Author Feedback and Discussion
This section has been added after reading the other reviews and author feedback. The authors have addressed my feedback well, and have even gone so far as to add entirely new experiments (as per the request of another reviewer). I do not believe that these were necessary for the purposes of acceptance, but cement my belief that this paper should be accepted. I have increased my score from a 6 to a 7 correspondingly.
There are no negative societal impacts of the work. My only concern is whether or not the method proposed can be made private (in some formal notion) and this is discussed above.",1) be up front about what kind of privacy model it is actually envisioning and,975,1
NIPS_2022_2755,NIPS_2022,"We believe that learning dynamics via a data-driven approach is effective in the following ways: 1) it allows us to construct simulators even when the mathematical model is unknown, and 2) it can lead to scientific discoveries (e.g., conservation laws). In order to clarify the significance of this research, we would like to ask for clarification of possible applications. In particular, please share your thoughts on how this could lead to scientific discoveries. If there is a gap, it would be good to mention it as a limitation.","1) it allows us to construct simulators even when the mathematical model is unknown, and",976,1
NIPS_2022_511,NIPS_2022,"1.Some theorems could be better formatted in the paper. For instance, the section on lower bounds is a bit hard to read with the lack of paper spacing. 2. It would be helpful if the authors could further explain the relationship between the lower bounds and the respective upper bounds the algorithms are able to achieve, as the statements in their current forms are not immediately clear.","1.Some theorems could be better formatted in the paper. For instance, the section on lower bounds is a bit hard to read with the lack of paper spacing.",977,0
NIPS_2022_1815,NIPS_2022,"Comparison with related works can be improved. The authors discuss a connection with programming language theory without providing references. They also do not discuss or compare against other adaptive optimizers, as [1, 2], which are also less sensitive to the initial learning rate and are more theoretically grounded.
Lacks large scale experiments (e.g. on ImageNet) and does not include transformers models, which are widely used neural architectures.
Limited Novelty. The idea of optimizing the learning rate using a first order method with gradient computed by back propagating through one step of the optimizer was presented in [3]. The novelty of this work lies in (i) applying the idea recursively, (ii) optimizing other hyperparameters like the momentum coefficient, (iii) providing a simple Pytorch implementation exploiting automatic differentiation and (iv) an independent experimental evaluation.
[1] Orabona, Francesco, and Tatiana Tommasi. ""Training deep networks without learning rates through coin betting."" Advances in Neural Information Processing Systems 30 (2017).
[2] Vaswani, Sharan, et al. ""Painless stochastic gradient: Interpolation, line-search, and convergence rates."" Advances in neural information processing systems 32 (2019).
[3] Baydin, Atilim Gunes, et al. ""Online Learning Rate Adaptation with Hypergradient Descent."" International Conference on Learning Representations. 2018.
Post author's response The authors addressed most of my concerns and promised to address others. Therefore I am increasing my score from 6 to 7.
The authors properly discussed the limitations of the method in Sec. 5. Main limitations are handling high learning rate values and implementation issues related to the PyTorch implementation.",5. Main limitations are handling high learning rate values and implementation issues related to the PyTorch implementation.,978,0
NIPS_2022_700,NIPS_2022,"1.This paper is an extended paper to Safaryan's, which reduces the significance.
2.Though authors have discussed the technical contribution, the main difficulty of extending to the arbitrary unbiased compression operators seems to be easy to handle.","1.This paper is an extended paper to Safaryan's, which reduces the significance.",979,0
NIPS_2022_1751,NIPS_2022,"weakness: - The novelty is limited. Why do we need RL here is not clear, compared with some adversarial manipulation on the gradients, e.g., directly maximizing the “reward” function w.r.t. gradients. RL is unstable and hard to train, the necessity should be addressed, otherwise, it seems like simply an adaptation of some techniques. - The evaluation is far from enough. This paper should also include some results on colored images, e.g. CIFAR10. - In practice, noisy gradients may also be considered to provide better privacy. This paper proposes a technique heavily rely on the accuracy of gradients, so, an ablation study is suggested. - This paper proposes a technique that seems to require a large amount of malicious users. An ablation study on the number of malicious users is suggested. - Given the limited contribution of this paper, it is suggested that this paper provides some investigation on why gradients generated by RL can be better, what kind of properties it possesses, what is its advantages over other methods, or under what settings this method has an overwhelming advantage. This paper also addresses some potential future works, many of them can be integrated organically in this paper to increase this paper’s contribution, e.g backdoor attacks. - Achieve SOTA on this task on some non-real world dataset is less significant, this paper is suggested to dig out some insights from other aspects, instead of just a SOTA.
Minor weakness:
Typos & Grammars: Line 128; Line 304;
Writing: The reviewer doesn’t see any insights w.r.t data heterogeneity in section 4, while that is in the section title.
It seems that selected malicious clients are required to perform much much much more computations compared with other clients. Although this paper includes a paragraph discussing (line 266- line 274). Some quantitative measure may be preferred, e.g. seconds for FL epoch, seconds for RL epoch.","- The novelty is limited. Why do we need RL here is not clear, compared with some adversarial manipulation on the gradients, e.g., directly maximizing the “reward” function w.r.t. gradients. RL is unstable and hard to train, the necessity should be addressed, otherwise, it seems like simply an adaptation of some techniques.",980,0
NIPS_2022_931,NIPS_2022,"• The overall idea is not novel, while the specific of the methods suggested here are. The idea of reweighting the training data according to their anomalousness degree has been done before (e.g. recent references [2,3,4]). • To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data. • Experiments on other challenging state-of-the-art datasets are desirable. • Limitations are not thoroughly discussed (e.g., the complexity).
Clarity: • The paper is not particularly well written. But, the main points are comprehensible. • The notations of the formulas are sometimes confusing, which makes it difficult to follow. For example, in line 193, “w” denotes the anomaly weight. However, w denotes the batch position at line 140.
Relation to prior work: The authors state that “we are the first one to study the abnormal detection with noisy data”. However, many studies have been proposed in this research field: e.g., [5,6,7]
Reproducibility: • The conducted experiments are well described. • It would be helpful if the entire code would be made accessible in case the paper is accepted.","• To construct the memory bank (i.e., the reference of the normality), the authors propose to reject “the top τ% patches with the highest outlier scores”. Do they implicitly assume that the ratio of the noise in the training data is known in advance? This assumption is strong in a real-world environment where the ratio of the contamination may vary according to the collected training data.",981,0
NIPS_2022_2410,NIPS_2022,"1. The method section is a bit messy and becomes hard to follow. 2. Notations in 3. are definitely over complex, and could be simplified, e.g. by dropping ‘i’ in Eq. 1-6. 3. The method is not entirely free from additional resources contrary to what the authors claim, since it requires an additional model (momentum encoder- local + global) to be trained and stored, as well as the queue for the training. 4. Some typos and minor editing issues. 5. The conclusions on the m hyperparameter are unclear. 6. Both figures 1 are non-informative and confuse the reader. 7. Lack of clear motivation why CVML among other cross-modal tasks and evaluation only on one of them seems to be quite limited.
The authors very briefly mention the limitations of their work, with no discussion of potentially negative impact. In my opinion, the limitations of this work are two-fold. First, as the authors mention, they only tackle the CVML task, however, to fully address this task, the results of state-of-the-art approaches on particular datasets should also be included, showing that they indeed struggle with catastrophic forgetting. Otherwise, it would be beneficial to address other cross-modal tasks.",1. The method section is a bit messy and becomes hard to follow.,982,0
NIPS_2022_42,NIPS_2022,"I do not see potential for negative social impact of the work, as the work is aimed at formally defining harm - research which may help autonomous systems reason about minimizing harm in the future. Thus, I list some limitations of the work I observed below.
Insufficient evaluation
The positive examples provided do not provide any guarantees that there are no counterexamples to the definition, and there are only 7 positive examples provided with no extensive human evaluation (by laypeople or by experts). I list two possible counterexamples below and would be curious to hear the authors’ thoughts.
The relationship between harm and responsibility of agents is not considered by the work
This criticism is aimed at the fact that the definition proposed in the work does not consider the intention of the agents.
Consider the following example:
A driver honks her car horn at the driver in front of her, startling an apartment resident dangling his legs off of his balcony railing. The resident falls off the balcony in fright, becoming gravely injured upon hitting the ground.
It seems strange to claim that the driver harmed the resident, because 1) she could not have known that the resident was in danger of falling 2) even if she did know, the resident is partially at fault for willfully placing himself in a precarious situation. To make the example even more clear-cut:
An apartment resident decides that he will jump off his balcony upon the first car honk he hears. A driver honks her horn at the driver in front of her. Hearing this horn honk, the resident jumps off the balcony, becoming injured upon hitting the ground.
Now, it seems even stranger to claim that the driver harmed the resident, because the resident took an action intended to harm himself, while the driver did not.
This issue could be resolved by using Halpern's concept of blame introduced in (Chockler and Halpern, 2014 - https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2204.pdf). The extended causal model could also solve this problem.
The idea of blame is also touched upon in Example 7. If Victoria knew she was able to rescue Betty without breaking her arm ( P = 2
) and chose to break her arm while rescuing her ( P = 1
), she caused Betty harm, because this goes against social norms. However, if she did not know she'd be able to do so and accidentally broke her arm ( P = 2
was not an option), it is hard to argue that she caused Betty harm, as attempting to rescue Betty is well within social norms, and Betty's utility increased overall.
Causing a decrease in utility in a socially unacceptable way (violating a default) is sufficient but not necessary for harm to occur
This criticism is aimed at the use of the ""default utility"" to represent all norms.
The work does not allow for situations where harm is done but reparations are made, and the agent in question is overall better off. For example:
A thief assaults and robs a passerby but later, out of a guilty conscience, gives the money back, adding some additional money to more-than compensate for any physical and emotional distress caused. The passerby is happier than they would have been otherwise, because they needed the money.
While the passerby is better off by the end of the story, the thief did cause harm to the passerby in the intermediate step of assaulting and robbing them. While this issue could be addressed by stating that the default world is the world where the thief does not assault/rob and gives the passerby money for no reason, this seems to be a difficult argument to make, as the thief is expected by social norms only to not commit any crimes. Setting to default world to the world where the thief gives money to the passerby would mean that everyone is harming the passerby by not giving money to them.
An even more clear-cut version of this example follows:
A thief assaults and robs a passerby. After the local media reports on the crimes committed, many people donate money to the passerby in sympathy. The passerby agrees that he is better off now than he would have been had he not been robbed and assaulted.
It seems that the thief's actions, contrary to his expectations, helped the passerby overall. Is it still true that the thief did not cause harm to the passerby?
The issue could also be fixed by using the extended causal model to, regardless of utility, label any world where the thief commits a crime as less normal than a world where the thief does not commit a crime.",1) she could not have known that the resident was in danger of falling,983,0
NIPS_2022_2233,NIPS_2022,"In lines 3-4, the statement is not necessarily true as there are also methods that focus on better distribution calibration during training (some of these ideas are mentioned later in the paper as well). I would recommend softening the message of this sentence to be more precise.
Language like 'Obviously' (line 47) and 'It is easy to see' (line 168) should generally be avoided in academic writing.
At the end of Sec. 2, it would be helpful to have a one or two sentence discussion to contextualize the proposed BATS method in the described related work.
Variations on the phrase 'We propose to rectify the features into the feature's typical set and then use these typical features to calculate the OOD score.' are repeated frequently throughout the paper. The λ
hyperparameter was not introduced in line 142 when it was first used.
The decreased BATS performance on Tiny-Imagenet OOD detection in Table 2 is not mentioned or discussed.
The paper needs to be proofread for typos. The following is a non-exhaustive list of the typos I found:
Line 2: 'which raises the attention on out-of-distribution (OOD) detection' is awkward phrasing.
Lines 74-75: 'large sufficiently' should read 'sufficiently large'.
Line 98: 'energe score' should read 'energy score'.
Line 109: 'is provable aligned' should read 'is provably aligned'.
Line 130: 'common-used layer' should read 'commonly used layer'.
Footnote 1: I did not grammatically understand the phrase: 'the pre-training outputs moving average estimators during iterations'. Maybe something like: 'The pre-trained model outputs moving average estimators at each iteration.'?
Line 184: 'a two-side rectified normal distribution' should read 'a two-sided rectified normal distribution'.
There should be a space between the abbreviation and the number in references (i.e., Fig. X, Table Y, Sec. Z).
Line 197: 'Fig.2 illustrate' should read 'Fig. 2 illustrates'.
Line 220: 'verse vice' should read 'vice versa'.
Line 225: 'Recent researches propose' should read 'Recent literature/work proposes'.
Line 235: 'models are standard pre-trained' is grammatically awkward. Maybe something like: 'models are pre-trained in a standard manner'?
Line 238: 'In specific,' should read 'Specifically,'.
Line 246: 'which cost more' should read 'which costs more'.
Line 252: 'The start learning rate' should read 'The starting learning rate'.
Line 284: 'Our BATS can reduce the variance that benefit the OOD detection but also introduce a bias.' should read 'Our proposed BATS method can reduce variance, which benefits OOD detection, but can also introduce a bias.'.
Line 285: 'Energy Score (The horizontal lines).' should read 'Energy Score (the horizontal lines)'.
Fig. 5: x-axis labels are missing. This is also the case for some figures in the appendix.
Sec. 6: 'Limitation and societal impact' should read 'Limitations and societal impact'.
Sec. 6: batch normalization is referred to in three different ways in the last paragraph (Batch Normalization, Batch-Norm, BN).
Line 693: 'our method surpass' should read 'our method surpasses'.
The references should be proofread (e.g., to ensure the year is not entered twice in a citation, the conference venue is listed instead of ArXiv when available, etc.).
The authors have done a good job in listing limitations of the BATS method. However, the addition of some potential negative societal implications would be helpful. For example, by truncating the features, certain biases in the data learned by the pre-trained model may be amplified. Furthermore, the process of truncation will inherently cause some information loss which may be crucial to model performance during deployment (building on the hyperparameter tuning discussion in the paper).",6: 'Limitation and societal impact' should read 'Limitations and societal impact'. Sec.,984,0
NIPS_2022_1073,NIPS_2022,"1)The authors claim that the solution they provided have a low discrimination risk (while minimally sacrificing utility) and improve the fairness of models. In addition to the fairness, the underline data distribution should be hold after imputation, which directly affect the effectiveness of the models. 2)While the theoretical results can be useful in understanding the behavior of fairness estimation under different settings, I am not sure if they will have much practical impact. 3)In Theorem 3, R_K is a scalar while Z_U is a vector or matrix, that means R_K and Z_U cannot be compared, please explain the corresponding constraint conditions in P_W and P_B. 4)There are three missing data contexts: missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR), does the findings in this paper be adequate for all the three missing data contexts?
Overall, the paper is well-written, and the contributions and limitations are clearly described. While the theoretical perspective for fairness estimation with missing data for graph data is novel, I am leaning towards reject due to concerns about practical impact, as mentioned above.","2)While the theoretical results can be useful in understanding the behavior of fairness estimation under different settings, I am not sure if they will have much practical impact.",985,0
NIPS_2022_2116,NIPS_2022,"Weakness: 1. Scalability or computation/storage cost is not discussed. 2. The empirical improvement is not impressive
The authors have adequately addressed the limitations and potential negative societal impact of their work.",2. The empirical improvement is not impressive The authors have adequately addressed the limitations and potential negative societal impact of their work.,986,1
NIPS_2022_1844,NIPS_2022,"Weakness
Limited tasks and analysis. Experiments are only conducted on GLUE, which leaves questions on the applicability of the proposed approach. Other than the aspect indicated by the authors that the AD-DROP could potentially be applied to different units in the model, there are still multiple dimensions that could have been considered to make the evaluation of the approach more thorough, convincing, and insightful. For example, 1) only Masked PLMs are considered; what happens if the base model is an autoregressive LM? 2) Other than NLU tasks, can it be applied to generation tasks? 3) How the approach scales with model sizes? In terms of overfitting, both limited data and large model size could be the source and this should also be investigated.
The motivation for including the hyper-parameter q
is unclear. I understood it is used to control the strength of enforcing the masking to the softmax, but isn't this sort of covered by the p
? It seems to me that a random sampling based on uniform distribution to decide the positions (in addition to Eq 6) could do the job. Including q
seems to require much more hyper-parameter tuning. Besides, no description of q
is given around Line 145, I had to guess what q is.
Some confusion about the experiments/analysis
In Table 1, the improvement of R-Drop with RoBERTa Base seems quite limited compared to the improvement with RoBERTa Large on the original paper. I assume the authors implemented and ran the R-Drop with RoBERTa. However, there is no description of how they chose the hyper-parameter for R-Drop and the search space for that.
Comparing Table 3 to Table 1, it looks like the variance of the improvement is rather notable. For example, on RTE with BERT, the improvement reported in Table 1 is 4.3 while when averaging over multiple rounds, the improvement in Table 3 is 2.7. It looks to me that it is more reasonable to report averaging numbers in Table 1 rather than the best possible numbers that are ever achieved?
In Table 4, I may misunderstand something here. It looks to me that it is almost impossible that the computation cost of AD-DROP on STS-B is the same as FT. AD-DROP needs TWO forward passes of the model. It requires running a forward pass of the model first to obtain the pseudo label as well as the attribution scores computation. Lastly, the model will run the forward pass again for back-propagation. On the other hand, FT only requires ONE forward pass and back-propagation. Would you clarify how the computational cost is defined and calculated here such that they are the same?",1) only Masked PLMs are considered; what happens if the base model is an autoregressive LM?,987,0
NIPS_2022_1007,NIPS_2022,1. The experimental results are slightly weak. It seems only very simple toy data are evaluated. 2. The efficiency of the method is not well studied. There are multiple phases of the method and the authors are better to show the computational aspects of the paper.,2. The efficiency of the method is not well studied. There are multiple phases of the method and the authors are better to show the computational aspects of the paper.,988,0
NIPS_2022_863,NIPS_2022,"Weakness:
I think the contribution above is important to the IRL community. However, other contributions of this paper may not be as significant as this one.
The new formulation of IRL. This maximum likelihood formulation is known [2]. The proof seems to be similar to Thm 3 in [3].
The algorithm. The authors mentioned that the algorithm enjoys computational efficiency and is capable of reward transferring. However, GAIL-type of algorithms that use state-dependent rewards in the discriminator [4] and scalable MaxEnt IRL algorithms [5] are able to achieve those benefits as well.
The experiments. I like the experiments on reward transferring. However, the main goal of proposing the new algorithm and theory is to reduce computation burden, but the experiments focus on the quality of the learned policy / reward after convergence. It’s unclear from the experiments whether the new algorithm is indeed faster. For example, an interesting experiment would be a comparison with MaxEnt IRL as in theory the proposed method is able to converge to a stationary point that’s the same as MaxEnt IRL, but faster.
Furthermore, the clarity of the paper can be improved. Line 118 to 135 is rather confusing and only until the second reading I was able to understand the math. In Line 119,\pi_\theta is parameterized by \theta. But in Line 123, \theta is used to parameterize the reward function.
[1] Kamoutsi, Angeliki, Goran Banjac, and John Lygeros. ""Efficient Performance Bounds for Primal-Dual Reinforcement Learning from Demonstrations."" International Conference on Machine Learning. PMLR, 2021.
[2] Jain, Vinamra, Prashant Doshi, and Bikramjit Banerjee. ""Model-free IRL using maximum likelihood estimation."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.
[3] Ziebart, Brian D., J. Andrew Bagnell, and Anind K. Dey. ""The principle of maximum causal entropy for estimating interacting processes."" IEEE Transactions on Information Theory 59.4 (2013): 1966-1980.
[4] Torabi, Faraz, Garrett Warnell, and Peter Stone. ""Generative adversarial imitation from observation."" arXiv preprint arXiv:1807.06158 (2018).
[5] Finn, Chelsea, Sergey Levine, and Pieter Abbeel. ""Guided cost learning: Deep inverse optimal control via policy optimization."" International conference on machine learning. PMLR, 2016.","33. No.01. 2019. [3] Ziebart, Brian D., J. Andrew Bagnell, and Anind K. Dey. ""The principle of maximum causal entropy for estimating interacting processes."" IEEE Transactions on Information Theory 59.4 (2013): 1966-1980. [4] Torabi, Faraz, Garrett Warnell, and Peter Stone. ""Generative adversarial imitation from observation."" arXiv preprint arXiv:1807.06158 (2018). [5] Finn, Chelsea, Sergey Levine, and Pieter Abbeel. ""Guided cost learning: Deep inverse optimal control via policy optimization."" International conference on machine learning. PMLR, 2016.",989,1
NIPS_2022_2294,NIPS_2022,"- Although the idea of ""learning from future"" looks new, the motivation behind it is a bit unclear. Why learn from the future is beneficial, especially considering this ""future"" is also from the current state and the ""history""? On the other hand, it also increases the ambiguity if the model (weights) is just cached without updating.
- Please double-check if the variables were correctly labeled in Fig. 3. From the description, the teacher model g_\phi was trained on labled data x_l and the student model on unlabeled data x_u. But what was shown in the figure is the other way around. Please also add more details to the caption to make it self-contained.
- In the proposed future self-training (FST), a new hyper-parameter \mu' was introduced, as in Eq. 4. But it was a bit unclear how to define this parameter and the necessity of it. The authors did an ablation study to test different \mu', but the result did not suggest a way to set it and it is also a bit unclear the significance of adding this additional parameter. What if removing it or is that possible to change it to a learnable parameter?
- Although the authors stated that the combination of FST-D and FST-W is more beyond the scope of their study, it would be better to have some preliminary investigation to see if these two types of learning schemes actually boost the learning.
- It would be better to include more state-of-the-art methods for the experimental comparison of semi-supervised learning (Table 5).
- L296, ""... of of ...""
The authors discussed the limitations of their work and acceptable solution to address it, but did not mention the potential negative societal impact.
A possible societal impact could be the bias within the learned model if there was also bias in the training data. The environmental impact could be another potential societal impact due to the large-scale and long-time training and the corresponding carbon emission.","- L296, ""... of of ..."" The authors discussed the limitations of their work and acceptable solution to address it, but did not mention the potential negative societal impact. A possible societal impact could be the bias within the learned model if there was also bias in the training data. The environmental impact could be another potential societal impact due to the large-scale and long-time training and the corresponding carbon emission.",990,0
NIPS_2022_859,NIPS_2022,"Weakness - The novelty of this paper is low, just a simple application of Fixmatch/Mixmatch in federated learning. - Missing important FSSL baselines, e.g., FedBYOL and FedU. - The theoretical proof given in this paper is only an analysis of data augmentation.
The comparison of accuracy results of SSL methods is not essential, because the proposed method only use Fixmatch and Mixmatch in this paper. Some FSSL baselines are missing, like FedBYOL, FedU, etc. Please include the results of more recent baseline methods.
Please supplement theoretical analysis to support the model designed, e.g., convergence analysis.","- The novelty of this paper is low, just a simple application of Fixmatch/Mixmatch in federated learning.",991,0
NIPS_2022_655,NIPS_2022,"Weakness: 1. The conclusion seems to be only for GCN. I wonder GAT[1] may exhibit a smaller degree bias, even smaller than graph contrastive learning methods. 2. From Figure 6 in Appendix A, the advantage of graph contrastive learning methods over GCN on Photo dataset is not obvious. The numerical values of their slopes are close. 3. There is a small gap between degree bias and theoretical analysis of clear community structure. 4. The improvement of the proposed method in Table 1 does not seem statistically significant because of high variance. 5. There are some related works designed for degree bias, such as SL-DSGCN[2]. But these methods are not set as baselines in the experimental comparison.
[1] Veličković P, Cucurull G, Casanova A, et al. Graph Attention Networks[C]//International Conference on Learning Representations. 2018. [2] Tang X, Yao H, Sun Y, et al. Investigating and mitigating degree-related biases in graph convoltuional networks[C]//Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020: 1435-1444.
In addition to the limitations mentioned in the paper, the generalization of the conclusion should be taken into consideration.",3. There is a small gap between degree bias and theoretical analysis of clear community structure.,992,0
NIPS_2022_144,NIPS_2022,"1. Some of the results are confusing and need more illustration (see comments below). 2. Experiments are only conducted on synthetic datasets and more results on real-world datasets especially common GNN benchmarks are expected.
The major limitation lies in the experiments, though most of contents seem reasonable and sound. The experiment datasets only cover synthetic datasets which are simple and limited. I suggest adding more experiments on real-world datasets, especially some common GNN benchmarks for link prediction and comparing with some SOTA approaches (even though they are not designed for OOD regime). Though I understand that the main focus of this paper lies in the theoretical insights and perspecitve, more results on benchmarks and comparison with more strong models (currently used baselines seem too weak and simple) could make this work more complete and convincing, especially that one of the contributions of this work is a new approach for OOD link prediction.
Also, there are some recent works on out-of-distribution generalization on graphs, e.g., [1, 2, 3], which are missing in the related works and need to be discussed.
[1] Size-Invariant Graph Representations for Graph Classification Extrapolations, ICML 21.
[2] Handling Distribution Shifts on Graphs: An Invariance Perspective, ICLR 22.
[3] From Local Structures to Size Generalization in Graph Neural Networks, ICML 21.",1. Some of the results are confusing and need more illustration (see comments below).,993,0
NIPS_2022_1136,NIPS_2022,"1.) Computational efficiency -- the key results make use of computing upper and lower empirical value functions for a given policy \pi, which requires optimizing over the empirical constraint set depending on the TD error and the Q-function class. While for linear MDPs this can be done some-what efficiently, in general it seems like such a computation would require time which is linear in the size of the test function class. Given that in two of the demonstrated examples (LSTD error test space and Bellman test space) the test function class has complexity proportional to the complexity of the Q-function class it is unclear how to efficiently carry out such computations beyond linear models.
2.) Presentation -- the paper is somewhat technical and the main results require introducing a good amount of notation. This, however, is not done in a very good way. I found myself often not being able to recall or understand notation introduced earlier in the paper. It does not help that a lot of the notation is also introduced on the fly and scattered between different sections. Further, given that the main result is bounding the off-policy width through OPC, it would have been good to give interpretable bounds soon after the OPC is introduced in Eq. 10. Some of these bounds are hidden in Appendix A, e.g. Lemma 4, Lemma 6, and it might have suited the presentation better if they were exhibited in the main paper.
Additional comments:
-- the notation used in the proof in C.1.1 for C^\pi_n is not good. It is important to include the dependence on \rho, otherwise the first inequality in the display under line 1081 would not hold.
-- the proof of Theorem 3 seems to be structured around using Proposition 4 and checking that Equation 37 holds, however, this is not done explicitly or at least I have missed it. Otherwise, Proposition 4 does not seem to be used anywhere in the paper.
Overall I think this is a good paper, however, improving the presentation and discussing computationally efficient algorithms beyond the linear setting can strengthen the paper even further.
This work is theoretical and it is hard to judge what the societal impact will be.","2.) Presentation -- the paper is somewhat technical and the main results require introducing a good amount of notation. This, however, is not done in a very good way. I found myself often not being able to recall or understand notation introduced earlier in the paper. It does not help that a lot of the notation is also introduced on the fly and scattered between different sections. Further, given that the main result is bounding the off-policy width through OPC, it would have been good to give interpretable bounds soon after the OPC is introduced in Eq.",994,0
ARR_2022_306_review,ARR_2022,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable.
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems.
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough.
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set.","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?",995,0
RD9q5vEe1Q,ICLR_2025,"- The authors primarily select the Sigmoid function for smoothing. However, as illustrated in Figure 2, when the extent of miscoverage exceeds a certain level, the EQ term actually decreases (which is also the case for the Gaussian error function). While the authors justify this with the goal of ensuring robustness, the rationale seems somewhat unconvincing.
- In connection with the above, if the Sigmoid function is used, the parameter $c$ can be considered a hyperparameter. Also given that Theorem 1 imposes constraints on $c$ for coverage guarantees, a more in-depth discussion on its selection, beyond the brief mention in Appendix D, would be appreciated.
- The interpretation of the miscoverage bound presented in Theorem 2 is rather ambiguous. The choice of the learning rate is a crucial aspect, yet theoretical discussion on this remains superficial.
- Similarly, the experiments lack detailed discussion on how the learning rate, which is critical for the performance of each method, was chosen. The statement on Line 330, “we select the most appropriate range of learning rates $\eta$ for the respective datasets and present the best results in the tables,” is not only vague but could potentially raise concerns about data leakage. More transparency in the tuning process is needed.
- Additionally, the choices of $h$ and $w$ in the ECI-cutoff method are not discussed.
Overall, while the proposed method is straightforward and built on a solid idea, there is room for more detailed discussion regarding the rationales, interpretations of the results, and specific hyperparameter tuning. Minor:
- Line 119: Please use \citep for Barber et al. (2021).
- Line 238: incorrect interval order
- Line 653: non-differential -> non-differentiable
- Are ""coverage""s in, e.g., Figure 3, rolling averages?","- In connection with the above, if the Sigmoid function is used, the parameter $c$ can be considered a hyperparameter. Also given that Theorem 1 imposes constraints on $c$ for coverage guarantees, a more in-depth discussion on its selection, beyond the brief mention in Appendix D, would be appreciated.",996,0
i1G4AWXHRv,ICLR_2025,"1. Transfer Overhead: The performance impact due to GPU-CPU transfer time, particularly for specific models (e.g., ViT-bigG), limits the efficacy of Superpipeline in certain scenarios.
2. Scalability Concerns: The dependency on effective data transfer strategies (e.g., batch vs. sequential) could affect scalability, requiring additional customization for certain model or hardware setups.","1. Transfer Overhead: The performance impact due to GPU-CPU transfer time, particularly for specific models (e.g., ViT-bigG), limits the efficacy of Superpipeline in certain scenarios.",997,0
dj940KfZl3,ICLR_2024,"- A fundamental problem with the work is the focus and claims related to modelling of disease trajectories or progression. It is not entirely clear what the authors mean when they use these terms, and since this is a critical part of the work, this should really be defined. Disease trajectory, I would understand to refer to the course of a disease over time. This could be in an individual or maybe as an average in a population. This would imply some predictive capability, and we are also told this in the abstract (see below). Yet there is as far as I can see no evidence that the proposed method can predict the future of individual patients or average patients. Instead it seems to me that what the approach is doing is instead to create images corresponding to different disease severities, which is certainly interesting, but a very different and generally easier problem. Loosely described, this could perhaps be called disease progression simulation, which is also a term used by the manuscript in places.
- ""PIE can allow healthcare providers to model disease imaging trajectories over time, predict future treatment responses"" - where is the evidence for this?
- ""Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient."" - how is it personalized?
- ""The learning rate in this iterative process is decaying exponentially with each iteration forward, which means that the algorithm is effectively exploring the solution space while maintaining a balance between convergence speed and stability."", I don't think this is supported by evidence/references.
- ""The physicians agree that simulated disease progressions generated by PIE closely matched physicians’ expectations 76.2% of the time, indicating high accuracy and quality."" - is this a relevant measure to compare to? Are physicians able to predict actual progression?
- The question the physicians were asked appears to be ""Does the below disease progression fit your expectation?"" It is unclear if this is supposed to match a development in disease severity or what the specific development in this particular case would be expected to be.
- ""However, all these methods have to use full sequential images and fail to address personalized healthcare in the imaging space. The lack of such time-series data, in reality, poses a significant challenge for disease progression simulation"". I am uncertain about what is meant by ""failing to address personalized healthcare in the imaging space"". Could more precise wording be used? Also I feel like the authors are overly focused on the requirement of sequential data as a limitation. Longitudinal data exists for a reason and it may be much more difficult if not impossible to derive individualized progression models from cross-sectional data alone. I would suggest the authors think about the wording here and present it not as a limitation of previous methods but rather as a situation where the proposed approach could be used where previous models may not.
- Explain abbreviation DDIM
- ""Due to the properties of DDIM, the step size would gradually decrease
with a constant factor."", what step size? No mention of step size before this point.
- Proposition 2 and 3, would benefit from some motivation, and explanation in text. There are variables and functions used without definition.
- ""In addition, Proposition 2 and 3 show as n grows bigger, the changes between steps would grow smaller. Eventually, the difference between steps will get arbitrarily small. Hence, the convergence of P IE is guaranteed and modifications to any inputs are bounded by a constant."" - I don't see how this follows. Could you help the reader a bit?
- What are the numbers presented in Table 1?
- ""To further assess the quality of our generated images, we surveyed 35 physicians and radiologists with 14.4 years of experience on average to answer a questionnaire on chest X-rays."" - why are the questions asked not
- ""Furthermore, a user study conducted with veteran physicians confirms that the simulated disease progressions generated by PIE meet real-world standards."", what real world standards?","- ""PIE can allow healthcare providers to model disease imaging trajectories over time, predict future treatment responses"" - where is the evidence for this?",998,0
yKksu38BpM,ICLR_2024,"- Only the rank correlation of the softmax probabilities for the **correct** class is considered. However, to be faithful enough, the surrogate model should also behave similarly to the NN for the **incorrect** classes. An important application of data attribution is to explain why a NN makes a wrong prediction. This is not considered in the paper.
- Eq. (4) is confusing. In the denominator, the $\cdot ^ {\frac{1}{2}}$ is applied to the inner product. However, according to Appendix C and the definition of cosine similarity, the $\cdot ^ {\frac{1}{2}}$ should be applied to the sum, not the inner product.
- The quality of Figure 2 could be improved.","- Only the rank correlation of the softmax probabilities for the **correct** class is considered. However, to be faithful enough, the surrogate model should also behave similarly to the NN for the **incorrect** classes. An important application of data attribution is to explain why a NN makes a wrong prediction. This is not considered in the paper.",999,0
cIgfXQBExO,ICLR_2024,"Summary: I believe the paper has good contributions and presentation, but is currently missing several important evaluations. If the paper had more robust and convincing experiments I would be open to raising my score.
Missing comparisons
- The paper claims SSIM and PSNR are not appropriate for views the model is hallucinating. It is true these are not the best metrics, but it then only reports one metric (LPIPS), and contains only one visual comparison against prior work (Figure 7) – this includes supplemental and website. In a task of novel view synthesis, if two out of the three reported metrics are weaker than prior work, visual results are necessary to convince reviewers the proposed model improves on results. More examples would also be helpful to understand the drop in PSNR and SSIM numbers.
- The paper says “long runtime makes typical generation-based metrics such as FID cost-prohibitive”. I’m confused: in my experience, FID can be run in a minute or two over thousands of images. Given FID is an important metric for hallucinated images, it (or perhaps KID, or similar) should be reported at least on a subset of images. The alternative could be a human A/B test, which is not as ideal, since it cannot be precisely replicated, but still could give more defense to the results vs. existing methods.
- PixelNeRF is not the state of the art on single-image novel view synthesis. Table 2 is central to the paper’s argument, but unfortunately the comparisons are limited to Zero-1-to-3 and PixelNeRF.
- The proposed method finetunes this on scenes, so should improve over Zero-1-to-3; so it would be good to see other baselines as well.
- These two baselines do not have mechanisms to deal with differing intrinsics. Perhaps a better comparison would be to train these only on e.g. CO3D or only on RealEstate10K
- As a result, it is hard to determine how effective the model is vs. prior work, or of the main contribution is training on large data?
- Perhaps some recent SOTA methods could be trained on the same data. For instance (comparison does not have to be to this baseline, just an idea:) Consistent View Synthesis with Pose-Guided Diffusion Models (Tseng et al., CVPR 23), SynSin (Wiles et al., CVPR 20)
- RegNeRF is better than DietNeRF in 3, 6 and 9 view. It should be considered: (RegNeRF: Regularizing Neural Radiance Fields (Niemeyer et al., CVPR 22))
-	Can the proposed model be finetuned e.g. on DTU for performance gain? If not, why not?","- Perhaps some recent SOTA methods could be trained on the same data. For instance (comparison does not have to be to this baseline, just an idea:) Consistent View Synthesis with Pose-Guided Diffusion Models (Tseng et al., CVPR 23), SynSin (Wiles et al., CVPR 20) - RegNeRF is better than DietNeRF in 3, 6 and 9 view. It should be considered: (RegNeRF: Regularizing Neural Radiance Fields (Niemeyer et al., CVPR 22)) - Can the proposed model be finetuned e.g. on DTU for performance gain? If not, why not?",1000,0

paper_id,venue,focused_review,point,human_discard,id
ACL_2017_270_review,ACL_2017,"Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.
Other minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.
2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.
While the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems.
Given these strengths, I am changing my recommendation score to 3. I have read the authors' responses.",2) the effect of removing inference composition from the model. Other minor issues:,1,1
ACL_2017_128_review,ACL_2017,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.","----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.",1,10
ACL_2017_769_review,ACL_2017,"In terms of the presentation, mathematical details of how the embeddings are computed are not sufficiently clear. While the authors have done an extensive evaluation, they haven't actually compared the system with an RL-based dialogue manager which is current state-of-the-art in goal-oriented systems. Finally, it is not clear how this approach scales to more complex problems. The authors say that the KB is 3K, but actually what the agent operates is about 10 (judging from Table 6).
- General Discussion: Overall, I think this is a good paper. Had the theoretical aspects of the paper been better presented I would give this paper an accept.","-General Discussion: Overall, I think this is a good paper. Had the theoretical aspects of the paper been better presented I would give this paper an accept.",1,17
ACL_2017_37_review,ACL_2017,"Weak results/summary of ""side-by-side human"" comparison in Section 5. Some disfluency/agrammaticality.
- General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether ""second"", ""third"", and ""last"" imply a side-specific or global enumeration.
2. Some reader confusion may be eliminated by explicitly defining what ""segment"" means in ""segment level"", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as ""a sequence-sequence [similarity matrix]"". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean ""word subsequence"" and ""word subsequence to word subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.
3. Currently, the variable symbol ""n"" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.
4. The statement ""This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice."" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases.
The authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than ""better"") than the VHRED baseline.
5. The authors may choose to insert into Figure 1 the explicit ""first layer"", ""second layer"" and ""third layer"" labels they use in the accompanying text.
6. Their is a pervasive use of ""to meet"" as in ""a response candidate can meet each utterace"" on line 280 which is difficult to understand.
7. Spelling: ""gated recurrent unites""; ""respectively"" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; ""baseline model over"" -> ""baseline model by""; ""one cannot neglects"".","-General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues:",1,21
ACL_2017_614_review,ACL_2017,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?","-General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms. Some additional questions for the authors :",1,35
ACL_2017_588_review,ACL_2017,"and the evaluation leaves some questions unanswered. - Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.
- Weaknesses: 1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.
2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.
3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.
4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.
- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?
2) Have you tried building a classifier that just takes h_i^e as inputs?
I have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.","- Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.",1,38
ACL_2017_484_review,ACL_2017,"The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.
- General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5. The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5.","-General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p.",1,56
ACL_2017_56_review,ACL_2017,"- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.
- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.
- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.
- General Discussion: This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.
I have read the author response. I look forward to seeing the revised version of the paper.","-General Discussion: This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors :",1,68
ACL_2017_108_review,ACL_2017,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?","- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?",1,74
ACL_2017_579_review,ACL_2017,"[-] Limited to completely deterministic, hand-engineered minimization rules; [-] Some relevant literature on OIE neglected; [-] Sound but not thorough experimental evaluation.
- General Discussion: This paper tackles a practical issue of most OIE systems, i.e. redundant, uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE, is designed to actually ""minimize"" extractions by removing overly specific portions and turning them into structured annotations of various types (similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE system (ClausIE) and test it on two publicly available datasets, showing that it effectively leads to more concise extractions compared to standard OIE approaches, while at the same time retaining accuracy.
Overall, this work focuses on an interesting (and perhaps underinvestigated) aspect of OIE in a sound and principled way. The paper is clearly written, sufficiently detailed, and accompanied by supplementary material and a neat Java implementation.
My main concern is, however, with the entirely static, deterministic and rule-based structure of MinIE. Even though I understand that a handful of manually engineered rules is technically the best strategy when precision is key, these approaches are typically very hard to scale, e.g. in terms of languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al., 2016). In other words, I think that this contribution somehow falls short of novelty and substance in proposing a pipeline of engineered rules that are mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance, I would have really appreciated an attempt to learn these minimization rules instead of hard-coding them.
Furthermore, the authors completely ignore a recent research thread on “semantically-informed” OIE (Nakashole et al., 2012; Moro and Navigli, 2012; 2013; Delli Bovi et al., 2015) where traditional extractions are augmented with links to underlying knowledge bases and sense inventories (Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only relevant in terms of related literature: in fact, having text fragments (or constituents) explicitly linked to a knowledge base would reduce the need for ad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example with ""Bill of Rights"" provided by the authors (line 554), an OIE pipeline with a proper Entity Linking module would recognize automatically the phrase as mention of a registered entity, regardless of the shape of its subconstituents.
Also, an underlying sense inventory would seamlessly incorporate the external information about collocations and multi-word expressions used in Section 6.2: not by chance, the authors rely on WordNet and Wiktionary to compile their dictionary of collocations.
Finally, some remarks on the experimental evaluation: - Despite the claim of generality of MinIE, the authors choose to experiment only with ClausIE as underlying OIE system (most likely the optimal match). It would have been very interesting to see if the improvement brought by MinIE is consistent also with other OIE systems, in order to actually assess its flexibility as a post-processing tool.
- Among the test datasets used in Section 7, I would have included the recent OIE benchmark of Stanovsky and Dagan (2016), where results are reported also for comparison systems not included in this paper (TextRunner, WOIE, KrakeN).
References: - Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using Cross-lingual Projection. NAACL-HLT, 2015.
- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an Open Information Extraction System from English to German. EMNLP 2016.
- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy of Relational Patterns with Semantic Types. EMNLP 2012.
- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic Network with Ontologized Relations. CIKM 2012.
- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm. IJCAI 2013.
- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information Extraction from Textual Definitions through Deep Syntactic and Semantic Analysis. TACL vol. 3, 2015.
- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open Information Extraction. EMNLP 2016.","- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy of Relational Patterns with Semantic Types. EMNLP 2012.",1,77
ACL_2017_588_review,ACL_2017,"I was a bit puzzled by the fact that using larger contexts, beyond the sentences with blanks in them, did not help the models. After all, you were in a way using additional context in the HierEnc model, which accumulates knowledge from other contexts. There are two possible explanations: Either the sentences with blanks in them are across the board more informative for the task than the sentences without. This is the explanation suggested in the paper, but it seems a bit unintuitive that this should be the case. Another possible explanation is that the way that you were using additional context in HierEnc, using the temporal network, is much more useful than by enlarging individual contexts C and feeding that larger C into the recurrent network. Do you think that that could be what is going on?
- General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.","-General Discussion: I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.",1,78
ACL_2017_579_review,ACL_2017,"[-] Limited to completely deterministic, hand-engineered minimization rules; [-] Some relevant literature on OIE neglected; [-] Sound but not thorough experimental evaluation.
- General Discussion: This paper tackles a practical issue of most OIE systems, i.e. redundant, uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE, is designed to actually ""minimize"" extractions by removing overly specific portions and turning them into structured annotations of various types (similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE system (ClausIE) and test it on two publicly available datasets, showing that it effectively leads to more concise extractions compared to standard OIE approaches, while at the same time retaining accuracy.
Overall, this work focuses on an interesting (and perhaps underinvestigated) aspect of OIE in a sound and principled way. The paper is clearly written, sufficiently detailed, and accompanied by supplementary material and a neat Java implementation.
My main concern is, however, with the entirely static, deterministic and rule-based structure of MinIE. Even though I understand that a handful of manually engineered rules is technically the best strategy when precision is key, these approaches are typically very hard to scale, e.g. in terms of languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al., 2016). In other words, I think that this contribution somehow falls short of novelty and substance in proposing a pipeline of engineered rules that are mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance, I would have really appreciated an attempt to learn these minimization rules instead of hard-coding them.
Furthermore, the authors completely ignore a recent research thread on “semantically-informed” OIE (Nakashole et al., 2012; Moro and Navigli, 2012; 2013; Delli Bovi et al., 2015) where traditional extractions are augmented with links to underlying knowledge bases and sense inventories (Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only relevant in terms of related literature: in fact, having text fragments (or constituents) explicitly linked to a knowledge base would reduce the need for ad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example with ""Bill of Rights"" provided by the authors (line 554), an OIE pipeline with a proper Entity Linking module would recognize automatically the phrase as mention of a registered entity, regardless of the shape of its subconstituents.
Also, an underlying sense inventory would seamlessly incorporate the external information about collocations and multi-word expressions used in Section 6.2: not by chance, the authors rely on WordNet and Wiktionary to compile their dictionary of collocations.
Finally, some remarks on the experimental evaluation: - Despite the claim of generality of MinIE, the authors choose to experiment only with ClausIE as underlying OIE system (most likely the optimal match). It would have been very interesting to see if the improvement brought by MinIE is consistent also with other OIE systems, in order to actually assess its flexibility as a post-processing tool.
- Among the test datasets used in Section 7, I would have included the recent OIE benchmark of Stanovsky and Dagan (2016), where results are reported also for comparison systems not included in this paper (TextRunner, WOIE, KrakeN).
References: - Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using Cross-lingual Projection. NAACL-HLT, 2015.
- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an Open Information Extraction System from English to German. EMNLP 2016.
- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy of Relational Patterns with Semantic Types. EMNLP 2012.
- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic Network with Ontologized Relations. CIKM 2012.
- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm. IJCAI 2013.
- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information Extraction from Textual Definitions through Deep Syntactic and Semantic Analysis. TACL vol. 3, 2015.
- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open Information Extraction. EMNLP 2016.","- Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using Cross-lingual Projection. NAACL-HLT, 2015.",1,80
ARR_2022_147_review,ARR_2022,"- Lack of illustrative examples regarding the model outputs.
- Some details regarding the knowledge collection process have been omitted (see ""Questions"" below).
QUESTIONS: - Fig. 2: Why did you discard the ""anatomy"" category?
- l. 221: How many query templates did you specify in total?
- l. 227: What's the size of the set of knowledge candidates?
- l. 550: Did you calculate the agreement between the annotators? Were the annotators authors of the paper?
MINOR: - Try to better align the figures with the text.
- fix punctuation: l. 336, l. 433, l. 445, l. 534 - Table 2: The highlighting of the numbers does not correspond to the caption (""highest scores are in bold, second highest scores in italic"")","- fix punctuation: l. 336, l. 433, l. 445, l.",1,92
ARR_2022_329_review,ARR_2022,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case.
General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice. 2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others. 3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format) - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps. - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus? - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in","- Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format) - Line 100: what is 'sharable format'?",1,102
ARR_2022_75_review,ARR_2022,"- The authors do not promise to release their code.
- I would like to see more in-depth analyses about the improvement, e.g. where the improvement really comes from. Does it come from data points where syntactic constraints are violated?",- The authors do not promise to release their code.,1,132
ARR_2022_319_review,ARR_2022,"1 The paper poses mostly subjective points which are themselves largely not novel or are flawed in themselves. It starts with the emphasized sentence on page 1 attributed to Jacovi et al which I will call ""Opinion A"": ""As an explanation method, the evaluation criteria of attribution methods should be how accurately it reflects the true reasoning process of the model (faithfulness), not how convincing it is to humans (plausibility)"" This point is debatable. Whether attributions should be purely faithful to model behaviour or offer human-interpretability is a decision to be made in the definition of an attribution method or its motivation. Neither option is wrong as long as the method is applied to its intended use case. Further, the noted choice of faithfulness has implications on some of the ""logic traps"" described subsequently. I elaborate on each in the comments section below.
Significant positioning fixes need to be implemented in this work starting with Opinion A. Either arguments for faithfulness need to be made or the paper needs to restrict itself to attributions whose primary concern is faithfulness. The latter option would have to be written from the perspective that faithfulness is an apriori goal.
The novelty of arguments being made needs to be addressed as well. I find that the cores of the ""logic traps"" are all known points and are often well considered in attribution, evaluation, and robustness work. Presently the paper does not demonstrate that (logic traps) ""in existing evaluation methods have been ignored for a long time"". If there is a gap in some segment of the community, a survey or systemization paper where applicable would be more appropriate.
2 Arguments and experiments with regard to ""model reasoning process"" are vague in key definitions and thus non-convincing. Specifically, no definition of ""model reasoning process"" is given.
Experiment outlined in Figure 7 argues that extractor's bits are equivalent to ""model reasoning process"" but this is arguable. The extractor can derive the same bits using different means or different bits using very similar means. While I do not disagree with the main points regarding model reasoning process and robustness, I do not think the experiments demonstrate them.
Detailed comments regarding Weakness 1): * Logic Trap 1: The point being made is obvious: ""The decision-making process of neural networks is not equal to the decision-making process of humans.""
The example Experiment 1 incorporates not a single attribution method. Is it presumed that all attributions will be wrong as there is no human explanation possible? Can we also say that any human explanation would be wrong for the same reason? If there is no ground truth, how can it be wrong for an attribution method to say anything?
Regardless, no-one *expects* models to fully replicate the reasoning of a human. Replicating human behaviour may be useful to gauge human-interpretability of explanations but this is precluded by Opinion A. Finally, it is well understood that what is correct to a human may not be correct in a model as per faithfulness vs. explainability discussion which is had alongside attribution evaluations in literature and in Opinion A (some examples from cited works below).
* Logic Trap 2: The second point made is a roundabout way of saying that ablation orders are varied: ""Using an attribution method as the ground truth to evaluate the target attribution method.""
The authors are rightly pointing out that numerous forms of attribution evaluation based on ablation or reconstruction inputs have been used to motivate attribution methods. Here the opposite conclusion to Opinion A is helpful. Expecting humans to interpret an attribution one way may lead to one ablation order whereas another form of interpretation may lead to another. There is no single correct metric because there is no single interpretation. This point has been made at least in [a].
* Logic Trap 3: The third point is known: ""The change in attribution scores maybe because the model reasoning process is really changed rather than the attribution method is unreliable.""
Ghorbani et al. (2019) note in their concluding discussion that fragility of attribution is reflective of fragility of the model (and that their attacks have not ""broken"" an attribution).
Likewise Alvarez-Melis et al. (2018) point out that the non-robust artifacts in attributions they discover are actually reasonable if faithfulness to model is the only goal of an attribution.
Thus Logic Trap 3 does not seem to add anything beyond Opinion A. * 3.1 -- ""Attacking attribution methods by replacing the target model.""
This section seems to be pointing out that attribution methods have a problem of domain faithfulness in that they often internally apply a model to inputs that they have not been trained on or don't expect to operate reliably on.
* 3.2 -- ""Should We Use Attributions Methods in a Black-Box Way?""
The paper argues that black-box is not a worthwhile goal. This is again highly subjective as there are several reasons, despite presented arguments, to prefer black-box methods, like 1) having explanations of the semantics of what a model is modeling instead of an explanation tainted by how the model is implemented, 2) black-box means model agnostic, hence can apply to any model structure, 3) some scenarios just do not have access to the model internals.
Other comments: - The term ""logic traps"" is not define or explained. Dictionaries and reference materials equate them to logical fallacies which I'm unsure is the intended meaning here.
- The term ""erasure-based"" is used in the intro but later the term ""ablation"" is used.
- Typo/word choice near ""with none word overlap"".
- Typo/grammar near ""there are works (...) disprove"" - Suggestion of 3.3, point 2 is unclear. Adversarial examples can be highly confident and I suspect means of incorporating confidence can themselves be subverted adversarially.
- Figure 6 is not useful. I presume the paths are supposed to be indicative of model behaviour but as mentioned in earlier comments, defining it is a crucial problem in explanation work. The Figure is suggestive of it being a triviality.
- Several points in the paper use phrases like ""A lot of works ..."" or ""most existing methods ..."".
I think it would be more appropriate to list the works or methods instead of noting the relative abundance.
- There are some inconsistencies in the notation for AOPC and the k parameter with potential related typos in its definition.
- Potential grammar issue near ""can be seen as an attribution"".
- Grammar issue near ""results are mainly depend"" - Grammar issue near ""achieve 86.4% accuracy"" - Where is footnote 1?
- In Figure 4, the resulting attributions for the target word ""good"" in LOO, Marg are not presented or indicated by color.
References from comments: [a] - Wang et al. ""Interpreting Interpretations: Organizing Attribution Methods by Criteria"".","2) black-box means model agnostic, hence can apply to any model structure,",1,140
ARR_2022_42_review,ARR_2022,"- It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
- The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
- The authors do not explore the performance drop when using local entities in the experiments.
Comments: 1. Please add more descriptions as the authors undergo the human annotation process.
2. It would be better to add examples for each setting.
Questions: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage?
2. Why did the significant performance drop occur when using local entities in the experiments?
3. Did you manage to measure the quality of the samples you collected using the proposed method?",2. It would be better to add examples for each setting. Questions:,1,144
AxPGO36LfE,EMNLP_2023,"1. missing results MER in X-SNS and testing its effectiveness?
2. The p-value in NER and POS change greatly as the p-value change. So maybe present some results of X-SNS with diverse p-values in a table for analysis? An efficient selection method for p-value is needed because searching for p-value mat becomes a major cost of applying X-SNS.
3. Lack of details about applying wiki data as a corpus.
4. X-SNS seems to work poorly in mT5 in Person and Spearman metrics, this requires explanation because if X-SNS works poorly in generation-based models, its application will become much shallower. And I believe more experiments for generation-based backbones may be better.",3. Lack of details about applying wiki data as a corpus.,1,174
ICLR_2021_2208,ICLR_2021,"+ Nice idea Consistent improvements over cross entropy for hierarchical class structures Improvements w.r.t other competitors (though not consistent) Good ablation study
The improvements are small The novelty is not very significant
More comments:
Figure 1: - It is not clear what distortion is at this stage - It is not clear what perturbed MNist is, and respectively: why is the error of a 3-layer CNN so high (12-16% error are reported)? CNNs with 2-3 layers can solve MNist with accuracy higher than 99.5%? - This figure cannot be presented on page 2 without proper definitions. It should be either presented on page 5, where the experiment is defined, or better explained
Page 4: It is said that s can be computed efficiently and this is shown in the appendix, but the version I have do not have an appendix Page 6: the XE+EMD method is not present in a comprehensible manner. 1) p_k symbols are used without definition (tough I think I these are the network predictions p(\hat{y}=k I) 2) the relation of the formula presented to the known EMD is not clear. The latter is a problem solved as linear programming or similar, and not a closed form formula 3) it is not clear what the role of \mu is and why can be set to 3 irrespective of the scale of metric D page 7: The experiments show small, but consistent improvements of the suggested method over standard cross entropy, and improvements versus most competitors in most cases
I have read the reviews of others and the author's response. My main impression of the work remains as it was: that it is nice idea with small but significant empirical success. However, my acquaintance with the previous literature in this subject is partial compared to the acquaintance of other reviewers, so It may well be possible that they are in a better position than me to see the incremental nature of the proposed work. I therefore reduce the rating a bit, to become closer to the consensus.",+ Nice idea Consistent improvements over cross entropy for hierarchical class structures Improvements w.r.t other competitors (though not consistent) Good ablation study The improvements are small The novelty is not very significant More comments: Figure 1:,1,246
ICLR_2021_910,ICLR_2021,"+ The paper’s topic is very relevant to questions being raised in the recent literature regarding the differences between supervised and episodic training.
+ The premise of studying the role of episodic fine-tuning from the perspective of feature (dis)entanglement is an interesting application of Frosst et al. (2019)’s work to the few-shot classification setting.
- Vague or inconsistent use of terminology.
- Poor writing and presentation.
- Performance of the proposed approach is not competitive with competing approaches, contrary to what's claimed in the submission. Recommendation
I recommend rejection. While the paper’s premise is interesting, the submission suffers from poor writing and presentation quality, and I’m not entirely convinced by the claimed causal relationship between representation (dis)entanglement and episodic fine-tuning.
Detailed justification
My main concerns have to do with the presentation of the results and the interpretation of episodic fine-tuning as a way to decrease the representation’s entanglement.
The list of competing approaches in Table 1 is incomplete and outdated. For instance, Tian et al. (2020)’s ""Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?"" obtains around 64.8% on mini-ImageNet 5-way 1-shot using a ResNet-12 architecture. Tian et al. (2020) also lists several approaches using a Conv4 backbone that achieve a mini-ImageNet 5-way 1-shot performance greater than 50.4%. I therefore disagree with the assertion that ""our method shows competitive performance to other methods"" and that ""when the backbone is shallow, we have outperformed all other methods with the same backbone"".
The paper hints at the fact that the Euclidean metric used by Prototypical Networks doesn’t work well with highly entangled representations. Another possible interpretation is that the backbone is pre-trained using a linear output layer, which computes the inner-product between the representation and class weight vectors, and that the squared distance computed by the Euclidean metric is not well suited to the resulting representation. According to that interpretation, part of what episodic fine-tuning does is correct for this mismatch in metrics between (meta-)training and (meta-)testing. I think the paper’s interpretation would be more convincing if the pre-trained backbone used a quadratic output layer (i.e. the logits are computed as the negative squared distance between the representations and class weight vectors) and therefore controlled for metric mismatch.
I have additional issues with the way in which results are presented:
Section 3.3 mentions a result presented in Section 4.4, then Section 3.4 begins by stating that the previous section concludes that the last layer in the backbone is more disentangled after episodic training. This means that in order to have the proper context to understand Section 3.4, the reader needs to jump forward and read Section 4.4. I recommend changing the order of the presentation so that it is more linear.
In Table 1, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant.
Overall, the submission could benefit from another round of careful proofreading. It contains several grammar mistakes which, while they do not significantly compromise clarity, make reading the paper harder than it should be. Examples include:
""[...] and crafted the hard episode to make necessary episodes fewer.""
""Disentanglement is the property whether the data-points [...]""
""Benefited by the understanding, [...]""
I also noted a few false or unsupported statements:
""Due to episodic training, meta-learning methods generalize better than traditional transfer-like methods for the novel classes."" Can the authors expand on this? What do they mean by ""generalize""? Do they refer to test classes from the same domain (e.g. mini-ImageNet test classes), or test classes from different domains (i.e. cross-domain generalization)? I don’t see the statement as generally accepted, especially given the many recent papers that show strong performance with well-tuned transfer learning baselines.
""Soft-Nearest Neighbour loss [...] is proposed by Frosst et al. (2019)"" Frosst et al. credits Salakhutdinov and Hinton (2007) for the soft nearest neighbour loss, which itself draws inspiration from Goldberger et al. (2005)’s Neighbourhood Component Analysis.
""Though the split is quite naive, the afterward episodic learning shows promising improvement."" Can the authors point to work that provides empirical evidence for this statement?
Finally, the submission’s use of terminology is vague or inconsistent at times:
The categorization of approaches as either ""supervised pre-training"" or ""meta few-shot learning"" feels incomplete to me. While some approaches (most recently Meta-Baseline and Meta-Dataset’s few-shot learners) do perform supervised pre-training followed by episodic fine-tuning, most well-known approaches such as Matching Networks, Prototypical Networks, MAML, etc. do not prescribe a supervised pre-training phase.
The term ""meta few-shot learning"" is not widely used in the literature and appears to be introduced in this paper as far as I can tell. It’s defined in the introduction to be the combination of supervised pre-training and episodic fine-tuning, but it’s also used in Section 3.2 to categorize Prototypical Networks, whose formulation does not prescribe a supervised pre-training phase. Questions
What do the authors mean when they state that ""[due] to the parallelism property, normal training literature is much faster than episodic training""? Isn’t the forward propagation through the embedding function in a few-shot learner such as Prototypical Networks just as parallelizable as the forward propagation in a supervised classifier?
The submission claims that the proposed approach alleviates the burden of training episodically on a large number of episodes. Given the performance of well-tuned supervised baselines, why should we perform episodic fine-tuning? Shouldn’t it be sufficient to use the pre-trained backbone as-is?
The submission follows Chen et al. (2020) for the pre-training and episodic fine-tuning procedure but doesn’t compare against it in Table 1. How come?
Additional feedback
This is arguably inconsequential, but the paper motivates few-shot learning using bird classification as an example, stating that ""an ornithologist typically can only obtain a few pictures per bird species"". I don’t know if I agree that this is typically the case: looking at the iNaturalist online database, thousands of pictures can be found for a large number of bird species.
The related work section appears to contain mostly pre-2020 references and does not mention recent work such as (Simple) CNAPs, SUR, CrossTransformers, etc.",+ The premise of studying the role of episodic fine-tuning from the perspective of feature (dis)entanglement is an interesting application of Frosst et al. (2019)’s work to the few-shot classification setting.,1,247
ICLR_2021_793,ICLR_2021,"While the backbone of the paper is strong, I think it could be improved in its head (motivation) and legs (experimental studies).
First, motivation. While the framing around rare words with the COVID-19 example is interesting, I think it has gaps. The introduction argues that since “COVID-19” is a rare word, in the course of training the model may lack the necessary signal to predict the masked word “lives.” But isn’t this fact exactly what should lead the model to improve its embedding of “COVID-19”? Because gradients flow into the embeddings both through the softmax layer and the input layer.
So while adding to the context may help the model get a foothold with more effective training signal for the masked token, it seems to me that the note could also “explain away” the rare word’s embedding in the input layer, reducing the learning signal on it. If that’s the case, then to the extent that TNF works, it would be by the tradeoff between improving the learning signal at the output layer for all words (and in contextualization) and degrading it at the input layer (for rare words).
As a broader example, see https://openreview.net/pdf?id=3Aoft6NWFej. That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency, whereas this paper argues essentially the opposite—that shortcuts must be added to hard cases in order to facilitate learning. It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful. Because it’s not clear where that line is, I think it’s not enough to motivate TNF from only one direction. It would be better to also have an explanation of why the note-taking approach does not also make things “too easy.” It’s not obvious to me how to best make this argument, though results from some of the ablations I will suggest below might help.
This brings me to my second point: Ablation experiments. If the motivation is to improve the representations of rare words in the input, then there are even simpler ways to do this. Experiments with simple baselines and ablations are important for figuring out why exactly TNF works.
First, if the note is such a useful addition to the word embedding, why not just use it to update the embeddings directly? At that rate, the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe. This suggests a critical ablation:
Initialize the word embeddings with word2vec, GloVe, or similar run over the wordpieces in the pretraining corpus. (Weirdly, I can’t find an example of this in the literature. It seems like an obvious thing to try. I may have just missed it.) Indeed, it seems to me that the framing in the paper could just as easily motivate this (much simpler) technique than TNF.
If TNF outperforms the critical ablation, that implies that its gains are coming from some of the other particulars of the technique, such as 1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or 2) the use of contextualized vectors for note embeddings (rather than the non-contextualized ones in the word embedding objectives).
To investigate these issues, I would suggest three more ancillary ablations on TNF:
Directly update the rare word’s embedding with a version of Eq. 5 rather than keeping a separate note dictionary.
Update the note embeddings via backprop instead of Eq. 5. This would amount to “partially tying” the input and output embeddings, giving more freedom to the input layer, which is partly what’s happening in TNF.
Pool over non-contextualized instead of contextualized representations in Eq. 4.
Finally, to address the “too easy” vs “too hard” distinction, two more ablations that might help would be:
Instead of using an exponential moving average for the note embedding update, just use the pooled context vectors from the last instance of the rare word (i.e., set γ
to 1 in Eq. 5).
instead of using an explicit note dictionary, augment the input context with retrieved text containing the rare word. See TEK-enriched representations (https://arxiv.org/pdf/2004.12006.pdf) for an example of this. For consistency, the exact last-seen context of the rare word could be used.
The first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF. This could then serve as a reference point for the second ablation, which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting “too easy.” (although context window sizes might also be a confound here, that could also be controlled carefully.)
All together I think these ablations would shed a lot of light on why TNF works, and make this work much more useful to researchers who wish to build on it in the future. However, I know I’ve suggested a lot of crazy experiments here. I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important. I am also sure the authors could come up with better ablations than these as well. But my sticking point is the first ablation — initializing with non-contextualized embeddings — which I think is critical. And I think it behooves the authors to address some of the lingering questions (including more written below), even if not all of them. Recommendation
Unfortunately, reject. The technique is simple and the results seem good, but the paper does not provide empirically-justified insight on why TNF works. I think ablations and investigation into the “why” aspect is the most important part of this kind of model engineering research.
More comments & questions
I am left with some more questions about how TNF works:
How does the quality of the representations of rare words specifically compare in your approach? Does it improve the representations of common words and contextualization at the expense of rare words? While it may be tricky to try to directly assess embedding or contextualization quality, breaking down the MLM perplexities by word frequency (or presence of rare words in the context) after removing the note dictionary might be informative. I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment. But any insight into this issue would be appreciated.
If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens, then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training? I am not aware of anyone showing such a thing to work, though I might have missed it. Just a thought.
While the pretraining corpus is huge, 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper. Questions:
What do the even rarer words look like? Are they just a source of noise? e.g., because they are components of names or don’t have clear and consistent semantic content?
What proportion of contexts contain words appearing less than 100 times? It seems that the 20% figure in the paper is meant to apply to your definition of rare words, which appear between 100 and 500 times.
What is the word vocabulary size? i.e., how many words appear more than 500 times, and less than 100?
Did you do any preliminary experiments with other thresholds? Would you expect this to work with more common words as well? Why or why not? (This may also relate to the “too easy” vs “too hard” issue.)
On pre-training efficiency results: I think Figs 3a and 3b need to be explicitly qualified a little better. AFAICT, having lower loss here doesn’t necessarily mean the model (modulo the note dictionary) is learning better, because it sees the notes in the input. So we’re looking at the loss in a different setting than we intend to fine-tune in. It’s still interesting to see, but I think it's best to include an explicit caveat.
What about training the models for more steps? Will the trend hold and performance improve overall, or will the gains eventually level off as the representations of rare words get better? Especially for pretrained models, since they are used as the starting point for many models, it is often worthwhile to train them longer (as in the RoBERTa paper), so it’s important to understand the usefulness of this method in that regime.
Typos etc.:
P.3: neglectable -> negligible
P.3: Representation -> Representations (in BERT acronym)
P.6 Sec. 4.1: after “MNLI” there is a space missing after the period.
P.6: “FULL-SENTENCES” would look better & be consistent with Liu et al if it were in small caps.
Please cite the individual dataset creators for the datasets in the GLUE benchmark.
Update: upped score from 4 to 5; see comment thread.
Update again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words.","1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or",1,259
ICLR_2021_147,ICLR_2021,"the empirical validation is weak. Therefore, more new models need to be compared. For more details, please refer to “Reasons for reject”
Reasons for accept: 1. The structure of this paper is clear and easy to read. Specifically, the motivation of this paper is clear and the structure is well organized; the related work is elaborated in detail; the experimental setup is complete. 2. Based on the use of replay to solve catastrophic forgetting, the current popular graph structure is introduced to capture the similarities between samples. Combined with the proposed Graph Regularization, this paper provides a new perspective for solving catastrophic forgetting. 3. The experimental results given in the paper can basically show that the proposed method is effective. The ablation study also verified the effectiveness of each component.
Reasons for reject: 1. The lack of comparison of experimental effects after replacing Graph Regularization with other regularization methods mentioned in this paper, or other distance measurement methods, eg., L2.
This paper compares relatively few baselines, especially recent studies. I hope to see the comparison results of some papers in the list below. The latest papers on the three types of methods (regularization, expansion, and rehearsal) for solving catastrophic forgetting are included. Therefore, if it can be compared with some of these models, it will be beneficial to the evaluation of GCL.
[1] Ostapenko O , Puscas M , Klein T , et al. Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning. ICML 2019 [2] Y Wu, Y Chen, et al. Large Scale Incremental Learning. CVPR 2019 [3] Liu Y , Liu A A , Su Y , et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020 [4] Zhang J , Zhang J , Ghosh S , et al. Class-incremental learning via deep model consolidation. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV) [5] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continuous learning of context-dependent processing in neural networks. Nature Machine Intelligence, 2019. [6] Wenpeng Hu, Zhou Lin, et al. Overcoming catastrophic forgetting for continual learning via model adaptation. ICLR 2019 [7] Rao D , Visin F , Rusu A A , et al. Continual Unsupervised Representation Learning. NeurIPS 2019","2020 IEEE Winter Conference on Applications of Computer Vision (WACV) [5] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continuous learning of context-dependent processing in neural networks. Nature Machine Intelligence, 2019. [6] Wenpeng Hu, Zhou Lin, et al. Overcoming catastrophic forgetting for continual learning via model adaptation. ICLR 2019 [7] Rao D , Visin F , Rusu A A , et al. Continual Unsupervised Representation Learning. NeurIPS 2019",1,260
ICLR_2021_910,ICLR_2021,"+ The paper’s topic is very relevant to questions being raised in the recent literature regarding the differences between supervised and episodic training.
+ The premise of studying the role of episodic fine-tuning from the perspective of feature (dis)entanglement is an interesting application of Frosst et al. (2019)’s work to the few-shot classification setting.
- Vague or inconsistent use of terminology.
- Poor writing and presentation.
- Performance of the proposed approach is not competitive with competing approaches, contrary to what's claimed in the submission. Recommendation
I recommend rejection. While the paper’s premise is interesting, the submission suffers from poor writing and presentation quality, and I’m not entirely convinced by the claimed causal relationship between representation (dis)entanglement and episodic fine-tuning.
Detailed justification
My main concerns have to do with the presentation of the results and the interpretation of episodic fine-tuning as a way to decrease the representation’s entanglement.
The list of competing approaches in Table 1 is incomplete and outdated. For instance, Tian et al. (2020)’s ""Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?"" obtains around 64.8% on mini-ImageNet 5-way 1-shot using a ResNet-12 architecture. Tian et al. (2020) also lists several approaches using a Conv4 backbone that achieve a mini-ImageNet 5-way 1-shot performance greater than 50.4%. I therefore disagree with the assertion that ""our method shows competitive performance to other methods"" and that ""when the backbone is shallow, we have outperformed all other methods with the same backbone"".
The paper hints at the fact that the Euclidean metric used by Prototypical Networks doesn’t work well with highly entangled representations. Another possible interpretation is that the backbone is pre-trained using a linear output layer, which computes the inner-product between the representation and class weight vectors, and that the squared distance computed by the Euclidean metric is not well suited to the resulting representation. According to that interpretation, part of what episodic fine-tuning does is correct for this mismatch in metrics between (meta-)training and (meta-)testing. I think the paper’s interpretation would be more convincing if the pre-trained backbone used a quadratic output layer (i.e. the logits are computed as the negative squared distance between the representations and class weight vectors) and therefore controlled for metric mismatch.
I have additional issues with the way in which results are presented:
Section 3.3 mentions a result presented in Section 4.4, then Section 3.4 begins by stating that the previous section concludes that the last layer in the backbone is more disentangled after episodic training. This means that in order to have the proper context to understand Section 3.4, the reader needs to jump forward and read Section 4.4. I recommend changing the order of the presentation so that it is more linear.
In Table 1, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant.
Overall, the submission could benefit from another round of careful proofreading. It contains several grammar mistakes which, while they do not significantly compromise clarity, make reading the paper harder than it should be. Examples include:
""[...] and crafted the hard episode to make necessary episodes fewer.""
""Disentanglement is the property whether the data-points [...]""
""Benefited by the understanding, [...]""
I also noted a few false or unsupported statements:
""Due to episodic training, meta-learning methods generalize better than traditional transfer-like methods for the novel classes."" Can the authors expand on this? What do they mean by ""generalize""? Do they refer to test classes from the same domain (e.g. mini-ImageNet test classes), or test classes from different domains (i.e. cross-domain generalization)? I don’t see the statement as generally accepted, especially given the many recent papers that show strong performance with well-tuned transfer learning baselines.
""Soft-Nearest Neighbour loss [...] is proposed by Frosst et al. (2019)"" Frosst et al. credits Salakhutdinov and Hinton (2007) for the soft nearest neighbour loss, which itself draws inspiration from Goldberger et al. (2005)’s Neighbourhood Component Analysis.
""Though the split is quite naive, the afterward episodic learning shows promising improvement."" Can the authors point to work that provides empirical evidence for this statement?
Finally, the submission’s use of terminology is vague or inconsistent at times:
The categorization of approaches as either ""supervised pre-training"" or ""meta few-shot learning"" feels incomplete to me. While some approaches (most recently Meta-Baseline and Meta-Dataset’s few-shot learners) do perform supervised pre-training followed by episodic fine-tuning, most well-known approaches such as Matching Networks, Prototypical Networks, MAML, etc. do not prescribe a supervised pre-training phase.
The term ""meta few-shot learning"" is not widely used in the literature and appears to be introduced in this paper as far as I can tell. It’s defined in the introduction to be the combination of supervised pre-training and episodic fine-tuning, but it’s also used in Section 3.2 to categorize Prototypical Networks, whose formulation does not prescribe a supervised pre-training phase. Questions
What do the authors mean when they state that ""[due] to the parallelism property, normal training literature is much faster than episodic training""? Isn’t the forward propagation through the embedding function in a few-shot learner such as Prototypical Networks just as parallelizable as the forward propagation in a supervised classifier?
The submission claims that the proposed approach alleviates the burden of training episodically on a large number of episodes. Given the performance of well-tuned supervised baselines, why should we perform episodic fine-tuning? Shouldn’t it be sufficient to use the pre-trained backbone as-is?
The submission follows Chen et al. (2020) for the pre-training and episodic fine-tuning procedure but doesn’t compare against it in Table 1. How come?
Additional feedback
This is arguably inconsequential, but the paper motivates few-shot learning using bird classification as an example, stating that ""an ornithologist typically can only obtain a few pictures per bird species"". I don’t know if I agree that this is typically the case: looking at the iNaturalist online database, thousands of pictures can be found for a large number of bird species.
The related work section appears to contain mostly pre-2020 references and does not mention recent work such as (Simple) CNAPs, SUR, CrossTransformers, etc.",+ The paper’s topic is very relevant to questions being raised in the recent literature regarding the differences between supervised and episodic training.,1,269
ICLR_2021_2769,ICLR_2021,"+ Clarifications: - I am a little confused about the definition of “missing data”. The authors seem to define it through D_S, which is the set of examples where no component is missing. If this is the case, then why even bother with missing components at all in the problem model? We could equivalently consider it a form of dataset shift – either a point is present, or it is not - In general, the motivation needs work – not quite clear how this should map onto real scenarios. For instance, top of 2.3 “fairness in the complete data domain is of primary interest” – it isn’t quite clear why this is true - More analysis of the missingness mechanisms would be nice as they are discussed in detail at the top - Would like to see more clarification of the difference between this paper and Martinez-Plumed (2019) – you don’t really touch on it in 1.2 - In Theorem 2, I’m not sure how to interpret “with probability at least 7 / 1440” – this seems very low. Is this a useful result in this case? My interpretation is that this result relies on there existing potentially a very odd sample, which doesn’t seem like such a useful thing to know. However I might be misunderstanding results of this type - Additionally, my confusion propagates in Fig 1a – is there some intuition for why this bound might work despite this possibly very low probability of holding? Would like some intuition
Other feedback: - The motivation section in the intro needs some work – would be good to focus more on the specific examples in the second paragraph. - Top of 2.1 – z_(0)I definition is confusing. Does this mean a certain number of “missing” tokens? The elements which are actually unobserved? The indices which have missing data? - Why is D_T called a domain if it is a distribution? - Top of 3.2 – how can the bound on the variance be shown? Not obvious to me - In COMPAS experiment design – why does it matter which feature you throw out? My understanding is that if any feature is missing, the whole data point is removed from the complete case domain and as such is a “missing” data point",- Why is D_T called a domain if it is a distribution?,1,278
ICLR_2021_1948,ICLR_2021,"a. Anonymisation Failure in References
i. A reference uncited in the manuscript body contains a non-anonymised set of author names to a paper with the same title as the system presented in this paper. This was not detected during initial review. ""Shuby Deshpande and Jeff Schneider. Vizarel: A System to Help Better Understand RL Agents.arXiv:2007.05577.""
b. Citations
i. There is egregiously missing information in almost all citations. This is quite obvious in all the missing dates in the manuscript text.
c. Clarity
i. There are a few unclear or misleadingly worded statements made as below:
1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).
2) ""stronger feedback loop between the researcher and the agent"" - This is at least confusing. In any learning setting, there is a strong interaction loop between experimentation by the researcher and resulting outcomes for the trained model.
3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.
4) ""For multi-dimensional action spaces, the viewport could be repurposed to display the variance of the action distribution, plot different projections of the action distribution, or use more sophisticated techniques (Huber)."" - It would be clearer to actually state what the sophisticated techniques from Huber are here.
ii. The framework could be clearer that it applies most directly as described on single agent RL. The same approach could be used with multi-agent RL but the observation state and visualisations around that get more confusing when there are multiple potentially different sets of observations. Is this not the case, please clarify?
d. Experimental rigour
i. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. Not every paper requires a user study, however, the contributions proposed by this particular manuscript require validation at some level from actual RL users (even a case study with some feedback from users would address this to some extent).
e. Novelty in Related Work
i. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See a sampling below.
1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }
2) @inproceedings{ Atrey2020Exploratory, title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning}, author={Akanksha Atrey and Kaleigh Clary and David Jensen}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rkl3m1BFDB} }
3) @inproceedings{ Puri2020Explain, title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution}, author={Nikaash Puri and Sukriti Verma and Piyush Gupta and Dhruv Kayastha and Shripad Deshmukh and Balaji Krishnamurthy and Sameer Singh}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SJgzLkBKPB} }
4) @article{reddy2019learning, title={Learning human objectives by evaluating hypothetical behavior}, author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey and Legg, Shane and Leike, Jan}, journal={arXiv preprint arXiv:1912.05652}, year={2019} }
5) @inproceedings{mcgregor2015facilitating, title={Facilitating testing and debugging of Markov Decision Processes with interactive visualization}, author={McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald}, booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, pages={53--61}, year={2015}, organization={IEEE} }
6) @article{puiutta2020explainable, title={Explainable Reinforcement Learning: A Survey}, author={Puiutta, Erika and Veith, Eric}, journal={arXiv preprint arXiv:2005.06247}, year={2020} }
7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }
8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }
9) @misc{sundararajan2020shapley, title={The many Shapley values for model explanation}, author={Mukund Sundararajan and Amir Najmi}, year={2020}, eprint={1908.08474}, archivePrefix={arXiv}, primaryClass={cs.AI} }
10) @misc{madumal2020distal, title={Distal Explanations for Model-free Explainable Reinforcement Learning}, author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere}, year={2020}, eprint={2001.10284}, archivePrefix={arXiv}, primaryClass={cs.AI} }
11) @article{Sequeira_2020, title={Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations}, volume={288}, ISSN={0004-3702}, url={http://dx.doi.org/10.1016/j.artint.2020.103367}, DOI={10.1016/j.artint.2020.103367}, journal={Artificial Intelligence}, publisher={Elsevier BV}, author={Sequeira, Pedro and Gervasio, Melinda}, year={2020}, month={Nov}, pages={103367} }
12) @article{Fukuchi_2017, title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents}, ISBN={9781450351133}, url={http://dx.doi.org/10.1145/3125739.3125746}, DOI={10.1145/3125739.3125746}, journal={Proceedings of the 5th International Conference on Human Agent Interaction}, publisher={ACM}, author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita}, year={2017}, month={Oct} }
4. Recommendation
a. I recommend this paper for rejection as the degree of change needed to validate the contribution through a user study or other validation is likely not feasible in the time and space needed. However, the contribution is potentially valuable to RL, so with the inclusion of this missing evaluation and additions to related work/contextualisation of the contribution, I would consider increasing my score and changing my recommendation.
5. Minor Comments/Suggestions
a. It is recommended to use the TensorFlow whitepaper citation for TensorBoard (https://arxiv.org/abs/1603.04467). This is the official response (https://github.com/tensorflow/tensorboard/issues/3437).","8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }",1,286
ICLR_2021_78,ICLR_2021,"weakness in comparisons to comparable virtual environments is given later.
d. The different variant agents being compared in the task benchmark are clearly explained and form an appropriate experimental design.
2. Novelty/Impact
a. The work aims to showcase a new challenge task, evaluation platform, and benchmark agent performance for goal recognition followed by collaborative planning. The work as described compares favourably to similar work in evaluation platforms and benchmarks referenced in the related work section and appendix. The differences are made clear, though the use of some featured distinctions are not demonstrated in the paper (e.g. visual observations are possible but not used in the benchmarking).
3. Experimental Rigour
a. This work is not primarily about demonstrating the benefits of a particular approach over others in a particular application. It demonstrates benchmarks for agent performance in a newly introduce problem setting. From that perspective, the paper has strong experimental rigour. The experimental design is appropriate, comparing multiple baselines and oracles with several sets of experimental variants from both automated planning and reinforcement learning communities.
b. The comparison of experimental variants is conducted with both a computationally-controlled agent and a human-controlled avatar to evaluate collaboration performance in increasingly realistic settings.
c. The claim that the computationally-controlled Alice agent is human-like is repeated throughout the paper. This is not justified in the actual text of the main paper, but is supported to a moderate degree (human-like in strategy/planning if not movement/behaviour) through experiments with human subjects that are described in the appendix.
d. Effort paid to ensure diverse avatars in experimentation.
4. Reproducibility
a. The work is almost entirely reproducible, with details of all agent architectures used for experiments provided with hyperparameters and architecture design. The authors describe that the environment will be released as open-source, which will then make the article wholly reproducible. This reviewer appreciated the level of additional detail provided in the appendix to improve this area of evaluation.
3. Weaknesses
1. This paper uses the term social intelligence to motivate the context for this challenge task. Social intelligence is a much broader term than what is actually being evaluated here and would require evaluating capabilities beyond goal recognition and coordinated planning/task execution. It is suggested to replace this claim with ""goal recognition and collaborative planning"".
2. From the motivation provided, i.e. evaluating social perception and collaboration, why is the task specifically about watching then helping and not both together or just helping or just watching or some combination of these activities fluidly occurring throughout the interaction?
3. Further, the work itself does not explicitly motivate why this, specific challenge task for goal recognition followed by collaborative planning is necessary for moving the state of the art in human-AI collaboration forward. However, it is a small leap to see the impact of this platform/task in evaluating applications like service robotics, social robotics, collaborative human-agent task performance, video games, etc. This reviewer can understand the impact of the work, but it would be clearer to explicitly discuss this.
4. It would be clearer to specify that this task is limited to situations where there is explicitly only one goal throughout the entire demonstration + execution episode. This is important since it precludes using this challenge task for research into agents that need to use goal recognition after the initial demonstration, potentially continuously over the course of execution. This second kind of continuous goal monitoring is more similar to real-world applications of watching and helping or assistive agents or social robotics, since the human collaborator can (and often will) change their mind.
5. Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. This is similar to situations like team sports, where all that matters is the final game score. Many real-world scenarios for human-AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For example, in a computer game where Bob represents a team-mate to Alice who is a human player, Bob can choose to steam-roll Alice and win the game by itself. However, this leads to lower subjective user experience for the human team-mate. In this case, the score might be greater than what Alice could accomplish on their own and the game might be won faster than Alice could on their own, but the experience would be different based on whether they are truly collaborating or one is over-shadowing the other.
6. A final assumption, is that there is no difference in expertise between Alice and Bob. The human is expected to be able to competently finish the task and for Bob to linearly cut down the time taken to perform this task. There are many real-world tasks in human-AI collaboration where this assumption does not hold and there could be non-linear interactions between success-rate and speed-up due to different levels of expertise between Alice and Bob.
7. The fixed Alice agent is called human-like through out the article and this was not properly justified anywhere in the main text of the paper. However, the appendix actually describes results that compare the performance of the computationally-controlled and human-controlled variants of Alice to human observers. This potentially justifies this weakness. For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.
8. Why aren't there benchmark results (more than one) for the goal recognition component similar to the planning task experimentation? If both parts of the task are important, it would be valuable to provide additional experiments to show comparisons between goal recognition approaches as well, even if that is in the appendix for space reasons.
9. There could be more analysis of the benchmark agent performance, 1) Why does the purely random agent work relatively well across tasks? 2) Why doesn't HRL work better? Is this due to less hyperparameter tuning compared to other approaches or due to some intrinsic aspect of the task itself? 3) Perhaps I missed this, but why not try a flat model-based or model-free RL without a hierarchy?
10. There are several comments about other environments in the related work section and appendix being toy environments. However, the tasks in the environment demonstrated in this paper only use a small set of predicates as goals. Similarly, it CAN generate visual observations but that isn't used by any of the baselines in the paper. Several comparisons to related virtual environments are made in appendix, but some of the features aren't used here either (humanoid agent - this challenge task works equally well with non-humanoid avatars/behaviours and realism - visual realism is present but it isn't clear if behavioural or physical realism is present due to seeming use of animations instead of physical simulation).
11. None of the tasks described allow the use of communication between agents or evaluate that. Other multi-agent environments like Particle Environments (below) allow for that. Communication is a natural part of collaboration and should have been mentioned if only to distinguish future work or work out of current scope.
a. @article{mordatch2017emergence, title={Emergence of Grounded Compositional Language in Multi-Agent Populations}, author={Mordatch, Igor and Abbeel, Pieter}, journal={arXiv preprint arXiv:1703.04908}, year={2017}}
12. ""planning and learning based baselines"", ""and multiple planning and deep reinforcement learning (DRL) baselines"", etc. - There is potential for confusion with the use of terms ""planning"" and ""learning"" methods to do what both fields (automated/symbolic planning and reinforcement learning) would potentially consider as planning tasks. It would be clearer to indicate this distinction in terminology.
13. The human-likeness evaluation experiment asked subjects to evaluate performance one agent video at a time. A more rigorous evaluation might compare two agents side by side and ask the human to guess the human performance. This could also be in addition to the current evaluation. The current evaluation is a ceiling on performance while the comparative evaluation is a potential floor.
4. Recommendation:
1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.
5. Minor Comments/Suggestions:
1. Some minor typos in the manuscript:
a. Using the inferred goals, both HP and Hybrid can offer effective. - page 6
b. IN(pundcake, fridge) - appendix table 2
c. This closeness perdition - appendix page 19","4. Recommendation:1. I recommend accepting this paper, which was clear, novel, empirically strong, and supremely reproducible. The strengths conveyed above outweighed the weaknesses.",1,300
ICLR_2021_1682,ICLR_2021,"+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.",- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.,1,302
ICLR_2021_1948,ICLR_2021,"a. Anonymisation Failure in References
i. A reference uncited in the manuscript body contains a non-anonymised set of author names to a paper with the same title as the system presented in this paper. This was not detected during initial review. ""Shuby Deshpande and Jeff Schneider. Vizarel: A System to Help Better Understand RL Agents.arXiv:2007.05577.""
b. Citations
i. There is egregiously missing information in almost all citations. This is quite obvious in all the missing dates in the manuscript text.
c. Clarity
i. There are a few unclear or misleadingly worded statements made as below:
1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).
2) ""stronger feedback loop between the researcher and the agent"" - This is at least confusing. In any learning setting, there is a strong interaction loop between experimentation by the researcher and resulting outcomes for the trained model.
3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.
4) ""For multi-dimensional action spaces, the viewport could be repurposed to display the variance of the action distribution, plot different projections of the action distribution, or use more sophisticated techniques (Huber)."" - It would be clearer to actually state what the sophisticated techniques from Huber are here.
ii. The framework could be clearer that it applies most directly as described on single agent RL. The same approach could be used with multi-agent RL but the observation state and visualisations around that get more confusing when there are multiple potentially different sets of observations. Is this not the case, please clarify?
d. Experimental rigour
i. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. Not every paper requires a user study, however, the contributions proposed by this particular manuscript require validation at some level from actual RL users (even a case study with some feedback from users would address this to some extent).
e. Novelty in Related Work
i. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See a sampling below.
1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }
2) @inproceedings{ Atrey2020Exploratory, title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning}, author={Akanksha Atrey and Kaleigh Clary and David Jensen}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rkl3m1BFDB} }
3) @inproceedings{ Puri2020Explain, title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution}, author={Nikaash Puri and Sukriti Verma and Piyush Gupta and Dhruv Kayastha and Shripad Deshmukh and Balaji Krishnamurthy and Sameer Singh}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SJgzLkBKPB} }
4) @article{reddy2019learning, title={Learning human objectives by evaluating hypothetical behavior}, author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey and Legg, Shane and Leike, Jan}, journal={arXiv preprint arXiv:1912.05652}, year={2019} }
5) @inproceedings{mcgregor2015facilitating, title={Facilitating testing and debugging of Markov Decision Processes with interactive visualization}, author={McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald}, booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, pages={53--61}, year={2015}, organization={IEEE} }
6) @article{puiutta2020explainable, title={Explainable Reinforcement Learning: A Survey}, author={Puiutta, Erika and Veith, Eric}, journal={arXiv preprint arXiv:2005.06247}, year={2020} }
7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }
8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }
9) @misc{sundararajan2020shapley, title={The many Shapley values for model explanation}, author={Mukund Sundararajan and Amir Najmi}, year={2020}, eprint={1908.08474}, archivePrefix={arXiv}, primaryClass={cs.AI} }
10) @misc{madumal2020distal, title={Distal Explanations for Model-free Explainable Reinforcement Learning}, author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere}, year={2020}, eprint={2001.10284}, archivePrefix={arXiv}, primaryClass={cs.AI} }
11) @article{Sequeira_2020, title={Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations}, volume={288}, ISSN={0004-3702}, url={http://dx.doi.org/10.1016/j.artint.2020.103367}, DOI={10.1016/j.artint.2020.103367}, journal={Artificial Intelligence}, publisher={Elsevier BV}, author={Sequeira, Pedro and Gervasio, Melinda}, year={2020}, month={Nov}, pages={103367} }
12) @article{Fukuchi_2017, title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents}, ISBN={9781450351133}, url={http://dx.doi.org/10.1145/3125739.3125746}, DOI={10.1145/3125739.3125746}, journal={Proceedings of the 5th International Conference on Human Agent Interaction}, publisher={ACM}, author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita}, year={2017}, month={Oct} }
4. Recommendation
a. I recommend this paper for rejection as the degree of change needed to validate the contribution through a user study or other validation is likely not feasible in the time and space needed. However, the contribution is potentially valuable to RL, so with the inclusion of this missing evaluation and additions to related work/contextualisation of the contribution, I would consider increasing my score and changing my recommendation.
5. Minor Comments/Suggestions
a. It is recommended to use the TensorFlow whitepaper citation for TensorBoard (https://arxiv.org/abs/1603.04467). This is the official response (https://github.com/tensorflow/tensorboard/issues/3437).","1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }",1,303
ICLR_2021_2836,ICLR_2021,"Are the assumptions of Lemma 1 really satisfied by SGLD? I don't see why the Markov chain induced by SGLD would have the posterior of interest as its stationary distribution, even if the step sizes are appropriately annealed.
More generally, the paper makes several strong claims of ""fast posterior simulation"" and ""guaranteed superiority of $\tilde{q}{\eta, \phi}^{(t)} o v e r
q\phi$"". However, the results stated in Lemma 1 to 3 could be seen as motivation for the proposed approach but do not constitute rigorous theoretical guarantees for the actual algorithm.
Finally, it is not clearly justified why we can use samples from all T
iterations (instead of just the samples from the T
th iteration) to estimate the gradients. Since the marginal distribution of the Markov chain is different for each time 0 ≤ t ≤ T
, using samples from all T
time steps would seem to add an additional layer of approximation compared to the objective stated in Equations 4--6. It would help to move Algorithm 1 (and any mention of practical approximations needed to make the algorithm work in practice) into the main body of the paper.
Minor comments
MCMC SGLD and VI are not proper nouns (i.e. it should be ""MCMC algorithms"" instead of ""MCMCs"")
P. 1: what does ""$$q(z)-mixed Markov chain"" mean?
P. 2: two-way -> two
P. 2: by (richly) ... functions -> by a (richly) ... function
P. 2: ""inspirition-driven designs""?
P. 2: because marginal -> because the marginal
P. 3: ""are variables on a Markov chain"" sound grammatically incorrect to me
P. 4: markov -> Markov",2: by (richly) ... functions -> by a (richly) ... function P.,1,305
ICLR_2021_32,ICLR_2021,"+ Clarity: the paper is well-written and easy to follow.
+ Cross-domain few-shot classification is very relevant to the few-shot learning research community, and the availability of unlabeled data for the test domains is a plausible assumption.
+ The proposed approach is sound and straightforward.
+ The paper makes an effort at explaining why STARTUP provides a performance improvement.
- The paper should be more rigorous when reporting which approach performs best in a given setting. Recommendation
I recommend acceptance. Overall the paper is clear, the problem being tackled is relevant to the research community, the proposed idea is sound, and evaluation is rigorous. My main concern has to do with the way in which best-performing approaches are reported, but that’s easily fixable in a subsequent version of the paper.
Detailed justification
I appreciate the quality of the writing. The proposed idea is straightforward and makes intuitive sense. The baselines chosen for comparison are reasonable.
The main concern I have is with the way in which results are reported in Table 1. I believe the authors bolded the best-performing entry in each setting without taking the 95% confidence intervals into account. As an example, can we say that STARTUP performs significantly better than Transfer in the ChestX 5-way 1-shot/5-shot settings when their 95% confidence intervals overlap as much as they do? In my opinion the more rigorous way to determine that would be to run a 95% confidence statistical test on the difference between the means and bold all entries for which the test is inconclusive in rejecting the hypothesis that the difference in mean to the best-performing entry is zero, like is done in the Meta-Dataset paper. This doesn’t change the main conclusions drawn by the paper, but should nevertheless be addressed. Questions
When training the student model, would there be a benefit to compute the loss as the KL-divergence between the label distributions output by the teacher and student models instead of the cross-entropy with the true label? Have the authors investigated this?
The proposed approach has the downside that it requires training and storing a separate student model for each target domain. Have the authors thought of more compact approaches, like domain-conditional self-training?
Additional feedback
The abstract mentions evaluating on a ""challenging benchmark with multiple domains"" but does not name it (BSCD-FSL). This is information that would be helpful to the reader.
The insistence on ""several hours of compute"" in the introduction as being a drawback of current recognition systems could be toned down. To me, training a model for several hours doesn’t sound unreasonable.
The Visual Task Adaptation Benchmark (VTAB) and Big Transfer (BiT) papers would be relevant to mention in the related work section. In particular, the VTAB paper investigates various representation learning strategies (including self-supervision) when transferring between very different base and novel domains.
The strategy to use logits on the label set of the base dataset as targets (and therefore leveraging the natural groupings induced by the base dataset classifier) reminds me in a way of Ngiam et al. (2018)’s work on ""Domain Adaptive Transfer Learning with Specialist Models"", which uses this to inform the weighting of classes used to pre-train a target domain-aware classifier on the base dataset. This paper’s proposed approach differs in many ways, but the similarities would be interesting to expand upon.",+ The paper makes an effort at explaining why STARTUP provides a performance improvement.,1,311
ICLR_2022_3247,ICLR_2022,"Weakness
Despite the contributions outlined above, I believe all of the following concerns need to be addressed in order for this research to be accepted.
A) A mismatch between motivation and contribution
The contributions provided by this study do not address the motivation as stated in the introduction. In the introduction, the authors raise the problems related to constructing positive and negative pairs in CL as the motivation for this study. Specifically, the following three points are mentioned as challenges in CL:
definition of semantically similar/dissimilar pair is contingent on downstream tasks,
practically data augmentation is used for a positive pair, but it still has a problem; although positive samples are valid, negative samples contain a non-negligible portion of invalid samples (class collision),
and the computation of positive and negative pairs grows quadratically with the size of the dataset.
Even though the authors state that they propose their method to examine these challenges, none of these challenges are addressed in Method/Results/Conclusion. Instead, they continue to use existing approaches that have the issues raised in the introduction. The authors should clearly state their contribution to 1) the definition of similar/dissimilar pairs independent of downstream tasks, 2) validity (especially for class collision) in negative samples, and 3) computation time issues mentioned in the motivation of this paper.
B) Derivation of the correspondence between NCA loss and contrastive loss
The derivation of Eq. 9 from Eq. 7, which is the main technical contribution of this study that leads to the correspondence between NCA loss and contrastive loss, has several concerns. The NCA uses all dataset except its own { x k : k ≠ i }
to evaluate the loss for the i
-th data. This study makes an assumption of { x k : k ≠ i } = { x j + : 1 ≤ j ≤ M } ∪ { x i − : 1 ≤ i ≤ N }
on this set to derive the contrastive loss from the NCA loss. I have two concerns about the validity of this assumption.
This assumption implies that the entire dataset can be divided into M
“positive” samples and N
“negative"" samples. However, experimentally the positive/negative samples are subsampled from the entire dataset during the loss evaluation. In fact, in the numerical experiments in the second half of the paper, the authors treat M
as a hyperparameter, which is a significantly smaller number than the dataset size, so this assumption does not hold.
The assumption implicitly assumes that positive and negative samples are mutually exclusive: { x j + } ∩ { x i − } = ∅
. However, in reality, negative samples are typically taken from random samples in the same minibatch for computational simplicity, which leads to collision with positive samples. The numerical experiments in this study treat the same, and again the assumptions are not supported also from this perspective.
Since the correspondence between NCA loss and contrastive loss is the main technical contribution of this study, the validity of the assumptions for its derivation directly relates to the validity of the contribution of this study itself. The authors should address the above two concerns and clarify the validity of the assumptions made in this study.
C) Concerns about experiments
C-1) There is a gap between the technical contribution of this study (proposal of loss variant using NCA) and the experimental contribution (performance improvement including unified use of existing methods). Although this study claims to have given a unified framework for CL from the perspective of NCA and to have shown improved performance in numerical experiments, the loss variants from the perspective of NCA and the unified framework that brings together existing methods and loss variants are mutually independent. The following three effects are thought to contribute to the experimental performance improvement separately:
The effect of NCA-derived loss variants (choice of L Na ),
the effect of combining existing methods (e.g., use of adversarial examples),
and the effect of heuristics with low relevance to the main technical contribution (choice of w ( x ) ).
Because these three effects were examined at once without clearly separating them, it is not clear how large the effect of the loss variant derived through the lens of NCA is. For example, the proposed method $\mathcal{L}\mathrm{IntNaCL} h a s p a r a m e t e r s \alpha , c h o i c e o f \mathcal{L}\mathrm{Na} , M , \lambda , g^1 , g^2 , a n d
w$. By setting these parameters, loss variants, including existing methods, can be treated in a unified manner. Since there is no description of the parameter settings of the proposed algorithm in Figure 1, it is not clear which of the three effects this performance improvement depends on. The authors should examine each of these three effects independently and separate the contributions in the text.
C-2) Concern that the effect of the number of observed samples has not been removed. The authors claimed the contribution of $\mathcal{L}\mathrm{VAR} w h e n M>1 a s t h e e f f e c t o f N C A − d e r i v e d l o s s v a r i a n t . T h e a u t h o r s s t a t e d t h a t t h e \mathcal{L}\mathrm{VAR} h a s t h e p r o p e r t y o f b e i n g a b l e t o l o w e r t h e v a r i a n c e o f t h e e s t i m a t o r . H o w e v e r , i t i s n o t f a i r t o n a i v e l y c o m p a r e t h e c o n t r a s t i v e l o s s c o r r e s p o n d i n g t o M=1 a n d t h e \mathcal{L}_\mathrm{VAR} f o r M>1 i n t h e s a m e f i x e d e p o c h , b e c a u s e e x p e r i m e n t a l l y , t h e m o d e l c a n o b s e r v e d i f f e r e n t p o s i t i v e s a m p l e s i n t h e a m o u n t p r o p o r t i o n a l t o M . S u p p o s e o n e w o u l d l i k e t o h i g h l i g h t t h e e f f e c t o f t h e d i f f e r e n c e i n t h e f u n c t i o n a l f o r m o f t h e l o s s e s p u r e l y . I n t h a t c a s e , t h e a u t h o r s s h o u l d u s e l o n g e p o c h s w h e r e t h e o p t i m i z a t i o n c o n v e r g e s s u f f i c i e n t l y o r m a k e a d j u s t m e n t s s o t h a t t h e n u m b e r o f p o s i t i v e s a m p l e t y p e s o b s e r v e d d o e s n o t c h a n g e f o r d i f f e r e n t
M$ for a fair comparison. In fact, the authors have also trained the existing method with a longer epoch, and the results show that the existing method is comparable to the proposed method when trained with a longer epoch.
Minor comments x adv
in Eq. 6 is not defined explicitly. “ g = g 0
” in the left-hand side for the definition of $\mathcal{L}\mathrm{VAR} , \mathcal{L}\mathrm{BIAS} , a n d \mathcal{L}_\mathrm{MIXUP} ( p .6 ) i s n o t c l e a r s i n c e “
g$” does not appear on the right-hand side of these equations.
The undefined variable x i 2 j −
is in the right-hand side of the definition of L MIXUP
in p.6.","2) validity (especially for class collision) in negative samples, and",1,340
ICLR_2022_2881,ICLR_2022,"Weakness
The additional VAE model has to be trained until convergence in each aggregation round of federated learning. This causes non-trivial computational overhead to the overall FL training process. The paper fails to provide the complexity analysis from either empirical or theoretical prosepectives.
The paper assumes that the unknown variables follow a certain prior distribution without sufficient discussion to justify the choices. For instance, the prior of \lambda_k is set to be a Beta-distribution, which is known to be computationally hard to compute. Thus, the authors rely on the Kumaraswamy's method to derive an approximate solution. In addition, an ablation study about the choice of priors may help understand the choice.
The paper essentially replaces the distribution of local datasets with the distribution of means and standard deviations of the feature maps and their shifted variants while citing existing works to support the feasibility of such process [1, 2, 3, 4]. In this work, two distributions must be correlated closely with the mapping between them ideally being bijective. However, [1, 2, 3, 4] use normalization techniques to accelerate the training of neural networks and have no discussion about the relationship between two distributions. The authors should provide more detailed discussion about the relationship and error bound between the distance of the data distributions and the distance of the corresponding feature map distributions of the datasets. Namely, verify whether the close feature map distributions imply or guarantee the close data distributions.
The authors fail to provide the convergence analysis for federated learning.
References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. ""Layer Normalization"". In: CoRR abs/1607.06450 (2016). [2] Sergey Ioffe and Christian Szegedy. ""Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"". In: Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015. Ed. by Francis R. Bach and David M. Blei. Vol. 37. JMLR Workshop and Conference Proceedings. JMLR.org, 2015, pp. 448–456. [3] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. ""Instance Normalization: The Missing Ingredient for Fast Stylization"". In: CoRR abs/1607.08022 (2016). [4] Yuxin Wu and Kaiming He. ""Group Normalization"". In: Int. J. Comput. Vis. 128.3 (2020), pp. 742–755.","37. JMLR Workshop and Conference Proceedings. JMLR.org, 2015, pp. 448–456. [3] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. ""Instance Normalization: The Missing Ingredient for Fast Stylization"". In: CoRR abs/1607.08022 (2016). [4] Yuxin Wu and Kaiming He. ""Group Normalization"". In: Int. J. Comput. Vis. 128.3 (2020), pp. 742–755.",1,344
ICLR_2022_3352,ICLR_2022,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.","+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.",1,351
ICLR_2022_2007,ICLR_2022,"However, there are several key limitations that need to be addressed before the paper can be recommended for acceptance: 1. Architectural details – While the network diagrams and descriptive text do a good job in providing a high-level overview, the lack of details make it difficult to evaluate the architecture in depth. For instance, a couple of questions come to mind: * Do all historical input features need to be known in the future? The problem formulation is confusing here as x and x’ are R^{* x M} which seems to imply identical lengths T and number of features T – despite the text mentioning x’ is from T to T+tau. * How are dimensions modified in each layer of the network? As the downsampling/upsampling parallels to U net appear to be a key part of the model, details on how this is performed is important. * What are the keys, queries and values used for each attention layer (ProbSparseAttn, MaskedAttn, ProbSparseCrossAttn), and how is ProbSpraseCrossAttn implemented concretely? * What is the length of the Conv1d filters in the various blocks, and are they purely linear transformations? Do dimensions change between each transformation? * Is masked self-attention essential in the Y-Future encoder, and any reason why ProbSparse is not preferred? Does this affect computational efficiency, given that forecasting horizons appear to be larger than history lengths in many experiments from Appendix E.2? 2. Related works -- While the authors do a good job of citing models for LSTF, the paper lacks references to modern neural forecasting architectures, many of which are attention-based and show improvements over LogTrans [2, 3] and DeepAR [1-3]. While computationally more inefficient, they also contain similar modifications to those proposed by the YFormer. For instance, [2, 3] use of distinct encoding mechanisms for historical inputs, future inputs, and static variables -- all of which are fed into a common attention-based decoder. In addition, [1] also trains the network using past targets as a regulariser (backcast). Comparisons to these models would help to further motivate the YFormer architecture as well. 3. Benchmarks -- Given the focus on LSTF, comparison to simpler architectures that allow for extended receptive fields, e.g. dilated convolutions with WaveNet, would be useful. This is particularly important for time series datasets, which can be prone to overfitting with complex models -- as shown by the short-horizon outperformance of DeepAR on the ECL dataset in the Informer paper.
Typos 1. DeepAR is also mentioned as a benchmark, although results are not included in the paper.
References 1. Oreshkin et al. N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING. ICLR 2020. 2. Lim et al. TEMPORAL FUSION TRANSFORMERS FOR INTERPRETABLE MULTI-HORIZON TIME SERIES FORECASTING. International Journal of Forecasting, Volume 3 Issue 4, 2021. 3. Eisenach et al. MQTRANSFORMER: MULTI-HORIZON FORECASTS WITH CONTEXT DEPENDENT AND FEEDBACK-AWARE ATTENTION. Arxiv 2020.",3. Eisenach et al. MQTRANSFORMER: MULTI-HORIZON FORECASTS WITH CONTEXT DEPENDENT AND FEEDBACK-AWARE ATTENTION. Arxiv 2020.,1,363
ICLR_2022_1123,ICLR_2022,"Regarding equation (6) and ""We approximately ignore the projection operator at each iteration as the step size α
is expected to be small"", the theoretical justification does not seem well-supported. First, according to the experiment section, α
is not small enough compared to ϵ
. Indeed, if the initial random perturbation ρ
is not equal to 0, the perturbation can get ""stuck"" on the pertubation set boundary after a few steps. If ρ
is close to ϵ
, the perturbation boundary can even be reached after one step. Hence, we cannot ignore the projections. Second, if we cannot ignore the projections, we lose the grid structure (like in Figure 1) used in the rest of the analysis to determine the algorithm. I might be wrong for both points but it would require some clarifications.
Regarding the sign consistency experiment. Could the high value for the sign consistency be due to the perturbation being ""stuck"" on the perturbation set boundary? If yes, the fact that the projections which were ignored in order to come up with the algorithm would be the responsible for the high value of the sign consistency. That would go against ""The proposed method I-PGD is based on the hypothesis that due to the neighborhood constraint by adversarial attacks, the probability p
that a specific element in the input obtains the same sign in two consecutive iterations would be high and stable"".
The theoretical approach boils down to a simple result ""{0 : 1 − p, 2 : p}"" for I-PGD2-AT and ""{1 : 1 − p, 3 : p}"" for I-PGD3-AT. There might be other approaches leading to this simple result without making the assumptions discussed above which need clarifications. It could just be an heuristic approach such as: PGD leads to perturbations close to the boundary or closer to the random start. This last sentence translates into your result ""{0 : 1 − p, 2 : p}"" and would just require a sweep over the probability p
. Such a sweep is done in Table 3 for I-PGD2-AT and it would be great to have the same for ""{1 : 1 − p, 3 : p}"" with I-PGD3-AT.
The ablation study on the probability p
in the appendix should have been in the main paper, more than the catastrophic overfitting discussion.
Minor. There are a few typos in the paper. Page 5 and in the appendix: {0 : 1 − p, 2 : 1} -> {0 : 1 − p, 2 : p}. In Section 4.6, the subscripts of ""I-PGD2-AT"" are lacking compared to Figure 5.
Minor. Why doing the catastrophic overfitting study in Figure 4 on Fast-AT rather than on the proposed method?
Minor. In Figure 5, you should add a curve ""PGD10 Acc on PGD10-AT"" to the plot for a more complete comparison.","1 − p, 2 : p}"" for I-PGD2-AT and ""{1 :",1,387
ICLR_2022_1971,ICLR_2022,"Weakness: Method:
1. Verification and Optimization:
The proposed HNPF method is for verification (e.g., check whether a given solution x is Pareto optimal), but not for optimization (e.g., find an (approximate) Pareto solution x). It needs an extra search method, such as random sampling in this work, to first generate a large number of feasible solutions to cover the whole search space. Therefore, the underlying optimization is indeed random sampling (independent from HNPF), which could be extremely inefficient for a non-trivial search space. It is not suitable to put and compare the proposed HNPF method with other optimization methods that can directly find the (approximate) Pareto solution.
Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems.
2. The Reason to Build the Model:
HNPF needs to first build a neural network to check whether a given solution x satisfies the Fritz-John Condition (FJC), which requires a large number of training samples (e.g., 11K for a two-dimensional problem). The learned model is mainly used to classify whether the extra randomly sampled solutions (e.g., 9K) are weak Pareto optimal or not. The reason for model building, such as the advantage over the simple FJC rule-based classification, is not well motivated and justified in this work.
The proposed Pareto filter in stage 2 is not discussed and compared with other related nondominated sorting algorithms (e.g., [2]).
3. Necessary Condition for Pareto Optimality:
The KKT[3] and FJC[4] are two types of first order necessary conditions for (local) Pareto efficiency (Pareto optimality). In my understanding, the multi-objective optimization based MTL algorithms mentioned in this work (Sener & Koltun, 2018; Lin et al., 2019a; Mahapatra & Rajan, 2020; Ma et al., 2020; Navon et al., 2021) mainly use the gradient-based multi-objective optimization methods (e.g., MGDA) [5-7], which is based on the KKT condition. For these methods, in each update step, the gradient can be written as a linear combination of the gradient for each objective with adaptive weights derived from the KKT condition. Therefore, they are all different from the simple linear scalarization with fixed weights. All the claims and analyses in this work that the previous works use simple linear scalarization is not correct.
The FJ condition is also for local Pareto convergence, similar to the KKT condition. The global convergence property is solely due to random sampling that only works for extremely low-dimensional problems. It is unfair to say the proposed algorithm can overcome the local convergence of other gradient-based methods. In addition, the proposed algorithm heavily depends on the Fritz-John condition, but the original work [4] is not cited.
4. Linear Scalarization and Convex Pareto Front:
It is well-known that the simple linear scalarization cannot find the non-convex part of the Pareto front [8]. This finding leads to the seminal work on NBI scalarization (Das & Dennis.,1998), which is indeed one fundamental work that inspires the proposed method in this work (section 4, first sentence). The claim ""it is incorrect to state that LS itself fails if the Pareto front is non-convex"" (appendix, page 15) is questionable.
Since the proposed HNPF can only verify whether a given solution is weak Pareto optimal, its ability to find the whole Pareto front totally depends on the extra sampling method (e.g., random sampling) to generate all the Pareto solutions (might be infinite). It is misleading to indicate the proposed HNPF method itself can find the whole Pareto front. In addition, since the Pareto set has measure zero and infinite cardinality, the random sampling + HNPF method can at most find a dense approximation to the Pareto set. Experiment:
5. Algorithms for Comparison:
All the multi-objective optimization based MTL algorithms are designed for optimizing a deep neural network with millions of parameters. They implicitly depend on the assumption that the deep neural network has good properties (e.g., no bad local optimum [9][10]) on its loss functions, which is consistent with other gradient-based single-objective optimization methods. They are not designed to find the global Pareto front for low-dimensional problems.
For low-dimensional problems, it is more suitable to compare with the model-free multi-objective optimization methods such as the multi-objective evolutionary algorithm [11,12] and multi-objective CMA-ES [13]. If the model building is allowed, Multi-Objective Bayesian Optimization (MOBO) algorithms can have a very good sampling efficiency for the low-dimensional problems [14,15]. It is also very common to conduct non-dominated filtering at the end of those model-free algorithms or MOBOs (e.g., only keeping the current non-dominated solutions).
6. Training + Sampling:
The proposed method needs to first sample 11k solutions to train the neural network model, then randomly generate extra 9K solutions for filtering. Is there any advantage over simply using FJC to filter 9K (or 11k + 9K) randomly sampling solutions?
7. Figure from Other Works:
Many figures in the main paper and the appendix are directly borrowed from other works. I think this is not appropriate even the credits are given to the original works. Reference:
[1] Ruchte, Michael, and Josif Grabocka. Multi-task problems are not multi-objective. arXiv preprint arXiv:2110.07301, 2021.
[2] Roy, Proteek Chandan, Kalyanmoy Deb, and Md Monirul Islam. An efficient nondominated sorting algorithm for large number of fronts. IEEE transactions on cybernetics 49, no. 3: 859-869, 2018.
[3] Kuhn, H. W., and A. W. Tucker. Nonlinear Programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 481-492. University of California Press, 1951.
[4] Da Cunha, N. O., and E. Polak. Constrained minimization under vectorvalued criteria in finite dimensional spaces. Journal of Mathematical Analysis and Applications, 19(1), 103–124 ,1967.
[5] Fliege, Jorg, and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research 51, no. 3: 479-494, 2000.
[6] Fliege, Jorg, and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on SQP techniques. SIAM Journal on Optimization 26, no. 4: 2091-2119, 2016.
[7] Desideri, Jean-Antoine. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique 350, no. 5-6: 313-318, 2012.
[8] Das, Indraneel, and John E. Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems. Structural optimization 14, no. 1: 63-69, 1997.
[9] Kawaguchi, Kenji. Deep learning without poor local minima. NeurIPS 2016.
[10] Kawaguchi, Kenji, and Leslie Kaelbling. Elimination of all bad local minima in deep learning. AISTATS 2020.
[11] Deb, Kalyanmoy, Amrit Pratap, Sameer Agarwal, and T. A. M. T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, no. 2: 182-197, 2002.
[12] Zhang, Qingfu, and Hui Li. ""MOEA/D: A multiobjective evolutionary algorithm based on decomposition."" IEEE Transactions on evolutionary computation 11, no. 6: 712-731, 2007.
[13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no. 1: 1-28, 2007.
[14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020.
[15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.","6: 712-731, 2007. [13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no.",1,391
ICLR_2022_1924,ICLR_2022,"While I found Section. 3 to be well written, but Section 4, which contains the math and equations, seems to be difficult to follow, especially for a person that is not necessarily familiar with VAEs and generative models. Perhaps a simple introduction of those could help.
An issue of the paper, from my perspective, is that the experiments are simulated and not real. Therefore, I am not fully convinced that the developed method will have practical impact. In particular, is it possible to test the method on some dataset that is well-known to be corrupted by systematic outliers (rather than simulating the outliers)? One example that I am aware of, is that, in robot perception, wrong correspondences is a big challenge. For example, in point cloud registration, establishing reliable matches between a pair of point clouds is a very challenging task. There is a huge amount of literature on this problem, but perhaps [ref1] and [ref2] could be good references. This maybe a future work direction for the authors to consider.
I also think an ablation study could help, where the impact of the size of the trusted set on the performance could be investigated. If the authors increase the size of the trusted set from <10% to 50%, do we expect a gain in the performance?
[Ref1] Yang, Heng, Jingnan Shi, and Luca Carlone. ""Teaser: Fast and certifiable point cloud registration."" IEEE Transactions on Robotics 37, no. 2 (2020): 314-333.
[Ref2] Yi, Kwang Moo, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, and Pascal Fua. ""Learning to find good correspondences."" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2666-2674. 2018.","2 (2020): 314-333. [Ref2] Yi, Kwang Moo, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, and Pascal Fua. ""Learning to find good correspondences."" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2666-2674. 2018.",1,392
ICLR_2022_2446,ICLR_2022,"W1. Unclear how bias model is implemented: In prior works, such as LearnedMixinH and Rubi for VQA, the bias-only model takes question-only model as a biased model which assumes that if an answer could be guessed only using the question, then that sample can be assumed to be biased. There isn't a straightforward parallel for this in color-MNIST. For color-MNIST, REPAIR proposes to train a classifier using only the RGB value of the input. It is unclear how 1) the biased model is trained for the proposed algorithm, 2) How were they trained for Rubi?
W2: Serious doubts about ""scalability"" and assumption about known bias: There are various questions about the ""scalability"" of the proposed method. One common theme across these datasets is that they can be ""learned"" (at least the biased version) with a much smaller amount of data than is present in the training set. Hence, a rejection sampling-based method can work even when the minority set (m) diminishes. Next, the proposed algorithm falls under the category called variously as ""supervised"", ""known"", or ""explicit"" bias mitigation algorithm. In other words, the algorithm assumes knowledge about which of the factors were biased so that a suitable ""bias-only"" model can be trained by leveraging only the ""bias"". Such methods can be unscalable to all but a single, well-defined type of bias. This could be addressed by demonstrating the algorithm's success in more difficult scenarios (https://arxiv.org/abs/2104.00170).","1) the biased model is trained for the proposed algorithm,",1,396
ICLR_2023_3624,ICLR_2023,"The paper is not very well written that some parts of it is hard to read.
1.1. Notation issues: 1) It should be α λ , β λ , γ λ ∈ R
in eqn (3); 2) Why is b
bolded in eqn (4)(5)(6)(7)(8) but not bolded before and after?
1.2. The paper is not self-contained that several techniques are used without introduction. For example, what is deep ensemble strategy? What is expected improvement? They seem to play an important role in the algorithm, but readers do not know what they are without referring to literature.
The empirical studies are puzzling.
2.1. What is average normalized regret? Why is it an indicator of the efficacy of the HPO algorithms?
2.2. I think a more convincing setting would be the following: 1) Choose a task with randomly initialized hyperparameter; 2) Apply DPL and other HPO methods; 3) Compare the final results (e.g. accuracy) of different methods; 4) Repeat these experiments to many tasks (NLP, CV etc.). After all, what we really care is the ultimate performance on the tasks.",3) Compare the final results (e.g. accuracy) of different methods;,1,401
ICLR_2023_3692,ICLR_2023,"One thing I'm debating with myself is the novelty of this paper. If we view Eq. (2) more generally, it has the form P o u t = P i n ω where ω
is the weighting function. It seems to me that this is a general formulation of many recent step-wise controllable generation methods, including Dexperts and GeDi. Both Dexperts and GeDi rely on discriminators to guide generation. To improve computation efficiency, it is natural to discard discriminators entirely and also avoid fine-tuning. One of the only ways to achieve controllable generation without discriminators and fine-tuning is 1) define attributes with keywords, and 2) impose the control at every generation step. Both these two ideas are not new; 1) appears at least in PPLM (in topic control where each topic is defined with a list of keywords) and 2) is the approach taken by at least Dexperts and GeDi. Both 1) and 2) are used in earlier methods such as [1] (see for example Section 2.2 therein). Therefore, on one end, I think the proposed method does not add much new ideas to the existing line of work in (step-wise) controllable generation. On the other hand, even with the general form above, the design of the weighting function ω
still requires careful design, which is the focus of the present paper. The proposed form, e.g., applying a tangent function on the exp-sum of attribute-related words (tokens), is an interesting way I haven't seen before.
The other concern I have is on the experiments. The authors demonstrate improved performance over PPLM and CTRL, in particular on efficiency, which is kind of expected. However, I am also curious about the comparison of the proposed approach with other inference-time controllable generation methods, such as Dexperts and GeDi. The proposed method would outperform these two methods on efficiency for sure (albeit less significantly so as compared to PPLM and CTRL), but I'm interested in the trade-off between efficiency and performance - it would make a very interesting case if the proposed approach achieves similar performance in terms of the evaluation metrics that the authors have already considered with improved efficiency compared to other inference-time controllable generation methods. Such comparison is currently missing from the experimental results.
[1] https://aclanthology.org/P17-4008.pdf",2) is the approach taken by at least Dexperts and GeDi. Both,1,416
ICLR_2023_1071,ICLR_2023,"Since most of the experiments focus on benchmarking, the merits of the paper rest on XTRA’s performance. While it shows promising results, I also wonder if the results are as positive as described in the paper. First, how much more data than EfficientZero does XTRA require when taking pretraining into account (I couldn’t find those numbers in the main text)? Second, the confidence intervals are quite large when transferring to similar tasks (see Fig. 6: Carnival, DemonAttack, Alien, Amidar, Bankheist, MsPacman, Wizard of Wor) — are the reported results significant? It’s also surprising that XTRA does better relative to EfficientZero when downstream tasks are dissimilar from pretraining (see Table 2, confidence intervals) — could the authors comment on this point, and describe how the confidence intervals are computed in this setting?
Depending on the answer to those questions, I wonder if this paper isn’t reporting negative results for cross-task RL transfer.
A more minor point: I wish the paper included better baselines — not just ablations of the proposed method. For example, how does XTRA perform against on-policy alternatives such as Actor-Mimic or Policy Distillation (e.g., on top of DrQ-v2)? In a similar vein, the authors claim their task-weighting scheme yields the same benefits as gradient surgery (see 3.2) but when is that empirically demonstrated?
One question re “Most improvements happen within the first stages of training.”: Do the authors have any insight on why that’s the case? From Fig. 8 it seems that the dynamics are the reason for this boost (but we’re missing “h+f” on this figure). Is it because the dynamics already have a good initialization (i.e., don’t move too much during finetuning) or is it because the initialization lets them quickly move through parameter space (i.e., large movement at first, but not much after)? Details:
What is the definition (formula) of s i
? I’m assuming it’s Eq. 2 but please clarify.
3.1, p. 3: “tasks is however a difficult” → remove a.
3.1, p. 4: you could cite Parisi et al.’s “Actor-Mimic”, and Rusu et al.’s “Policy Distillation”.
3.1, p. 4: ( π ^ , u ^ , z ^ )
should be ( π ^ , v ^ , z ^ ) ?
3.2, eq. 1: The sum should be ∑ i m
, right?
4.1, p. 7: “our proposed frames” → “our proposed framework”?
Table 2, p. 7: \cite{} → \citep{} for Ye et al., 2021.
4.1, p. 8: “This results indicates that (i) selecting […] and (2) in the […]” → enumeration consistency.",8: “This results indicates that (i) selecting […] and (2) in the […]” → enumeration consistency.,1,417
ICLR_2023_1797,ICLR_2023,"There are some paragraphs that are poorly written, and make some key concepts difficult to understand.
There are some conceptual gaps that make me doubt about the overall rigor of the paper.
The more complex M U S T a d a p t i v e
model is only slightly better than the simpler M U S T u n i f o r m
baseline as measured by the in-domain automatic and human evaluations.
Suggestions and questions for the authors.
Good work! Do you intend to release the code in case the paper is accepted?
In the second paragraph of page 2 you mention that ""Extensive experimental results [...] show that the dialogue system trained by our proposed MUST achieves a better performance than those trained by any single user simulator"", however I feel like the amount of experimentation described is not extensive, so I would suggest toning down this statement.
In page 4 you introduce M U S T C R L
but you don't include that model in any of your results. In the paragraph ""Challenges to leverage multiple user simulators"" you mention some challenges related to this model. Are these challenges the reason why you did not analyze it? If that is the case then I suggest not giving it its own item (II) above, and instead give an overview of the ""adaptive"" variant of your framework. If this is not the case, then why was it not included?
In the same ""Challenges to leverage multiple user simulators."" paragraph you mention: ""unnecessary efforts will be costed for easily-adapted user simulators"". I suggest rephrasing this statement as I don't think it's understandable in its current form.
In the first paragraph of page 5 you link the ""uniform adaptation"" to reducing the catastrophic forgetting issue. However there is no experiment providing evidence that without ""uniform forgetting"" there's catastrophic forgetting. I suggest rephrasing this, or providing evidence that this is actually the case.
In the first paragraph of section 5.2.2 you mention: "" S y S − M U S T m e r g i n g
is trained by G P T I L
for implementing M U S T m e r g i n g
strategy"". First of all the statement is redundant by mentioning the ""merging"" strategy twice, but more importantly, I had understood that the merging strategy implied sampling dialogues from a set of user simulators and use these to train the final model. Does this statement mean that you generated dialogues with G P T I L
, which was trained on dialogues produced by 3 agenda-based simulators and MultiWoZ restaurant data, and define this method as sampling dialogues from several simulators?
In the ""Automatic Evaluation"" paragraph of section 5.3 you say that the reason for the ""merging"" strategy not performing as well as the ""uniform"" and ""adaptive"" could be ""because the merging strategy cannot effectively leverage multiple user simulators"" which is a cyclical explanation. I suggest removing it or providing a more plausible explanation.
In the third paragraph of section 5.3 you mention that the ""uniform"" and ""adaptive"" strategies achieve 2.4 absolute value improvements"", but improvement over what?
Finally, I suggest providing more details about the human evaluation. In Appendix B.2 you give some more details and mention that you ""tell them how to judge if the generated dialogue is successful"", ""them"" being the evaluators. But what was your definition of a successful dialogue? Every slot filled, for example? or something else?
Typos and minor corrections
Page 1, paragraph 4, line 3: best-performed -> best-performing
P. 2, p. 1, l. 10: You reference challenges i and ii
but you used 1 and 2 in the same paragraph for describing challenges. In the abstract you use i and ii
for referring to the two types of adaptation rather than the challenges they address. I suggest being consistent with numbering.
P. 2, p. 2, l. 5: Here you mention "" M U S T a d a p t i v e
is indeed more efficient for leveraging multiple user simulators by our visualization analysis"", which makes me wonder: more efficient than what? and what visualization analysis do you mean? I suggest clarifying this in the paper.
P. 2, p. 3, l. 8: convergences -> converges
P. 2, p. 6, l. 4: if accomplishing -> if it is accomplishing
P. 3, p. 1, l. 1: Once the database result -> When the database result
P. 4, p. 2, l. 1: first sample -> first samples
P. 4, p. 2, l. 4: by RL algorithms -> with RL
P. 4, p. 4, l. 11: Not sure what is meant by ""unnecessary efforts will be costed for easily-adapted user simulators"". I suggest rephrasing this.
P. 4, p. 6, l. 1: recalls us a similar thought -> reminds us of a similar concept
P. 4, p. 6, l. 3: weakly-performed -> weakly-performing; well-performed -> well-performing
P. 4, p. 6, l. 4: ""should reduce the interaction with user simulators that dialogue system has performed well and allocate more interactions with those user simulators that dialogue system has not performed well"" -> ""should reduce the interaction with user simulators with which the dialogue system has performed well and increase interactions in the opposite case.""
P. 5, p. 2, l. 5: masker's -> maker's
P. 5, p. 4, l. 7: ""p is expected to assign lower weights to user simulators that the system agent S already performs well and higher weights to those user simulators that S performs not well"" -> ""p is expected to assign lower weights to user simulators with which the system agent S already performs well and higher weights to those user simulators with which S performs poorly"".
P. 5, p. 7, l. 2: what do you mean with ""latter"" and ""former"" terms? Do you mean $\underbrace{\bar{x}j}{\text{exploitation}} + \underbrace{\sqrt{\frac{2\ln t}{T_{j,t}}}}_{\text{exploration}}$ in equation 2?
P. 5, p. 8, l. 2: has been interacted so far -> has been interacted with so far
P. 5, footnote, l. 2: ""Then the index of the arm will be played from t = K + 1 to T is the sum of two terms: ..."" makes no grammatical sense. Please correct it.
P. 6, Algorithm 1: I suggest changing the verbs from -ing form to imperative, so initializing -> initialize, synthesizing -> synthesize, using -> use, evaluating -> evaluate, updating -> update. I also suggest clarifying what the lowercase s mentioned in the input is used for.
P. 6, p. 1, l. 2-3: I think here you used both τ and s
to refer to the smoothing factor for distribution p
P. 6, p. 3, l. 3: ""that the dialogue system has performed well"" -> ""with which the dialogue system has performed well""
P. 7, p. 4, l. 4: ""test the systems by them"" -> ""test the systems with them""
P. 7, p. 4, l. 5: ""there usually has a gap"" -> ""there usually is a gap""
P. 9, Figure 2: There's no need to write ""Tested by"" for each subfigure. The name of the user simulator is enough. Also the label for (a) seems to be wrong.
P. 14: There's mention to U-GPT here, but nowhere in the main text.","1: first sample -> first samples P. 4, p. 2, l.",1,424
ICLR_2023_3811,ICLR_2023,"Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","3: U ^ 1 ′ l − 1 should be U ^ 1 ′ t − 1 i.e. the super-index of U ′ should be t − 1 not l − 1 P. 4, p. 5, l.",1,434
ICLR_2023_1888,ICLR_2023,"1.Lock of novelty. In contrast to DALL-E, this paper uses a BERT-like bidirectional encoding during the pretraining phase. And similar to BEiT-3, pretrain on both mono-modal and image-text pair data, and perform mask-and-reconstruct loss. 2.The reviewer thought the result in this paper was not very satisfactory.",2.The reviewer thought the result in this paper was not very satisfactory.,1,452
ICLR_2023_705,ICLR_2023,"Not really well-written (although well-organized).
I am a bit confused by some of the experimental results. The PCQM4Mv1 and MolNet benchmarks are somewhat okay. On FS-Mol, O
-GNN replaces a backbone transformer-based residual network. However, the results using "" O
-GNN without rings"" as a backbone are not shown, so it is not clear if the improvement is due to the use of GNN or because of the ring modeling. A similar pattern happens in the drug-drug interaction prediction task.
The theoretical analysis shows that few layers of O
-GNN are at least as expressive as many more layers of a regular-GNN. But this does not necessarily qualify as an advantage in practice, unless 1) O
-GNN layers uses fewer parameters than a regular GNN with the same expressivity; 2) O
-GNN is faster to train than a regular GNN with the same expressivity. To put it in a different way, why do I have to use few layers of O
-GNN, when I can get the same expressivity with many regular-GNN layers (and perhaps also using fewer parameters or being computationally faster)? I wonder if the authors can provide evidence that using O
-GNN is really more advantageous than using regular-GNN once the expressivity is similar.",1) O -GNN layers uses fewer parameters than a regular GNN with the same expressivity;,1,467
ICLR_2023_2537,ICLR_2023,"The method part is at different parts very hard to follow, and I found various parts confusing. Below is a list of points that I think need to be addressed: • p. 3, section 3.1: The lower primitive policy π L j
is introduced with superscript j
, where j
denotes ""an indicator of the current subgoal reaching capability of the lower primitive"". In my opinion this sentence does not sufficiently explain j
. The indicator j
is not further clarified, does not change in the pseudocode, and is dropped later (section 3.2). So why is the superscript j
needed? • p. 4, Definition 1: What is D T V
? I'm assuming total variation, but it is not introduced. • p. 4, The proof for the suboptimality of the lower primitive is missing. It can probably be derived from the proof for the upper policy, however, seeing that they differ with respect to lambda it is necessary to explain where this difference comes from. • p. 5, Eq. 10: What is ϵ
? This needs to be introduced. • p. 5, section 3.4: Why does Eq. 10 incentivize the higher policy to make ""reasonable progress towards achieving the final goal""? Does it not just incentivize generating subgoals that are hard to distinguish from the subgoals of dataset D g
? • p. 5, section 3.5: It reads like the off-policy RL objectives can be referred to as J D H and J D L
. Shouldn't the off-policy objective be J θ H H
and J^L_{\theta_L} of Eq. 11 and Eq. 12? Because J D H
is defined above as the IRL objective (Eq. 10). J D L
is not defined before. The authors need to be more precise about the different objectives. • p. 5, Eq. 11 & 12: λ
is here introduced as a hyperparameter whereas in section 3.2, λ
was used as a factor for the upper bound of suboptimality. To avoid confusion different names should be used.
From the experiments it is not clear how much the approach relies on the expert trajectories to learn to solve the tasks. How many expert trajectories were used for each of the two tasks? How were the expert trajectories generated? How does the number of expert demonstrations influence the system's performance? For example, how do fewer or more demonstrations affect the performance? Could the system deal with ""bad expert demonstrations"" that are not useful in solving the task? For example, what would happen when a couple of random rollouts are included in the demonstration dataset? I think these questions need clarification in the description of the experiments, further discussion, and maybe additional experiments.
What is the effect of resetting the subgoal buffer, i.e., the hyperparameter u
? How does u
affect the sample efficiency of the approach? An ablative study evaluating different values of u
would help understand how resetting the subgoal dataset affects the method.
How is CRISP-BC different to CRISP-IRL? Why does it perform better than CRISP-IRL in the Pick and Place task? Highlighting the difference between the two versions would also help understanding the approach.
The method requires that the simulator can be reset to an arbitrary state. This is a major restriction for applying the method. The authors acknowledge this limitation but a more detailed discussion on how this limitation could be weakened would improve the paper. For example, it prohibits applying the system on real robots. Thus, the claim of the abstract that the method “[…] is suitable for most real world robotic control tasks” is too strong.
Please fix the citation style. Use citet only of the citation is part sentence, grammatically (as a subject or object). Otherwise ,use \citep. Here an example: “Learning effective hierarchies of policies has garnered substantial research interest in RL Barto & Mahadevan (2003)” should be “Learning effective hierarchies of policies has garnered substantial research interest in RL (Barto & Mahadevan, 2003)”. Minor corrections and suggestions:
typo on p. 1: ""balnced""
bottom of p. 5: it should be Algorithm 2 instead of Algorithm 0
typo on p. 9: ""tn maze navigation""
Figure 1 should be referenced in the text.
introduce the abbreviation IRL on p. 3 when first mentioning inverse reinforcement learning","• p. 5, Eq.10: What is ϵ ? This needs to be introduced.",1,469
ICLR_2023_4640,ICLR_2023,"Weakness:
This proposed method is an incremental improvement based on existing works such as pseudo-label few-shot object detection and teacher-student semi-supervised object detection.
It is not fair to directly compare the proposed method with many existing methods because the proposed adaptive mining uses additional images where there is an implicit assumption that novel objects must be presented in these images even if they don’t need manual annotations. I think that the authors should report few-shot results using the same training images as other common few-shot object detection methods. In fact, there are unlabeled instances in few-shot training images because only a few instances are labeled and multiple instances are presented and unlabeled in training images. Furthermore, it is not clear how many additional images are used in the proposed method.
The recent few-shot methods report experimental results based on a common few-shot object detection setting and a generalized few-shot object detection setting. However, this paper only reports results using a generalized few-shot object detection setting.
The proposed method is more complex including more hyper-parameters ( α
, N, δ
, and β
) and fine-tuning stages (mined implicit novel instances and clear few-shot samples). It is not practical for real-world applications.
The hyper-parameter δ
appeared in different equations such as Eqs. 6 and 7. Please confirm if they are the same.",6 and 7. Please confirm if they are the same.,1,471
ICLR_2023_4759,ICLR_2023,"Weakness:
The concept of cosine distance seems misused. In Eq. (5), the reviewer suppose it was 1 − cos ⁡ ( f ( x s ) , M ) .
The sensitivity of hyper-parameters in the proposed method are not specified and discussed, e.g., δ
in Eq. (5), λ
in Eq. (6) and α
in Eq. (7), β
in Eq. (8). These hyper-parameters determine the effectiveness of the proposed method.
Many concepts are vague. This makes it hard to clearly understand the proposed method, for exmaple, 1) n i
in Eq. (2) denotes class id or a set of class samples? 2) In Eq.(5), I
denotes a set of examples that should be a set I
Some assumptions are not rigorously verified. For example, the authors assume that samples neear the mid-point of two protypes are hardest samples.
The authors try to find the outlier anchor using the protypes and correct them. The outliers are assumed as noise which will pull away the prototype. So, why do we keep these outliers? If these outliers can be identified, why do not delete these outliers instead of correct them?
There are some typos and format problems, for example, 1) the citation format in Introduction, 2) lambda --> λ
in Eq. (6); 3) ""We take the sphere with m"" --> M",1) n i in Eq. (2) denotes class id or a set of class samples?,1,475
ICLR_2023_47,ICLR_2023,"Clarity:
In the related works section, first paragraph, many italicized terms are used but not explained. I was familiar with all terms except primacy bias, which I had to look up separately while reading. But in general, these terms should be quickly defined to make the paper easier to read.
The first paragraph of 5.1.2 is written poorly and hard to read, please fix it. Experiments:
IQMs are listed (which is great) but the authors should probably explain IQM and the “Fraction of runs with score > τ
” statistics for readers unfamiliar with the benchmarking paper that introduced these metrics.
While the provided analysis on different algorithm design choices in light of high replay ratios is useful, I’m not sure that what the authors analyzed is the most sensible thing to analyze. The paper is about replay ratio scaling through resetting network parameters. Therefore, there should be analysis on two things: 1) different replay ratios, and 2) how/when to reset network parameters. The authors provide analysis of 1) through trying different replay ratios, but insufficient analysis of 2) as in Section 4 they define their reset strategies and just stick with it. I think the paper really should have this analysis to be more useful as an empirical study to inform design choices for other researchers.
In combination with the experiments in Section 5, it would be useful to have an experiment with an offline RL algorithm (perhaps IQL or CQL). Offline RL is increasingly popular and also being deployed on real robots, so the authors should study their replay ratio increases with an offline algorithm to make the paper more useful for researchers.
Minor details:
“The number of agent updates per environment step is usually called replay ratio, and most standard algorithms were designed to have values around or below 1” → not sure if they were designed to have values around or below 1, I think this statement would be more accurate if it was something like “most standard algorithms are trained with a replay ratio around 1.”
“under tasks switches” → “under task switches”
“with a same algorithm” → “with the same algorithm”
“for evaluation and comparisons, we follow the protocol suggested by Agarwal et a. (2021)” missing a period","1) through trying different replay ratios, but insufficient analysis of",1,476
ICLR_2023_3031,ICLR_2023,"- The technique seems to be in its early stages and it seems as if tuning needs to be performed for every new dynamical system and setting. First, the different training phases are difficult to follow and makes me wondering if the pipeline is robust enough. Also, please explain how was chosen to use a 4-layer U-Net for the Lorenz-63 and only a 3-layer U-Net for the Lorenz-96. - Network structure: it is not clear which losses are minimized at the different phases, in particular, the perturbator+flow operator is not clear. From eq. 11, it looks as if the dynamics and the data fidelity losses were applied on the same current state x_hat, while my understanding was that L_rec was calculated on the perturbator's output while L_dyn was calculated on the flow operator's output. If the hybrid loss is calculated on the final output of the 2 blocks, I don't se how we can enforce the decoupling of the two goals as stated. - It is difficult to understand the size of the input data and latent data. In particular, please give axis labels on Figure 1 images, as it first looks like a 2D spatial problem, while later it is explained that the input is of size time and location. Please also clarify what does T represent in Figure 2: I guess the time dimension, and in this case where is the location dimension? - Please explain best the following sentence: 'For the case in which the prior dynamics are unbiased': in a real setting, how can we know if the prior dynamics are biased or not? - 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive? - Why stopping at 8 and 10 blocks for the experiments, while it looks as if the results are always improving with more blocks?
Typos: Specifically, The perturbator
The perturbator uses the observations and labels it has learned to perturb the reconstructed states to make it deviate from the original flow --> please rephrase
Figure 3: the color do not match Figure 1, as here the colors are also linked to training/no training. Maybe use another sign to indicate training/no training, as a red line surrounding the box.",- 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive?,1,478
jvOvJ3XSjK,ICLR_2024,"- **The paper is missing important related work.** The main motivation for this method seems to be that it's hard to differentiate through the optimization problem for arbitrary optimization problems. However, there has been work in the predict-then-optimize literature that addresses this problem exactly. Specifically, Shah et al. [1] learn ``decision losses'' that learn the mapping from $\hat{\zeta} \to regret(x^\star(\hat{\zeta}), \zeta)$ (as defined in eq. (3)) to get around the issue of having to learn differentiable surrogates (there have also been more recent follow-ups [2,3] that make this more efficient). Even more generally, because you do not consider combinatorial optimization problems but rather continuous optimization problems, you could just use numerical differentiation methods to find the exact derivatives $\frac{\partial regret}{\partial \hat{\zeta}}$. For reasonable values of $|z| = 30, 50$ (as in this paper) along with some clever optimization tricks (e.g., warm-starting, GPU-acceleration, etc.) this may not be doable. There has also been a paper [4] that says you can get away with very simple surrogate gradients as well. All in all, these are just the tip of the iceberg. This space is not nearly as unexplored as the related work in this paper suggests, and it is important to compare LtOF to these alternate approaches.
- **The experiments are poorly designed and implemented.** I have several concerns with the experiments in this paper:
1. *Two out of three domains do not have reasonable baselines:* Nonconvex QP and AC-OPF compare only to two-stage and not any EPO alternatives. As this paper has noted, the fact that you can do better than two-stage is well-known at this point. The question is about whether you can do better than other reasonable EPO alternatives like [1].
1. *No comparison to prediction + pre-trained LtO:* One simple baseline would be to compare to optimizing over a pre-trained LtO model. In Figures 2 and 3 you argue that this is bad, but never show experimental results on it? If (a) you train the LtO model on the distribution of $\zeta$ and then (b) you pre-train the predictive model on a two-stage loss and then fine-tune it on a pre-trained LtO model, the distribution shift may not be too bad and perhaps it may not perform too badly? Is this the EPO with Proxy Regret baseline? It doesn't seem to be defined anywhere. Do you do any of the things suggested above to improve performance? How do you do hyperparameter tuning? Overall, this baseline seems to be a straw man.
1. *Other concerns:* Where are the error bars? There seems to be an overload of $k$ as a measure of complexity of the mapping of $z \to \zeta$ and also a measure of complexity for $F_\omega$. Often for EPO, it works better to have a simpler predictive model $F_\omega$ due to overfitting issues; have you tried this? Also, it is often useful to ""mix a little two-stage loss"" into the EPO objective. Have you tried this?
- **I don't think that the *idea* is particularly novel/interesting.** Taken together, I believe that you *could* choose to solve predict-than-optimize problems using LtOF, and even that you could do better than two-stage. However, as the paper has noted as well, this has been studied time and again. Moreover, the idea of using LtO to solve for $x^\star(\zeta)$ is not particularly novel, imo. I think the more interesting question is about whether you *should* do it in this way and when, and I don't think the paper has answered this question. Specifically:
1. *Generalization of LtOF to similar problems:* What happens if we want to change the number of stocks in our portfolio optimization problem at test time? In EPO, predictions are typically made per-stock $i$ using covariates $z_i \to \hat{\zeta}_i$. As a result, we can, for example, remove some of the stocks from our portfolio optimization problem at test-time without having to re-train.
1. *Being able to use the optimal solver at test time:* From Table 2, we can see that the regret before restoration for the AC-OPF domain is *very* different than after restoration. This suggests that LtOF is not able to learn how to make good decisions. As a result, it seems like it would be useful to be able to use an optimal planner at test-time. This seems even more true in, e.g., shortest path or ranking problems where there is a lot of structure in the constraints. This begs the question, when should you (a) predict the intermediate parameters, e.g., using prediction + LtO separately, versus (b) predict decisions directly from features?
1. *Interpretability/Fairness/Additional Considerations:* This is hard enough for prediction alone, does LtOF make this easier or harder in the predict-then-optimize case? References:
1. Shah, Sanket, et al. ""Decision-Focused Learning without Differentiable Optimization: Learning Locally Optimized Decision Losses."" NeurIPS (2022).
2. Zharmagambetov, Arman, et al. ""Landscape surrogate: Learning decision losses for mathematical optimization under partial information."" arXiv preprint arXiv:2307.08964 (2023).
3. Shah, Sanket, et al. ""Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize."" arXiv preprint arXiv:2305.16830 (2023).
4. Sahoo, Subham Sekhar, et al. ""Backpropagation through combinatorial algorithms: Identity with projection works."" ICLR (2023).","4. Sahoo, Subham Sekhar, et al. ""Backpropagation through combinatorial algorithms: Identity with projection works."" ICLR (2023).",1,543
7sMR09VNKU,ICLR_2024,"**Major**:
- The limitation acknowledged in section 5 (_""training our model requires having optimal trajectories""_) is very strong. It undermines quite a bit of the setup and background about learning from data being general, having a ton of applications, etc. Moreover, I don't believe this is even mentioned until the experiments section--it should be in the abstract.
- The experiments section is unconvincing and pretty weak. For how many thousands of learning algorithms have been thrown at the cartpole and inverted pendulum over the last 30+ years, comparing to a single algorithm is not an adequate baseline. There are many model-based and model-free approaches that have worked well on those two toy-ish problems; I'm not convinced that the authors' algorithm is required or SOTA here.
- A few experimental/implementation details have been swept under the rug -- the most prominent (unless I missed it) being the role of $T$ and $M$ -- how big do they need to be in theory, in practice, how do you set them.
- No code was provided (as far as I can tell in the reviewer console). **Minor**:
- Table 1 is not a good way to present this data (which isn't compared to the other alg.), the tolerances seem pretty loose as well.
- I have no idea what Table 2 and the related section are talking about. It seems like it is somehow implementing an MPC-style algorithm, but it's really unclear.
- The related work doesn't acknowledge almost anything written from about 1965 to 2005, and only slightly more pre-2015. The words ""system identification"" do not appear in the paper at all, which is concerning.",- No code was provided (as far as I can tell in the reviewer console). **Minor**:,1,551
BCRZq5nNZu,ICLR_2024,"- Originality
- Contributions are incremental and novelty is limited.
- Quality
- [Page-1, Section-1, Para-3] When a model sequentially learns over a sequence of datasets (in this case chunks) without having the measures of retaining the past knowledge it tends to forget the past learning as evident in the CL literature in both cases of homogeneous and heterogeneous data (chunk) distributions. Therefore, it is expected that the model performance will drop in such cases, hence the second claim is a valid expectation in such a setting and does not constitute a significant outcome of the analysis.
- In practical scenarios where task boundaries are not pre-known or specified, per-chunk weight averaging could easily worsen the model performance as averaging is done without consideration of the current chunk's domain/distribution as compared to the past chunks.
- Per-chunk weight averaging can be seen as the simplest form of knowledge aggregation technique in a continual learning setting. Hence, it reduces forgetting in the ideal (class-balanced) scenario which is the usual expectation from such techniques and does not constitute high significance as more sophisticated methods have already been developed in CL literature like weight regularization, data replay and incorporating additional parameters.
- The chunking setting described in the papers is ideal and data for such settings is also generated under simplified and impractical assumptions which will not scale to the online settings with changing data distributions in real-world scenarios.
- I am keen to hear the response of the authors on this and hope that they can change my point of view.
- Clarity
- It is difficult to keep track of the different data preparation techniques for ""offline SGD"", ""standard CL"" and ""chunking"" methods. It would be better to have clear algorithms and/or pictorial illustrations for the same.
- [Page-2, Section-2, Para-2] Please elaborate on the classification problems being referred to here.
- [Figure-2] The Font size is too small, please increase it and also add an explanation of the models shown in the legend (Move them from Appendix A to the Figure caption).
- Inconsistent use of terminologies. Please clarify the following terminologies (maybe in a tabular format) so that the reader can refer to them whenever required:
- ""offline learning""
- ""plain SGD learning""
- ""full CL setting""
- ""standard CL""
- ""online CL""
- Significance
- [Table-2] As ""per-chunk weight averaging"" strategy involves updating weight parameters. It would make sense to compare it with the existing ""weight regularization"" based CL strategies like the below methods:
- [EWC] -> Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan et al. ""Overcoming catastrophic forgetting in neural networks."" Proceedings of the national academy of sciences 114, no. 13 (2017): 3521-3526.
- [SI] -> Zenke, Friedemann, Ben Poole, and Surya Ganguli. ""Continual learning through synaptic intelligence."" In International conference on machine learning, pp. 3987-3995. PMLR, 2017.
- Typographical errors:
- [Page-1, Section-1, Para-1] ""thwart"" -> ""thwarted"", ""focuses CL"" -> ""focuses of CL""
- [Page-2, Section-2, Para-2] ""called called"" -> ""called""",- I am keen to hear the response of the authors on this and hope that they can change my point of view.,1,558
yaR0hqaGbI,ICLR_2025,"## Weaknesses
- **[Major]** Why are PDO and GlobalE presented as separate baselines? One minimizes the KL-divergence between the distribution of predicted labels on the probe set and a uniform prior and the other maximizes the entropy of the distribution. These are mathematically identical.
- **[Major]** What is the point of introducing the framework of V-usable information? It is easy to compute the KL divergence between the distribution of labels in your training set and the distribution of predicted labels directly. The problem stated in Eq. 1 can be solved by simply choosing the ordering that attains the lowest loss.
- **[Major]** What is the interpretation of $\log_2 P_{\text{LLM}}(a \mid \Pi(\mathcal{A}) \oplus \emptyset)$? From my understanding, you are simply concatenating a bunch of labels and prompting the LLM to get a probability score for the label of a probing example. However, it seems as though prompting the LLM in such a manner can result in gibberish since this type of prompt clearly is very contrived. Why not directly compute an empirical distribution on the true training examples?
- **[Major]** The framework proposed in this work fixes the order of the exemplars. However, the order should be **query dependent.** Based on the observation of [1], many ICL/RAG works order the exemplars based on their similarity to the query [2,3,4], but this is not discussed or used as a baseline in this work. Compared to LLM inference with long contexts, a simple similarity based reordering should be very cheap.
- **[Major]** It seems like there is no statistically significant difference in performance in the vast majority of the experiments...
- **[Minor]** What is the point of using LLM generate probes? I understand that this saves annotation cost, but in the many shot setting where we are already required to annotate ~150 examples, why not manually annotate a few extra samples to use as a validation set? I am skeptical of whether or not the validation set is reliable when it is purely synthetic.
- **[Minor]** The writing for Section 3 needs significant improvement. In its current state, one must repeatedly look at prior work in order to get any understanding of what the actual problem set up is. See comments below:
1. Many terms are used without being defined. For example, ""probing samples"" in Line 125 or ""optimal ignorance requirement"" in Line 184 are not terms that are used in the broader ML community, and should be properly defined.
2. What is $\pi(i)$ in line 194? This is never defined.
3. What is the difference between $\mathcal{A}$ and $A$ in Equation 4? Is this a typo?
4. It is not mentioned that the ""probing samples"" are LLM generated until line 199.
## References
[1] Lost in the Middle: How Language Models Use Long Contexts (https://arxiv.org/abs/2307.03172)
[2] In-Context Learning for Text Classification with Many Labels (https://aclanthology.org/2023.genbench-1.14.pdf)
[3] What Makes Good In-Context Examples for GPT-3? (https://arxiv.org/pdf/2101.06804)
[4] Retrieval-Augmented Generation for Large Language Models: A Survey (https://arxiv.org/pdf/2312.10997)",2. What is $\pi(i)$ in line 194? This is never defined.,1,565
oFIU5CBY9p,ICLR_2025,"* The write-up is quite hard to understand and the notation seems overwhelming at some points. I know the basics of denoising diffusion models, but I was not quite able to follow in what space the diffusion happens and how exactly it is mapped back and forth into the tabular representation. Figure 1 doesn’t help much to gain a better understanding. In particular, I would have appreciated a section which would detail the generation procedure. I do not understand where in Figure 1 noise is input to generate synthetic samples and in diffusion models there are different generation paradigms as well, such as latent diffusion etc. I do not think the write-up is very accessible in its current form.
* Related Work. This is not the first attempt to build a tabular foundation models trained on multiple datasets. Notable approaches include TabPFN (Hollmann et al., 2023). I also wonder how LaTable compares to other approaches (although mainly focused on classification), such as Yak et al. (2023) or Zhu et al. (2023). It is unfortunate, that these competing approaches are neither discussed nor compared in the evaluation.
* Conditional Generation / Classification is not evaluated. The authors describe how conditional generation can be implemented with LaTable, but do not test is as far as I see. This also trivally allows to use LaTable for the classification task (by conditioning on all tabular features and letting the model generate the label). Here, it would be insightful to compare LaTable’s performance to models such as TabPFN, XTab, or GREAT. Also in addition, zero-shot and fine-tuning with the entire dataset should be considered for a comprehensive evaluation.
* The evaluation only uses ML Efficiency (Train Synth., Test Real, TSTR). For a comprehensive picture, the performance of a model trained on the real data should be included as an upper baseline to assess the data quality gap in the TSTR table. In addition, there could be further data quality metrics, including metrics such as the Discriminator metric (e.g. used in Borisov et al., 2023) where a model is trained to differentiate between original and synthetic data and its performance is reported. Also some quantitative results could complement the evaluation.
The overall impression of the submission suggests that not much care was taken to prepare the current manuscript for submission and several important things have been neglected:
* The citation format is incorrect, citep is not used properly
* The Appendix and Supplementary materials are missing
* There are several formatting issues, e.g., Figure 2
* Text in formulas should be wrapped in \text, e.g. \text{softmax} (eqn. 1, 2)
* There is no code available.
While these points may be fixed, I think for submission at a respected venue such as ICLR more care should be taken.
**Summary.** Overall, the submission seems to be rushed and I think a thorough revision is needed. This should include a more accessible write-up, studying classification performance and additional data quality metrics, and comparing with other attempts for building large tabular models. Typos:
* Please check if “e.g.” should be followed by a comma.
* l. 237: transformer‘s
* l. 207 mathbb{R} instead of normal R
* Caption of Figure 2: Capitalization of LaTable ---------------- **References**
Zhu, B., Shi, X., Erickson, N., Li, M., Karypis, G., and Shoaran, M. XTab: cross-table pretraining for tabular transformers. In Proceedings of the 40th International Conference on Machine Learning (pp. 43181-43204), 2023
Yak, Scott, Yihe Dong, Javier Gonzalvo, and Sercan Arik. ""IngesTables: Scalable and Efficient Training of LLM-Enabled Tabular Foundation Models."" In NeurIPS 2023 Second Table Representation Learning Workshop. 2023.
Hollmann, Noah, Samuel Müller, Katharina Eggensperger, and Frank Hutter. ""TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second."" In The Eleventh International Conference on Learning Representations, ICLR 2023.
Vadim Borisov, Kathrin Sesler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. ICLR 2023.",* Please check if “e.g.” should be followed by a comma.,1,568
Z30Mdbv5jO,ICLR_2025,"- The lack of precise mathematical formulation raises doubt about proposition 1 and its proof:
- The main inequality in equation 17 (appendix) is justified only verbally and is not obvious to me.
- A counterexample for the inequality in line 841 could be a dataset mainly consisting of dark rooms with all ground truth renderings being black except for one where the lights are on (with possibly very complex geometry or even transparent and reflective materials). For this case, fitting the marginal distribution can be easier than fitting the conditional distribution, if the given scene s happens to be the one with lights on.
- Moreover, if the 3D structure (point cloud) is obtained from 2D images only via stereo reconstruction (DUSt3R [1]), then an image encoder should be theoretically able to encode the images in the same way as the composition of DUSt3R and the point cloud encoder used in the method. Following this argument, a strict inequality as in proposition 1 is not reasonable.
- Some lack of clarity:
- What exactly is the implementation of the learnable embedding function PosEmb in equation 3?
- What is meant with ""uncertainty of unconstrained images"" (line 265)? Does this mean that generated images are not fully 3D-consistent?
- Regarding the confidence-aware 3DGS optimization in section 4.4, it is unclear how the loss in equation 6 is still used / relevant for the final loss in equation 7, which does not use equation 6 anymore.
- Somewhat unfair comparison with baselines:
- The proposed method relies heavily on the strong priors learned by DUSt3R [1] and the video diffusion model.
- All baselines are trained more or less from scratch on small-scale datasets compared to the large-scale datasets for training video diffusion models.
- Therefore, it is not that surprising that the proposed method generalizes much better to other datasets out of the training/finetuning data distribution.
- Restriction to view interpolation:
- Given that the method makes use of a pre-trained video diffusion model, the restriction to interpolation of the camera trajectory between two input views is unsatisfactory.
- In many cases, two views as conditioning already eliminate uncertainty in the reconstruction task entirely such that deterministic approaches like pixelSplat [4] and MVSplat [5] already produce detailed and sharp reconstructions.
- The proposed generative approach with a strong prior trained on large-scale single view data should be able to extrapolate from input views to some extent, which the paper misses to evaluate.
- This is especially critical, if the paper motivates the method with the limitation of generalizable 3D reconstruction methods that ""struggle to generate high-quality images in areas not visible from the input perspectives"" (lines 143f.).
- Missing relevant related work and possible baseline:
- latentSplat [2] aims to bridge the gap between regression models for generalizable NVS and generative models for 3D reconstruction and is therefore a very relevant work and potential baseline.
- GeNVS [3] was one of the first approaches to generative novel views with a diffusion model conditioned on 3D-aware (pixelNeRF) features and is therefore important related work.
Minor comments:
- The paper sometimes uses the terms ""conditioning"" and ""guidance"" interchangeably, although they have different meanings to me (conditioning: giving something as additional input to the diffusion model; guidance: using CFG or also gradients (e.g. classifier guidance) to affect the sampling process). A more precise use of these terms would be helpful.
- The ablation study in table 3 could be extended to contain all different combinations of leaving out individual components in order to find possible dependencies between them. References:
- [1] DUSt3R: Geometric 3D Vision Made Easy. CVPR 2024
- [2] latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction. ECCV 2024
- [3] Generative Novel View Synthesis with 3D-Aware Diffusion Models. ICCV 2023
- [4] pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction. CVPR 2024
- [5] MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images. ECCV 2024",- GeNVS [3] was one of the first approaches to generative novel views with a diffusion model conditioned on 3D-aware (pixelNeRF) features and is therefore important related work. Minor comments:,1,587
bWT6OBJ71x,ICLR_2025,"- It seems that a version of this phenomenon was first pointed out for modular addition in Appendix K / Figure 21 in [1]. This is worth some discussion from the authors. In particular, while these additional experiments help strengthen this conjecture, it’s not clear to me what new understanding the ‘completeness hypothesis’ gives us.
- The ‘representation completeness hypothesis’ is not clearly formalized / or operationalized.
- For the toy tasks considered, initial embeddings for models that perform very well on the task (and whose final models are reasonably well-described by the task-metric-interpretation) have weaker completeness signals than other permutations (Fig 1 for the remainder task) or for arrangements (Figure 2 for XOR). “Significantly exceeds the average” does not seem, to me, like a strong enough result to validate this hypothesis.
- In a similar vein, the definition for the ‘correct’ idealized/interpretable representation is not obvious. The task metrics hypothesized in this work seem imperfect; some discussion on some of the failures of prediction is probably merited. In particular, whether the prediction error explained by an imprecise metric definition, idealized interpretation, or general messiness in the final model (and if the latter, why doesn’t completeness hold for real models).
- These experiments only motivate the hypothesis for toy algorithmic tasks (and not e.g. transformers trained on text). This should probably be noted.
- The varying dimensionality experiments seem under-motivated and a poor fit for the rest of the paper.
[1] Liu, Ziming, et al. ""Towards understanding grokking: An effective theory of representation learning."" Advances in Neural Information Processing Systems 35 (2022): 34651-34663.","- The varying dimensionality experiments seem under-motivated and a poor fit for the rest of the paper. [1] Liu, Ziming, et al. ""Towards understanding grokking: An effective theory of representation learning."" Advances in Neural Information Processing Systems 35 (2022): 34651-34663.",1,597
QfyZ28FpVY,ICLR_2025,"Please refer to “Questions” for Major Concerns.
**Minor points**
- In Fig. 1, the middle panel incorrectly depicts DNA barcodes binding to the target protein; it should show “building blocks” instead. A correct version can be found in (Shmilovich et al., 2023).
- In line 104, ZIP loss has not been defined.
- In line 147, the statement “While existing methods offer improved scalability and the ability to capture complex molecular interactions, they still face challenges in interpretability” is vague and needs further elaboration.","- In line 104, ZIP loss has not been defined.",1,629
b57IG6N20B,ICLR_2025,"**Major**
1. It seems the compression ratio only depends on the depth of the Encoder (Line 201). How about the effect of different configurations of RVQ? Could the authors provide some ablation studies over the configurations of RVQ (i.e., the codex size, the dimensions of each code, etc.)?
2. As AE+RVQ has a certain ability for representation learning, how about the classification performance from the quantized embeddings (after quantizer)? **Minor**
1. I’m not sure whether the amount of iEEG datasets is enough, as the SWEC iEEG dataset only contains 14 hours of recording (across 15 subjects). Additional publicly available iEEG datasets the authors should be aware of:
- Brain TreeBank (https://neurips.cc/virtual/2024/poster/97751): Their dataset (https://braintreebank.dev/) contains 43 hours of iEEG recording (not preprocessed).
- Du-IN (https://arxiv.org/abs/2405.11459): Their dataset contains 36 hours of iEEG recording (after bipolar reference).
To further prove the advantage of pre-training on clean data source (i.e., iEEG) -- **training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly**, could the authors provide results of the model pre-trained on both SWEC iEEG dataset and Brain TreeBank? Besides, could the authors provide the effect of different reference methods (e.g., laplacian/bipolar reference instead of median reference, Line 274)?
2. More downstream classification can be included, e.g., 4 classification tasks in BrainBERT[1] based on the Brain TreeBank dataset. **Reference**:
[1] Wang C, Subramaniam V, Yaari A U, et al. BrainBERT: Self-supervised representation learning for intracranial recordings[J]. arXiv preprint arXiv:2302.14367, 2023.",- Brain TreeBank (https://neurips.cc/virtual/2024/poster/97751): Their dataset (https://braintreebank.dev/) contains 43 hours of iEEG recording (not preprocessed).,1,631
NIPS_2016_279,NIPS_2016,"Weakness: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world. 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.","1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.",1,665
NIPS_2016_321,NIPS_2016,#ERROR!,"+ The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects:",1,674
NIPS_2016_374,NIPS_2016,"weakness is the presentation: - From my understanding of the submission instructions, the main part of the paper should include all that one needs to understand the paper (even if proofs may be in supplementary material). I thus found it awkward to have a huge algorithm listing as in Alg. 2, without any accompanying text explaining it, and to have algorithms in the supplementary material without giving at least a brief idea of the algorithms in the main body of the paper. This makes it hard to read the paper, and I think it is not appropriate for publication. - Perhaps something more should be done to convince the reader that a query of the type SEARCH is feasible in some realistic scenario. One other little thing: - what is meant by ""and quantities that do appear"" in line 115?","- what is meant by ""and quantities that do appear"" in line 115?",1,679
NIPS_2016_321,NIPS_2016,#ERROR!,+ The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case.,1,689
NIPS_2017_357,NIPS_2017,"- the manuscript is mainly a continuation of previous work on OT-based DA
- while the derivations are different, the conceptual difference is previous work is limited
- theoretical results and derivations are w.r.t. the loss function used for learning (e.g.
hinge loss), which is typically just a surrogate, while the real performance measure would
be 0/1 loss. This also makes it hard to compare the bounds to previous work that used 0-1 loss
- the theorem assumes a form of probabilistic Lipschitzness, which is not explored well.
Previous discrepancy-based DA theory does not need Prob.Lipschitzness and is more flexible
in this respect.
- the proved bound (Theorem 3.1) is not uniform w.r.t. the labeling function $f$. Therefore,
it does not suffice as a justification for the proposed minimization procedure.
- the experimental results do not show much better results than previous OT-based DA methods
- as the proposed method is essentially a repeated application of the previous work, I would have
hoped to see real-data experiments exploring this. Currently, performance after different number
of alternating steps is reported only in the supplemental material on synthetic data.
- the supplemental material feels rushed in some places. E.g. in the proof of Theorem 3.1, the
first inequality on page 4 seems incorrect (as the integral is w.r.t. a signed measure, not a
prob.distr.). I believe the proof can be fixed, though, because the relation holds without
absolute values, and it's not necessary to introduce these in (3) anyway.
- In the same proof, Equations (7)/(8) seem identical to (9)/(10)
questions to the authors:
- please comment if the effect of multiple BCD on real data is similar to the synthetic case ***************************
I read the author response and I am still in favor of accepting the work.","- In the same proof, Equations (7)/(8) seem identical to (9)/(10) questions to the authors:",1,715
NIPS_2017_217,NIPS_2017,"- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].
- ""Embedding"" is an overloaded word for a scalar value that represents object ID.
- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.
- Keypoint detection results should be included in the experiments section.
- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?
- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.
Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.","- ""Embedding"" is an overloaded word for a scalar value that represents object ID.",1,721
NIPS_2017_337,NIPS_2017,"of the manuscript stem from the restrictive---but acceptable---assumptions made throughout the analysis in order to make it tractable. The most important one is that the analysis considers the impact of data poisoning on the training loss in lieu of the test loss. This simplification is clearly acknowledged in the writing at line 102 and defended in Appendix B. Another related assumption is made at line 121: the parameter space is assumed to be an l2-ball of radius rho.
The paper is well written. Here are some minor comments:
- The appendices are well connected to the main body, this is very much appreciated.
- Figure 2 and 3 are hard to read on paper when printed in black-and-white.
- There is a typo on line 237.
- Although the related work is comprehensive, Section 6 could benefit from comparing the perspective taken in the present manuscript to the contributions of prior efforts.
- The use of the terminology ""certificate"" in some contexts (for instance at line 267) might be misinterpreted, due to its strong meaning in complexity theory.","- The appendices are well connected to the main body, this is very much appreciated.",1,731
NIPS_2017_560,NIPS_2017,"weakness, but this seems not to be a problem in most examples.
3. Equation 2.6 is wrong as written; as it does not make sense to divide by a vector. (easy to fix, but I surprised at the sloppiness here given that the paper well written overall).
4. Just for clarity, in eq 1.1, state clearly that F_\theta(x) is submodular in x for every \theta.
5. Can some nonconvex constraint sets which have an easy projection be handled as well?
6. What if the projection onto set K can be computed only approximately?",6. What if the projection onto set K can be computed only approximately?,1,737
NIPS_2017_390,NIPS_2017,"+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode, etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained. Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.
Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.","+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.",1,750
NIPS_2017_390,NIPS_2017,"+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode, etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained. Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.
Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.",- Why the zero-shot part specifically works so well should be better explained. Details:,1,756
NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","- p.5, l.164, ""must be null"" - should this be ""must be zero""?",1,758
NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","- For Figure 3, labels for the y axes are missing.",1,761
NIPS_2017_480,NIPS_2017,"and limitations.
Other comments:
* Section 2.1: maybe itâs not necessary to introduce discounts and rewards at all, given that neither are used in the paper?
* Section 3.1: the method for finding the factors seems very brittle, and to rely on disentangled feature representations that are not noisy. Please discuss these limitations, and maybe hint at how factors could be found if the observations were a noisy sensory stream like vision.
* Line 192: freezing the partitioning in the first iteration seems like a risky choice that makes strong assumptions about the coverage of the initial data. At least discuss the limitations of this.
* Section 4: there is a mismatch between these options and the desired properties discussed in section 2.2: in particular, the proposed options are not âsubgoal optionsâ because their distribution over termination states strongly depends on the start states? Same for the Treasure Game.
* Line 218: explicitly define what the âgreedyâ baseline is.
* Figure 4: Comparing the greedy results between (b) and (c), it appears that whenever a key is obtained, the treasure is almost always found too, contrasting with the MCTS version that explores a lot of key-but-no-treasure states. Can you explain this?",* Line 218: explicitly define what the âgreedyâ baseline is.,1,768
NIPS_2017_390,NIPS_2017,"+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode, etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained. Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.
Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.",+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.,1,772
NIPS_2018_857,NIPS_2018,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo? [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018. ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.","4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112?",1,799
NIPS_2018_685,NIPS_2018,"weakness of this paper, in my view, is that the main message is not very clear. The abstract emphasized both the novel characterization of \Phi-mixability and the AGAA. The characterization of \Phi-mixability, however, seems to suggest that generalizing the notion of mixability (at least in the direction of this paper) is not really helpful. The argument for the superiority of the AGAA is not strong enough, as I said in the preceding paragraph. I did not have the time to check the very long proofs. Other comments: 1. The presentation can be improved. This paper uses many terminologies and symbols without defining them first. For example, ""Vovk's exponential mixing"" in the first paragraph can be confusing to non-experts of online learning, and this confusion can be easily avoided by rewriting the sentence. The loss \ell is not defined before the appearance of R_\ell. The definition of H in (5) is missing. 2. The if and only if condition (9) looks similar to the condition proposed in the following two papers. a. ""A descent lemma beyond Lipschitz gradient continuity: First-order methods revisited and applications"" by Bauschke et al. b. ""Relatively smooth convex optimization by first-order methods, and applications"" by Lu et al. There is perhaps be some relationship. 3. ln 96, p. 3: The claim that ""we make no topological assumption on A"" is unnecessarily too strong. 4. ln 247, p. 6: The similarity with Vovk's work on the fundamental nature of the logarithmic loss is mentioned. However, this similarity is clear to me, as Vovk's result is about the choice of the loss function, instead of the definition of mixability. Please elaborate.","3. ln 96, p.3: The claim that ""we make no topological assumption on A"" is unnecessarily too strong.",1,804
NIPS_2018_695,NIPS_2018,"Weakness: a) There is no quantitative comparison between AE-NAM and VAE-NAM. It is necessary to answer that when one-to-many is not concerned, which one, AE-NAM or VAE-NAM should be used. In another word, the superior of VAE-NAM comes from V or AE? b) It contains full of little mistakes or missing references. For example: i. Line 31, and Line 35, mix use 1) and ii); ii. Line 51, 54, 300, 301, missing reference; iii. Equation 2): use E() but the context is discussing C(); iv. Line 174: what is mu_y? missing \ in latex? v. Line 104: gramma error? 3. Overall evaluation: a) I think the quality of this paper is marginally above the acceptance line. But there are too many small errors in the paper",3. Overall evaluation: a) I think the quality of this paper is marginally above the acceptance line. But there are too many small errors in the paper,1,855
NIPS_2019_1089,NIPS_2019,"- The paper can be seen as incremental improvements on previous work that has used simple tensor products to representation multimodal data. This paper largely follows previous setups but instead proposes to use higher-order tensor products. ****************************Quality**************************** Strengths: - The paper performs good empirical analysis. They have been thorough in comparing with some of the existing state-of-the-art models for multimodal fusion including those from 2018 and 2019. Their model shows consistent improvements across 2 multimodal datasets. - The authors provide a nice study of the effect of polynomial tensor order on prediction performance and show that accuracy increases up to a point. Weaknesses: - There are a few baselines that could also be worth comparing to such as âStrong and Simple Baselines for Multimodal Utterance Embeddings, NAACL 2019â - Since the model has connections to convolutional arithmetic units then ConvACs can also be a baseline for comparison. Given that you mention that âresulting in a correspondence of our HPFN to an even deeper ConACâ, it would be interesting to see a comparison table of depth with respect to performance. What depth is needed to learning âflexible and higher-order local and global intercorrelationsâ? - With respect to Figure 5, why do you think accuracy starts to drop after a certain order of around 4-5? Is it due to overfitting? - Do you think it is possible to dynamically determine the optimal order for fusion? It seems that the order corresponding to the best performance is different for different datasets and metrics, without a clear pattern or explanation. - The model does seem to perform well but there seem to be much more parameters in the model especially as the model consists of more layers. Could you comment on these tradeoffs including time and space complexity? - What are the impacts on the model when multimodal data is imperfect, such as when certain modalities are missing? Since the model builds higher-order interactions, does missing data at the input level lead to compounding effects that further affect the polynomial tensors being constructed, or is the model able to leverage additional modalities to help infer the missing ones? - How can the model be modified to remain useful when there are noisy or missing modalities? - Some more qualitative evaluation would be nice. Where does the improvement in performance come from? What exactly does the model pick up on? Are informative features compounded and highlighted across modalities? Are features being emphasized within a modality (i.e. better unimodal representations), or are better features being learned across modalities? ****************************Clarity**************************** Strengths: - The paper is well written with very informative Figures, especially Figures 1 and 2. - The paper gives a good introduction to tensors for those who are unfamiliar with the literature. Weaknesses: - The concept of local interactions is not as clear as the rest of the paper. Is it local in that it refers to the interactions within a time window, or is it local in that it is within the same modality? - It is unclear whether the improved results in Table 1 with respect to existing methods is due to higher-order interactions or due to more parameters. A column indicating the number of parameters for each model would be useful. - More experimental details such as neural networks and hyperparameters used should be included in the appendix. - Results should be averaged over multiple runs to determine statistical significance. - There are a few typos and stylistic issues: 1. line 2: ""Despite of being compactâ -> âDespite being compactâ 2. line 56: âWe refer multiway arraysâ -> âWe refer to multiway arraysâ 3. line 158: âHPFN to a even deeper ConACâ -> âHPFN to an even deeper ConACâ 4. line 265: ""Effect of the modelling mixed temporal-modality features."" -> I'm not sure what this means, it's not grammatically correct. 5. equations (4) and (5) should use \left( and \right) for parenthesis. 6. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite promising. Weaknesses: - Not really a weakness, but there is a paper at ACL 2019 on ""Learning Representations from Imperfect Time Series Data via Tensor Rank Regularizationâ which uses low-rank tensor representations as a method to regularize against noisy or imperfect multimodal time-series data. Could your method be combined with their regularization methods to ensure more robust multimodal predictions in the presence of noisy or imperfect multimodal data? - The paper in its current form presents a specific model for learning multimodal representations. To make it more significant, the polynomial pooling layer could be added to existing models and experiments showing consistent improvement over different model architectures. To be more concrete, the yellow, red, and green multimodal data in Figure 2a) can be raw time-series inputs, or they can be the outputs of recurrent units, transformer units, etc. Demonstrating that this layer can improve performance on top of different layers would be this work more significant for the research community. ****************************Post Rebuttal**************************** I appreciate the effort the authors have put into the rebuttal. Since I already liked the paper and the results are quite good, I am maintaining my score. I am not willing to give a higher score since the tasks are rather straightforward with well-studied baselines and tensor methods have already been used to some extent in multimodal learning, so this method is an improvement on top of existing ones.","- The paper is well written with very informative Figures, especially Figures 1 and 2.",1,860
NIPS_2019_1039,NIPS_2019,"Weakness: My main concern is about the problem itself. The proposed method uses ANN to accelerate NN query, and claims achieving some ""constant approximation"". The ""approximation"" here means that in each step the two merged clusters differ only by constant times of the minimum ""dissimilarity"". However, with such relaxation, the output hierarchical tree can be completely different from the exact solution. It is not clear from the paper how to compare the quality of these two hierarchical trees. Obviously, if the two trees are not close, the acceleration would not make much sense. Although the proposed algorithm runs faster, it may not achieve the desired goal. The paper does give some empirical evidence (Table 1) to suggest that the approximate solution is of ""good quality"". But the experiment is conducted only on a few small datasets and thus not very convincing. There are a few papers discussing how to define a suitable objective function for the HC problem (e.g. [1] and [2]). I would like to see some discussions and comparisons with those approaches. - [1] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. STOC'16 - [2] Vincent Cohen-Addad et al. Hierarchical Clustering: Objective Functions and Algorithms. SODA'17",- [1] Sanjoy Dasgupta. A cost function for similarity-based hierarchical clustering. STOC'16 - [2] Vincent Cohen-Addad et al. Hierarchical Clustering: Objective Functions and Algorithms. SODA'17,1,862
NIPS_2019_933,NIPS_2019,"+ I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.","- p13, l457: some notations seem to be undefined (w_t, W_t).",1,892
NIPS_2019_1338,NIPS_2019,", this paper is a solid submission. The idea is interesting and effective. It outperforms the state of the art. Strength: + The paper is well written and the explanations are clear. + The quantitative results (especially Table 2) clearly demonstrate the effectiveness of the proposed method. + Figure 1 is well designed and useful to understand the model. + Qualitative results in Figure 2 is convincing and demonstrates the consistency of the attention module across different classes. Weakness: - Motivation behind 3.2 Section 3.2 describes the cropping network that uses a 2d continuous boxcar function. Motivation for this design choice is weak, as previous attempts in local attention have used Gaussian masks [a], simple bilinear sampling using spatial transformers [b], or even pooling methods [c]. If this makes a difference, it would be great to demonstate it in an experiment. At minimum, bilinear sampling should be compared against. [a] Gregor, Karol, et al. ""Draw: A recurrent neural network for image generation."" ICML, 2015. [b] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" NeurIPS, 2015. [c] He, Kaiming, et al. ""Mask r-cnn."" Proceedings of the IEEE international conference on computer vision. 2017. - Discrepancy between eq. 9 and Figure 1. From eq. 9, it seems like the output patches are not cropped parts of the input image but just masked versions of the input image where most pixels are black. Is this correct? In this case, Figure 1 is misleading. And if so, wouldn't zooming on the region of interest using bilinear sampling provide better results? - Class-Center Triplet Loss The formulation of class-center triplet loss (L_CCT) is not entirely convincing. While the authors claim L2 normalization is introduced to ease the setting of a proper margin, this also has a different effect. This would in fact, divert the formulation to be different from the traditional definition of a margin. For example, these two points in the semantic feature space could be close, but far away after the normalization that projects them on a unit hypersphere. And the other way around is also true. Especially given the fact that the unnormalized version of phi is used also in L_CLS, the effect of this formulation is not obvious. In fact, the formulation resembles the cosine distance in an inner product, and the margin would be set -- roughly speaking -- on the cosine angle. The authors should discuss this in their paper. I find the current explanation misleading. - Backbone CNN Although I assume so, in Section 3.3 / Figure 1, it is not clear which backbone CNNs share their weights, and which don't (if some don't). Is the input image going through the same CNN as the local patches? Are the local patches going through the same CNN? I suggest some coloring to make it clear if not all are shared. - Minor issues L15: ""must be limited to one paragraph"". L193: L_CAT --> L_CCT Equation 11: it would be clearer with indices under the max function. L215: ""unit sphere"" -> ""unit hypersphere"". Unless the dimension of the semantic feature space is 3, which in this case should be mentioned. Potential Enhancements: * This paper is targeting zero-shot classification but since the multi-attention module is a major contribution by itself, it could have been validated on other tasks. An obvious one is fine-grained classification, on CUB-200 for instance. It is maybe possible for the authors to report this result since they already use CUB-200, but I would understand if it is not done in the rebuttal. ==== POST REBUTTAL ==== The additional results have made the submission even stronger than before. I am therefore more confident in the rating.",+ The paper is well written and the explanations are clear.,1,913
NIPS_2019_1397,NIPS_2019,"weakness of the manuscript. Clarity: The manuscript is well-written in general. It does a good job in explaining many results and subtle points (e.g., blessing of dimensionality). On the other hand, I think there is still room for improvement in the structure of the manuscript. The methodology seems fully explainable by Theorem 2.2. Therefore, Theorem 2.1 doesn't seem necessary in the main paper, and can be move to the supplement as a lemma to save space. Furthermore, a few important results could be moved from the supplement back to the main paper (e.g., Algorithm 1 and Table 2). Originality: The main results seem innovative to me in general. Although optimizing information-theoretic objective functions is not new, I find the new objective function adequately novel, especially in the treatment of the Q_i's in relation to TC(Z|X_i). Relevant lines of research are also summarized well in the related work section. Significance: The proposed methodology has many favorable features, including low computational complexity, good performance under (near) modular latent factor models, and blessing of dimensionality. I believe these will make the new method very attractive to the community. Moreover, the formulation of the objective function itself would also be of great theoretical interest. Overall, I think the manuscript would make a fairly significant contribution. Itemized comments: 1. The number of latent factors m is assumed to be constant throughout the paper. I wonder if that's necessary. The blessing of dimensionality still seems to hold if m increases slowly with p, and computational complexity can be still advantageous compared to GLASSO. 2. Line 125: For completeness, please state the final objective function (empirical version of (3)) as a function of X_i and the parameters. 3. Section 4.1: The simulation is conducted under a joint Gaussian model. Therefore, ICA should be identical with PCA, and can be removed from the comparisons. Indeed, the ICA curve is almost identical with the PCA curve in Figure 2. 4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data). 5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z). 6. Line 429: It seems we don't need Gaussian assumption to obtain Cov(Z_j, Z_k | X_i) = 0. 7. Line 480: Why do we need to combine with law of total variance to obtain Cov(X_i, X_{l != i} | Z) = 0? 8. Lines 496 and 501: It seems the Z in the denominator should be p(z). 9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments: 10. The manuscript could be more reader-friendly if the mathematical definitions for H(X), I(X;Y), TC(X), and TC(X|Z) were state (in the supplementary material if no space in the main article). References to these are necessary when following the proofs/derivations. 11. Line 208: black -> block 12. Line 242: 50 real-world datasets -> 51 real-world datasets (according to Line 260 and Table 2) 13. References [7, 25, 29]: gaussian -> Gaussian Update: Thanks to the authors' for the response. A couple minor comments: - Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials. - Regarding the Gaussian evaluation metric, I think it would be helpful to include the comments as a note in the paper.",8. Lines 496 and 501: It seems the Z in the denominator should be p(z).,1,915
NIPS_2020_1248,NIPS_2020,"1. The paper seeks to answer a set of important research questions, however, for some questions, the analysis is a bit complicated and not very convincing. See the detailed comments below. 2. Throughout the analysis, there are some sections that establish the solution without checking the premise or assumptions first. See the detailed comments below. 3. The writing is a bit hard to follow. See the comments in [Clarity].",3. The writing is a bit hard to follow. See the comments in [Clarity].,1,940
NIPS_2020_887,NIPS_2020,"- Part of the method (2), relies on having a list of orientation words. It's not clear how the orientation words are determined. The effect of k (for the number of top-k selected orientation words) is also not studied. - Most of the gains comes from components 2) and 3) and it's not certain how much of usefulness of the method is specific to this dataset. - The ablation study is not as thorough as it can be (it adds the components in order). Ideally, it would also show the effect of 2) and 3) without using 1) (with ResNet features instead of GloRe) and effect of 3) without 2) - The proposed method is very task and data specific and it is not clear whether it would be of interest to the broader NeurIPS community.","- The ablation study is not as thorough as it can be (it adds the components in order). Ideally, it would also show the effect of",1,943
NIPS_2020_663,NIPS_2020,"- The contribution of the paper is too marginal. DRO with KL ambiguity sets has been extensively studied. For example, the nonparametric case, see Faury et al ""Distributionally Robust Counterfactual Risk Minimization"" (AAAI 2020) and Si et al ""Distributionally Robust Policy Evaluation and Learning in Offline Contextual Bandits"" (ICML 2020). These papers propose explicit wort-case distributions, tractable algorithms for the modified robust ERM (via reweighting), and complete theoretical analysis thereof (consistency, sample complexity, etc.). - Moreover, in the particular parametric case of DRO on exponential families (the subject of this manuscript) has already been studied in Hu et al. ""Kullback-Leibler Divergence Constrained Distributionally Robust Optimization"" - The paper lacks solid theoretical background. Since everything is parametric, I'd expect explicit rates of convergence involvind all probalem complexity parameters (n, m, p, etc.) To make the rest of my points clear, let me recall the following notations are used in the paper: - n: the dimensionality of the covariate (i.e feature vector) X. Thus X is random vector in R^n. BTW, in the context of ML or stats, I'd use another notation here, as n conventionally stands for ""sample size"". - m: the dimensionality of the output variable Y. Thus X is a random vector in R^m. - p: the dimensionality of the ambient space in which the model parameter theta lives. - N: the number of samples in the training dataset - (x_1,y_1),...,(x_N,y_N): and iid sample of size N from the unknown joint distribution of X and Y. - C the number of distinct feature vectors x_i in the sample. Thus 1 <= C <= N. WLOG, tet the distinct feature vectors be x_1,...,x_C. - N_c (with 1 <= c <= C): #{i | x_i = x_c}, i.e number of examples whose features vector equals x_c. - Line 95 to 98: Since the covariates (i.e features) are continuous, we are certain to have C = N and N_c = 1 for all c. - Line 99 to 102: The model in (5) has C + dim(Theta)^C, which is rougly N + dim(Theta)^N parameters in case of continuous covariates (see previous comment). This cannot possibly work as soon as dim(Theta) > 1. - Proposition 4.2: This should be rewritten to clearly outline the dependence on the sample size N. Also, the third term on the right is mysterious. Also, at what rates do kappa_1 and kappa_2 go to zero (if they do...). - Line 227: I don't see how you let ""N_c tend to infinity"" in view of my above comment on N_c = 1 almost certainly (see my previous comments). The rest of the analysis (Lemma 4.4) is therefore awkward. - Why are these experimental setups presented in section 5 relevant to the subject ? - Documentation on the datasets in Table 2 should be provided (n = ?, m = ?, etc.).",- m: the dimensionality of the output variable Y. Thus X is a random vector in R^m.,1,964
NIPS_2021_1092,NIPS_2021,"Missing related work this paper is essentially trying to learn representations that are invariant to standard augmentations. As a result, this makes the proposed method very similar to self-supervised learning (SSL). For example, the same regulariser already appeared in [1]. This decreases somewhat the novelty of CR-VAE as these ideas have been well investigated for representation learning. From my perspective, this paper uses ideas from SSL to improve VAE’s. This is not inherently an issue as long as SSL is correctly discussed (note that lines 119-121 won’t be true anymore). Specifically:
[1]is very related to your method and uses the same regulariser. The key difference is that instead of reconstructing the image (learning p(x|z)) they use contrastive learning to maximize the mutual information I[Z, X]. This exemplifies well the relation between your method and SSL.
Most SSL methods aim to make representations of an augmented and unaugmented example very similar. For example [2] does that by minimizing the variance instead of KL divergence (see their Appendix A for relation to KL divergence). Similar ideas of learning approximately invariant representations go way back, e.g., [3] from 2006.
[4] seems to be another very related work (although online since the beginning of March so contemporary). In particular, the Variational Invariant Compressor (sec. 4.1.) is essentially a VAE that takes as input augmented inputs but reconstructs unaugmented inputs to ensure “consistency” of the encoder.
Lack of results showing the usefulness of the learned representations: an important motivation of the work seems to be about learning useful representations, yet empirical results do not really support this claim:
the experimental results of downstream linear classifications are extremely weak compared to typical SSL methods, which suggests that the learned representations of CR-VAE are not useful for downstream tasks. Indeed, the best CIFAR-10 accuracy in this paper is 71.4% while standard SSL with linear evaluation achieves more than 94% (e.g. SimCLR Appendix B.9.).
The authors evaluate representation learning using mutual information, but (1) they do not compare to typical representation learning baselines; (2) it is well known that higher mutual information does not mean better representations [5].
Somewhat incomplete experimental results: Given that this is a completely empirical paper (based on sensible intuition) I find the experimental section to be somewhat incomplete. Here are results that I would have liked to see:
(As previously said) a result that shows the usefulness of representations
Lineplot showing for one dataset (e.g. CIFAR10) the effect of λ
on the log-likelihood of CR-VAE, that would help to understand the robustness of the model to the additional hyperparameter.
Experiment analyzing the impact of the choice of augmentations on CR-VAE, to understand the importance of the choice of augmentations.
Standard errors for table 2/4/5/6. Especially table 5 as you claim SOTA.
Qualitative samples from a CR-VAE for 2D images to understand the effect of the regulariser.
Suggestions / specific issues
I’m not fond of your use of “consistency”, which means something very specific in (frequentist) statistics and the title could thus be understood as a method to ensure that the variational posterior converges to the true posterior (similar to [6]). I would at least consider using another word. I understand if the authors ultimately keep it given that it’s (unfortunately) the term used in semi-supervised learning.
The paper [7] is in the references but never actually cited in the text. Given that this paper shows comparable results on CIFAR10 and really shows the importance of data augmentation for density modeling, I think that it should be correctly discussed in your related work section.
Line 202 please explain in the text why more active units should be better Questions
Why did you choose the KL divergence in this direction? Please explain in the text, and ideally add an experimental comparison.
Another way of forcing “consistency” of the representations would be to augment the input but reconstruct the unaugmented examples (as in [4]), Have you considered this instead of your regulariser? I’d be curious to hear what you think, although I wouldn’t be surprised if “unaugmenting” the inputs is harder to do in practice.
Minor Suggestions
Line 18: state of the art performance -> state of the art log-likelihoods (I initially thought that SOTA performance was on downstream classification)
figure 1: I would add the sub captions for those that skim the paper: (a) VAE (b) VAE with augmentations (c) CR-VAE (ours)
Line 71 / 91 : e.g. -> e.g.,
Line 82: is unable -> may be unable
Line 148: will conduct -> conduct
Line 148 will apply -> apply
Line 185: Finally -> Finally,
Line 191: likelihood -> likelihoods
Equation 5 : KLs use different fonts
Line 198: metrics -> metric
Line 199/200: using different quotes on left and right of “active” References
[1] Federici, Marco, et al. ""Learning robust representations via multi-view information bottleneck."" arXiv preprint arXiv:2002.07017 (2020).
[2] Zbontar, Jure, et al. ""Barlow twins: self-supervised learning via redundancy reduction."" arXiv preprint arXiv:2103.03230 (2021).
[3] Hadsell, Raia, Sumit Chopra, and Yann LeCun. ""Dimensionality reduction by learning an invariant mapping."" 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). Vol. 2. IEEE, 2006.
[4] Dubois, Yann, et al. ""Lossy Compression for Lossless Prediction."" arXiv preprint arXiv:2106.10800 (2021).
[5] Tschannen, Michael, et al. ""On mutual information maximization for representation learning."" arXiv preprint arXiv:1907.13625 (2019).
[6] Wang, Yixin, and David M. Blei. ""Frequentist consistency of variational Bayes."" Journal of the American Statistical Association 114.527 (2019): 1147-1161.
[7] Jun, Heewoo, et al. ""Distribution augmentation for generative modeling."" International Conference on Machine Learning. PMLR, 2020.
As previously stated, the authors need to discuss the limitation of CR-VAE's representation compared to standard representation learning methods.
Potential societal impact is adequately discussed.","2. IEEE, 2006. [4] Dubois, Yann, et al. ""Lossy Compression for Lossless Prediction."" arXiv preprint arXiv:2106.10800 (2021). [5] Tschannen, Michael, et al. ""On mutual information maximization for representation learning."" arXiv preprint arXiv:1907.13625 (2019). [6] Wang, Yixin, and David M. Blei. ""Frequentist consistency of variational Bayes."" Journal of the American Statistical Association 114.527 (2019): 1147-1161. [7] Jun, Heewoo, et al. ""Distribution augmentation for generative modeling."" International Conference on Machine Learning. PMLR, 2020. As previously stated, the authors need to discuss the limitation of CR-VAE's representation compared to standard representation learning methods. Potential societal impact is adequately discussed.",1,1020
NIPS_2021_1027,NIPS_2021,"weakness of this paper is that the results are asymptotic (i.e., only holds when the number of samples approaches infinite). It is unclear how realistic are the phenomena described in this paper. For example, the convergence rate in Theorem 6 depends polynomially on the term 1/c_\mu, which is the minimum data density over all state-action pairs. For MDPs with large state/action space, the term could be much larger than the number of samples. In other words, it’s unclear whether the dominating term is still characterized by Eq. (6) in real-world OPE tasks.
In addition, this paper doesn’t directly tackle the difficulty of linear OPE tasks. On the one hand, the assumptions such as 1/c_\mu-dependence or upper bound of |F^#_\gamma|_2 exclude some natural hard instances. Are these assumptions an artifact of the analysis, or necessary for the result? On the other hand, the sample complexity of tile-coding estimators is exponential in the dimension d, which is not very realistic.
Minor issues which may cause unnecessary confusion: - The notation f* in Definition 8 and 9 is used to denote different functions. - Does Definition 3 depend on the policy \pi being evaluated (because the term (P\phi)(s,a) depends on \pi)?",- The notation f* in Definition 8 and 9 is used to denote different functions.,1,1021
NIPS_2021_998,NIPS_2021,"• Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear. Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work","• Unprofessional writing.- Most starkly, “policies” is misspelled in the title.",1,1022
NIPS_2021_695,NIPS_2021,"While reading the manuscript, I encountered some confusions that need authors' explanation: 1) Throughout the paper, it seems to me that the final output is x ^ ( y )
. However, the algorithm summarized in Alg. 1 seeks to update y
, which I assume to be related to the noisy observation. Is there a hidden step x ^ ( y ) = f ( y ) + y
for dumping the final output? or am I misinterpreting the variables? 2) Sampling y
seems quite unusual if my interpretation of y t
is correct. In [1], I saw their formulations are for x
. 3) I did not quite see why from the equation above eq 7 we can have eq 7. Could the authors provide some intuitive explanations? Derivations are also welcome.
A discussion between your method and [1] is needed.
Is it possible to construct a simple toy problem such that the oracle MMSE denoiser is reachable. In this way, we can compare CNN with this oracle denoiser, which may bridge the gap between theory and experiment. Can the experimental setup (Compressive Sensing with Bernoulli-Gaussian distribution) in [2] directly reusable to yours?
In experiments, deepRED is treated as the PnP baseline. Although RED and PnP are closely related, the exact equivalence between the two is still an open question. I suggest including one exact PnP baseline (PnP-ISTA or PnP-ADMM) in the experiments.
Minor: In line 245 Section 4, the sentence implies that PnP is coupled with ADMM. However, this is a long-standing misunderstanding of the PnP framework. Many works have shown that PnP is compatible with gradient-based [3], primal-dual [4], and message passing algorithms [5]. 2)
[3] Kamilov et al. A Plug-and-Play Priors Approach for Solving Nonlinear Imaging Inverse Problems
[4] Ono et al. Primal-Dual Plug-and-Play Image Restoration
[5] Metzler et al. Learned D-AMP: Principled Neural Network-Based Compressive Image Recovery",2) [3] Kamilov et al. A Plug-and-Play Priors Approach for Solving Nonlinear Imaging Inverse Problems [4] Ono et al. Primal-Dual Plug-and-Play Image Restoration [5] Metzler et al. Learned D-AMP: Principled Neural Network-Based Compressive Image Recovery,1,1031
NIPS_2021_1610,NIPS_2021,"Limitations are well defined, aside of that of point 1) above, which I feel is a critical flow, if not addressed properly.","1) above, which I feel is a critical flow, if not addressed properly.",1,1042
NIPS_2021_1759,NIPS_2021,"The extension from the EH model is natural. In addition, there has been literature that proves the power of FNN from a theoretical point of view, whereas this paper fails to make a review in this regard. Among other works, Schmidt-Hieber (2020) gave an exact upper bound of the approximation error for FNNs involving the least-square loss. Since the DeepEH optimizes a likelihood-based loss, this paper builds up its asymptotic properties by following assumptions and proofs of Theorems 1 and 2 in Schmidt-Hieber (2020) as well as theories on empirical processes.
Additional Feedback: 1) In the manuscript, P
mostly represents a probability but sometimes for a cumulative distribution function (e.g., Eqs. (3) and (4) and L44, all in Appendix), which leads to confusion. 2). The notation K
is abused too: it is used both for a known kernel function (e.g., L166) and the number of layers (e.g., L176). 3). What is K b
in estimating baseline hazard (L172)?",3). What is K b in estimating baseline hazard (L172)?,1,1047
NIPS_2021_1865,NIPS_2021,"and the limitation of the work are not discussed.
Clarity: The submission is clearly written and it is organized OK.
Significance: The results are important and probably the results will be used by other researchers or practitioners. It seems that it does advance the state of the art based on numerical experiments. It provides an environment based on a real-world dataset.
I enjoyed reading the paper. I have few comments:
Q1- In several places of the paper the term ""bi-level reinforcement learning (RL) based optimization framework"" is used. This is misleading since bi-level optimization is a known type of problem in mathematical programming in which the solution of an optimization problem is a constraint in the main problem (see the problem definition and the formulation in [1] as a recent paper in this field). This is not the case in the problem that you have defined. The algorithm that is defined, has two levels, which are commonly named hierarchical RL. I suggest revising the paper accordingly to avoid any confusion in the future.
Q2- Several recent papers are not missed in the literature review section. I would add and review the contributions of [3, 4, 5, 6]. For older papers see [2].
Minor comments: ""Although these methods mainly focus on TSPs or VRPs, of which all orders’ information is known in advance and much fewer constraints are considered comparing with DPDP, learning-based methods have shown great potential to help solve large-scale DPDPs and reach superior performance."" This is not correct. There are several works on RL for VRP which consider stochastic VRP problems in which the demands and their locations are not known a priori.
[1] Tahernejad, S., Ralphs, T.K. & DeNegre, S.T. A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation. Math. Prog. Comp. 12, 529–568 (2020). https://doi.org/10.1007/s12532-020-00183-6
[2] Berbeglia, Gerardo, Jean-François Cordeau, and Gilbert Laporte. ""Dynamic pickup and delivery problems."" European journal of operational research 202, no. 1 (2010): 8-15.
[3] Karami, Farzaneh, Wim Vancroonenburg, and Greet Vanden Berghe. ""A periodic optimization approach to dynamic pickup and delivery problems with time windows."" Journal of Scheduling 23, no. 6 (2020): 711-731.
[4] Ulmer, Marlin W., Barrett W. Thomas, Ann Melissa Campbell, and Nicholas Woyak. ""The restaurant meal delivery problem: Dynamic pickup and delivery with deadlines and random ready times."" Transportation Science 55, no. 1 (2021): 75-100.
[5] Su, Zhiyuan, Wantao Li, Jicao Li, and Bin Cheng. ""Heterogeneous fleet vehicle scheduling problems for dynamic pickup and delivery problem with time windows in shared logistics platform: formulation, instances and algorithms."" International Journal of Systems Science: Operations & Logistics (2021): 1-25.
[6] Györgyi, Péter, and Tamás Kis. ""A probabilistic approach to pickup and delivery problems with time window uncertainty."" European Journal of Operational Research 274, no. 3 (2019): 909-923.
There is no social impact and the limitations of the work are not discussed.",3 (2019): 909-923. There is no social impact and the limitations of the work are not discussed.,1,1055
NIPS_2021_952,NIPS_2021,"- Some important points about the method and the experiments are left unclear (see also questions below). - The writing could be improved (see also Typos & Additional Questions below) - Multiple runs and significance tests are missing. This makes it hard to judge the improvements (Table 2 & 3).
Most Important Questions - Line 156: What is q_ij^k here exactly? I thought q_ij was a state flag, such as “2” or “0”. But you tokenize it and encode it, so it sounds more like it is something like “Copy(snow)”? (If it is the latter, then what is the meaning of tokenizing and encoding something like “Len(9)”?) - 192: What exactly is storyline and what do you need it for? - The baseline takes the predicate logic constraints as input: How does T6 know what to do with these inputs? Was the model trained on this but without the NRETM module? Can you give an example of what the input looks likes? How do these inputs guide which sentences should be generated? Looking at the datsset, it feels like one would need at least the first 2 sentences or so to know how to continue. Maybe this information is now in your constraints but it would be important to understand what they look like and how they were created. Is there no other suitable baseline for this experiment? - What is the overhead of your method compared to standard decoding approaches? (you mention GBS can only be used with T5-Base, so your method is more efficient? That would be important to point out) - What happens if the decoding process cannot find a sequence that satisfies all constraint? - Document-level MT: How do you know at test time whether the system translates a particular sentence or not? - How many sentences are misaligned by Doc-mBART25? What are the s-BLEU and d-BLEU values on the subset that NRETM aligns correctly and Doc does not? - Why was NEUROLOGIC not used as a comparison baseline? - What is dynamic vs static strategy? In which experiment did you show that dynamic works better than static (from conclusion)?
Typos & Additional Questions - Line 40: you could mention here that the examples will be translated into logic forms in the next section. - Paragraph starting at line 53: Why did you choose these datasets? How will they help evaluate the proposed approach? - Line 75: a and b should be bold faced? - 83: “that used” -> “that are used” - 83: “details” -> “for details” - Paragraph at line 86: At this point, the state matrix is unclear. What are the initial values? How can the state matrix be used to understand if a constraint is satisfied or not? - 98: “take[s]” & “generate[s]” - 108: “be all” -> “all be” - Paragraph at line 101: What is dynamic vs static strategy? - Paragraph at line 109: The state flag explanation would greatly benefit from an example. Does q_i refer to whether a particular U_i is satisfied? - Eq 2: What is the meaning of N? Can it change depending on the definition of U_k? Does it mean this constraint is not relevant for x_i? - 133: Figure 1 should be Figure 2 - Figure 2: What exactly do the “&” rows track? - Figure 2: Is the state flag matrix equal to the state matrix? If not, how do you go from one to the other? - Line 146: What does the inf in the superscript signify? - 177: What is the symbolic operator? - Paragraph at line 194: Without understanding what a storyline is, it is not clear what the constraints are. An example might be helpful here. - Line 204: what is the ROUGH-L metric? Do you mean ROUGE-L? - Line 223: How do you obtain the morphological inflections for the concepts? - 237: @necessity [of] integrating” - 3.3: How exactly is the document-level MT done? Is the entire input document the input to T5? - 293: “because” typo - 3.4 where/how exactly is the sentence index used?
The paper's broader impact section discusses general potential benefits and issues of text generation (from large language models). It could maybe be tailored a bit better by discussing what effect this proposed work would have on the potential benefits and issues.",- Why was NEUROLOGIC not used as a comparison baseline?,1,1061
NIPS_2021_1952,NIPS_2021,"[Clarified in rebuttal] The evaluation and the methods proposed, while highly informative and useful, are computationally expensive.
The authors mention this in their supplementary material.
While the mixing time estimation is interesting a comparison with theoretical bounds, in particular [1] would have been more interesting.
Possible citations:
[2] could be cited as a more recent survey of training methods along with the citation on line 33
[3] was recently published in ICML and proposes a novel method to train RBMs which is unrelated to CD-k, a similar analysis of that method compared to Rdm-k would be very interesting.
[4] proposed a method to train RBMs using random walk tours which provide an almost unbiased estimate of the negative statistics
they additionally observed a similar effect where samples from shorter tours (k) demonstrated memorization and longer tours (k) demonstrated generalization, which is corroborated by the observations in the current paper.
[5] proposed using the up and coming unbiased couplings method to train EBMs including RBMs, since this method is scalable to larger RBMs it could be the subject of such a study.
Other criteria
Originality : Existing problem, marginally novel solution, exciting and novel results.
Quality : Extremely high quality of writing.
Clarity : Experiments are well justified, motivations are well explained, results are well elucidated.
Significance : High significance in terms of insight, prohibitive computational cost will hamper utility.
[1] - Tosh, Christopher. ""Mixing rates for the alternating Gibbs sampler over restricted Boltzmann machines and friends."" International Conference on Machine Learning. PMLR, 2016.
[2] - Song, Yang, and Diederik P. Kingma. ""How to train your energy-based models."" arXiv preprint arXiv:2101.03288 (2021).
[3] - Grathwohl, Will, et al. ""Oops I Took A Gradient: Scalable Sampling for Discrete Distributions."" arXiv preprint arXiv:2102.04509 (2021).
[4] - Savarese, Pedro, Mayank Kakodkar, and Bruno Ribeiro. ""From monte carlo to las vegas: Improving restricted boltzmann machine training through stopping sets."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.
[5] - Qiu, Yixuan, Lingsong Zhang, and Xiao Wang. ""Unbiased contrastive divergence algorithm for training energy-based latent variable models."" International Conference on Learning Representations. 2019.","32. No.1. 2018. [5] - Qiu, Yixuan, Lingsong Zhang, and Xiao Wang. ""Unbiased contrastive divergence algorithm for training energy-based latent variable models."" International Conference on Learning Representations. 2019.",1,1069
NIPS_2021_422,NIPS_2021,"Experimental results leave some questions open, i.e.: - One experiment to estimates the quality of uncertainty estimates measures how often the true feature importance lies within a 95% credible interval. However, the experiments uses pseudo feature importance because no true feature importance is available. The correctness of the pseudo feature importance relies on Prop 3.2 and a large enough perturbation value to be chosen. This makes it difficult to judge to what degree the experiment can be trusted because the difference between the tested method and the pseudo feature importance is only the number of perturbations. The experiment could be strengthened in two ways. 1. set up a (toy) dataset where true feature importance is clearly defined. 2. What are the results when choosing BayesSHAP N=10k perturbations vs BayesLIME N=10k? These should be nearly identica; otherwise the assumption of the pseudo feature importance being (nearly) equal to true feature importance is compromised. Results could also be reported for BayesSHAP N=100 vs BayesLime N=10k and vice versa. - Correctness of Estimated Number of Perturbations: How can you be sure that G doesn’t simply overestimate? Maybe a value a lot less than G would have been sufficient? - Human Evaluation Experiment: Users were asked to guess a number from an image where the explanation was masked. Figure 8 indicates that each user was given30 such images and line 230 suggests that all 30 images where a masked version of number “4”. This seems like an unbalanced setup which make it difficult to determine the meaningfulness of this experiment. Questions
User Study: Why did you decide to erase the explanations and measure failure to guess the correct image rather than keeping just the explanations and measuring success of guessing the correct image?
B.1: Why is it okay to make these assumptions?
Theorem 3.3: How do we know that S is large enough?
After Authors' Response All my questions and weakness concerns were appropriately addressed, therefore I raised my score.
A ""Broader Impacts and Limitations"" section could be added that discusses potential dangers of trusting explanations. It could for example discuss that the choice of N is important or that explanations can still be wrong.",1. set up a (toy) dataset where true feature importance is clearly defined.,1,1074
NIPS_2021_2445,NIPS_2021,"and strengths in their analysis with sufficient experimental detail, it is admirable, but they could provide more intuition why other methods do better than theirs.
The claims could be better supported. Some examples and questions(if I did not miss out anything)
Why using normalization is a problem for a network or a task (it can be thought as a part of cosine distance)? How would Barlow Twins perform if their invariance term is replaced with a euclidean distance?
Your method still uses 2048 as the batch size, I would not consider it as small. For example, Simclr uses examples in the same batch and its batch size changes between 256-8192. Most of the methods you mentioned need even much lower batch size.
You mentioned not sharing weights as an advantage, but you have shared weights in your results, except Table 4 in which the results degraded as you mentioned. What stops the other methods from using different weights? It should be possible even though they have covariance term between the embeddings, how much their performance would be affected compared with yours?
My intuition is that a proper design might be sufficient rather than separating variance terms.
- Do you have a demonstration or result related to your model collapsing less than other methods? In line 159, you mentioned gradients become 0 and collapse; it was a good point, is it commonly encountered, did you observe it in your experiments?
- I am also not convinced to the idea that the images and their augmentations need to be treated separately, they can be interchangeable.
- Variances of the results could be included to show the stability of the algorithms since it was another claim in the paper(although ""collapsing"" shows it partly, it is a biased criteria since the other methods are not designed for var/cov terms).
- How hard is it to balance these 3 terms?
- When someone thinks about gathering two batches from two networks and calculate the global batch covariance in this way; it includes both your terms and Barlow Twins terms. Can anything be said based on this observation, about which one is better and why? Significance:
Currently, the paper needs more solid intuition or analysis or better results to make an impact in my opinion. The changes compared with the prior work are minimal. Most of the ideas and problems in the paper are important, but they are already known.
The comparisons with the previous work is valuable to the field, they could maybe extend their experiments to the more of the mentioned methods or other variants.
The authors did a great job in presenting their work's limitations, their results in general not being better than the previous works and their extensive analysis(tables). If they did a better job in explaining the reasons/intuitions in a more solid way, or include some theory if there is any, I would be inclined to give an accept.","- Variances of the results could be included to show the stability of the algorithms since it was another claim in the paper(although ""collapsing"" shows it partly, it is a biased criteria since the other methods are not designed for var/cov terms).",1,1076
NIPS_2021_396,NIPS_2021,"Major points:
1- Confusing description: Authors claim to learn ""inter-relationship among clips"" (line 12) and state that other methods ""consider each clip-text pair separately"" (line 36), ""each clip-text pair is independently encoded"" (line 107). This sounds to me like they would combine several clip-text data points in some way and integrate knowledge between data points. However, the method samples multiple combinations of frames for the same data point, therefore I find these main claims confusing. E.g. ClipBERT simply states in its abstract that ""a single or a few sparsely sampled short clips from a video are used at each training step"" which I find to be a much more fitting description.
2- Effect of pre-training: Authors load pre-trained weights from ClipBERT and then pre-train again on TGIF-QA dataset. They claim that this has advantages (line 335), however, there is no ablation to support this claim: It would be helpful to see the results on MSRVTT-QA and MSVD-QA without pre-training on TGIF-QA dataset.
3- No comparisons to ClipBERT:
ClipBERT results are missing from Table 5 (Performance on TGIF-QA), while they are added in Table 3 (Performance on MSRVTT-QA and MSVD-QA) and no reason is given on why these results are missing. This is important as ClipBERT reports better performance on TGIF-QA than the paper and has a very similar setup (same model, no more than 1 clip during testing, see ClipBERT paper table 8 second-last row.)
The ablations in Table 1 should be additionally done either on TGIF-QA or on MSRVTT-QA and compared to the ClipBERT result described above.
4- Hyper-parameters: Authors use dataset-specific hyperparameters tuning (line 276) which could be another reason for some of the improvements over the ClipBERT method. What was the hyper-parameter tuning approach?
5- Table 1: This table reports the ablation study on the effect of SKG and SKR components. However, there is no result with SKG alone. Also, the authors don't report the mean and std of multiple experiments. Would be great if the authors report mean/std and also results with only SKG.
Given the above concerns, it seems that the main improvement in this paper comes from taking ClipBERT and pretraining it on TGIF-QA before applying it to other datasets. This is contradicting to the story which reasons that the improvements are due to the proposed new modules.
Minor points:
1- Motivation is missing as to why the method is evaluated exclusively on VideoQA and not on other tasks like e.g. Video Classification.
2- There are no details on pre-training and fine-tuning time complexity.
3- Architecture details are missing: ""multimodal transformer is applied to produce the clip-text feature"" (line 133). I would assume the output is a single feature vector obtained from the CLS token, however, this should be specified directly. Otherwise, it would be very hard to reproduce the paper.
Typos: The paper is difficult to follow. The structure of the paper needs to be improved and also there are lots of typos and grammar errors:
78 ""Not"" should be ""Note""
-Figure 3 sentence 1 ""to be extracted"" sentence is missing a proper verb.
129 ""all clips number"" is not a proper term, should be ""number of all clips"".
129: ""Each clip ... is uniformly sampled ... frames"" should be rephrased to ""frames are sampled per clip"" or similar.
136 ""dimension"" should be ""dimensions""
144 ""including"" is confusing here. Does this mean: siamese clips have semantics that are similar ""to"" the anchor clip?
153: ""predictions on their predicted class probabilities"": Duplicate ""predict"" is incorrect
225, 278: ""excavate"" means physically digging (e.g. with a shovel), would be better to replace it with ""use"" here.
364 ""Boarder"" should be ""Broader""
375 ""look"" should be ""looking""","129 ""all clips number"" is not a proper term, should be ""number of all clips"". 129: ""Each clip ... is uniformly sampled ... frames"" should be rephrased to ""frames are sampled per clip"" or similar.",1,1078
NIPS_2021_311,NIPS_2021,"- The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- What happens if a too small corpus is chosen? Can this be detected?,1,1079
NIPS_2021_159,NIPS_2021,"Weakness/questions:
PLUR covers diverse tasks, but there are some common tasks not included (e.g. code completion, C code repair like DeepFix). Is it because they do not fit the PLUG framework? It'd be great to discuss a bit more 1) what kinds of tasks are not included or do not fit in the current framework, and 2) if there is a prospect to cover more tasks by extending the PLUG framework.
Were there any technical challenges that needed to be addressed when developing the unified framework? If there were, it might be worthwhile to mention these insights in the paper, as they may help future work to build on the proposed framework. Typos/grammar:
L373: ""one risk is that""
The authors discuss limitations and broader impact of their work in L373-379.","1) what kinds of tasks are not included or do not fit in the current framework, and",1,1081
NIPS_2021_1994,NIPS_2021,"Future directions pointed out by the authors include 1) analyze the effect of fine-tuning to bring down the approximation error terms; and 2) study other possible conditions under which pretext tasks are helpful for downstream learning, since CI is sufficient but likely not necessary.
Societal impact is not applicable since this work is theoretical.",1) analyze the effect of fine-tuning to bring down the approximation error terms; and,1,1085
NIPS_2021_2445,NIPS_2021,"and strengths in their analysis with sufficient experimental detail, it is admirable, but they could provide more intuition why other methods do better than theirs.
The claims could be better supported. Some examples and questions(if I did not miss out anything)
Why using normalization is a problem for a network or a task (it can be thought as a part of cosine distance)? How would Barlow Twins perform if their invariance term is replaced with a euclidean distance?
Your method still uses 2048 as the batch size, I would not consider it as small. For example, Simclr uses examples in the same batch and its batch size changes between 256-8192. Most of the methods you mentioned need even much lower batch size.
You mentioned not sharing weights as an advantage, but you have shared weights in your results, except Table 4 in which the results degraded as you mentioned. What stops the other methods from using different weights? It should be possible even though they have covariance term between the embeddings, how much their performance would be affected compared with yours?
My intuition is that a proper design might be sufficient rather than separating variance terms.
- Do you have a demonstration or result related to your model collapsing less than other methods? In line 159, you mentioned gradients become 0 and collapse; it was a good point, is it commonly encountered, did you observe it in your experiments?
- I am also not convinced to the idea that the images and their augmentations need to be treated separately, they can be interchangeable.
- Variances of the results could be included to show the stability of the algorithms since it was another claim in the paper(although ""collapsing"" shows it partly, it is a biased criteria since the other methods are not designed for var/cov terms).
- How hard is it to balance these 3 terms?
- When someone thinks about gathering two batches from two networks and calculate the global batch covariance in this way; it includes both your terms and Barlow Twins terms. Can anything be said based on this observation, about which one is better and why? Significance:
Currently, the paper needs more solid intuition or analysis or better results to make an impact in my opinion. The changes compared with the prior work are minimal. Most of the ideas and problems in the paper are important, but they are already known.
The comparisons with the previous work is valuable to the field, they could maybe extend their experiments to the more of the mentioned methods or other variants.
The authors did a great job in presenting their work's limitations, their results in general not being better than the previous works and their extensive analysis(tables). If they did a better job in explaining the reasons/intuitions in a more solid way, or include some theory if there is any, I would be inclined to give an accept.",- How hard is it to balance these 3 terms?,1,1090
NIPS_2021_1637,NIPS_2021,"There is little insight into when the proposed method may break or may not work as well as other competing methods. How does the efficacy of the proposed method vary with the size of the validation dataset? When would the single-step approximation be assumed in eq. (9). Is there any relevant form of data augmentation that may be hard to parametrize in a differentiable way? If so and if such transformations were to be critical for some applications, then wouldn't the competing approaches based on discrete optimization be more preferable?
It is unclear what the benefits of the Bayesian approach are. For example, one could set the noise term η = 0
in eq.(13) and I suspect that the method may still work reasonably well (not shown in the manuscript). The noise injection may potentially improve the diversity of sampled augmentations, but I feel this should be shown to substantiate the benefits of the Bayesian approach.
Minor comments
line 103: ""differential"" => ""differentiable""
line 124: ""we pick k<< n for simplicity"" -> what do we lose from this assumption?
line 182: ""pseudo one-step update"" -> why pseudo?
line 204: ""second-order derivative … computed more efficiently"" => efficient in time or space?
line 232: no space after '.'
line 255: ""Tab. 3 and Tab. 2"" => Tab. 2 and Tab. 3
The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.","3 and Tab. 2"" => Tab. 2 and Tab. 3 The main limitations in my view are the lack of insights into the limitations of the proposed approach as elaborated above. NA for societal impact.",1,1095
NIPS_2021_311,NIPS_2021,"- The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- The paper leaves some natural questions open (see questions below).,1,1096
NIPS_2021_1251,NIPS_2021,"- Typically, expected performance under observation noise is used for evaluation because the decision-maker is interested in the true objective function and the noise is assumed to be noise (misleading, not representative). In the formulation in this paper, the decision maker does care about the noise; rather the objective function of interest is the stochastic noisy function. It would be good to make this distinction clearer upfront. - The RF experiment is not super compelling. It is not nearly as interesting as the FEL problem, and the risk aversion does not make a significant difference in average performance. Overall the empirical evaluation is fairly limited. - It is unclear why the mean-variance model is the best metric to use for evaluating performance - Why not also evaluate performance in terms of the VaR or CVaR? - The MV objective is nice for the proposed UCB-style algorithm and theoretical work, but for evaluation VaR and CVaR also are important considerations
Writing: - Very high quality and easy to follow writing - Grammar: - L164: “that that” - Figure 5 caption: “Simple regret fat the reprted”
Questions: - Figure 2: “RAHBO not only leads to strong results in terms of MV, but also in terms of mean objective”? Why is it better than GP-UCB on this metric? Is this an artifact of the specific toy problem?
Limitations are discussed and potential future directions are interesting. “We are not aware of any societal impacts of our work” – this (as with an optimization algorithm) could be used for nefarious endeavors and could be discussed.",- L164: “that that” - Figure 5 caption: “Simple regret fat the reprted” Questions:,1,1097
NIPS_2021_998,NIPS_2021,"• Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear. Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work","• At times, information is not given in an easy-to-understand way.",1,1099
NIPS_2022_1094,NIPS_2022,"As far as I understood, Theorems 2.1&3.1&3.2 establish generalization bounds (in Definition 3, when defining oracles, samples z
are derived from the distribution D
). Meanwhile, experiments depict the train losses only.
Theorem 2.1. requires τ ≤ L
, although the question regarding how often this condition is satisfied is not discussed. The only guaranteed inequality is τ ≤ 2 L
. The paper says that τ
can be much smaller but, for example, in the case of convex functions (what is a subclass of non-convex functions) τ ≥ M − 1 M L . Typos/suggestions:
line 59: usually, the optimization problem is E | ∇ F ( x ) | 2 ≤ ε 2 ;
page 4: I would suggest adding the definition of class F ( Δ , L )
in the environment of “definition” in the text so that the reader does not search for it in the text;
Definition 3 presumable contains two typos: on line 73, x
is not defined; on line 74 sup x , y
is redundant (otherwise, the last sentence does not make sense);
line 100: in O F m 2 , l , σ 2 l
must be capital
Algorithm 1, line 3: what is b 0
? (defined only later, in Theorem 3.1)
Algorithm 1, line 12: x t ⟶ x r
Algorithm 1: I would suggest leaving a tilde out over m ~ r
; the tilde makes the full of different notations (e.g. line 14) algorithm even more complicated;
Theorem 3.1: b 0
is an integer, rounding in the theorem is absent;
Theorem 3.1: 1 / L
is always larger 1 / K L
The second weakness ("" τ
is much smaller than L
"") can be considered a limitation.","1 / L is always larger 1 / K L The second weakness ("" τ is much smaller than L "") can be considered a limitation.",1,1106
NIPS_2022_1572,NIPS_2022,"• There should be a clearer explanation of the distinction between ChebBase and ChebNet.
• It would be interesting to showcase some of the filters that ChebNetII has learned. What distinguishes them from the ones learned by BernNet?
Minor points:
• The best results should also be highlighted in Table 3.",• The best results should also be highlighted in Table 3.,1,1107
NIPS_2022_2510,NIPS_2022,"weakness
The main assumption behind the development of the theory is the fact that Lipchitz neural networks are ""often perceived as not expressive"", which I do not remember ever reading in the literature. Most of the paper seems fuzzy: it goes from BCE to robustness certification to a (trivial) convergence proposition (prop 4) to consideration about float32/64 (ex 1)... it is just too much. There are no transitions and at no moment the reader understands what is the point.
One of the biggest problem is the writing style that makes the paper hard and annoying (sorry...) to read. For example among many other problems:
- sentences must contain a verb and end with a '.'(even when it ends with a formula);
- theorem should be self contained, exterior references should remain exceptional;
- organize the equations in order to be easily readable;
- separate definitions and remarks (e.g. def 1 & 2 contains definitions and remarks);
- separate propositions and definitions (e.g. corollary 1 contains a definition and a proposition);
- the proofs rely on many technical tools such as Lebesgue measure but the pre-requisite (Borel space etc) are never explicitely given.
Most of the proofs in the appendix should be re-written. More personal but I think the footnotes in the paper could be avoided. The paper is full of references and it costs the readability of the arguments. The reader gets overwhelmed very quickly for the wrong reasons. References
most reference are incomplete (many article are reference as arxiv instead than the published and peer-reviewed version) Others
l94: one needs additional hypothesis on AllNet to claim that they have finite Lipschitz constant (Lipschitz activation would do).
l216: the reference to cor 1 seems dubious: it is a definition of \epsilon separation rather than about bias.",- organize the equations in order to be easily readable;,1,1115
NIPS_2022_96,NIPS_2022,"]
Instruction they construct may have somewhat unrealistic assumptions: 1) all attributes are known to be user before searching, 2) preferences are defined before searching the items, and 3) even those conditions are not changed during browsing the web shop.
Including ablation study on the effect of using image feature would be needed.","1) all attributes are known to be user before searching,",1,1122
NIPS_2022_2123,NIPS_2022,"weakness of current theories on D-SGD and provided an improved explanation on its convergence. Now, their theory can explain the convergence when the spectral gap is zero and also better align with empirical performance of neural networks trained with D-SGD.
The paper introduced an interesting random walk model on a graph as a proxy for the training dynamics of D-SGD. Based on this, they define a novel concept of n W
, which is a key in their analysis.
The paper performed exhaustive experiments on various types of graph (including time-varying topology) to confirm the theory. Weaknesses
I believe the writing of the paper can be much improved. There were notations (e.g. ζ , P ¯
) that are used before defined, so I had to go between the main text and appendix back and forth.
In line 149, the paper is giving the rate in terms of n W
, which is not yet defined. I think it is logically better to move the result after Section 3.2 (after n W
is defined).
There were many typos and some of them slowed down my reading significantly. Typos
line 193, page 6: ζ = d + 2
instead of ζ = d + 1
displayed equation below line 471, page 17: γ k − 1 2
instead of γ k 2
(think about z ( 1 ) )
displayed equation below line 473, page 17: γ k − 1
instead of γ k
(same reason)
line 476, page 17: assumptions of ""Lemma"" 2
proof of Lemma 4, page 18: Missing subscript of b ( t )
. Result is correct, but the proof has a wrong scaling. The second line leads to γ ( 1 − r ) b n i + j ( t ) + ( 1 − r ) t
-- should be fixed.
line 501, page 18: the inequality is in the wrong direction. Should be b ( t + 1 ) ≥ . . . .
line 521, page 19: ∇ h ( x )
Figure 13, page 27: the figure and caption say different γ","... line 521, page 19: ∇ h ( x ) Figure 13, page 27: the figure and caption say different γ",1,1129
NIPS_2022_778,NIPS_2022,"While the manuscript claims that MiniRTS is harder than following natural language instructions in some grid world navigation environments (e.g., [1]). For starters, the problem of training from natural language seems harder for two reasons. First, understanding natural language instructions seems a harder problem than the command setup proposed in the paper. Furthermore, the problem that this previous navigation work analyzes is usually under sparse rewards (success/fail) while in the setting proposed in the paper there are dense demonstrations that may alleviate the complexity of the task.
In terms of novelty, constraining RL with demonstrations as has been extensively studied in the literature [2,3] and the architectural challenges for solving the language-grounded task were introduced in MiniRTS [4].
[1] Chevalier-Boisvert et al. BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning. 2018 [2] Yang et al. Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. 2020 [3] Goecks et al. Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments [4] Hu et al. Hierarchical Decision Making by Generating and Following Natural Language Instructions
The author address some limitations of their work and the setting discussed in the paper is interesting from its potential societal impact of having RL agent that trade-off following human commands with the optimality of the actions it executes.",2020 [3] Goecks et al. Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments [4] Hu et al. Hierarchical Decision Making by Generating and Following Natural Language Instructions The author address some limitations of their work and the setting discussed in the paper is interesting from its potential societal impact of having RL agent that trade-off following human commands with the optimality of the actions it executes.,1,1140
NIPS_2022_1016,NIPS_2022,"Unlike Nystrom and random Fourier features method, the proposed method doesn't have theoretical guarantees for the quality of the approximation.
The paper is unlikely to be interesting outside of the neuroscience community, as random Fourier features (along with some improved methods) work well/fast/with guarantees for kernel approximation. Summary
I think it's an interesting paper with some room for improvement; 7 (accept).
The authors adequately addressed the limitations and potential negative societal impact.",7 (accept). The authors adequately addressed the limitations and potential negative societal impact.,1,1144
NIPS_2022_17,NIPS_2022,"Consider demonstrating with a simple example what your results imply in terms of the number of problems you need to solve for some real data set. Also show via some simple experiment that standard approaches (such as 10-fold CV) can fail (as you claim they do) and that your method solves this problem.
Move section 4 to the appendix since ordinary lasso classification is not a standard or useful model.
State clearly what your contributions are in relation to previous work such as [1, 2, 3, 4] and cite their work where appropriate. In particular there seems to be overlap with [1] that isn't accounted for in the work.
Consider amending the literature section in the paper by including some or all of [1, 2, 3], and [4] regarding homotopy methods for the lasso.
Include at the very least [5] in your literature section on hyper-parameter tuning for the lasso.
It is not standard to call the Elastic Net (or elastic net) ElasticNet. Consider adding a space.
l. 58. There should be a period at the end of the sentence.
l. 77. w.h.p. is not a standard abbreviation. Consider spelling it out.
l. 208. There should be a period at the end of the equation.
l. 271. There should be a period after ϵ .
l. 386. There is an extra space after ""values"".
[2] P. Garrigues and L. Ghaoui, “An homotopy algorithm for the lasso with online observations,” in Advances in neural information processing systems 21, Vancouver, Canada, Dec. 2008, vol. 21, pp. 489–496. [Online]. Available: https://proceedings.neurips.cc/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf
[3] D. M. Malioutov, M. Cetin, and A. S. Willsky, “Homotopy continuation for sparse signal representation,” in Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005, Philadelphia, USA, Mar. 2005, vol. 5, pp. v733–v736. doi: 10.1109/ICASSP.2005.1416408.
[4] M. Osborne, B. Presnell, and B. Turlach, “A new approach to variable selection in least squares problems,” IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389–403, Jul. 2000, doi: 10.1093/imanum/20.3.389.
[5] Q. Bertrand, Q. Klopfenstein, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon, “Implicit differentiation of Lasso-type models for hyperparameter optimization,” in Proceedings of the 37th International Conference on Machine Learning, Nov. 2020, pp. 810–821. Accessed: Jun. 27, 2022. [Online]. Available: https://proceedings.mlr.press/v119/bertrand20a.html",58. There should be a period at the end of the sentence. l.,1,1154
NIPS_2022_869,NIPS_2022,"The authors distill a ViT-B/16 into a RN50. While the former takes more FLOPs than the later, ViT-B/32 is cheaper than RN50, and the CLIP version of that architecture outperforms both the author's RN50 and the CLIP RN50 model. While this doesn't totally undercut the presented results, it's a bit strange to me why the authors chose the particular distillation pair that they did --- it seems like ViT-B/32 (or even EfficientNet or MobileNet) would have been a good choice for student, and perhaps L/14-336px CLIP as the teacher would have made a more compelling setup.
The ablations suggest that the cosine similarity loss isn't really required: the best zero-shot imagenet performance is actually achieved only with the cross-modal loss, and the other datasets seem to be within a close margin. This isn't really a /negative/, per-say, but a bit contrary to the story told.
It would have been nice to see linear probe results for the other datasets like Pets37 as well.
Even assuming that RN50 is the best choice of student, the empirical results are somewhat unimpressive. Specifically, I believe that the authors choose anchor points to be task-specific prompts and the author's model is domain adapted to (unlabelled) dataset-specific images, i.e., there's reason to believe that the author's method is specifically tuned to the tasks described. RN50-CLIP, which is not tuned to the specific tasks described, achieves only slightly worse imagenet linear probe accuracy: 73.3 vs 74.8.
The zero shot results in table 3 are interesting, but a bit incomplete. The ViT-B/32-CLIP model smaller than the author's distilled RN50 model achieves only slightly worse performance on these tasks, but it isn't presented. Again, this observation doesn't invalidate the author's results, it's just that one really needs to buy that RN50 specifically is an interesting student model.
Overall, the authors clearly ""win"" on this particular architecture by a few accuracy points on linear probe imagenet, but it's not clear that this is really the best architecture to distill to (particularly because ViT-B/32 is smaller and there's already a CLIP model for that). To the author's credit, they recognize this on L288, but I would have preferred to see that the author's method generalizes to other teacher/student models, instead of just this one combination.
Overall, the authors have a promising core result: it's possible to distill the knowledge of a CLIP model into a smaller set of weights. And, the innovation that enables this distillation (that image-text distances should be preserved in addition to image-image distances) is novel. However, the empirical results are slightly underwhelming: the authors model, by virtue of the selection of the anchor points and distillation over unlabelled images from the corpora of interest, is tuned to this particular set of datasets (vs. CLIP that isn't). Their performance improvement over CLIP is somewhat small in magnitude (73.3 vs 74.8). I would have liked to have seen: 1) distillation to significantly more efficient architectures vs. RN50 (ViT-B/32 CLIP is already smaller) and with better teachers than ViT-B/16; and 2) linear probe evaluations on unseen tasks, where the images haven't been seen by the distillation model at training time.
Presentation fixes:
It should be specified that Table 3 results are all zero shot.
L71: week --> weak
The broader impact statement is a bit hollow -- I would have appreciated a fuller discussion of embedded systems and/or low resource computation
Table 4's caption has an incomplete sentence.
The authors mainly focus on making higher performance, lower-resource versions of existing CLIP models. Given that CLIP's negative impacts have been discussed in the original work and follow ups, I don't think significant additional discussion is needed here.
I did think that the ""Broader impacts"" section was a bit short: can more information about FLOPs/energy use/on-device computation be discussed?",1) distillation to significantly more efficient architectures vs. RN50 (ViT-B/32 CLIP is already smaller) and with better teachers than ViT-B/16; and,1,1160
NIPS_2022_532,NIPS_2022,"1. Imitation Learning: The proposed method needs to be trained by behavioral cloning, which means 1) it requires a carefully well-designed algorithm (e.g., ODA-T/B/K) to generate the supervised data set. 2) More importantly, the data generated by ODA with a time limit L is indeed not a perfect teacher for behavioral cloning. Since all the ODAs are not designed and optimized for bounded time performance, their behavior (the state-action pairs) for the first L time is not the optimal policy under time limit L.
Since the generic algorithm procedure of ODA can be encoded as a MDP, is it possible to use reinforcement learning to train the model? Would the RL-based approach find a better policy for bounded time performance that is aware of the time limit T？
2. Performance with Different Time Limits: It seems that the proposed method is both trained and tested with a single fixed time limit T = 1000s for all problems. However, in practice, the applications could have very different run time limits. Will the proposed method generalize well to different time limits (such as 1/10/100/2000/5000s)?
3. Comparison with PMOCO: PMOCO is the only learning-based approach in the comparison, but it has a very small cardinality (feasible points) for most problems. Its approximated Pareto front is also relatively sparse (which means a small set of solutions) in Figure A.3. This result is a bit counter-intuitive.
In my understanding, PMOCO is a construction-based neural combinatorial optimization algorithm, of which one important advantage is the very fast run time. The PMOCO paper [1] reports it can generate 101 solutions for 100 MOKP(2-100) instances (hence 10,100 solutions in total) in only 15s, and 10,011 solutions for 100 MOTSP(3-100) instances (hence 1,001,100 solutions in total) in 33 minutes (~2000s). In this work, with a large time limit of 1,000s, I think POMO should be able to generate a dense set of solutions for each instance.
In addition, while a dataset with 100 instances could provide enough supervised ODA state-action pairs (with a time limit L = 1000s for each instance) for imitation learning, it is far from enough for PMOCO's RL-based training. Since PMOCO does not require any supervised data and the MOKP instances can be easily generated on the fly, is it more suitable to train PMOCO under the same wall-clock time with the proposed method?
[1] Pareto set learning for neural multi-objective combinatorial optimization. ICLR 2022.
The limitations of this work are 1) the requirement of ODAs and a well-designed IP solver; 2) it loses the guarantee for finding the whole Pareto front. They have been properly discussed in the paper (see remark at the end of Section 4.1 and Conclusion).
I do not see any potential negative societal impact of this work.",1) the requirement of ODAs and a well-designed IP solver;,1,1171
NIPS_2022_1716,NIPS_2022,"I don’t see any potential negative societal impact in this work.
[1] Van de Panne, Michiel, Ryan Kim, and Eugene Fiume. ""Virtual wind-up toys for animation."" In Graphics Interface, pp. 208-208. CANADIAN INFORMATION PROCESSING SOCIETY, 1994. [2] Holden, Daniel, Taku Komura, and Jun Saito. ""Phase-functioned neural networks for character control."" ACM Transactions on Graphics (TOG) 36, no. 4 (2017): 1-13. [3] Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. ""Random synaptic feedback weights support error backpropagation for deep learning."" Nature communications 7, no. 1 (2016): 1-10. [4] Nøkland, Arild. ""Direct feedback alignment provides learning in deep neural networks."" Advances in neural information processing systems 29 (2016). [5] Frenkel, Charlotte, Martin Lefebvre, and David Bol. ""Learning without feedback: Fixed random learning signals allow for feedforward training of deep neural networks."" Frontiers in neuroscience 15 (2021): 629892.","4 (2017): 1-13. [3] Lillicrap, Timothy P., Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. ""Random synaptic feedback weights support error backpropagation for deep learning."" Nature communications 7, no.",1,1173
NIPS_2022_2814,NIPS_2022,"1. A solution towards removing the position encoding is not discussed. 2. Importance of quantifying the strength of PPP is not clear to me. 3. Authors state that reliable PPP metrics are important for understanding PPP effects in different tasks. While this point is surely intriguing, such an explanation or understanding is not explicitly given in the article. Can the authors explicitly explain what type of understanding one reaches by looking at the PPP maps? 4. The conclusion of the article remains a bit vague. While the proposed metrics have some more desirable attributes, value of these attributes for applications is unclear to me. How will this actually improve the practice or our understanding?",1. A solution towards removing the position encoding is not discussed.,1,1174
NIPS_2022_628,NIPS_2022,"I don’t find anything that would potentially have a negative societal impact.
[1] Agarwal, Rishabh, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc Bellemare. ""Deep reinforcement learning at the edge of the statistical precipice."" Advances in neural information processing systems 34 (2021): 29304-29320. [2] Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. ""Deep reinforcement learning that matters."" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018. [3] Whiteson, Shimon, Brian Tanner, Matthew E. Taylor, and Peter Stone. ""Protecting against evaluation overfitting in empirical reinforcement learning."" In 2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL), pp. 120-127. IEEE, 2011. [4] Fu, Wei, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. ""Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning."" arXiv preprint arXiv:2206.07505 (2022).","1. 2018. [3] Whiteson, Shimon, Brian Tanner, Matthew E. Taylor, and Peter Stone. ""Protecting against evaluation overfitting in empirical reinforcement learning."" In 2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL), pp. 120-127. IEEE, 2011. [4] Fu, Wei, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. ""Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning."" arXiv preprint arXiv:2206.07505 (2022).",1,1175
NIPS_2022_2813,NIPS_2022,"weakness (insight and contribution), my initial rating is borderline. Strengths:
+ The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.
+ The ablation studies and analysis in Section 4.4 is well organized and clearly written. It is easy to follow the analysis and figure our the contribution of each component. Also, Figure 2 is well designed and clear to illustrate the pipeline.
+ The experimental analysis is comprehensive. The analysis on computation time and inference speed is also provided. Weakness:
- (major concern) The contribution is somehow limited. The main contribution is applying optimal transport for few-shot adaptation of CLIP. After reading the paper, it is not clear enough to me why Optimal Transport is better than other distance. Especially, the insight behind the application of Optimal Transport is not clear. I would like to see more analysis and explanation on why Optimal Transport works well. Otherwise, it seems that this work is just an application work on a specific model and a specific task, which limits the contribution.
- The recent related work CoCoOp [1] is not compared in the experiments. Although it is a CVPR'22 work that is officially published after the NeurIPS deadline, as the extended version of CoOp, it is necessary to compare with CoCoOp in the experiments.
- In the approach method, there lacks a separate part or subsection to introduce the inference strategy, i.e., how to use the multiple prompts in the test stage.
- Table 2 mixed different ablation studies (number of prompts, visual feature map, constraint). It would be great if the table can be split into several tables according to the analyzed component.
- The visualization in Figure 4 is not clear. It is not easy to see the attention as it is transparent. References
[1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.
After reading the authors' response and the revised version, my concerns (especially the contribution of introducing the optimal transport distance for fine-tuning vision-language models) are well addressed and I am happy to increase my rating.","+ The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.",1,1179

review_point,paper_id,focused_review,grounding_specificity_label,chatgpt_grounding_specificity_definitions_score,chatgpt_grounding_specificity_definitions_rationale,chatgpt_grounding_specificity_definitions_examples_score,chatgpt_grounding_specificity_definitions_examples_rationale,chatgpt_grounding_specificity_definitions_incontext_learning_score,chatgpt_grounding_specificity_definitions_incontext_learning_rationale
- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.,ARR_2022_317_review,"- Lack of novelty: - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.
- Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER.
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives.",1.0,4,"the review point is fully grounded as it clearly pertains to the discussion on novelty, referencing the related work section of the paper, so it is obvious to the authors what part is being addressed. however, while the comment highlights a lack of novelty, it does not specify in detail what could be improved or suggest how to enhance the novel contribution of the paper. thus, it is under-specific regarding what actions the authors should take to improve the draft.",5,"the review point is fully grounded and specific. it explicitly refers to the 'related work' section by mentioning that adversarial attacks by perturbing text are summarized there. it then goes on to specify the issue with the paper, which is a lack of novelty—the only new effort being the application of existing ideas on video-text models. this level of detail helps the authors clearly understand what part of their paper is being addressed and what the perceived issue is, thus fulfilling both grounding and specificity criteria.",2,"the comment is weakly grounded and not specific. it provides some context by referring to adversarial attacks on nlp models and image-text models, but does not clearly identify a specific section or describe precisely what needs to be addressed regarding the lack of novelty. the authors cannot confidently determine which part the comment addresses without more detailed guidance."
"4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.",ACL_2017_779_review,"However, there are many points that need to be address before this paper is ready for publication.
1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.
Also, how is the montecarlo sampling done? 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ). 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space. - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here 392 : Do you mean 2016?
Nitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.",1.0,4,"the comment explicitly mentions a part of the paper, indicating that a whole section and experimental results are dedicated to fleshing out assumptions. this provides full grounding as the authors can clearly identify which part of the paper is being addressed. however, the comment lacks specificity in detailing what is wrong with dedicating a whole section to this purpose or how it can be improved, making the comment fully grounded but under-specific.",4,"the review point explicitly mentions which part of the paper it is addressing by referring to the section dedicated to the assumptions and experimental results. this provides full grounding as the authors can clearly identify which part of the paper the comment is addressing. however, it does not specify in detail what makes the section 'not so useful,' lacking specificity regarding what exactly needs to be improved or revised. therefore, it is fully grounded, but under-specific.",4,"the comment is fully grounded as it refers to a specific section of the paper where assumptions and experimental results are discussed. however, it lacks specificity because it does not clearly state what needs to be changed or improved about the section, only that it is excessive."
- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -,ACL_2017_494_review,"- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes ""morph-fitting"", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of ""attract"" and ""repel"" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does ""looking"" - ""look"" + ""walk"" = ""walking""? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.",1.0,2,"the review comment is weakly grounded and not specific. while it suggests adding additional baselines, such as character embeddings, it does not specifically identify or reference a particular part of the paper that needs this revision. without explicit mention of where in the paper these additional baselines should be considered, the authors may struggle to precisely address this feedback. furthermore, the comment lacks specific guidance on how these additional baselines should be integrated or evaluated, leaving room for uncertainty in how to implement the suggestions.",4,"the review point is fully grounded as it identifies the work as an extension of existing retrofitting work and suggests additional baselines, which implies that the comment addresses the methodology or experimental section where retrofitting work is discussed. however, the comment is under-specific because it suggests the addition of baselines (e.g. character embeddings) without detailing what existing issues these additional baselines would address or how they would improve the work. thus, authors may infer what part of the paper needs attention, but they are not given specific guidance on what issues the additional baselines are meant to solve.",3,"the comment can be considered weakly grounded as it does not refer to a specific section, table, or figure. however, it provides some specificity by suggesting that additional baselines, such as character embeddings, should be included. the suggestion gives enough hint for authors to infer where additional evaluations might be needed."
"1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021)",ARR_2022_233_review,"Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released.
1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them. First of all, will the author release the dataset or will it remain private?
Are the guidelines used to train the annotators publicly available?
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines.
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported.
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available?
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances.",1.0,1,"the review point suggests an additional reference but does not specify where in the paper this reference should be added or why it is relevant. it does not provide grounding by mentioning a specific section, figure, or any unique part of the paper that needs this reference. furthermore, it lacks specificity in explaining why this reference is important or how it contributes to the paper. without an obvious link to a particular part of the paper, authors might find it challenging to determine where to incorporate the suggested reference.",4,"the review point is fully grounded because it mentions a specific reference ('detecting and explaining unfairness in consumer contracts through memory networks' by ruggeri et al, 2021) that the authors should consider including in their paper. however, it is under-specific because it does not detail why this reference is relevant or what specific part of the paper it should be connected to, leaving the authors without clear guidance on how this citation improves their draft.",4,"the comment is fully grounded as it provides a specific additional reference for the authors to consider. however, it lacks specifics on how or why this reference is relevant or how it should be integrated into the paper, making it under-specific."
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).,ARR_2022_98_review,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges.
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system. 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.",1.0,2,"the comment suggests including examples of the system on actual texts compared to other components and models. however, it does not mention any specific part of the paper or detail what needs to be addressed. the authors might have some hint that it relates to sections discussing the system or its evaluation, but cannot confidently determine the exact part. additionally, the comment lacks specificity in terms of describing what examples should illustrate or how this would improve the paper.",4,"the review point is fully grounded because it is clear that the suggestion relates to including examples of the system applied to actual texts, distinguishing it from other components and models. however, it is under-specific as it does not specify what kind of examples are needed, what the criteria for these examples should be, or why these examples would be beneficial. to be more specific, the reviewer could have detailed what purpose these examples would serve or which components and models for comparison should be highlighted.",3,"the comment is weakly grounded, as it does not specify a particular section, table, or figure, but the authors could infer where they might add examples of the system. it is somewhat specific, as it suggests improving the draft by providing examples on actual texts, but it does not specify in detail the nature of these examples or which parts of the paper could benefit from them."
- A number of claims from this paper would benefit from more in-depth analysis.,ARR_2022_232_review,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed.
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements. In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset? Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption.",1.0,2,"the comment is weakly grounded as it does not specify which specific claims in the paper need more in-depth analysis. therefore, the authors cannot confidently determine which part of the paper the comment addresses. moreover, the comment lacks specificity as it does not indicate what is lacking in the current analysis or suggest how the analysis should be improved. overall, the comment provides only a general hint that certain claims require more analysis but fails to give the authors concrete guidance.",2,"the review point is weakly grounded and not specific. the comment mentions that a number of claims could use more in-depth analysis but does not provide explicit details on which specific claims in the paper are being addressed. this makes it difficult for the authors to confidently identify the part of the paper the comment is addressing. additionally, the comment lacks specificity as it does not detail what is wrong with the current analysis or how it should be improved, providing no guidance on what needs to be addressed.",1,"the comment is not grounded at all. it does not reference any specific part of the paper or make it clear which claims need more analysis. additionally, it is unspecific about what kind of in-depth analysis is required or where these claims are located in the paper."
"- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).",ICLR_2022_3352,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.",1.0,4,"the comment acknowledges that the paper is generally not difficult to follow but hints there are specific areas of potential confusion. it states that these are listed in point 3, which provides full grounding because the authors can easily refer to point 3 for more detailed feedback. however, without point 3 included in this review point, there is no specificity about what the confusion entails in these areas.",2,"the review point is weakly grounded and not specific. it mentions that certain parts of the paper could cause confusion, hinting at potential clarity issues. however, it does not specify which sections or elements are problematic and only refers vaguely to these points. the mention of being listed in another point does not provide immediate guidance within this comment, leaving the authors unable to precisely identify what or where the issues lie without further context.",2,"the comment is weakly grounded, as it vaguely refers to 'several places' that may cause confusion without specifying where these places are in the paper. additionally, it does not provide specific guidance on what exactly is confusing or how to improve those areas."
1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new.,NIPS_2018_476,"Weakness] 1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new. 2) Theoretical proofs of existing algorithm might be regarded as some incremental contributions. 3) Experiments are somewhat weak: 3-1) I was wondering why Authors conducted experiments with lambda=1. According to Corollary 1 and 2 lambda should be sufficiently large, however it is completely ignored for experimental setting. Otherwise the proposed algorithm has no difference from [10]. 3-2) In Figure 3, different evaluations are shown in different dataset. It might be regarded as subjectively selected demonstrations. 3-3) I think clustering accuracy is not very significant because there are many other sophisticated algorithms, and initializations are still very important for nice performance. It show just the proposed algorithm is OK for some applications. [Minor points] -- comma should be deleted at line num. 112: ""... + \lambda(u-v), = 0 ..."". -- ""close-form"" --> ""closed-form"" at line num.195-196. --- after feedback --- I understand that the contribution of this paper is a theoretical justification of the existing algorithms proposed in [10]. In that case, the experimental validation with respect to the sensitivity of ""lambda"" is more important rather than the clustering accuracy. So Fig. 1 in feedback file is nice to add paper if possible. I think dropping symmetry is helpful, however it is not new idea that is already being used. So, it will not change anything in practice to use it. Furthermore, in recent years, almost all application researchers are using some application specific extensions of NMF such as sparse NMF, deep NMF, semi-NMF, and graph-regularized NMF, rather than the standard NMF. Thus, this paper is theoretically interesting as some basic research, but weak from an application perspective. Finally, I changed my evaluation upper one as: ""Marginally below the acceptance threshold. I tend to vote for rejecting this submission, but accepting it would not be that bad.""",1.0,2,"this review comment is weakly grounded and not specific. it does mention the originality of the idea and algorithm, suggesting a lack of novelty, but it does not refer to specific parts of the paper such as sections or claims that the authors can identify as needing improvement. moreover, the comment lacks specificity regarding what aspect of the originality is lacking (e.g., is it the conceptual framework, the methodology, the results, etc.?) and does not provide guidance on how to address these issues. this makes it difficult for the authors to pinpoint where exactly the comment applies and what specifically needs to be revised to enhance originality.",2,"the review point addresses the originality of the paper's main idea and algorithm, suggesting that they are not new. however, it does not reference any specific part of the paper or specific elements within the paper that are lacking originality, making it weakly grounded. additionally, the comment does not provide details on what aspects of the variable splitting idea or algorithm make them unoriginal, so it lacks specificity. therefore, based on the criteria provided, this comment would be rated as 'weakly grounded and not specific'.",2,"the comment is weakly grounded and not specific. while it points out the lack of originality due to the ideas and algorithms not being new, it does not mention where these claims can be verified within the paper nor does it specify which parts of the paper present these ideas and algorithms. consequently, the authors may struggle to confidently identify the exact sections this feedback pertains to."
1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.,5UW6Mivj9M,"1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.
2) Relatedly, it was hard to discern what was novel in the paper and what had already been tried by others.
3) Since the improvement in numbers is not large (in most cases, just a couple of points), it is hard to tell if this improvement is statistically significant and if it translates to qualitative improvements in performance.",1.0,2,"the comment is weakly grounded as it does not specify which part of the paper makes it difficult to follow. while it hints at issues related to 'experimental procedures and evaluations,' the authors cannot accurately determine which sections need revision. additionally, the comment lacks specificity in detailing what exactly is unclear about the procedures or evaluations, thus making it not specific.",1,"the review point does not provide any specific guidance regarding which part of the paper was hard to follow or identify any specific issues with the experimental procedures and evaluations. as a result, the authors cannot confidently determine which sections require clarification or improvement. this lack of grounding and specificity renders the comment not helpful in guiding authors to improve their draft.",1,"the comment is not grounded as it does not specify any part of the paper where the issue occurs. it lacks detail on what exactly is wrong with the experimental procedures and evaluations, hence making it highly unspecific."
1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.,ACL_2017_554_review,"1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families.
2) The paper does not talk about the training speed impact with more details.
- General Discussion:",2.0,1,"the review comment is not grounded as it does not specify which part of the paper it addresses, such as a specific section, figure, or any identifiable part. the comment is also not specific because it does not provide details or examples to illustrate what is missing or what should be addressed about the theoretical proofs and convergence properties. as it stands, the authors are left unsure about where exactly to make revisions and what specific aspects need improvement.",2,"the review point is weakly grounded and not specific. while it points out a general issue related to the theoretical proofs and convergence properties, it does not explicitly identify which part of the paper this critique applies to, nor does it provide guidance on what specific aspects of 'theory profs' or 'convergence properties' are lacking or need improvement. the authors would have difficulty identifying the precise section needing revision or knowing exactly what is required to address the reviewer's concerns.",1,"the comment is not grounded at all, as it does not mention what the proposed algorithm is, or where it was mentioned. it is highly unspecific, providing no details on what part of the paper this issue relates to."
"- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.",ARR_2022_82_review,"- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer). - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.",2.0,5,"the review point is fully grounded and specific because it explicitly refers to a particular aspect of the paper, namely the proposed post-processing steps used to filter out 'false-positive' neurons. the authors can easily identify the part of the paper being addressed. additionally, the comment specifies what is missing, which is a demonstration or evidence of the importance of these post-processing steps through an ablation study. this provides clear guidance on how to improve the draft, making the comment both fully grounded and specific.",5,"the review point is fully grounded and specific. it explicitly refers to the use of integrated gradients and post-processing steps within the paper, indicating a precise area of focus. the comment specifies the need for an ablation study to assess the importance of these post-processing steps, clearly detailing what is wrong (lack of demonstrated importance) and what action is needed (conduct an ablation study). this provides clear guidance to the authors on how to improve their draft.",5,"the comment is fully grounded, as it specifically mentions the use of integrated gradients and the post-processing steps discussed in the paper. it clearly specifies what needs to be addressed by suggesting an ablation study to demonstrate the importance of these post-processing steps, thus providing clear guidance to the authors on how to improve their draft."
"3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",ACL_2017_318_review,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",2.0,4,"the review comment is fully grounded as it addresses the comparison of proposed models with those considering different senses but not sememes, which is a specific aspect of the paper. it mentions the potential relevance of the mst baseline and suggests that the discussion on baselines is insufficiently described, leading to an emphasis on soft vs. hard word sense disambiguation. however, the comment is under-specific because, while it identifies the area that needs improvement, it could provide more details on what exactly is missing about the baseline comparisons or which related works to consider. therefore, it is rated as fully grounded but under-specific.",5,"the review point is fully grounded and specific. it clearly addresses the part of the paper concerning the comparison of proposed models and their relation to other models that consider different senses but not sememes. the comment specifically mentions the mst baseline as an example and critiques the insufficient description of this comparison, focusing instead on the soft vs. hard word sense disambiguation. additionally, it provides a suggestion for strengthening the paper by including more baselines based on related work. this helps the authors understand exactly which part of the paper needs improvement and what specific changes to make, enabling them to enhance their draft effectively.",3,"the comment is weakly grounded as it lacks explicit references to sections or specific parts of the paper, but it addresses the issue of comparing models with different considerations. it specifies that including more baselines would improve the paper, indicating what needs to be addressed."
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.,ACL_2017_818_review,"- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations. What are some interesting action verbs and corresponding attributes relations? The paper also lacks analysis/discussion on what kind of mistakes their model makes.
- The number of object pairs (3656) in the dataset is very small. How many distinct object categories are there? How scalable is this approach to larger number of object pairs?
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.
General Discussion/Suggestions: - The authors should discuss the following work and compare against mining attributes/attribute distributions directly and then getting a comparative measure. What are the advantages offered by the proposed method compared to a more direct approach?
Extraction and approximation of numerical attributes from the Web Dmitry Davidov, Ari Rappoport ACL 2010 Minor typos: 1. In the abstract (line 026), the authors mention 'six' dimensions, but in the paper, there is only five.
2. line 248: Above --> The above 3. line 421: object first --> first 4. line 654: more skimp --> a smaller 5. line 729: selctional --> selectional",2.0,2,"the review point identifies an issue regarding the selection of 'frame similarity factors and attributes similarity factors,' which the authors might have some clue about within their methodology or results sections. however, it does not explicitly mention or reference any specific section or part of the paper, which makes the grounding weak. furthermore, while the comment points out that there is a lack of clarity, it does not provide further specificity or guidance on what exactly is unclear or how it could be clarified, resulting in an unspecific comment.",2,"the comment is weakly grounded because it hints at an issue related to 'frame similarity factors and attributes similarity factors,' but it does not specify where in the paper this issue occurs, making it challenging for the authors to locate the exact section being discussed. additionally, the comment is not specific in terms of detailing what precisely is unclear or how it might be improved, merely stating that it is 'a bit unclear' without identifying the cause or suggesting improvements.",2,the comment is weakly grounded as it does not reference a specific part of the paper where the frame similarity factors and attributes similarity factors are discussed. the authors are likely left guessing which section or context this refers to. while the comment identifies a specific issue—unclear selection criteria—it does not ground the issue in a particular section or provide enough context for the authors to confidently know where the problem lies.
"- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig.",ARR_2022_149_review,"- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.
Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349",2.0,4,"the comment addresses a specific aspect of the paper related to the description of knowledge bases and societal biases, indicating it should be clear to the authors which part of the paper it pertains to. there is a reference to an example in a figure, suggesting this part is recognizable by the authors. however, the comment does not provide specific details or guidance on how to address the identified issue of societal biases, which makes the comment fully grounded but under-specific.",4,"the comment is fully grounded as it makes a literal mention of figure (implicitly fig.) in the paper, indicating a specific location that the reviewer is addressing. the comment highlights a specific concern about societal biases in knowledge bases, which are pertinent to the authors' findings or methodology, indicating a weakness or an omission in the draft. while it suggests a need for considering societal biases, it doesn't elaborately specify how this should be addressed or improved in the paper, making it under-specific about the potential solution or improvement.",2,"the comment is weakly grounded and not specific. it alludes to knowledge bases and examples, which may be inferred from various parts of the paper, but it does not explicitly identify any specific section or figure. furthermore, while it raises the issue of societal biases in knowledge bases, it does not provide specific details or guidance on how to address this issue or clarify the example mentioned in the figure."
"- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",ACL_2017_365_review,"1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.
2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.
3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.
References: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.
Multi-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.
- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",2.0,1,"the review point is not grounded at all and does not mention any specific part of the paper. it raises a generic issue about showing why something is not working and suggests improving it, but it does not provide any specific context or details about which part of the paper this relates to or what specifically needs attention. consequently, the authors would have difficulty identifying what part of their paper this comment addresses or what specific changes are being recommended.",1,"the review point is not grounded as it does not specify a particular section, figure, or part of the paper that it is addressing. it also lacks specificity, as it does not clearly identify what is wrong with the attention mechanism in the seq2seq mtl or what needs to be improved. the comment is quite general and does not provide actionable guidance to the authors on what aspect of their work needs revision. consequently, the authors would find it challenging to discern the exact issue or how to address it based on this comment.",2,"the comment is weakly grounded because it doesn't explicitly mention a specific part of the paper or provide clear context that allows the authors to identify where exactly the issue lies. it hints at the seq2seq mtl attention mechanism, but without specific sections, figures, or tables referred to, it's not fully grounded. additionally, it's not specific in detailing what needs to be addressed or modified to improve the draft."
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.,ARR_2022_303_review,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses. However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text. Table 1's revised caption mentions 2.9K paragraphs as the size.
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR.
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column. I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work. S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base",2.0,5,"the review point is fully grounded as it specifically references section s3.1 and sup. fig. 6 in the paper, which allows the authors to accurately identify the area being addressed. additionally, it clearly specifies the issue with the paper, which is its reliance on supplemental materials, making it not truly independent. this detailed feedback will help the authors know exactly where to focus their revisions and what needs to be addressed, fulfilling both grounding and specificity dimensions effectively.",5,"the review point is fully grounded and specific. it explicitly mentions the sections of the paper, such as s3.1 and the reference to sup. fig. 6, which clearly identifies the parts being addressed, making it easy for the authors to locate the areas in question. additionally, it specifies the issue related to the reliance on supplemental materials, indicating that the paper is not self-contained, especially in the context of model comparison and the span vs. sentence investigation. this level of detail helps guide the authors on what specific aspects need improvement, fulfilling both grounding and specificity criteria.",4,"the comment is fully grounded as it mentions specific references like 's3.1 reference to sup. fig. 6' and notes about the model comparison. however, it doesn't go into sufficient specificity about what exactly should be improved or how the reliance on supplemental space affects the independence of the paper."
1. Lack of numerical results. The reviewer is curious about how to apply it to some popular algorithms and their performance compared with existing DP algorithms.,NIPS_2022_670,1. Lack of numerical results. The reviewer is curious about how to apply it to some popular algorithms and their performance compared with existing DP algorithms. 2. The presentation of this paper is hard to follow for the reviewer.,2.0,2,"the comment is weakly grounded as it does not specify any particular section or part of the paper where numerical results should be included or improved. it gives a general indication that numerical results are lacking, but does not pinpoint a specific section that needs them. additionally, while it expresses curiosity about the application to popular algorithms, it does not specify which algorithms or what specific numerical comparisons should be incorporated. therefore, while the comment suggests an area for improvement, it lacks specificity and grounding in the context of the paper.",3,"the review point provides some specificity by indicating the need for numerical results and a comparison of performance with existing differential privacy (dp) algorithms. however, it is weakly grounded because it does not specify which specific part of the paper this comment applies to or which algorithms should be considered. the authors can infer that some results section may need elaboration, but cannot confidently determine exactly where to focus their improvements.",2,"the comment is weakly grounded and not specific. while it indicates a lack of numerical results and a curiosity about the application to popular algorithms, it does not specify where in the paper this issue arises or what exact details are missing. additionally, the comment does not guide the authors on how to address the issue."
"- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.",NIPS_2017_351,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?",2.0,2,"this review point is weakly grounded and not specific. while it addresses the issue of the 'probabilistic connection' in the paper, it does not provide enough information for the authors to confidently identify the specific part of the paper where this issue occurs, leading to weak grounding. additionally, the comment does not specify exactly what needs to be formally connected or what language adjustments are needed, making it unspecific.",3,"the comment is weakly grounded and specific. the authors may not be able to identify the precise section of the paper being addressed since the comment does not directly mention a specific section, figure, or table. the comment addresses the connection of a probabilistic concept, which could be anywhere in the paper, leaving some ambiguity. however, it is specific in detailing that the issue is with the clarity of the probabilistic connection and suggests that the connection be either formalized or clarified in the language. this specificity in what needs to be improved elevates it above a level 2 score.",3,"the comment is weakly grounded because it does not specify which section or part of the paper the 'probabilistic connection' is discussed in. however, it is specific in terms of what needs to be addressed: either formalizing the connection or adjusting the language."
"2. Structural optimization seems one of the main components and it has been emphasized several times. However, it seems the optimization algorithm is directly from some previous works. That is a little bit confusing and reduces the contribution.",ICLR_2022_3332,"Weakness: 1. The writing needs a lot of improvement. Many of the concepts or notations are not explained. For example, what do “g_\alpha” and “vol(\alpha)” mean? What is an “encoding tree”(I believe it is not a common terminology)? Why can the encoding tree be used a tree kernel? Other than complexity, what is the theoretic advantage of using these encoding trees? 2. Structural optimization seems one of the main components and it has been emphasized several times. However, it seems the optimization algorithm is directly from some previous works. That is a little bit confusing and reduces the contribution. 3. From my understanding, the advantage over WL-kernel is mainly lower complexity, but compared to GIN or other GNNs, the complexity is higher. That is also not convincing enough. Of course, the performance is superior, so I do not see it as a major problem, but I would like to see more explanations of the motivation and advantages. 4. If we do not optimize the structural entropy but just use a random encoding tree, how does it perform? I think the authors need this ablation study to demonstrate the necessity of structural optimization.",2.0,3,"the comment is weakly grounded and specific. the authors can infer that the review is addressing the section dealing with structural optimization, as that part is mentioned. however, it does not pinpoint a specific part like a section, figure, or table. the comment clearly specifies what needs to be addressed: the relationship between the structural optimization algorithm and previous works, as it affects the originality and contribution of the paper.",3,"the review comment is weakly grounded and specific. the authors would have some idea about the parts of the paper being addressed, particularly because 'structural optimization' is mentioned as being repeatedly emphasized, but the grounding is weak since it does not precisely identify a section or figure. however, the comment specifies the issue being that the optimization algorithm appears to be from previous works and this might reduce the contribution. this clear specification of what is wrong makes it fall under weakly grounded and specific.",2,"the comment is weakly grounded because it does not reference a specific section or part of the paper where structural optimization is discussed. it is not clear exactly where the optimization algorithm is described or how it relies on previous works. additionally, the comment does not offer specific guidance on what needs to be addressed regarding the presentation or originality of the optimization algorithm."
"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.",ACL_2017_494_review,"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors. - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding baselines. General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.
Minor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.
- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.
- Line 223: x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.
- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.
- I have read the author response",3.0,2,"the review point is weakly grounded and not specific. the comment hints at an analysis related to the 'morph-fitted embeddings', suggesting it addresses a part of the paper discussing evaluation results. however, it does not precisely identify which section or analysis part it refers to, leaving authors uncertain about the exact part of their paper being critiqued. additionally, the comment does not provide specific details on what is lacking or how the analysis can be improved, making it not specific regarding what needs to be addressed.",2,"the comment suggests that there is a lack of analysis regarding the morph-fitted embeddings in the evaluation section, implying the authors should provide more analysis. however, it does not explicitly mention which part of the paper it addresses or specify what exactly needs to be addressed within that part. the authors may have a hint or guess that it relates to sections discussing morph-fitted embeddings and their evaluation, but the comment lacks specific details about what is wrong or missing with these sections.",2,"the comment is weakly grounded and not specific. it hints at an analysis that is not present regarding the morph-fitted embeddings. however, it does not specify where in the paper this is relevant or what exactly needs to be addressed."
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.,ARR_2022_121_review,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)).
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays. Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference.",3.0,3,"the review point suggests considering feature engineering to improve performance and refers to uto et al. (2020) as an example, achieving a high qwk score with hand-crafted features. however, it lacks clear indications of where in the document this suggestion should be applied, making the comment weakly grounded. the specificity is adequate as it provides a specific example (uto et al.'s feature set), indicating what needs to be addressed in this context. therefore, since the comment is weakly grounded but provides specific details for improvement, the appropriate score is 3.",5,"the review point is fully grounded and specific. it explicitly references the work of uto et al. (2020), a specific study that achieves a notable performance metric (qwk of 0.801) using hand-crafted features, which is clearly identifiable to the authors. additionally, the comment gives a specific suggestion to use uto et al.'s same feature set to potentially improve the results in the paper. this allows the authors to exactly identify where to apply the suggestion and gives them a clear direction on how to improve their draft by suggesting a precise methodology or feature set to enhance performance.",5,"the comment is fully grounded as it references a specific work (uto et al. 2020) and discusses the use of feature engineering, providing a concrete example of a system that successfully used such features. it is also specific in its suggestion, offering a potential way to improve the performance of the discussed system by using the same feature set that yielded a qwk of 0.801."
"- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here. Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.",ACL_2017_128_review,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.",3.0,2,"the comment weakly grounds the issue by referring to the general claim about the model's ability to generalize to different knowledge. it does not specify which part of the paper, such as a section or statement, makes this claim, leaving the authors unable to accurately pinpoint the exact location of the issue. furthermore, while it questions the use of certain terms like 'knowledge' and the suitability of certain elements as knowledge representations, it does not provide enough specificity on what needs to be changed or clarified in the paper. the comment lacks concrete suggestions or detailed critique that would help the authors improve their explanation or justify their existing usage, leading to a weak grounding and unspecific assessment.",3,"the review comment addresses an issue related to the paper's claim about model generalization and the interpretation of 'knowledge'. however, it does not specifically mention where in the paper these claims are made, leaving the grounding weak. the comment is specific about what the reviewer finds misleading—the use of 'knowledge' to describe syntax or semantics rather than external world knowledge. therefore, while the comment is specific about what the problem is, it lacks full grounding because it doesn't direct the authors to a particular section of the paper.",3,"the comment is weakly grounded, as it does not explicitly mention where in the paper this claim is made, although the authors can likely infer the relevant part. the specificity is present because the reviewer provides a detailed critique of the claim, explaining the issues with the terminology and the underlying assumptions."
* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.,NIPS_2020_257,"* In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section. * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks). * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.",3.0,2,"the review point lacks full grounding as it does not explicitly mention specific parts of the paper that are being addressed. the comment broadly discusses the usage of reinforcement learning for a static vqa task, but does not provide clear indicators of which sections or elements of the paper relate to this critique. likewise, the comment remains under-specified in terms of detailing exactly what the supposed issue is (e.g., specific problems with data efficiency or difficulties in training) within the context of the paper. authors may have difficulty pinpointing the exact part of their work that this applies to and understanding the detailed critique.",2,"the review point is weakly grounded and not specific. it raises a potential concern about the use of reinforcement learning for a static vqa task but does not specify which part of the paper this criticism is directed at. although it mentions a general topic (reinforcement learning for vqa tasks), it does not clearly identify a particular section, figure, or result in the paper that embodies this criticism. similarly, the comment lacks specificity about what exactly might be inefficient or hard about the models, nor does it provide suggestions on how this could be addressed or improved.",2,"the comment is weakly grounded because it does not specify any particular section or experiment where the use of reinforcement learning is discussed. it's also not specific, as it expresses a personal belief without detailing specific issues or providing examples of how this could impact model efficiency or training. therefore, it offers limited actionable guidance for the authors."
"• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).",ICLR_2021_1944,"I have several concerns regarding this paper.
• Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).
• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).
• Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments.
• Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this.
• Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3].
• Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling.
• Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable.
• Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting.
[1] https://arxiv.org/pdf/2003.00982.pdf [2] https://ogb.stanford.edu/ [3] https://paperswithcode.com/task/node-classification ==========
After reading the authors comments.
I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score.
I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.",3.0,5,"the comment is fully grounded and specific. it references the part of the paper where the authors mention the computation of the optimal transport distance, specifically noting that it takes seconds on a 36-core machine. the reviewer also explicitly requests clarification on the scalability of the method to less powerful machines, providing a clear path for the authors to address this in their paper. furthermore, the reviewer asks for specifics on the computation method, mentioning the use of the sinkhorn method and its potential output as a doubly stochastic matrix, which adds specificity to what needs further explanation. this precision will be highly useful for authors wanting to improve their draft.",5,"this comment is fully grounded and specific. the reviewer explicitly identifies the part of the paper they are addressing, which is the approach to computing the optimal transport distance. they question its scalability and provide specific suggestions on what needs to be addressed, including how the method scales on a machine with fewer cores and clarification on the computation method related to the sinkhorn method. the authors can easily identify the relevant section and understand the specific issues raised.",5,"the comment is fully grounded as it references the specific part of the paper, likely under the 'approach' section or discussing the computation of optimal transport distance. it is also specific in detailing what needs to be addressed, such as scalability concerns and the specifics of the computation method used."
"• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.",ICLR_2021_1213,"weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under “Additional Comments” as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. Summary:
• The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex.
• The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known “random shuffling” sampling strategy.
• Specifically, AdaGrad-window is shown to achieve O ~ ( T − 1 / 2 )
rate of convergence, whereas AdaGrad-truncation attains ( T − 1 / 2 )
convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.
• The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.
• In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition “consistency ratio” over epochs. Strengths:
• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.
• I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.
• Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.
• Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new “consistency condition” is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors’ approach to proving the results. Weaknesses:
• Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn’t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.
• Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r
. I couldn’t figure out how it is possible to compute the value r
ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r
weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.
• The related work which is listed in Table 1, within the group “Adaptive Gradient Methods” prove \emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.
• As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.
• Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion.
• This is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.
Additional Comments:
• I haven’t seen the definition that x t , m + 1 = x t + 1 , 1
in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?
• Second bullet point of your contributions claim that “[consistency] condition is easy to verify”. I do not agree with this as I cannot see how someone could guarantee/compute the value r
ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?
• In Assumption A3, I understand that G t e i = g t , i and G t e = ∑ i = 1 m g t , i
. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.
• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i
enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?
• In Corollary 1, authors state that “the computational complexity is nearly O ( m 5 / 2 n d 2 ϵ − 2 ) ~
”. A similar statement exists in Corollary 2. Could you please explain what “nearly” means in this context?
• In Lemma 8 in the supplements, a a T and b b T
in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that a a T or b b T
correspond to something like g t , j 2 – g t − 1 , j 2
. I am not sure if this construction fits into Lemma 8 because, for instance, the expression g t , j 2 – g t − 1 , j 2
is difference of two rank-1 matrices, which could have rank \leq 2. Hence, there may not exist some vector a
such that a a T = g t , j 2 – g t − 1 , j 2
, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors.
• In the supplements, in section “A.1.7 PROOF OF MAIN THEOREM 1”, in the expression following the first line, I didn’t understand how you obtained the last upper bound to ∇ f ( x t , i )
. Could you please explain how this is obtained? Score:
I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:
I am not convinced about the importance of consistency ratio and that it is a verifiable condition.
Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.
(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.
Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.
======================================= Post-Discussions =======================================
I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score.
Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice.
Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.",3.0,2,"the comment suggests that an epoch-wise analysis could provide insights into the behavior of optimization algorithms but does not specify which part of the paper needs this analysis or how it should be integrated. the suggestion is quite general and lacks explicit reference to any particular section or aspect of the paper that requires attention. this makes it weakly grounded. additionally, while the comment implies a need for depth in analysis (such as the effect of batch size and sampling strategies), it does not provide concrete specifics on current deficiencies or how to enhance the paper's content, which limits its specificity.",3,"the review point is weakly grounded but specific. it suggests performing an epoch-wise analysis in the context of finite sum settings to gain insights into optimization algorithms, but it does not cite a specific part of the paper that it is addressing. the reviewers mention what is wrong/missing—suggesting further investigation into batch size, sampling strategies, and comparative analysis—but the authors cannot precisely determine which section, figure, or experiment in the paper this feedback is addressing. the authors have a general idea of what might be addressed but not a precise one.",4,"the comment is fully grounded as it pertains to the suggestion of conducting epoch-wise analysis for finite sum settings. this would likely be obvious to the authors due to its specificity in context. however, it lacks full specificity on what exact part of the paper should incorporate these analyses or how precisely it should be integrated. nonetheless, it specifies what additional analyses could be done (e.g., investigating effect of batch size or sampling strategies), but does not reference a specific section or part of the paper."
"4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.",ICLR_2023_2057,"1 - The main idea of using ensemble of neural networks is trivial and very common in machine learning literature. The paper doesn't provide any specific adaptation to the homomorphic encryption domain. 2 - The discussion on the homomorphic encryption schemes is completely missing. What type of HE do you use? 3 - How do you preform majority voting in the encrypted domain? Most of HE schemes do not support argmax operation. 4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.",3.0,2,"the comment is weakly grounded as it does not explicitly mention a specific section or part of the paper that it addresses. the authors may have some hint or guess about where it could be applied, likely in a section discussing sequential ensembling or homomorphic encryption. however, the comment lacks specificity regarding what exactly is wrong or needs to be changed about the study on noise accumulation effects. while it mentions the limitation of homomorphic encryption with deep neural networks, it does not provide specific examples or details on how to address this limitation. thus, it is weakly grounded and not specific.",2,"the comment is weakly grounded and not specific. while it mentions 'sequential ensembling' and the 'context of homomorphic encryption', it does not refer to a specific part of the paper, making it difficult for the authors to pinpoint exactly where the issue lies. furthermore, although it notes a limitation regarding noise accumulation, it does not provide specific details or guidance on what exactly needs to be addressed or improved in the paper. as a result, the authors may not be able to confidently determine what part of the paper the reviewer is addressing or what specific changes are needed.",2,"the comment is weakly grounded and not specific. it does not mention a specific part of the paper where 'sequential ensembling' is discussed, hence authors may not confidently determine the context. while it specifies the issue about noise accumulation in homomorphic encryption, it does not specify where this limitation is discussed or how it should be addressed."
"2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.",9RugvdmIBa,"1. Such strategy requires extra parallel data, which might not exist in many datasets/tasks especially during the pre-training stage. The authors did not consider such cases to propose some cheap ways to acquire such parallel data. Also, utilizing the parallel data for training increase the size of context window, which require larger context window for models and might be expensive.
2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.",3.0,2,"the comment addresses a potential issue with the implementation of the question answering method, specifically pointing out the use of template mapping. this provides a clue about which part of the paper the authors need to look into (likely related to their methodology or approach for question answering), but it is not explicitly referring to any part of the paper, making the grounding weak. while the comment identifies a problem with generalization due to the use of template mapping, it does not provide specific details or suggestions for improvement, resulting in low specificity.",2,"the review point identifies an issue related to question answering and template mapping, suggesting that it might cause poor generalization. however, it lacks specificity and grounding. the comment does not provide explicit information about which part of the paper is addressed, nor does it clarify what exactly needs to be revised or improved upon. the authors might guess the intended meaning, but they cannot confidently determine the specific part being discussed or exactly what is wrong with it, thus resulting in weak grounding and low specificity.",2,"the comment is weakly grounded as it doesn't specify where in the paper the issue regarding template mapping is discussed, making it difficult for authors to pinpoint the exact part of the paper it addresses. it is also not specific, as it does not sufficiently elaborate on what needs to be addressed related to 'wh-types' questions not being transformable."
"- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.",NIPS_2017_502,"Weakness]
- This paper is poorly written.
- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
- Sharing the style of citations and bullet items is confusing.
- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing.
- The descriptions of baseline models are unclear.
- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled. [Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results. [Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.
Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article.
In sum, as far as I am concerned this work makes a contribution but is insufficient.",3.0,2,"the comment mentions an issue with comparing model performance, implying a problem with how the comparison is framed. however, it does not specify a particular section or figure of the paper, resulting in weak grounding. the authors might have a hint that this pertains to the results or methodology section, but they cannot confidently pinpoint it. additionally, the comment outlines a shift in focus towards proving the importance of projection errors and differentiating losses, but it does not pinpoint the precise aspect or detail changes needed in those sections, thus making it not specific. therefore, the score is 2 for weakly grounded and not specific.",3,"the review point is weakly grounded as the authors cannot confidently determine which part of the paper it addresses. it does not explicitly mention a specific part like a section or figure. however, it is specific in what needs to be addressed by clearly stating that comparing model performance on pre-trained synthetic data is deemed unfair and suggests that it would be more beneficial to demonstrate the importance of the proposed three projection errors. it also advises about the necessity of providing fine-tuning details on real-world datasets with different losses.",5,the comment is fully grounded as it discusses the need to compare model performance specifically tied to synthetic data pre-training. it clearly specifies what needs to be addressed: demonstrating the significance of the three projection errors and providing performance comparisons with fine-tuning on real-world datasets. this detail ensures the authors know precisely what aspect of their experiment needs revision and why.
"- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.",ARR_2022_236_review,"- My main criticism is that the ""mismatched"" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has. - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise)
- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant?",4.0,5,"the review point is fully grounded and specific. it explicitly refers to the issue with the 'collected 'pristine' set of tweets,' indicating that it may contain misinformation and out-of-context images. this provides full grounding since the authors can clearly identify the dataset components being addressed. moreover, it specifically suggests the need for 'more analysis around the quality of the collected dataset and the amount of noise it potentially has,' clearly specifying what needs to be improved in the paper. therefore, it meets both dimensions of grounding and specificity at the highest level.",5,"the review point is fully grounded and specific. it explicitly refers to the dataset used in the research as being 'artificially created' and further specifies the 'collected 'pristine' set of tweets.' it identifies potential issues with the dataset, namely noise, misinformation, and context problems. the comment also makes a specific suggestion for improvement by asking for more analysis on the quality and noise level of the dataset. this provides clear guidance to the authors on what and where improvements should be made, thus making it highly actionable.",2,"the review comment lacks precise grounding because it does not specify where in the paper the dataset is discussed or how the issues are presented. while the comment does mention specific concerns about the dataset being artificially created and potentially noisy, the authors cannot confidently identify which part of the paper this feedback addresses or what exactly needs to be revised regarding that section. as a result, the comment is weakly grounded and not specific enough."
"1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.",ACL_2017_331_review,"The document-independent crowdsourcing annotation is unreliable. - General Discussion: This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here.
1) Is it necessary to treat concept map extraction as a separate task?
On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.
2) How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.
I appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.",4.0,2,"the review point lacks specific grounding as it does not reference any particular section or illustration (such as a figure or table) in the paper where the issue is discussed. moreover, the comment is somewhat vague about what exactly needs to be improved or revised in the paper regarding the task of concept map extraction. although it hints at the comparison with generic summarization systems, it fails to clearly direct the authors to a particular part of their draft and specify clear actionable suggestions for improvement. therefore, it is weakly grounded and not specific.",4,"the review comment is fully grounded because it explicitly addresses the issue of whether concept map extraction should be treated as a separate task in the context of the paper. it references the task of concept map extraction, which is likely a specific part of the paper that the authors can identify. however, the comment is under-specific because it does not clearly specify the issue that needs to be addressed in this part. the comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenges related to node number and readability of concept maps, but it does not provide detailed guidance on what specific changes or improvements should be made to the paper. therefore, the overall specificity of the review point is limited, even though the grounding is clear.",2,the comment is weakly grounded and not specific. it questions the necessity of treating concept map extraction as a separate task but does not specify the section or part of the paper where this is discussed. it provides some reasoning but does not clearly guide the authors on what needs to be addressed.
- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?,ACL_2017_108_review,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?",4.0,4,"the review comment is fully grounded because it addresses specific issues in the paper, such as the 'first mention of multigraph', 'many previous', and 'solving this task', which are unique to the paper and authors can precisely identify them. however, the comment lacks specificity in detailing what exactly is wrong with 'many previous' and 'solving this task'. while it does suggest providing a short description for 'multigraph', it does not explain why 'many previous' sounds odd or specify which task 'solving this task' refers to. thus, it is fully grounded and under-specific.",4,"the review point is fully grounded, as it explicitly addresses specific issues within the paper: the need for a description of the notion of a multigraph and the clarity of certain phrases like 'many previous' and 'solving this task.' the specificity is strong regarding the issues of unclear phrases and potential lack of definition, but it falls slightly short of providing explicit instructions on how to address the phrase 'many previous sounds a little odd'. however, the grounding is clear as it directly points out the areas and offers insight into what the issues are with those areas, but there could be more specificity in guiding the authors on how to rectify those specific language issues.",5,"the comment is fully grounded and specific. it mentions specific parts of the paper that are addressed; the 'first mention of multigraph,' which would benefit from a short description, and the phrase 'many previous,' which sounds odd. the reviewer also points out a vague reference with 'solving this task' and asks for clarification on which task it refers to. this offers clear guidance for improvement."
"- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",ACL_2017_614_review,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?",5.0,5,"the comment explicitly refers to the poor performance on nouns and mentions the oracle gap for ppdbclus, clearly indicating the part of the results or analyses concerning these elements. it also provides a specific location in the text (lines 124-126) where the claim about the generalizability to all parts of speech is made. the reviewer clearly specifies the contradiction and desires an explanation for the performance gap. this makes the comment both fully grounded and specific, as it provides enough details for the authors to identify and address the issue in their paper.",5,"the review point is fully grounded and specific. it explicitly mentions the issue of poor performance on nouns and references specific sections (124-126) where the questionable claim is made about the generalizability of the clustering approach to all parts of speech. this enables the authors to pinpoint where in the paper the contradiction exists and also specifies the nature of the problem—namely, the discrepancy between the empirical results and the paper's claims. the comment asks for a better understanding of this gap, guiding authors on what to focus on to improve the paper.",5,the comment is fully grounded as it specifies the concern with the performance on nouns and provides a location in the paper (lines 124-126) where a contradictory claim is made. it is specific about what is problematic (the inconsistency in performance on nouns conflicting with the claim of generalizability across parts of speech) and asks for clarification and understanding of the gap between different clustering approaches.
"- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.",ACL_2017_333_review,"There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained. - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both ""represent the meaning"". Are both indeed necessary? Did you trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?
- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.
- page 1, lines 93-96: please provide a reference for this passage: ""This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.""
- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks.""
- Figure 2: What is ""MLP""? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).
- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).
- Table 2: what does ""#(ref)"" mean?
- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove ""the"" word in this line? "" SGD as our optimizing algorithms"" instead of ""SGD as our the optimizing algorithms.""
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "" council of europe again slams french prison conditions"" (again or against?)
- typo ""supper script"" -> ""superscript"" (4 times)",5.0,5,"the review point is fully grounded as it explicitly mentions section 4.3, focusing on 'model parameters and training.' the specificity is high because it clearly lists specific parameters (word embedding size, gru hidden states, alpha, beta 1 and 2, epsilon, beam size) that need further explanation regarding how their values were achieved. this provides the authors direct guidance on which part of their paper needs revision and what aspect of that part requires more elaboration.",5,"the review point is fully grounded, as it explicitly mentions section 4.3 and the specific elements within that section related to model parameters and training. it also clearly specifies what needs to be addressed, namely explaining how the values for various parameters such as word embedding size, gru hidden states, alpha, beta 1 and 2, epsilon, and beam size were achieved. this level of specificity and grounding allows the authors to directly locate the relevant section and understand the precise issues they need to address.",5,"the comment is fully grounded as it explicitly mentions the section (section 4.3) and the topic (model parameters and training) that is being addressed. it specifies exactly what needs to be explained further, including parameters like word embedding size, gru hidden states, alpha, beta 1 and 2, epsilon, and beam size."
"14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?",ARR_2022_202_review,"1. The write-up has many typos and some formulas/explanations are confusing.
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes.
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data.
4. More strong baselines should be included/discussed in the experiments.
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives.
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing.
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling?
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.”
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training?
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code?
12. Line 450: L_{pc} or L_{cp}| in Eq. 7?
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”?
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here?",5.0,5,"the comment explicitly mentions 'table 3,' providing full grounding by identifying a specific part of the paper. it further specifies what is wrong—namely, that strong baselines for mcnc are missing, citing an example source '[1]' to provide a reference point. this allows the authors to clearly understand the issue and what needs to be addressed, thus meeting both grounding and specificity criteria comprehensively.",5,"the review point explicitly mentions table 3, providing full grounding as the authors can accurately identify which part of the paper is being addressed. additionally, it specifies what needs to be addressed: the inclusion or justification of not including certain strong baselines in the comparison, thereby providing specificity. the reviewer even hints at specific external work, 'such as the baselines in [1],' which bolsters the specificity of the comment. this makes the review point fully grounded and specific, fulfilling the requirement for a score of 5.",5,"the comment is fully grounded and specific. it explicitly mentions table 3 and specifies what needs to be addressed, namely the lack of comparison with strong baselines, such as those in reference [1]. it also requests a justification for this omission, providing clear guidance for the authors on how to improve this part of the paper."
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures?,ACL_2017_108_review,"Clarification is needed in several places.
1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures? - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.",5.0,5,"the review point is fully grounded and specific. it explicitly mentions the part of the paper it addresses ('the discussion in section 5.2'), allowing the authors to easily identify the relevant section. the comment also specifies what needs to be addressed: the discussion is considered 'so abstract' that it doesn't provide insights into why the new model is better than mh. furthermore, the reviewer gives a specific suggestion for improvement by asking for examples of spurious structures. this level of detail helps guide authors in making precise revisions to their draft.",5,"the review point is fully grounded as it explicitly mentions 'section 5.2,' allowing authors to easily identify the specific part of the paper being addressed. additionally, it is specific because it details what needs to be improved in this section: the discussion is too abstract and lacks insights that demonstrate why the new model is better than mh, and it calls for examples of spurious structures to be provided. this level of detailed feedback makes it clear to the authors how they can improve their draft.",5,"the review point is fully grounded as it explicitly mentions the specific section of the paper (section 5.2) that needs addressing. it also specifies the issue, which is the abstract nature of the discussion and the need for examples of spurious structures to clarify why the new model is better than mh."
"681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?",ACL_2017_818_review,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",5.0,5,"the review point is fully grounded and specific. it explicitly mentions the need to discuss results related to inferring knowledge on objects and including results for model (b), which can be easily identified in the paper. additionally, it offers a specific suggestion to use consistent terminology in tables 1 and 2, as well as addressing the omission of 'objects' in the discussion of verbs. this detailed guidance allows the authors to precisely identify and address the areas needing improvement, fulfilling both grounding and specificity criteria.",5,"the review points are fully grounded and specific. the first comment explicitly mentions discussing the results for the task of inferring knowledge on objects and including results for model (b), while also suggesting consistency in terminology between tables 1 and 2. the second comment pinpoints the issue of not mentioning objects in the section 'latent in verbs'. both comments clearly identify the specific areas of the paper being addressed and specify what needs to be improved, ensuring the authors know exactly what to focus on to enhance their draft.",5,"the comment is fully grounded as it references specific locations in the paper ('681 as mentioned above') and specific tables ('tables 1 and 2'), making it clear where the issues are found. it is also specific as it provides detailed suggestions for improvement, such as discussing the results for the task of inferring knowledge on objects and including results for model (b). it also suggests using consistent terminology for the model in different tables and questions the omission of objects in a particular section ('778 ""latent in verbs""')."
"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.",ARR_2022_215_review,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.",5.0,5,"the review point is fully grounded and specific. it explicitly mentions the exact location in the paper (lines 078-086) where two hypotheses about multilinguality and country/language-specific bias are raised. the reviewer also clearly specifies the issue: these hypotheses are not further studied or discussed in the paper, which is misleading. furthermore, they suggest that the paper should delve deeper into these topics. this detailed feedback provides precise guidance on how the authors could improve the draft, by either addressing these hypotheses further or revising their presentation.",5,"the review comment is fully grounded as it explicitly mentions specific lines (078-086) where the two hypotheses about multilinguality and country/language-specific bias are discussed. additionally, the comment is specific because it identifies the issue: the hypotheses are not studied further in the paper and are not mentioned or discussed again, which is misleading. the reviewer clearly states that while the phrasing of the hypotheses might not be optimal, their underlying ideas are valuable and should be explored further in the paper. this feedback provides the authors with a clear understanding of what part of the paper is addressed and what the issue is, which helps them know where and how to improve their draft.",5,"the comment is fully grounded as it explicitly mentions where in the paper the hypotheses are raised (lines 078-086). it also specifically details the issue, which is that these hypotheses are not studied or discussed further in the paper. the comment provides clear guidance on what needs to be addressed with suggestions to delve deeper into these topics."
"- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.",ACL_2017_726_review,"- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it. General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of “near-state-of-the-art accuracies” on ATIS and GeoQuery, which doesn’t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.
- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.
- Table 6: what is the breakdown of the score by correctness and incompleteness?
What % of incompleteness do these queries exhibit?
- What is expertise required from crowd-workers who produce the correct SQL queries? - It would be helpful to see some analysis of the 48% of user questions which could not be generated.
- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages. - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?
I thank the authors for their response.",5.0,5,"the review point is fully grounded as it explicitly mentions 'table 4', allowing authors to easily identify the part of the paper being addressed. it is also specific because it clearly specifies what needs to be addressed regarding table 4, namely, clarification on the splits used for obtaining the atis numbers. this specificity can effectively guide the authors in improving their draft by providing the needed clarification.",5,"the review point is fully grounded and specific. it explicitly mentions 'table 4', allowing the authors to easily identify which part of the paper is being addressed. it is specific because it clearly asks for clarification on 'what splits are used for obtaining the atis numbers', providing clear guidance on what information needs to be added or clarified. this comment will help the authors improve their draft by addressing a specific gap in the information provided in table 4.",5,the comment is fully grounded as it explicitly mentions table 4. it clearly specifies what clarification is needed regarding the splits used for obtaining the atis numbers. the additional sentence thanking the authors for their response does not detract from the grounding or specificity of the comment.
"1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.",ICLR_2023_3449,"1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",5.0,5,"the review point explicitly mentions which parts of the paper it addresses, namely section 3.1 and 3.2, identifying the topic of spurious features. it provides comparison with well-known concepts, backdoor triggers, and gives specific examples from external work (chen et al. (2017) and gu et al. (2019)) to describe what is meant by artificial patterns used as triggers. this helps the authors understand both the parts of the paper that should be reconsidered and the nature of the critique, offering a clear pathway for improvement.",5,"the review point is fully grounded and specific. it explicitly references specific sections of the paper (section 3.1 and 3.2), thus providing full grounding. the comment is specific about the issue, drawing a comparison between spurious features in the paper and backdoor triggers, which are artificial patterns appearing infrequently in the training set. it even provides examples from external work by referencing chen et al. (2017) and gu et al. (2019) to clarify the issue. the detail helps the authors understand precisely which parts of their paper need attention and why, providing clear guidance for improvement.",5,"the comment is fully grounded as it specifically mentions sections 3.1 and 3.2, making it easy for authors to know what part of the paper is being addressed. additionally, it is specific as it discusses the similarity of spurious features to backdoor triggers and provides examples from external work, detailing what is wrong or needs to be considered concerning these sections."
"2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).",ACL_2017_318_review,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",5.0,4,"the review point is fully grounded as it references a specific part of the paper, namely the 'evaluation on the word analogy task' and mentions that 'the semantic relations are explicitly encoded by the sememes.' this gives clear cues to the authors about which section is being addressed. however, it is under-specific in detailing what exactly is unfair about the evaluation or how it should be improved beyond the acknowledgment of a potential bias. the rationale lacks detailed specifics about what makes the evaluation unfair or any concrete suggestions for improvement.",5,"the comment is fully grounded as it explicitly refers to the evaluation on the word analogy task, making it clear which part of the paper is being addressed. it is also specific because it mentions the core issue—the fairness of the evaluation given that semantic relations are explicitly encoded by the sememes. additionally, it references that the authors themselves acknowledge this point, suggesting a need for more detailed exploration in this area. this level of detail and grounding provides clear guidance that can help authors improve their draft.",4,"the comment is fully grounded as it refers to a specific task, the word analogy task, and also notes the potential issue with the evaluation being possibly unfair. however, while it hints at the reason (semantic relations are explicitly encoded by the sememes), it could provide more details or suggestions on how to address the fairness issue, thus lacking in specificity."
"2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",ARR_2022_12_review,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",5.0,5,"the review point is fully grounded as it explicitly mentions 'table 3' and specific metrics such as 'chinese mos-q' and 'chinese and english mos-v,' allowing the authors to accurately identify which part of the paper is being addressed. it is also specific as it raises questions about the interpretation of specific results within that table, specifically pointing out issues with the comparability of nvsb to gt mel a and the overlap in 95% ci for baseline and nvsb. this detailed feedback helps the authors clearly understand what needs to be addressed.",5,"the review comment explicitly mentions that it addresses the results shown in table 3, which provides full grounding. the comment then specifies the issues to be addressed: (a) the comparable nature of nvsb to gt mel a for chinese mos-q, and (b) the overlapping 95% ci for baseline and nvsb for chinese and english mos-v. this level of detail clearly guides the authors on what needs interpretation and potential improvement, thus making it fully grounded and specific.",5,"the comment is fully grounded as it explicitly mentions table 3 and specifically highlights what interpretation issues need to be addressed regarding the results for chinese mos-q and chinese and english mos-v with respect to nvsb, gt mel a, baseline, and overlapping 95% ci."
"4. Section 3.2.1: The first expression for J ( θ ) is incorrect, which should be Q ( s t 0 , π θ ( s t 0 ) ) .",ICLR_2021_863,"Weakness 1. The presentation of the paper should be improved. Right now all the model details are placed in the appendix. This can cause confusion for readers reading the main text. 2. The necessity of using techniques includes Distributional RL and Deep Sets should be explained more thoroughly. From this paper, the illustration of Distributional RL lacks clarity. 3. The details of state representation are not explained clear. For an end-to-end method like DRL, it is crucial for state representation for training a good agent, as for network architecture. 4. The experiments are not comprehensive for validating that this algorithm works well in a wide range of scenarios. The efficiency, especially the time efficiency of the proposed algorithm, is not shown. Moreover, other DRL benchmarks, e.g., TD3 and DQN, should also be compared with. 5. There are typos and grammar errors.
Detailed Comments 1. Section 3.1, first paragraph, quotation mark error for ""importance"". 2. Appendix A.2 does not illustrate the state space representation of the environment clearly. 3. The authors should state clearly as to why the complete state history is enough to reduce POMDP for the no-CSI case. 4. Section 3.2.1: The first expression for J ( θ )
is incorrect, which should be Q ( s t 0 , π θ ( s t 0 ) )
. 5. The paper did not explain Figure 2 clearly. In particular, what does the curve with the label ""Expected"" in Fig. 2(a) stand for? Not to mention there are multiple misleading curves in Fig. 2(b)&(c). The benefit of introducing distributional RL is not clearly explained. 6. In Table 1, only 4 classes of users are considered in the experiment sections, which might not be in accordance with practical situations, where there can be more classes of users in the real system and more user numbers. 7. In the experiment sections, the paper only showed the Satisfaction Probability of the proposed method is larger than conventional methods. The algorithm complexity, especially the time complexity of the proposed method in an ultra multi-user scenario, is not shown. 8. There is a large literature on wireless scheduling with latency guarantees from the networking community, e.g., Sigcomm, INFOCOM, Sigmetrics. Representative results there should also be discussed and compared with.
====== post rebuttal: My concern regarding the experiments remains. I will keep my score unchanged.",5.0,5,"the review point is fully grounded as it explicitly references a specific section (3.2.1) and a particular expression in the paper. it is clear to the authors which part is being addressed. additionally, the review point is very specific as it identifies the exact issue with the expression for j ( θ ) and provides the correct version it should be, allowing the authors to make the necessary correction. this level of detail both grounds the comment fully and clearly specifies the needed revision, thus meriting a score of 5.",5,"the review comment is fully grounded and specific. it explicitly mentions section 3.2.1, allowing the authors to confidently identify the part of the paper being addressed. additionally, it specifies that the first expression for j(θ) is incorrect and provides the corrected form as q(s t0, π θ(s t0)), clearly detailing what needs to be addressed.",5,the comment is fully grounded as it explicitly mentions section 3.2.1 and the specific expression for j(θ) that is incorrect. it provides precise information on what needs to be addressed by specifying the correct expression. this level of detail helps the authors identify the exact issue and make the necessary correction.
- 241: It would also be good to state the maximum number of tasks done by any annotator.,ARR_2022_215_review,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.",5.0,4,"the comment is fully grounded as it clearly refers to something specific that should be mentioned in the paper: the maximum number of tasks done by any annotator. it is obvious to the authors which part of the paper this would refer to, likely the section discussing annotation details or methodology. however, the comment lacks specificity in that it does not elaborate on why this information is important or what impact it might have, thus it is under-specific.",4,"the review point is fully grounded as it refers to a specific element within the paper ('241'), which likely indicates a line number or reference point that the authors can easily identify. however, the comment is under-specific as it does not provide further details on why stating the maximum number of tasks done by any annotator is necessary or how this information will improve the manuscript. it indicates what information should be added but not the reasoning behind this suggestion.",4,"the comment is fully grounded because it explicitly mentions line 241, allowing the authors to easily identify the part of the paper being addressed. however, while it indicates what information (maximum number of tasks done by any annotator) should be added, it does not elaborate on why this information is important or how it would improve the draft, which makes it less specific."
"- The parameters in Table 1, the model and the experiments seem to be only good for image data and ViT. Did the authors try to apply the same principles to other areas research areas such as NLP or simpler models in the image domain (CNNs)? I understand the latter might be due to the focus about state of the art performance, but it would show that the method can generalize to different architectures and tasks, not just transformers in vision.",ICLR_2023_2934,"- Fig. 1 leaves me with some doubts. It would seem that the private task is solved by using only a head operating on the learned layer for the green task (devanagari). This is at least what I would expect for the claims of the method to still uphold, because if the private task head can alter the weights of the Transformer layer 1 then information from the private task is flowing into the network. I would appreaciate if the authors could clarify this.
- Overall a lot of choices seem to lead towards the necessity for large compute power. The choice of modifying hyperparameters only by a one-hop neighbor is quite restrictive and it implies that we have to evolve/search for quite a while before stumbling on the correct hyperparams. The layer cloning and mutation probability hyperparameter is set at random by the evolutionary process, implying that the level of overall randomicity is very high and therefore large training times are needed to get stable results or be able to reproduce the claimed results (considering the authors use DNN architectures). The authors mention that ""the score can be defined to optimize a mixture of factors depending on application requirements"". It would have been nice to see what the tradeoff between training time and model size vs optimal multi-task performance is, especially considering these high levels of randomicity present in the proposed approach. (and also for others to be able to reproduce somewhat similar results on lesser compute).
- The parameters in Table 1, the model and the experiments seem to be only good for image data and ViT. Did the authors try to apply the same principles to other areas research areas such as NLP or simpler models in the image domain (CNNs)? I understand the latter might be due to the focus about state of the art performance, but it would show that the method can generalize to different architectures and tasks, not just transformers in vision.",5.0,5,"the review point is fully grounded and specific. it explicitly mentions the parameters in 'table 1' and clearly addresses the parts of the paper related to the model and experiments, specifically concerning their application to image data and vit. the comment is also specific in suggesting that the authors try to apply the principles to other research areas such as nlp and simpler models in the image domain (cnns). furthermore, the reviewer provides reasoning by acknowledging the current focus on state-of-the-art performance but suggests that demonstrating the method's generalization to different architectures and tasks would be beneficial. this detailed feedback provides clear guidance for the authors on areas for potential improvement and broadening the scope of their work.",5,"the review point is fully grounded because it explicitly mentions table 1, specifying which part of the paper it addresses. it also raises a question regarding the generalizability of the parameters, model, and experiments beyond image data and vit, indicating the need to explore other research areas such as nlp or simpler models like cnns. additionally, it clarifies why this exploration is relevant, as it would demonstrate the method's ability to apply to different architectures and tasks. the comment is specific in what it suggests should be addressed, thus fulfilling both grounding and specificity requirements.",4,"the comment is fully grounded as it explicitly references table 1, making it clear which part of the paper is being addressed. however, it is not as specific as it could be; it raises questions about the applicability of the model to other research areas without pinpointing a specific issue in the table or the model that might be problematic if applied elsewhere."
1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.,NIPS_2020_295,"1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones. 2. Some methods use epochs and pretrain epochs as 200, while the reported InvP uses 800 epochs. What are the results of InvP with epochs as 200? It would be more clear after adding these results into the tables. 3. The proposed method adopts memory bank to update vi, as detailed in the beginning of Sec.3. What the results would be when adopting momentum queue and current batch of features? As the results of SimCLR and MoCo are better than InsDis, it would be nice to have those results.",5.0,5,"the review point is fully grounded and specific. it explicitly addresses the experimental comparisons section by mentioning methods like moco and simclr that test results with wider backbones, such as resnet50 (2×) and resnet50 (4×). the specificity is high as it clearly states the expectation for the authors to consider evaluating their proposed invp method using these wider backbones. authors can clearly identify the part of the paper being addressed and the missing aspect in their experimental comparisons.",5,"this review point is fully grounded and specific. it explicitly identifies the part of the paper related to 'experimental comparisons' and specifies what needs to be addressed by mentioning specific methods ('moco and simclr') and the idea of testing results with 'wider backbones like resnet50 (2×) and resnet50 (4×)'. this level of detail allows the authors to easily understand what improvements or additional experiments are being suggested, making it highly useful for helping authors improve their draft.",3,"the comment is weakly grounded because it does not specify where in the paper the experimental comparisons are discussed. however, it clearly specifies what needs to be addressed, namely the inclusion of experimental comparisons with wider backbones like resnet50 (2×) and resnet50 (4×)."
"5. **(Performance of TTA methods)** This is an interesting observation, that using non-standard benchmarks breaks a lot of popular TTA methods. If the authors can evaluate TTA on more conditions of natural distribution shift, like WILDS [9], it could really strengthen the paper.",X4ATu1huMJ,"**Overall comment**
The paper discusses evaluating TTA methods across multiple settings, and how to choose the correct method during test-time. I would argue most of the methods/model selection strategies that are discussed in the paper are not novel and/or existed before, and the paper does not have a lot of algorithmic innovation.
While this discussion unifies various prior methods and can be a valuable guideline for practitioners to choose the appropriate TTA method, there needs to be more experiments to make it a compelling paper (i.e., add MEMO [1] as a method of comparison, add WILDS-Camelyon 17 [9], WILDS-FMoW [9], ImageNet-A [7], CIFAR-10.1 [8] as dataset benchmarks). But I do feel the problem setup is very important, and adding more experiments and a bit of rewriting can make the paper much stronger.
**Abstract and Introduction**
1. The paper mentions model restarting to avoid error propagation. There has been important work in TTA, where the model adapts its parameters to only one test example at a time, and reverts back to the initial (pre-trained) weights after it has made the prediction, doing the process all over for the next test example. This is also an important setting to consider, where only one test example is available, and one cannot rely on batches of data from a stream. For example, see MEMO [1].
2. (nitpicking, not important to include in the paper) “under extremely long scenarios all existing TTA method results in degraded performance”, while this is true, the paper does not mention some recent works that helps alleviate this. E.g., MEMO [1] in the one test example at a time scenario, or MEMO + surgical FT [2] where MEMO is used in the online setting, but parameter-efficient updating helps with feature distortion/performance degradation. So the claim is outdated.
3. It would be good to cite relevant papers such as [4] as prior works that look into model selection strategies (but not for TTA setting) to motivate the problem statement.
**Section 3.2, model selection strategies in TTA**
1. While accuracy-on-the-line [3] shows correlation between source (ID) and target (OOD) accuracies, some work [4] also say source accuracy is unreliable in the face of large domain gaps. I think table 3 shows the same result. Better to cite [4] and add their observation.
2. Why not look at agreement-on-the-line [5]? This is known to be a good way of assessing performance on the target domain without having labels. For example, A-Line [6] seems to have good performance on TTA tasks. This should also be considered as a model selection method.
**Section 4.1, datasets**
1. Missing some key datasets such as ImageNet-A [7], CIFAR-10.1 [8]. It is important to consider ImageNet-A (to show TTA’s performance on adversarial examples) and CIFAR-10.1, to show TTA’s performance on CIFAR-10 examples where the shift is natural, i.e., not corruptions. Prior work such as MEMO [1] has used some of these datasets.
**Section 4.3, experimental setup**
1. The architecture suite that is used is limited in size. Only ResNext-29 and ResNet-50 are used. Since the paper’s goal is to say something rigorous about model selection strategies, it is important to try more architectures to have a comprehensive result. At least some vision-transformer architecture is required to make the results strong. I would suggest trying RVT-small [12] or ViT-B/32 [13].
Why do the authors use SGD as an optimizer for all tasks? It is previously shown that [14] SGD often performs worse for more modern architectures. The original TENT [15] paper also claims they use SGD for ImageNet and for everything else they use Adam [16].
**Section 5, results**
1. (Table 1) It might be easier if the texts mention that each row represent one method, and each column represents one model selection strategy. When the authors say “green” represents the best number, they mean “within a row”.
2. (Different methods’ ranking under different selection strategies) The results here are not clear and hard to read. How many times does one method outperform the other, when considering all different surrogate based metrics across all datasets? If the goal is to show consistency of AdaContrast as mentioned in the introduction, a better way of presenting this might be making something similar to table 1 of [17].
3. What does the **Median** column in Table 2 and 3 represent? There is no explanation given for this in paper.
4. I assume the 4 surrogate strategies are: S-Acc, Cross-Acc, Ent and Con. If so, then the statement **“While EATA is significantly the best under the oracle selection strategy (49.99 on average) it is outperformed for example by Tent (5th method using oracle selection) when using 3 out of 4 surrogate-based metrics”** is clearly False according to the last section of Table 2: Tent > EATA on Cross-Acc and Con, but EATA > Tent when using S-Acc and Ent.
5. **(Performance of TTA methods)** This is an interesting observation, that using non-standard benchmarks breaks a lot of popular TTA methods. If the authors can evaluate TTA on more conditions of natural distribution shift, like WILDS [9], it could really strengthen the paper.",5.0,4,"the review point is fully grounded as it explicitly discusses 'the performance of tta methods' and possibly relating to observations made in the experiment section in the paper. although it references non-standard benchmarks and suggests evaluating under different conditions (citing wilds [9] as an example), it could be clearer in terms of specificity. while it suggests testing under more conditions of natural distribution shift, it does not clarify what specific shortcomings or issues were identified under the non-standard benchmarks used that need addressing. therefore, it is fully grounded but under-specific.",5,"the review point is fully grounded as it explicitly mentions the evaluation of tta (test-time adaptation) methods. it is clear that the comment addresses the performance of tta methods when using non-standard benchmarks. furthermore, it is specific as it suggests evaluating tta on more conditions of natural distribution shift, such as wilds, to strengthen the paper. the review point provides a clear and actionable suggestion for the authors to improve their paper by addressing a specific and identifiable issue.",3,"the comment is weakly grounded because it does not specify where in the paper the observation about tta methods using non-standard benchmarks is made. however, it is specific in suggesting an improvement by evaluating tta on more conditions of natural distribution shift, such as those found in wilds."
"- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.",BTr3PSlT0T,"- I express skepticism about whether the number of videos in the benchmark can achieve a robust assessment. The CVRR-ES benchmark includes only 214 videos, with the shortest video being just 2 seconds. Upon reviewing several videos from the anonymous link, I noticed a significant proportion of short videos. I question whether such short videos can adequately cover 11 categories. Moreover, current work that focuses solely on designing Video-LLMs, without specifically constructing evaluation benchmarks, provides a much larger number of assessment videos than the 214 included in CVRR-ES, for example, Tarsier [1].
- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.
- In the motivation, it is mentioned that the goal is to build human-centric AI systems. Does the paper's reflection on this point merely consist of providing a human baseline? I think that offering more fine-grained visual examples would be more helpful for human-AI comparisons.
- I think that the contribution of the DSCP is somewhat overstated and lacks novelty. Such prompt engineering-based methods have already been applied in many works for data generation, model evaluation, and other stages. The introduction and ablation experiments of this technology in the paper seem redundant.
- The discussion on DSCP occupies a significant portion of the experimental analysis. I think that the current analysis provided in the paper lacks insight and does not fully reflect the value of CVRR-ES, especially in terms of human-machine comparison.
- The phrase should be ""there exist a few limitations"" instead of ""there exist few limitations"" in line 520.
- The paper does not provide prompt templates for all the closed-source and open-source Video-LLMs used, which will influence the reproducibility.
The problems discussed in this paper are valuable, but the most crucial aspects of benchmark construction and evaluation are not entirely convincing. Instead, a significant amount of space is dedicated to introducing the DSCP method. I don't think it meets the acceptance standards of ICLR yet. I will consider modifying the score based on the feedback from other reviewers and the authors' responses. ***
[1] Wang J, Yuan L, Zhang Y. Tarsier: Recipes for Training and Evaluating Large Video Description Models[J]. arXiv preprint arXiv:2407.00634, 2024.",5.0,5,"the review point is fully grounded as it clearly refers to the lack of detail regarding the distribution of video lengths in the dataset, which is an identifiable part of the paper. it specifies the issue that needs to be addressed by suggesting the inclusion of a table showing the distribution of video lengths and explanations on how a balanced representation was ensured. this makes it easy for the authors to understand what part of their draft needs revision and how to improve it.",5,"the comment is fully grounded as it explicitly mentions the need to address the 'distribution of videos of different lengths within the benchmark,' which is an identifiable part of the paper. it is specific because it clearly states what is needed: 'a table showing the distribution of video lengths across the dataset' and an explanation of 'how they ensured a balanced representation of different video lengths across the 11 categories.' this detailed guidance on what is missing and how to address it will help the authors improve their draft.",5,"the comment is fully grounded, as it explicitly refers to the lack of explanation within the paper regarding the distribution of videos of different lengths. it provides a clear guideline on what specific additions (a table showing distribution and explanations) are needed to address the issue."

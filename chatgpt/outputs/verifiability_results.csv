review_point,paper_id,focused_review,verifiability_label,chatgpt_verifiability_definitions_score,chatgpt_verifiability_definitions_rationale,chatgpt_verifiability_definitions_examples_score,chatgpt_verifiability_definitions_examples_rationale,chatgpt_verifiability_definitions_incontext_learning_score,chatgpt_verifiability_definitions_incontext_learning_rationale
1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.,ACL_2017_554_review,"1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families.
2) The paper does not talk about the training speed impact with more details.
- General Discussion:",1,1,"the review point contains a claim that the paper lacks a discussion on theory proofs and convergence properties of the proposed algorithm. however, there is no justification or evidence provided to support why these aspects are necessary for the paper. the reviewer misses an opportunity to explain the importance of including these details, making the claim unverifiable.",1,"the review point contains a claim that the paper does not delve into theory proofs or present convergence properties of the proposed algorithm. however, no supporting evidence or justification is provided to verify this claim. there is no explanation or reasoning given as to why these elements are considered important or necessary for the paper. to make this claim verifiable, the reviewer could have explained the importance of these components in the context of the paper or referenced related works that include such elements.",no claim,"there is no claim in this comment, as it only states something is missing in the paper without expressing any subjective opinion or suggestion for improvement."
"4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.",ACL_2017_779_review,"However, there are many points that need to be address before this paper is ready for publication.
1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.
Also, how is the montecarlo sampling done? 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ). 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space. - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here 392 : Do you mean 2016?
Nitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.",1,3,"the review point contains a claim that a whole section plus experimental results is too much space dedicated to fleshing out assumptions. while it provides a subjective opinion on the paper's structure and content, it lacks specific examples or detailed explanations that would justify why the space allocated is excessive. the reviewer suggests that it is too much space, which implies a suggestion for reduction, but the authors would need to make significant effort to understand the justification and reasoning behind this claim, as the comment does not thoroughly elaborate on the potential impact or relevance of reducing this space.",1,"the review point contains a claim that dedicating a whole section of the paper plus experimental results is too much space for discussing assumptions, but it does not provide any supporting evidence or justification for this opinion. the reviewer does not explain why they believe this amount of space is excessive or why it may not be useful, making it difficult for the authors to act on this suggestion.",3,"the claim is 'dedicating a whole section of the paper plus experimental results is a lot of space', which is somewhat verifiable. the reviewer expresses an opinion on the paper's allocation of space for assumptions and experimental results, but the reasoning is not extensively detailed. the comment provides some context but lacks in-depth explanation or references to support the claim fully."
"- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.",ARR_2022_82_review,"- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer). - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.",1,3,"the review point contains a claim suggesting that an ablation study may be needed to demonstrate the importance of post-processing steps. the reviewer partially supports this claim by stating that although the paper proposes post-processing steps to filter out 'false-positive' neurons, it does not show their importance. this offers some context but lacks detailed reasoning or references to justify why an ablation study is necessary. thus, it is somewhat verifiable, requiring significant effort from the authors to fully understand the justification.",3,"the review point contains several claims: the use of integrated gradients is noted as studied in existing papers, and there’s an assertion about the absence of evidence showing the importance of post-processing steps. the suggestion for an ablation study is an implicit claim that such a study would validate the importance of these steps. these claims are somewhat supported by referencing the absence of demonstrated importance for post-processing, but they lack specific examples, detailed explanations, or references to support their validity. while the reviewer points out a potential gap, the justification provided is not fully articulated, requiring significant effort from the authors to address or refute the claims.",3,"the claim in the comment is that post-processing steps are important and possibly need an ablation study. the reviewer implies these steps have significance but lacks specific evidence or references to fully support the claim. the suggestion for an ablation study provides some guidance, which partially verifies the claim, but more detailed justification is needed."
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.,ACL_2017_818_review,"- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations. What are some interesting action verbs and corresponding attributes relations? The paper also lacks analysis/discussion on what kind of mistakes their model makes.
- The number of object pairs (3656) in the dataset is very small. How many distinct object categories are there? How scalable is this approach to larger number of object pairs?
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.
General Discussion/Suggestions: - The authors should discuss the following work and compare against mining attributes/attribute distributions directly and then getting a comparative measure. What are the advantages offered by the proposed method compared to a more direct approach?
Extraction and approximation of numerical attributes from the Web Dmitry Davidov, Ari Rappoport ACL 2010 Minor typos: 1. In the abstract (line 026), the authors mention 'six' dimensions, but in the paper, there is only five.
2. line 248: Above --> The above 3. line 421: object first --> first 4. line 654: more skimp --> a smaller 5. line 729: selctional --> selectional",1,1,"the review comment contains a claim that the selection process for frame similarity factors and attributes similarity factors is unclear. however, it does not provide any supporting evidence or justification for this claim. there are no explanations, specific examples, or references to help the authors understand why these factors are unclear. thus, the comment is unverifiable.",no claim,"the review point does not contain any claim, suggestion, or opinion. it is a factual statement expressing a lack of clarity about how specific factors are selected. it does not require justification or supporting evidence, as it does not make a judgment or suggestion about the paper.",1,"the claim is that 'it's a bit unclear', which is a subjective observation. however, there is no justification or evidence provided to support this claim, making it unverifiable."
- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -,ACL_2017_494_review,"- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes ""morph-fitting"", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of ""attract"" and ""repel"" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does ""looking"" - ""look"" + ""walk"" = ""walking""? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.",1,2,"the review point contains two claims: that the work is a 'fairly straightforward extension of existing retrofitting work,' and a suggestion for 'some additional baselines (e.g. character embeddings).' the statement about the work being a straightforward extension lacks supporting evidence or justification. the suggestion for additional baselines is valid but lacks sufficient reasoning or explanation as to why these baselines would be beneficial or necessary. therefore, the support is minimal and vague, placing it in the 'borderline verifiable' category.",2,"the review point contains a claim suggesting that it would be beneficial to include additional baselines, such as character embeddings, which implies that the current baselines may be insufficient. however, the justification provided is vague and lacks detailed reasoning or evidence as to why character embeddings or other baselines would improve the evaluation. the claim is made without a clear rationale or context, making it borderline verifiable.",3,"the claim is ""fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines"". it is somewhat verifiable as it suggests adding baselines like character embeddings, but lacks detailed justification or references for why these additional baselines are necessary or how they would enhance the evaluation."
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.,ARR_2022_303_review,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses. However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text. Table 1's revised caption mentions 2.9K paragraphs as the size.
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR.
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column. I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work. S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base",1,2,"the review point contains the claim that the paper relies too heavily on supplemental space, affecting its independence. however, the justification for why this is problematic is vague and insufficiently articulated. while the reviewer mentions specific sections (s3.1 and sup. fig. 6) and describes this as a recurring issue within model comparison and span vs. sentence investigation, they do not explicitly clarify the negative impact this reliance has on the paper's quality or how it could be rectified. as a result, the authors might find it challenging to fully understand or address the concern based on this feedback alone.",3,"the review point contains a claim that the paper relies heavily on supplementary materials, which makes it not truly independent. this claim is somewhat supported by the reviewer mentioning references to supplementary figures and sections (e.g., s3.1, sup. fig. 6), indicating specific instances where this reliance occurs. however, the comment lacks detailed explanation or sufficient reasoning on why this is problematic or why it affects the paper's independence, thus requiring the authors to infer much of the justification on their own.",3,"the claim is that 'the paper is not truly independent given this problem', and while it provides some support regarding reliance on supplemental space (s3.1 reference to sup. fig. 6), it lacks a detailed justification or explanation. therefore, the verification is somewhat present but not fully comprehensive."
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).,ARR_2022_98_review,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges.
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system. 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.",1,1,"the review point suggests that it would be beneficial to see examples of the system on actual texts compared to other components and models. this is a subjective opinion suggesting a change, thus constituting a claim. however, the reviewer does not provide any justification or reasoning for this suggested addition, nor are there references or logical explanations supporting why such examples would be beneficial. consequently, this review point is considered unverifiable as it lacks the necessary evidence or explanation to support the claim.",1,"the review point contains a suggestion ('it will be nice to see some examples of the system on actual texts') but does not provide any justification or reasoning as to why examples are needed or how they would improve the paper. as such, it lacks supporting evidence or context, making it unverifiable.",2,"the claim is 'it will be nice to see some examples', which suggests a desire for additional content. this claim is not fully backed by any specific rationale or reference to why these examples are necessary, making the verification limited."
- A number of claims from this paper would benefit from more in-depth analysis.,ARR_2022_232_review,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed.
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements. In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset? Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption.",1,1,"the review comment implies a claim that the paper's claims require more in-depth analysis. however, it does not provide any supporting evidence, examples, or justification for why or how the in-depth analysis would be beneficial or what specific claims require further analysis. therefore, it lacks verifiability as per the criteria provided.",1,"the review point contains a claim suggesting that a number of claims in the paper require more in-depth analysis. however, it does not provide any specific examples, reasoning, or evidence to support why these claims need further analysis or which parts of the paper should be elaborated upon. due to the lack of any justification or verification, this comment receives a score of 1, which is 'unverifiable'.",1,"the claim is that some claims in the paper would benefit from more in-depth analysis, but there is no justification or evidence provided to clarify which claims are lacking or why they need more analysis. this makes the claim unverifiable."
"5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?",ACL_2017_489_review,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.",1,1,"the review point contains a claim indicating that the reviewer found it difficult to understand the overall results and coherence of the experiments related to the research question and hypothesis. however, there is no supporting evidence or specific reasoning provided to justify why the reviewer finds it difficult to follow the empirical results and analyses. no logical reasoning, common sense appeal, or external references are offered to support this claim, leaving it unverifiable for the authors. the authors would struggle to improve their paper based on this feedback without further detail.",1,"the review comment includes a claim that the paper is difficult to understand in terms of how the empirical results and analyses connect to the research question and specific hypothesis. however, it provides no supporting evidence or specific examples to justify this claim. without further detail or reasoning, the authors are left without guidance on how to address this issue in their paper. therefore, the claim is considered unverifiable.",3,"the claim is that it is difficult to get the whole picture from the empirical results and analyses presented. the comment suggests a lack of clarity in how the experiments relate to the overall research question and specific hypothesis. while the reviewer questions the coherence of the results and how they fit together, the justification isn't extensive and would require significant effort from the authors to address."
"- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).",ICLR_2022_3352,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.",1,1,"the comment contains a claim that the paper 'may cause confusion' in several places, but it fails to provide any supporting evidence, justification, or specific examples. the reviewer references 'point 3' without elaborating or explaining how or why certain parts could be confusing, making it unverifiable as a standalone review point.",1,"the review point contains a claim stating that there are parts of the paper that may cause confusion. however, it fails to provide any supporting evidence or specific examples to justify this claim. the reviewer mentions that a list is provided in another point, but without that information here, this comment remains unverifiable.",1,"the claim is that there are places in the paper that may cause confusion, but no detailed reasoning or evidence is provided to support this. the comment only states the potential for confusion, without further elaboration or justification."
- the required implicit call to the Witness oracle is confusing.,NIPS_2018_914,"of the paper are (i) the presentation of the proposed methodology to overcome that effect and (ii) the limitations of the proposed methods for large-scale problems, which is precisely when function approximation is required the most. While the intuition behind the two proposed algorithms is clear (to keep track of partitions of the parameter space that are consistent in successive applications of the Bellman operator), I think the authors could have formulated their idea in a more clear way, for example, using tools from Constraint Satisfaction Problems (CSPs) literature. I have the following concerns regarding both algorithms: - the authors leverage the complexity of checking on the Witness oracle, which is ""polynomial time"" in the tabular case. This feels like not addressing the problem in a direct way. - the required implicit call to the Witness oracle is confusing. - what happens if the policy class is not realizable? I guess the algorithm converges to an \empty partition, but that is not the optimal policy. minor: line 100 : ""a2 always moves from s1 to s4 deterministically"" is not true line 333 : ""A number of important direction"" -> ""A number of important directions"" line 215 : ""implict"" -> ""implicit"" - It is hard to understand the figure where all methods are compared. I suggest to move the figure to the appendix and keep a figure with less curves. - I suggest to change the name of partition function to partition value. [I am satisfied with the rebuttal and I have increased my score after the discussion]",1,1,"the review point contains a claim that the required implicit call to the witness oracle is confusing. however, it lacks any supporting evidence or justification. the reviewer does not explain why it is confusing or provide any context or examples to support this claim, making it unverifiable.",1,"the review point contains a claim that the 'implicit call to the witness oracle is confusing,' which is a subjective opinion about how the method is presented or explained in the paper. however, this statement lacks supporting evidence or justification for why it is confusing, providing no logical reasoning or references to help the authors understand and address this issue. therefore, the comment is considered unverifiable.",2,"the claim is that the call to the witness oracle is confusing. however, the review point lacks sufficient verification or justification, making it difficult for the authors to understand why it is considered confusing or how they might address this confusion."
1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.,5UW6Mivj9M,"1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.
2) Relatedly, it was hard to discern what was novel in the paper and what had already been tried by others.
3) Since the improvement in numbers is not large (in most cases, just a couple of points), it is hard to tell if this improvement is statistically significant and if it translates to qualitative improvements in performance.",1,2,"the comment contains a claim that the paper is hard to follow, specifically regarding the experimental procedures and evaluations. the reviewer attempts to support this claim by saying they read it multiple times but still struggled. however, this justification is somewhat vague as it lacks specific examples or explanations of what made the paper hard to follow and does not provide guidance on how to improve clarity. the authors may struggle to understand exactly what areas need improvement and why.",1,"the review comment makes a claim that the paper is extremely hard to follow, specifically regarding the experimental procedures and evaluations. however, it does not provide any justification, reasoning, or examples to support this claim. the reviewer shares their personal experience of reading the paper multiple times but does not elaborate on what specifically makes it hard to follow, leaving this claim unverified. without elaboration or examples, the authors would struggle to identify how to address the issue, resulting in a score of 1 for unverifiability.",1,"the claim is that 'the paper was extremely hard to follow', which indicates a subjective opinion that the authors might need to address. however, there is no supporting evidence or detailed reasoning provided to substantiate this claim, making it unverifiable."
"- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",ACL_2017_614_review,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?",2,3,"the review point contains a claim that the performance of ppdbclus is not generalizable to all parts of speech as suggested by the paper, due to its poor performance on nouns. the reviewer also expresses concern about the oracle gap for ppdbclus being higher than most clustering approaches. while the review provides some logical reasoning by highlighting the discrepancy between the expectation and actual performance, it lacks specific examples, detailed explanations, or references to support the claim about generalizability. the authors would understand the concern but may need more detailed information to address the issue comprehensively.",4,"the review comment contains multiple claims about the performance of the clustering approach and its generalizability across different parts of speech. the reviewer points out that the oracle gap for ppdbclus is higher than most clustering approaches and identifies a contradiction with the paper's claim about overall generalizability (lines 124-126). the comment provides logical connections between the performance results and the claims made in the paper. however, it could be more verifiable by offering further detailed explanations or external references to strengthen the evaluation of the clustering approach's performance. thus, the support is mostly sufficient but has minor gaps.",4,"the review point makes the claim of 'relatively poor performance on nouns,' supporting it with a comparison of oracle gap for ppdbclus against other clustering approaches. it also points out a contradiction in the claims regarding the generalizability of the clustering approach to all parts of speech. although the reviewer provides reasoning and comparison to other approaches, there is a minor gap as it could be further strengthened with additional evidence or examples."
"3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",ACL_2017_318_review,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",3,4,"the review point contains claims, such as the need for clearer comparison of models and the inclusion of more baselines. the first claim regarding the comparison of models is supported by the suggestion of considering the mst baseline as an example, indicating a clear expectation for clarification or modification. the statement that the paper’s strength could be improved by including more baselines is logical and common sense, as stronger baselines typically provide more robust comparisons in academic research. however, it mentions related work without providing specific references, which represents a minor gap that could be enhanced by including more detailed examples or references. thus, the claim is mostly but not completely verifiable.",3,"the review point expresses a claim regarding the lack of clarity in comparing the proposed models to those that only consider different senses but not sememes. it suggests that the mst baseline might be such a model, but this is not sufficiently described. additionally, it recommends the inclusion of more baselines from related work to strengthen the paper. the comment provides some support for its claims by implying the importance of stronger baselines and suggesting that the mst might be a relevant comparison. however, it lacks specific examples or references to substantiate the claims about the inadequacy of current descriptions and the necessity of additional baselines. thus, it is somewhat verifiable but with room for more comprehensive justification.",3,"the claim is that 'the paper would be stronger with the inclusion of more baselines based on related work'. the review mentions the unclear comparison to models that focus on different senses and suggests the possibility that the mst baseline might be such an example. while there is support for the claim by pointing out the need for more baselines, further details or references would strengthen the verification, making it somewhat verifiable."
"1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.",ACL_2017_331_review,"The document-independent crowdsourcing annotation is unreliable. - General Discussion: This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here.
1) Is it necessary to treat concept map extraction as a separate task?
On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.
2) How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.
I appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.",3,4,"the review point contains claims regarding the necessity of treating concept map extraction as a separate task. the reviewer supports their claim by providing logical reasoning: they mention that many generic summarization systems already build a similar knowledge graph, which implies redundancy, and express concern about readability issues related to an increasing number of nodes in a concept map. while the reviewer's reasoning is mostly clear and logical, it could benefit from additional details or specific examples/references to further strengthen the argument. nonetheless, the justification is sufficient for the authors to understand the critique and consider potential improvements.",4,"the review point contains a claim questioning the necessity of treating concept map extraction as a separate task. this is supported by the reasoning that generic summarization systems already build a similar knowledge graph, which implies overlap. additionally, it mentions an increase in complexity as the node number grows, potentially impacting readability. while these points provide a mostly solid basis for the claim, the reasoning could be made more robust by including references to specific systems or studies that have addressed similar challenges. overall, the logical arguments provided fill minor gaps, justifying a 'mostly verifiable' score.",3,"the review point suggests an implicit claim that treating concept map extraction as a separate task may not be necessary due to existing systems building similar knowledge graphs, and that general summaries could be more readable. it provides some reasoning but lacks detailed support or references, making it somewhat verifiable."
"1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.",ARR_2022_114_review,"By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials). For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.
As another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\in \mathcal{R}^{r \times r}$ is parameterized as $V U^T$ where $U,V\in \mathcal{R}^{r \times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\mathcal{R}^{r \times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here?
In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.
2. For each HMM with rank r, add a baseline smaller HMM with state size being r.",3,2,"the review point suggests adding a baseline smaller pcfg with specific parameterizations, which implies a claim that this would be beneficial. however, the justification is minimal and vague. the reviewer mentions that 'parsing f-1 might not be directly comparable, but perplexity can still be compared' without offering detailed reasoning or evidence on why this baseline is necessary or advantageous. the explanation lacks depth and fails to fully articulate why these changes should be made, making it difficult for the authors to understand and implement the suggestion. thus, the comment is borderline verifiable with some support but is insufficiently detailed.",1,"the review point contains a suggestion for adding a baseline with a specific parameterization for the matrices in the pcfg. however, it lacks any justification or reasoning for why this particular setup should be implemented. the comment mentions a potential issue with comparability in parsing f-1 but does not provide any explanation or evidence to support the suggestion or address the comparability concern. as such, the claim is unverifiable without further explanation or context.",no claim,"the review point suggests adding a baseline smaller pcfg with state size being r, but it doesn't contain any subjective opinion or judgment. it merely provides a suggestion without requiring verification."
"3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.",NIPS_2016_9,"Weakness: The authors do not provide any theoretical understanding of the algorithm. The paper seems to be well written. The proposed algorithm seems to work very all on the experimental setup, using both synthetic and real-world data. The contributions of the papers are enough to be considered for a poster presentation. The following concerns if addressed properly could raise to the level of oral presentation: 1. The paper does not provide an analysis on what type of data the algorithm work best and on what type of data the algorithm may not work well. 2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this. 3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.",3,4,"the review comment contains a claim suggesting that empirical evidence should be shown to support the claim made in the paper about the proposed algorithm's performance on the column subset selection problem. the comment is mostly verifiable as it provides logical reasoning by referencing the paper's claim and suggesting evidence to support it. however, it could be more detailed by specifying what kind of empirical evidence is needed, leaving a minor gap in support.",1,"the review point contains a claim that empirical evidence should be provided to demonstrate that the proposed algorithm works better for the column subset selection problem. however, the review does not provide any justification or evidence as to why this suggestion is necessary. there is no logical reasoning, reference to common practices, or external sources to substantiate the claim, making it unverifiable.",3,"the claim in this review point is that it 'would be good to show some empirical evidence' for the proposed algorithm's effectiveness in the column subset selection problem, as stated in the paper's contribution. the comment suggests that empirical evidence is needed to support the claim made in the paper. while the reviewer alludes to the need for evidence and ties it to part of the paper's contribution, the rationale is somewhat lacking in detail, such as what kind of empirical evidence might suffice or why it is crucial, leaving the reasoning somewhat incomplete."
* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.,NIPS_2020_257,"* In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section. * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks). * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.",3,1,"the review point contains a claim that the need for reinforcement learning for a static vqa task may make the approach less data efficient and harder to train models that use gradient descent. however, this claim is not supported by any evidence, reasoning, or references. the reviewer states it as a personal belief without providing justification, making it unverifiable. authors would benefit from additional information or reasoning to better understand and address this claim.",2,"the review point presents a claim that the need for reinforcement learning in the context of a static vqa task may reduce data efficiency and increase training difficulty. the reasoning provided is based on the reviewer's personal belief, which is a subjective opinion. the claim is identified as potentially debatable, but it lacks explicit supporting evidence such as data, references to existing literature, or a more detailed explanation that would make it easier for authors to assess and follow the justification. therefore, it is somewhat supported but falls into the category of 'borderline verifiable' as authors may struggle to utilize this feedback effectively without further elaboration.",2,"the claim here is that using reinforcement learning for a static vqa task may be a potential weakness, making the approach less data efficient and harder to train with gradient descent. the reviewer mentions this as a personal belief and acknowledges that it can be open for debate. while there is some common-sense reasoning that reinforcement learning in certain contexts could lead to inefficiencies, it is vaguely articulated and lacks concrete evidence or references, making it borderline verifiable."
"2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).",ACL_2017_318_review,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",4,4,"the review point contains a claim that the evaluation on the word analogy task is unfair due to the explicit encoding of semantic relations by sememes. this claim is supported by referencing the authors' own acknowledgment of this point, indicating the reviewer has used available information from the paper to back up their claim. however, while the reasoning is made clear, the reviewer could bolster the verifiability by providing more detailed explanation on how exactly this explicit encoding impacts fairness, or by citing common practices or standards in the field for evaluations in similar contexts. hence, it is rated as 'mostly verifiable'.",4,"the review point contains a claim that the evaluation on the word analogy task is unfair due to the explicit encoding of semantic relations by sememes, which the authors themselves acknowledge. this claim is supported by reasoning that aligns with common sense in the field, since semantic relationships directly encoded could lead to biased performance in word analogy tasks. the reviewer also hints at additional details provided below, which suggests there is more context or evidence backing up the claim, albeit not provided directly in this statement. the claim is sufficiently supported by logical reasoning and common understanding, but it could be bolstered by specific examples or references to make it fully verifiable.",4,"the claim here is 'task seems a bit unfair', and it has been mostly verified by mentioning that the semantic relations are explicitly encoded by the sememes, as pointed out by the authors themselves. this provides substantial support for the claim, though a bit more in-depth explanation could further solidify the verification."
"- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.",ARR_2022_236_review,"- My main criticism is that the ""mismatched"" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has. - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise)
- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant?",4,3,"the review point contains a claim that the dataset might have a lot of noise, which is a subjective opinion. the reviewer provides some support by suggesting that the collected 'pristine' set of tweets may actually contain misinformation and out-of-context images. however, while this reasoning gives a basis for the concern, it lacks explicit support or references to strengthen the claim further. the comment implies a suggestion for more analysis around the dataset's quality and noise, but it requires significant effort from the authors to act on the feedback without additional guidance or references. thus, it falls under 'somewhat verifiable' as it gives partial support without comprehensive justification.",4,"the review point contains a claim that the artificially created dataset might have a lot of noise, including misinformation and out-of-context images. the reviewer supports this claim by providing logical reasoning about the potential issues with the 'pristine' set of tweets. however, while the reviewer suggests that more analysis around the dataset's quality is needed, the comment lacks detailed evidence or references. the reasoning is mostly supported by plausible concerns regarding dataset quality, making it mostly verifiable but with minor gaps that could be filled with further details or examples.",3,"the comment includes a claim about the dataset potentially having a lot of noise, particularly in the 'pristine' set of tweets. while the reviewer provides specific examples of what the noise might consist of, such as misinformation or out-of-context images, the justification lacks detailed evidence or references to support this observation. therefore, the comment is somewhat verifiable but lacks sufficient supporting details."
- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?,ACL_2017_108_review,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?",4,2,"the review point contains claims that suggest the need for improvements in clarity and description. however, the support for these claims is insufficient and somewhat vague. the comment mentions that 'some readers may benefit' from a short description of a multigraph, but it does not provide a strong rationale or evidence to support this suggestion. additionally, the comment on phrases sounding odd and the ambiguity of 'solving this task' lacks detailed explanations or specific suggestions for improvement, making it challenging for authors to follow the justification. this results in it being 'borderline verifiable'.",1,"the review point contains claims about how the text could be improved, such as providing a short description of the multigraph and addressing wording issues ('many previous: sounds a little odd'). however, it lacks any supporting evidence or justification to verify these claims. the reviewer does not provide reasoning or suggestions about how a description of a multigraph might benefit readers, nor does it explain why 'many previous' sounds odd, leaving the claims unverifiable.",2,"the comment contains an implicit claim that providing a short description of the notion of a multigraph would benefit some readers, which is relatively vague and lacks strong justification. there are also subjective opinions that 'many previous' sounds odd and a query about 'solving this task'. the claims lack comprehensive verification or supporting details."
"- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).",CEPkRTOlut,"No ethics section, but there are ethical issues that deserve discussion (see the ethics section).
Also a few, mostly minor points:
- When the corpus was created, participants were told to speak in such a way to make the intent of the speech unambiguous. This may lead to over-emphasis compared with natural speech. There was no mention of any evaluation of the data to avoid this.
- The corpus was created with only ambiguous sentences, and the non-ambiguous content was taken from another source. There is a chance that different recording qualities between news (LSCVR) and crowdsourced data could artificially raise the ability of the model to distinguish between ambiguous (tag 1 or 2) and non-ambiguous (tag 0) sentences.
- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).
- It would be hard to reproduce the fine tuning of the IndoBART model without a little more information. Was it fine-tuned for a certain number of steps, for example?",4,4,"the review point contains a claim suggesting that the conclusion about the superiority of the direct model over the other proposed system is questionable, given the significant difference in the amount of data used for training them. the reviewer supports this claim by providing logical reasoning: the difference in training data size may influence the performance comparison, especially since the performance difference is only a few percentage points. the reasoning is sufficiently supported by pointing out the relative amount of training data and its potential impact on the conclusions about model performance. however, it could be improved by providing more detail or specific examples of how the data disparity impacts model comparison. overall, the claim is mostly verifiable with minor gaps.",4,"the review point presents a claim regarding the comparison between the text disambiguation model and the end-to-end system, suggesting that the conclusion about the superiority of the direct model is questionable. this claim is primarily based on the difference in the amount of data used for training each model, as well as the minor performance difference between the proposed systems. the reasoning provided is mostly verifiable, as it logically questions the conclusion drawn from the experimental setup and results, while highlighting a potential flaw in the basis for comparison. however, the reviewer could improve the explanation by providing more detailed mathematical or statistical justifications or by referencing specific sections in the paper where the performance and data usage are documented, hence the minor gap leading to a score of 4.",4,"the claim here is that the conclusion about the superiority of the direct model is questionable due to the significantly lower amount of data used for training the text disambiguation model compared to the end-to-end system. this claim is mostly verified by comparing the data usage and the reported performance difference between the models, but the explanation lacks some details or references that could make it fully verifiable."
"- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",NIPS_2017_645,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",4,4,"the review point contains a claim that the authors have mainly focused on ssc and failed to contrast their method with several other subsequent methods such as tsc and greedy subspace clustering. the claim is mostly verifiable because the reviewer provides specific methods (tsc and park's method) that should be considered, claiming they are computationally efficient and offer similar guarantees. the rationale is based on the importance of comparative analysis in academic work, which is common knowledge in the field. however, the justification could be strengthened by explaining why these methods are particularly relevant or by providing references. thus, it is sufficiently supported with minor gaps.",3,"the review point contains a claim that the authors focus mainly on ssc and lack contrast with other methods like tsc and greedy subspace clustering by park. the comment states that these methods are computationally efficient and come with similar guarantees. however, the support for this claim is somewhat limited as it does not provide specific examples, detailed explanations, or references to justify why these methods should be compared or how they are similar. this requires significant effort from the authors to follow the justification, making it somewhat verifiable.",2,"the claim here is that the authors focus mainly on ssc and do not contrast their method with other methods. there is some limited verification, as the reviewer lists alternative methods and implies their similarity in terms of computational efficiency and guarantees. however, the reasoning is vague, lacking detailed explanation or references to fully justify the claim."
"- ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.",NIPS_2018_25,"- My understanding is that R,t and K (the extrinsic and intrinsic parameters of the camera) are provided to the model at test time for the re-projection layer. Correct me in the rebuttal if I am wrong. If that is the case, the model will be very limited and it cannot be applied to general settings. If that is not the case and these parameters are learned, what is the loss function? - Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned. - ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper. - During evaluation at test time, how is the 3D alignment between the prediction and the groundtruth found? - Please comment on why the performance of GTSeeNet is lower than that of SeeNetFuse and ThinkNetFuse. The expectation is that groundtruth 2D segmentation should improve the results. - line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD? - What does NoSeeNet mean? Does it mean D=1 in line 96? - I cannot parse lines 113-114. Please clarify.",4,3,"the review point makes a claim that 'semantic' segmentation is not low-level and suggests that statements about it being a low-level cue should be removed from the paper. the reviewer supports this claim with an explanation that categories are specified for each pixel, which differentiates semantic segmentation from low-level cues. however, the justification could be enhanced with further detail or reasoning. for instance, the reviewer could explain why specifying categories for each pixel is not considered low-level beyond stating the fact. the current explanation provides some support but requires significant effort from authors to understand the standards or why this classification is important in the context of the paper.",4,"the review point contains a claim that semantic segmentation is not low-level, suggesting that statements regarding semantic segmentation being a low-level cue should be removed from the paper. the claim is mostly supported with an explanation that 'the categories are specified for each pixel,' which implies a higher level of detail or abstraction, and thus challenges the claim of being low-level. however, while the explanation provides a reasonable basis for the claim, it could benefit from additional support, such as references to common definitions or examples from the literature to completely verify the claim.",5,"the claim is that 'semantic segmentation is not low-level since the categories are specified for each pixel,' and it is well-verified. the reviewer provides clear reasoning that specifying categories for each pixel makes it not a low-level cue, which justifies the request to remove related statements from the paper."
"• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.",ICLR_2021_1213,"weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under “Additional Comments” as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. Summary:
• The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex.
• The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known “random shuffling” sampling strategy.
• Specifically, AdaGrad-window is shown to achieve O ~ ( T − 1 / 2 )
rate of convergence, whereas AdaGrad-truncation attains ( T − 1 / 2 )
convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.
• The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.
• In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition “consistency ratio” over epochs. Strengths:
• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.
• I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.
• Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.
• Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new “consistency condition” is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors’ approach to proving the results. Weaknesses:
• Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn’t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.
• Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r
. I couldn’t figure out how it is possible to compute the value r
ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r
weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.
• The related work which is listed in Table 1, within the group “Adaptive Gradient Methods” prove \emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.
• As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.
• Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion.
• This is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.
Additional Comments:
• I haven’t seen the definition that x t , m + 1 = x t + 1 , 1
in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?
• Second bullet point of your contributions claim that “[consistency] condition is easy to verify”. I do not agree with this as I cannot see how someone could guarantee/compute the value r
ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?
• In Assumption A3, I understand that G t e i = g t , i and G t e = ∑ i = 1 m g t , i
. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.
• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i
enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?
• In Corollary 1, authors state that “the computational complexity is nearly O ( m 5 / 2 n d 2 ϵ − 2 ) ~
”. A similar statement exists in Corollary 2. Could you please explain what “nearly” means in this context?
• In Lemma 8 in the supplements, a a T and b b T
in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that a a T or b b T
correspond to something like g t , j 2 – g t − 1 , j 2
. I am not sure if this construction fits into Lemma 8 because, for instance, the expression g t , j 2 – g t − 1 , j 2
is difference of two rank-1 matrices, which could have rank \leq 2. Hence, there may not exist some vector a
such that a a T = g t , j 2 – g t − 1 , j 2
, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors.
• In the supplements, in section “A.1.7 PROOF OF MAIN THEOREM 1”, in the expression following the first line, I didn’t understand how you obtained the last upper bound to ∇ f ( x t , i )
. Could you please explain how this is obtained? Score:
I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:
I am not convinced about the importance of consistency ratio and that it is a verifiable condition.
Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.
(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.
Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.
======================================= Post-Discussions =======================================
I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score.
Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice.
Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.",4,3,"the comment suggests performing an 'epoch-wise analysis' to gain insights into optimization algorithms, particularly in finite sum settings. the reviewer provides some context on why this analysis could be beneficial, such as investigating batch sizes and sampling strategies for both deterministic and stochastic methods. while these points offer moderate support for the claim, they lack specific examples, concrete evidence, or references that would make the rationale more compelling and easier for authors to follow.",3,"the review point contains a claim that suggests epoch-wise analysis could provide insights into optimization algorithms, particularly in finite sum settings. the reviewer provides some reasoning by mentioning the investigation of batch size and sampling strategies, as well as comparative analysis between deterministic and stochastic methods. however, the reasoning lacks specific examples, references, or detailed explanations on how the epoch-wise analysis will achieve these insights, making it somewhat verifiable. the authors will need to make significant efforts to understand how exactly the suggested changes will improve their work.",3,"the reviewer suggests conducting an epoch-wise analysis for better insights into optimization algorithms, which is a claim. while the suggestion is supported by a potential benefit (insights into behaviors of optimization algorithms, effect of batch size, and sampling strategies), it lacks detailed justification or references to similar analyses or established practices. this makes it somewhat verifiable, as it requires further details for full verification."
"- L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).",ARR_2022_161_review,"The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments. The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas).
### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.
- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,
- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity. - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.
- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?
--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).
- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.
- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’. - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already. - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.
--- ### Typos/Style: Very well written paper, no remarks here.",5,5,"the review point makes a claim that it is common to average over subword representations in contrast to the method used in the paper. this claim is supported by a specific reference to a work by hewitt and manning (2019), which lends credibility and context to the suggestion. this explicit reference provides robust evidence and clear reasoning to support the claim, making it fully verifiable.",4,"the review point contains a claim that averaging over subword representations is a common practice, and this is supported by providing a reference to hewitt and manning's work. while the reference lends credibility to the claim, the explanation could be more detailed about why averaging is preferred in certain cases or how it compares to taking the first subword token. the support is mostly verifiable due to the inclusion of an external reference, but it could benefit from additional reasoning or context.",5,"the claim suggests a common practice for verb embedding by averaging over subword representations, supported by a specific reference to hewitt and manning (2019), making it fully verifiable."
"- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.",ACL_2017_483_review,"- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was performed on.
General Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with ""sequence-to-sequence models"". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not enough well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be appropriate.
- 590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.
- 745: This could alternatively show that structural cues are less important for this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).
[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection."" EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016.",5,3,"the review comment makes claims about specific areas that need clarification or improvement within the paper. it highlights a lack of clarity in the pooling method for embedding features and the explanation of equation (7). however, while these issues are identified, the review lacks detailed reasoning or examples for the authors to understand why these are problematic. the reviewer does suggest that e_i needs more precise definition and that the lhs of equation (7) might be better represented as a conditional probability. yet, the support for these claims is somewhat vague, lacking explicit, detailed explanations or references to external work. therefore, while the claims are supported to a certain degree, more detailed justification is required for a higher verifiability score.",no claim,"the review point consists of specific questions and factual observations regarding ambiguities in the presentation of the model and specific equations in the paper. it does not contain subjective opinions, suggestions, or claims that require justification or evidence to support them. the reviewer is asking for clarifications on the pooling method and the meaning of variables in an equation, as well as noting a potentially incorrect expression. therefore, these aspects fall under the category of general questions and objective statements, which do not necessitate verification.",4,"the comment contains claims that 'two things must be improved in the presentation of the model' regarding the unclear elements: the pooling method and equation (7). the reviewer provides specific details about these issues, such as the question about the random variable e_i and the conditional probability aspect of the equation, which offers mostly verifiable support. the clarity of these points helps in understanding the basis for the claims."
"1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses).",ICLR_2022_562,"1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses). 2. One other thing (although this is not the main focus of this paper), the authors should provide comparisons between strategies that result in fast convergence (without sparsity) vs. sparse methods? For example, do non-sparse, fast convergence methods (like FedProx, FedMAX, and others) result in small enough number of epochs compared to sparse methods? Can the fast convergence methods be augmented with sparisity ideas successfully without resulting in significant loss of accuracy? Some discussion and possibly performance numbers are needed here.",5,5,"the review point contains a claim that the experiments section is a primary weakness because it only presents results on the cifar-10 dataset and does not include other datasets from federated learning benchmarks. this claim is thoroughly supported by clear reasoning, as the reviewer suggests the inclusion of additional datasets, specifically referencing leaf, and provides external references to relevant works, namely fedprox and fedmax, to improve the comprehensiveness of the experimental evaluation. the mention of these references and datasets provides strong and specific evidence to support the claim, making it fully verifiable.",5,"the review point identifies a specific weakness in the paper—limited experimentation solely on the cifar-10 dataset. the reviewer provides thorough evidence by suggesting additional datasets from the federated learning benchmarks like leaf, and points to external references such as fedprox and fedmax papers that detail different datasets and model types. the claim is robustly supported with relevant references, making it fully verifiable. the reviewer also highlights the potential impact of addressing this issue, thus guiding the authors to improve their paper.",5,"the claims are that the experiments section is the main weakness, as it only presents results on the cifar-10 dataset and does not consider many other datasets from federated learning benchmarks. these claims are thoroughly supported with specific examples, references to external works, and suggestions on how to improve by looking at relevant works like fedprox and fedmax. this provides clear and precise reasoning that helps the authors understand the claim and how to address it."
"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].",8fLgt7PQza,"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].
2. In the experiment or appendix section, I did not clearly see the formulas for Sensitivity and Specificity, nor were there any corresponding references, which is quite confusing to me. Moreover, using Accuracy as a metric in cases of highly imbalanced labels is unreasonable. For instance, in the MIMIC-III Mortality Prediction task, the positive rate is 5.42%. If I predict that all patients will survive, I can still achieve an accuracy of 94.58%. Previous works, such as GraphCare [1], have adopted AUROC and AUPRC as evaluation metrics.
3. The article is overly long and filled with detailed content, making it easy for readers to miss important points.
- [1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024
- [2] MedRetriever: Target-driven interpretable health risk prediction via retrieving unstructured medical text. CIKM 2021
- [3] Biomedical knowledge graph-enhanced prompt generation for large language models. Arxiv 2023",5,3,"the review point contains several claims: the workload is immense, the article's contribution is incremental and akin to a combination of graphrag and graphcare; key baselines were not cited; essential algorithms should have been introduced. the justification is somewhat provided: the combination of existing methods implies incremental contribution; however, the reasoning lacks detailed examples or further explanation. the claim about missing citations suggests that some algorithms should have been discussed, but it primarily relies on the reader's knowledge of the significance of these algorithms, which requires further elaboration. some support is provided but there are key missing elements, which makes it require significant effort from the authors to follow the justification.",5,"the review point provides a clear and precise reasoning for its claims, explicitly comparing the article's contribution with existing work, and citing specific references. the reviewer highlights that the contribution appears incremental by stating it is essentially a combination of graphrag and graphcare, and provides relevant citations [1]. the reviewer further supports their claim about missing citations by specifically identifying key algorithms that should have been included, such as medretriever and kgrag, and providing references for these [2, 3]. this detailed explanation and referencing thoroughly supports the claims, making them fully verifiable.",5,"the claims are that the contribution is incremental and many key baselines were not cited, justified by the comparison to graphrag and graphcare, and the suggestion of additional references such as medretriever and kgrag. this is fully verifiable due to the clear reasoning and references to specific prior works."
1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.,NIPS_2020_295,"1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones. 2. Some methods use epochs and pretrain epochs as 200, while the reported InvP uses 800 epochs. What are the results of InvP with epochs as 200? It would be more clear after adding these results into the tables. 3. The proposed method adopts memory bank to update vi, as detailed in the beginning of Sec.3. What the results would be when adopting momentum queue and current batch of features? As the results of SimCLR and MoCo are better than InsDis, it would be nice to have those results.",5,4,"the review point contains a claim suggesting that the experimental comparisons are not sufficient, and it suggests testing the proposed invp with wider backbones like resnet50 (2×) and resnet50 (4×), similar to other methods like moco and simclr. this claim is mostly verifiable as the reviewer provides some justification by comparing the experiment's setup with other methods. while the reasoning is logical and based on common knowledge of the field, the reviewer could enhance the comment by providing more detailed reasoning on why using wider backbones would be beneficial for the study. however, the current level of detail is sufficient for the authors to consider the inclusion of such experiments.",4,"the review point contains a claim that the experimental comparisons are not enough and suggests that additional comparisons using wider backbones like resnet50 (2×) and resnet50 (4×) should be included. the reviewer justifies this claim by referencing common methods like moco and simclr, which are known in the field for testing results with these wider backbones. this provides a sufficient and mostly verifiable basis for the claim, grounded in common practice. however, the reviewer could enhance the claim by further explaining why using wider backbones would provide more valuable insights or specific improvements. therefore, it is rated as 'mostly verifiable'.",3,"the claim here is that 'the experimental comparisons are not enough,' suggesting that testing with wider backbones like resnet50 (2×) and resnet50 (4×) could add value. the comment provides partial support by referencing methods like moco and simclr, but lacks a deeper explanation or evidence on why this would be beneficial specifically for the paper's proposed method, invp. the justification is not fully articulated, making it somewhat verifiable."
"2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury.",NIPS_2020_204,"1.The authors have done a good job with placing their work appropriately. One point of weakness is insufficient comparison to approaches that aim to reduce spatial redudancy, or make the networks more efficient specifically the ones skipping layers/channels. Comparison to OctConv and SkipNet even for a single datapoint with say the same backbone architecture will be valuable to the readers. 2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury. 3. The authors should add results from [57] and discuss the comparison. Recent alternatives to MSDNets should be compared and discussed. 4. Efficient backbone architectures and approaches tailoring the computation by controlling convolutional operator have the added advantage that they can be generally applied to semantic (object recognition) and dense pixel-wise tasks. Extension of this approach, unlike other approaches exploiting spatial redundancy to alternate vision tasks is not straightforward. The authors should discuss the implications of this approach to other vision tasks.",5,2,"the review point contains a claim that the authors need to show a specific graph to understand the source of performance improvement, suggesting that the current analysis might be lacking. the justification provided is somewhat vague, mentioning the need to determine if improvements come solely from network design or from specific characteristics of the imagenet dataset. while the reviewer states this is important to distinguish and discusses the potential for unfair advantage, the reasoning behind why this graph specifically would elucidate these issues is not clearly articulated. the authors may struggle to fully understand the justification for the suggested changes without further explanation. thus, it is borderline verifiable with some support but lacking in detailed rationale.",3,"the review point contains a claim suggesting that the authors need to provide a graph to clarify whether the performance improvement is due to the network design or the nature of imagenet. the reviewer raises a valid point regarding the potential for an unfair advantage from exploiting spatial redundancies. however, the justification for this claim is somewhat limited, as it lacks detailed reasoning or references to support the idea that a large fraction of images can be done with lower resolution or that this would confer an advantage. the reviewer provides some reasoning, but there is room for a more detailed explanation to fully support the claim, so it requires some effort from the authors to interpret and act on the suggestions.",3,"the claim in this review point suggests the need to understand the performance improvement source. while it points out a potential issue—whether performance gains are due to network design or dataset characteristics—it provides some reasoning for why this analysis is important. however, it lacks specific examples or detailed justification, requiring significant effort from authors to follow the justification."
"1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]?",ICLR_2022_1420,"Weakness:
Lack of novelty. The key idea, i.e., combining foreground masks to remove the artifacts from the background, is not new. Separate handling of foreground from background is a common practice for dynamic scene novel view synthesis, and many recent methods do not even require the foreground masks for modeling dynamic scenes (they jointly model the foreground region prediction module, e.g., Tretschk et al. 2021).
Lack of controllability. 1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]? 2) Even for controlling facial expression, it is highly limited to the mouth region only. From overall qualitative results and demo video, it is not clear the method indeed can handle overall facial expression including eyes, nose, and wrinkle details, and the diversity in mouth shape that the model can deliver is significantly limited.
Low quality. The results made by the proposed method are of quite low quality. 1) Low resolution: While many previous works introduce high-quality view synthesis results with high-resolution (512x512 or more), this paper shows low resolution results (256x256) for some reasons. Simply saying the problem of resources is not a convincing argument since many existing works already proved the feasibility of high resolution image synthesis using implicit function. Due to this low-resolution nature, many high-frequency details (e.g., facial wrinkles), which are the key to enabling photorealistic face image synthesis, are washed out. 2) In many cases, the conditioning facial expressions do not match that from the synthesized image. From the demo video, while mouth opening or closing are somehow synchronized with conditioning videos, there exists a mismatch in the style of the detailed mouth shape.",5,4,"the review point contains a claim that the proposed method cannot handle headpose. the reviewer supports this claim by referencing a previous work (gafni et al. iccv 2021) that can control both facial expression and headpose. although the mention of a specific prior work adds verifiability, the reviewer does not explain why the current method cannot incorporate headpose control like the cited work. therefore, while the comment is mostly verifiable due to the reasonable comparison with existing literature, minor gaps in detail and reasoning prevent it from being fully verifiable.",5,"the review point is fully verifiable as the claim made involves a specific comparison with previous work (gafni et al., iccv 2021). the reference to this specific work provides a clear basis for the claim, as it mentions that the referenced work already addresses the issue of controlling both facial expression and headpose, which the current paper does not handle. this comparison and the provided reference allow the authors to clearly understand the claim and address it by looking into the referenced work. moreover, the reviewer questions the possibility of extending the current method to include headpose parameters, directly relating to the referenced work and its capabilities, thus providing a logical and common-sense avenue for verification.",5,"the claim is 'cannot handle the headpose,' suggesting a limitation in the proposed method. it is fully verifiable as it provides a reference to a previous work (gafni et al. iccv 2021) that has already addressed this issue, making the claim thoroughly supported by evidence."
"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.",ARR_2022_311_review,"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector.
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data?",5,2,"the review point contains several claims about the weaknesses of the paper's experiments. it mentions that the experiments are weak, constrained to extremely low-resource regimes, and only applied to sentence classification. while the reviewer provides some context, such as the limitations of a short paper and an opinion that the augmentation method could be used on more nlp tasks, these points are not sufficiently supported with explicit examples, detailed explanations, or references. the justification is vague, particularly regarding what other experimental settings and nlp tasks could have been explored, making it difficult for the authors to follow the reviewer's reasoning and suggestions fully.",3,"the review comment includes several claims that are somewhat supported. the reviewer mentions weaknesses in the paper's experiments, specifically stating that the setting is on an extremely low-resource regime and that sentence classification is an easier task. they suggest that the augmentation method has potential for more nlp tasks, which wasn't shown. the reviewer provides reasoning to some extent, by pointing out the limited experimental setting and the simplicity of the task, but lacks detailed examples, references, or specific justifications to fully support their claims. this makes it somewhat verifiable as authors will have to infer further details on their own to completely follow the justification.",3,"the review point contains several claims: 'main weaknesses of the paper are the experiments,' 'setting is only on extremely low-resource regime,' and 'sentence classification is an easier task.' the reviewer suggests improvements and expresses expectations for stronger experiments. however, the justification provided lacks depth and specific examples, making it somewhat verifiable but leaving room for more detailed support."
"3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: “The may be attributed…” -> “This may be attributed…",ARR_2022_68_review,"1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data?
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail.
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback.
Moreover, Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers. 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8.
Line 277: “The may be attributed…” -> “This may be attributed…",5,4,"the review point contains a claim that the adopted baseline models are weak. the reviewer supports this claim by stating that the author did not compare their work with campos et al. (2020), which also uses feedback in qa tasks, and did not make comparisons with other domain adaptation methods referenced in section 8. while the comments mostly provide logical reasoning to support the claim, they could be more explicit in explaining why such comparisons are necessary or how they would strengthen the authors' work. therefore, the score is rated as mostly verifiable.",5,"the review point contains explicit claims about the weaknesses in the choice of baseline models, supported by detailed explanations. the reviewer points out that the authors do not compare their results to campos et al. (2020), despite it being relevant as it uses feedback in qa tasks, and notes the lack of comparison with other domain adaptation methods referenced in section 8. these statements are backed by logical reasoning and references, providing a thorough and robust justification for the claims. thus, the comment is fully verifiable.",3,"the claim that 'the adopted baseline models are weak' is somewhat verifiable. the reviewer provides some context by mentioning the lack of comparison with specific works, which gives partial support to the claim. however, the rationale could have been more comprehensive with further explanation or justification."
"2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.",ICLR_2022_1794,"1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.
2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.
3 On the segmentation mask involved with cancer on CSAW-S, the segmentation results of DEEPLAB3-DEIT-S cannot be concluded as better than DEEPLAB3-RESNET50. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance.
Questions: 1 For the grid search of learning rate, is it done on the validation set?
Minor problems: 1 The n number for Camelyon dataset in Table 1 is not consistent with the descriptions in the text in Page 4.",5,5,"the review point presents a claim that additional analysis and comments on performance trending are required, along with disagreement with the authors' perspective on the benefits of increased model capacity for cnns and vits. the reviewer fully verifies their disagreement by referencing figure 3 and providing specific examples: deit-b models' performance compared to deit-t on aptos2019, and deit-s on aptos2019, isic2019, and chexpert. they also point out that cnns provide more consistent performance improvements with increasing capacity, except on isic2019. this thorough referencing of data and logical reasoning makes the justification highly robust and verifiable.",5,"the review point contains a clear claim that the author's viewpoint about the benefits of increased model capacity for vits and cnns is incorrect. it thoroughly supports this claim by referring to specific evidence from figure 3 and providing a detailed analysis of the performance differences across various datasets (aptos2019, isic2019, and chexpert). it highlights specific cases where deit models do not show significant improvement, offering quantitative evidence (e.g., 0.1% difference) and noting consistent improvements in cnns. this precise reasoning and specific reference to evidential data make the claim fully verifiable.",5,"the claim is the disagreement with the authors' viewpoint that both cnns and vits benefit similarly from increased model capacity. this claim is thoroughly supported by specific observations from figure 3 regarding performance comparisons between deit-b, deit-t, and deit-s models on various datasets, showcasing how vits do not consistently outperform as capacity increases, unlike cnns. the reviewer's argument is backed by clear evidence and analysis, making it fully verifiable."
"- In figure 5, the y-axis label may use ""Exact Match ratio"" directly.",ARR_2022_113_review,"The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases.
- Figure 2, it is not clear about ""merge target"". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, ""including that it optimizes for the wrong objective"". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work. - In figure 5, the y-axis label may use ""Exact Match ratio"" directly.
- Line 409, could you cite the ""R2"" metric?
- Appendix A, the authors state ""better model score cannot result in better hypothesis"". You'd better state clearly what idea hypothesis you want. "" a near-optimal model score"" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph?",5,2,"the review point suggests a change to the y-axis label in figure 5 to use 'exact match ratio' directly, which indicates an implicit claim about the appropriateness or clarity of the current label. however, the suggestion does not provide any reasoning, explanation, or justification for why this change would be beneficial or necessary. the authors might struggle to understand why this suggestion is made without further context. hence, it is rated as borderline verifiable.",no claim,"the review point provides a suggestion to change the label on the y-axis of a figure to 'exact match ratio'. this is a factual suggestion about a possible improvement in labeling and does not contain any subjective opinion, claim, or need for justification. it is purely descriptive, focusing on clarity in presentation, and thus falls under 'no claim'.",no claim,"the review point suggests a change in labeling for clarity, but it does not contain a subjective opinion or claim that requires verification. it simply indicates a possible revision for clarity."
"4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately.",NIPS_2018_707,"weakness of the paper is the lack of experimental comparison with the state of the art. The paper spends whole page explaining reasons why the presented approach might perform better under some circumstances, but there is no hard evidence at all. What is the reason not to perform an empirical comparison to the joint belief state approach and show the real impact of the claimed advantages and disadvantages? Since this is the main point of the paper, it should be clear when the new modification is useful. 3) Furthermore, there is an incorrect statement about the performance of the state of the art method. The paper claims that ""The evidence suggests that in the domain we tested on, using multi-valued states leads to better performance."" because the alternative approach ""was never shown to defeat prior top AIs"". This is simply incorrect. Lack of an experiment is not evidence for superiority of the method that performed the experiment without any comparison. 4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately. 5) As explained in 1), the presented method is quite heuristic. The algorithm does not actually play the blueprint strategy, only few values are used in the leaf states, which cannot cover the whole variety of the best response values. In order to assess whether the presented approach might be applicable also for other games, it would be very useful to evaluate it on some substantially different domains, besides poker. Clarity: The paper is well written and organized, and it is reasonably easy to understand. The impact of the key differences between the theoretic inspiration and the practical implementation should be explained more clearly. Originality: The presented method is a novel modification of continual resolving. The paper clearly explains the main distinction form the existing method. Significance: The presented method seems to substantially reduce the computational requirements of creating a strong poker bot. If this proofs to be the case also for some other imperfect information games, it would be a very significant advancement in creating algorithms for playing these games. Detailed comments: 190: I guess the index should be 1 339: I would not say MCCFR is currently the preferred solution method, since CFR+ does not work well with sampling 349: There is no evidence the presented method would work better in stratego. It would depend on the specific representation and how well would the NN generalize over the types of heuristics. Reaction to rebuttal: 1) The formulation of the formal statement should be clearer. Still, while you are using the BR values from the blueprint strategy in the computation, I do not see how the theory can give you any real bounds the way you use the algorithm. One way to get more realistic bounds would be to analyze the function approximation version and use error estimates from cross-valiadation. 2) I do not believe head-to-head evaluation makes too much sense because of well known intransitivity effects. However, since the key difference between your algorithm and DeepStack is the form of the used leaf evaluation function, it would certainly not take man-years to replace the evaluation function with the joint belief in your framework. It would be very interesting to see comparison of exploitability and other trade-offs on smaller games, where we can still compute it. 4) I meant the use of the example for save resolving. 5) There is no need for strong agents for some particular games to make rigorous evaluation of equilibrium solving algorithms. You can compute exploitability in sufficiently large games to evaluate how close your approach is to the equilibrium. Furthermore, there are many domain independent algorithms for approaximating equilibriua in these games you can compare to. Especially the small number of best response values necessary for the presented approach is something that would be very interesting to evaluate in other games. Line 339: I just meant that I consider CFR+ to be ""the preferred domain-independent method of solving imperfect-information games"", but it is not really important, it was a detailed comment.",5,2,"the review point contains a claim that the rock-paper-scissors example is inspired by previous work and asks the authors to cite the source. however, it lacks specific details or references to support this claim, such as mentioning which prior work or examples it refers to. this makes the justification somewhat vague and insufficient for the authors to easily verify or address the reviewer's request.",2,"the review point contains a claim that the rock-paper-scissors example is inspired by a prior example in many previous works and requests that a source be cited. however, it fails to provide any specific references, examples, or detailed explanations to verify the claim, making it difficult for the authors to know which sources need to be cited. the comment provides some support for its claim by indicating that prior work exists, but the lack of specificity results in insufficient justification, leading to a borderline verifiable score.",4,"the claim is that the rock-paper-scissors example is clearly inspired by previous work, and the reviewer suggests citing the source appropriately. this claim is mostly verifiable as it relies on the common knowledge that this example has been used before in literature. the verification could be stronger if specific references to prior work are provided."
"2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.",ARR_2022_123_review,"1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable.
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied.
1) It's better to adopt experiment settings consistent with previous work.
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved.
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021.",5,5,"the review point contains a claim suggesting that the baselines used in the tables are weak. this claim is thoroughly supported by explicit evidence: specific numerical comparisons are provided between the study's results and those obtained by voita et al. (2019) using the cadec model. the comparisons are detailed, providing clear benchmarks on deixis, lexical cohesion, ellipsis (infl.), and ellipsis (vp), effectively illustrating the performance gap. this robust evidence and quantitative assessment thoroughly justify the claim, making it fully verifiable.",5,"the review point provides a claim that the baselines used for comparison are weak, and this is thoroughly backed with explicit evidence. the reviewer lists specific numerical results achieved by the cadec model (voita et al., 2019), allowing a direct quantitative comparison with the current work's performance. this detailed comparison offers clear reasoning and leaves little room for ambiguity or interpretation. the reference to specific outcomes from prior work additionally supports the claim, making it robustly verifiable.",5,"the claim is that the baselines used for comparison are weak. this is fully verifiable because the reviewer provides specific examples and data points to demonstrate the inadequacy of the baselines, referencing voita et al. (2019) for stronger results. the reviewer makes a clear comparison between the approaches, effectively supporting their claim with data from the literature."
"1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.",ARR_2022_311_review,"__1. Lack of significance test:__ I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results.
__2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)?
__4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result?
__4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample?
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation",5,1,"the comment contains a claim that inconsistent spacing affects the 'beauty' of the tables, but it lacks any supporting evidence or justification for why this is an important issue or how it impacts the paper beyond aesthetics. there is no logical reasoning or reference provided to support the claim, making it unverifiable.",no claim,"the review point highlights an inconsistency in the formatting of the tables concerning spaces between accuracy and standard deviation. this is an observational statement about the presentation and does not contain a subjective opinion or suggestion that requires justification or verification. the comment points out an objective formatting issue, which doesn't involve a claim that needs supporting evidence or reasoning.",1,"the claim here is related to the inconsistency in spacing affecting 'beauty', which is a subjective opinion. however, it lacks any support or justification on why this is an important issue or how it impacts the manuscript beyond aesthetics. hence, it is unverifiable."
"- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).",YRJDZYGmAZ,"Many significant problems are found in the current format of the paper that prevents the understanding of the concept. The problem includes but is not limited to confusing writing, inconsistency of notations, expressions of novelty, experimental presentation etc. It is highly recommended that the authors to re-write the paper, re-organize the content and better polish the text for the reader to better understand.
The major problems:
* Novelty is limited:
- The proposed method as in Sec. 3.2 is very similar to DAPL but extended to multi-source scenarios. The only difference is to introduce an additional [DOM]. Note, that DAPL has not been peer-reviewed.
- The motivation for domain-aware mixup is confusing. I cannot be convinced and do not understand in the current writing, how it can enforce to learn domain-specific knowledge. The corresponding literature regarding mixup in the feature space is also not referenced and discussed (e.g. [1]).
- The description for deriving the domain-aware mixup is confusing. I assume the authors are trying to develop a method so that the learned prompt shares the knowledge between source and target domains (depending on Eq. 9)?
* Writing:
- In the first sentence of Abstract: “large vision-language models … strong performance in MFDA”. There is no such reference applying large VL models in MFDA. In fact, MFDA is a rarely studied problem.
- The description of the problem setting (MFDA) should be clearly explained at the beginning (abstract or introduction) so that the reader can refer better to the limitations of prior works.
- Paragraphs 1 & 2 in the introduction: the connection is missing, and ‘prompt learning’ suddenly jumps in, making the concept broken.
- Fig. 1 is not referred to in the paper.
- Related work: after describing the related prior works of each field, it's suggested to write a couple of sentences to distinguish between them to show the novelty of the proposed method.
- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).
- There is significant text overlapping with DAPL in the preliminary sections of both papers (only with some rewording..). It should be strictly prohibited.
- What is [DOM] in Eq. 4? I assume it is a domain ID? And I assume [DOM] is the non-learnable component near the description of Eq. 4?
- Notation: what is subscript d in Eq. 4 and superscript d in Eq. 5? They are not explained in the text. I assume they are the domain IDs?
- What does it mean by ‘d*k categories’ as in the sentence after Eq. 5?
- Eq. 6 is very confusing. For the outer summation on d \in {s, u}, what is the purpose of computing the similarity between the target domain prompt and source image features? How does the learning on unlabeled target data is realized?
- What is inter-source domain mixup? In the current format of writing, I don’t understand why maintaining it will harm the representation learning on the target domain. The motivation is weak.
- In the second paragraph on page 6, the notation of target domain data y_t is different from Section 3.
- In Fig. 3, letters v and f are used to represent the features of “painting” and “real”. But v is used to represent text prompts as in Eq. 3
- The feature mix-up formulation in Fig. 3 is different than Eq. 8. One uses \gamma and another one uses \lambda? and the weighting is different?
- It is really confusing that the letter “t” is used to refer to text and target domain.
- What are D^s and D^u in Eq. 10? They are never defined. I assume they are source and target domains, which is inconsistent with what is described in the problem setting. The problem setting is borrowed from (Yue et al., 2021a). But Eq. 10 is copied from DAPL paper. Please keep everything consistent throughout the paper. Also, Eq. 9 requires source data as well, why only D^u is passed to L_u as in Eq. 10?
- The notations for loss functions in Eq. 7, 9, and 10 should be consistent.
- Table 5 in the last sentence of Page 8 should be Figure 5.
- The experimental setting/comparison is very confusing. What is “single best”, which can be both setting and method as in Table 1&2? What is source combined? Which rows in Tables 1&2 refer to the MFDA? How come the “Large model” in Table 1&2 can be the setting, it should be the model architecture.
- For Figure 6&7, they are hard to see the differences. It is suggested to use a table to report the numbers.
[1] Adversarial Domain Adaptation with Domain Mixup. AAAI 2020.",5,5,"the review comment contains a claim that the description of the mfda setting is very confusing, particularly regarding the labeling and notation used in the method section. the reviewer thoroughly supports this claim by referencing the original mfda paper (yue et al., 2021a), pointing out discrepancies between the descriptions in the current paper and the original work. furthermore, the reviewer asks specific questions that highlight inconsistencies and points to potential issues with labeling in source domains during training. the references and logical reasoning provided make the claim fully verifiable.",4,"the review point contains a claim that the description of the mfda setting is confusing due to differences in notation and problem setting compared to the original mfda paper by yue et al., 2021a. the reviewer supports this claim by referencing the apparent inconsistency with the original work and providing specific examples from the paper's text (e.g., target domain being unlabeled in the original paper vs. the description in the current paper). however, the explanation could be more detailed by clarifying why these differences are problematic or providing additional context from the original mfda paper to help the authors understand the importance of addressing this inconsistency. thus, it is mostly verifiable with minor gaps.",4,"the claim is that 'the description of the mfda setting is very confusing.' the reviewer provides detailed reasoning by comparing it with the original mfda paper and noting discrepancies in the description. this reasoning supports the claim, making it mostly verifiable. however, it could benefit from a bit more specificity in exactly how the description differs or where these inconsistencies lie."
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.,ARR_2022_121_review,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)).
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays. Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference.",5,5,"the review point contains a claim suggesting that using feature engineering, specifically the feature set from uto et al. (2020), might improve the performance of the current work. this claim is thoroughly supported by a reference to uto et al. (2020), who achieved a high qwk score of 0.801 using hand-crafted features. the reference to an external work provides strong and relevant evidence supporting the suggestion. the reasoning is clear, precise, and connects the potential benefit of feature engineering to a documented successful implementation in a similar context, making the claim fully verifiable.",4,"the review point suggests that the use of feature engineering could potentially improve performance and references uto et al. (2020) as evidence. the mention of uto et al.'s system achieving a qwk of 0.801 by using hand-crafted features provides a logical basis for the suggestion. however, the reasoning could be made more robust with additional details or insights into why the same feature set might specifically benefit the current work, making it mostly verifiable rather than fully verifiable.",3,"the claim is that using uto et al. (2020)'s feature set could improve the results, and it mentions a specific result (qwk of 0.801) achieved by a similar system. however, the suggestion to use this feature set is not fully verifiable as it lacks detailed reasoning or direct evidence for the improvement, making it somewhat verifiable."
"- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.",ACL_2017_516_review,"Missing related work on anchor words Evaluation on 20 Newsgroups is not ideal Theoretical contribution itself is small - General Discussion: The authors propose a new method of interactive user specification of topics called Tandem Anchors. The approach leverages the anchor words algorithm, a matrix-factorization approach to learning topic models, by replacing the individual anchors inferred from the Gram-Schmidt algorithm with constructed anchor pseudowords created by combining the sparse vector representations of multiple words that for a topic facet. The authors determine that the use of a harmonic mean function to construct pseudowords is optimal by demonstrating that classification accuracy of document-topic distribution vectors using these anchors produces the most improvement over Gram-Schmidt. They also demonstrate that their work is faster than existing interactive methods, allowing interactive iteration, and show in a user study that the multiword anchors are easier and more effective for users.
Generally, I like this contribution a lot: it is a straightforward modification of an existing algorithm that actually produces a sizable benefit in an interactive setting. I appreciated the authors’ efforts to evaluate their method on a variety of scales. While I think the technical contribution in itself is relatively small (a strategy to assemble pseudowords based on topic facets) the thoroughness of the evaluation merited having it be a full paper instead of a short paper. It would have been nice to see more ideas as to how to build these facets in the absence of convenient sources like category titles in 20 Newsgroups or when initializing a topic model for interactive learning.
One frustration I had with this paper is that I find evaluation on 20 Newsgroups to not be great for topic modeling: the documents are widely different lengths, preprocessing matters a lot, users have trouble making sense of many of the messages, and naive bag-of-words models beat topic models by a substantial margin. Classification tasks are useful shorthand for how well a topic model corresponds to meaningful distinctions in the text by topic; a task like classifying news articles by section or reviews by the class of the subject of the review might be more appropriate. It would also have been nice to see a use case that better appealed to a common expressed application of topic models, which is the exploration of a corpus.
There were a number of comparisons I think were missing, as the paper contains little reference to work since the original proposal of the anchor word model.
In addition to comparing against standard Gram-Schmidt, it would have been good to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference”. I also would have liked to have seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models”, both of which provide useful insights into the anchor selection process.
I had some smaller notes: - 164: …entire dataset - 164-166: I’m not quite sure what you mean here. I think you are claiming that it takes too long to do one pass? My assumption would have been you would use only a subset of the data to retrain the model instead of a full sweep, so it would be good to clarify what you mean.
- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.
- 337: Usenet should be capitalized - 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also, did you remove headers, footers, and/or quotes from the messages?
- 436-440: I would have liked to see a bit more explanation of what this tells us about confusion.
- 692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora.
POST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different.",X,2,"the comment questions the choice of operators used by the authors and implies that the inclusion of additional operators (like 'and' or 'element-wise max') might be beneficial. however, the reviewer provides limited reasoning or evidence to support why the suggested operators should be considered as better choices. although there is an attempt to connect these operators to concepts like union and intersection, the justification is not fully articulated, leaving the authors with an unclear basis to understand the necessity or consequences of considering these operators. thus, the comment is borderline verifiable as it contains vague support for its claim.",1,"the review point contains a claim questioning the decision not to consider the 'and' operator or element-wise max, suggesting that they might correspond to union and intersection ideas. however, it doesn't provide any supporting evidence or justification for why these alternatives should be considered or why they are relevant. the comment merely implies that the reviewer didn't understand the authors' choices, but without any logical reasoning or references, it remains an unverifiable claim.",1,"the comment implies a suggestion for consideration by asking about the absence of certain operators ('the and operator or element-wise max'), but it does not provide any evidence, justification, or reasoning as to why these should have been considered. the comment simply presents a speculative question without supporting the claim with logical reasoning or references, making it unverifiable."
"- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?",ARR_2022_23_review,"The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?",X,x,"the review point does not contain any claims, opinions, or suggestions; it is purely a request for clarification. it asks the authors to elaborate on why they choose to select only 10 answers from all correct answers and whether this affects the underestimation of performance. since it's a question aiming for further explanation, it does not require any justification from the reviewer.",no claim,the review point consists of a question directed at the authors seeking clarification about their methodology. it inquires about the reasoning behind selecting 10 answers from all correct answers and whether this choice affects the performance underestimation. this is a factual question rather than a statement containing an opinion or suggestion and thus does not require verification.,no claim,"this review point merely asks a question about the selection of answers and its potential impact, without making any subjective claim or opinion. it does not require verification."
"- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?",ARR_2022_112_review,"- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix.
L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about ""Type of Skills"" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of ""skill"", ""knowledge"" and ""attitude"" correspond to ""hard"" and ""soft"" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider ""skill"" and ""knowledge"" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: ""Pre-training"" and ""pretraining"" are not spelled consistently.",X,2,"the review point contains claims suggesting that the authors should describe the traits of the experts and justify the need for experts in annotation. however, the justification provided is insufficient. the comment poses questions about the nature of the experts (linguistic or domain) and whether annotation posed any challenges, indicating areas that may need clarification or expansion. while these questions imply a requirement for further explanation, they do not provide specific reasoning or evidence to clearly support the claim that such descriptions are essential. therefore, the support is vague, and the authors might struggle to understand the full reasoning behind the requests without additional context or examples.",3,"the review comment contains claims suggesting that the authors should provide more information about the traits of the experts and justify why experts are necessary for annotation. it raises specific questions about the type of experts and the differences in annotation outcomes compared to non-experts. while these questions imply a need for justification, the reviewer does not provide sufficient reasoning or evidence to support these claims. the authors are left to infer the significance of each point on their own, which requires significant effort.",1,"the claim that authors should describe more about the traits of the experts and justify why annotation must be carried out by experts is made without any supporting evidence or reasoning provided. it does not explain why such details are necessary or how they would improve the paper, leaving the claim unverifiable."
- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,NIPS_2018_232,"- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",X,2,"the review point makes a claim that the abstract lacks a description of how the idea was evaluated and what the outcome was. however, the review does not provide sufficient evidence or justification for this claim. it does not explain why these elements are essential for the abstract or how their absence impacts the paper. the comment provides minimal support with the suggestion that these aspects should be included, thus offering some direction to the authors but lacking the depth and clarity needed for them to fully understand the justification behind the claim.",1,"the review point contains a claim about the abstract lacking a description of how the idea was evaluated and what the outcome was. however, it does not provide any supporting evidence or justification for this claim. the reviewer does not elaborate on what specific aspects are lacking or why these aspects are important for the abstract. additionally, the comment on minor language issues is vague and lacks specific examples, making it unverifiable.",2,"the claim is that the abstract 'lacks description of how the idea was evaluated and what was the outcome'. it provides some support, mentioning that there are 'minor language issues,' but this justification is vague and not fully articulated, making it borderline verifiable."
"4. Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?",NIPS_2017_631,"1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model â favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.
2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?
3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.
4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?
5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).
6.	The first two bullets about contributions (at the end of the intro) can be combined together.
7.	Other errors/typos:
a.	L14 and 15: repetition of word âimagineâ
b.	L42: missing reference
c.	L56: impact -> impacts
Post-rebuttal comments:
The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper.
However I still have few comments --
1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).
3. The citation for the MRN model (in the rebuttal) is incorrect. It should be -- @inproceedings{kim2016multimodal,
title={Multimodal residual learning for visual qa},
author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle={Advances in Neural Information Processing Systems}, pages={361--369}, year={2016} }
4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.",X,x,"the review point does not contain a claim, opinion, or suggestion; it consists of factual observations and a request for clarification. the reviewer points out a specific observation about the deterioration in performance when conditional batch norm is applied to an additional layer and requests the authors to explain why this might be happening. since there is no evaluative or subjective assertion, the statement does not require any justification or evidence from the reviewer, thus falling under the 'no claim' category.",no claim,"the review point is asking for clarification on an observation without making any explicit claim or opinion that requires verification. it is a request for elaboration on why applying conditional batch norm to additional layers results in performance deterioration, but it does not imply any assertion about why this is the case. therefore, it is categorized as a normal statement that does not need justification.",no claim,"the comment merely states an observation from table 2 regarding performance changes when applying conditional batch norm to different layers. it does not contain a subjective opinion or suggestion, nor does it require verification. the question asked is seeking clarification rather than making a claim."
"- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).",NIPS_2017_110,"of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: ""Then we made it explicit"" instead of ""Then we have explicit it""",X,3,"the review point contains a suggestion for clarification regarding the specific aspects of the model. it provides some justification by pointing out that it should be clear that infinite subdivisions are not being used and certain parameters are bounded. however, it lacks detailed explanations or references that would make it easier for the authors to understand why these specific aspects are important to highlight. while the comment includes an example, the reasoning behind why this clarification is necessary is not fully articulated, leaving room for further detail or explanation.",4,"the review point contains a suggestion for including specific aspects of the model in the draft. it also provides a reasoning that these aspects, such as not having infinite subdivisions for certain parameters and having bounded parameters, should be made clear in the draft. the justification provided is mostly supported by logical reasoning about why these details are critical to understanding the model's application. however, the reviewer could enhance the comment by giving more detailed explanations or referencing specific sections where this clarity is lacking.",1,"the comment makes a suggestion to 'consider introducing aspects' which implies a claim that certain information should be made explicit. however, this claim lacks any supporting evidence or justification for why these aspects need clarification, making it unverifiable."
"3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.",NIPS_2017_356,"]
My major concerns about this paper is the experiment on visual dialog dataset. The authors only show the proposed model's performance on discriminative setting without any ablation studies. There is not enough experiment result to show how the proposed model works on the real dataset. If possible, please answer my following questions in the rebuttal.
1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?
2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting?
3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.",X,3,"the comment makes a claim by suggesting that the author conduct an ablation study on the visdial dataset to further validate their model's performance. the reviewer expresses specific interest in the performance of att(+h) without relevant attention retrieval. although the comment highlights areas for further investigation, it lacks detailed justification or reasoning regarding why these specific experiments would be beneficial or necessary for enhancing the paper. while the motivation for exploring these experimental parameters is implied, the justification is not fully articulated, which requires significant effort from the authors to understand why these experiments should be conducted.",3,"the review point contains a claim suggesting that an ablation study should be conducted on the visdial dataset to further verify the proposed model. the reviewer expresses interest in the performance of att(+h) without considering relevant attention retrieval from the attention memory. the underlying suggestion is inferred to improve the model's evaluation. the justification is somewhat provided as the reviewer wants to see how the model performs without a specific component, which can be somewhat logical but lacks explicit reasoning or references to support why this ablation study is necessary, other than the interest of the reviewer. this makes the comment somewhat verifiable but missing key elements like well-articulated reasons or supporting evidence.",1,"the review point contains a claim suggesting an ablation study on the visdial dataset to back up the proposed visual reference resolution model. however, the request for the ablation study lacks any supporting evidence or justification. it doesn't state why the ablation study is necessary or how it would specifically contribute to backing up the model's performance, making it unverifiable."
"2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?",NIPS_2021_1222,"Claims: 1.a) I think the paper falls short of the high-level contributions claimed in the last sentence of the abstract. As the authors note in the background section, there are a number of published works that demonstrate the tradeoffs between clean accuracy, training with noise perturbations, and adversarial robustness. Many of these, especially Dapello et al., note the relevance with respect to stochasticity in the brain. I do not see how their additional analysis sheds new light on the mechanisms of robust perception or provides a better understanding of the role stochasticity plays in biological computation. To be clear - I think the paper is certainly worthy of publication and makes notable contributions. Just not all of the ones claimed in that sentence.
1.b) The authors note on lines 241-243 that “the two geometric properties show a similar dependence for the auditory (Figure 4A) and visual (Figure 4B) networks when varying the eps-sized perturbations used to construct the class manifolds.” I do not see this from the plots. I would agree that there is a shared general upward trend, but I do not agree that 4A and 4B show “similar dependence” between the variables measured. If nothing else, the authors should be more precise when describing the similarities.
Clarifications: 2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.
2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?
2.c) The authors report mean capacity and width in figure 2. I think this is the mean across examples as well as across seeds. Is the STD also computed across examples and seeds? The figure caption says it is only computed across seeds. Is there a lot of variability across examples?
2.d) I am unsure why there would be a gap between the orange and blue/green lines at the minimum strength perturbation for the avgpool subplot in figure 2.c. At the minimum strength perturbation, by definition, the vertical axis should have a value of 1, right? And indeed in earlier layers at this same perturbation strength the capacities are equal. So why does the ResNet50 lose so much capacity for the same perturbation size from conv1 to avgpool? It would also be helpful if the authors commented on the switch in ordering for ATResNet and the stochastic networks between the middle and right subplots.
General curiosities (low priority): 3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.
3.b) There appears to be a pretty big difference in the overall trends of the networks when computing the class manifolds vs exemplar manifolds. Specifically, I think the claims made on lines 191-192 are much better supported by Figure 1 than Figure 2. I would be interested to hear what the authors think in general (i.e. at a high/discussion level) about how we should interpret the class vs exemplar manifold experiments.
Nitpick, typos (lowest priority): 4.a) The authors note on line 208 that “Unlike VOneNets, the architecture maintains the conv-relu-maxpool before the first residual block, on the grounds that the cochleagram models the ear rather than the primary auditory cortex.” I do not understand this justification. Any network transforming input signals (auditory or visual) would have to model an entire sensory pathway, from raw input signal to classification. I understand that VOneNets ignore all of the visual processing that occurs before V1. I do not see how this justifies adding the extra layer to the auditory network.
4.b) It is not clear why the authors chose a line plot in figure 4c. Is the trend as one increases depth actually linear? From the plot it appears as though the capacity was only measured at the ‘waveform’ and ‘avgpool’ depths; were there intermediate points measured as well? It would be helpful if they clarified this, or used a scatter/bar plot if there were indeed only two points measured per network type.
4.c) I am curious why there was a switch to reporting SEM instead of STD for figures 5 & 6.
4.c) I found typos on lines 104, 169, and the fig 5 caption (“10 image and”).",X,x,"the review point is asking for clarification on a specific technical detail regarding the computation method used in the paper, particularly regarding how clean exemplar manifolds are constructed for non-stochastic networks. this constitutes a request for clarification rather than a subjective opinion or claim. it does not require justification, evidence, or support since it is not making a direct evaluative statement about the authors' work. the comment is aimed at understanding the authors' methodology better, and thus fits the 'no claim' category.",no claim,"the review point is asking a question about how the methodology is applied specifically for creating clean exemplar manifolds for non-stochastic networks and how a calculation is performed in the study (i.e., computation of the denominator of figure 2.c). it does not contain any opinion, suggestion, or subjective statement that requires verification. it simply seeks clarification about the methodology or process described in the paper.",no claim,"the comment is primarily asking for a clarification on how clean exemplar manifolds are constructed and how a specific computation is carried out in the paper. it does not contain any subjective opinion or suggestion, merely posing questions to understand the methodology better. therefore, it is classified as 'no claim'."
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?,ARR_2022_93_review,"1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings.
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section.
3. Lack of screenshots of the experimental interface
Comments: 1. Please add screenshots of the interface that was designed.
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2?
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked.
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model?
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020.
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122.
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13).
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928.
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087.",X,x,"the review point consists of questions asking for clarification regarding the purpose of the average duration reported in table 1 and whether it includes time spent by the user waiting for the model to generate a response. these are normal statements requesting further explanation from the authors and do not contain claims, opinions, or suggestions. as such, they do not require justification or verification and are labeled as 'no claim'.",no claim,"the review point consists of questions that request clarification about the purpose and details of the average duration reported in table 1. it does not contain any claims, opinions, or suggestions that require verification or justification. instead, it is seeking information from the authors about the table's content, making it a factual inquiry rather than a subjective statement or claim.",no claim,"the comment asks a question and points out a lack of explanation regarding a factual element in the table. it does not express a subjective opinion or suggestion, nor does it require verification or justification."
"4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.",OvoRkDRLVr,"1. The paper proposes a multimodal framework built atop a frozen Large Language Model (LLM) aimed at seamlessly integrating and managing various modalities. However, this approach seems to be merely an extension of the existing InstructBLIP.
2. Additionally, the concept of extending to multiple modalities, such as the integration of audio and 3D modalities, has already been proposed in prior works like PandaGPT. Therefore, the paper appears to lack sufficient novelty in both concept and methodology.
3. In Table 1, there is a noticeable drop in performance for X-InstructBLIP. Could you please clarify the reason behind this? If this drop is due to competition among different modalities, do you propose any solutions to mitigate this issue?
4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.",X,4,"the review point makes a claim that a cautious approach should be taken regarding the contribution due to the promised dataset not being publicly available. this is supported by factual information about the availability status of the dataset. the rationale is mostly verifiable as the comment logically connects the unavailability of the dataset with the call for caution. however, the reviewer could strengthen their point with further explanation or evidence, such as detailing the importance of making the dataset available, which would fill the minor gap in support.",4,"the claim that a cautious approach should be taken due to the promised dataset not being publicly available is mostly verifiable. the reviewer notes the lack of public availability of the dataset, which is a factual statement. the claim that caution should be exercised is supported by the implication that accessibility of the dataset is crucial for verifying the contribution. the reasoning provided is mostly sufficient, but it could benefit from further details or references about the importance of dataset availability in evaluating scientific contributions.",3,"the claim here is that 'a cautious approach should be taken regarding this contribution until the dataset is openly accessible', suggesting that the dataset's unavailability impacts the evaluation of the contribution. the reasoning provided is somewhat general and lacks specific supporting evidence or references that would make it more robust, thus making the claim somewhat verifiable."
"4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand. Fig.",ICLR_2022_1842,"weakness, right?
Sec. 4.2: just for clarity, is each object's bounding box (for ray intersection) computation axis-aligned with the object coordinate system or the world/scene coordinate system?
Is there anything that constrains (in a soft or hard manner) the outgoing fractions to sum up/integrate to 1 or at most 1 for a given incoming light direction?
Fig. 10: What exactly is N in this figure? N is used in the main text to refer to the number of objects and to the number of point samples along a ray, neither of which seems like the right parameter here.
Minor suggestions for improvements:
Fig. 7: I currently cannot see much in this figure, a comparison to a white/grey environment map would make it easier to tell that there is an effect.
I'm not a fan of the equation two lines after Eq. 4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand.
Fig. 8: Switching out columns 2 and 3 would make the difficult comparison between No Indirect and Full Model easier.
There's a typo at the end of page 2: from from",X,2,"the review point contains two claims. firstly, the reviewer suggests that something needs to be changed to be mathematically correct, which indicates a subjective opinion. however, the claim lacks specific reasoning or a clear explanation as to why it is mathematically incorrect, making it hard for the authors to follow the suggestion. secondly, the reviewer questions the notation 'l_l', implying that the notation should be explained. the reviewer's question implies that prior introduction is necessary but does not provide a clear justification for this claim. therefore, both claims are borderline verifiable as they provide little guidance to the authors on how to address these issues.",2,"the review point contains a claim suggesting a change for mathematical correctness and a suggestion that the notation should be introduced earlier. however, the reviewer does not provide sufficient explanation or justification for why they believe the current form is mathematically incorrect or why the notation 'l_l' is problematic. the statement 'unless that makes a bunch of other equations messy' is vague and does not offer clear guidance. the claim is borderline verifiable as it is based on the reviewer's belief without robust evidence or detailed explanation, making it difficult for the author to follow the reasoning fully.",2,"the claim is 'this needs to be changed to be mathematically correct,' which suggests a modification. however, the rationale provided is vague, mentioning potential issues with equations becoming messy but without clear justification or detailed reasoning."
"2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",ARR_2022_12_review,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",X,x,"the review point consists of questions asking for clarification on how to interpret specific results presented in table 3. it does not include any claims, opinions, or suggestions that require verification. the reviewer is seeking further explanation rather than providing an evaluative comment.",no claim,"this review point poses a question asking for clarification or interpretation of the results shown in table 3. it does not contain any subjective statement, opinion, or suggestion that requires justification. the reviewer is seeking to understand specific results, but has not indicated any stance or required action on the part of the authors.",no claim,"the review point consists of questions asking for clarifications on interpreting specific results in table 3. it does not contain any claim, opinion, or suggestion, and only seeks factual responses, which do not require verification or justification."
"- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?",ARR_2022_186_review,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient
My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1? - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used? - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence.",X,x,"the review point consists of questions asking for clarification about the use of the challenge set in the paper. it does not contain any claims, opinions, or suggestions, and therefore does not require any verification. the reviewer is seeking clarification on how the challenge set is used and whether it is also used to augment training material, along with the data split. these are factual queries rather than evaluative or inferential statements that need justification or evidence.",no claim,"the review point primarily consists of questions seeking clarification regarding the use of the challenge set (cs) in the paper. it asks for factual information and does not contain any subjective opinion, suggestion, or judgment that would require justification. the reviewer is not making a claim or providing an evaluation but instead is asking the authors to clarify their methodology concerning the use of cs.",no claim,"the review point consists of a series of questions and clarifications but does not include any subjective opinion, suggestion, or evaluative claim that would require justification. it simply seeks clarification and further explanation regarding the use of the challenge set."
3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates?,NIPS_2016_339,"weakness of the model. How would the values in table 1 change without this extra assumption? 3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates? 4. An answer to this point may be beyond the scope of this work, but it may be interesting to think about it. It is mentioned (lines 104-106) that ""the examples [...] should maximally disambiguate the concept being taught from other possible concepts"". How is disambiguation measured? How can disambiguation be maximized? Could there be an information theoretic approach to these questions? Something like: the teacher chooses samples that maximally reduce the entropy of the assumed posterior of the student. Does the proposed model do that? Minor points: â¢ line 88: The optimal policy is deterministic. Hence I'm a bit confused by ""the stochastic optimal policy"". Is above defined ""the Boltzmann policy"" meant? â¢ What is d and h in equation 2? â¢ line 108: ""to calculate this ..."" What is meant by ""this""? â¢ Algorithm 1: Require should also include epsilon. Does line 1 initialize the set of policies to an empty set? Are the policies in line 4 added to this set? Does calculateActionValues return the Q* defined in line 75? What is M in line 6? How should p_min be chosen? Why is p_min needed anyway? â¢ Experiment 2: Is the reward 10 points (line 178) or 5 points (line 196)? â¢ Experiment 2: Is 0A the condition where all tiles are dangerous? Why are the likelihoods so much larger for 0A? Is it reasonable to average over likelihoods that differ by more than an order of magnitude (0A vs 2A-C)? â¢ Text and formulas should be carefully checked for typos (e.g. line 10 in Algorithm 1: delta > epsilon; line 217: 1^-6;)",X,x,"the review point consists of a request for clarification rather than a claim or opinion. it asks for details about the model parameters and inquires about the method of parameter selection, without suggesting changes or providing subjective opinions that require verification. therefore, it is classified as containing 'no claim'.",no claim,"the review point primarily consists of questions asking for clarifications regarding the model parameters and the method of choosing parameters. there are no subjective opinions, suggestions, or judgments present in the comment. the reviewer is seeking factual information about the study, and therefore, this does not require any justification or verification.",no claim,"the comment poses questions seeking clarification on specific parameters and their selection process. it doesn't express an opinion or suggestion, nor does it make any subjective claims that need verification. it consists of factual inquiries that do not require any justification."
"3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.",NIPS_2021_2168,"1.The motivation to investigate a graph structured model is to capture the global dependency structure in the sentence which different from existing sequence models that tend to focus on the dependency between each word and its close preceding words. However,the encoder and decoder is based on Transformer，which can draw global dependencies in sentence. Therefore, I am a bit confused about the complex approach of this article.
2.Table3 does not use the C3D feature to do the experiment, I think this paper should compare the result of C3D features in MSR-VTT with [1].On the MSR-VTT dataset, the performance of the method has not improved much on CIDEr, within 0.3%.
3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.
4.Constructing the generation tree, constructing the ground-truth dependency trees, calculating loss and reward all consume many computing resources. Is the time cost of the training process large?
[1] Zhang at al.Object Relational Graph with Teacher-Recommended Learning for Video Captioning.CVPR'2020 Typos:e.g.The last two rows of table3 are wrongly blackened on the METEOR on dataset MSR-VTT.",X,1,"the review point contains a claim indicating that there is an issue in the tables not listing certain cases. however, the reviewer does not provide any justification or reasoning for why these cases should be included, no explanation of the significance of this omission, and no references or evidence to validate their observation. therefore, the claim is considered unverifiable as it lacks supporting evidence or justification.",3,"the review point contains claims regarding the performance drop observed in the ablation experiment when reinforcement learning is removed, as well as the missing cases in the tables. these judgments imply subjective assessments about the experiment's findings and presentation. the reviewer does not provide specific examples, detailed explanations, or supporting references about how the tables are insufficiently listing the cases for dependency tree and rl. therefore, while the comment provides some support for its claim, critical elements are missing, requiring significant effort from the authors to follow the justification.",3,"the claim is that the performance dropped without reinforcement learning compared to without dependency tree, and that the tables do not include cases where neither dependency tree nor rl is used. the review comment provides partially verifiable observations about the content of the ablation experiments and tables but lacks detailed evidence or reference to support the claim fully."
"- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.",ACL_2017_726_review,"- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it. General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of “near-state-of-the-art accuracies” on ATIS and GeoQuery, which doesn’t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.
- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.
- Table 6: what is the breakdown of the score by correctness and incompleteness?
What % of incompleteness do these queries exhibit?
- What is expertise required from crowd-workers who produce the correct SQL queries? - It would be helpful to see some analysis of the 48% of user questions which could not be generated.
- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages. - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?
I thank the authors for their response.",X,no claim,"the comment is primarily asking for clarification about the specifics of 'table 4,' particularly regarding the splits used to obtain the atis numbers. it does not contain any claims, opinions, or suggestions about the paper's content that require justification. therefore, it does not need any verification.",no claim,"the review point primarily consists of a request for clarification and a thank you note to the authors for their response. it does not contain any subjective opinion, suggestion, or evaluative judgment that requires justification. the request for clarification ('what splits are used for obtaining the atis numbers?') is a factual, descriptive inquiry and does not constitute a claim. therefore, this review point does not fall under any of the claim categories outlined in the verifiability aspect.",3,"the claim is 'needs a little more clarification'. the verification is not extensive enough. additionally, the reviewer expresses gratitude for the authors' response, but this does not contribute to verifying the claim."

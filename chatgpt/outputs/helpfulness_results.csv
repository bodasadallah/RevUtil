review_point,paper_id,focused_review,helpfulness_label,chatgpt_helpfulness_definitions_score,chatgpt_helpfulness_definitions_rationale,chatgpt_helpfulness_definitions_examples_score,chatgpt_helpfulness_definitions_examples_rationale,chatgpt_helpfulness_definitions_incontext_learning_score,chatgpt_helpfulness_definitions_incontext_learning_rationale
1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.,ACL_2017_554_review,"1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families.
2) The paper does not talk about the training speed impact with more details.
- General Discussion:",1.0,3,"the comment identifies a weakness in the paper, specifically the lack of theoretical proofs and discussion on convergence properties of the proposed algorithm. however, it lacks depth and specific guidance on how the authors can address this issue. while the comment points towards an area needing improvement, it does not provide detailed, actionable suggestions for how the authors might go about enhancing this aspect of their draft.",2,"the review point identifies a weakness by pointing out the lack of theoretical proofs and convergence properties in the proposed algorithm. however, it is vague and lacks actionable guidance for the authors on how to address or improve this aspect. it does not provide specific suggestions or questions to guide the authors in enhancing their draft, making it only slightly beneficial.",4,"the comment is mostly helpful as it clearly indicates an area where the paper is lacking—in-depth theoretical exploration and demonstration of convergence properties. while it doesn't provide specific guidance on how to address this, it clearly identifies a significant gap, thus offering valuable insight for improving the draft."
"2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",ARR_2022_12_review,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",1.0,2,"the review point identifies a potential area of confusion or lacking detail in the interpretation of the results presented in table 3. however, it doesn't provide clear guidance or suggestions on how the authors could improve or clarify these interpretations within the table. the feedback is somewhat vague and lacks actionable advice, making it only slightly beneficial for the authors. providing specific suggestions on improving the interpretation or presentation of the results would have increased its helpfulness.",3,"the review point raises questions about the interpretation of results presented in table 3, specifically regarding the metrics chinese mos-q and chinese and english mos-v. while it identifies potential areas of confusion for the reader, it does not provide detailed guidance or actionable suggestions on how the authors might clarify or expand upon these results. therefore, the feedback is somewhat helpful as it highlights an area for improvement but lacks depth and clear direction for the authors to enhance their draft.",3,"the comment is somewhat helpful as it highlights a specific area of confusion in interpreting the results in table 3. however, it lacks depth and detail on how the authors could clarify or improve their presentation of these results, thus offering only partial guidance."
- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.,ARR_2022_317_review,"- Lack of novelty: - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.
- Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER.
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives.",1.0,3,"the comment identifies a lack of novelty in the paper, which is an important aspect for authors to consider. however, it lacks depth and actionable suggestions for how the authors might address this issue. the reviewer notes that the only new effort is applying known ideas to video-text models, but does not offer recommendations for enhancing the novelty of the work or exploring new directions, leaving the authors with some insight but limited guidance for improvement.",3,"the comment identifies a weakness in the paper regarding the lack of novelty, specifically mentioning that similar adversarial attack methods have been applied to other models and suggesting that the paper only recalibrates these methods for video-text models. this feedback provides some insight for the authors to consider the novelty and originality of their work. however, the comment lacks depth, as it does not offer detailed suggestions on how to address or improve this aspect, making it only somewhat helpful.",3,"the comment identifies a potential weakness in terms of novelty by pointing out that adversarial attacks have been applied to similar models before. however, it lacks specific suggestions or detailed guidance on how the authors might address or highlight their novelty, making it somewhat helpful but incomplete."
"- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.",ARR_2022_82_review,"- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer). - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.",1.0,4,"the review point identifies a specific weakness in the paper, namely the lack of demonstrated importance for the proposed post-processing steps through an ablation study. the suggestion to include an ablation study is clear and actionable, guiding the authors on how to improve their draft. however, the comment could be made more comprehensive by explaining why an ablation study would be beneficial in this context, which prevents it from being rated a 5.",4,"the review point is mostly helpful as it provides clear and actionable feedback on a specific area for improvement. it identifies the need for the paper to demonstrate the importance of the post-processing steps through an ablation study. the feedback is specific enough for the authors to understand what is lacking and what action can be taken to improve their draft. however, it could be expanded by suggesting how the ablation study could be structured or integrated into the paper for full comprehensiveness.",4,"the comment provides a specific suggestion to perform an ablation study to evaluate the importance of post-processing steps, which is actionable feedback that can significantly help the authors improve their draft by strengthening the experimental evidence for their methodology."
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.,ACL_2017_818_review,"- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations. What are some interesting action verbs and corresponding attributes relations? The paper also lacks analysis/discussion on what kind of mistakes their model makes.
- The number of object pairs (3656) in the dataset is very small. How many distinct object categories are there? How scalable is this approach to larger number of object pairs?
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.
General Discussion/Suggestions: - The authors should discuss the following work and compare against mining attributes/attribute distributions directly and then getting a comparative measure. What are the advantages offered by the proposed method compared to a more direct approach?
Extraction and approximation of numerical attributes from the Web Dmitry Davidov, Ari Rappoport ACL 2010 Minor typos: 1. In the abstract (line 026), the authors mention 'six' dimensions, but in the paper, there is only five.
2. line 248: Above --> The above 3. line 421: object first --> first 4. line 654: more skimp --> a smaller 5. line 729: selctional --> selectional",1.0,3,"the comment identifies a specific area of the paper that lacks clarity—how the frame similarity factors and attributes similarity factors are selected. however, it does not provide detailed suggestions or guidance on how this could be clarified or improved in the draft. therefore, while it points out a weakness, it lacks depth and completeness, offering only somewhat helpful feedback to the authors.",3,"the comment identifies an area that is unclear in the paper regarding the selection of frame similarity and attribute similarity factors. however, it lacks depth and specifics on how the authors could address this issue, such as suggesting ways to clarify these factors or offering potential solutions. while the authors can gain some insights from this feedback, it is not fully comprehensive, leaving them with the task of determining how to solve the issue.",3,"the comment identifies a potential area of confusion or lack of clarity in the selection of frame similarity and attribute similarity factors. however, it does not provide specific guidance on how to clarify or improve this aspect, making it somewhat helpful but lacking depth and completeness in its feedback."
"- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",ACL_2017_365_review,"1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.
2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.
3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.
References: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.
Multi-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.
- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",1.0,3,"the review point identifies a broad weakness related to the failure of an attention mechanism but lacks specificity and details on how to address the issue. while it encourages authors to explore the reasons behind the failure, it does not provide clear, actionable steps or suggestions for improvement. this leaves authors with some insights but without detailed guidance, thereby making the comment somewhat helpful.",4,"the comment is mostly helpful because it highlights a meaningful improvement area: understanding and addressing why the attention mechanism in seq2seq mtl fails and suggesting to modify it for better performance. however, it lacks specific guidance on how to achieve it or what specific changes could be made to the attention mechanism, preventing it from being fully comprehensive. nonetheless, it provides a clear direction for the authors to further explore and improve their work.",3,"the comment highlights a general area of improvement by suggesting an exploration of why the attention mechanism fails and hints at the possibility of modifying it. however, it lacks specific guidance or concrete steps, which would make it more actionable for the authors to address."
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.,ARR_2022_303_review,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses. However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text. Table 1's revised caption mentions 2.9K paragraphs as the size.
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR.
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column. I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work. S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base",1.0,3,"the comment identifies a significant issue with the paper's dependency on supplemental materials, which could be important for the authors to address. however, it lacks specific suggestions or detailed explanations on how the authors might restructure their content to make the paper more independent. therefore, while it points out a critical area for improvement, the guidance provided is somewhat vague and incomplete, limiting its overall usefulness to the authors.",3,"the comment identifies a weakness in the paper's reliance on supplemental material, which affects the paper's independence and comprehensibility. however, it lacks detailed guidance on how the authors might address this issue, such as integrating key supplemental information into the main text or offering specific sections that need expansion. while the feedback provides an important insight, it does not fully address how the authors can implement improvements, placing it in the 'somewhat helpful' category.",3,"the comment identifies an issue with the paper's reliance on supplemental material, which is a legitimate concern for the authors to address. however, it lacks specificity and actionable guidance on how to resolve this issue. while it points out the dependency on supplemental material, it does not suggest a clear solution or improvement, making it somewhat helpful but not fully comprehensive."
"1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021)",ARR_2022_233_review,"Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released.
1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them. First of all, will the author release the dataset or will it remain private?
Are the guidelines used to train the annotators publicly available?
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines.
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported.
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available?
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances.",1.0,2,"the comment suggests an additional reference for the authors to consider, which could be beneficial if the suggested paper is relevant. however, the comment lacks clarity and does not explain why this specific reference is important or how it would help improve the draft. without context or explanation, the guidance is minimal, making it only slightly beneficial for the authors.",2,"the review point provides a reference to an additional paper, which might be relevant to the topic of explainable nlp datasets. however, it lacks context or explanation about why this reference is important or how it could be integrated into the draft. without detailed guidance or a specific explanation, the comment only offers minimal assistance to the authors in terms of improving their manuscript.",2,"the comment is barely helpful as it suggests adding a reference, which might be beneficial, but does not provide reasoning or context on why this reference is relevant or how it specifically contributes to the improvement of the draft."
- A number of claims from this paper would benefit from more in-depth analysis.,ARR_2022_232_review,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed.
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements. In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset? Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption.",1.0,2,"the comment identifies a potential weakness by suggesting that a number of claims require more in-depth analysis. however, it is vague and lacks specificity, as it does not point out which claims specifically need further analysis or provide guidance on how to achieve this, making it only slightly beneficial for the authors.",2,"this comment identifies an improvement area by suggesting that the paper's claims would benefit from more in-depth analysis. however, it is vague and lacks clarity, as it does not specify which claims need more analysis or provide any guidance on how that analysis could be improved. therefore, it provides minimal actionable feedback for the authors.",3,"the comment is somewhat helpful as it identifies a general area for improvement—providing more in-depth analysis of certain claims. however, it lacks specificity regarding which claims need further analysis and does not offer detailed guidance or suggestions. the authors can gain some insights, but the feedback is not fully actionable or comprehensive."
"- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).",ICLR_2022_3352,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.",1.0,2,"the comment acknowledges potential areas of confusion in the paper but is vague and lacks clarity. it indicates that these areas are listed in another point (point 3), but as a standalone comment, it does not provide sufficient details or specific guidance. thus, it is barely helpful on its own and requires additional information for it to be more beneficial to the authors.",2,"the comment identifies a potential issue with the clarity of the paper, suggesting there are several places that may cause confusion. however, it lacks specificity and detailed guidance on how to address these areas, providing only minimal benefit to the authors. it references point 3 but doesn't provide direct examples or solutions within this comment itself, which limits its helpfulness as standalone feedback.",2,"the comment identifies a potential issue with the clarity of the paper, but it is vague and lacks specific examples or guidance on which parts are confusing. the reference to 'point 3' could imply more details, but without that, the feedback provides only minimal help for the authors to address issues of clarity and comprehensibility."
- the required implicit call to the Witness oracle is confusing.,NIPS_2018_914,"of the paper are (i) the presentation of the proposed methodology to overcome that effect and (ii) the limitations of the proposed methods for large-scale problems, which is precisely when function approximation is required the most. While the intuition behind the two proposed algorithms is clear (to keep track of partitions of the parameter space that are consistent in successive applications of the Bellman operator), I think the authors could have formulated their idea in a more clear way, for example, using tools from Constraint Satisfaction Problems (CSPs) literature. I have the following concerns regarding both algorithms: - the authors leverage the complexity of checking on the Witness oracle, which is ""polynomial time"" in the tabular case. This feels like not addressing the problem in a direct way. - the required implicit call to the Witness oracle is confusing. - what happens if the policy class is not realizable? I guess the algorithm converges to an \empty partition, but that is not the optimal policy. minor: line 100 : ""a2 always moves from s1 to s4 deterministically"" is not true line 333 : ""A number of important direction"" -> ""A number of important directions"" line 215 : ""implict"" -> ""implicit"" - It is hard to understand the figure where all methods are compared. I suggest to move the figure to the appendix and keep a figure with less curves. - I suggest to change the name of partition function to partition value. [I am satisfied with the rebuttal and I have increased my score after the discussion]",1.0,2,"the comment identifies a potential issue with the implicit call to the witness oracle being confusing. however, it lacks clarity and detail about why it is confusing or how the authors might address this problem. to be more helpful, the review should offer specific guidance or examples to help the authors understand and resolve the confusion.",2,"the comment identifies a potential area of confusion regarding the implicit call to the witness oracle, which could be seen as a weakness in the presentation or clarity of the draft. however, the comment lacks specific details, clarity, or suggestions on how to address this confusion. therefore, while it points out an issue, it provides minimal guidance for the authors to improve their manuscript.",2,"the comment identifies a potential area of confusion for the authors to address. however, it lacks clarity and specific guidance on how to resolve the issue, making it only slightly beneficial for the authors in improving their draft."
1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new.,NIPS_2018_476,"Weakness] 1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new. 2) Theoretical proofs of existing algorithm might be regarded as some incremental contributions. 3) Experiments are somewhat weak: 3-1) I was wondering why Authors conducted experiments with lambda=1. According to Corollary 1 and 2 lambda should be sufficiently large, however it is completely ignored for experimental setting. Otherwise the proposed algorithm has no difference from [10]. 3-2) In Figure 3, different evaluations are shown in different dataset. It might be regarded as subjectively selected demonstrations. 3-3) I think clustering accuracy is not very significant because there are many other sophisticated algorithms, and initializations are still very important for nice performance. It show just the proposed algorithm is OK for some applications. [Minor points] -- comma should be deleted at line num. 112: ""... + \lambda(u-v), = 0 ..."". -- ""close-form"" --> ""closed-form"" at line num.195-196. --- after feedback --- I understand that the contribution of this paper is a theoretical justification of the existing algorithms proposed in [10]. In that case, the experimental validation with respect to the sensitivity of ""lambda"" is more important rather than the clustering accuracy. So Fig. 1 in feedback file is nice to add paper if possible. I think dropping symmetry is helpful, however it is not new idea that is already being used. So, it will not change anything in practice to use it. Furthermore, in recent years, almost all application researchers are using some application specific extensions of NMF such as sparse NMF, deep NMF, semi-NMF, and graph-regularized NMF, rather than the standard NMF. Thus, this paper is theoretically interesting as some basic research, but weak from an application perspective. Finally, I changed my evaluation upper one as: ""Marginally below the acceptance threshold. I tend to vote for rejecting this submission, but accepting it would not be that bad.""",1.0,2,"the comment identifies a weakness in terms of originality, which is crucial for the authors to understand. however, it is vague and lacks specific details or suggestions on how the authors might address this issue. without guidance on how to improve the originality or differentiate their work, the feedback is only slightly beneficial. providing examples of how to enhance originality or suggestions for unique contributions could have made the review more helpful.",2,"the comment identifies a weakness in terms of originality, noting that the ideas of variable splitting and the algorithm are not new. however, it lacks specificity and actionable guidance that could help the authors address this point. the comment could be more helpful if it included suggestions for differentiating the work from existing literature or improving upon existing ideas. currently, it provides minimal insight or direction for the authors to enhance their draft.",2,"the comment is barely helpful as it points out a limitation regarding originality, but it lacks specificity and does not provide specific guidance on how the authors could address this issue or improve the draft."
3. The innovations of network architecture design and constraint embedding are rather limited. The authors discussed that the performance is limited by the performance of the oracle expert.,NIPS_2022_69,"1. This work uses an antiquated GNN model and method, it seriously impacts the performance of this framework. The baseline algorithms/methods are also antiquated. 2. The experimental results did not show that this work model obviously outperforms other variant comparison algorithms/models. 3. The innovations of network architecture design and constraint embedding are rather limited.
The authors discussed that the performance is limited by the performance of the oracle expert.",1.0,2,"the review point identifies a potential weakness in the network architecture design and constraint embedding, noting limited innovations and limitations due to the oracle expert's performance. however, the comment is vague and lacks clarity, as it does not provide specific guidance or detailed suggestions on how the authors can address these limitations. thus, it offers minimal assistance to the authors in terms of actionable feedback for improving their draft.",2,"the comment points out limited innovation in network architecture design and constraint embedding, which suggests an area for improvement. however, it lacks clarity and specific guidance on how the authors could address or enhance these aspects to improve their draft. mentioning the oracle expert as a performance limiter is a start, but without detailed suggestions or discussion on how to overcome these limitations, the comment remains vague and barely helpful for authors seeking concrete improvements.",3,"the comment is somewhat helpful, as it identifies limitations in the network architecture design and constraint embedding. however, it lacks actionable suggestions or detailed insights on how the authors might address these issues or improve their draft."
"4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.",ACL_2017_779_review,"However, there are many points that need to be address before this paper is ready for publication.
1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.
Also, how is the montecarlo sampling done? 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ). 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space. - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here 392 : Do you mean 2016?
Nitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.",2.0,3,"the comment highlights an issue regarding the allocation of space to the assumptions and corresponding experimental results, suggesting that it may be excessive. however, the feedback is somewhat incomplete as it lacks specific guidance on how the authors might address this issue, such as which parts could be condensed or how to effectively integrate the necessary information into other sections. nevertheless, it provides some insight into concerns about the paper's structure, making it somewhat helpful for the authors.",2,"the comment identifies a potential issue with the paper's structure, specifically suggesting that the authors may have allocated too much space to discussing assumptions and related experimental results. however, the comment lacks clarity on why this is a significant problem or how the authors could address it to improve their draft. without specific guidance or suggestions, the comment offers limited actionable feedback, making it barely helpful.",2,"the comment is barely helpful, as it points out an issue with the paper's structure and allocation of sections. however, it fails to provide specific guidance on how to improve the draft or what changes should be made. the suggestion remains implicit and lacks depth, making it only slightly beneficial for the authors."
"- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig.",ARR_2022_149_review,"- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.
Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349",2.0,2,"the comment identifies a potential weakness regarding societal biases in the knowledge bases but does not provide specific guidance or examples on how to address this issue. additionally, the mention of the figure example is vague and lacks context or detail, making it difficult for the authors to understand what specific improvements are needed. the feedback could benefit from more clarity and actionable suggestions to be more beneficial to the authors.",3,"the review comment identifies a potential weakness regarding societal biases in the knowledge bases used, which is an important issue to address. however, the feedback is somewhat lacking in depth and specificity. while it suggests authors consider societal biases, it doesn't provide specific guidance on what to look for or how to mitigate those biases if present. additionally, the comment about not being convinced with the example in fig is vague and could benefit from more detailed feedback to be more actionable. overall, the authors can gain some insights from this comment, but more detailed suggestions would make it more useful.",3,"the comment identifies a potential weakness in the paper related to societal biases in knowledge bases and provides some initial feedback about liking the approach taken by the authors. however, it lacks specificity and actionable guidance for the authors to address these concerns in a concrete way. the authors may gain some insight into areas they need to consider, but the feedback could be more detailed and focused to effectively help them improve the draft."
"- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.",NIPS_2017_351,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?",2.0,4,"the comment identifies a specific area of weakness regarding the probabilistic connection being insufficiently formal. it offers clear and actionable feedback by suggesting two potential routes for the authors to improve their draft: either formalize the connection more rigorously or adjust the language to clarify its motivational role. however, the comment could be expanded with more specific guidance or examples on how to achieve the suggested improvements, which would make it fully comprehensive. hence, it is rated as mostly helpful.",4,"the comment provides mostly helpful feedback by highlighting a specific weakness in the draft regarding the probabilistic connection. it clearly suggests that the authors either formalize this connection more rigorously or adjust the language to clarify its motivational purpose. while the feedback is actionable and provides a clear direction for improvement, additional details on how the authors might achieve this could have made it fully comprehensive and increased the score to 5.",4,"the comment is mostly helpful as it provides specific feedback regarding the lack of formal connection in terms of probability. it suggests two concrete actions: either formalize the probabilistic connection or adjust the language to clarify its motivational intent. this guidance is actionable, allowing the authors to address a specific weakness in their draft."
"- The applicability of the robust training scheme seems unlikely to scale to practical datasets, particularly those supported on high-dimensional domains. It seems like the accuracy would scale unfavorably unless the size of V scales exponentially with the dimension.",NIPS_2020_916,"My major complaints can be characterized into the following bullet points: - Robustness is argued only indirectly by way of Lipschitz constants. While the authors present a novel formulation (i.e., differing from the standard low-lipschitz=>robustness claims in the ML literature) of how controlling the lipschitz function of the classifier controls sensitivity to adversarial perturbations, this setting differs slightly from standard classification settings. In the standard classification setting, where labels are 1-hot vectors in R^dim(Y), a classifier typically returns as a label the argmax of the vector-valued function. Robustness then is usually considered as whether the argmax returns the right label, rather than a strictly-convex loss applied to the one-hot-label: this work incorporates 'confidence' into the robustness evaluation. - The applicability of the robust training scheme seems unlikely to scale to practical datasets, particularly those supported on high-dimensional domains. It seems like the accuracy would scale unfavorably unless the size of V scales exponentially with the dimension. - The example data distribution could be better chosen. As it stands, a classifier with (true) loss would require a Lipschitz constant that tends to infinity. A more standard/practical dataset would admit a perfect classifier that has a valid Lipschitz constant.",2.0,3,"the comment raises a concern regarding the scalability of the robust training scheme to practical, high-dimensional datasets, suggesting that accuracy might degrade unless certain conditions are met. this feedback identifies a potential weakness in the methodology. however, it lacks detailed guidance or specific suggestions on how the authors could address this issue or test the scalability. while the authors can gain some insights from the comment, they may need more comprehensive advice to effectively improve their draft.",3,"the comment identifies a potential weakness regarding the scalability of the robust training scheme to high-dimensional datasets. while this provides some insight into a possible limitation, it lacks depth and specificity about how the authors might address or validate this concern within their work. more detailed suggestions or examples on testing the scalability or strategies to improve it would make the feedback more actionable. therefore, it is somewhat helpful but does not fully guide the authors towards improvements.",4,"the comment is mostly helpful as it identifies a potential limitation of the robust training scheme in high-dimensional domains and suggests that the accuracy might scale unfavorably. while it does not provide a specific solution, it clearly highlights a significant issue for the authors to consider and address in their work."
"2. The results, while mostly based on ""standard"" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only ""standard"" to a small group of experts).",NIPS_2016_370,", and while the scores above are my best attempt to turn these strengths and weaknesses into numerical judgments, I think it's important to consider the strengths and weaknesses holistically when making a judgment. Below are my impressions. First, the strengths: 1. The idea to perform improper unsupervised learning is an interesting one, which allows one to circumvent certain NP hardness results in the unsupervised learning setting. 2. The results, while mostly based on ""standard"" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only ""standard"" to a small group of experts). 3. The paper is locally well-written and the technical presentation flows easily: I can understand the statement of each theorem without having to wade through too much notation, and the authors do a good job of conveying the gist of the proofs. Second, the weaknesses: 1. The biggest weakness is some issues with the framework itself. In particular: 1a. It is not obvious that ""k-bit representation"" is the right notion for unsupervised learning. Presumably the idea is that if one can compress to a small number of bits, one will obtain good generalization performance from a small number of labeled samples. But in reality, this will also depend on the chosen model class used to fit this hypothetical supervised data: perhaps there is one representation which admits a linear model, while another requires a quadratic model or a kernel. It seems more desirable to have a linear model on 10,000 bits than a quadratic model on 1,000 bits. This is an issue that I felt was brushed under the rug in an otherwise clear paper. 1b. It also seems a bit clunky to work with bits (in fact, the paper basically immediately passes from bits to real numbers). 1c. Somewhat related to 1a, it wasn't obvious to me if the representations implicit in the main results would actually lead to good performance if the resulting features were then used in supervised learning. I generally felt that it would be better if the framework was (a) more tied to eventual supervised learning performance, and (b) a bit simpler to work with. 2. I thought that the introduction was a bit grandiose in comparing itself to PAC learning. 3. The main point (that improper unsupervised learning can overcome NP hardness barriers) didn't come through until I had read the paper in detail. When deciding what papers to accept into a conference, there are inevitably cases where one must decide between conservatively accepting only papers that are clearly solid, and taking risks to allow more original but higher-variance papers to reach a wide audience. I generally favor the latter approach, I think this paper is a case in point: it's hard for me to tell whether the ideas in this paper will ultimately lead to a fruitful line of work, or turn out to be flawed in the end. So the variance is high, but the expected value is high as well, and I generally get the sense from reading the paper that the authors know what they are doing. So I think it should be accepted. Some questions for the authors (please answer in rebuttal): -Do the representations implicit in Theorems 3.2 and Theorem 4.1 yield features that would be appropriate for subsequent supervised learning of a linear model (i.e., would linear combinations of the features yield a reasonable model family)? -How easy is it to handle e.g. manifolds defined by cubic constraints with the spectral decoding approach?",2.0,3,"the comment highlights a potential issue with the accessibility of the results, noting that the techniques used may be considered standard only to a small group of experts. this implies that the paper might be too challenging for a broader audience, hinting at a need for additional explanation or context. however, the feedback is somewhat vague and does not provide explicit suggestions on how to make the content more accessible, such as recommending more detailed explanations or background information. thus, while it identifies a valid issue, the guidance provided is not comprehensive, leaving room for the authors to seek more specific advice.",3,"the comment identifies a weakness regarding the accessibility and originality of the results due to their reliance on 'standard' techniques that are familiar primarily to a small group of experts. however, the feedback lacks depth as it does not provide specific suggestions or alternative approaches to make the work more accessible or demonstrably original. this level of feedback gives the authors some insight into an area for improvement but does not fully guide them in how to address it.",2,"the comment is barely helpful, as it highlights the complexity and non-obviousness of the results being based on 'standard' techniques but does not offer actionable guidance on how the authors might address this perception or provide clarifications for broader accessibility."
"- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).",CEPkRTOlut,"No ethics section, but there are ethical issues that deserve discussion (see the ethics section).
Also a few, mostly minor points:
- When the corpus was created, participants were told to speak in such a way to make the intent of the speech unambiguous. This may lead to over-emphasis compared with natural speech. There was no mention of any evaluation of the data to avoid this.
- The corpus was created with only ambiguous sentences, and the non-ambiguous content was taken from another source. There is a chance that different recording qualities between news (LSCVR) and crowdsourced data could artificially raise the ability of the model to distinguish between ambiguous (tag 1 or 2) and non-ambiguous (tag 0) sentences.
- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).
- It would be hard to reproduce the fine tuning of the IndoBART model without a little more information. Was it fine-tuned for a certain number of steps, for example?",2.0,4,"the comment points out a potential weakness in the experimental design regarding the disparity in data used for training different models. it questions the validity of the authors' conclusion about the superiority of one model over another. this feedback is relatively clear and provides an actionable insight, prompting the authors to re-evaluate their conclusion based on the training data's sufficiency and fairness. however, the comment could be more helpful if it included suggestions for how the authors might address this discrepancy, such as proposing additional experiments or analyses that could strengthen their conclusions.",3,"the comment identifies a potential weakness in the conclusions drawn from the experimental results, specifically the amount of data used for training the different models and how this could affect the results. this could alert the authors to reconsider their interpretation of the results, making it somewhat helpful. however, the feedback lacks depth as it does not suggest how to address this issue, such as exploring if the smaller dataset biases the results or suggesting additional experiments. thus, it only provides incomplete insights into improving the draft.",4,"the comment is mostly helpful as it provides a clear observation that the difference in data size used for training could affect the validity of the conclusions drawn. it suggests that the authors should reconsider their conclusions regarding the effectiveness of the direct model versus the end-to-end system, although it could be more explicit in suggesting specific actions to improve the analysis."
"- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.",NIPS_2020_1454,"- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice. - Claims to be SOTA on three datasets, but this does not seem to be the case. Does not evaluate on what it trains on (see ""additional feedback"").",2.0,2,"the comment points out a perceived weakness that the contributions over previous methods are small and mostly engineering-based, but it lacks specificity and actionable guidance for the authors. the remark highlights an issue but does not provide clear suggestions on how to address the perceived similarity with previous work, hence it is only slightly beneficial for the authors.",2,"the comment identifies a weakness in the paper by stating that the contributions are small and the work is hard to differentiate from previous methods. however, it only provides limited guidance on how to address this issue, lacking clear, detailed suggestions for improvements, making it only slightly beneficial for the authors.",4,"the comment is mostly helpful as it highlights a potential weakness in the contribution of the work compared to existing methods, suggesting that the improvements might be minor and largely engineering-focused. this implies the authors may need to better differentiate their work or more clearly articulate the unique contribution. however, it could be more explicit in suggesting how the authors can address this or what specific changes could highlight the novel aspects of their work."
"2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).",ACL_2017_318_review,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",3.0,3,"the comment identifies a potential issue with the evaluation on the word analogy task, noting that the semantic relations might be explicitly encoded by the sememes. however, it is somewhat vague and lacks specific details about how this impacts the fairness of the evaluation or how the authors might address this issue, leaving it somewhat incomplete. it provides a point of consideration for the authors, which can lead to improvement, but does not offer substantial guidance or actionable steps.",3,"the comment identifies a potential issue in the evaluation method of the word analogy task, pointing out the explicit encoding of semantic relations by the sememes. however, the feedback is somewhat incomplete, as it mentions more details below but does not provide immediate actionable suggestions or fully elaborate on how this affects the evaluation's fairness. thus, while it provides a helpful insight, it lacks depth, which limits its helpfulness.",3,"the comment is somewhat helpful as it highlights a potential issue with the evaluation on the word analogy task, suggesting an area for improvement. however, it lacks specific guidance on addressing the concern, making the feedback less actionable for the authors."
"- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?",ARR_2022_23_review,"The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?",3.0,3,"the comment identifies a potential weakness in the methodology by questioning the rationale behind selecting only 10 answers, which might lead to an underestimation of performance. however, it lacks depth and does not offer specific suggestions or guidance on how the authors could address this issue, thus only providing limited actionable feedback.",3,"the review point identifies an area for clarification regarding the selection of 10 answers, potentially impacting the performance evaluation. however, it lacks depth by not suggesting concrete ways the authors could address this issue or expand on how it affects the study's conclusions. the comment gives some guidance on an area needing improvement but does not fully outline the steps to address it, making it only somewhat helpful.",3,"the comment is somewhat helpful as it raises a concern about the selection of answers and its potential impact on performance estimation. however, it does not provide a specific suggestion or next step for the authors to address this issue."
"1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.",ACL_2017_331_review,"The document-independent crowdsourcing annotation is unreliable. - General Discussion: This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here.
1) Is it necessary to treat concept map extraction as a separate task?
On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.
2) How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.
I appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.",3.0,2,"the comment brings up an interesting point about the necessity of treating concept map extraction as a separate task and highlights the challenges with readability as the node number increases. however, it is barely helpful because it lacks specific actionable suggestions or detailed guidance for the authors on how to address the concern. the authors may find it difficult to derive concrete next steps for improving their draft based on this feedback alone.",3,"the review point identifies a potential weakness in the necessity of treating concept map extraction as a separate task, but it does not provide a clear rationale or actionable suggestions for improvement. the comment raises a relevant point regarding the readability of general summaries as the node number increases, suggesting some insight into potential readability issues. however, it lacks specific guidance or clarity on how the authors might address the concern, leaving it incomplete and somewhat helpful.",4,"the comment is mostly helpful as it raises a relevant question about the necessity of treating concept map extraction as a separate task. it also provides specific points for the authors to consider regarding the implications of current approaches and the challenges posed by the increase in node numbers. the feedback could be expanded with more specific suggestions, but it still offers valuable guidance for authors to refine their draft."
"- When trained and evaluated with the same time-step, Figure 5 shows similar performance between the baseline model and the time-aware model. This makes the effectiveness of the proposed methods questionable when the goal is just to achieve good performance. Maybe under some scenarios where the training time-step and evaluation time-step are different, the proposed method might make more sense.",8lwWBSa1pJ,"- The motivation referring to the Nyquist-Shannon sampling theorem does not seem to be consistent with the observation of numerical experiments. From the sampling theorem, as long as the sampling frequency is high enough, there won't be any information loss. But the numerical experiments suggest that the non time-aware model performs poorly even when the sampling frequency is the highest. This inconsistency makes the connection to the sampling theorem questionable.
- During the training phase, the sampling step is randomly selected from a log-uniform distribution. It is suggested that this distribution helps stabilize the training process, but no theoretical nor numerical analysis is provided. Some ablation studies using different sampling distribution might provide some insights to the choice.
- In the numerical evaluation, performance is compared across different observation rate. Given any time-step, the time-aware model can provide the appropriate prediction by conditioning on the time-step. But for the model trained with a fixed time-step, the information of the observation time-step does not seem to be adjusted. For example, for a model trained with $\Delta t=1$ms, when evaluating at $\Delta t=2$ms, instead of applying the model once, one might want to apply the model twice to have a more accurate prediction given the knowledge of the doubled time-step. Without doing some kind of adjustments for the baseline models make the fairness of comparisons questionable.
- When trained and evaluated with the same time-step, Figure 5 shows similar performance between the baseline model and the time-aware model. This makes the effectiveness of the proposed methods questionable when the goal is just to achieve good performance. Maybe under some scenarios where the training time-step and evaluation time-step are different, the proposed method might make more sense.",3.0,3,"the review point raises a legitimate concern about the effectiveness of the proposed method, as it questions whether the time-aware model provides significant benefits when trained and evaluated at the same time-step. however, it lacks depth and specificity in terms of providing actionable suggestions or alternative approaches. while it hints that the proposed method may be more effective under different scenarios, it does not offer concrete examples or guidance on how to test or address this issue. therefore, it is somewhat helpful, but not sufficiently detailed to guide the authors in making significant improvements to their draft.",3,"the comment identifies a potential issue with the proposed methods, specifically questioning their effectiveness when trained and evaluated under the same time-step conditions. it hints at the possibility that the method might be more applicable under different conditions but does not offer specific guidance or fully analyze alternative scenarios. the authors can gain some insight from this observation, but it lacks sufficient depth and actionable advice to significantly aid them in improving their draft. therefore, it falls under the 'somewhat helpful' category, as it points out a weakness but does not provide comprehensive guidance for addressing it.",5,"the comment is highly helpful as it identifies a potential limitation in the current evaluation setup and suggests a specific scenario where the proposed method might demonstrate its advantages. this provides clear guidance for the authors on how to potentially strengthen their arguments and improve their evaluations, making it an actionable and constructive piece of feedback."
"5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?",ACL_2017_489_review,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.",3.0,3,"the comment identifies a significant weakness regarding the clarity and integration of the empirical results in relation to the research question and hypothesis. however, it lacks specificity and actionable guidance on how the authors might address this issue, such as suggesting ways to clarify the narrative or offer specific areas where integration could be improved. this makes the comment somewhat helpful as it points out a critical area for improvement but does not fully guide the authors on how to enhance their draft.",4,"the comment is mostly helpful as it clearly identifies a weakness in the presentation of the empirical results and analyses. it points out the difficulty in understanding the overall contribution of the experiments to the underlying research question and specific hypothesis. this feedback is actionable as it urges the authors to improve the clarity and connectivity of their empirical findings. however, the comment could be more detailed by offering specific suggestions on how to achieve this, such as proposing ways to restructure the results or highlighting which parts are unclear.",4,"the comment is mostly helpful as it highlights a key issue with the paper regarding the clarity and coherence of the empirical results and analyses. the reviewer points out that the experiments lack connection to the overall research question and specific hypothesis. this indicates that the authors need to improve the organization and integration of their experimental findings to provide a clearer narrative. while the feedback is constructive and actionable, it could be more specific in suggesting how the authors might restructure their presentation for better clarity, thus not fully reaching the 'highly helpful' category."
"2. In section 2, ""Finally, INRs operate on a per-data-instance basis, meaning that one time-series instance is required to train an INR"". This claim is true but I don't think it is an advantage. A model that can only handle a single time series data is almost useless.",FpElWzxzu4,"1. In the intro section, the claim that Transformers reply on regularly sampled time-series data is wrong. For example, [1] shows that the Transformer model handles irregularly-sampled time series well for imputation.
2. In section 2, ""Finally, INRs operate on a per-data-instance basis, meaning that one time-series instance is required to train an INR"". This claim is true but I don't think it is an advantage. A model that can only handle a single time series data is almost useless.
3. In section 3.1, why $c_i^t$ is a vector rather than a scalar? It just denotes a temporal coordinate of a time and should be a scalar.
4. Why the frequency of the Fourier feature is uniformly sampled?
5. ""Vectors Bm ∈ R1×DψF , δm ∈ R1×DψF denote the phase shift and the bias respectively"". I think what the author wants to claim is that B represents bias while δm denotes phase shift.
6. In Figure 5, it is unclear what the vertical axis represents.
[1] NRTSI: Non-Recurrent Time Series Imputation, ICASSP 2023.",3.0,3,"the comment points out a potential weakness in the claim made by the authors in section 2 about the limitation of inrs when dealing with only single time-series data. it provides some insight by challenging the notion that this is advantageous, suggesting that this could be seen as a drawback. however, the comment could be more helpful if it provided specific guidance or suggestions on how to address or reframe the argument to strengthen the draft. it identifies an area for improvement but lacks depth and actionable advice, making it somewhat helpful.",3,"the comment identifies a potential weakness in the authors' claim about the advantage of inrs operating on a per-data-instance basis. it challenges the notion that handling a single time series data is beneficial, pushing the authors to reconsider or justify this point. however, it could be more helpful by providing suggestions or questions that guide the authors on how to address this issue or improve the discussion in their draft.",4,"the comment is mostly helpful as it clearly identifies a claim that may be overstated in the paper and provides a reason why this might not be an advantage. this gives authors an insight into a potential weakness, but further guidance on how to address this in the draft could make it more comprehensive."
"- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?",ARR_2022_186_review,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient
My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1? - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used? - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence.",3.0,3,"the comment identifies a specific area of confusion regarding the use of the challenge set and requests clarification on whether it is used for training or evaluation purposes. however, it is somewhat helpful because while it points out a potential gap in the draft, it does not offer any suggestions for improvement or fully explore the implications of the confusion. thus, it provides some insight that can guide the authors in clarifying their methodology, but would benefit from being more detailed or offering constructive suggestions.",3,"the comment identifies specific areas of ambiguity regarding the use of the challenge set and its role in the research. it raises questions about whether the challenge set is used for augmenting training data and asks for clarification on the data split if that is the case. however, the comment lacks depth or detailed guidance on how the authors should address these issues, such as suggesting specific ways to clarify or present this information in the document. thus, it offers some insights but does not completely fulfill the authors' needs for improvement.",3,"the comment is somewhat helpful because it points out a lack of clarity regarding the use of the challenge set, specifically whether it is used only for evaluation or also for training. it prompts clarification on the data split. however, the feedback could be more detailed in suggesting how the authors might clarify these issues in their draft."
"- vulnerability discovery methodology is also questionable. The authors consider a single vulnerability at a time, and while they acknowledge and address the data imbalance issue, I am not sure about the ecological validity of such a study. Previous work has considered multiple CVEs or CWEs at a time, and report whether or not the code contains any such vulnerability. Are the authors arguing that identifying one vulnerability at a time is an intended use case? In any case, the results are difficult to interpret (or are marginal improvements at best).",5EHI2FGf1D,"- no comparison against baselines. The functionality similarity comparison study reports only accuracy across optimization levels of binaries, but no baselines are considered. This is a widely-understood binary analysis application and many papers have developed architecture-agnostic similarity comparison (or often reported as codesearch, which is a similar task).
- rebuttal promises to add this evaluation
- in addition, the functionality similarity comparison methodology is questionable. The authors use cosine similarity with respect to embeddings, which to me makes the experiment rather circular. In contrast, I might have expected some type of dynamic analysis, testing, or some other reasoning to establish semantic similarity between code snippets.
- rebuttal addresses this point.
- vulnerability discovery methodology is also questionable. The authors consider a single vulnerability at a time, and while they acknowledge and address the data imbalance issue, I am not sure about the ecological validity of such a study. Previous work has considered multiple CVEs or CWEs at a time, and report whether or not the code contains any such vulnerability. Are the authors arguing that identifying one vulnerability at a time is an intended use case? In any case, the results are difficult to interpret (or are marginal improvements at best).
- addressed in rebuttal
- This paper is very similar to another accepted at Usenix 2023: Can a Deep Learning Model for One Architecture Be Used for Others?
Retargeted-Architecture Binary Code Analysis. In comparison to that paper, I do not quite understand the novelty here, except perhaps for a slightly different evaluation/application domain. I certainly acknowledge that this submission was made slightly before the Usenix 2023 proceedings were made available, but I would still want to understand how this differs given the overall similarity in idea (building embeddings that help a model target a new ISA).
- addressed in rebuttal
- relatedly, only x86 and ARM appear to be considered in the evaluation (the authors discuss building datasets for these ISAs). There are other ISAs to consider (e.g., PPC), and validating the approach against other ISAs would be important if claiming to build models that generalize to across architectures.
- author rebuttal promises a followup evaluation",3.0,3,"the comment identifies potential weaknesses in the vulnerability discovery methodology used by the authors, questioning its ecological validity and comparing it with previous work. however, it lacks specific suggestions or guidance on how to address these concerns. the comment opens up valuable discussion points regarding the approach taken but doesn't provide detailed, actionable suggestions for improvement, making it somewhat helpful for the authors.",3,"this comment is somewhat helpful because it identifies a key weakness in the methodology, specifically the focus on a single vulnerability, and questions its ecological validity. it also brings up the concern that results are either difficult to interpret or show marginal improvements. however, the comment lacks depth and specificity in its suggestions, as it does not provide concrete guidance on how the authors might address these issues or enhance the relevance of their results. while the authors can gain insights from this feedback, it does not fully guide them to improve their draft comprehensively.",3,"the comment is somewhat helpful as it raises a valid concern regarding the methodology of the study. it questions the ecological validity and compares it to previous work, suggesting that the single vulnerability approach may not be the most effective. however, it lacks specific, actionable suggestions for the authors on how they can address or improve this aspect in their study."
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?,ARR_2022_93_review,"1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings.
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section.
3. Lack of screenshots of the experimental interface
Comments: 1. Please add screenshots of the interface that was designed.
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2?
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked.
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model?
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020.
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122.
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13).
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928.
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087.",3.0,4,"the comment is mostly helpful as it identifies a specific area of concern regarding the average duration reported in table 1 and its clarity in the manuscript. by pointing out the lack of explanation and questioning whether it includes user wait time, the reviewer prompts the authors to provide more clarity and detail, which is actionable feedback. however, the suggestion could be expanded further by proposing specific ways to address the issue, such as providing a clearer definition or context for the reported metric, hence it is not rated as a 5.",4,"the comment provides clear feedback by specifically questioning the purpose and explanation of the average duration reported in table 1. it highlights the lack of supporting explanation and questions whether it includes the waiting time for model response. this comment directs the authors to clarify this point in their draft, which is actionable feedback. however, it could be more comprehensive by suggesting how the authors might improve clarity or why this detail is important, thus it does not reach the maximum helpfulness score.",4,"the comment is mostly helpful as it identifies a specific weakness in the paper (lack of explanation regarding average duration in table 1) and implies the need for clarification. this guidance is clear and actionable, though it could be slightly enhanced by more detailed suggestions or context."
"- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here. Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.",ACL_2017_128_review,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.",3.0,3,"the review point identifies a specific concern regarding the use of the term 'knowledge' and challenges the generalization claim of the model. however, it lacks depth in explaining why the term 'knowledge' is misleading beyond mentioning syntactic versus semantic distinctions. additionally, while it questions the substructure representation as a sequence of words, it does not provide an alternative approach or further clarification. thus, the comment is somewhat helpful as it offers insight into potential issues but doesn't fully guide the authors on how to address these concerns in their draft.",4,"the review point effectively highlights a specific issue with the terminology and representation used in the paper, questioning the validity of calling the substructure 'knowledge' and suggesting that its representation as a sequence of words might be problematic. this feedback is clear and directs attention to a conceptual misunderstanding or misrepresentation that could mislead readers. however, to be fully comprehensive and warrant a score of 5, it could offer specific suggestions on how to address the issue, such as offering alternative terminology or methods of representation that align more accurately with the intended meaning or standard usage in the field.",4,"the comment is mostly helpful as it highlights a potential issue with the claim regarding how the model generalizes knowledge. it provides actionable feedback by questioning the appropriateness of calling the substructure 'knowledge' and suggests clarification on this matter. however, it could be expanded by specifying how the authors might address or clarify this in their paper."
"2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.",NIPS_2021_291,"The writing is clear and the motivation is clarified clearly. Besides, the theoretical grounding and experimental evaluation are not sufficient to show their originality and significance. Here are some of the suggestions: 1) I would like to see ablation studies for the proposed training method, the traditional backpropagation framework refers as the baseline. 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.",3.0,4,"the review comment is mostly helpful as it highlights a significant area that requires attention: how the paper handles different types of inputs, which is crucial for understanding the versatility and applicability of the proposed method. the suggestion to discuss and present solutions for this is clear and actionable. additionally, it mentions a secondary issue with the citations being 'a bit disordered,' which, although not very descriptive, points out a technical area for improvement. the feedback could be more comprehensive by providing examples or more specific guidance on how to structure the discussion, but overall it provides practical steps for the authors to enhance their draft.",4,"the review comment identifies a specific area for improvement, asking the authors to discuss and present solutions for dealing with different types of inputs, which is likely a crucial aspect for understanding the applicability of the research. additionally, the comment points out an issue with disordered citations, an actionable detail that can help improve the draft's organization and readability. while the feedback is clear and actionable, it lacks detailed guidance or suggestions on how to address these points, which would make it a fully comprehensive and highly helpful comment.",4,"the comment is mostly helpful as it identifies a potential gap in the paper regarding the discussion on handling different types of inputs, suggesting it would be valuable to include such a discussion and solutions. additionally, the comment points out an issue with the organization of citations. while the feedback is actionable and clear, it could be expanded with more specific guidance or examples to be fully comprehensive."
"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.",ARR_2022_311_review,"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector.
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data?",3.0,3,"the comment identifies a notable weakness in the experimental aspect of the paper as being limited to extremely low-resource settings and easier tasks, which restricts its relevance to real-world applications. however, it does not provide specific suggestions on how the authors might broaden their experiments or which additional tasks to consider. while the comment suggests that the proposed method has potential for more nlp tasks, it stops short of offering detailed guidance on how to explore this potential. therefore, the feedback offers some insights into areas for improvement but lacks depth and specificity needed to be more actionable and comprehensive for the authors.",3,"the review point identifies specific weaknesses in the paper, including the limited experimental setting and the narrow application of the augmentation method. however, while the reviewer gives some insights into potential improvement areas, such as exploring more nlp tasks beyond sentence classification and expanding the experimental setup, the feedback lacks detailed guidance or actionable steps for the authors to address these issues comprehensively. this makes the comment somewhat helpful for improving the draft.",4,"the comment is mostly helpful because it identifies clear weaknesses in the experimental setup and suggests areas for improvement. it highlights that the experiments are limited to a specific regime and suggests exploring a broader range of tasks. however, it could be more actionable by providing specific suggestions on how to broaden the experiments, making it a less-than-perfect score."
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).,ARR_2022_98_review,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges.
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system. 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.",3.0,4,"the comment is mostly helpful because it encourages the authors to include examples of their system applied to real texts, which can enhance the clarity and applicability of their work. however, it could be improved by specifying what types of examples or aspects of the system to focus on, which would make the feedback more comprehensive and impactful.",3,"the comment suggests an area for improvement by asking for examples of the system on actual texts, which could provide clarity and context for the authors. however, it lacks specificity and depth in terms of how these examples would add value or address potential weaknesses in the draft. this feedback gives the authors some direction but does not fully guide them in improving the draft comprehensively.",4,"the comment is mostly helpful, as it provides a clear suggestion to include examples of the system on actual texts, which would help the authors improve the clarity and persuasiveness of their work. however, it could be expanded with more specific guidance on what type of examples to include or how they should be presented."
"- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",ACL_2017_614_review,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?",3.0,4,"the comment identifies a specific weakness in the approach, specifically the poor performance on nouns, which contradicts the claim of generalizability to all parts of speech. it requests clarification on the oracle gap for a particular model, indicating that the reviewer has a concern related to the comprehensive performance of the approach. this feedback is actionable as it points out an inconsistency that the authors can address by providing additional explanation or results. however, the comment could be expanded to suggest specific ways to address the identified issues or further elaborate on the implications of this performance gap, thus achieving the highest score of 5.",4,"the comment provides clear and actionable feedback by identifying a specific weakness related to the poor performance on nouns and the contradiction in the generalization claim. the reviewer expresses concern about the discrepancy and asks for clarification on the gap, which could guide the authors in addressing these issues. however, the comment could be more helpful if it included suggestions on how to investigate or improve the performance discrepancy, thus lacks full comprehensiveness.",4,"the comment is mostly helpful as it clearly points out a discrepancy between the claimed generalizability of the clustering approach and its actual performance on nouns. it suggests that further analysis or clarification is needed, which could significantly help the authors address a potential weakness in their draft. however, the comment could be even more helpful by providing specific suggestions on how to approach resolving this gap."
- This pipeline style method including two models does not give better average results for both XVNLI and MaRVL. Baseline models in the experiments are not well introduced.,4kuLaebvKx,"- The chained impacts of image captioning and multilingual understanding model in the proposed pipeline. If the Image caption gives worse results and the final results could be worse. So The basic performance of the image caption model and multilingual language mode depends on the engineering model choice when it applies to zero-shot.
- This pipeline style method including two models does not give better average results for both XVNLI and MaRVL. Baseline models in the experiments are not well introduced.",3.0,3,"the review point identifies two areas of concern: the effectiveness of the pipeline style method and the inadequate introduction of baseline models. however, the feedback lacks depth and specific guidance. it does not suggest how the method could be improved or how the introduction of baseline models could be made clearer. thus, while it provides some insights into potential weaknesses, it does not fully empower the authors with actionable steps to address these issues.",3,"the review point identifies a weakness by stating that the baseline models used in the experiments are not well introduced, which suggests an area for improvement. however, the comment lacks depth and specificity, as it does not provide clear guidance on how to better introduce the baseline models, nor does it provide detailed feedback regarding the performance comparison aspect. it gives authors some insight into addressing the presentation of their experiments, but does not fully address their needs for improving the draft.",4,"the comment is mostly helpful because it identifies weaknesses in the experiment's methodology (the lack of a clear introduction to baseline models) and clarifies the results of the pipeline style method in a specific context (xvnli and marvl). however, it could be expanded by offering suggestions on how to improve the exposition or the experimental setup."
"4) You perform ""on par or better"" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to ""on par"" and all the rest to ""better"". I think this wording should be corrected, but otherwise I'm fine with the experimental results.",ACL_2017_105_review,"Maybe the model is just an ordinary BiRNN with alignments de-coupled.
Only evaluated on morphology, no other monotone Seq2Seq tasks.
- General Discussion: The authors propose a novel encoder-decoder neural network architecture with ""hard monotonic attention"". They evaluate it on three morphology datasets.
This paper is a tough one. One the one hand it is well-written, mostly very clear and also presents a novel idea, namely including monotonicity in morphology tasks. The reason for including such monotonicity is pretty obvious: Unlike machine translation, many seq2seq tasks are monotone, and therefore general encoder-decoder models should not be used in the first place. That they still perform reasonably well should be considered a strong argument for neural techniques, in general. The idea of this paper is now to explicity enforce a monotonic output character generation. They do this by decoupling alignment and transduction and first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments.
However, the authors are unclear on this point. I have a few questions: 1) How do your alignments look like? On the one hand, the alignments seem to be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input character can be aligned with zero, 1, or several output characters. However, this seems to contrast with the description given in lines 311-312 where the authors speak of several input characters aligned to 1 output character. That is, do you use 1-to-many, many-to-1 or many-to-many alignments?
2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first stage, align input and output characters monotonically with a 1-to-many constraint (one can use any monotone aligner, such as the toolkit of Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to predict exactly these 1-to-many alignments. For example, flog->fliege (your example on l.613): First align as in ""f-l-o-g / f-l-ie-ge"". Now use any tagger (could use an LSTM, if you like) to predict ""f-l-ie-ge"" (sequence of length 4) from ""f-l-o-g"" (sequence of length 4). Such an approach may have been suggested in multiple papers, one reference could be [*, Section 4.2] below.
My two questions here are: 2a) How does your approach differ from this rather simple idea?
2b) Why did you not include it as a baseline?
Further issues: 3) It's really a pitty that you only tested on morphology, because there are many other interesting monotonic seq2seq tasks, and you could have shown your system's superiority by evaluating on these, given that you explicitly model monotonicity (cf. also [*]).
4) You perform ""on par or better"" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to ""on par"" and all the rest to ""better"". I think this wording should be corrected, but otherwise I'm fine with the experimental results.
5) You say little about your linguistic features: From Fig. 1, I infer that they include POS, etc. 5a) Where did you take these features from?
5b) Is it possible that these are responsible for your better performance in some cases, rather than the monotonicity constraints?
Minor points: 6) Equation (3): please re-write $NN$ as $\text{NN}$ or similar 7) l.231 ""Where"" should be lower case 8) l.237 and many more: $x_1\ldots x_n$. As far as I know, the math community recommends to write $x_1,\ldots,x_n$ but $x_1\cdots x_n$. That is, dots should be on the same level as surrounding symbols.
9) Figure 1: is it really necessary to use cyrillic font? I can't even address your example here, because I don't have your fonts.
10) l.437: should be ""these"" [*] @InProceedings{schnober-EtAl:2016:COLING, author = {Schnober, Carsten and Eger, Steffen and Do Dinh, Erik-L\^{a}n and Gurevych, Iryna}, title = {Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks}, booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers}, month = {December}, year = {2016}, address = {Osaka, Japan}, publisher = {The COLING 2016 Organizing Committee}, pages = {1703--1714}, url = {http://aclweb.org/anthology/C16-1160} } AFTER AUTHOR RESPONSE Thanks for the clarifications. I think your alignments got mixed up in the response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1, 1-1, and later make many-to-many alignments from these.
I know that you compare to Nicolai, Cherry and Kondrak (2015) but my question would have rather been: why not use 1-x (x in 0,1,2) alignments as in Schnober et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much your results would have differed from such a rather simple baseline. ( A tagger is a monotone model to start with and given the monotone alignments, everything stays monotone. In contrast, you start out with a more general model and then put hard monotonicity constraints on this ...) NOTES FROM AC Also quite relevant is Cohn et al. (2016), http://www.aclweb.org/anthology/N16-1102 .
Isn't your architecture also related to methods like the Stack LSTM, which similarly predicts a sequence of actions that modify or annotate an input? Do you think you lose anything by using a greedy alignment, in contrast to Rastogi et al. (2016), which also has hard monotonic attention but sums over all alignments?",3.0,3,"the comment highlights a potential issue with the wording that could reflect cognitive bias, which is relevant for ensuring the accuracy and objectivity of the claims made in the paper. however, the feedback is somewhat vague because it does not provide a specific suggestion for an alternative phrasing or detailed guidance on how to address the bias. the authors can gain some insights from this comment, but it does not fully address how they should improve the draft in this respect.",4,"the review point highlights a specific issue with the wording used by the authors, which is a common cognitive bias in the field. the reviewer suggests that this wording should be corrected to accurately reflect the experimental results. while the comment clearly identifies a specific area for improvement, it lacks details on how to reword the statement or how prevalent the issue is in the draft, making it mostly helpful but not fully comprehensive. the authors are guided to significantly improve the clarity and accuracy of their reporting, which is valuable feedback.",4,"the comment is mostly helpful because it points out a specific issue related to the interpretation of results in the paper and suggests that the authors should reconsider their wording regarding performance claims. however, it does not specify exactly how the authors should phrase their results, thus leaving some room for interpretation."
"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.",ACL_2017_699_review,"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc.
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).",4.0,4,"the comment is mostly helpful because it identifies a specific area of weakness in the manuscript, specifically regarding the need for discussions about the convergence of the proposed joint learning process involving rnn and copyrnn. the feedback is clear and suggests that more discussion is needed for readers to understand how stable points in the probabilistic metric space are obtained. this is crucial for the replicability of the results. however, the comment could be further enhanced by providing more specific guidance or suggestions on how to address this issue, making it fully comprehensive and impactful.",4,"the comment is mostly helpful because it points out a specific area where the paper could be improved: the need for discussion on the convergence of the proposed joint learning process. the comment provides actionable feedback by indicating that this discussion would help readers understand how stable points in probabilistic metric space are obtained, which is important for reproducibility. however, the comment could be more detailed by offering suggestions on what aspects of convergence should be discussed or how the authors might demonstrate this in their paper.",5,"the comment is highly helpful as it clearly identifies a key area where the paper can be improved. it suggests specific content that should be added to the discussion to aid reader understanding and reproducibility of the results. this provides the authors with a clear, actionable suggestion that can significantly enhance the quality and clarity of the paper."
"681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?",ACL_2017_818_review,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",4.0,4,"the review comment provides actionable feedback by pointing out specific areas where the paper could be improved. it suggests discussing results related to inferring knowledge on objects and advises consistency in terminology usage for model b across tables, which can help clarify the paper's findings. additionally, the reviewer prompts the authors to include a mention of objects when discussing verbs, likely to improve the comprehensiveness of the argument. however, while these suggestions are clear and actionable, the rationale could be expanded upon for greater depth. as it stands, it delivers mostly helpful guidance for the authors in improving their draft.",4,"the review point provides clear and actionable feedback by identifying specific weaknesses in the draft. it suggests discussing results for the task of inferring knowledge on objects and including results for model (b), which can guide the authors to improve clarity and consistency in presenting their results. furthermore, it highlights the use of inconsistent terminology in tables, encouraging uniform terminology for better understanding. it also raises a question about the omission of 'objects' in the discussion of 'latent in verbs'. however, the comment could be expanded with more detailed suggestions or examples to be fully comprehensive.",5,"the comment is highly helpful as it provides specific, actionable feedback on what the authors need to address. it suggests discussing results for a particular task, requests inclusion of additional results for a specifically mentioned model, and advises maintaining consistent terminology across tables, enhancing clarity and coherence in the paper."
- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -,ACL_2017_494_review,"- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes ""morph-fitting"", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of ""attract"" and ""repel"" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does ""looking"" - ""look"" + ""walk"" = ""walking""? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.",4.0,3,"the comment identifies a potential weakness by suggesting the inclusion of additional baselines such as character embeddings. however, it lacks depth and specific guidance on how these additions could be achieved or why they are important. while the authors can gain some insight from this feedback, it does not fully address their needs for improving the draft and would benefit from more detailed suggestions.",3,"the comment identifies a potential weakness by labeling the work as an extension of existing retrofitting approaches, which can help the authors in understanding a perceived lack of novelty. it also provides a suggestive improvement area by recommending the inclusion of additional baselines like character embeddings. however, the feedback is somewhat incomplete and lacks detailed guidance on how to implement the suggestion effectively. while it offers authors some insight into potential areas of improvement (novelty and comparison), further elaboration on why these elements are important or how to execute them could have made the feedback more beneficial.",4,"the comment is mostly helpful as it provides a clear suggestion for improvement by recommending the inclusion of additional baselines, such as character embeddings. while it could be more detailed in explaining why these additional baselines might be beneficial, it gives the authors a concrete action to enhance their paper."
"14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?",ARR_2022_202_review,"1. The write-up has many typos and some formulas/explanations are confusing.
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes.
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data.
4. More strong baselines should be included/discussed in the experiments.
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives.
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing.
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling?
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.”
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training?
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code?
12. Line 450: L_{pc} or L_{cp}| in Eq. 7?
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”?
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here?",4.0,4,"the comment is mostly helpful as it identifies a specific area for improvement regarding the comparison of baselines in table 3. it points out the absence of strong baselines for mcnc, which could be critical for the evaluation section. the suggestion to justify this omission provides clear guidance for authors; however, it could be expanded by suggesting which baselines to include or explaining the importance of these comparisons further. hence, it is mostly helpful, but not comprehensive enough to warrant a higher score.",4,"the review point identifies a specific weakness in the manuscript — the lack of comparison with strong baselines for the mcnc, which is an important aspect of evaluating the proposed method. it provides clear feedback and invites the authors to justify this omission, which is actionable as it prompts the authors to address a potential shortcoming in their evaluation or to include these comparisons in the revised draft. however, the feedback could be expanded by suggesting additional baselines or providing more context on why these specific baselines are important, which would elevate its comprehensiveness.",4,"the comment is mostly helpful, as it identifies a potential weakness in the manuscript regarding the comparison with strong baselines in table 3. it prompts the authors to either include these baselines or justify their absence, which can significantly improve the robustness and credibility of the results presented. however, while the suggestion is clear, it could be expanded to provide more specific guidance on what constitutes 'strong baselines' or how to justify their exclusion, which would make it even more actionable."
"2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't.",ARR_2022_223_review,"The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.
1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning.
2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation.
3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels?
4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation?
5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance. The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice.
6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is.
7. The majority of Model transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim.
8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like ""our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures""
1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks.
2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't.
3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts.
4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White.
5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained.
6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained.
7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy.
8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels.
9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt.
10. Line 074: This sentence is confusing. Perhaps something like ""Thus"" over ""Hence only""?
11. Line 165: Remove ""remedy,""",4.0,3,"the comment identifies a specific area for improvement by suggesting a distinction between different types of hard prompt work. however, it lacks depth and clarity as it does not provide specific guidance on how to implement this distinction, nor does it explain the significance of making this distinction in the context. while the authors can infer that addressing this point might enhance their draft, the feedback would benefit from additional detail or examples to be more helpful.",4,"the comment is mostly helpful as it provides a clear suggestion to distinguish between different types of hard prompt work in the manuscript. this distinction could help clarify the contribution and facilitating understanding. however, the comment could be expanded to include more detailed suggestions or examples on how to implement this distinction, making it more comprehensive and beneficial for the authors.",4,"the comment is mostly helpful as it provides a clear and actionable suggestion for distinguishing between different types of hard prompt work. while it specifies what the authors should consider, it could be expanded to provide more detail or context about how making this distinction would benefit the paper."
8.L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case? Minor Points:,NIPS_2017_53,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.",4.0,4,"the review point is mostly helpful as it provides clear feedback by demanding clarification on specific aspects of the bilinear layer implementation. it prompts the authors to differentiate their approach from previous methods, questions dimensionality, and suggests areas for clarification regarding the use of equation (3). while it effectively highlights areas of potential confusion and suggests a clearer explanation, it could be refined with more specific suggestions or examples to be fully comprehensive.",4,"the review point is mostly helpful as it provides clear and actionable feedback on specific sections of the paper that require clarification. the reviewer suggests explaining how the implemented bilinear layer differs from other approaches and raises detailed questions about the dimensionality and swapping of layers. while it prompts the authors to clarify and improve their draft, it could be more comprehensive by perhaps offering suggestions or examples of how to enhance these explanations, thus reaching a level 5 score.",4,"the comment is mostly helpful, as it points out specific areas where clarification is needed regarding the differences between the implemented bilinear layer and other approaches, as well as how it relates to other methods like the hadamard product and mcb approaches. it also questions if the compression of representations is still applicable. while actionable, the feedback could be further refined with more detailed suggestions or guidance on how to address these queries."
"- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",NIPS_2017_645,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",4.0,4,"the comment is mostly helpful as it identifies a specific weakness in the manuscript, which is the lack of comparison between the authors' method and other similar methods such as thresholded subspace clustering (tsc) and greedy subspace clustering by park. this differentiation is crucial for understanding the novelty and effectiveness of their approach. however, while the comment provides a clear area for improvement, it could be expanded by offering more specific guidance on how to implement these comparisons or what specific aspects to focus on in those comparisons to be fully comprehensive and impactful.",4,"the comment clearly identifies a specific area for improvement in the draft—namely, the need for the authors to compare their method with other similar methods like thresholded subspace clustering and greedy subspace clustering by park, which have similar computational efficiency and guarantees. this is a valid and actionable point that could strengthen the authors' argument and provide a more comprehensive evaluation of their approach. however, the comment can be expanded to suggest how the comparison should be made or what kind of analysis would be beneficial, which would make it even more helpful.",4,"the comment is mostly helpful. it provides clear feedback on the need to compare the proposed method with other relevant approaches in the literature. mentioning specific methods (such as tsc and greedy subspace clustering) helps guide the authors' improvement efforts, though the comment could be expanded by suggesting specific aspects of comparison or discussing the potential impact on the conclusions."
"4. Does the claim ""It can be seen from the table that our proposed modules improve in both accuracy and completeness"" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?",NIPS_2022_738,"W1) The paper states that ""In order to introduce epipolar constraints into attention-based feature matching while maintaining robustness to camera pose and calibration inaccuracies, we develop a Window-based Epipolar Transformer (WET), which matches reference pixels and source windows near the epipolar lines."" It claims that it introduces ""a window-based epipolar Transformer (WET) for enhancing patch-to-patch matching between the reference feature and corresponding windows near epipolar lines in source features"". To me, taking a window around the epipolar line into account seems like an approximation to estimating the uncertainty region around the epipolar lines caused by inaccuracies in calibration and camera pose and then searching within this region (see [Förstner & Wrobel, Photogrammetric Computer Vision, Springer 2016] for a detailed derivation of how to estimate uncertainties). Is it really valid to claim this part of the proposed approach as novel?
W2) I am not sure how significant the results on the DTU dataset are: a) The difference with respect to the best performing methods is less than 0.1 mm (see Tab. 1). Is the ground truth sufficiently accurate enough that such a small difference is actually noticeable / measurable or is the difference due to noise or randomness in the training process? b) Similarly, there is little difference between the results reported for the ablation study in Tab. 4. Does the claim ""It can be seen from the table that our proposed modules improve in both accuracy and completeness"" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?
W3) I am not sure what is novel about the ""novel geometric consistency loss (Geo Loss)"". Looking at Eq. 10, it seems to simply combine a standard reprojection error in an image with a loss on the depth difference. I don't see how Eq. 10 provides a combination of both losses.
W4) While the paper discusses prior work in Sec. 2, there is mostly no mentioning on how the paper under review is related to these existing works. In my opinion, a related work section should explain the relation of prior work to the proposed approach. This is missing.
W5) There are multiple parts in the paper that are unclear to me: a) What is C in line 106? The term does not seem to be introduced. b) How are the hyperparameters in Sec. 4.1 chosen? Is their choice critical? c) Why not include UniMVSNet in Fig. 5, given that UniMVSNet also claims to generate denser point clouds (as does the paper under review)? d) Why use only N=5 images for DTU and not all available ones? e) Why is Eq. 9 a reprojection error? Eq. 9 measures the depth difference as a scalar and no projection into the image is involved. I don't see how any projection is involved in this loss.
Overall, I think this is a solid paper that presents a well-engineered pipeline that represents the current state-of-the-art on a challenging benchmark. While I raised multiple concerns, most of them should be easy to address. E.g., I don't think that removing the novelty claim from W1 would make the paper weaker. The main exception is the ablation study, where I believe that the DTU dataset is too easy to provide meaningful comparisons (the relatively small differences might be explained by randomness in the training process.
The following minor comments did not affect my recommendation:
References are missing for Pytorch and the Adam optimizer.
Post-rebuttal comments
Thank you for the detailed answers. Here are my comments to the last reply:
Q: Relationship to prior work.
Thank you very much, this addresses my concern.
A: Fig. 5 is not used to claim our method achieves the best performance among all the methods in terms of completeness, it actually indicates that our proposed method could help reconstruct complete results while keeping high accuracy (Tab. 1) compared with our baseline network [7] and the most relevant method [3]. In that context, we not only consider the quality of completeness but also the relevance to our method to perform comparison in Fig. 5.
As I understand lines 228-236 in the paper, in particular ""The quantitative results of DTU evaluation set are summarized in Tab. 1, where Accuracy and Completeness are a pair of official evaluation metrics. Accuracy is the percentage of generated point clouds matched in the ground truth point clouds, while Completeness measures the opposite. Overall is the mean of Accuracy and Completeness. Compared with the other methods, our proposed method shows its capability for generating denser and more complete point clouds on textureless regions, which is visualized in Fig. 5."", the paper seems to claim that the proposed method generates denser point clouds. Maybe this could be clarified?
A: As a) nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation, b) the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets), c) the final results are the average across 22 test scans, we think that fewer errors could indicate better performance. However, your point about the accuracy of DTU GT is enlightening, and we think it's valuable future work.
This still does not address my concern. My question is whether the ground truth is accurate enough that we can be sure that the small differences between the different components really comes from improvements provided by adding components. In this context, stating that ""the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets)"" does not answer this question as, even though DTU has the most accurate GT, it might not be accurate enough to measure differences at this level of accuracy (0.05 mm difference). If the GT is not accurate enough to differentiate in the 0.05 mm range, then averaging over different test scans will not really help. That ""nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation"" does also not address this question. Since the paper claims improvements when using the different components and uses the results to validate the components, I do not think that answering the question whether the ground truth is accurate enough to make these claims in future work is really an option. I think it would be better to run the ablation study on a dataset where improvements can be measured more clearly.
Final rating
I am inclined to keep my original rating (""6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.""). I still like the good results on the Tanks & Temples dataset and believe that the proposed approach is technically sound. However, I do not find the authors' rebuttals particularly convincing and thus do not want to increase my rating. In particular, I still have concerns about the ablation study as I am not sure whether the ground truth of the DTU dataset is accurate enough that it makes sense to claim improvements if the difference is 0.05 mm or smaller. Since this only impacts the ablation study, it is also not a reason to decrease my rating.",4.0,3,"the review point raises a valid question about the claim regarding the results shown in the table, encouraging the authors to further validate or justify their claim. additionally, the suggestion to use another dataset for the ablation study introduces a constructive idea for enhancing the study's rigor. however, the feedback could be more detailed in explaining why the reviewer is uncertain about the claim's validity or how exactly the suggestion to use a different dataset might benefit the study. thus, while it is insightful, it lacks depth in guiding the authors to a full understanding of how to address the comment effectively.",4,"the comment is mostly helpful because it questions the validity of a claim made by the authors and suggests using another dataset for the ablation study. this provides clear and actionable feedback by encouraging the authors to verify their results on additional data and ensure their claims are well-supported. however, it could be expanded by providing more context or examples of how the current dataset may not be sufficient.",4,"the comment is mostly helpful as it challenges the authors' claim and suggests using another dataset for the ablation study to strengthen their findings. this provides clear guidance for further analysis, although it could be enhanced by specifying what the expected outcome or benefit of such a study might be."
- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,NIPS_2018_232,"- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",4.0,4,"the comment is mostly helpful as it offers specific feedback about the weaknesses in the abstract, pointing out the lack of information on the evaluation process and its outcomes. this guidance is valuable for authors to understand what specific elements are missing in their abstract. however, the comment could be expanded by providing more detailed suggestions on how the authors could improve this section, such as recommending specific metrics or evaluation results to include, or examples of language issues to address. therefore, it doesn't fully reach a 'highly helpful' level but provides clear and actionable feedback.",4,"the comment provides clear and actionable feedback by pinpointing a specific area for improvement: the need for a description of how the idea in the abstract was evaluated and what the outcomes were. this guidance is helpful as it directs the authors to enhance the clarity and completeness of their abstract. however, while the feedback is useful, it could be expanded further with suggestions on how to address these issues, which would make it fully comprehensive and impactful.",4,"the comment identifies a specific area in the abstract that needs improvement, namely the lack of description of the evaluation and outcome, which is actionable. however, it could be more detailed about which aspects of the language need addressing, making it mostly helpful."
"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].",8fLgt7PQza,"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].
2. In the experiment or appendix section, I did not clearly see the formulas for Sensitivity and Specificity, nor were there any corresponding references, which is quite confusing to me. Moreover, using Accuracy as a metric in cases of highly imbalanced labels is unreasonable. For instance, in the MIMIC-III Mortality Prediction task, the positive rate is 5.42%. If I predict that all patients will survive, I can still achieve an accuracy of 94.58%. Previous works, such as GraphCare [1], have adopted AUROC and AUPRC as evaluation metrics.
3. The article is overly long and filled with detailed content, making it easy for readers to miss important points.
- [1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024
- [2] MedRetriever: Target-driven interpretable health risk prediction via retrieving unstructured medical text. CIKM 2021
- [3] Biomedical knowledge graph-enhanced prompt generation for large language models. Arxiv 2023",4.0,4,"the review comment is mostly helpful as it clearly identifies weaknesses in the manuscript related to its incremental contribution and the omission of key citations and baseline algorithms. it suggests specific algorithms and works, like medretriever and kgrag, that the authors should consider citing or comparing against. while the comment offers clear guidance on areas to improve, it could be more comprehensive by providing suggestions on how to potentially enhance the original contribution or offering further details on how the missed citations affect the work. these additions could elevate the comment to the highest level of helpfulness.",4,"the review point provides clear and actionable feedback by highlighting two key weaknesses: the incremental nature of the article’s contribution and the omission of key baseline citations. it specifies the need for introducing essential rag algorithms relevant to the focus on ehr, such as medretriever and commonly used graphrag algorithms like kgrag. this information guides the authors on how to improve their draft by emphasizing the uniqueness and completeness of their research. however, the comment could be expanded to offer more detailed guidance on how the authors might address these issues, such as suggesting ways to enhance the contribution or further discussion on how to integrate these baselines into their work.",5,"the comment is highly helpful as it identifies the incremental nature of the contribution, suggesting that the work presented is not novel enough and merely combines existing techniques. moreover, it provides specific examples of key algorithms and baselines that should have been cited, hence giving authors actionable steps to incorporate relevant references and better position their work within the existing literature."
"1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification?",ARR_2022_356_review,"1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558).
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria?
1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation?
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant.
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching).",4.0,4,"the review point is mostly helpful because it clearly identifies a specific issue with the classification of extreme speech within the paper. the reviewer points out the difficulty in distinguishing between 'derogatory' and 'exclusionary' types of extreme speech, which is an important aspect of the research. the reviewer offers a specific example from the provided sample data, highlighting the need for clearer definitions or criteria. while the comment raises relevant questions about the annotations and the influence of local regulations, it could be enhanced with more detailed suggestions or guidance on how to address the ambiguity in classification. this feedback is actionable and valuable but could be further refined for maximum impact.",4,"the review comment is mostly helpful as it clearly points out the need for a more precise distinction between the classes of extreme speech, specifically between 'derogatory' and 'exclusionary' categories. it also raises an important question regarding the local regulation's role in annotations and its potential impact on zero-shot cross-country classification, suggesting areas where the authors can improve the clarity and robustness of their definitions and analysis. however, while the comment provides valuable insights, it could be further refined by offering specific suggestions on how to improve the differentiation and address the concerns raised, which would enhance its comprehensiveness.",5,"the comment is highly helpful as it thoroughly identifies a specific issue with the clarity of class distinctions in the paper. the reviewer highlights the difficulty in differentiation between derogatory and exclusionary extreme speech and uses a specific example from the sample data to illustrate this confusion. furthermore, the review point raises questions about the influence of local regulations on annotations, and its implications on zero-shot cross-country classification. these details provide concrete guidance for the authors to improve the clarity and consistency of their work."
"- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.",NIPS_2021_37,", * Typos/Comments)
Overall, I like and value the research topic and motivation of this paper and lean positive. However, some details are not clear enough. I would update my rating depending on the authors' feedback. The details are as follows.
+ Interesting and important research problem. This paper focuses on how to obtain disentangle representations for feature-level augmentation. This topic is interesting and important, and will attract many interests of the NeurIPS community.
+ Good quality of writing and organization. Overall, the writing quality is good and the paper is well organized. It is comfortable to read this paper, although some details are not clear.
+ Comprehensive experiments. Experiments are conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ).
- Relative difficulty score and generalized cross-entropy (GCE) loss. It is not clear how the relative difficulty score W ( x )
in Eq. (1) is used in the pipeline. W(x) is not mentioned again in both the overall objective functions Eq. (2) or Algorithm 1. Since readers may not be familiar with the generalized cross-entropy (GCE) loss, it is encouraged to briefly introduce the formulation and key points of the GCE loss to make this paper more self-contained.
- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.
- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.
- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?
- Figure 1 is not clear. First, it seems that the two y towards L CE
are the outputs of C i
, but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?
- Table 4 reported a much lower performance of ""swapping"" on BAR compared to the other three datasets. Is there any explanation for this, like the difference of datasets?
- Sensitivity to hyperparameters. The proposed framework consists of three important hyperparameters, ( λ dis , λ s w a p b , λ swap )
. It is not clear whether the framework is sensitive to these hyperparameters and how these hyperparameters are determined.
* (Suggestion) Illustration of backpropagation. As introduced in Line 167-168, the loss from C i
is not backpropagated to E b
. It would be clearer if this can be added in Figure 1.
* Line 280. Is ""the first row and column ... respectively"" a typo? It is a little confusing for me to understand this.
* Typos in Algorithm 1. Are λ dis and λ s w a p b
missed in L dis and L swap ?
* Typo in Line 209. Corrputed -> Corrupted.
============================= After rebuttal ===================================
After reading the authors' response to my questions and concerns, I would like to vote for acceptance.
The major strengths of this paper are:
The research problem, unbiased classification via learning debiased representation, is interesting and would attract the NeurIPS audience's attention.
The proposed method is simple but effective. The method is built on top of LfF [12] and further considers (1) intrinsic and bias feature disentanglement and (2) data augmentation by swapping the bias features among training samples.
The paper is clearly written and well organized.
These strengths and contributions are also pointed out by other colleague reviewers.
My main concerns were:
Unclear technical details of the GCE loss and the relative difficulty score. This concern was also shared with Reviewer 8Ai1 and iKKw. The authors' response clearly introduced the details and addressed my concern well.
Sensitivity to hyper-parameters. The authors' response provided adequate results to show the sensitivity to hyper-parameters. Other details of implementation and analysis of experimental results. The authors' responses clearly answered my questions.
Considering both strengths and the weakness, I am happy to accept this paper.
The authors have adequately addressed the limitations and potential negative societal impact of their work.",4.0,4,"the comment is mostly helpful as it points out a lack of clarity regarding how disentanglement is achieved in the study, which is a critical aspect that the authors should address to improve their work. by specifically mentioning the need to highlight how disentanglement is realized and guaranteed without certain bias types, it provides a clear direction for the authors to refine their manuscript. however, the comment could be expanded with more detailed suggestions on how to address this issue, which would make it even more impactful.",4,"the review point identifies a clear area of concern regarding the lack of clarity on how disentanglement is guaranteed in the study. it acknowledges that the authors have recognized disentanglement as a limitation in their 'broader impacts and limitations' section but emphasizes the importance of illustrating how disentanglement is achieved and ensured without bias. the comment provides specific guidance to the authors by suggesting the need for clarification on this aspect, making it mostly helpful. however, it does not offer specific suggestions on how the authors might address the issue, which would be needed for a score of 5.",4,the comment is mostly helpful as it identifies a specific potential weakness related to the clarity and implementation of 'disentanglement'. it suggests a focus on how disentanglement is achieved and urges clarification on the method's effectiveness and potential biases. this provides actionable feedback for the authors to improve their explanation or methodology in the paper.
"3. The ICL-HAR, while improving consistency and verifiability, has greatly impedes the accuracy scores (dropping from 70.4 to 55.6 on TRIP). This should be discussed or at least acknowledged in the main text in more detail.",Yz4VKLeZMG,"1. Generalizability: both fine-tuning and in-context learning strategies seem to be tailored for shifting model attention to a smaller chunk of key information, which is confirmed by the attention weight analysis. This makes me to worry to what extent this method could be generalized to other datasets where the information is not presented in contrastive pairs, and where the conflicting information is not restricted to one or two sentences but widely spread in the entire passage. For example, the task of identifying conflicting sentences might have over-simplified the reasoning task by taking the short-cut to ignore lots information, which just happens to be trivial in these specific datasets.
2. The developed strategies, while interesting, might just be marginally relevant to the cognitive process of heuristic / analytical dual passes of human reasoning. The heuristic reasoning process is more related to the information being utilized and the amount of attention paid to more fine-grained details. For instance, in online language comprehension, comprehenders might ignore fine-grained syntactic structured and rely on the semantic meaning of the words and their prior knowledge to interpret ""the hearty meal was devouring..."" as ""the hearty meal was devoured..."" (Kim and Osterhout, 2005). However, the heuristic process is less concerned with gratuity of final decision, as implied by the HAR model. The HAR framework breaks the reasoning tasks into multiple sub-tasks, where the gratuity of the decision gradually becomes finer-grained. This might be better characteristics as step-by-step chain of reasoning rather than heuristic decision-making.
3. The ICL-HAR, while improving consistency and verifiability, has greatly impedes the accuracy scores (dropping from 70.4 to 55.6 on TRIP). This should be discussed or at least acknowledged in the main text in more detail.",4.0,4,"the review point is mostly helpful as it clearly identifies a specific issue with the draft: the decrease in accuracy scores due to the use of icl-har. it advises the authors to discuss or acknowledge this issue in more detail in the main text, providing a clear and actionable direction for improvement. however, the feedback could be further enhanced by providing additional suggestions or context on how the authors might address the drop in accuracy or balance it with the benefits of improving consistency and verifiability.",4,"the review comment is mostly helpful as it provides clear and actionable feedback. it highlights a specific weakness in the paper regarding the impact of using icl-har on the accuracy scores and suggests that this issue should be discussed or acknowledged in more detail in the main text. while the comment could benefit from suggesting more detailed improvements or guidance on how the authors might address this issue, it still offers valuable insights that can help the authors enhance their draft.",4,"the comment is mostly helpful, as it clearly identifies a weakness (the decrease in accuracy scores) and suggests that it should be discussed or acknowledged in more detail in the main text. while it points out an important issue, it could be expanded with more specific suggestions on how the authors might address this problem."
- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?,ACL_2017_108_review,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?",5.0,3,"the comment addresses a few areas that could potentially improve the clarity of the draft, such as providing a short description of a multigraph and clarifying ambiguous terms like 'many previous' and 'this task.' however, the feedback is somewhat fragmented and does not offer detailed guidance in how the authors might address these issues. while the comment identifies these weaknesses, it does not thoroughly explore or provide comprehensive suggestions, leaving the authors with only a partial understanding of how to enhance these aspects of their draft.",4,"the review point is mostly helpful as it provides clear and actionable feedback on specific areas of the draft that may need clarification or revision. by suggesting that the notion of a multigraph should have a short description, it helps improve accessibility for readers who may not be familiar with the concept. it also identifies the awkward phrasing 'many previous,' prompting the authors to re-evaluate and possibly rephrase for clarity. lastly, it points out ambiguity with 'solving this task,' asking for specification of the task being referred to. however, the comment could be expanded with specific suggestions on how to implement these improvements, which would make it more comprehensive and impactful.",4,"the comment is mostly helpful as it provides clear and specific feedback on multiple aspects where the authors can improve clarity and specificity in their draft. it calls for providing a short description of 'multigraph' for reader comprehension, clarifies the ambiguous phrase 'many previous,' and indicates ambiguity in the 'solving this task' statement, prompting the authors to clarify which task they are referring to. however, it could have been more comprehensive by providing suggestions or examples on how to address these issues."
"- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.",ACL_2017_483_review,"- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was performed on.
General Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with ""sequence-to-sequence models"". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not enough well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be appropriate.
- 590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.
- 745: This could alternatively show that structural cues are less important for this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).
[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection."" EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016.",5.0,5,"the comment thoroughly identifies specific weaknesses in the presentation of the model, namely clarifying the pooling method and the ambiguity in equation (7). it offers detailed and actionable feedback by specifying line numbers and explaining the clarity required for both the pooling method and the interpretation of variables in the equation. additionally, it prompts the authors to consider the correctness of the equation as a conditional probability, which could significantly improve the draft's clarity and correctness. thus, it empowers the authors to make significant improvements with clear guidance.",4,"the review point identifies specific weaknesses in the presentation of the model, namely the lack of clarity in the pooling method for embedding features and the ambiguity in the definition and condition of equation (7). it suggests clear and actionable feedback by pointing out these areas, allowing authors to improve their work significantly. however, it could be expanded with more detailed suggestions or examples to achieve a thoroughly comprehensive impact, thus it's rated as mostly helpful.",5,"the comment thoroughly identifies specific weaknesses in the draft and provides detailed, actionable suggestions that empower the authors to significantly improve their draft. it clearly specifies the areas that need clarification, such as the pooling method and the interpretation of the equation, which will help the authors address these precise issues effectively."
"- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?",ARR_2022_112_review,"- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix.
L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about ""Type of Skills"" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of ""skill"", ""knowledge"" and ""attitude"" correspond to ""hard"" and ""soft"" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider ""skill"" and ""knowledge"" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: ""Pre-training"" and ""pretraining"" are not spelled consistently.",5.0,5,"the comment is highly helpful because it thoroughly identifies a specific area of weakness in the manuscript related to the description of expert traits and their justification for involvement in the annotation process. the reviewer asks targeted questions about the nature and necessity of involving experts, including their expertise and the potential differences in the annotation process if carried out by non-experts. this detailed and focused feedback empowers the authors to significantly improve their draft by addressing these points, providing clarity and justification for their methodological choices.",5,"the review point thoroughly identifies a potential weakness in the manuscript regarding the description and justification of the use of experts for annotation. it offers detailed and clear suggestions by asking specific questions about the nature of the experts (linguistic or domain), the justification for using experts over non-experts, and whether any unique linguistic challenges were involved in the annotation process. these questions prompt the authors to provide a more comprehensive explanation, which can significantly improve the clarity and justification of their methodology. the feedback is actionable and constructive, empowering the authors to enhance their draft effectively.",5,"the comment is highly helpful as it clearly identifies a gap in the description concerning the traits of the experts involved in the annotation process. it then offers detailed and actionable suggestions for how the authors can improve their draft. the reviewer asks specific questions about the nature of the experts, the necessity of expert annotation, the type of expertise required, and potential challenges. this provides the authors with a clear direction for refinement and enhancement of their paper."
"3. Experiments. Probably the biggest concern with the paper is with the experiments. The paper reports only self comparisons. The paper also doesn't explain why this is so, which adds to the poor motivation problem. In a generative setting comparisons with SketchRNN could be performed.",NIPS_2020_1371,"When reading the paper, I've got the impression that the paper is not finished with couple of key experiments missing. Some parts of the paper lack motivation. Terminology is sometimes unclear and ambiguous. 1. Terminology. The paper uses terms ""animation"", ""generative"", ""interpolation"". See contribution 1 in L40-42. While the paper reported some interpolation experiments, I haven't found any animation or generation experiments. It seems the authors equate interpolation and animation (Section 4.2) which is not correct. I consider animation is a physically plausible motion. Like a person opens a mouth, car moves, while interpolation is just warping one image into the other. Fig 7 shows exactly this with the end states being plausible states of the system. The authors should fix the ambiguity to avoid misunderstanding. The authors also don't report any generation results. Can I sample a random shape from the learnt distribution? If not the I don't think it's correct to say the model is generative. 2. Motivation. It's not clear why the problem is important from practical standpoint? Why one would like to interpolate between two icons? Motivation behind animation is more clear, but in my opinion, the paper doesn't do animation. I believe from a practical standpoint letting the user to input text and be able to generate an icon would also be important. Again, I have hard time understanding why shape autoencoding and interpolation is interesting. 3. Experiments. Probably the biggest concern with the paper is with the experiments. The paper reports only self comparisons. The paper also doesn't explain why this is so, which adds to the poor motivation problem. In a generative setting comparisons with SketchRNN could be performed.",5.0,4,"the comment is mostly helpful as it identifies a significant weakness in the paper related to the experiments, specifically the lack of comparative analysis with other methods like sketchrnn. it suggests an area for improvement by recommending comparisons with existing methods, which is actionable feedback. however, the comment could be more comprehensive by providing additional details or guidance on how to effectively carry out these comparisons, which would elevate the score to a 5.",4,"the comment provides clear feedback on a specific weakness in the paper's experiments, noting the lack of comparative analysis, such as with sketchrnn, that could strengthen the paper's findings. it also points out the issue with the paper's motivation for the current experiment setup. while the comment is mostly clear and actionable, suggesting a potential comparison method, it could be enhanced by providing more detail or specific guidance on how to improve the motivation and expand the experimental setup. overall, it offers a helpful and largely comprehensive critique that authors can act upon to improve their manuscript.",4,"the comment is mostly helpful, as it identifies a major weakness in the paper related to the experiments and suggests a potential action for improvement. it points out the lack of comparative analysis with other methods and recommends including comparisons with sketchrnn. however, the feedback could be further refined by providing more detailed guidance on how to integrate these suggestions or addressing any specific issues related to the paper's methodology or experiments."
"3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.",ACL_2017_588_review,"and the evaluation leaves some questions unanswered. - Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.
- Weaknesses: 1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.
2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.
3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.
4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.
- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?
2) Have you tried building a classifier that just takes h_i^e as inputs?
I have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.",5.0,3,"the comment identifies a key weakness in the description of the hierenc method, specifically regarding the averaging of representations. however, while the reviewer highlights the potential issue of noise introduction due to averaging, the feedback lacks depth and does not provide specific suggestions for how the authors can address this problem or improve their description. despite recognizing a critical issue, the comment does not fully empower the authors to make comprehensive improvements. thus, it is somewhat helpful.",5,"the review point is highly helpful as it clearly identifies a significant weakness in the description of hierenc and provides a detailed explanation of why the current approach may not be effective. the reviewer describes the issue with averaging representations for each context instantiation, highlighting the potential for noise that could arise from this method. such feedback gives the authors a clear understanding of the specific problem and implies the need for a more refined or different approach, thus empowering them to significantly improve their draft.",5,"the comment is highly helpful as it identifies a specific issue with the description of hierenc, providing clear reasoning as to why the approach may not be effective. the comment suggests that using the average of representations for all possible entities could introduce noise, which is a significant weakness that the authors should address. this feedback empowers the authors to reconsider and potentially revise their methodology to improve clarity and effectiveness."
"3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: “The may be attributed…” -> “This may be attributed…",ARR_2022_68_review,"1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data?
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail.
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback.
Moreover, Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers. 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8.
Line 277: “The may be attributed…” -> “This may be attributed…",5.0,4,"the review comment is mostly helpful as it provides a clear identification of weaknesses in the draft, specifically regarding the selection of baseline models for comparison. it highlights the omission of comparisons with relevant work by campos et al. (2020) and other domain adaptation methods, urging the authors to consider these for a more robust evaluation. additionally, it offers a corrective suggestion for a typographical error, which contributes to the clarity of the text. however, the comment could be further expanded with more specific guidance on how to incorporate these comparisons to enhance the draft's comprehensiveness.",4,"the review point provides a clear and actionable feedback by highlighting weaknesses in the choice of baseline models and the lack of comparison with relevant works, such as campos et al. (2020) and other domain adaptation methods cited in section 8. this specific guidance helps the authors understand areas for improvement in their experimental setup. additionally, there is a minor comment on a grammatical correction, which is helpful but not central to the improvement of the overall draft. the feedback is mostly helpful, as it gives the authors clear directions on how to enhance the rigor and completeness of their comparative analysis, although it could provide more detailed insights or further explanations for a higher score.",4,"the comment is mostly helpful as it identifies a specific weakness in the experimental setup (the use of weak baseline models) and provides actionable suggestions for improvement by recommending comparison to specific related work and domain adaptation methods. additionally, it offers a correction for a typographical error. however, it could be more detailed on why these comparisons are important, which would make it fully comprehensive."
"- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).",YRJDZYGmAZ,"Many significant problems are found in the current format of the paper that prevents the understanding of the concept. The problem includes but is not limited to confusing writing, inconsistency of notations, expressions of novelty, experimental presentation etc. It is highly recommended that the authors to re-write the paper, re-organize the content and better polish the text for the reader to better understand.
The major problems:
* Novelty is limited:
- The proposed method as in Sec. 3.2 is very similar to DAPL but extended to multi-source scenarios. The only difference is to introduce an additional [DOM]. Note, that DAPL has not been peer-reviewed.
- The motivation for domain-aware mixup is confusing. I cannot be convinced and do not understand in the current writing, how it can enforce to learn domain-specific knowledge. The corresponding literature regarding mixup in the feature space is also not referenced and discussed (e.g. [1]).
- The description for deriving the domain-aware mixup is confusing. I assume the authors are trying to develop a method so that the learned prompt shares the knowledge between source and target domains (depending on Eq. 9)?
* Writing:
- In the first sentence of Abstract: “large vision-language models … strong performance in MFDA”. There is no such reference applying large VL models in MFDA. In fact, MFDA is a rarely studied problem.
- The description of the problem setting (MFDA) should be clearly explained at the beginning (abstract or introduction) so that the reader can refer better to the limitations of prior works.
- Paragraphs 1 & 2 in the introduction: the connection is missing, and ‘prompt learning’ suddenly jumps in, making the concept broken.
- Fig. 1 is not referred to in the paper.
- Related work: after describing the related prior works of each field, it's suggested to write a couple of sentences to distinguish between them to show the novelty of the proposed method.
- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).
- There is significant text overlapping with DAPL in the preliminary sections of both papers (only with some rewording..). It should be strictly prohibited.
- What is [DOM] in Eq. 4? I assume it is a domain ID? And I assume [DOM] is the non-learnable component near the description of Eq. 4?
- Notation: what is subscript d in Eq. 4 and superscript d in Eq. 5? They are not explained in the text. I assume they are the domain IDs?
- What does it mean by ‘d*k categories’ as in the sentence after Eq. 5?
- Eq. 6 is very confusing. For the outer summation on d \in {s, u}, what is the purpose of computing the similarity between the target domain prompt and source image features? How does the learning on unlabeled target data is realized?
- What is inter-source domain mixup? In the current format of writing, I don’t understand why maintaining it will harm the representation learning on the target domain. The motivation is weak.
- In the second paragraph on page 6, the notation of target domain data y_t is different from Section 3.
- In Fig. 3, letters v and f are used to represent the features of “painting” and “real”. But v is used to represent text prompts as in Eq. 3
- The feature mix-up formulation in Fig. 3 is different than Eq. 8. One uses \gamma and another one uses \lambda? and the weighting is different?
- It is really confusing that the letter “t” is used to refer to text and target domain.
- What are D^s and D^u in Eq. 10? They are never defined. I assume they are source and target domains, which is inconsistent with what is described in the problem setting. The problem setting is borrowed from (Yue et al., 2021a). But Eq. 10 is copied from DAPL paper. Please keep everything consistent throughout the paper. Also, Eq. 9 requires source data as well, why only D^u is passed to L_u as in Eq. 10?
- The notations for loss functions in Eq. 7, 9, and 10 should be consistent.
- Table 5 in the last sentence of Page 8 should be Figure 5.
- The experimental setting/comparison is very confusing. What is “single best”, which can be both setting and method as in Table 1&2? What is source combined? Which rows in Tables 1&2 refer to the MFDA? How come the “Large model” in Table 1&2 can be the setting, it should be the model architecture.
- For Figure 6&7, they are hard to see the differences. It is suggested to use a table to report the numbers.
[1] Adversarial Domain Adaptation with Domain Mixup. AAAI 2020.",5.0,4,"the review point identifies a significant area of confusion within the method section regarding the description of the mfda setting. it compares the draft's description with a previous paper (yue et al., 2021a), highlighting discrepancies and raising important questions about the use of labeled and unlabeled data. this specific feedback will help authors understand the areas that are unclear and align their draft with existing literature, thus improving the clarity and accuracy of their paper. however, the comment could be more helpful if it provided specific suggestions on how to address the confusion, making it fully comprehensive. hence, it is mostly helpful but can be refined further.",4,"the review point identifies specific weaknesses in the description of the mfda setting, such as the lack of clarity regarding sparse labels, target distribution notation, and discrepancies with previous work by yue et al. (2021a). it requests clarification on whether unlabeled data in source domains is used during training and highlights inconsistencies with the referenced paper, providing clear guidance on areas that need improvement. however, the review point could be further refined by suggesting explicitly how the authors might clarify these inconsistencies, which would elevate it to a highly helpful comment.",4,"the comment is mostly helpful as it highlights a key area of confusion in the manuscript. it identifies specific issues with the description of the mfda setting and provides sufficient context by referencing the original paper for comparison. however, it could be more comprehensive by suggesting specific clarifications or adjustments to the paper's text, ensuring the authors address the noted inconsistencies."
"2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.",ICLR_2022_1794,"1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.
2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.
3 On the segmentation mask involved with cancer on CSAW-S, the segmentation results of DEEPLAB3-DEIT-S cannot be concluded as better than DEEPLAB3-RESNET50. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance.
Questions: 1 For the grid search of learning rate, is it done on the validation set?
Minor problems: 1 The n number for Camelyon dataset in Table 1 is not consistent with the descriptions in the text in Page 4.",5.0,5,"the review point is highly helpful as it provides a thorough identification of a specific area where the authors may have made an incorrect conclusion regarding their results. the reviewer not only challenges the authors' viewpoint with a clear counterargument, backed by specific examples from the data presented in figure 3 but also encourages further analysis and commentary to improve the manuscript. this detailed and constructive feedback empowers the authors to critically reconsider their interpretation of model performance among different architectures and make corresponding improvements to their draft. moreover, the suggestion to add analysis on performance trending adds actionable guidance for enhancement.",4,"the review point provides mostly helpful feedback by clearly addressing the authors' claim regarding the performance trending with increased model capacity in vits and cnns. the reviewer disputes the authors' viewpoint with specific reference to the models and datasets in figure 3, offering evidence that challenges the claim. although the feedback is specific and actionable, suggesting further analysis and comments, it could be expanded with more detailed suggestions on what additional analysis should entail to strengthen it further.",4,"the comment is mostly helpful as it provides clear and actionable feedback regarding the need for more analysis and commentary on the performance trends. the reviewer disagrees with the authors' viewpoint and gives specific examples from figure 3 to support their argument, offering the authors an opportunity to revisit their analysis. however, it could be more comprehensive if it included suggestions for how the authors might address these concerns or improve the clarity of their argument."
"2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work ""Targeting Ultimate Accuracy: Face Recognition via Deep Embedding"", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.",NIPS_2016_208,"1. The novelty is a little weak. It is not clear what's the significant difference and advantage compared to NCA [6] and ""Small codes and large image databases for recognition"", A. Torralba et al., 2008, which used NCA in deep learning. 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work ""Targeting Ultimate Accuracy: Face Recognition via Deep Embedding"", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.",5.0,4,"the comment is mostly helpful as it clearly identifies a weakness in the paper, specifically the omission of relevant state-of-the-art references that use a similar methodology. it provides an actionable suggestion by mentioning a specific study (baidu's work) that should be cited or discussed to enhance the paper's comparison. additionally, it points out a specific result that outperforms the one in the current draft, which is pertinent for authors to consider. however, the comment could be refined by offering more specific guidance on how to incorporate this information into the draft or by suggesting other specific gaps to address, which would make it fully comprehensive and warrant a higher score.",4,"the comment is mostly helpful as it identifies a clear weakness in the paper by pointing out that some state-of-the-art references, like baidu's work, are missing in the experiment of face recognition. it provides specific details about the missing reference, including the method used and the results achieved, which are relevant for the authors to consider. this feedback gives actionable insights for the authors to improve their draft by comparing their results with state-of-the-art methods. however, it could be expanded by suggesting how the inclusion of this reference and comparison might impact the authors’ conclusions or methodology, which would enhance its comprehensiveness.",4,"the comment is mostly helpful because it identifies a specific area where the paper can be improved by referencing an important related work. it provides a clear example of the missing reference and suggests the inclusion of this work for a more comprehensive discussion. however, it could be more helpful by explaining the significance of the comparison further or by suggesting how the authors can incorporate this information effectively."
"- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.",ACL_2017_726_review,"- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it. General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of “near-state-of-the-art accuracies” on ATIS and GeoQuery, which doesn’t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.
- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.
- Table 6: what is the breakdown of the score by correctness and incompleteness?
What % of incompleteness do these queries exhibit?
- What is expertise required from crowd-workers who produce the correct SQL queries? - It would be helpful to see some analysis of the 48% of user questions which could not be generated.
- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages. - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?
I thank the authors for their response.",5.0,3,"the comment identifies an area that requires clarification, specifically concerning the splits used to obtain the atis numbers in table 4. however, it lacks depth and detail since it does not provide any specific suggestions or guidance on how the authors can clarify this information in the table. while the authors can gain some insight into what needs addressing, the feedback is somewhat helpful but not fully comprehensive.",3,"the comment is somewhat helpful as it identifies a specific area in the document (table 4) that requires clarification regarding the splits used for the atis numbers. however, it lacks depth in providing detailed suggestions or guidance on how the authors could make this clarification. while it helps bring attention to an area needing improvement, it does not thoroughly address the issue or suggest a comprehensive action plan for the authors.",4,"the comment is mostly helpful, as it identifies a specific area where the authors can improve the draft by clarifying the splits used for obtaining the atis numbers. it provides clear and actionable feedback, although it could be expanded to provide more depth or context on why this clarification is needed."
"2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly",ICLR_2023_977,"the evaluation section has 2 experiments, but only 2 very insightful detailed examples. The paper can use a few more examples to illustrate more differences of the output sequences. This would allow the reader to internalize how the non-monotonicity in a deeper way.
Questions: In details, how does the decoding algorithm actually avoid repetitions? In other way, how does other models actually degrade validation perplexity using their decoding algorithm?
Typos, Grammar, etc.: Page 7, section 4.2, par. 2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly",5.0,3,"the comment identifies two specific issues with the document's references to tables and figures, which provides the authors with concrete items to correct. however, it lacks additional context or suggestions beyond simply pointing out the misdirected callouts. including more guidance on how these inconsistencies could impact the understanding of the document or suggesting a thorough check of all references might have made it more impactful.",3,"the comment identifies a specific mistake related to the incorrect referencing of tables and figures, which is somewhat helpful for the authors. correcting such errors is important for enhancing the clarity and accuracy of the visual references within the paper. however, the comment could be expanded with additional guidance on any potential implications this misreference might have had on the interpretation or understanding of the related content. the feedback, while valuable, lacks depth and is not comprehensive enough to address broader aspects of the draft's improvement.",5,"the comment is highly helpful as it clearly identifies specific issues with the callouts in the document and provides precise instructions on how to correct them, allowing the authors to significantly improve the accuracy and clarity of the paper."
"- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:",ACL_2017_31_review,"] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.
- No detailed error analysis is provided.
- A feature comparison with prior work is shallow, missing two relevant papers.
- The paper has several obscure descriptions, including typos.
[General Discussion:] The paper would be more impactful if it states novelties more explicitly. Is the paper presenting the first neural network based approach for event factuality identification? If this is the case, please state that.
The paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of Table 3 and 4. What are dominant sources of errors made by the best system BiLSTM+CNN(Att)? What impacts do errors in basic factor extraction (Table 3) have on the overall performance of factuality identification (Table 4)? The analysis presented in Section 5.4 is more like a feature ablation study to show how useful some additional features are.
The paper would be stronger if it compares with prior work in terms of features. Does the paper use any new features which have not been explored before? In other words, it is unclear whether main advantages of the proposed system come purely from deep learning, or from a combination of neural networks and some new unexplored features. As for feature comparison, the paper is missing two relevant papers: - Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.
- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.
Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.
The paper would be more understandable if more examples are given to illustrate the underspecified modality (U) and the underspecified polarity (u). There are two reasons for that. First, the definition of 'underspecified' is relatively unintuitive as compared to other classes such as 'probable' or 'positive'.
Second, the examples would be more helpful to understand the difficulties of Uu detection reported in line 690-697. Among the seven examples (S1-S7), only S7 corresponds to Uu, and its explanation is quite limited to illustrate the difficulties.
A minor comment is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by: - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*; - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.
- The title of Section 3 ('Baseline') is misleading. A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.
- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.
- Table 2 seems to show factuality statistics only for all sources. The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.
- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.
- Section 4.1 says, ""Aux_Words can describe the *syntactic* structures of sentences,"" whereas section 5.4 says, ""they (auxiliary words) can reflect the *pragmatic* structures of sentences."" These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.
- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6. What is the auxiliary word for 'ensure' in S7?
- Line 162: 'event go in S1' should be 'event go in S2'.
- Line 315: 'in details' should be 'in detail'.
- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.
- Line 771: 'recent researches' should be 'recent research' or 'recent studies'. 'Research' is an uncountable noun.
- Line 903: 'Factbank' should be 'FactBank'.",5.0,4,"the review comment identifies a specific weakness in the organization of section 3.2, pointing out that the explanations for different features are intertwined and confusing. it provides a clear and actionable suggestion to improve coherence by separating the paragraphs for lexical and sentence-level features. while the suggestion is clear and actionable, offering a more detailed explanation or example of how to execute this separation could increase its comprehensiveness and elevate the comment to a score of 5.",4,"the review point is mostly helpful as it clearly identifies the issue of confusion in the explanations for features in section 3.2 due to intertwined descriptions. it provides an actionable suggestion for improvement by recommending the use of separate paragraphs for lexical features and sentence-level features. however, the feedback could be more comprehensive by offering additional details on how to effectively restructure or examples of possible organization strategies, making it not fully comprehensive but still significant for authors.",4,"the comment is mostly helpful as it clearly identifies an issue with the organization and clarity of the explanations in section 3.2 and provides a specific suggestion for improvement by recommending separate paragraphs for different feature types. however, it could be more comprehensive by including examples of the confusing intertwined explanations."
"- In figure 5, the y-axis label may use ""Exact Match ratio"" directly.",ARR_2022_113_review,"The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases.
- Figure 2, it is not clear about ""merge target"". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, ""including that it optimizes for the wrong objective"". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work. - In figure 5, the y-axis label may use ""Exact Match ratio"" directly.
- Line 409, could you cite the ""R2"" metric?
- Appendix A, the authors state ""better model score cannot result in better hypothesis"". You'd better state clearly what idea hypothesis you want. "" a near-optimal model score"" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph?",5.0,2,"the comment identifies a potential improvement area regarding labeling in figure 5, but it is vague and lacks clarity. it suggests using 'exact match ratio' directly as a label, but does not provide sufficient context or rationale for why this change is beneficial or necessary. the feedback is minimal and provides little actionable guidance for the authors, making it only slightly beneficial.",2,"the review comment identifies an area for improvement by suggesting a clearer label for the y-axis in figure 5. however, it lacks detailed explanation or reasoning as to why 'exact match ratio' would be a better label, making it somewhat vague. while it could offer some benefit by suggesting a clearer label, it does not provide enough guidance to be considered more than barely helpful.",4,"the comment is mostly helpful, as it provides a clear and specific suggestion to improve the clarity and accuracy of figure 5's y-axis label. this would guide the authors to make precise edits that enhance the presentation of their data."
"8: s/expensive approaches2) allows/expensive approaches,2) allows/ p.8: s/estimates3) is/estimates, and3) is/ In the references: Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers. Dusenberry et al. (2020) was published in ICML 2020 Osawa et al. (2019) was published in NeurIPS 2019 Swiatkowski et al. (2020) was published in ICML 2020 p. 13, supplement, Fig.",ICLR_2021_872,"The authors push on the idea of scalable approximate inference, yet the largest experiment shown is on CIFAR-10. Given this focus on scalability, and the experiments in recent literature in this space, I think experiments on ImageNet would greatly strengthen the paper (though I sympathize with the idea that this can a high bar from a resources standpoint).
As I noted down below, the experiments currently lack results for the standard variational BNN with mean-field Gaussians. More generally, I think it would be great to include the remaining models from Ovadia et al. (2019). More recent results from ICML could also useful to include (as referenced in the related works sections). Recommendation
Overall, I believe this is a good paper, but the current lack of experiments on a dataset larger than CIFAR-10, while also focusing on scalability, make it somewhat difficult to fully recommend acceptance. Therefore, I am currently recommending marginal acceptance for this paper.
Additional comments
p. 5-7: Including tables of results for each experiment (containing NLL, ECE, accuracy, etc.) in the main text would be helpful to more easily assess
p. 7: For the MNIST experiments, in Ovadia et al. (2019) they found that variational BNNs (SVI) outperformed all other methods (including deep ensembles) on all shifted and OOD experiments. How does your proposed method compare? I think this would be an interesting experiment to include, especially since the consensus in Ovadia et al. (2019) (and other related literature) is that full variational BNNs are quite promising but generally methodologically difficult to scale to large problems, with relative performance degrading even on CIFAR-10. Minor
p. 6: In the phrase ""for 'in-between' uncertainty"", the first quotation mark on 'in-between' needs to be the forward mark rather than the backward mark (i.e., ‘ i n − b e t w e e n ′ ).
p. 7: s/out of sitribution/out of distribution/
p. 8: s/expensive approaches 2) allows/expensive approaches, 2) allows/
p. 8: s/estimates 3) is/estimates, and 3) is/
In the references:
Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers.
Dusenberry et al. (2020) was published in ICML 2020
Osawa et al. (2019) was published in NeurIPS 2019
Swiatkowski et al. (2020) was published in ICML 2020
p. 13, supplement, Fig. 5: error bar regions should be upper and lowered bounded by [0, 1] for accuracy.
p. 13, Table 2: Splitting this into two tables, one for MNIST and one for CIFAR-10, would be easier to read.",5.0,3,"the review point highlights formatting and reference issues, such as capitalization errors and incorrect citation details, which should be addressed to improve the professionalism and accuracy of the manuscript. however, these comments are somewhat helpful as they primarily address editorial issues rather than substantive content deficiencies. the feedback provides specific examples (e.g., capitalization errors and publication locations), allowing the authors to correct these errors. despite this, the comment lacks depth regarding the manuscript's content, limiting its impact on improving the overall quality of the paper beyond surface-level edits.",2,"the review point is primarily focused on typographical and formatting issues in the authors' references and text, such as capitalization and correct publication assignments. while these corrections are valid and necessary for improving the manuscript's professionalism and accuracy, the comments are relatively minor. they do not address substantial content, methodology, or interpretive aspects of the paper that would significantly enhance the authors' ability to improve their draft overall. therefore, the review does provide some useful feedback, but it is minimal in scope, which aligns with a score of 2 for barely helpful.",5,"the comment is highly helpful as it provides detailed and explicit feedback on multiple formatting errors within the text and references. the reviewer identifies specific issues, including incorrect punctuation and capitalization, as well as missing publication details in the references. this gives the authors a clear set of actions to improve the presentation and accuracy of their manuscript."
"2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments:",NIPS_2019_1131,"1. There is no discussion on the choice of ""proximity"" and the nature of the task. On the proposed tasks, proximity on the fingertip Cartesian positions is strongly correlated with proximity in the solution space. However, this relationship doesn't hold for certain tasks. For example, in a complicated maze, two nearby positions in the Euclidean metric can be very far in the actual path. For robotic tasks with various obstacles and collisions, similar results apply. The paper would be better if it analyzes what tasks have reasonable proximity metrics, and demostrate failure on those that don't. 2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments: 1. The diversity term, defined as the facility location function, is undirected and history-invariant. Thus it shouldn't be called ""curiosity"", since curiosity only works on novel experiences. Please use a different name. 2. The curves in Figure 3 (a) are suspiciously cut at Epoch = 50, after which the baseline methods seem to catch up and perhaps surpass CHER. Perhaps this should be explained.",5.0,4,"the comment identifies specific areas where the draft could be improved, such as missing analyses for parameters like σ in the rbf kernel and η and τ in equation (7). it references particular figures and equations, providing authors with clear guidance on where to focus their improvements. however, the feedback could be considered mostly helpful rather than highly helpful because it does not offer detailed suggestions on how to conduct the ablation studies or analyze these parameters, leaving some room for refinement in providing more comprehensive guidance.",4,"the review point identifies specific weaknesses in the paper, such as the missing ablation study, which could lead to confusion for practitioners. it provides clear and actionable feedback on areas needing improvement, such as analyzing the role of the sigma in the rbf kernel and the effects of eta and tau in equation (7) on performance. while it is mostly helpful, offering detailed guidance, it could be expanded with more concrete suggestions on how to conduct these analyses.",4,"the comment is mostly helpful as it provides specific guidance on areas where the paper can be improved, particularly regarding the inclusion of an ablation study. it mentions the importance of analyzing the role of certain parameters in the experiments, such as \\sigma in the rbf kernel, and suggests additional analysis for parameters \\eta and \\tau in equation (7). however, the comment could be enhanced by including more concrete suggestions or additional context on how these changes can improve understanding or performance, making it more actionable for the authors."
"781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",ACL_2017_818_review,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",5.0,4,"the review point addresses two specific issues in the manuscript: an antecedent that is missing for 'both tasks' and reference formatting errors. the feedback is clear and actionable, particularly regarding the need to check references for format and bibliographic details. however, it could be improved by providing more guidance on the specific location or section where the antecedent is missing and offering examples of correct reference formatting. despite these minor gaps, the comment is mostly helpful in guiding authors to make necessary corrections.",4,"the comment provides actionable feedback in pointing out specific weaknesses, such as the missing antecedent for 'both tasks' and suggesting specific areas for improvement, such as referencing format and bibliographic details check. however, it could be expanded with examples or suggestions on how to address the missing antecedent to make it more comprehensive. overall, it gives clear direction in areas that need correction, making it mostly helpful to the authors.",5,"the comment is fully helpful, as it tells the authors exactly what they should do. it specifically identifies the issues regarding the antecedent and reference formats, providing clear guidance on how to correct these issues and improve the overall quality of the draft."

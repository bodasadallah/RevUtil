Aspect: verifiability_extraction
Review Point: 1. The Methodology section is very hard to follow. The model architecture description is rather confusing and sometimes uses inconsistent notation. For example, Section 2.2 introduces $v^p_{t-1}$ in the description which does not appear in the equations. Some of the notation pertaining to the labels ($l_0$, $l_{t-1}$) initially gives the impression that a sequence of tokens are being generated as the label, which is not the case.

Rationale: The review point contains several claims and judgments about the Methodology section. It states that the section is "very hard to follow" and that the model architecture description is "confusing" and uses "inconsistent notation." These are subjective assessments that require justification or evidence. Additionally, the reviewer provides specific examples, such as the introduction of $v^p_{t-1}$ and the confusion regarding the notation for labels, which further supports the claims made. These elements indicate that the review point is not merely factual but contains claims that could help the authors improve their draft by addressing the identified issues.


Aspect: grounding_specificity
Review Point: - Regarding the proposed upweighing and KNN methods: For the majority of language and score combinations (see Figure 3), the impact that the methods have on idiomatic vs random data is similar; hence the proposed MT modelling methods seem far from idiom-specific. Therefore, the results simply appear to indicate that "better NMT systems are also better at idiomatic translations".

Rationale: The comment refers to the proposed upweighing and KNN methods and mentions Figure 3, which helps the authors identify the specific part of the paper being addressed, making it fully grounded. The comment also specifies the issue by stating that the impact of the methods on idiomatic versus random data is similar, suggesting that the methods are not idiom-specific. This provides clear guidance on what needs to be addressed in the paper. Therefore, the comment is fully grounded and specific, aligning with category 5.


Aspect: actionability
Review Point: - This method seems to only work for generative models that can be fine-tuned as an in/outpainting model.

Rationale: The review point makes an observation about the method, stating that it seems to only work for generative models that can be fine-tuned as an in/outpainting model. However, it does not provide any explicit or implicit suggestions for the authors to take action on. There is no guidance on how this observation should influence the authors' work or any recommendations for improvement. As a result, the comment lacks actionability, leaving the authors without direction on how to apply this information to enhance their draft. Therefore, it is rated as a 1 on the actionability scale.


Aspect: helpfulness
Review Point: 2. The presentation of this paper is hard to follow for the reviewer.

Rationale: The comment indicates that the presentation of the paper is difficult to follow, which is a critical issue that could hinder the paper's accessibility and comprehension. However, it lacks specificity and does not provide any guidance on which parts of the paper are confusing or how the authors might improve the clarity and organization of their presentation. Without actionable advice or specific examples, the authors are left without a clear path to address the issue, making the feedback barely helpful. Therefore, it aligns with a score of 2.


Aspect: verifiability
Review Point: - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph?

Rationale: The review point claims that the proposed method reduces computation time by limiting the search space to ancestral graphs, which results in less information compared to the richer search space of DAGs as used in [10]. This claim is supported by logical reasoning, explaining the trade-off between computation time and information richness. However, the comment lacks specific examples or references to quantify the information loss or to provide a detailed comparison with [10]. While the reasoning is clear, the absence of concrete data or references makes the claim somewhat verifiable, as it requires further evidence to fully substantiate the argument. Therefore, the comment aligns with a label of 3: Somewhat Verifiable.



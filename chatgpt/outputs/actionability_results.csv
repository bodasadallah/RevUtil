review_point,paper_id,focused_review,actionability_label,chatgpt_actionability_definitions_incontext_learning_score,chatgpt_actionability_definitions_incontext_learning_rationale
"1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021)",ARR_2022_233_review,"Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released.
1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them. First of all, will the author release the dataset or will it remain private?
Are the guidelines used to train the annotators publicly available?
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines.
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported.
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available?
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances.",1.0,3,"The comment is somewhat actionable. It explicitly suggests adding a reference to a specific paper, which is a clear action. However, it does not provide further context or explanation on how this reference should be integrated into the draft, making it somewhat vague."
"- Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.",ICLR_2022_2196,"weakness] Modeling:
The rewards are designed based on a discriminator. As we know, generative adversarial networks are not easy to train since generative networks and discriminative networks are trained alternatively. In the proposed method, the policy network and the discriminator are trained alternatively. I doubt if it is easy to train the model. I would like to see the training curves for rewards value.
The detailed alignment function used in Eq. (1) and Eq. (3) need to be provided.
Experiment: - The results are not satisfying. In the experiment, the generation quality of the proposed method is not good as traditional generative networks in terms of FID. In the image parsing part, the results are far behind the compared methods. - Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.",1.0,2,"The comment is barely actionable. It implies that the results should be made comparable to existing methods, but it does not provide any concrete guidance on how to achieve this. The authors would need to infer the action and figure out how to implement it on their own."
"2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",ARR_2022_12_review,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",1.0,1,The comment is unactionable. It poses questions about interpreting results in Table 3 but does not provide any guidance or suggestions on how to address these interpretations. The authors are left without a clear action to improve their draft based on this comment.
"- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).",ICLR_2022_3352,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.",1.0,1,"The comment is unactionable. It vaguely mentions that there are places that may cause confusion but does not specify what those places are or how to address them. Without further details or specific examples, the authors do not know what they should do to improve their draft."
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.,NIPS_2017_217,"Weakness:
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
- Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4. Comments:
The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.",1.0,1,"The comment is unactionable. It points out that the paper is incremental with respect to a previous work, but it does not provide any guidance or suggestions on how the authors can improve their draft. There is no explicit or implicit action stated."
"- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?",ARR_2022_23_review,"The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?",1.0,2,"The comment is borderline actionable. It implicitly suggests that the authors should clarify why only 10 answers are selected and how this affects performance estimation. However, the action is vague, as it does not provide guidance on how to address these questions in the draft."
1. Symbols are a little bit complicated and takes a lot of time to understand.,NIPS_2018_461,"1. Symbols are a little bit complicated and takes a lot of time to understand. 2. The author should probably focus more on the proposed problem and framework, instead of spending much space on the applications. 3. No conclusion section Generally I think this paper is good, but my main concern is the originality. If this paper appears a couple years ago, I would think that using meta-learning to solve problems is a creative idea. However, for now, there are many works using meta-learning to solve a variety of tasks, such as in active learning and reinforcement learning. Hence, this paper seems not very exciting. Nevertheless, deciding the number of clusters and selecting good clustering algorithms are still useful. Quality: 4 of 5 Clarity: 3 of 5 Originality: 2 of 5 Significance: 4 of 5 Typo: Line 240 & 257: Figure 5 should be Figure 3.",1.0,2,"The comment is barely actionable. It provides an implicit suggestion that the symbols are complicated, implying that they should be simplified or clarified. However, it does not provide concrete guidance on how to achieve this, leaving the authors uncertain about how to address the issue."
- A number of claims from this paper would benefit from more in-depth analysis.,ARR_2022_232_review,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed.
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements. In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset? Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption.",1.0,3,"The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are."
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.,ACL_2017_818_review,"- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations. What are some interesting action verbs and corresponding attributes relations? The paper also lacks analysis/discussion on what kind of mistakes their model makes.
- The number of object pairs (3656) in the dataset is very small. How many distinct object categories are there? How scalable is this approach to larger number of object pairs?
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.
General Discussion/Suggestions: - The authors should discuss the following work and compare against mining attributes/attribute distributions directly and then getting a comparative measure. What are the advantages offered by the proposed method compared to a more direct approach?
Extraction and approximation of numerical attributes from the Web Dmitry Davidov, Ari Rappoport ACL 2010 Minor typos: 1. In the abstract (line 026), the authors mention 'six' dimensions, but in the paper, there is only five.
2. line 248: Above --> The above 3. line 421: object first --> first 4. line 654: more skimp --> a smaller 5. line 729: selctional --> selectional",1.0,2,"The comment is borderline actionable. It implies that the authors need to clarify how the frame similarity factors and attributes similarity factors are selected, but it does not provide concrete guidance on how to address this issue. The action is implicit and vague, leaving the authors uncertain about how to proceed."
"2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.",NIPS_2022_728,"Weakness 1. The setup of capturing strategy is complicated and is not easy for applications in real life. To initialize the canonical space, the first stage is to capture the static state using a moving camera. Then to model motions, the second stage is to capture dynamic states using a few (4) fixed cameras. Such a 2-stage capturing is not straightforward. 2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively. 3. The quantitative experiments (Tab. 2 and Tab. 3) show that the fidelity of rendered results highly depends on the 2-stage training strategy. In a general capturing case, other methods can obtain more accurate rendered images. Oppositely, Tab. 2 shows that it is not easy to fuse the designed 2-stage training strategy into current mainstream frameworks, such as D-NeRF, Nerfies and HyperNeRF. It verifies that the 2-stages training strategy is not a general design for dynamic NeRF.
[1] Innmann, Matthias, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, and Marc Stamminger. ""Volumedeform: Real-time volumetric non-rigid reconstruction."" In European conference on computer vision (ECCV), pp. 362-379. Springer, Cham, 2016.",1.0,1,"The comment is unactionable. It merely states that the idea of using a volumetric representation in the deformation field is not novel and references a previous work, VolumeDeform. However, it does not provide any guidance or suggestions on how the authors could improve or differentiate their work."
- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.,ARR_2022_317_review,"- Lack of novelty: - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.
- Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER.
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives.",1.0,1,The comment is unactionable. It points out a lack of novelty in the work but does not provide any actionable suggestions or directions for improvement. It merely states an observation without guiding the authors on how to address the issue or enhance the novelty of their work.
"- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.",RnYd44LR2v,"- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.
- A central aspect of evaluating adversarial robustness is the attacks used to measure it. In the paper, this is described with sufficient details only in the appendix. In particular for the non $\ell_p$-threat models I think it would be important to discuss the strength (e.g. number of iterations) of the attacks used, since these are not widely explored in prior works.
[A] https://arxiv.org/abs/1908.08016
[B] https://arxiv.org/abs/2105.12508",1.0,3,"The comment is somewhat actionable. It provides some context about prior works and mentions specific datasets and studies, which could guide the authors to compare or contrast their work with these studies. However, it does not explicitly state what action the authors should take or how they should incorporate this information into their draft."
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?,ARR_2022_93_review,"1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings.
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section.
3. Lack of screenshots of the experimental interface
Comments: 1. Please add screenshots of the interface that was designed.
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2?
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked.
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model?
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020.
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122.
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13).
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928.
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087.",1.0,3,"The comment is somewhat actionable. It implicitly suggests that the authors should provide an explanation for the average duration reported in Table 1 and clarify whether it includes the time spent by the user waiting for the model to generate a response. However, it does not provide concrete guidance on how to address this issue."
- The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning,NIPS_2019_1366,"Weakness: - Although the method discussed by the paper can be applied in general MDP, the paper is limited in navigation problems. Combining RL and planning has already been discussed in PRM-RL~[1]. It would be interesting whether we can apply such algorithms in more general tasks. - The paper has shown that pure RL algorithm (HER) failed to generalize to distance goals but the paper doesn't discuss why it failed and why planning can solve the problem that HER can't solve. Ideally, if the neural networks are large enough and are trained with enough time, Q-Learning should converge to not so bad policy. It will be better if the authors can discuss the advantages of planning over pure Q-learning. - The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning",1.0,2,"The comment is barely actionable. It provides an implicit suggestion that the authors should consider the time complexity related to the reply buffer size. However, it does not offer concrete guidance on how to address or mitigate this issue, leaving the authors uncertain about the specific actions to take."
1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.,5UW6Mivj9M,"1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.
2) Relatedly, it was hard to discern what was novel in the paper and what had already been tried by others.
3) Since the improvement in numbers is not large (in most cases, just a couple of points), it is hard to tell if this improvement is statistically significant and if it translates to qualitative improvements in performance.",1.0,2,"The comment is barely actionable. It provides an implicit action by indicating that the paper is difficult to follow, particularly regarding the experimental procedures and evaluations. However, it does not specify how the authors should improve the clarity or what specific aspects need clarification, leaving the action vague."
- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].,NIPS_2017_217,"- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].
- ""Embedding"" is an overloaded word for a scalar value that represents object ID.
- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.
- Keypoint detection results should be included in the experiments section.
- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?
- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.
Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.",1.0,1,"The comment is unactionable. It provides a critique of the paper being incremental and lacking technical substance, but it does not offer any guidance or suggestions on how the authors could address this issue or improve their work."
1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm.,NIPS_2022_2315,Weakness: 1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm. 2) The justification for isotropic representation and contractive search could be more solid.,1.0,1,"The comment is unactionable. It points out a perceived issue with the proposed methods being independent and lacking connection, but it does not provide any guidance or suggestions on how the authors could address or improve this aspect in their draft."
"- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",ACL_2017_365_review,"1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.
2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.
3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.
References: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.
Multi-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.
- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",3.0,3,"The comment is somewhat actionable. It suggests an action implicitly, which is to investigate why the attention mechanism fails and to modify it so that it works. However, it is vague on how to execute this action, as it does not provide specific guidance or steps on how to identify the reasons for failure or how to change the attention mechanism."
"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.",ARR_2022_311_review,"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector.
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data?",3.0,3,"The comment is somewhat actionable. It identifies a weakness in the paper, specifically the limited scope of experiments, and suggests that the proposed method could be applied to more NLP tasks. However, it does not provide concrete guidance on how to expand the experiments or which specific tasks to consider, leaving the action somewhat vague."
"5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?",ACL_2017_489_review,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.",3.0,3,"The comment is somewhat actionable. It explicitly states that the paper's empirical results and analyses are difficult to understand in terms of the overall research question and specific hypothesis. However, it is vague on how to address this issue, as it does not provide specific guidance on how to clarify or organize the information."
1. The introduction to orthogonality in Part 2 could be more detailed.,oKn2eMAdfc,"1. The introduction to orthogonality in Part 2 could be more detailed.
2. No details on how the capsule blocks are connected to each other.
3. The fourth line of Algorithm 1 does not state why the flatten operation is performed.
4.The presentation of the α-enmax function is not clear.
5. Eq. (4) does not specify why BatchNorm is used for scalars (L2-norm of sj).
6. The proposed method was tested on relatively small datasets, so that the effectiveness of the method was not well evaluated.",3.0,3,"The comment is somewhat actionable. It explicitly states an action, which is to make the introduction to orthogonality more detailed. However, it is vague on how to execute this action, as it does not provide specific guidance on what details should be added or how to enhance the section."
"2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.",NIPS_2022_1034,"Regarding the background: the authors should consider adding a preliminary section to introduce the background knowledge on the nonparametric kernel regression, kernel density estimation, and the generalized Fourier Integral theorem, which could help the readers easily follow the derivation of Section 2 and understand the motivation to use the Fourier Integral theorem as a guide to developing a new self-attention mechanism.
Regarding the experimental evaluation: the issues are three-fold. 1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification. 2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive. 3) Since the FourierFormer need customized operators for implementation, the authors should also provide the memory/time cost profiling compared to popular Transformer architectures. Based on these issues, the efficiency and effectiveness of the FourierFormer are doubtful.
-------After Rebuttal------- Thank authors for the detailed response. Most of my concerns have been addressed. I have updated my scores to 6.",3.0,3,"The comment is somewhat actionable. It explicitly states that the experiments should be more comprehensive and general, but it is vague on how to execute this action. The reviewer mentions that the model size is limited and the baselines are restrictive, but does not provide specific guidance on how to address these issues."
1.The main part can be more concise (especially for the introduction part)and including empirical results.,NIPS_2022_1564,"1.The main part can be more concise (especially for the introduction part)and including empirical results.
2.Given the new introduced hyper-parameters, it is still not clear whether this new proposed method is empirically useful. How to choose hyper-parameters in a more practical training setting?
3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue.
The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.",3.0,3,"The comment is somewhat actionable. It explicitly suggests making the main part more concise and including empirical results. However, it is vague on how to execute these actions, as it does not specify which parts should be made more concise or what kind of empirical results should be included."
"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.",ARR_2022_215_review,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.",4.0,3,"The comment is somewhat actionable. It explicitly points out that the paper raises hypotheses that are not studied or discussed further, which is misleading. However, it is vague on how the authors should address this issue. The reviewer suggests that the paper should go deeper into the topics, but does not provide concrete guidance on how to do so."
- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?,ACL_2017_108_review,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?",4.0,3,"The comment is somewhat actionable. It provides explicit actions, such as giving a short description of a multigraph and clarifying the phrase 'many previous' and 'Solving this task'. However, it is vague on how to execute these actions, as it does not specify what the short description should include or how to rephrase the odd-sounding parts."
"- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",NIPS_2017_645,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",4.0,4,"The comment is mostly actionable. It implicitly suggests that the authors should compare their method with other methods like TSC and greedy subspace clustering by Park. The action is concrete because it specifies which methods to consider for comparison, but it is not explicitly stated."
- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,NIPS_2018_232,"- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",4.0,3,"The comment is somewhat actionable. It explicitly states that the abstract lacks a description of how the idea was evaluated and what the outcome was. However, it is vague on how to execute this action as it does not provide specific guidance on what details to include or how to structure this information."
"* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.",39n570rxyO,"This paper has weaknesses to address:
* The major weakness of this paper is the extremely limited experiments section. There are many experiments, yet almost no explanation of how they're run or interpretation of the results. Most of the results are written like an advertisement, mostly just stating the method outperforms others. This leaves the reader unclear why the performance gains happen. Ultimately it's not clear when/why the findings would generalize. The result is that some claims appear to be quite overstated. For example, L423-L424 states *""embeddings of domains with shared high-level semantics cluster together, as depicted in Appendix E.1. For example, embeddings of mono and stereo audio group closely, as do those of banking and economics.""* But this is cherry-picked---Temperature is way closer to Mono and Stereo Audio than Banking is to Economics.
* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.
* It's unclear how many variables actually overlap between training/testing, which seems to be a key element to make the model outperform others. Yet this isn't analyzed. Showing that others fail by ignoring other variables should be a key element of the experiments.",4.0,3,"The comment is somewhat actionable. The action is stated directly, as it points out that important experimental details are missing or relegated to the Appendix, and that explanations or interpretations are lacking. However, it is vague on how to execute the action, as it does not specify which details should be included or how to improve the explanations."
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not ablated. How important is the added complexity? Will one IN do?,NIPS_2017_434,"---
This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?
2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated. This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.
3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?
4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?
5. Are the learned object state embeddings interpretable in any way before decoding?
6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
* How many different kinds of physical interaction can be in one simulation?
* How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?
Preliminary Evaluation ---
Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.",4.0,3,"The comment is somewhat actionable. It implicitly suggests an action, which is to perform an ablation study on the use of multiple INs at different speeds in the dynamics predictor. However, it is vague on how to execute this action, as it does not provide specific guidance on how to conduct the ablation study or what aspects to focus on."
"3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.",Pb1DhkTVLZ,"1. The assessment criteria for the performance of large language models is limited to accuracy metrics. Such a limited view does not necessarily provide a comprehensive representation of the performance of large language models in real-world applications.
2. The method exhibits dependence on similar examples from the training dataset. This raises potential concerns regarding the distribution consistency between the training and test datasets adopted in the study. An in-depth visualization and analysis of the data distributions might be beneficial to address such concerns.
3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.",4.0,2,"The comment is borderline actionable. It implicitly suggests that the authors should consider expanding the evaluative framework to include more tasks and models. However, it does not provide concrete guidance on how to do this or which specific tasks or models to include, leaving the authors with a vague idea of how to proceed."
"1: ""The uncertainty is defined based on the posterior distribution."" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data. p.",ICLR_2021_2047,"As noted below, I have concerns around the experimental results. More specifically, I feel that there is a relative lack of discussion around the (somewhat surprising) outperformance of baselines that VPBNN is aiming to approximate, and I feel that the experiments are missing what I see as key VPBNN results that otherwise leave the reader with questions. Additionally, I think the current paper would benefit from including measurements and discussion around the specifics of computational and memory costs of their method. Recommendation
In general, I think this could be a great paper. However, given the above concerns, I'm currently inclined to suggest rejection of the paper in its current state. I would highly recommend that authors push further on the noted areas!
Additional comments
p. 1: ""The uncertainty is defined based on the posterior distribution."" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data.
p. 2: ""The MC dropout requires a number of repeated feed-forward calculations with randomly sampled weight parameters in order to obtain the predictive distribution."" This should be updated to indicate that in MC dropout, dropout is used (in an otherwise deterministic model) at test time with ""a number of repeated feed-forward calculations"" to effectively sample from the approximate posterior, but not directly via different weight samples (as in a variational BNN). With variational dropout, this ends up having a nice interpretation as a variational Bayes method, though no weight distributions are typically directly used with direct MC dropout.
p. 2: Lakshminarayanan et al. (2017) presented random seed ensembles, not bootstrap ensembles (see p. 4 of their work for more info). They used the full dataset, and trained M ensemble members with different random seeds, rather than resampled data.
p. 4: For variance propagation in a dropout layer with stochastic input, it's not exactly clear from the text how variance from the inputs and dropout is being combined into an output Gaussian. I believe using a Gaussian is an approximation, and while that would be fine, I think it would be informative to indicate that. The same issue comes up with local reparameterization for BNNs with parameter distributions, where they can be reparameterized exactly as output distributions (for, say, mean-field Gaussian weight dists) so long as the inputs are deterministic. Otherwise, the product of, say, two Gaussian RVs is non-Gaussian.
p. 7: Figure 1 is too small.
p. 7: ""Estimation of ρ is possible by observing the outputs of middle layers several times under the approximate predictive distribution. The additional computation cost is still kept quite small compared to MC dropout."" How exactly is ρ
estimated? Is it a one-time cost irregardless of data that can then be used for all predictions from the trained model? Without details, this seems like a key component that can yield arbitrary amounts of uncertainty.
p. 7, 8: For the language modeling experiment, why do you think VPBNN was able to achieve lower perplexity values than MC dropout? The text generally focuses on VPBNN as an approximation to MC dropout, and yet it outperforms it. The text would greatly benefit from more discussion around this point.
p. 8: For the OOD detection experiment, I'm surprised that ρ = 0
was the only VPBNN model used, since Section 5.1 and Figure 1 indicated that it led to overconfident models. Can you include results with other settings of ρ
? Moreover, from Figure 1 we see that (for that model) VPBNN with ρ = 0
qualitatively yielded the same amount of predictive variance as the Taylor approximation. However, in Table 2, we see VPBNN with ρ = 0
outperform MC dropout (with 100 or 2000 samples) and the Taylor approximation. Why do you think this is the case, particularly if the standard deviation was used as the uncertainty signal for the OOD decision. I see that ""This is because the approximation accuracy of the Taylor approximation is not necessarily high as shown in Section B"", but I did not find Section B or Figure 3 to be clear. I think the text would benefit from more discussion here, and from the additional experiments for ρ .
Can you include a discussion and measurements for FLOPS and memory usage for VPBNN? Specifically, given the discussion around efficiency and the implementation that doubles the dimensionality of the intermediates throughout the model, I believe it would be informative to have theoretical and possibly runtime measurements. Minor
p. 1: s/using the dropout/using dropout/
p. 1: s/of the language modeling/of language modeling/
p. 2: s/is the representative of/is representative of/
p. 2: s/In the deep learning/In deep learning/
p. 2: s/This relations/This relation/
p. 5: Need to define s
as the sigmoid function in the LSTM cell equations.",5.0,5,"The comment is highly actionable. It provides an explicit action by suggesting an update to the explanation of uncertainty, and it is concrete in detailing how to implement this change. The authors know exactly how to revise the statement for clarity by incorporating the suggested explanation."
"3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. ""we typically considers"" in the above of (7) 2. ""two permutation"" in the above of Theorem 1 3. ""until converge"" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.",ICLR_2022_2323,"Weakness:
1. The literature review is inaccurate, and connections to prior works are not sufficiently discussed. To be more specific, there are three connections, (i) the connection of (1) to prior works on multivariate unlabeled sensing (MUS), (ii) the connection of (1) to prior works in unlabeled sensing (US), and (iii) the connection of the paper to (Yao et al., 2021).
(i) In the paper, the authors discussed this connection (i). However, the experiments shown in Figure 2 do not actually use the MUS algorithm of (Zhang & Li, 2020) to solve (1); instead the algorithm is used to solve the missing entries case. This seems to be an unfair comparison as MUS algorithms are not designed to handle missing entries. Did the authors run matrix completion prior to applying the algorithm of (Zhang & Li, 2020)? Also, the algorithm of (Zhang & Li, 2020) is expected to fail in the case of dense permutation.
(ii) Similar to (i), the methods for unlabeled sensing (US) can also be applied to solve (1), using one column of B_0 at a time. There is an obvious advantage because some of the US methods can handle arbitrary permutations (sparse or dense), and they are immune to initialization. In fact, these methods were used in (Yao et al., 2021) for solving more general versions of (1) where each column of B has undergone arbitrary and usually different permutations; moreover, this can be applied to the d-correspondence problem of the paper. I kindly wish the authors consider incoporating discussions and reviews on those methods.
(iii) Finally, the review on (Yao et al., 2021) is not very accurate. The framework of (Yao et al., 2021), when applied to (1), means that the subspace that contains the columns of A and B is given (when generating synthetic data the authors assume that A and B come from the same subspace). Thus the first subspace-estimation step in the pipeline of (Yao et al., 2021) is automatically done; the subspace is just the column space of A. As a result, the method of (Yao et al., 2021) can handle the situation where the rows of B are densely shuffled, as discussed above in (ii). Also, (Yao et al., 2021) did not consider only ""a single unknown correspondence"". In fact, (Yao et al., 2021) does not utilize the prior knowledge that each column of B is permuted by the same permutation (which is the case of (1)), instead it assumes every column of B is arbitrarily shuffled. Thus it is a more general situation of (1) and of the d-correspondence problem. Finally, (Yao et al., 2021) discusses theoretical aspects of (1) with missing entries, while an algorithm for this is missing until the present work.
2. In several places the claims of the paper are not very rigorous. For example,
(i) Problem (15) can be solved via linear assignment algorithms to global optimality, why do the authors claim that ""it is likely to fall into an undesirable local solution""? Also I did not find a comparison of the proposed approach with linear assignment algorithms.
(ii) Problem (16) seems to be ""strictly convex"", not ""strongly convex"". Its Hessian has positive eigenvalues everywhere but the minimum eigenvalue is not lower bounded by some positive constant. This is my feeling though, as in the situation of logistic regression, please verify this.
(iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Experiments show that the algorithm needs > 1000 iterations to converge. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). Also I did not see any report on running times. Thus I feel uncomfortable to see the author claim in Section 5 that ""we propose a highly efficient algorithm"".
3. Even though an error bound is derived in Theorem 1 for the nuclear norm minimization problem, there is no guarantee of success on the alternating minimization proposal. Moreover, the algorithm requires several parameters to tune, and is sensitive to initialization. As a result, the algorithm has very lage variance, as shown in Figure 3 and Table 1. Questions:
1. In (3) the last term r+H(pi_P) and C(pi_P) is very interesting. Could you provide some intuition how it shows up, and in particular give an example?
2. I find Assumption 1 not very intuitive; and it is unclear to me why ""otherwise the influence of the permutation will be less significant"". Is it that the unknown permutation is less harmful if the magnitudes of A and B are close?
3. Solving the nuclear norm minimization program seems to be NP-hard as it involves optimization over permutation matrices and a complicated objective. Is there any hardness result for this problem?
Suggestions: The following experiments might be useful.
1. Sensitivity to permutation sparsity: As shown in the literature of unlabeled sensing, the alternating minimization of (Abid et al., 2017) works well if the data are sparsely permuted. This might also apply to the proposed alternating minimization algorithm here.
2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.
3. Sensitivity to other hyper-parameters.
Minor Comments on language usage: (for example)
1. ""we typically considers"" in the above of (7)
2. ""two permutation"" in the above of Theorem 1
3. ""until converge"" in the above of (14)
4. ......
Please proofread the paper and fix all language problems.",5.0,5,"The comment is highly actionable. It contains explicit actions and provides concrete details on how to implement them. The authors are instructed to proofread the paper and fix language issues, with specific examples given to guide them."
"* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.",NIPS_2017_104,"---
There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.
* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?
* How does this setting relate to question answering or visual question answering?
* How does the model perform on the same train data it's seen already? How much does it overfit?
* How hard is it to find intuitive attention examples as in figure 4?
* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.
* The related works section would be better understood knowing how the model works, so it should be presented later.",5.0,5,"The comment is highly actionable. It contains explicit actions and provides concrete suggestions on how to improve the presentation of the model in section 4. The authors are advised to replace some natural language descriptions with notation and add breakout diagrams, which gives them clear guidance on how to enhance the clarity of their presentation."
"- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.",BTr3PSlT0T,"- I express skepticism about whether the number of videos in the benchmark can achieve a robust assessment. The CVRR-ES benchmark includes only 214 videos, with the shortest video being just 2 seconds. Upon reviewing several videos from the anonymous link, I noticed a significant proportion of short videos. I question whether such short videos can adequately cover 11 categories. Moreover, current work that focuses solely on designing Video-LLMs, without specifically constructing evaluation benchmarks, provides a much larger number of assessment videos than the 214 included in CVRR-ES, for example, Tarsier [1].
- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.
- In the motivation, it is mentioned that the goal is to build human-centric AI systems. Does the paper's reflection on this point merely consist of providing a human baseline? I think that offering more fine-grained visual examples would be more helpful for human-AI comparisons.
- I think that the contribution of the DSCP is somewhat overstated and lacks novelty. Such prompt engineering-based methods have already been applied in many works for data generation, model evaluation, and other stages. The introduction and ablation experiments of this technology in the paper seem redundant.
- The discussion on DSCP occupies a significant portion of the experimental analysis. I think that the current analysis provided in the paper lacks insight and does not fully reflect the value of CVRR-ES, especially in terms of human-machine comparison.
- The phrase should be ""there exist a few limitations"" instead of ""there exist few limitations"" in line 520.
- The paper does not provide prompt templates for all the closed-source and open-source Video-LLMs used, which will influence the reproducibility.
The problems discussed in this paper are valuable, but the most crucial aspects of benchmark construction and evaluation are not entirely convincing. Instead, a significant amount of space is dedicated to introducing the DSCP method. I don't think it meets the acceptance standards of ICLR yet. I will consider modifying the score based on the feedback from other reviewers and the authors' responses. ***
[1] Wang J, Yuan L, Zhang Y. Tarsier: Recipes for Training and Evaluating Large Video Description Models[J]. arXiv preprint arXiv:2407.00634, 2024.",5.0,5,"The comment is highly actionable. It contains an explicit action and provides concrete details on how to implement it. The authors are instructed to include a table showing the distribution of video lengths and to explain how they ensured a balanced representation across categories, which gives them clear guidance on what needs to be done."
"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.",eI6ajU2esa,"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.
- Under noisy conditions, many TTA methods can achieve desirable results, while the improvement brought by this paper's method is relatively low.
- In appendix A.2.1, under noisy conditions, the average performance improvement brought by this paper's method is very low and can even be counterproductive under certain noise conditions. Does this indicate an issue with the approach of changing input data?
- How to verify the reliability of the long-range photometric consistency in section 3.3? Are there any ablation study results reflecting the performance gain brought by each part?
- The explanation of the formula content in Algorithm 1 in the main body is not clear enough.
[A] Temporal Coherent Test-Time Optimization for Robust Video Classification. ICLR23
[B] Video Test-Time Adaptation for Action Recognition. CVPR23",5.0,4,"The comment is mostly actionable. It implicitly suggests an action, which is to compare the proposed method with test-time adaptation (TTA) methods. The comment provides concrete details on how to implement this action by suggesting a comparison based on experimental results. However, the action is not stated directly, which makes it slightly less actionable."
"1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.",q09vTY1Cqh,"1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.
2. Time efficiency: For code completion tasks, it is also important to focus on the time efficiency. I recommend the authors could add corresponding experiments to make it clear.
3. Missing some related work: Recently there are some papers on similar topics, such as repo-level benchmark [1], API invocation [2]. The authors could consider discussing them in related work to better tease out research directions.
[1] Liu, Tianyang et al. “RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems.” (2023)
[2] Zhang, Kechi et al. “ToolCoder: Teach Code Generation Models to use API search tools.”(2023)",5.0,5,The comment is highly actionable. It explicitly suggests that the authors compare their work with existing code completion commercial applications like Copilot. It also provides concrete details on how to implement this action by suggesting testing on a smaller subset of RepoEval and emphasizing the importance of comparing with state-of-the-art systems.
"681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?",ACL_2017_818_review,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",5.0,5,"The comment is highly actionable. It contains explicit actions and concrete details on how to implement them. The reviewer clearly states that the authors should discuss the results for the task of inferring knowledge on objects and include results for model (B). Additionally, the reviewer suggests using consistent terminology for the model in Tables 1 and 2, and questions why objects are not mentioned in the context of 'latent in verbs'. The authors know exactly what needs to be done and how to apply these suggestions."
"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].",8fLgt7PQza,"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].
2. In the experiment or appendix section, I did not clearly see the formulas for Sensitivity and Specificity, nor were there any corresponding references, which is quite confusing to me. Moreover, using Accuracy as a metric in cases of highly imbalanced labels is unreasonable. For instance, in the MIMIC-III Mortality Prediction task, the positive rate is 5.42%. If I predict that all patients will survive, I can still achieve an accuracy of 94.58%. Previous works, such as GraphCare [1], have adopted AUROC and AUPRC as evaluation metrics.
3. The article is overly long and filled with detailed content, making it easy for readers to miss important points.
- [1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024
- [2] MedRetriever: Target-driven interpretable health risk prediction via retrieving unstructured medical text. CIKM 2021
- [3] Biomedical knowledge graph-enhanced prompt generation for large language models. Arxiv 2023",5.0,4,"The comment is mostly actionable. It explicitly states that the contribution of the article is incremental and suggests that key baselines were not cited. It also provides concrete examples of baselines that should be included, such as MedRetriever and KGRAG. However, it does not provide detailed guidance on how to integrate these baselines into the paper."
"2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments:",NIPS_2019_1131,"1. There is no discussion on the choice of ""proximity"" and the nature of the task. On the proposed tasks, proximity on the fingertip Cartesian positions is strongly correlated with proximity in the solution space. However, this relationship doesn't hold for certain tasks. For example, in a complicated maze, two nearby positions in the Euclidean metric can be very far in the actual path. For robotic tasks with various obstacles and collisions, similar results apply. The paper would be better if it analyzes what tasks have reasonable proximity metrics, and demostrate failure on those that don't. 2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments: 1. The diversity term, defined as the facility location function, is undirected and history-invariant. Thus it shouldn't be called ""curiosity"", since curiosity only works on novel experiences. Please use a different name. 2. The curves in Figure 3 (a) are suspiciously cut at Epoch = 50, after which the baseline methods seem to catch up and perhaps surpass CHER. Perhaps this should be explained.",5.0,4,"The comment is mostly actionable. It implicitly suggests that the authors should conduct an ablation study on certain parameters, specifically mentioning the \sigma in the RBF kernel and the parameters \eta and \tau in equation (7). While the action is not stated directly, it is concrete enough for the authors to understand what needs to be done."
"- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.",ACL_2017_333_review,"There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained. - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both ""represent the meaning"". Are both indeed necessary? Did you trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?
- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.
- page 1, lines 93-96: please provide a reference for this passage: ""This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.""
- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks.""
- Figure 2: What is ""MLP""? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).
- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).
- Table 2: what does ""#(ref)"" mean?
- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove ""the"" word in this line? "" SGD as our optimizing algorithms"" instead of ""SGD as our the optimizing algorithms.""
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "" council of europe again slams french prison conditions"" (again or against?)
- typo ""supper script"" -> ""superscript"" (4 times)",5.0,5,"The comment is highly actionable. The action is explicitly stated, and it is concrete. The authors are directed to explain how they achieved the values for specific parameters, and they know exactly what needs to be done to address this comment."
"2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps? [1] https://github.com/watchernyu/REDQ",ICLR_2023_1214,"As the authors note, it seems the method still requires a few tweaks to work well empirically. For example, we need to omit the log of the true rewards and scale the KL term in the policy objective to 0.1. While the authors provide a brief intuition on why those modifications are needed, I think the authors should provide more concrete analysis (e.g., empirical results) on what problems the original formula have and how the modifications fix them. Also, it would be better if the authors provide ablation results on those modifications. For example, does the performance drop drastically if the scale of the KL term changes (to 0.05, 0.2, 0.5, ...) ?
The compute comparison vs. REDQ on Figure 3 seems to be misleading. First, less runtime does not necessarily mean less computational cost. Second, if the authors had used the official implementation of REDQ [1], it should be emphasized that this implementation is very inefficient in terms of runtime. In detail, the implementation feeds forward to each Q-network one at a time while this feed-forward process is embarrassingly parallelizable. The runtime of REDQ will drop significantly if we parallelize this process.
The ablation experiments seem to show that the value term for the encoder is not necessary. It would be better to provide an explanation on this result.
Some of the equations seem to have typos. 1) On equation (4), the first product and the second product have exactly the same form. 2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions
I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps?
[1] https://github.com/watchernyu/REDQ",5.0,5,"The comment is mostly actionable. It contains explicit actions and provides concrete details on how to implement them. The reviewer suggests a specific change in the algorithm (using s_n instead of s_t) and requests additional results (average return results with more environment steps), which are clear and actionable."
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.,ARR_2022_121_review,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)).
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays. Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference.",5.0,4,"The comment is mostly actionable. The action is implicit but concrete. The reviewer suggests considering feature engineering to improve performance, specifically mentioning Uto et al. (2020)'s feature set as a potential avenue. While the action is not directly stated, the authors can infer that they should explore using these features to enhance their results."
- The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.,ICLR_2023_1765,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.",5.0,5,"The comment is highly actionable. The action is stated explicitly and provides concrete details on how to implement it. The authors are instructed to adjust the size of the text inside the figure and labels to match the manuscript text size, which is a clear and specific action."
"2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison? Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison: Guo Lu, et al., ""Content Adaptive and Error Propagation Aware Deep Video Compression."" ECCV 2020.",ICLR_2022_1522,"Weakness:
The overall novelty seems limited since the instance-adaptive method is from existing work with no primary changes. Here are some main questions and concerns:
1). How many optimization steps are used to produce the final reported performance in Figure.1 as well as in some other figs and tables?
2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison?
Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison:
Guo Lu, et al., ""Content Adaptive and Error Propagation Aware Deep Video Compression."" ECCV 2020.",5.0,4,"The comment is mostly actionable. It contains both explicit and implicit actions. The reviewer explicitly asks for the precise bitrate range used for BD-rate comparison, which is a concrete request. Additionally, the reviewer suggests discussing or comparing with a related work, which is an implicit action but provides a specific reference, making it concrete."
"3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.",ICLR_2023_3705,"1)The main assumption is borrowed from other works but is actually rarely used in the optimization field. Moreover, the benefits of this assumption is not well investigated. For example, a) why it is more reasonable than the previous one? B) why it can add gradient norm L_1 \nabla f(w_1) in Eqn (3) or why we do not add other term? It should be mentioned that a milder condition does not mean it is better, since it may not reflect the truth. For me, problem B) is especially important in this work, since the authors do not well explain and investigate it.
2)Results in Theorem 1 show that Adam actually does not converge, since this is a constant term O(D_0^{0.5}\delta) in Eqn. (5). This is not intuitive, the authors claim it is because the learning rate may not diminish. But many previous works, e.g. [ref 1], can prove Adam-type algorithms can converge even using a constant learning rate. Of course, they use the standard smooth condition. But (L0,L1)-smoothness condition should not cause this kind of convergence, since for nonconvex problem, in most cases, we only need the learning rate to be small but does not care whether it diminishes to zero.
[ref 1] Dongruo Zhou, Jinghui Chen, et al. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization
3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.
4)Under the same assumption, the authors use examples to show the advantage of Adam over GD and SGD. This is good. But one issue is that is the example reasonable or does it share similar properties with practical problems, especially for networks. This is important since both SGD and ADAM are widely used in the deep learning field.
5)In the work, when comparing SGD and ADAM, the authors explain the advantage of adam comes from the cases when the local smoothness varies drastically across the domain. It is not very clear for me why Adam could better handle this case. Maybe one intuitive example could help.
6)The most important problem is that this work does not provide new insights, since it is well known that the second order moment could help the convergence of Adam. This work does not provide any insights beyond this point and also does not give any practical solution to further improve.",5.0,3,"The comment is somewhat actionable. It explicitly states that the authors should explain the challenges when analyzing Adam under the (L0,L1)-smoothness condition, but it is vague on how to execute this action. The comment does not provide specific guidance on what aspects of the challenges need to be addressed or how to differentiate from Zhang et al."
"2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.",ICLR_2021_1504,"W1) The authors should compare their approach (methodologically as well as experimentally) to other concept-based explanations for high-dimensional data such as (Kim et al., 2018), (Ghorbani et al., 2019) and (Goyal et al., 2019). The related work claims that (Kim et al., 2018) requires large sets of annotated data. I disagree. (Kim et al., 2018) only requires a few images describing the concept you want to measure the importance of. This is significantly less than the number of annotations required in the image-to-image translation experiment in the paper where the complete dataset needs to be annotated. In addition, (Kim et al., 2018) allows the flexibility to consider any given semantic concept for explanation while the proposed approach is limited either to semantic concepts captured by frequency information, or to semantic concepts automatically discovered by representation learning, or to concepts annotated in the complete dataset. (Ghorbani et al., 2019) also overcomes the issue of needing annotations by discovering useful concepts from the data itself. What advantages does the proposed approach offer over these existing methods?
W2) Faithfulness of the explanations with the pretrained classifier. The methods of disentangled representation and image-to-image translation require training another network to learn a lower-dimensional representation. This runs the risk of encoding some biases of its own. If we find some concerns with the explanations, we cannot infer if the concerns are with the trained classifier or the newly trained network, potentially making the explanations useless.
W3) In the 2-module approach proposed in the paper, the second module can theoretically be any explainability approach for low-dimensional data. What is the reason that the authors decide to use Shapely instead of other works such as (Breiman, 2001) or (Ribeiro et al., 2016)?
W4) Among the three ways of transforming the high-dimensional data to low-dimensional latent space, what criteria should be used by a user to decide which method to use? Or, in other words, what are the advantages and disadvantages of each of these methods which might make them more or less suitable for certain tasks/datasets/applications?
W5) The paper uses the phrase “human-interpretable explainability”. What other type of explainability could be possible if it’s not human-interpretable? I think the paper might benefit with more precise definitions of these terms in the paper.
References mentioned above which are not present in the main paper:
(Ghorbani et al., 2019) Amirata Ghorbani, James Wexler, James Zou, Been Kim. Towards Automatic Concept-based Explanations. NeurIPS 2019.
(Goyal et al., 2019) Yash Goyal, Amir Feder, Uri Shalit, Been Kim. Explaining Classifiers with Causal Concept Effect (CaCE). ArXiv 2019.
—————————————————————————————————————————————————————————————— ——————————————————————————————————————————————————————————————
Update after rebuttal: I thank the authors for their responses to all my questions. However, I believe that these answers need to be justified experimentally in order for the paper’s contributions to be significant for acceptance. In particular, I still have two major concerns. 1) the faithfulness of the proposed approach. I think that the authors’ answer that their method is less at risk to biases than other methods needs to be demonstrated with at least a simple experiment. 2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.",5.0,5,"The comment is highly actionable. The reviewer explicitly states the actions needed: to back up the argument for using Shapely value explanations by comparing them experimentally with other methods and to include a discussion on the advantages and disadvantages of different methods for transforming data. The actions are concrete, and the authors know exactly what needs to be done."
"- Limited Experiments - Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.",zpayaLaUhL,"- Limited Experiments
- Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.
- The input for the analysis is limited to only 100 or 200 samples from wikitext-2. It would be desirable to experiment with a larger number of samples or with datasets from various domains.
- Findings are interesting, but no statement of what the contribution is and how practical impact on the community or practical use. (Question A).
- Results contradicting those reported in existing studies (Clark+'19) are observed but not discussed (Question B).
- I do not really agree with the argument in Section 5 that word embedding contributes to relative position-dependent attention patterns. The target head is in layer 8, and the changes caused by large deviations from the input, such as only position embedding, are quite large at layer 8. It is likely that the behavior is not such that it can be discussed to explain the behavior under normal conditions. Word embeddings may be the only prerequisites for the model to work properly rather than playing an important role in certain attention patterns.
- Introduction says to analyze ""why attention depends on relative position,"" but I cannot find content that adequately answers this question.
- There is no connection or discussion of relative position embedding, which is typically employed in recent Transformer models in place of learnable APE (Question C).",5.0,5,"The comment is highly actionable. It explicitly states the need for more experiments and provides concrete details on how to implement this action. The reviewer suggests investigating the generalization of results to other models and provides specific examples, such as including analysis for GPT-2 and replicating Figure 2 for GPT-2. This gives the authors clear guidance on how to improve their draft."

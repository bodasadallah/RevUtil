{"custom_id": "request-0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point:\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\n\n"}]}}
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point:\n- Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.\n\n"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.\n\n"}]}}
{"custom_id": "request-3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n\n"}]}}
{"custom_id": "request-4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point:\n- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.\n\n"}]}}
{"custom_id": "request-5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n\n"}]}}
{"custom_id": "request-6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n1. Symbols are a little bit complicated and takes a lot of time to understand.\n\n"}]}}
{"custom_id": "request-7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n\n"}]}}
{"custom_id": "request-8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n\n"}]}}
{"custom_id": "request-9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.\n\n"}]}}
{"custom_id": "request-10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point:\n- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\n"}]}}
{"custom_id": "request-11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.\n\n"}]}}
{"custom_id": "request-12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point:\n2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n\n"}]}}
{"custom_id": "request-13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point:\n- The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning\n\n"}]}}
{"custom_id": "request-14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point:\n1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n\n"}]}}
{"custom_id": "request-15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].\n\n"}]}}
{"custom_id": "request-16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point:\n1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm.\n\n"}]}}
{"custom_id": "request-17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.\n\n"}]}}
{"custom_id": "request-18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n\n"}]}}
{"custom_id": "request-19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n\n"}]}}
{"custom_id": "request-20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point:\n1. The introduction to orthogonality in Part 2 could be more detailed.\n\n"}]}}
{"custom_id": "request-21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point:\n2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.\n\n"}]}}
{"custom_id": "request-22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point:\n1.The main part can be more concise (especially for the introduction part)and including empirical results.\n\n"}]}}
{"custom_id": "request-23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.\n\n"}]}}
{"custom_id": "request-24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?\n\n"}]}}
{"custom_id": "request-25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.\n\n"}]}}
{"custom_id": "request-26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n\n"}]}}
{"custom_id": "request-27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point:\n* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.\n\n"}]}}
{"custom_id": "request-28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point:\n1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not ablated. How important is the added complexity? Will one IN do?\n\n"}]}}
{"custom_id": "request-29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point:\n3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n\n"}]}}
{"custom_id": "request-30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point:\n1: \"The uncertainty is defined based on the posterior distribution.\" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data. p.\n\n"}]}}
{"custom_id": "request-31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. \"we typically considers\" in the above of (7) 2. \"two permutation\" in the above of Theorem 1 3. \"until converge\" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.\n\n"}]}}
{"custom_id": "request-32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point:\n* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.\n\n"}]}}
{"custom_id": "request-33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point:\n- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.\n\n"}]}}
{"custom_id": "request-34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point:\n- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.\n\n"}]}}
{"custom_id": "request-35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point:\n1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.\n\n"}]}}
{"custom_id": "request-36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point:\n681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 \"latent in verbs\": why don't you mention objects here?\n\n"}]}}
{"custom_id": "request-37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n\n"}]}}
{"custom_id": "request-38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point:\n2. \u00c2 Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \\sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \\lambda changes the performance, but it would be nice to see how \\eta and \\tau in equation (7) affect performance. Minor comments:\n\n"}]}}
{"custom_id": "request-39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point:\n- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.\n\n"}]}}
{"custom_id": "request-40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point:\n2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps? [1] https://github.com/watchernyu/REDQ\n\n"}]}}
{"custom_id": "request-41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point:\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n\n"}]}}
{"custom_id": "request-42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment mentions that the paper does not treat concept map extraction as a separate task but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point:\n- The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.\n\n"}]}}
{"custom_id": "request-43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment states directly that an experiment where the image is occluded should be done.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison? Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison: Guo Lu, et al., \"Content Adaptive and Error Propagation Aware Deep Video Compression.\" ECCV 2020.\n\n"}]}}
{"custom_id": "request-44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions some specific actions that the authors didn't do. This implies that the authors should, in fact, implement these mentioned actions.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The comment mentions that some claims need more in-depth analysis, but the comment didn't mention what these claims are.\nscore: 3\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.\n\n"}]}}
{"custom_id": "request-45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. The reviewer stated that they would like to see more analysis, but they were not concrete on which type of analysis they wanted to see.\nscore: 3\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions that it would be nice to have examples of the system on actual texts. So, the authors can infer that this is an action that needs to be done.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hints that the paper should dig into the theory proofs, but it didn't specify how to do that.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment mentions directly that an example of the inconsistency should be made.\nscore: 5\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point:\n2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.\n\n"}]}}
{"custom_id": "request-46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: This comment is not actionable at all. It doesn't tell the authors what to do. It just states what the authors have done in their draft, but it doesn't give any direction on what needs to be improved.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: The comment is mostly actionable. The stated action is concrete, but the action is stated in an indirect way. The comment mentions how the Challenge Set is used. This urges the authors to explain this in their draft.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment is somewhat actionable. The stated action is direct, but it's still vague how to carry out this action. They stated that the paper needs to include more baselines; however, they didn't mention which baselines should be included.\nscore: 3\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: The comment is fully actionable. The action is stated directly, and the action is concrete, and the authors know exactly what needs to be done and how they should implement the action. The comment directly mentions that the description should be revised, and it concretely mentions how it should be revised.\nscore: 5\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: The comment is barely actionable. It gives an implicit and indirect action; however, the action is vague, and the authors would not know how to implement it. The comment hinted that the authors can improve Algorithm 2 but didn't mention how they can improve it.\nscore: 2\n\n\nReview Point:\n- Limited Experiments - Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.\n\n"}]}}

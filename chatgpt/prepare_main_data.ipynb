{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_data = pd.read_csv('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/all_review_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204917"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### remove the taken examples: \n",
    "taken = pd.read_csv(f'../data/taken_for_human_data/taken_1k_samples_human_annotation_sampled.csv')['point']\n",
    "taken  = pd.concat([taken, pd.read_csv(f'../data/human_annotation_gathered/all_human_annotations_processed.csv')['review_point']])\n",
    "taken = pd.concat([taken, pd.read_csv(f'../data/taken_for_human_data/filtered_samples.csv')['point']])\n",
    "print(len(all_data))\n",
    "## remove the taken samples from all\n",
    "all_data = all_data[~all_data['point'].isin(taken)]\n",
    "len(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_id', 'venue', 'focused_review', 'point'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bad_points(point):\n",
    "    ## remove the points that have less than 5 words\n",
    "    ## remove points that ends in ; or :\n",
    "    ## remove points that starts with . or + \n",
    "    ## remove points that has avg word length less than 4\n",
    "    ## if the word \"Strengths\" is in the first few words\n",
    "    ## if @ in in the begining of the first few words\n",
    "\n",
    "    point = point['point']\n",
    "    if len(point.split()) < 5:\n",
    "        return False\n",
    "    if point[-1] in [';', ':']:\n",
    "        return False\n",
    "    if point[0] in ['.', '+', '@']:\n",
    "        return False\n",
    "    if np.mean([len(w) for w in point.split()]) < 4:\n",
    "        return False\n",
    "    first_5 = point.split()[:5]\n",
    "    for w in first_5:\n",
    "        if 'strength' in w.lower():\n",
    "            return False\n",
    "        if 'recommndation' in w.lower():\n",
    "            return False\n",
    "        if '@' in w:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over the points and filter the bad ones\n",
    "all_data = all_data[all_data.apply(filter_bad_points, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt filteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3155991/2290027933.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped.apply(lambda group: group.sample(n=min(len(group), samples_per_group), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venue\n",
      "ICLR_2025     2932\n",
      "ICLR_2024     1924\n",
      "EMNLP_2023    1027\n",
      "NIPS_2020      953\n",
      "ARR_2022       857\n",
      "ICLR_2022      857\n",
      "ICLR_2023      857\n",
      "NIPS_2018      666\n",
      "NIPS_2022      523\n",
      "ACL_2017       395\n",
      "ICLR_2021      380\n",
      "NIPS_2019      310\n",
      "NIPS_2021      239\n",
      "NIPS_2017       80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "total_samples = 12000\n",
    "\n",
    "# Uniform sampling\n",
    "grouped = all_data.groupby(\"venue\")\n",
    "samples_per_group = max(1, total_samples // grouped.ngroups)  # Divide total_samples evenly\n",
    "sampled_df = (\n",
    "    grouped.apply(lambda group: group.sample(n=min(len(group), samples_per_group), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# If more rows are still needed due to rounding, sample the remainder\n",
    "remainder = total_samples - len(sampled_df)\n",
    "if remainder > 0:\n",
    "    remaining_sample = all_data.loc[~all_data.index.isin(sampled_df.index)].sample(n=remainder, random_state=42)\n",
    "    sampled_df = pd.concat([sampled_df, remaining_sample]).reset_index(drop=True)\n",
    "\n",
    "print(sampled_df.value_counts(\"venue\"))\n",
    "\n",
    "sampled_df['id'] = range(1, len(sampled_df) + 1)\n",
    "sampled_df.to_csv(f'/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/unfiltered_main_chatgpt_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/taken_for_human_data/filtered_samples.csv'\n",
    "\n",
    "# unfiltered_data = pd.read_csv(path)\n",
    "# ### get rows that has the human_difficulty column as 1\n",
    "# unfiltered_data = unfiltered_data[unfiltered_data['human_discard'] == 1]\n",
    "# unfiltered_data.to_csv(f'/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/discarded_by_human.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/discarded_by_human.csv'\n",
    "# unfiltered_data = pd.read_csv(path)\n",
    "\n",
    "# unfiltered_data = unfiltered_data[unfiltered_data.apply(filter_bad_points, axis=1)]\n",
    "# before_len = len(unfiltered_data)\n",
    "# after_len = len(unfiltered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''Evaluate the following review point and determine if it should be discarded based on these criteria:\n",
    "\n",
    "**discard the point if:**\n",
    "- It is incomplete or cut off.\n",
    "- It highlights a strength or positive aspect of the paper.\n",
    "- It's just some typo fixes. \n",
    "- It does not address a limitation or weakness of the paper.\n",
    "- if the whole point is just mentions of some references.\n",
    "**Output \"1\" for dicarded points and \"0\" for accepted points.**\n",
    "**Review Point:**\n",
    "{point}\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "path = '/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/unfiltered_main_chatgpt_data.csv'\n",
    "unfiltered_data = pd.read_csv(path)\n",
    "\n",
    "print(len(unfiltered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of the prompts is {'custom_id': '1', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o', 'temperature': 0, 'messages': [{'role': 'user', 'content': 'Evaluate the following review point and determine if it should be discarded based on these criteria:\\n\\n**discard the point if:**\\n- It is incomplete or cut off.\\n- It highlights a strength or positive aspect of the paper.\\n- It\\'s just some typo fixes. \\n- It does not address a limitation or weakness of the paper.\\n- if the whole point is just mentions of some references.\\n**Output \"1\" for dicarded points and \"0\" for accepted points.**\\n**Review Point:**\\n6) 279: add \"be\"7) l. 352: give an example of a nontrivial internal path.\\n'}]}}\n",
      "Batch file for main chatgpt unfiltered data of 15k model gpt-4o, temperature 0 is created with id batch_67add443e26c8190a6d82c9a0b19e24b\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "\n",
    "for i,row in unfiltered_data.iterrows():\n",
    "    review_point = row['point']\n",
    "    prompt = PROMPT.format(point=review_point)  \n",
    "    line = {\n",
    "        \"custom_id\": f\"{row['id']}\", \n",
    "        \"method\": \"POST\", \n",
    "        \"url\": \"/v1/chat/completions\", \n",
    "        \"body\": {\"model\": model_name,\n",
    "        ########### UNCOMMENT AGAIN #########\n",
    "        # \"response_format\" :{ \"type\": \"json_object\" },\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": \n",
    "        [{\"role\": \"user\", \"content\": prompt}],}}\n",
    "    lines.append(line)\n",
    "\n",
    "print(f'sample of the prompts is {lines[0]}')\n",
    "\n",
    "### Write batch input file\n",
    "batch_file_path = f\"main_data/main_chatgpt_data_unfiltered_batch_input.jsonl\"\n",
    "with open(batch_file_path, 'w') as f:\n",
    "    for l in lines:\n",
    "        json.dump(l, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "### upload the batch file\n",
    "batch_input_file = client.files.create(\n",
    "file=open(batch_file_path, \"rb\"),\n",
    "purpose=\"batch\")\n",
    "\n",
    "### create the batch request\n",
    "batch_input_file_id = batch_input_file.id\n",
    "batch_data = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": f\"batch file for  main chatgpt unfiltered data of 15k model gpt-4o, temperature 0\"\n",
    "    })\n",
    "batch_metadata = {\n",
    "    \"batch_id\": batch_data.id,\n",
    "    \"data_pth\": path,\n",
    "    \"batch_input_file_id\": batch_input_file_id,\n",
    "    \"batch_file_path\": batch_file_path\n",
    "}\n",
    "\n",
    "with open(f\"main_data/main_chatgpt_data_unfiltered_batch_input_meta_data.json\", 'w') as f:\n",
    "    json.dump(batch_metadata, f, indent=4)\n",
    "print(f\"Batch file for main chatgpt unfiltered data of 15k model gpt-4o, temperature 0 is created with id {batch_data.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data):\n",
    "    \n",
    "    batch_id = batch_data['batch_id']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = f\"main_data/main_chatgpt_data_unfiltered_batch_output.jsonl\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')\n",
    "\n",
    "def save_results(batch_data):\n",
    "    errors = 0\n",
    "    chatgpt_response = pd.read_json(f\"main_data/main_chatgpt_data_unfiltered_batch_output.jsonl\", lines=True)\n",
    "\n",
    "    # raw_data_df = pd.read_csv(path)\n",
    "    raw_data_df = unfiltered_data.copy()\n",
    "    ones = 0\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "\n",
    "        ## try to load the answer as a json object\n",
    "        try:\n",
    "            id = raw_data_df.iloc[i]['id']\n",
    "            chatgpt_row = chatgpt_response[chatgpt_response['custom_id']==id]\n",
    "            chatgpt_row = chatgpt_row.iloc[0].copy()\n",
    "            answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "            ## assert that the answer is  0 or 1\n",
    "            assert answer in ['0', '1']\n",
    "            if answer == '1':\n",
    "                ones += 1\n",
    "            row = raw_data_df.iloc[i].copy()\n",
    "            row [f'chatgpt_discard'] = answer\n",
    "            final_df.append(row)\n",
    "        except:\n",
    "            errors += 1\n",
    "            print(\"No valid JSON found.\")\n",
    "            continue\n",
    "    print(f\"Errors: {errors}\")\n",
    "    print(f\"Total ones: {ones}\")\n",
    "    final_df = pd.DataFrame(final_df)\n",
    "    final_df.to_csv(f\"/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/human_chatgpt_filtered_data.csv\", index=False)\n",
    "\n",
    "    print(f\"Final data size is {len (final_df[final_df['chatgpt_discard']=='0'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file for main chatgpt data is completed\n",
      "No valid JSON found.\n",
      "No valid JSON found.\n",
      "No valid JSON found.\n",
      "No valid JSON found.\n",
      "No valid JSON found.\n",
      "Errors: 5\n",
      "Total ones: 919\n",
      "Final data size is 11076\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "batch_data = json.load(open(f'main_data/main_chatgpt_data_unfiltered_batch_input_meta_data.json'))\n",
    "\n",
    "if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "    print(f\"Batch file for main chatgpt data is completed\")\n",
    "    retrive_batch_and_save_results(batch_data)\n",
    "    save_results(batch_data)\n",
    "else:\n",
    "    print(f\"Batch file for main chatgpt data is not completed yet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chatgpt_discard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chatgpt_discard'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiscard\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchatgpt_discard\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchatgpt_discard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     11\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_discard\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchatgpt_discard\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chatgpt_discard'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score,cohen_kappa_score, accuracy_score, recall_score\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(f'/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/main_data/archive/discarded_by_human.csv')\n",
    "print(len(data))\n",
    "data.rename(columns={'Discard': 'human_discard'}, inplace=True)\n",
    "data['human_discard'] = data['human_discard'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "data['chatgpt_discard'] = data['chatgpt_discard'].astype(int)\n",
    "data['human_discard'] = data['human_discard'].astype(int)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(data['human_discard'], data['chatgpt_discard'])}\")\n",
    "print(f\"Recall: {recall_score(data['human_discard'], data['chatgpt_discard'])}\")\n",
    "print(f\"F1 Score: {f1_score(data['human_discard'], data['chatgpt_discard'])}\")\n",
    "print(f\"Cohen's Kappa: {cohen_kappa_score(data['human_discard'], data['chatgpt_discard'])}\")\n",
    "\n",
    "cm = confusion_matrix(data['human_discard'], data['chatgpt_discard'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0', '1'])\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

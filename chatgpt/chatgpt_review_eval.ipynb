{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "model_name = 'gpt-4o'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CONTEXT_EXAMPLES = { \n",
    "\n",
    "'actionability_examples': '''\n",
    "Review Point:\n",
    "- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n",
    "score: 1\n",
    "rationale: This is not actionable at all. after reading the comment, the authors still have no idea what should be done.\n",
    "\n",
    "Review Point:\n",
    "- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n",
    "score: 2\n",
    "rationale: This is barely actionable. The comment mentions that there's a weakness in the experiments, but doesn't specify what the weakness is. \n",
    "\n",
    "Review Point:\n",
    "4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n",
    "score: 3\n",
    "rationale: Corrected how? this action is not concrete.\n",
    "\n",
    "Review Point:\n",
    "It will be nice to see some examples of the system on actual texts (vs. other components & models).\n",
    "score: 4\n",
    "rationale: The action is concrete, but it is not stated directly.\n",
    "\n",
    "Review Point:\n",
    "- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n",
    "score: 5\n",
    "rationale: There is a concrete, and emplicit action.\n",
    "''',\n",
    "'grounding_specificity_examples': '''\n",
    "Review Point:\n",
    "The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n",
    "score: 1\n",
    "rationale: The comment is not gorunded at all, as it does not mention what the proposed algorithm is, or where it was mentioned.\n",
    "\n",
    "Review Point:\n",
    "Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\n",
    "score: 2\n",
    "rationale: It's weakly grounded and not specific. The comment mentions that the paper does not treat concept map extraction as a separate task, but does not mention where this is mentioned in the paper. It also does not mention what needs to be addressed.\n",
    "\n",
    "Review Point:\n",
    "- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n",
    "score: 3\n",
    "rationale: The comment is not well grounded, but the authors can guess which part does this comment refer to, and it's specific.\n",
    "\n",
    "Review Point:\n",
    "The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\n",
    "score: 4\n",
    "rationale: The comment is grounded, as it mentions the part that needs addressing, however, it is not specific enough.\n",
    "\n",
    "Review Point:\n",
    "- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?\n",
    "score: 5\n",
    "rationale: The comment is fully grounded, as it mentions the specific table, and also mentions what needs addressing in it. \n",
    "''',\n",
    "\n",
    "'verifiability_examples' : '''\n",
    "Review Point:\n",
    "It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n",
    "score: 1\n",
    "rationale: The claim is \"a bit unclear\", and it is not verifiable at all. \n",
    "\n",
    "Review Point:\n",
    "It will be nice to see some examples of the system on actual texts (vs. other components & models).\n",
    "score: 2\n",
    "rationale: The claim is \"It will be nice to see some examples\", and it has limited verification.\n",
    "\n",
    "Review Point:\n",
    "Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?\n",
    "score: 3\n",
    "rationale: The claim is \"needs a little more clarification\" .The vefirication is not extensive enough. \n",
    "\n",
    "Review Point:\n",
    "The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\n",
    "score: 4\n",
    "rationale: The claim here is \"task seems a bit unfair\", and it has been mostly verified. \n",
    "\n",
    "Review Point:\n",
    "- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features.\n",
    "score: 5\n",
    "rationale: The claim is \"are somewhat intertwined and thus confusing\", and it is fully verifiable.\n",
    "\n",
    "Review Point:\n",
    "The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n",
    "score: X\n",
    "rationale: There is no claim in this comment, as it only mentions somehtiing that is missing in the paper, without expressing any sunjective opinion.\n",
    "''',\n",
    "\n",
    "'helpfulness_examples' : '''\n",
    "\n",
    "Review Point:\n",
    "Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.\n",
    "score: 1\n",
    "rationale: The comment is not helpful at all, as it does not suggest what the authors should do to improve the paper.\n",
    "\n",
    "Review Point:\n",
    "Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n",
    "score: 2\n",
    "rationale: The comment is barely helpful, as the action is implicit and vague.\n",
    "\n",
    "Review Point:\n",
    "The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\n",
    "score: 3\n",
    "rationale: The comment is somewhat helpful, as it suggests what the authors should do to improve the paper, but it is not concrete, and vague.\n",
    "\n",
    "Review Point:\n",
    "Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\n",
    "score: 4\n",
    "rationale: The comment is mostly helpful, as it states a concrete and explicit action.\n",
    "\n",
    "Review Point:\n",
    "781 \"both tasks\": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.    \n",
    "score: 5\n",
    "rationale: The comment is fully helpful, as it tells the authors exactly what they should do.\n",
    "'''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = {}\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "\n",
    "for aspect in aspects:\n",
    "    # data = pd.read_csv(f\"test_data/{aspect}_test_data.csv\")\n",
    "    data = pd.read_csv(f\"test_data/random_test_data.csv\")\n",
    "    examples = IN_CONTEXT_EXAMPLES[f'{aspect}_examples']\n",
    "    test_data[aspect] = {\n",
    "        'data': data,\n",
    "        'examples': examples\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['focused_review', 'actionability', 'review_point',\n",
       "       'actionability_label', 'rationale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_incontext_examples['actionability'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect,prompt_type, in_context_examples):\n",
    "\n",
    "    prompt = ''\n",
    "    if prompt_type == 'definitions':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[f'{aspect}_no_examples'])\n",
    "\n",
    "    elif prompt_type == 'definitions_examples':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect])\n",
    "    elif prompt_type == 'definitions_incontext_learning':\n",
    "        examples = ''\n",
    "        examples_str = []\n",
    "        ##3 group examples by the label and choose a random example from each group\n",
    "        for label in in_context_examples[f'{aspect}_label'].unique():\n",
    "            ## keep sampling a line till it is not the same as the currrent review point\n",
    "            while True:\n",
    "                row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                row = row.iloc[0]\n",
    "                if row['review_point'] != review_point:\n",
    "                    break\n",
    "        \n",
    "            score = row[f'{aspect}_label']\n",
    "            rationale = row['rationale']\n",
    "            examples_str.append(f'''\n",
    "Review Point: {row['review_point']}\n",
    "rationale: {rationale}\n",
    "score: {score}\n",
    "''')\n",
    "        ## shuffle the list \n",
    "        random.shuffle(examples_str)\n",
    "        examples = '\\n'.join(examples_str)\n",
    "\n",
    "        prompt = BASE_PROMPT_EXAMPLES.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect],examples=examples)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Chatgpt Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "prompt_types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "\n",
    "def chatgpt_inf(test_data, prompt_types, save_path):\n",
    "## iterate over the df \n",
    "    for aspect in test_data.keys():\n",
    "        responses = []\n",
    "        fails = 0\n",
    "        examples  = test_data[aspect]['examples']\n",
    "        data = test_data[aspect]['data']\n",
    "        for idx ,row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            for prompt_type in prompt_types:\n",
    "                \n",
    "                review_point = row['review_point']\n",
    "                prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, examples=examples)\n",
    "                # print(prompt_type)\n",
    "                # print(prompt)\n",
    "\n",
    "                ## call the gpt4 model,and output the error message if the call fails\n",
    "                try:\n",
    "                    clue_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    completion = client.chat.completions.create(\n",
    "                    response_format={ \"type\": \"json_object\" },\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        clue_message\n",
    "                    ]\n",
    "                    )\n",
    "                    response = json.loads(completion.choices[0].message.content.lower())\n",
    "                    score = response['score']\n",
    "                    rationale = response['rationale']\n",
    "                except Exception as e:      \n",
    "                    print(f\"Failed for {idx} and {aspect} with error {e}\")\n",
    "                    fails += 1\n",
    "                    score = 'NA'\n",
    "                    rationale = 'NA'\n",
    "\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_score'] = score\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_rationale'] = rationale\n",
    "            responses.append(row)\n",
    "        responses = pd.DataFrame(responses)\n",
    "        responses.to_csv(f\"{save_path}/{aspect}_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chatgpt_batch_inf(test_data,aspect, prompt_type, save_path):\n",
    "   in_context_examples =  all_incontext_examples[aspect]\n",
    "   lines = []\n",
    "   for i,row in test_data.iterrows():\n",
    "      review_point = row['review_point']\n",
    "      prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, in_context_examples=in_context_examples)   \n",
    "      line = {\n",
    "         \"custom_id\": f\"{row['id']}\", \n",
    "         \"method\": \"POST\", \n",
    "         \"url\": \"/v1/chat/completions\", \n",
    "         \"body\": {\"model\": model_name,\n",
    "         \"temperature\": 0.1,\n",
    "         \"messages\": \n",
    "         [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "         {\"role\": \"user\", \"content\": prompt}],}}\n",
    "      lines.append(line)\n",
    "\n",
    "   ### Write batch input file\n",
    "   batch_file_path = f\"batch_data/{aspect}_batch_input.jsonl\"\n",
    "   with open(batch_file_path, 'w') as f:\n",
    "      for l in lines:\n",
    "         json.dump(l, f)\n",
    "         f.write('\\n')\n",
    "\n",
    "   ### upload the batch file\n",
    "   batch_input_file = client.files.create(\n",
    "   file=open(batch_file_path, \"rb\"),\n",
    "   purpose=\"batch\")\n",
    "\n",
    "   ### create the batch request\n",
    "   batch_input_file_id = batch_input_file.id\n",
    "   batch_data = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\",\n",
    "      metadata={\n",
    "         \"description\": f\"batch file for  {aspect} model gpt-4o, temperature 0.1\"\n",
    "      })\n",
    "   batch_metadata = {\n",
    "      \"batch_id\": batch_data.id,\n",
    "      \"aspect\": aspect,\n",
    "      \"prompt_type\": prompt_type,\n",
    "      \"batch_input_file_id\": batch_input_file_id,\n",
    "      \"batch_file_path\": batch_file_path\n",
    "   }\n",
    "\n",
    "   with open(f\"batch_data/{aspect}_batch_input_meta_data.json\", 'w') as f:\n",
    "      json.dump(batch_metadata, f, indent=4)\n",
    "      \n",
    "   print(f\"Batch file for {aspect} and {prompt_type} has been created and uploaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will run the batch inference, and save the meta-data for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rationale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rationale'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_data) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     test_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maspect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mchatgpt_batch_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefinitions_incontext_learning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 6\u001b[0m, in \u001b[0;36mchatgpt_batch_inf\u001b[0;34m(test_data, aspect, prompt_type, save_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,row \u001b[38;5;129;01min\u001b[39;00m test_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m    review_point \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_point\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m    prompt \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_point\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreview_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_context_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_context_examples\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m      7\u001b[0m    line \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      9\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m       [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     15\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],}}\n\u001b[1;32m     16\u001b[0m    lines\u001b[38;5;241m.\u001b[39mappend(line)\n",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m, in \u001b[0;36mget_prompt\u001b[0;34m(review_point, aspect, prompt_type, in_context_examples)\u001b[0m\n\u001b[1;32m     16\u001b[0m             row \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m             score \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maspect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m             rationale \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrationale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m             examples_str\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mReview Point: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_point\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mrationale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrationale\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mscore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m## shuffle the list \u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rationale'"
     ]
    }
   ],
   "source": [
    "# aspects = ['actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "aspects = [ 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "\n",
    "\n",
    "for aspect in aspects:\n",
    "    test_data = pd.read_csv(f\"test_data/{aspect}_test_data.csv\")\n",
    "\n",
    "    ### check if test_data has id column and add one if not\n",
    "    if 'id' not in test_data.columns:\n",
    "        test_data['id'] = range(1, len(test_data) + 1)\n",
    "        test_data.to_csv(f\"test_data/{aspect}_test_data.csv\", index=False)\n",
    "        \n",
    "    chatgpt_batch_inf(test_data=test_data,aspect=aspect, prompt_type='definitions_incontext_learning', save_path='results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data):\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    batch_id = batch_data['batch_id']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = f\"batch_output/{aspect}_chatgpt_output.jsonl\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_results(batch_data):\n",
    "\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    chatgpt_response = json.load(open(f\"batch_output/{aspect}_chatgpt_output.jsonl\"))\n",
    "    raw_data_df = pd.read_csv(f\"test_data/{aspect}_test_data.csv\")\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "        id = raw_data_df.iloc[i]['id']\n",
    "        chatgpt_row = chatgpt_response[chatgpt_response['id']==id]\n",
    "\n",
    "\n",
    "        answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "        match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            json_obj = json.loads(json_str)  # Convert to Python dictionary\n",
    "            chat_gpt_score = json_obj['score']\n",
    "            chat_gpt_rationale = json_obj['rationale']\n",
    "\n",
    "            row = raw_data_df.iloc[i]\n",
    "            row [f'chatgpt_{aspect}_{prompt_type}_score'] = chat_gpt_score\n",
    "            row [f'chatgpt_{aspect}_{prompt_type}_rationale'] = chat_gpt_rationale\n",
    "            final_df.append(row)\n",
    "\n",
    "        else:\n",
    "            print(\"No valid JSON found.\")\n",
    "            continue\n",
    "    final_df = pd.DataFrame(final_df)\n",
    "    final_df.to_csv('outputs/actionability_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this every once and a while, and it will check if the batch file has been completed, and if so, it will retrieve the results and save them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir('batch_data'):\n",
    "    if 'batch_input_meta_data.json' in file:\n",
    "        batch_data = json.load(open(f'batch_data/{file}'))\n",
    "\n",
    "        if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "            print(f\"Batch file for {batch_data['aspect']} and {batch_data['prompt_type']} has been completed\")\n",
    "            retrive_batch_and_save_results(batch_data)\n",
    "            save_results(batch_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect,prompt_type, in_context_examples):\n",
    "\n",
    "    prompt = ''\n",
    "    if prompt_type == 'definitions':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[f'{aspect}_no_examples'])\n",
    "\n",
    "    elif prompt_type == 'definitions_examples':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect])\n",
    "    elif prompt_type == 'definitions_incontext_learning' or prompt_type == 'chain_of_thoughts':\n",
    "        examples = ''\n",
    "        examples_str = []\n",
    "        ##3 group examples by the label and choose a random example from each group\n",
    "        for label in in_context_examples[f'{aspect}_label'].unique():\n",
    "            ## keep sampling a line till it is not the same as the currrent review point\n",
    "            while True:\n",
    "                row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                row = row.iloc[0]\n",
    "                if row['review_point'] != review_point:\n",
    "                    break\n",
    "            \n",
    "            score = row[f'{aspect}_label']\n",
    "            rationale = row['rationale'] if prompt_type == 'definitions_incontext_learning' else row['chain_of_thoughts']\n",
    "\n",
    "\n",
    "            examples_str.append(f'''\n",
    "Review Point: {row['review_point']}\n",
    "rationale: {rationale}\n",
    "score: {score}\n",
    "''')\n",
    "        ## shuffle the list \n",
    "        random.shuffle(examples_str)\n",
    "        examples = '\\n'.join(examples_str)\n",
    "\n",
    "        prompt = BASE_PROMPT_EXAMPLES.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect],examples=examples)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Chatgpt Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "prompt_types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "\n",
    "def chatgpt_inf(test_data, prompt_types, save_path):\n",
    "## iterate over the df \n",
    "    for aspect in test_data.keys():\n",
    "        responses = []\n",
    "        fails = 0\n",
    "        examples  = test_data[aspect]['examples']\n",
    "        data = test_data[aspect]['data']\n",
    "        for idx ,row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            for prompt_type in prompt_types:\n",
    "                \n",
    "                review_point = row['review_point']\n",
    "                prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, examples=examples)\n",
    "                # print(prompt_type)\n",
    "                # print(prompt)\n",
    "\n",
    "                ## call the gpt4 model,and output the error message if the call fails\n",
    "                try:\n",
    "                    clue_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    completion = client.chat.completions.create(\n",
    "                    response_format={ \"type\": \"json_object\" },\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        clue_message\n",
    "                    ]\n",
    "                    )\n",
    "                    response = json.loads(completion.choices[0].message.content.lower())\n",
    "                    score = response['score']\n",
    "                    rationale = response['rationale']\n",
    "                except Exception as e:      \n",
    "                    print(f\"Failed for {idx} and {aspect} with error {e}\")\n",
    "                    fails += 1\n",
    "                    score = 'NA'\n",
    "                    rationale = 'NA'\n",
    "\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_score'] = score\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_rationale'] = rationale\n",
    "            responses.append(row)\n",
    "        responses = pd.DataFrame(responses)\n",
    "        responses.to_csv(f\"{save_path}/{aspect}_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chatgpt_batch_inf(test_data,aspect, prompt_type, save_path, temp=0):\n",
    "   in_context_examples =  all_incontext_examples[aspect]\n",
    "   lines = []\n",
    "\n",
    "   for i,row in test_data.iterrows():\n",
    "      review_point = row['review_point']\n",
    "      prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, in_context_examples=in_context_examples)   \n",
    "      line = {\n",
    "         \"custom_id\": f\"{row['id']}\", \n",
    "         \"method\": \"POST\", \n",
    "         \"url\": \"/v1/chat/completions\", \n",
    "         \"body\": {\"model\": model_name,\n",
    "         \"response_format\" :{ \"type\": \"json_object\" },\n",
    "         \"temperature\": temp,\n",
    "         \"messages\": \n",
    "         [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "         {\"role\": \"user\", \"content\": prompt}],}}\n",
    "      lines.append(line)\n",
    "\n",
    "   print(f'sample of the prompts is {lines[0]}')\n",
    "\n",
    "   ### Write batch input file\n",
    "   batch_file_path = f\"batch_data/{aspect}_batch_input.jsonl\"\n",
    "   with open(batch_file_path, 'w') as f:\n",
    "      for l in lines:\n",
    "         json.dump(l, f)\n",
    "         f.write('\\n')\n",
    "\n",
    "   ### upload the batch file\n",
    "   batch_input_file = client.files.create(\n",
    "   file=open(batch_file_path, \"rb\"),\n",
    "   purpose=\"batch\")\n",
    "\n",
    "   ### create the batch request\n",
    "   batch_input_file_id = batch_input_file.id\n",
    "   batch_data = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\",\n",
    "      metadata={\n",
    "         \"description\": f\"batch file for  {aspect} model gpt-4o, temperature {temp}\"\n",
    "      })\n",
    "   batch_metadata = {\n",
    "      \"batch_id\": batch_data.id,\n",
    "      \"aspect\": aspect,\n",
    "      \"prompt_type\": prompt_type,\n",
    "      \"batch_input_file_id\": batch_input_file_id,\n",
    "      \"batch_file_path\": batch_file_path\n",
    "   }\n",
    "\n",
    "   with open(f\"batch_data/{aspect}_batch_input_meta_data.json\", 'w') as f:\n",
    "      json.dump(batch_metadata, f, indent=4)\n",
    "      \n",
    "   print(f\"Batch file for {aspect} and {prompt_type} has been created and uploaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will run the batch inference, and save the meta-data for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of the prompts is {'custom_id': '1', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o', 'response_format': {'type': 'json_object'}, 'temperature': 0.0, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in evaluating peer review comments with respect to different aspects.\\n'}, {'role': 'user', 'content': '\\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: “Will this review point help authors improve their draft?”. There is no correlation between the aspect score and the length of the review point.\\n\\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\\n    \"rationale\": RATIONALE\\n    \"score\": SCORE_VALUE,\\n    \\nactionability:\\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \\n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \\n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\\n- It’s more important for actions to be concrete so the authors know how to apply them. It’s also preferred that actions be stated directly rather than inferred.\\nDefinitions:\\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\\nVague: After Identifying the action, the authors still don’t know how to carry out this action.\\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\\n1: Unactionable\\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\\nExamples: \\nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\\n2: Borderline Actionable\\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\\nExamples:\\nIt is not clear if this trend holds across different model architectures.\\nI wonder what would happen if you used another dataset.\\nI am curious about the outcome of changing some of the hyperparameter values.\\n3: Somewhat Actionable\\nDefinition: The comment explicitly states an action but is vague on how to execute it.\\nExamples:\\nYou should address the lack of technical novelty in this paper.\\nI think some parameter values should be looked at.\\nComparing your method to other previous related work is highly suggested.\\n4: Mostly Actionable\\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\\nExamples:\\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don\\'t, which affects beauty.\\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\\n5: Highly Actionable\\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\\nExamples:\\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\\nWhat will happen if you use the evaluation metric X instead of Y?\\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\\n\\n\\nFirst output the rationale for the score you have given and then the score value.\\n\\n\\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break it down step by step:\\n\\n1. **Concrete vs. Vague**: \\n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\\n\\n2. **Explicit vs. Implicit**:\\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\\n\\n3. **Conclusion**:\\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\\nscore: 3\\n\\n\\nReview Point: - the required implicit call to the Witness oracle is confusing.\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break it down step by step:\\n\\n1. **Concrete vs. Vague**: \\n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\\n\\n2. **Explicit vs. Implicit**:\\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\\n\\n3. **Conclusion**:\\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\\nscore: 1\\n\\n\\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break it down step by step:\\n\\n1. **Concrete vs. Vague**: \\n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\\n\\n2. **Explicit vs. Implicit**:\\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\\n\\n3. **Conclusion**:\\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\\nscore: 2\\n\\n\\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break it down step by step:\\n\\n1. **Concrete vs. Vague**: \\n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\\n\\n2. **Explicit vs. Implicit**:\\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\\n\\n3. **Conclusion**:\\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\\nscore: 4\\n\\n\\nReview Point: - Finally, and similarly to above, i’d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range “inference” capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I’d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break it down step by step:\\n\\n1. **Concrete vs. Vague**: \\n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\\n\\n2. **Explicit vs. Implicit**:\\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\\n\\n3. **Conclusion**:\\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\\nscore: 5\\n\\n\\nReview Point:\\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\\n\\n'}]}}\n",
      "Batch file for actionability and chain_of_thoughts has been created and uploaded successfully\n",
      "sample of the prompts is {'custom_id': '1', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o', 'response_format': {'type': 'json_object'}, 'temperature': 0.0, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in evaluating peer review comments with respect to different aspects.\\n'}, {'role': 'user', 'content': '\\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: “Will this review point help authors improve their draft?”. There is no correlation between the aspect score and the length of the review point.\\n\\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\\n    \"rationale\": RATIONALE\\n    \"score\": SCORE_VALUE,\\n    \\ngrounding_specificity:\\nThis aspect measures how explicitly a review comment is based on a part of the paper. This is important so the authors know which part of their paper causes the issue and needs to be revised. Further, it measures how specifically the comment identifies what is the issue with this part of the paper. This aspect has two dimensions: (1) what part of the paper does this comment address, and (2) what is wrong with this part?\\nDefinitions:\\nGrounding: Measures how well the authors can identify what is being addressed by the comment. (This can be no grounding, weak grounding, or full grounding). \\n* Weak grounding means that the author can’t precisely identify the part of the paper being addressed by the point, but they have some hint or guess about it. \\n* Full grounding means the authors can accurately identify which part is being addressed. This can be done by:\\n   - Making literal mentions of sections, tables, figures, etc.\\n     - The point discusses something unique to the paper that the authors can identify.\\n     - General comments that do not need to mention specific parts of the paper, but the authors can easily infer which parts are addressed\\nSpecificity: Measures how much the reviewer detailed what is wrong/missing in this area. If the comment mentions some external work, it also measures whether it mentions specific examples. \\n  Grounding and specificity is rated on a scale from 1-5, and we will now provide a definition for each.\\n1: Not Grounded\\nDefinition: This comment is not grounded at all. It does not identify a specific area in the paper. The comment is highly unspecific.\\nExamples\\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\\nThe paper is not very well-written, possibly hurriedly written, so it is not easy to read. A lot is left desired in presentation and formatting, especially in figures/tables.\\nThe paper discusses a hot topic in the field now. However, one major drawback to this draft is that the analysis is poor.\\n2: Weakly grounded and not specific\\nDefinition: The authors can not confidently determine which part the comment addresses. Further, the comment does not specify what needs to be addressed in this part.\\nExamples\\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal. \\nAdd more details while describing your method\\nSomething appears off when reading some figures’ captions.\\n3: Weakly grounded and specific\\nDefinition: The authors can not confidently determine which part the comment addresses. However, the comment clearly specifies what needs to be addressed in this part.\\nExamples\\nsome figures need their captions to be more precise and to define all variables used in the figure.\\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\\nThe notation used for the equations is not the same, and it varies between different equations.\\n4: Fully grounded and under-specific\\nDefinition: The comment explicitly mentions which part of that paper it addresses, or it should be obvious to the authors. However, this comment does not specify what needs to be addressed in this part.\\nExamples\\nIn Figure 7, the results and supplemental video results show that SurfGAN seems out of place.\\nI don’t like the formatting of L125.\\nThe relationship between this work and the previous methods is not exposed.\\n5: Fully grounded and specific\\nDefinition: The comment explicitly mentions which part of that paper it addresses, it is obvious to the authors. The comment specifies what needs to be addressed in this part.\\nExamples\\nIn defining the UFE-layer as graph-based aggregation, the paper presents the motivation that an \\'observation map\\' type approach as used in older single-pixel methods (like [4]) is sub-optimal, because it requires a resolution trade-off. This is not experimentally demonstrated—and should be, because it is central to the whole \\'graph-based\\' premise.\\nI think it would be extremely useful if the authors could evaluate different types of self-supervised features, besides DINO. Other options include MoCo-v3 or MSN, which are all also based on the ViT architecture.\\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\\n\\n\\nFirst output the rationale for the score you have given and then the score value.\\n\\n\\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\\nrationale: *Grounding:**\\n- The comment mentions \"dedicating a whole section of the paper plus experimental results,\" which implies a reference to a specific part of the paper. However, it does not provide any explicit identifiers such as section names, numbers, or unique content that would allow the authors to accurately identify the part being addressed.\\n- Without any specific references or unique identifiers, the authors cannot determine with confidence which section or content the comment is addressing. This lack of clarity indicates that the comment is not grounded.\\n\\n**Specificity:**\\n- The comment provides a general critique about the space dedicated to assumptions and experimental results but does not specify what is wrong or missing. It lacks details on why the space is considered excessive or how it could be improved.\\n- There are no specific examples, suggestions, or detailed explanations provided, indicating a lack of specificity.\\n\\n**Conclusion:**\\n- Given the analysis, the comment is not grounded because the authors cannot identify with confidence which part of the paper is being addressed. Additionally, the comment is not specific as it does not detail what needs to be addressed or improved.\\n- Therefore, the appropriate label for this comment is \"1: Not Grounded.\"\\n\\nscore: 1\\n\\n\\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however,  somehting needs to be done about this.\\nrationale: **Grounding:**\\n1. **Identification of the Part Addressed:** The comment mentions \"using integrated gradients to measure the attribution\" and \"post-processing steps to filter out the \\'false-positive\\' neurons.\" These references give the authors some indication of the areas being discussed, likely sections of the paper where these methods are described.\\n2. **Grounding Level:** The authors have a hint about which part of the paper is being addressed due to the mention of specific methods and processes. However, the comment does not provide explicit references to sections, tables, or figures, nor does it discuss something unique to the paper that would allow for full grounding. Therefore, the grounding is weak.\\n\\n**Specificity:**\\n1. **Detailing What is Wrong/Missing:** The comment states that \"something needs to be done about this\" regarding the post-processing steps but does not specify what exactly needs to be addressed or improved.\\n2. **Specificity Level:** The comment lacks detail about what is wrong or missing in the post-processing steps. It does not provide specific suggestions or examples of what should be changed or considered. Thus, the comment is not specific.\\n\\n**Conclusion:**\\n- The comment is weakly grounded because the authors can guess the general area being addressed but cannot pinpoint it with confidence.\\n- The comment is not specific because it fails to detail what needs to be addressed or improved.\\n\\nGiven this analysis, the label \"2: Weakly grounded and not specific\" is appropriate for this comment.\\nscore: 2\\n\\n\\nReview Point: 1) Is it necessary to treat concept map extraction like the way it is? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\\nrationale: ### Step 1: Evaluate Grounding\\n\\n- **Grounding Definition**: Grounding measures how well the authors can identify what is being addressed by the comment. Full grounding means the authors can accurately identify which part is being addressed, either through literal mentions or by discussing something unique to the paper.\\n\\n- **Comment Analysis**: The comment explicitly mentions \"concept map extraction,\" which is a specific aspect of the paper. This makes it clear to the authors which part of the paper is being addressed.\\n\\n- **Conclusion on Grounding**: The comment is fully grounded because it directly identifies the \"concept map extraction\" part of the paper.\\n\\n### Step 2: Evaluate Specificity\\n\\n- **Specificity Definition**: Specificity measures how much the reviewer detailed what is wrong or missing. A specific comment provides clear guidance or examples of what needs to be addressed.\\n\\n- **Comment Analysis**: The comment raises questions about the necessity of the current approach to concept map extraction and mentions potential issues with readability as the node number increases. However, it does not provide specific suggestions or detailed guidance on how to address these issues.\\n\\n- **Conclusion on Specificity**: The comment is under-specific because it highlights potential issues but lacks detailed suggestions or specific guidance on what needs to be addressed.\\n\\n### Step 3: Conclusion\\n\\n- **Overall Evaluation**: The comment is fully grounded because it clearly identifies the specific part of the paper being addressed. However, it is under-specific because it does not provide detailed suggestions or specific guidance on what needs to be addressed.\\n\\n- **Final Categorization**: Based on the analysis, the comment is best categorized as \"4: Fully grounded and under-specific.\"\\nscore: 4\\n\\n\\nReview Point: 1. The authors should make clear the distinction of when the proposed method is trained using only weak supervision and when it is semi-supervised trained. For instance, in Table 1, I think the proposed framework row refers to the semi-supervised version of the method, thus the authors should rename the column to ‘Fully supervised’ from ‘Supervised’. Maybe a better idea is to specify the data used to train ALL the parts of each model and have two big columns ‘Mixture training data’ and ‘Single source data’ which will make it much more prevalent of what is which.\\nrationale: ### Grounding:\\n1. **Identification of Specific Part**: The comment explicitly mentions \"Table 1,\" which is a specific part of the paper. This allows the authors to accurately identify the section being addressed.\\n2. **Unique Discussion**: The comment discusses the distinction between weak supervision and semi-supervised training, which is likely a unique aspect of the paper\\'s methodology. This further helps in grounding the comment.\\n\\nGiven these points, the comment is fully grounded because it clearly identifies the specific part of the paper being addressed.\\n\\n### Specificity:\\n1. **Details on What Needs to be Addressed**: The comment specifies that the authors should clarify the distinction between weak supervision and semi-supervised training. It also suggests renaming a column in Table 1 and provides a detailed suggestion for restructuring the table with two big columns: \\'Mixture training data\\' and \\'Single source data\\'.\\n2. **Clear Instructions**: The comment provides clear instructions on what changes are needed, making it specific.\\n\\nBased on these observations, the comment is specific because it details what is wrong and provides concrete suggestions for improvement.\\n\\n### Conclusion:\\nThe review comment is both fully grounded and specific. It identifies the exact part of the paper being addressed (Table 1) and provides detailed instructions on what needs to be changed and how.\\nscore: 5\\n\\n\\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)\\'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)\\'s same feature set could also improve the results of this work.\\nrationale: ### Grounding:\\n1. **Identification of Specific Part**: The comment does not explicitly mention a specific section, table, or figure in the paper. However, it discusses the potential improvement of the system\\'s performance, which is likely a unique aspect of the paper that the authors can identify.\\n2. **Inference by Authors**: The authors can infer that the comment addresses the section of the paper where the system\\'s performance is discussed, even though it is not explicitly mentioned. This suggests weak grounding, as the authors have a hint about the relevant part but cannot pinpoint it with certainty.\\n\\n### Specificity:\\n1. **Detailing What is Wrong/Missing**: The comment specifies that the use of feature engineering, particularly the feature set from Uto et al. (2020), might improve the system\\'s performance. This provides a clear direction for what could be addressed or explored further.\\n2. **Mention of External Work**: The comment references Uto et al. (2020) and provides a specific example of a feature set that achieved a QWK of 0.801, adding specificity to the suggestion.\\n\\n### Conclusion:\\n- **Grounding**: The comment is weakly grounded because it does not explicitly identify a specific part of the paper, but the authors can infer the relevant section.\\n- **Specificity**: The comment is specific in suggesting the use of feature engineering and referencing Uto et al. (2020) as an example.\\n\\nGiven this analysis, the label \"3: Weakly grounded and specific\" is appropriate because the comment lacks explicit grounding but provides specific guidance on what could be improved.\\nscore: 3\\n\\n\\nReview Point:\\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\\n\\n'}]}}\n",
      "Batch file for grounding_specificity and chain_of_thoughts has been created and uploaded successfully\n",
      "sample of the prompts is {'custom_id': '1', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o', 'response_format': {'type': 'json_object'}, 'temperature': 0.0, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in evaluating peer review comments with respect to different aspects.\\n'}, {'role': 'user', 'content': '\\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: “Will this review point help authors improve their draft?”. There is no correlation between the aspect score and the length of the review point.\\n\\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\\n    \"rationale\": RATIONALE\\n    \"score\": SCORE_VALUE,\\n    \\nverifiability:\\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims\\' justification or validation can come before or after the claim. Claims don’t need to be stated directly; they can also be inferred.\\nDefinitions:\\nOpinion & Claims\\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\\n    - Ex: “Important methods like X are not discussed”. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\\nVerification\\nThe claim is verified by providing logical reasoning.\\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\\nThe claim is verified by providing external references.\\nNormal Statements\\nNormal statements should be given the label \"No Claim\".\\nIndicating that something exists, or missing without indicating that it should be removed or included.\\nGeneral statements about the paper, that don’t include an opinion.\\nObjective and factual statements that don’t need any kind of verification.\\nAsking for clarifications and general questions.\\nLogical statements, or things that can be inferred directly.\\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\\n    - Example: This paper is well written, and the experimentation methods are well designed.\\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\\n1: Unverifiable\\nDefinition: The comment contains a claim without any supporting evidence or justification.\\nExamples\\nThe results fall behind previous work, and the reasons for this should be investigated.\\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\\n2: Borderline Verifiable\\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\\nExamples\\nThis method shouldn’t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn’t work for them.\\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\\nIn the experiments, the transfer tasks are too artificial. “At the pretraining stage, we train the models with examples from two classes (“bird\" vs. “frog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST”.\\n3: Somewhat Verifiable\\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\\nExamples\\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\\n4: Mostly Verifiable\\nDefinition: The comment’s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\\nExamples\\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\\n5: Fully Verifiable\\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\\nClear and precise reasoning or explanation.\\nReferences to external works/data, when applicable, are specific and relevant.\\nCommon-sense arguments are logically unassailable.\\nExamples\\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\\nThe paper’s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\\n No Claim\\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\\nClear and precise reasoning or explanation.\\nReferences to external works/data, when applicable, are specific and relevant.\\nCommon-sense arguments are logically unassailable.\\nExamples\\nHow would this method perform empirically with multiple constraints?\\nEntropy requires significant computation.\\nThis algorithm is slow, as it relies on an O(N²) algorithm.\\n\\n\\nFirst output the rationale for the score you have given and then the score value.\\n\\n\\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\\n\\nNext, we examine how well this claim is verified:\\n\\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"\\'such distribution\\' cannot refer to the discussion above.\"\\n\\n2. **Verification Assessment**:\\n   - **Logical Explanation**: The reviewer\\'s comment that \"\\'such distribution\\' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\\n\\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer\\'s justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\\nscore: 3\\n\\n\\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\\nrationale: 1. **Identify the Presence of Claims:**\\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\\n\\n2. **Analysis:**\\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\\n\\n3. **Verification of Claims:**\\n   - Since there are no claims identified, there is nothing to verify.\\n\\n4. **Conclusion:**\\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\\nscore: X\\n\\n\\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\\nrationale: 1. **Identify Claims:**\\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper\\'s originality.\\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\\n\\n2. **Evaluate Verification:**\\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn\\'t elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\\n\\n3. **Assessment of Verification Level:**\\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\\n\\n4. **Conclusion:**\\n   - The review comment contains claims but insufficient verification for those claims.\\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\\nscore: 2\\n\\n\\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.\\nrationale: 1. **Identify Claims:**\\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\\n   - It then points out that \"the paper doesn’t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper\\'s analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\\n\\n2. **Verification of Claims:**\\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\\n\\n3. **Conclusion:**\\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\\n\\n4. **Final Label Decision:**\\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\\nscore: 1\\n\\n\\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\\nrationale: 1. **Identify Claims in the Comment:**\\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it\\'s not stating an objective fact; rather, it\\'s implying a potential issue with the dataset.\\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset\\'s quality and noise level.\\n\\n2. **Assess the Verification of the Claim:**\\n   - The comment provides reasoning by pointing out that the \"collected \\'pristine\\' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\\n\\n3. **Conclusion:**\\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\\nscore: 4\\n\\n\\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are â_x0080__x009c_attributeâ_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in â_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\\nrationale: 1. **Identifying Claims:**\\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\\n\\n2. **Assessing Verification:**\\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\\n\\n3. **Conclusion on Verification Level:**\\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\\n\\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\\nscore: 5\\n\\n\\nReview Point:\\n- It\\'s a bit unclear how the frame similarity factors and attributes similarity factors are selected.\\n\\n'}]}}\n",
      "Batch file for verifiability and chain_of_thoughts has been created and uploaded successfully\n",
      "sample of the prompts is {'custom_id': '1', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o', 'response_format': {'type': 'json_object'}, 'temperature': 0.0, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in evaluating peer review comments with respect to different aspects.\\n'}, {'role': 'user', 'content': '\\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: “Will this review point help authors improve their draft?”. There is no correlation between the aspect score and the length of the review point.\\n\\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\\n    \"rationale\": RATIONALE\\n    \"score\": SCORE_VALUE,\\n    \\nhelpfulness:\\nAssign a subjective score to reflect the value of the review comment to the authors.\\n\\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\\n\\n1. The comment is not helpful at all\\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\\nExamples\\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\\nIt might be good to add more comments.\\nIn the experiments, the transfer tasks are too artificial.\\n2. The comment is barely helpful\\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\\nExamples\\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\\\hat{d} ~ \\\\hat{D}. Whether they can be fused for learning.\\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\\n3. The comment is somewhat helpful\\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\\nExamples\\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren\\'t they needed during training? Are they fixed the same across the different setups?\\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\\n4. The comment is mostly helpful\\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\\nExamples\\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it\\'s optimal rate.\\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\\n5. The comment is highly helpful\\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\\nExamples\\nI think there’s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don’t include a lot of details about your proposed algorithm there.\\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\\n\\n\\nFirst output the rationale for the score you have given and then the score value.\\n\\n\\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\\nrationale: To properly evaluate the review comment using the given definitions, let\\'s apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\\n\\n1. **Identify Specificity:**\\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\\n\\n2. **Provide Justification:**\\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\\n\\n3. **Suggest Improvements:**\\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\\n\\n4. **Assess Actionability:**\\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback’s actionability is therefore minimal.\\n\\n**Conclusion:**\\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\\nscore: 2\\n\\n\\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\\nrationale: To evaluate the review comment according to the given definitions, let\\'s break down the criteria and assess each aspect:\\n\\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear—namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\\n\\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\\n\\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\\n\\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\\n\\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\\nscore: 5\\n\\n\\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\\nrationale: ### Step-by-Step Evaluation:\\n\\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\\n\\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\\n\\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\\n\\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\\n\\n### Conclusion:\\n\\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\\nscore: 3\\n\\n\\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\\nrationale: 1. **Identify Specificity:**  \\n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\\n\\n2. **Provide Justification:**  \\n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\\n\\n3. **Suggest Improvements:**  \\n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.” These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\\n\\n4. **Assess Actionability:**  \\n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\\n\\n**Conclusion:**  \\nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\\nscore: 4\\n\\n\\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons.\\nrationale: To evaluate the review comment according to the given definitions, let\\'s analyze and break down the criteria:\\n\\n### Step by Step Evaluation:\\n\\n1. **Identify Specificity**:  \\n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper\\'s proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper\\'s method compares or contrasts with existing studies.\\n\\n2. **Provide Justification**:  \\n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\\n\\n3. **Suggest Improvements**:  \\n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\\n\\n4. **Assess Actionability**:  \\n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn\\'t assist in enhancing the draft.\\n\\n### Conclusion:\\n\\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\\nscore: 1\\n\\n\\nReview Point:\\n- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.\\n\\n'}]}}\n",
      "Batch file for helpfulness and chain_of_thoughts has been created and uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "aspects = ['actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "# aspects = [ 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "# aspects = [ 'actionability']\n",
    "# prompt_type = 'chain_of_thoughts'\n",
    "# prompt_types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "prompt_type= 'chain_of_thoughts'\n",
    "\n",
    "for aspect in aspects:\n",
    "    test_data = pd.read_csv(f\"test_data/{aspect}_test_data.csv\")\n",
    "\n",
    "    ### check if test_data has id column and add one if not\n",
    "    if 'id' not in test_data.columns:\n",
    "        test_data['id'] = range(1, len(test_data) + 1)\n",
    "        test_data.to_csv(f\"test_data/{aspect}_test_data.csv\", index=False)\n",
    "        \n",
    "    chatgpt_batch_inf(test_data=test_data,aspect=aspect, prompt_type=prompt_type, temp=0.0, save_path='results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data):\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    batch_id = batch_data['batch_id']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = f\"batch_output/{aspect}_chatgpt_output.jsonl\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "def save_results(batch_data):\n",
    "\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    chatgpt_response = pd.read_json(f\"batch_output/{aspect}_chatgpt_output.jsonl\", lines=True)\n",
    "    raw_data_df = pd.read_csv(f\"test_data/{aspect}_test_data.csv\")\n",
    "    chatgpt_input = pd.read_json(f\"batch_data/{aspect}_batch_input.jsonl\", lines=True)\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "        id = raw_data_df.iloc[i]['id']\n",
    "        chatgpt_row = chatgpt_response[chatgpt_response['custom_id']==id]\n",
    "\n",
    "        input = chatgpt_input[chatgpt_input['custom_id']==id].iloc[0]['body']['messages'][1]['content']\n",
    "\n",
    "        chatgpt_row = chatgpt_row.iloc[0]\n",
    "\n",
    "        answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "        ## try to load the answer as a json object\n",
    "        try:\n",
    "            json_obj = json.loads(answer)\n",
    "            chat_gpt_score = json_obj['score']\n",
    "            chat_gpt_rationale = json_obj['rationale']\n",
    "            row = raw_data_df.iloc[i]\n",
    "            score_key = f'chatgpt_{aspect}_{prompt_type}_score'\n",
    "            rationale_key = f'chatgpt_{aspect}_{prompt_type}_rationale'\n",
    "            row [score_key] = chat_gpt_score\n",
    "            row [rationale_key] = chat_gpt_rationale\n",
    "            row ['prompt'] = input\n",
    "            final_df.append(row)\n",
    "\n",
    "        except:\n",
    "            print(\"No valid JSON found.\")\n",
    "            continue\n",
    "        # match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
    "        # if match:\n",
    "        #     json_str = match.group(1)\n",
    "        #     json_obj = json.loads(json_str)  # Convert to Python dictionary\n",
    "        #     chat_gpt_score = json_obj['score']\n",
    "        #     chat_gpt_rationale = json_obj['rationale']\n",
    "\n",
    "        #     row = raw_data_df.iloc[i]\n",
    "        #     row [f'chatgpt_{aspect}_{prompt_type}_score'] = chat_gpt_score\n",
    "        #     row [f'chatgpt_{aspect}_{prompt_type}_rationale'] = chat_gpt_rationale\n",
    "        #     row [f'prompt'] = input\n",
    "        #     final_df.append(row)\n",
    "\n",
    "        # else:\n",
    "        #     print(\"No valid JSON found.\")\n",
    "        #     continue\n",
    "    final_df = pd.DataFrame(final_df)\n",
    "    final_df.to_csv(f'outputs/{aspect}_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this every once and a while, and it will check if the batch file has been completed, and if so, it will retrieve the results and save them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file for grounding_specificity and chain_of_thoughts is still running\n",
      "Batch file for helpfulness and chain_of_thoughts is still running\n",
      "Batch file for actionability and chain_of_thoughts is still running\n",
      "Batch file for verifiability and chain_of_thoughts is still running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "for file in os.listdir('batch_data'):\n",
    "    if 'batch_input_meta_data.json' in file:\n",
    "        batch_data = json.load(open(f'batch_data/{file}'))\n",
    "\n",
    "        if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "            print(f\"Batch file for {batch_data['aspect']} and {batch_data['prompt_type']} has been completed\")\n",
    "\n",
    "            \n",
    "            retrive_batch_and_save_results(batch_data)\n",
    "            save_results(batch_data)\n",
    "        else:\n",
    "            print(f\"Batch file for {batch_data['aspect']} and {batch_data['prompt_type']} is still running\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

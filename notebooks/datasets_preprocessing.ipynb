{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "substan_train  = []\n",
    "substan_test = []\n",
    "\n",
    "with open('../data/substan-review/train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        substan_train.append(json.loads(line))\n",
    "\n",
    "with  open('../data/substan-review/test.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        substan_test.append(json.loads(line))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.35k/1.35k [00:00<00:00, 7.24MB/s]\n",
      "Downloading data: 100%|██████████| 3.13G/3.13G [01:40<00:00, 31.2MB/s]\n",
      "Downloading data: 100%|██████████| 367M/367M [00:44<00:00, 8.29MB/s] \n",
      "Downloading data: 100%|██████████| 398M/398M [00:13<00:00, 30.3MB/s] \n",
      "Generating train split: 76273 examples [00:07, 10149.36 examples/s]\n",
      "Generating validation split: 9621 examples [00:01, 7328.75 examples/s]\n",
      "Generating test split: 10594 examples [00:01, 8080.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "reviewer2_data = load_dataset(\"GitBag/Reviewer2_PGE_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "substan_reviews = []\n",
    "\n",
    "for i in range(len(substan_train)):\n",
    "    substan_reviews.append(substan_train[i]['review'])\n",
    "for i in range(len(substan_test)):\n",
    "    substan_reviews.append(substan_test[i]['review'])\n",
    "\n",
    "print(len(substan_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for x in reviewer2_data:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [13:16<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "from rapidfuzz import fuzz\n",
    "matches = 0\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "match_dict = {}\n",
    "not_found_ids = []\n",
    "for i in tqdm(range(len(substan_reviews))):\n",
    "    # for j in tqdm(range(len(reviewer2_data['train']['review']))):\n",
    "    found = False\n",
    "    for split in reviewer2_data:\n",
    "\n",
    "        for review in reviewer2_data[split]['review']:\n",
    "            s1 = substan_reviews[i].lower()\n",
    "            s2 = review.lower()\n",
    "            if abs(len(s1) - len(s2)) > 100:\n",
    "                continue\n",
    "            # distance = normalized_levenshtein.distance(s1, s2)\n",
    "            sim = fuzz.ratio(s1, s2)\n",
    "            if sim >= 70.0:\n",
    "                match_dict[i] = split\n",
    "                matches += 1\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        not_found_ids.append(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "asap_data = []\n",
    "\n",
    "with open('../data/asap/dataset/aspect_data/review_with_aspect.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        asap_data.append(json.loads(line))\n",
    "\n",
    "aspects = {}\n",
    "\n",
    "for i in range(len(asap_data)):\n",
    "    t = asap_data[i]['text']\n",
    "    for label in asap_data[i]['labels']:\n",
    "        aspect = label[2]\n",
    "        begin = label[0]\n",
    "        end = label[1]\n",
    "        if aspect not in aspects:\n",
    "            aspects[aspect] = []\n",
    "        \n",
    "        aspects[aspect].append(t[begin:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample \n",
    "\n",
    "rep_neg = sample(aspects['replicability_negative'], 10)\n",
    "rep_pos = sample(aspects['replicability_positive'], 10)\n",
    "comp_neg = sample(aspects['meaningful_comparison_negative'], 10)\n",
    "comp_pos = sample(aspects['meaningful_comparison_positive'], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_neg = [[x, 'replicability_negative'] for x in rep_neg]\n",
    "rep_pos = [[x, 'replicability_positive'] for x in rep_pos]\n",
    "comp_neg = [[x, 'meaningful_comparison_negative'] for x in comp_neg]\n",
    "comp_pos = [[x, 'meaningful_comparison_positive'] for x in comp_pos]\n",
    "\n",
    "df = pd.DataFrame(rep_neg + rep_pos + comp_neg + comp_pos, columns=['text', 'aspect'])\n",
    "df.to_csv('asap_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviewer2_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGitBag/Reviewer2_PGE_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "reviews = pd.read_csv(\"ARR22_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_reviews = reviews.sample(n=10)\n",
    "random_reviews = reviews.iloc[[63,109,175,661,525,57,392,488,238,136]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63     The paper proposed an unsupervised summarizati...\n",
       "109    This paper proposes a method to improve the pe...\n",
       "175    This paper deals with the problem of table-bas...\n",
       "661    This paper compared the self-attention functio...\n",
       "525    This paper proposes to examine the problem of ...\n",
       "57     This paper introduces a domain-specific model ...\n",
       "392    The paper investigates the transferability of ...\n",
       "488    This paper proposes a novel supervised contras...\n",
       "238    This paper claims the Distinctâ€™s bias that ten...\n",
       "136    This paper studies style transfer for language...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_reviews['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "\n",
    "\n",
    "base_prompt = \"\"\"You are an assistant with the task to evaluate a scientefic doucment review with respect to different aspects.\n",
    "The aspects are:\n",
    "Actionability: How much does the review communicate to the authors the actions thety should take to improve the draft.\n",
    "Constructive: How much does the review convey the feedback in a helpful and a constractive way.\n",
    "Specificity: How much the review addresses specific elements in the draft, and not just mentions general points. \n",
    "\n",
    "Review the given review, and output one score for each point mentioned above. The scores should be in the range from 0 to 5, with 0 being the lowest, and 5 being the best.\n",
    "\n",
    " \n",
    "Review: {review}\"\"\"\n",
    "\n",
    "base_prompt2 = \"\"\"I will give you a review of a scientific document. Please evaluate the review with respect to the following aspects:\n",
    "you should give a score from 0 to 5 for each aspect.\n",
    "The aspects are:\n",
    "Actionability: How much does the review communicate to the authors the actions thety should take to improve the draft.\n",
    "Constructive: How much does the review convey the feedback in a helpful and a constractive way.\n",
    "Specificity: How much the review addresses specific elements in the draft, and not just mentions general points. \n",
    "\n",
    " \n",
    "Review: {review}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "responses = []\n",
    "\n",
    "for idx ,review in tqdm(enumerate(random_reviews['review'])):\n",
    "    \n",
    "    prompt = base_prompt2.format(review = review)\n",
    "    try:\n",
    "      # correct_answers.append(clue[\"target\"])\n",
    "        clue_message = {\"role\": \"user\", \"content\": prompt}\n",
    "        completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            clue_message\n",
    "        ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content.lower()\n",
    "\n",
    "    except:\n",
    "        response = \"Sorry, I don't know that one.\"\n",
    "\n",
    "    responses.append(response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('temp.txt', 'w') as f:\n",
    "    for review in random_reviews['review']:\n",
    "        review = review.replace('\\n', ' ')\n",
    "        f.write(review + '\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: \n",
      "actionability: 2\n",
      "constructive: 1\n",
      "specificity: 3\n",
      "\n",
      "overall score: 2\n",
      "\n",
      "comments: the review could have been more constructive in providing suggestions for improvement, rather than focusing solely on the lack of novelty. it does address specific elements in the draft, which is a strength. however, it could have been more helpful in guiding the authors on how to enhance their work.\n",
      "\n",
      "\n",
      "\n",
      "Review 2: \n",
      "actionability: 3\n",
      "constructive: 4\n",
      "specificity: 3\n",
      "\n",
      "overall score: 3.3\n",
      "\n",
      "\n",
      "\n",
      "Review 3: \n",
      "actionability: 4\n",
      "constructive: 3\n",
      "specificity: 3\n",
      "\n",
      "overall score: 3.3\n",
      "\n",
      "\n",
      "\n",
      "Review 4: \n",
      "actionability: 3\n",
      "constructive: 4\n",
      "specificity: 4\n",
      "\n",
      "overall score: 3.67\n",
      "\n",
      "\n",
      "\n",
      "Review 5: \n",
      "score for actionability: 3\n",
      "score for constructive: 4\n",
      "score for specificity: 2\n",
      "\n",
      "overall, the review provides some constructive feedback on the paper, particularly in pointing out the lack of significance tests and clarity in presenting results. however, it could benefit from more specific examples and suggestions on how to improve these aspects. the review does convey the feedback in a helpful way, but could be more detailed in providing actionable steps for the authors to take in addressing the issues raised.\n",
      "\n",
      "\n",
      "\n",
      "Review 6: \n",
      "actionability: 4\n",
      "constructive: 5\n",
      "specificity: 3\n",
      "\n",
      "overall score: 4\n",
      "\n",
      "\n",
      "\n",
      "Review 7: \n",
      "actionability: 3\n",
      "constructive: 4\n",
      "specificity: 4\n",
      "\n",
      "overall score: 3.6\n",
      "\n",
      "\n",
      "\n",
      "Review 8: \n",
      "actionability: 3\n",
      "constructive: 4\n",
      "specificity: 4\n",
      "\n",
      "overall score: 4\n",
      "\n",
      "explanation: the review provides specific feedback on the results shown in table 2, mentions the lack of availability of code for reproducibility, and suggests improvements in the writing. the feedback is constructive and specific, offering clear areas for the authors to focus on for improvement. however, the feedback could be more actionable by providing more concrete suggestions for how to address the identified issues.\n",
      "\n",
      "\n",
      "\n",
      "Review 9: \n",
      "actionability: 4\n",
      "constructive: 5\n",
      "specificity: 5\n",
      "\n",
      "overall score: 4.67\n",
      "\n",
      "\n",
      "\n",
      "Review 10: \n",
      "actionability: 3\n",
      "constructive: 4\n",
      "specificity: 3\n",
      "\n",
      "overall score: 3.3\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, response in enumerate(responses):\n",
    "    print(f\"Review {idx+1}: \\n{response}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

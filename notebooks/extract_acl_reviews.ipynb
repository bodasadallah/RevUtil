{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper describes several ways to encode arbitrarily long sequences of digits using something called the major system. In the major system, each digit is mapped to one or more characters representing consonantal phonemes; the possible mappings between digit and phoneme are predefined. The output of an encoding is typically a sequence of words constrained such that digits in the original sequence correspond to characters or digraphs in the output sequence of words; vowels added surrounding the consonant phonemes to form words are unconstrained. This paper describes several ways to encode your sequence of digits such that the output sequence of words is more memorable, generally by applying syntactic constraints and heuristics.\n",
      "I found this application of natural language processing concepts somewhat interesting, as I have not read an ACL paper on this topic before. However, I found the paper and ideas presented here to have a rather old-school feel. With much of the focus on n-gram models for generation, frequent POS-tag sequences, and other heuristics, this paper really could have been written 15-20 years ago. I am not sure that there is enough novelty in the ideas here to warrant publication in ACL in 2017. There is no contribution to NLP itself, e.g. in terms of modeling or search, and not a convincing contribution to the application area which is just an instance of constrained generation.  Since you start with one sequence and output another sequence with a very straightforward monotonic mapping, it seems like a character-based sequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with Neural Networks; Sutskever et al. 2014) would work rather well here, very likely with very fluent output and fewer moving parts (e.g. trigram models and POS tag and scoring heuristics and postprocessing with a bigram model). You can use large amounts of training from an arbitrary genre and do not need to rely on an already-tagged corpus like in this paper, or worry about a parser. This would be a 2017 paper. \n",
      "-------------------\n",
      "The paper is clearly written, and the claims are well-supported.  The Related Work in particular is very thorough, and clearly establishes where the proposed work fits in the field.\n",
      "I had two main questions about the method: (1) phrases are mentioned in section 3.1, but only word representations are discussed.  How are phrase representations derived? \n",
      "(2) There is no explicit connection between M^+ and M^- in the model, but they are indirectly connected through the tanh scoring function.  How do the learned matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what would be the benefits/drawbacks of linking the two together directly, by enforcing some measure of dissimilarity?\n",
      "Additionally, statistical significance of the observed improvements would be valuable.\n",
      "Typographical comments: - Line 220: \"word/phase pair\" should be \"word/phrase pair\" - Line 245: I propose an alternate wording: instead of \"entities are translated to,\" say \"entities are mapped to\".  At first, I read that as a translation operation in the vector space, which I think isn't exactly what's being described.\n",
      "- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement in F-measure\" - Line 636: extraneous commas in citation - Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing) - Line 727: extraneous period and comma in citation \n",
      "-------------------\n",
      "This paper considers the problem of KB completion and proposes ITransF for this purpose. Unlike STransE that assigns each relation an independent matrix, this paper proposes to share the parameters between different relations. A model is proposed where a tensor D is constructed that contains various relational matrices as its slices and a selectional vector \\alpha is used to select a subset of relevant relational matrix for composing a particular semantic relation. The paper then discuss a technique to make \\alpha sparse. \n",
      "Experimental results on two standard benchmark datasets shows the superiority of ITransF over prior proposals.\n",
      "The paper is overall well written and the experimental results are good. \n",
      "However, I have several concerns regarding this work that I hope the authors will answer in their response.\n",
      "1. Just by arranging relational matrices in a tensor and selecting (or more appropriately considering a linearly weighted sum of the relational matrices) does not ensure any information sharing between different relational matrices. \n",
      "This would have been the case if you had performed some of a tensor decomposition and projected the different slices (relational matrices) into some common lower-dimensional core tensor. It is not clear why this approach was not taken despite the motivation to share information between different relational matrices. \n",
      "2. The two requirements (a) to share information across different relational matrices and (b) make the attention vectors sparse are some what contradictory. \n",
      "If the attention vector is truly sparse and has many zeros then information will not flow into those slices during optimisation. \n",
      "3. The authors spend a lot of space discussing techniques for computing sparse attention vectors. The authors mention in page 3 that \\ell_1 regularisation did not work in their preliminary experiments. However, no experimental results are shown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for this task. To this reviewer, it appears as an obvious baseline to try, especially given the ease of optimisation. You use \\ell_0 instead and get into NP hard optimisations because of it. Then you propose a technique and a rather crude approximation in the end to solve it. All that trouble could be spared if \\ell_1 was used. \n",
      "4. The vector \\alpha is performing a selection or a weighing over the slices of D. It is slightly misleading to call this as “attention” as it is a term used in NLP for a more different type of models (see attention model used in machine translation). \n",
      "5. It is not clear why you need to initialise the optimisation by pre-trained embeddings from TransE. Why cannot you simply randomly initialise the embeddings as done in TransE and then update them? It is not fair to compare against TransE if you use TransE as your initial point.\n",
      "Learning the association between semantic relations is an idea that has been used in related problems in NLP such as relational similarity measurement [Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It would be good to put the current work with respect to such prior proposals in NLP for modelling inter-relational correlation/similarity.\n",
      "Thanks for providing feedback. I have read it. \n",
      "-------------------\n",
      "The paper presents a new neural approach for summarization. They build on a standard encoder-decoder with attention framework but add a network that gates every encoded hidden state based on summary vectors from initial encoding stages. Overall, the method seems to outperform standard seq2seq methods by 1-2 points on three different evaluation sets.\n",
      "Overall, the technical sections of the paper are reasonably clear. Equation 16 needs more explanation, I could not understand the notation. The specific contribution,  the selective mechanism, seems novel and could potentially be used in other contexts.  The evaluation is extensive and does demonstrate consistent improvement. One would imagine that adding an additional encoder layer instead of the selective layer is the most reasonable baseline (given the GRU baseline uses only one bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch?  The quality of the writing, especially in the intro/abstract/related work is quite bad. This paper does not make a large departure from previous work, and therefore a related work nearby the introduction seems more appropriate. In related work, one common good approach is highlighting similarities and differences between your work and previous work, in words before they are presented in equations. Simply listing works without relating them to your work is not that useful. Placement of the related work near the intro will allow you to relieve the intro of significant background detail and instead focus on more high level. \n",
      "-------------------\n",
      "This paper develops an LSTM-based model for classifying connective uses for whether they indicate that a causal relation was intended. The guiding idea is that the expression of causal relations is extremely diverse and thus not amenable to syntactic treatment, and that the more abstract representations delivered by neural models are therefore more suitable as the basis for making these decisions.\n",
      "The experiments are on the AltLex corpus developed by Hidley and McKeown. The results offer modest but consistent support for the general idea, and they provide some initial insights into how best to translate this idea into a model. The paper distribution includes the TensorFlow-based models used for the experiments.\n",
      "Some critical comments and questions: - The introduction is unusual in that it is more like a literature review than a full overview of what the paper contains. This leads to some redundancy with the related work section that follows it. I guess I am open to a non-standard sort of intro, but this one really doesn't work: despite reviewing a lot of ideas, it doesn't take a stand on what causation is or how it is expressed, but rather only makes a negative point (it's not reducible to syntax). We aren't really told what the positive contribution will be except for the very general final paragraph of the section.\n",
      "- Extending the above, I found it disappointing that the paper isn't really clear about the theory of causation being assumed. The authors seem to default to a counterfactual view that is broadly like that of David Lewis, where causation is a modal sufficiency claim with some other counterfactual conditions added to it. See line 238 and following; that arrow needs to be a very special kind of implication for this to work at all, and there are well-known problems with Lewis's theory (see http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments elsewhere in the paper that the authors don't endorse the counterfactual view, but then what is the theory being assumed? It can't just be the temporal constraint mentioned on page 3!\n",
      "- I don't understand the comments regarding the example on line 256. The authors seem to be saying that they regard the sentence as false. If it's true, then there should be some causal link between the argument and the breakage. \n",
      "There are remaining issues about how to divide events into sub-events, and these impact causal theories, but those are not being discussed here, leaving me confused.\n",
      "- The caption for Figure 1 is misleading, since the diagram is supposed to depict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that this diagram is needlessly imprecise. I suppose it's okay to leave parts of the standard model definition out of the prose, but then these diagrams should have a clear and consistent semantics. What are all the empty circles between input and the \"LSTM\" boxes? The prose seems to say that the model has a look-up layer, a Glove layer, and then ... what? How many layers of representation are there? The diagram is precise about the pooling tanh layers pre-softmax, but not about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems like it's just the leftmost/final representation that is directly connected to the layers above. I suggest depicting that connection clearly.\n",
      "- I don't understand the sentence beginning on line 480. The models under discussion do not intrinsically require any padding. I'm guessing this is a requirement of TensorFlow and/or efficient training. That's fine. If that's correct, please say that. I don't understand the final clause, though. How is this issue even related to the question of what is \"the most convenient way to encode the causal meaning\"? I don't see how convenience is an issue or how this relates directly to causal meaning.\n",
      "- The authors find that having two independent LSTMs (\"Stated_LSTM\") is somewhat better than one where the first feeds into the second. This issue is reminiscent of discussions in the literature on natural language entailment, where the question is whether to represent premise and hypothesis independently or have the first feed into the second. I regard this as an open question for entailment, and I bet it needs further investigation for causal relations too. \n",
      "So I can't really endorse the sentence beginning on line 587: \"This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers.\" This is very surprising since we are talking about subparts of a sentence that might share a lot of information.\n",
      "- It's hard to make sense of the hyperparameters that led to the best performance across tasks. Compare line 578 with line 636, for example. Should we interpret this or just attribute it to the unpredictability of how these models interact with data?\n",
      "- Section 4.3 concludes by saying, of the connective 'which then', that the system can \"correctly disambiguate its causal meaning\", whereas that of Hidey and McKeown does not. That might be correct, but one example doesn't suffice to show it. To substantiate this point, I suggest making up a wide range of examples that manifest the ambiguity and seeing how often the system delivers the right verdict. This will help address the question of whether it got lucky with the example from table 8. \n",
      "-------------------\n",
      "This paper proposes a method for detecting causal relations between clauses, using neural networks (\"deep learning\", although, as in many studies, the networks are not particularly deep).  Indeed, while certain discourse connectives are unambiguous regarding the relation they signal (e.g. 'because' is causal) the paper takes advantage of a recent dataset (called AltLex, by Hidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal relations when the relation is not explicitly marked.  Arguing that convolutional networks are not as adept as representing the relevant features of clauses as LSTMs, the authors propose a classification architecture which uses a Glove-based representation of clauses, input in an LSTM layer, followed by three densely connected layers (tanh) and a final decision layer with a softmax.\n",
      "The best configuration of the system improves by 0.5-1.5% F1 over Hidey and MCkeown's 2016 one (SVM classifier).  Several examples of generalizations where the system performs well are shown (indicator words that are always causal in the training data, but are found correctly to be non causal in the test data). \n",
      "Therefore, I appreciate that the system is analyzed qualitatively and  quantitatively.\n",
      "The paper is well written, and the description of the problem is particularly clear. However a clarification of the differences between this task and the  task of implicit connective recognition would be welcome.  This could possibly  include a discussion of why previous methods for implicit connective  recognition cannot be used in this case.\n",
      "It is very appreciable that the authors uploaded their code to the submission site (I inspected it briefly but did not execute it).  Uploading the (older) data (with the code) is also useful as it provides many examples.  It was not clear to me what is the meaning of the 0-1-2 coding in the TSV files, given that the paper mentions binary classification. I wonder also, given that this is the data from Hidey and McKeown, if the authors have the right to repost it as they do.  -- One point to clarify in the paper would be the meaning of \"bootstrapping\", which apparently extends the corpus by about 15%: while the construction of the corpus is briefly but clearly explained in the paper, the additional bootstrapping is not.  While it is certainly interesting to experiment with neural networks on this task, the merits of the proposed system are not entirely convincing.  It seems indeed that the best configuration (among 4-7 options) is found on the test data, and it is this best configuration that is announced as improving over Hidey by \"2.13% F1\".  However, a fair comparison would involve selecting the best configuration on the devset.\n",
      "Moreover, it is not entirely clear how significant the improvement is. On the one hand, it should be possible, given the size of the dataset, to compute some statistical significance indicators.  On the other hand, one should consider also the reliability of the gold-standard annotation itself (possibly from the creators of the dataset).  Upon inspection, the annotation obtained from the English/SimpleEnglish Wikipedia is not perfect, and therefore the scores might need to be considered with a grain of salt.\n",
      "Finally, neural methods have been previously shown to outperform human engineered features for binary classification tasks, so in a sense the results  are rather a confirmation of a known property. It would be interesting to see experiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The analysis of results could try to explain why the neural method seems to favor  precision over recall. \n",
      "-------------------\n",
      "This paper describes a state-of-the-art CCG parsing model that decomposes into tagging and dependency scores, and has an efficient A* decoding algorithm. \n",
      "Interestingly, the paper slightly outperforms Lee et al. (2016)'s more expressive global parsing model, presumably because this factorization makes learning easier. It's great that they also report results on another language, showing large improvements over existing work on Japanese CCG parsing. One surprising original result is that modeling the first word of a constituent as the head substantially outperforms linguistically motivated head rules.  Overall this is a good paper that makes a nice contribution. I only have a few suggestions: - I liked the way that the dependency and supertagging models interact, but it would be good to include baseline results for simpler variations (e.g. not conditioning the tag on the head dependency).\n",
      "- The paper achieves new state-of-the-art results on Japanese by a large margin. However, there has been a lot less work on this data - would it also be possible to train the Lee et al. parser on this data for comparison?\n",
      "- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging models for CCG and SRL, and may be worth citing. \n",
      "-------------------\n",
      "This paper presents evaluation metrics for lyrics generation exploring the need for the lyrics to be original,but in a similar style to an artist whilst being fluent and co-herent. The paper is well written and the motivation for the metrics are well explained.   The authors describe both hand annotated metrics (fluency, co-herence and match) and an automatic metric for ‘Similarity'. Whilst the metric for Similarity is unique and interesting the paper does not give any evidence of this as an effective automatic metric as correlations between this metric and the others are low, (which they say that they should be used separately). The authors claim it can be used to meaningfully analyse system performance but we have to take their word for it as again there is no correlation with any hand-annotated performance metric.  Getting worse scores than a baseline system isn’t evidence that the metric captures quality (e.g. you could have a very strong baseline).\n",
      "Some missing references, e.g. recent work looking at automating co-herence, e.g. using mutual information density (e.g. Li et al. 2015). In addition, some reference to style matching from the NLG community are missing (e.g. Dethlefs et al. 2014 and the style matching work by Pennebaker). \n",
      "-------------------\n",
      "This paper studies how to properly evaluate systems that produce ghostwriting of rap lyrics. \n",
      "The authors present manual evaluation along three key aspects: fluency, coherence, and style matching. \n",
      "They also introduce automatic metrics that consider uniqueness via maximum training similarity, and stylistic similarity via rhyme density.\n",
      "I can find some interesting analysis and discussion in the paper. \n",
      "The way for manually evaluating style matching especially makes sense to me.\n",
      "There also exist a few important concerns for me.\n",
      "I am not convinced about the appropriateness of only doing fluency/coherence ratings at line level. \n",
      "The authors mention that they are following Wu (2014), but I find that work actually studying a different setting of hip hop lyrical challenges and responses, which should be treated at line level in nature. \n",
      "While in this work, a full verse consists of multiple lines that normally should be topically and structurally coherent. \n",
      "Currently I cannot see any reason why not to evaluate fluency/coherence for a verse as a whole.\n",
      "Also, I do not reckon that one should count so much on automatic metrics, if the main goal is to ``generate similar yet unique lyrics''. \n",
      "For uniqueness evaluation, the calculations are performed on verse level. \n",
      "However, many rappers may only produce lyrics within only a few specific topics or themes. \n",
      "If a system can only extract lines from different verses, presumably we might also get a fluent, coherent verse with low verse level similarity score, but we can hardly claim that the system ``generalizes'' well. \n",
      "For stylistic similarity with the specified artist, I do not think rhyme density can say it all, as it is position independent and therefore may not be enough to reflect the full information of style of an artist.\n",
      "It does not seem that the automatic metrics have been verified to be well correlated with corresponding real manual ratings on uniqueness or stylistic matching. \n",
      "I also wonder if one needs to evaluate semantic information commonly expressed by a specified rapper as well, other than only caring about rhythm.\n",
      "Meanwhile, I understand the motivation for this study is the lack of *sound* evaluation methodology. \n",
      "However, I still find one statement particularly weird: ``our methodology produces a continuous numeric score for the whole verse, enabling better comparison.'' \n",
      "Is enabling comparisons really more important than making slightly vague but more reliable, more convincing judgements?\n",
      "Minor issue: Incorrect quotation marks in Line 389 \n",
      "-------------------\n",
      "This paper proposes to present a more comprehensive evaluation methodology for the assessment of automatically generated rap lyrics (as being similar to a target artist).  While the assessment of the generation of creative work is very challenging and of great interest to the community, this effort falls short of its claims of a comprehensive solution to this problem.\n",
      "All assessment of this nature ultimately falls to a subjective measure -- can the generated sample convince an expert that the generated sample was produced by the true artist rather than an automated preocess?  This is essentially a more specific version of a Turing Test.   The effort to automate some parts of the evaluation to aid in optimization and to understand how humans assess artistic similarity is valuable.  However, the specific findings reported in this work do not encourage a belief that these have been reliably identified.\n",
      "Specifically -- Consider the central question: Was a sample generated by a target artist?        The human annotators who were asked this were not able to consistently respond to this question.        This means either 1) the annotators did not have sufficient expertise to perform the task, or 2) the task was too challenging, or some combination of the two.   The proposed automatic measures also failed to show a reliable agreement to human raters performing the same task.        This dramatically limits their efficacy in providing a proxy for human assessment.   The low interannotator agreement may be \"expected\" because the task is subjective, but the idea of decomposing the evaluation into fluency and coherence components is meant to make it more tractable, and thereby improve the consistency of rater scores.  A low IAA for an evaluation metric is a cause for concern and limits its viability as a general purpose tool.   Specific questions/comments: - Why is a line-by-line level evaluation prefered to a verse level analysis. \n",
      "Specifically for \"coherence\", a line by line analysis limits the scope of coherence to consequtive lines.\n",
      "- Style matching -- This term assumes that these 13 artists each have a distinct style, and always operate in that style. I would argue that some of these artists (kanye west, eminem, jay z, drake, tupac and notorious big) have produced work in multiple styles.  A more accurate term for this might be \"artist matching\".\n",
      "- In Section 4.2 The central automated component of the evaluation is low tf*idf with existing verses, and similar rhyme density.  Given the limitations of rhyme density -- how well does this work.  Even with the manual intervention described?\n",
      "- In Section 6.2 -- This description should include how many judges were used in this study? In how many cases did the judges already know the verse they were judging?  In this case the test will not assess how easy it is to match style, but rather, the judges recall and rap knowledge. \n",
      "-------------------\n",
      "The paper analyzes the story endings (last sentence of a 5-sentence story) in the corpus built for the story cloze task (Mostafazadeh et al. 2016), and proposes a model based on character and word n-grams to classify story endings. \n",
      "The paper also shows better performance on the story cloze task proper (distinguishing between \"right\" and \"wrong\" endings) than prior work.\n",
      "Whereas style analysis is an interesting area and you show better results than prior work on the story cloze task, there are several issues with the paper. \n",
      "First, how do you define \"style\"? Also, the paper needs to be restructured (for instance, your section \"Results\" actually mixes some results and new experiments) and clarified (see below for questions/comments): right now, it is quite difficult for the reader to follow what data is used for the different experiments, and what data the discussion refers to.\n",
      "(1) More details about the data used is necessary in order to assess the claim that \"subtle writing task [...] imposes different styles on the author\" (lines 729-732). How many stories are you looking at, written by how many different persons? And how many stories are there per person? From your description of the post-analysis of coherence, only pairs of stories written by the same person in which one was judged as \"coherent\" and the other one as \"neutral\" are chosen. Can you confirm that this is the case? So perhaps your claim is justified for your \"Experiment 1\". However my understanding is that in experiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs. \"wrong\", we do not have the same writers. So I am not convinced lines 370-373 are correct.\n",
      "(2) A lot in the paper is simply stated without any justifications. For instance how are the \"five frequent\" POS and words chosen? Are they the most frequent words/POS? ( Also theses tables are puzzling: why two bars in the legend for each category?). Why character *4*-grams? Did you tune that on the development set? If these were not the most frequent features, but some that you chose among frequent POS and words, you need to justify this choice and especially link the choice to \"style\". How are these features reflecting \"style\"?\n",
      "(3) I don't understand how the section \"Design of NLP tasks\" connects to the rest of the paper, and to your results. But perhaps this is because I am lost in what \"training\" and \"test\" sets refer to here.\n",
      "(4) It is difficult to understand how your model differs from previous work. \n",
      "How do we reconcile lines 217-219 (\"These results suggest that real understanding of text is required in order to solve the task\") with your approach?\n",
      "(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh et al., but this is a very bad choice of terms. What exactly does a \"right\" or \"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")? \n",
      "I took a quick look, but couldn't find the exact prompts given to the Turkers. \n",
      "I think this needs to be clarified: as it is, the first paragraph of your section \"Story cloze task\" (lines 159-177) is not understandable.\n",
      "Other questions/comments: Table 1. Why does the \"original\" story differ from the coherent and incoherent one? From your description of the corpus, it seems that one Turker saw the first 4 sentences of the original story and was then ask to write one sentence ending the story in a \"right\" way (or did they ask to provide a \"coherent\" ending?) and one sentence ending the story in a \"wrong\" way (or did they ask to provide an \"incoherent\" ending)? I don't find the last sentence of the \"incoherent\" story that incoherent... If the only shoes that Kathy finds great are $300, I can see how Kathy doesn't like buying shoes ;-) This led me to wonder how many Turkers judged the coherence of the story/ending and how variable the judgements were. What criterion was used to judge a story coherent or incoherent? Also does one Turker judge the coherence of both the \"right\" and \"wrong\" endings, making it a relative judgement? Or was this an absolute judgement? This would have huge implications on the ratings.\n",
      "Lines 380-383: What does \"We randomly sample 5 original sets\" mean?\n",
      "Line 398: \"Virtually all sentences\"? Can you quantify this?\n",
      "Table 5: Could we see the weights of the features?  Line 614: \"compared to ending an existing task\": the Turkers are not ending a \"task\" Line 684-686: \"made sure each pair of endings was written by the same author\" -> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\" pairs, according to your description.\n",
      "Line 694: \"shorter text spans\": text about what? This is unclear.\n",
      "Lines 873-875: where is this published? \n",
      "-------------------\n",
      "This paper proposes a method for building dialogue agents involved in a symmetric collaborative task, in which the agents need to strategically communicate to achieve a common goal.   I do like this paper.  I am very interested in how much data-driven techniques can be used for dialogue management.  However, I am concerned that the approach that this paper proposes, is actually not specific to symmetric collaborative tasks, but to tasks that can be represented as graph operations, such as finding an intersection between objects that the two people know about.\n",
      "In Section 2.1, the authors introduce symmetric collaborative dialogue setting. \n",
      " However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs explored (Cognition '86), and Walker's furniture layout task (Journal of Artificial Research '00).\n",
      "On line 229, the authors say that this domain is too rich for slot-value semantics.  However, their domain is based on attribute value pairs, so their domain could use a semantics represenation based on attribute value-pairs, such as first order logic.\n",
      "Section 3.2 is hard to follow.        The authors often refer to Figure 2, but I didn't find this example that helpful.        For example, for section 3.1, at what point of the dialogue does this represent?  Is this the same after `anyone went to columbia?' \n",
      "-------------------\n",
      "In this work, the authors extend MS-COCO by adding an incorrect caption to each existing caption, with only one word of difference. \n",
      "The authors demonstrate that two state-of-the-art methods (one for VQA and one for captioning) perform extremely poorly at a) determining if a caption is fake, b) determining which word in a fake caption is wrong, and c) selecting a replacement word for a given fake word.\n",
      "This work builds upon a wealth of literature regarding the underperformance of vision/language models relative to their apparent capacities. I think this work makes concrete some of the big, fundamental questions in this area: are vision/language models doing \"interesting\" things, or not? The authors consider a nice mix of tasks and models to shed light on the \"broken-ness\" of these settings, and perform some insightful analyses of factors associated with model failure (e.g., Figure 3).\n",
      "My biggest concerns with the paper are similarity to Ding et al. That being said, I do think the authors make some really good points; Ding et al. generate similar captions, but the ones here differ by only one word and *still* break the models -- I think that's a justifiably fundamental difference. That observation demonstrates that Ding et al.'s engineering is not a requirement, as this simple approach still breaks things catastrophically.\n",
      "Another concern is the use of NeuralTalk to select the \"hardest\" foils.              While a clever idea, I am worried that the use of this model creates a risk of self-reinforcement bias, i.e., NeuralTalk's biases are now fundamentally \"baked-in\" to FOIL-COCO.  I think the results section could be a bit longer, relative to the rest of the paper (e.g. I would've liked more than one paragraph -- I liked this part!)\n",
      "Overall, I do like this paper, as it nicely builds upon some results that highlight defficiencies in vision/language integration. In the end, the Ding et al. similarity is not a \"game-breaker,\" I think -- if anything, this work shows that vision/language models are so easy to fool, Ding et al.'s method is not even required.\n",
      "Small things: I would've liked to have seen another baseline that simply concatenates BoW + extracted CNN features and trains a softmax classifier over them. The \"blind\" model is a nice touch, but what about a \"dumb\" vision+langauge baseline? I bet that would do close to as well as the LSTM/Co-attention. That could've made the point of the paper even stronger.\n",
      "330: What is a supercategory? Is this from WordNet? Is this from COCO? \n",
      "I understand the idea, but not the specifics.\n",
      "397: has been -> were 494: that -> than 693: artefact -> undesirable artifacts (?)\n",
      "701: I would have included a chance model in T1's table -- is 19.53% [Line 592] a constant-prediction baseline? Is it 50% (if so, can't we flip all of the \"blind\" predictions to get a better baseline?) I am not entirely clear, and I think a \"chance\" line here would fix a lot of this confusion.\n",
      "719: ariplane ~~ After reading the author response... I think this author response is spot-on. Both my concerns of NeuralTalk biases and additional baselines were addressed, and I am confident that these can be addressed in the final version, so I will keep my score as-is. \n",
      "-------------------\n",
      "This paper addresses the problem of disambiguating/linking textual entity mentions into a given background knowledge base (in this case, English Wikipedia).  (Its title and introduction are a little overblown/misleading, since there is a lot more to bridging text and knowledge than the EDL task, but EDL is a core part of the overall task nonetheless.)  The method is to perform this bridging via an intermediate layer of representation, namely mention senses, thus following two steps: (1) mention to mention sense, and (2) mention sense to entity.  Various embedding representations are learned for the words, the mention senses, and the entities, which are then jointly trained to maximize a single overall objective function that maximizes all three types of embedding equally.   Technically the approach is fairly clear and conforms to the current deep processing fashion and known best practices regarding embeddings; while one can suggest all kinds of alternatives, it’s not clear they would make a material difference.  Rather, my comments focus on the basic approach.  It is not explained, however, exactly why a two-step process, involving the mention senses, is better than a simple direct one-step mapping from word mentions to their entities.  (This is the approach of Yamada et al., in what is called here the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its simplification SPME) do better.  By why, exactly?  What is the exact difference, and additional information, that the mention senses have compare4ed to the entities?  To understand, please check if the following is correct (and perhaps update the paper to make it exactly clear what is going on).   For entities: their profiles consist of neighboring entities in a relatedness graph.                    This graph is built (I assume) by looking at word-level relatedness of the entity definitions (pages in Wikipedia).  The profiles are (extended skip-gram-based) embeddings.   For words: their profiles are the standard distributional semantics approach, without sense disambiguation.   For mention senses: their profiles are the standard distributional semantics approach, but WITH sense disambiguation.  Sense disambiguation is performed using a sense-based profile (‘language model’) from local context words and neighboring mentions, as mentioned briefly just before Section 4, but without details.  This is a problem point in the approach.  How exactly are the senses created and differentiated?  Who defines how many senses a mention string can have?  If this is done by looking at the knowledge base, then we get a bijective mapping between mention senses and entities -– that is, there is exactly one entity for each mention sense (even if there may be more entities). \n",
      " In that case, are the sense collection’s definitional profiles built starting with entity text as ‘seed words’?                    If so, what information is used at the mention sense level that is NOT used at the entity level?  Just and exactly the words in the texts that reliably associate with the mention sense, but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many such words are there, on average, for a mention sense?                    That is, how powerful/necessary is it to keep this extra differentiation information in a separate space (the mention sense space) as opposed to just loading these additional words into the Entity space (by adding these words into the Wikipedia entity pages)?   If the above understanding is essentially correct, please update Section 5 of the paper to say so, for (to me) it is the main new information in the paper.   It is not true, as the paper says in Section 6, that “…this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison”.  The TAC KBP evaluations for the past two years have hosted EDL tasks, involving eight or nine systems, all performing exactly this task, albeit against Freebase, which is considerably larger and more noisy than Wikipedia.  Please see http://nlp.cs.rpi.edu/kbp/2016/ .   On a positive note: I really liked the idea of the smoothing parameter in Section 6.4.2.\n",
      "Post-response: I have read the authors' responses.  I am not really satisfied with their reply about the KBP evaluation not being relevant, but that they are interested in the goodness of the embeddings instead.  In fact, the only way to evaluate such 'goodness' is through an application.  No-one really cares how conceptually elegant an embedding is, the question is: does it perform better? \n",
      "-------------------\n",
      "This paper presents a corpus of annotated essay revisions.  It includes two examples of application for the corpus: 1) Student Revision Behavior Analysis and 2) Automatic Revision Identification The latter is essentially a text classification task using an SVM classifier and a variety of features. The authors state that the corpus will be freely available for research purposes.\n",
      "The paper is well-written and clear. A detailed annotation scheme was used by two annotators to annotate the corpus which added value to it. I believe the resource might be interesting to researcher working on writing process research and related topics. I also liked that you provided two very clear usage scenarios for the corpus.  I have two major criticisms. The first could be easily corrected in case the paper is accepted, but the second requires more work.\n",
      "1) There are no statistics about the corpus in this paper. This is absolutely paramount. When you describe a corpus, there are some information that should be there.  I am talking about number of documents (I assume the corpus has 180 documents (60 essays x 3 drafts), is that correct?), number of tokens (around 400 words each essay?), number of sentences, etc.  I assume we are talking about 60 unique essays x 400 words, so about 24,000 words in total. Is that correct? If we take the 3 drafts we end up with about 72,000 words but probably with substantial overlap between drafts.\n",
      "A table with this information should be included in the paper.\n",
      "2) If the aforementioned figures are correct, we are talking about a very small corpus. I understand the difficulty of producing hand-annotated data, and I think this is one of the strengths of your work, but I am not sure about how helpful this resource is for the NLP community as a whole. Perhaps such a resource would be better presented in a specialised workshop such as BEA or a specialised conference on language resources like LREC instead of a general NLP conference like ACL.\n",
      "You mentioned in the last paragraph that you would like to augment the corpus with more annotation. Are you also willing to include more essays?\n",
      "Comments/Minor: - As you have essays by native and non-native speakers, one further potential application of this corpus is native language identification (NLI).\n",
      "- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\". \n",
      "Be more specific.\n",
      "- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant. \n",
      "-------------------\n",
      "This paper presents a gated attention mechanism for machine reading. \n",
      "A key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop reasoning by fine-grained gated filter. \n",
      "It's interesting and intuitive for machine reading. \n",
      "I like the idea along with significant improvement on benchmark datasets, but also have major concerns to get it published in ACL.\n",
      "- The proposed GA mechanism looks promising, but not enough to convince the importance of this technique over other state-of-the-art systems, because engineering tricks presented 3.1.4 boost a lot on accuracy and are blended in the result.\n",
      "- Incomplete bibliography: Nearly all published work in reference section refers arxiv preprint version. \n",
      "This makes me (and future readers) suspicious if this work thoroughly compares with prior work. Please make them complete if the published version is available.  - Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned as previous work that is unpublished preprint. \n",
      "I don't think this is necessary at all. Alternately, I would like the author to replace it with vanilla GA (or variant of the proposed model for baseline). \n",
      "It doesn't make sense that result from the preprint which will end up being the same as this ACL submission is presented in the same manuscript. \n",
      "For fair blind-review, I didn't search on arvix archive though.\n",
      "- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2, and GA (fix L(w)) is for K=3 in table 2. \n",
      "Does this mean that GA-- is actually AS Reader? \n",
      "It's not clear that GA-- is re-implementation of AS. \n",
      "I assumed K=1 (AS) in table 2 uses also GloVe initialization and token-attention, but it doesn't seem in GA--.  - I wish the proposed method compared with prior work in related work section (i.e. what's differ from related work).\n",
      "- Fig 2 shows benefit of gated attention (which translates multi-hop architecture), and it's very impressive. It would be great to see any qualitative example with comparison. \n",
      "-------------------\n",
      "This paper presents an interesting model for reading comprehension, by depicting the multiplicative interactions between the query and local information around a word in a document, and the authors proposed a new gated-attention strategy to characterize the relationship. The work is quite solid, with almost state of art result on the whole four cloze-style datasets achieved. Some of the further improvement can be helpful for the similar tasks.\n",
      "Nevertheless, I have some concerns on the following aspect: 1. The authors have referred many papers from arXiv, but I think some really related works are not included. Such as the works from Caiming Xiong, et al. https://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et al. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on enhancing the attention operation to modeling the interaction between documents and queries. Although these works are not evaluated on the cloze-style corpus but the SQuAD, an experimental or fundamental comparison may be necessary.\n",
      "2. There have been some studies that adopts attention mechanism or its variants specially designed for the Reading Comprehension tasks, and the work actually share the similar ideas with this paper. My suggestion is to conduct some comparisons with such work to enhance the experiments of this paper. \n",
      "-------------------\n",
      "This paper presents several weakly supervised methods for developing NERs. The methods rely on some form of projection from English into another language. The overall approach is not new and the individual methods proposed are improvements of existing methods. For an ACL paper I would have expected more novel approaches.\n",
      "One of the contributions of the paper is the data selection scheme. The formula used to calculate the quality score is quite straightforward and this is not a bad thing. However, it is unclear how the thresholds were calculated for Table 2. The paper says only that different thresholds were tried. Was this done on a development set? There is no mention of this in the paper. The evaluation results show clearly that data selection is very important, but one may not know how to tune the parameters for a new data set or a new language pair.  Another contribution of the paper is the combination of the outputs of the two systems developed in the paper. I tried hard to understand how it works, but the description provided is not clear.  The paper presents a number of variants for each of the methods proposed. Does it make sense to combine more than two weakly supervised systems? Did the authors try anything in this direction.\n",
      "It would be good to know a bit more about the types of texts that are in the \"in-house\" dataset. \n",
      "-------------------\n",
      "This paper describes a model for cross-lingual named entity recognition (NER). \n",
      "The authors employ conditional random fields, maximum entropy Markov, and neural network-based NER methods. In addition, authors propose two methods to combine the output of those methods (probability-based and ranking-based), and a method to select the best training instances from cross-lingual comparable corpora. The cross-lingual projection is done using a variant of Mikolov’s proposal. In general, the paper is easy to follow, well-structured, and the English quality is also correct. The results of the combined annotations are interesting.\n",
      "Detailed comments: I was wondering which is the motivation behind proposing a Continuous Bag-of-word (CBOW) model variation. You don’t give much details about this (or the parameters employed). Was the original model (or the Continuous Skip-gram model) offering low results? I suggest to include also the results with the CBOW model, so readers can analyse the improvements of your approach. \n",
      "Since you use a decay factor for the surrounding embeddings, I suggest to take a look to the exponential decay used in [1].\n",
      "Similarly to the previous comment, I would like to look at the differences between the original Mikolov’s cross-lingual projections and your frequency weighted projections. These contributions are more valuable if readers can see that your method is really superior.\n",
      "“the proposed data selection scheme is very effective in selecting good-quality projection-labeled data and the improvement is significant” ← Have you conducted a test of statistical significance? I would like to know if the differences between result in this work are significant.  I suggest to integrate the text of Section 4.4 at the beginning of Section 4.2. \n",
      "It would look cleaner. I also recommend to move the evaluation of Table 2 to the evaluation section.\n",
      "I miss a related work section. Your introduction includes part of that information. I suggest to divide the introduction in two sections.\n",
      "The evaluation is quite short (1.5 pages with conclusion section there). You obtain state-of-the-art results, and I would appreciate more discussion and analysis of the results.\n",
      "Suggested references: [1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word sense disambiguation: An evaluation study. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907). \n",
      "-------------------\n",
      "Update after author response:  1. My major concern about the optimization of model's hyperparameter (which are numerous) has not been addressed. This is very important, considering that you report results from folded cross-validation.  2. The explanation that benefits of their method are experimentally confirmed with 2% difference -- while evaluating via 5-fold CV on 200 examples -- is quite unconvincing.\n",
      "======================================================================== Summary: In this paper authors present a complex neural model for detecting factuality of event mentions in text. The authors combine the following in their complex model:                          (1) a set of traditional classifiers for detecting event mentions, factuality sources, and source introducing predicates (SIPs), (2) A bidirectional attention-based LSTM model that learns latent representations for elements on different dependency paths used as input, (2) A CNN that uses representations from the LSTM and performs two output predictions (one to detect specific from underspecified cases and another to predict the actual factuality class).  From the methodological point of view, the authors are combining a reasonably familiar methods (att-BiLSTM and CNN) into a fairly complex model. However, this model does not take raw text (sequence of word embeddings) as input, but rather hand-crafted features (e.g., different dependency paths combining factuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted features is somewhat surprising if coupled with complex deep model. The evaluation seems a bit tainted as the authors report the results from folded cross-validation but do not report how they optimized the hyperparameters of the model. Finally, the results are not too convincing -- considering the complexity of the model and the amount of preprocessing required (extraction of event mentions, SIPs, and clues), a 2% macro-average gain over the rule-based baseline and overall 44% performance seems modest, at best (looking at Micro-average, the proposed model doesn't outperform simple MaxEnt classifier).\n",
      "The paper is generally well-written and fairly easy to understand. Altogether, I find this paper to be informative to an extent, but in it's current form not a great read for a top-tier conference.    Remarks: 1. You keep mentioning that the LSTM and CNN in your model are combined \"properly\" -- what does that actually mean? How does this \"properness\" manifest? What would be the improper way to combine the models?\n",
      "2. I find the motivation/justification for the two output design rather weak:      - the first argument that it allows for later addition of cues (i.e manually-designed features) kind of beats the \"learning representations\" advantage of using deep models. \n",
      "        - the second argument about this design tackling the imbalance in the training set is kind of hand-wavy as there is no experimental support for this claim.  3. You first motivate the usage of your complex DL architecture with learning latent representations and avoiding manual design and feature computation.  And then you define a set of manually designed features (several dependency paths and lexical features) as input for the model. Do you notice the discrepancy?  4. The LSTMs (bidirectional, and also with attention) have by now already become a standard model for various NLP tasks. Thus I find the detailed description of the attention-based bidirectional LSTM unnecessary. \n",
      "5. What you present as a baseline in Section 3 is also part of your model (as it generates input to your model). Thus, I think that calling it a baseline undermines the understandability of the paper.  6. The results reported originate from a 5-fold CV. However, the model contains numerous hyperparameters that need to be optimized (e.g., number of filters and filter sizes for CNNs). How do you optimize these values? Reporting results from a folded cross-validation doesn't allow for a fair optimization of the hypeparameters: either you're not optimizing the model's hyperparameters at all, or you're optimizing their values on the test set (which is unfair).  7. \" Notice that some values are non-application (NA) grammatically, e.g., PRu, PSu, U+/-\" -- why is underspecification in ony one dimension (polarity or certainty) not an option? I can easily think of a case where it is clear the event is negative, but it is not specified whether the absence of an event is certain, probable, or possible.  Language & style: 1. \" to a great degree\" -> \"great degree\" is an unusual construct, use either \"great extent\" or \"large degree\" 2. \" events that can not\" -> \"cannot\" or \"do not\" 3. \" describes out networks...in details shown in Figure 3.\" - > \"...shown in Figure 3 in details.\" \n",
      "-------------------\n",
      "This paper proposes a novel strategy for zero-resource translation where (source, pivot) and (pivot, target) parallel corpora are available. A teacher model for p(target|pivot) is first trained on the (pivot, target) corpus, then a student model for p(target|source) is trained to minimize relative entropy with respect to the teacher on the (source, pivot) corpus. When using word-level relative entropy over samples from the teacher, this approach is shown to outperform previous variants on standard pivoting, as well as other zero-resource strategies.\n",
      "This is a good contribution: a novel idea, clearly explained, and with convincing empirical support. Unlike some previous work, it makes fairly minimal assumptions about the nature of the NMT systems involved, and hence should be widely applicable.\n",
      "I have only a few suggestions for further experiments. First, it would be interesting to see how robust this approach is to more dissimilar source and pivot languages, where intuitively the true p(target|source) and p(target|pivot) will be further apart. Second, given the success of introducing word-based diversity, it was surprising not to see a sentence n-best or sentence-sampling experiment. This would be more costly, but not much more so since you’re already doing beam search with the teacher. Finally, related to the previous, it might be interesting to explore transition from word-based diversity to sentence-based as the student converges and no longer needs the signal from low-probability words.\n",
      "Some further comments: line 241: Despite its simplicity -> Due to its simplicity 277: target sentence y -> target word y 442: I assume that K=1 and K=5 mean that you compare probabilities of the most probable and 5 most probable words in the current context. If so, how is the current context determined - greedily or with a beam?\n",
      "Section 4.2. The comparison with an essentially uniform distribution doesn’t seem very informative here: it would be extremely surprising if p(y|z) were not significantly closer to p(y|x) than to uniform. It would be more interesting to know to what extent p(y|z) still provides a useful signal as p(y|x) gets better. This would be easy to measure by comparing p(y|z) to models for p(y|x) trained on different amounts of data or for different numbers of iterations. \n",
      "Another useful thing to explore in this section would be the effect of the mode approximation compared to n-best for sentence-level scores.\n",
      "555: It’s odd that word beam does worse than word greedy, since word beam should be closer to word sampling. Do you have an explanation for this?\n",
      "582: The claimed advantage of sent-beam here looks like it may just be noise, given the high variance of these curves. \n",
      "-------------------\n",
      "The paper describes an extension of word embedding methods to also provide representations for phrases and concepts that correspond to words.  The method works by fixing an identifier for groups of phrases, words and the concept that all denote this concept, replace the occurrences of the phrases and words by this identifier in the training corpus, creating a \"tagged\" corpus, and then appending the tagged corpus to the original corpus for training.  The concept/phrase/word sets are taken from an ontology.  Since the domain of application is biomedical, the related corpora and ontologies are used.  The researchers also report on the generation of a new test dataset for word similarity and relatedness for real-world entities, which is novel.\n",
      "In general, the paper is nicely written.  The technique is pretty natural, though not a very substantial contribution. The scope of the contribution is limited, because of focused evaluation within the biomedical domain.\n",
      "More discussion of the generated test resource could be useful.  The resource could be the true interesting contribution of the paper.\n",
      "There is one small technical problem, but that is probably just a matter of mathematical expression than implementation.\n",
      "Technical problem: Eq. 8: The authors want to define the MAP calculation.                          This is a good idea, thought I think that a natural cut-off could be defined, rather than ranking the entire vocabulary.                          Equation 8 does not define a probability; it is quite easy to show this, even if the size of the vocabulary is infinite.  So you need to change the explanation (take out talk of a probability).\n",
      "Small corrections: line: 556: most concepts has --> most concepts have \n",
      "-------------------\n",
      "This paper proposes an approach to learning a semantic parser using an encoder-decoder neural architecture, with the distinguishing feature that the semantic output is full SQL queries. The method is evaluated over two standard datasets (Geo880 and ATIS), as well as a novel dataset relating to document search.\n",
      "This is a solid, well executed paper, which takes a relatively well established technique in the form of an encoder-decoder with some trimmings (e.g. data augmentation through paraphrasing), and uses it to generate SQL queries, with the purported advantage that SQL queries are more expressive than other semantic formalisms commonly used in the literature, and can be edited by untrained crowd workers (familiar with SQL but not semantic parsing). I buy that SQL is more expressive than the standard semantic formalisms, but then again, were there really any queries in any of your three datasets where the standard formalisms are unable to capture the full semantics of the query? I.e. are they really the best datasets to showcase the expressivity of SQL? Also, in terms of what your model learns, what fraction of SQL does it actually use? I.e. how much of the extra expressivity in SQL is your model able to capture? Also, does it have biases in terms of the style of queries that it tends to generate? That is, I wanted to get a better sense of not just the *potential* of SQL, but the actuality of what your model is able to capture, and the need for extra expressivity relative to the datasets you experiment over. Somewhat related to this, at the start of Section 5, you assert that it's harder to directly produce SQL. You never actually show this, and this seems to be more a statement of the expressivity of SQL than anything else (which returns me to the question of how much of SQL is the model actually generating).\n",
      "Next, I would really have liked to have seen more discussion of the types of SQL queries your model generates, esp. for the second part of the evaluation, over the SCHOLAR dataset. Specifically, when the query is ill-formed, in what ways is it ill-formed? When a crowd worker is required to post-edit the query, how much effort does that take them? Equally, how correct are the crowd workers at constructing SQL queries? Are they always able to construct perfect queries (experience would suggest that this is a big ask)? In a similar vein to having more error analysis in the paper, I would have liked to have seen agreement numbers between annotators, esp. for Incomplete Result queries, which seems to rely heavily on pre-existing knowledge on the part of the annotator and therefore be highly subjective.\n",
      "Overall, what the paper achieves is impressive, and the paper is well executed; I just wanted to get more insights into the true ability of the model to generate SQL, and a better sense of what subset of the language it generates.\n",
      "A couple of other minor things: l107: \"non-linguists can write SQL\" -- why refer to \"non-linguists\" here? Most linguists wouldn't be able to write SQL queries either way; I think the point you are trying to make is simply that \"annotators without specific training in the semantic translation of queries\" are able to perform the task l218: \"Is is\" -> \"It is\" l278: it's not clear what an \"anonymized utterance\" is at this point of the paper l403: am I right in saying that you paraphrase only single words at a time? \n",
      "Presumably you exclude \"entities\" from paraphrasing?\n",
      "l700: introduce a visual variable in terms of line type to differentiate the three lines, for those viewing in grayscale There are various inconsistencies in the references, casing issues (e.g. \"freebase\", \"ccg\"), Wang et al. (2016) is missing critical publication details, and there is an \"In In\" for Wong and Mooney (2007) \n",
      "-------------------\n",
      "This paper introduces a new approach to semantic parsing in which the model is equipped with a neural sequence to sequence (seq2seq) model (referred to as the “programmer”) which encodes a natural language question and produces a program. The programmer is also equipped with a ‘key variable’ memory component which stores (a) entities in the questions (b) values of intermediate variables formed during execution of intermediate programs. These variables are referred to further build the program.                    The model is also equipped with certain discrete operations (such as argmax or 'hop to next edges in a KB'). A separate component (\"interpreter/computer\") executes these operations and stores intermediate values (as explained before). Since the ‘programmer' is inherently a seq2seq model, the \"interpreter/computer” also acts as a syntax/type checker only allowing the decoder to generate valid tokens. For example, the second argument to the “hop” operation has to be a KB predicate. Finally the model is trained with weak supervision and directly optimizes the metric which is used to evaluate the performance (F score). \n",
      "Because of the discrete operations and the non differentiable reward functions, the model is trained with policy gradients (REINFORCE). Since gradients obtained through REINFORCE have high variance, it is common to first pretrain the model with a max-likelihood objective or find some good sequences of actions trained through some auxiliary objective. This paper takes a latter approach in which it finds good sequences via an iterative maximum likelihood approach. The results and discussion sections are presented in a very nice way and the model achieves SOTA results on the WebQuestions dataset when compared to other weakly supervised model.\n",
      "The paper is written clearly and is very easy to follow.\n",
      "This paper presents a new and exciting direction and there is scope for a lot of future research in this direction. I would definitely love to see this presented in the conference.\n",
      "Questions for the authors (important ones first) 1. Another alternative way of training the model would be to bootstrap the parameters (\\theta) from the iterative ML method instead of adding pseudo gold programs in the beam (Line 510 would be deleted). Did you try that and if so why do you think it didn’t work? \n",
      "2. What was the baseline model in REINFORCE. Did you have a separate network which predicts the value function. This must be discussed in the paper in detail. \n",
      "3. Were there programs which required multiple hop operations? Or were they limited to single hops. If there were, can you provide an example? ( I will understand if you are bound by word limit of the response) 4. Can you give an example where the filter operation would be used? \n",
      "5. I did not follow the motivation behind replacing the entities in the question with special ENT symbol Minor comments: Line 161 describe -> describing Line 318 decoder reads ‘)’ -> decoder generates ‘)' \n",
      "-------------------\n",
      "This paper proposes an approach for classifying literal and metaphoric adjective-noun pairs. The authors create a word-context matrix for adjectives and nouns where each element of the matrix is the PMI score. They then use different methods for selecting dimensions of this matrix to represent each noun/adjective as a vector. The geometric properties of average, nouns, and adjective vectors and their normalized versions are used as features in training a regression model for classifying the pairs to literal or metaphor expressions. Their approach performs similarly to previous work that learns a vector representation for each adjective.\n",
      "Supervision and zero-shot learning. The authors argue that their approach requires less supervision (compared to previous work)  and can do zero-shot learning. I don’t think this is quite right and given that it seems to be one of the main points of the paper, I think it is worth clarifying. The approach proposed in the paper is a supervised classification task: The authors form vector representations from co-occurrence statistics, and then use the properties of these representations and the gold-standard labels of each pair to train a classifier. The model (similarly to any other supervised classifier) can be tested on words that did not occur in the training data; but, the model does not learn from such examples. Moreover, those words are not really “unseen” because the model needs to have a vector representation of those words.\n",
      "Interpretation of the results. The authors provide a good overview of the previous related work on metaphors. However, I am not sure what the intuition about their approach is (that is, using the geometric properties such as vector length in identifying metaphors). For example, why are the normalized vectors considered? It seems that they don’t contribute to a better performance. \n",
      "Moreover, the most predictive feature is the noun vector; the authors explain that this is a side effect of the data which is collected such that each adjective occurs in both metaphoric and literal expressions. ( As a result, the adjective vector is less predictive.) It seems that the proposed approach might be only suitable for the given data. This shortcoming is two-fold: (a) From the theoretical perspective (and especially since the paper is submitted to the cognitive track), it is not clear what we learn about theories of metaphor processing. ( b) From the NLP applications standpoint, I am not sure how generalizable this approach is compared to the compositional models.\n",
      "Novelty. The proposed approach for representing noun/adjective vectors is very similar to that of Agres et al. It seems that the main contribution of the paper is that they use the geometric properties to classify the vectors. \n",
      "-------------------\n",
      "This work proposes a self-learning bootstrapping approach to learning bilingual word embeddings, which achieves competitive results in tasks of bilingual lexicon induction and cross-lingual word similarity although it requires a minimal amount of bilingual supervision: the method leads to competitive performance even when the seed dictionary is extremely small (25 dictionary items!) or is constructed without any language pair specific information (e.g., relying on numerals shared between languages).  The paper is very well-written, admirably even so. I find this work 'eclectic' in a sense that its original contribution is not a breakthrough finding (it is more a 'short paper idea' in my opinion), but it connects the dots from prior work drawing inspiration and modelling components from a variety of previous papers on the subject, including the pre-embedding work on self-learning/bootstrapping (which is not fully recognized in the current version of the paper). I liked the paper in general, but there are few other research questions that could/should have been pursued in this work. These, along with only a partial recognition of related work and a lack of comparisons with several other relevant baselines, are my main concern regarding this paper, and they should be fixed in the updated version(s).\n",
      "*Self-learning/bootstrapping of bilingual vector spaces: While this work is one of the first to tackle this very limited setup for learning cross-lingual embeddings (although not the first one, see Miceli Barone and more works below), this is the first truly bootstrapping/self-learning approach to learning cross-lingual embeddings. However, this idea of bootstrapping bilingual vector spaces is not new at all (it is just reapplied to learning embeddings), and there is a body of work which used exactly the same idea with traditional 'count-based' bilingual vector spaces. I suggest the authors to check the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP 2013), and recognize the fact that their proposed bootstrapping approach is not so novel in this domain. There is also related work of Ellen Riloff's group on bootstrapping semantic lexicons in monolingual settings.\n",
      "*Relation to Artetxe et al.: I might be missing something here, but it seems that the proposed bootstrapping algorithm is in fact only an iterative approach which repeatedly utilises the previously proposed model/formulation of Artetxe et al. The only difference is the reparametrization (line 296-305). It is not clear to me whether the bootstrapping approach draws its performance from this reparametrization (and whether it would work with the previous parametrization), or the performance is a product of both the algorithm and this new parametrization. Perhaps a more explicit statement in the text is needed to fully understand what is going on here.\n",
      "*Comparison with prior work: Several very relevant papers have not been mentioned nor discussed in the current version of the paper. For instance, the recent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word embeddings without bilingual corpora' seems very related to this work (as the basic word overlap between the two titles reveals!), and should be at least discussed if not compared to. Another work which also relies on mappings with seed lexicons and also partially analyzes the setting with only a few hundred seed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of seed lexicons in learning bilingual word embeddings': these two papers might also help the authors to provide more details for the future work section (e.g., the selection of reliable translation pairs might boost the performance further during the iterative process). Another very relevant work has appeared only recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word vectors, orthogonal transformations and the inverted softmax'. This paper also discusses learning bilingual embeddings in very limited settings (e.g., by relying only on shared words and cognates between two languages in a pair). As a side note, it would be interesting to report results obtained using only shared words between the languages (such words definitely exist for all three language pairs used in the experiments). This would also enable a direct comparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n",
      "*Seed dictionary size and bilingual lexicon induction: It seems that the proposed algorithm (as discussed in Section 5) is almost invariant to the starting seed lexicon, yielding very similar final BLI scores regardless of the starting point. While a very intriguing finding per se, this also seems to suggest an utter limitation of the current 'offline' approaches: they seem to have hit the ceiling with the setup discussed in the paper; Vulic and Korhonen (ACL 2016) showed that we cannot really improve the results by simply collecting more seed lexicon pairs, and this work suggests that any number of starting pairs (from 25 to 5k) is good enough to reach this near-optimal performance, which is also very similar to the numbers reported by Dinu et al. (arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more discussion on how to break this ceiling and further improve BLI results with such 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers on the same dataset, so again it would be very interesting to link this work to the work of Smith et al. In other words, the authors state that in future work they plan to fine-tune the method so that it can learn without any bilingual evidence. This is an admirable 'philosophically-driven' feat, but from a more pragmatic point of view, it seems more pragmatic to detect how we can go over the plateau/ceiling which seems to be hit with these linear mapping approaches regardless of the number of used seed lexicon pairs (Figure 2).\n",
      "*Convergence criterion/training efficiency: The convergence criterion, although crucial for the entire algorithm, both in terms of efficiency and efficacy, is mentioned only as a side note, and it is not entirely clear how the whole procedure terminates. I suspect that the authors use the vanishing variation in crosslingual word similarity performance as the criterion to stop the procedure, but that makes the method applicable only to languages which have a cross-lingual word similarity dataset. I might be missing here given the current description in the paper, but I do not fully understand how the procedure stops for Finnish, given that there is no crosslingual word similarity dataset for English-Finnish.\n",
      "*Minor: - There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416): https://www.clarin.si/repository/xmlui/handle/11356/1074 - Since the authors claim that the method could work with a seed dictionary containing only shared numerals, it would be very interesting to include an additional language pair which does not share the alphabet (e.g., English-Russian, English-Bulgarian or even something more distant such as Arabic and/or Hindi).\n",
      "*After the response: I would like to thank the authors for investing their time into their response which helped me clarify some doubts and points raised in my initial review. I hope that they would indeed clarify these points in the final version, if given the opportunity. \n",
      "-------------------\n",
      "The paper presents a self-learning framework for learning of bilingual word embeddings. The method uses two embeddings (in source and target languages) and a seed lexicon. On each step of the mapping learning a new bilingual lexicon is induced. Then the learning step is repeated using the new lexicon for learning of new mapping. The process stops when a convergence criterion is met.\n",
      "One of the strengths is that the seed lexicon is directly encoded in the learning process as a binary matrix. Then the self-learning framework solves a global optimization problem in which the seed lexicon is not explicitly involved. Its role is to establish the initial mapping between the two embeddings. This guarantees the convergence. The initial seed lexicon could be quite small (25 correspondences).\n",
      "The small size of the seed lexicon is appealing for mappings between languages for which there are not large bilingual lexicons.\n",
      "It will be good to evaluate the framework with respect to the quality of the two word embeddings. If we have languages (or at least one of the languages) with scarce language resources then the word embeddings for both languages could differ in their structure and coverage. I think it could be simulated on the basis of the available data via training the corresponding word embeddings on different subcorpora for each language. \n",
      "-------------------\n",
      "This one is a tough call, because I do think that there are some important, salvageable technial results in here (notably the parsing algorithm), but the paper as a whole has very little cohesion.        It is united around an overarching view of formal languages in which a language being \"probabilistic\" or not is treated as a formal property of the same  variety as being closed under intersection or not.  In my opinion, what it  means for a formal language to be probabilistic in this view has not been  considered with sufficient rigor for this viewpoint to be compelling.\n",
      "I should note, by the way, that the value of the formal results provided mostly does not depend on the flimsiness of the overarching story.  So what we have here is not bad research, but a badly written paper.  This needs  more work.\n",
      "I find it particulary puzzling that the organization of the paper leaves so little space for elucidating the parsing result that soundness and completeness are relegated to a continuation of the paper in the form of supplementary notes.  I also find the mention of probabilistic languages in the title of the paper to be very disingenuous --- there is in fact no probabilistic reasoning in this submission.\n",
      "The sigificance of the intersection-closure result of section 3 is also being somewhat overstated, I think.  Unless there is something I'm not understanding about the restrictions on the right-hand sides of rules (in which case, please elaborate), this is merely a matter of folding a finite intersection into the set of non-terminal labels. \n",
      "-------------------\n",
      "The paper introduces a simple and effective method for morphological paradigm completion in low-resource settings. The method uses a character-based seq2seq model trained on a mix of examples in two languages: a resource-poor language and a closely-related resource-rich language; each training example is annotated with a paradigm properties and a language ID. Thus, the model enables transfer learning across languages when the two languages share common characters and common paradigms. While the proposed multi-lingual solution is not novel (similar architectures have been explored in syntax, language modeling, and MT), the novelty of this paper is to apply the approach to morphology. Experimental results show substantial improvements over monolingual baselines, and include a very thorough analysis of the impact of language similarities on the quality of results. The paper is interesting, very clearly written, I think it’ll be a nice contribution to the conference program.  Detailed comments:  — My main question is why the proposed general multilingual methodology was limited to pairs of languages, rather than to sets of similar languages? For example, all Romance languages could be included in the training to improve Spanish paradigm completion, and all Slavic languages with Cyrillic script could be mixed to improve Ukrainian. It would be interesting to see the extension of the models from bi-lingual to multilingual settings.  — I think Arabic is not a fair (and fairly meaningless) baseline, given how different is its script and morphology from the target languages. A more interesting baseline would be, e.g., a language with a partially shared alphabet but a different typology. For example, a Slavic language with Latin script could be used as a baseline language for Romance languages. If Arabic is excluded, and if we consider a most distant language in the same the same family as a baseline, experimental results are still strong.  — A half-page discussion of contribution of Arabic as a regularizer also adds little to the paper; I’d just remove Arabic from all the experiments and would add a regularizer (which, according to footnote 5, works even better than adding Arabic as a transfer language).               — Related work is missing a line of work on “language-universal” RNN models that use basically the same approach: they learn shared parameters for inputs in multiple languages, and add a language tag to the input to mediate between languages. Related studies include a multilingual parser (Ammar et al., 2016), language models (Tsvetkov et al., 2016), and machine translation (Johnson et al., 2016 ) Minor:  — I don’t think that the claim is correct in line 144 that POS tags are easy to transfer across languages. Transfer of POS annotations is also a challenging task.   References:  Waleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. \" Many languages, one parser.” TACL 2016.  Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \" Polyglot neural language models: A case study in cross-lingual phonetic representation learning.” NAACL 2016.\n",
      "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n",
      "-- Response to author response:  Thanks for your response & I'm looking forward to reading the final version! \n",
      "-------------------\n",
      "This paper proposes a joint neural modelling approach to PAS analysis in Japanese, based on Grid-RNNs, which it compares variously with a conventional single-sequence RNN approach.\n",
      "This is a solidly-executed paper, targeting a well-established task from Japanese but achieving state-of-the-art results at the task, and presenting the task in a mostly accessible manner for those not versed in Japanese. Having said that, I felt you could have talked up the complexity of the task a bit, e.g. wrt your example in Figure 1, talking through the inherent ambiguity between the NOM and ACC arguments of the first predicate, as the NOM argument of the second predicate, and better describing how the task contrasts with SRL (largely through the ambiguity in zero pronouns). I would also have liked to have seen some stats re the proportion of zero pronouns which are actually intra-sententially resolvable, as this further complicates the task as defined (i.e. needing to implicitly distinguish between intra- and inter-sentential zero anaphors). One thing I wasn't sure of here: in the case of an inter-sentential zero pronoun for the argument of a given predicate, what representation do you use? Is there simply no marking of that argument at all, or is it marked as an empty argument? My reading of the paper is that it is the former, in which case there is no explicit representation of the fact that there is a zero pronoun, which seems like a slightly defective representation (which potentially impacts on the ability of the model to capture zero pronouns); some discussion of this would have been appreciated.\n",
      "There are some constraints that don't seem to be captured in the model (which some of the ILP-based methods for SRL explicitly model, e.g.): (1) a given predicate will generally have only one argument of a given type (esp. NOM and ACC); and (2) a given argument generally only fills one argument slot for a given predicate. I would have liked to have seen some analysis of the output of the model to see how well the model was able to learn these sorts of constraints. More generally, given the mix of numbers in Table 3 between Single-Seq and Multi-Seq (where it is really only NOM where there is any improvement for Multi-Seq), I would have liked to have seen some discussion of the relative differences in the outputs of the two models: are they largely identical, or very different but about the same in aggregate, e.g.? In what contexts do you observe differences between the two models? Some analysis like this to shed light on the internals of the models would have made the difference between a solid and a strong paper, and is the main area where I believe the paper could be improved (other than including results for SRL, but that would take quite a bit more work).\n",
      "The presentation of the paper was good, with the Figures aiding understanding of the model. There were some low-level language issues, but nothing major: l19: the error propagation -> error propagation l190: an solution -> a solution l264 (and Figure 2): a bread -> bread l351: the independence -> independence l512: the good -> good l531: from their model -> of their model l637: significent -> significance l638: both of -> both and watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\") \n",
      "-------------------\n",
      "The paper presents two approaches for generating English poetry. The first approach combine a neural phonetic encoder predicting the next phoneme with a phonetic-orthographic HMM decoder computing the most likely word corresponding to a sequence of phonemes. The second approach combines a character language model with a weigthed FST to impose rythm constraints on the output of the language model. For the second approach, the authors also present a heuristic approach which permit constraining the generated poem according to theme (e.g;, love) or poetic devices (e.g., alliteration). The generated poems are evaluated both instrinsically by comparing the rythm of the generated lines with a gold standard and extrinsically by asking 70 human evaluators to (i) determine whether the poem was written by a human or a machine and (ii) rate poems wrt to readability, form and evocation.  The results indicate that the second model performs best and that human evaluators find it difficult to distinguish between human written and machine generated poems.\n",
      "This is an interesting, clearly written article with novel ideas (two different models for poetry generation, one based on a phonetic language model the other on a character LM) and convincing results.\n",
      " For the evaluation, more precision about the evaluators and the protocol would be good. Did all evaluators evaluate all poems and if not how many judgments were collected for each poem for each task ? You mention 9 non English native speakers. Poems are notoriously hard to read. How fluent were these ?  In the second model (character based), perhaps I missed it, but do you have a mechanism to avoid generating non words ? If not, how frequent are non words in the generated poems ?\n",
      "In the first model, why use an HMM to transliterate from phonetic to an orhographic representation rather than a CRF?  Since overall, you rule out the first model as a good generic model for generating poetry, it might have been more interesting to spend less space on that model and more on the evaluation of the second model. In particular, I would have been interested in a more detailed discussion of the impact of the heuristic you use to constrain theme or poetic devices. How do these impact evaluation results ? Could they be combined to jointly constrain theme and poetic devices ?  The combination of a neural mode with a WFST is reminiscent of the following paper which combine character based neural model to generate from dialog acts with an WFST to avoid generating non words. YOu should relate your work to theirs and cite them.  Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge Goyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni COLING 2016 \n",
      "-------------------\n",
      "This work describes a gated attention-based recurrent neural network method for reading comprehension and question answering. This method employs a self-matching attention technique to counterbalance the limited context knowledge of gated attention-based recurrent neural networks when processing passages. Finally, authors use pointer networks  with signals from the question attention-based vector to predict the beginning and ending of the answer. \n",
      "Experimental results with the SQuAD dataset offer state-of-the-art performance compared with several recent approaches.  The paper is well-written, structured and explained. As far as I know, the mathematics look also good. In my opinion, this is a very interesting work which may be useful for the question answering community.\n",
      "I was wondering if the authors have plans to release the code of this approach. \n",
      "From that perspective, I miss a bit of information about the technology used for the implementation (theano, CUDA, CuDNN...), which may be useful for readers.\n",
      "I would appreciate if authors could perform a test of statistical significance of the results. That would highlight even more the quality of your results.\n",
      "Finally, I know that the space may be a constraint, but an evaluation including some additional dataset would validate more your work. \n",
      "-------------------\n",
      "This paper presents a new dataset with annotations of products coming from online cybercrime forums. The paper is clear and well-written and the experiments are good. Every hypothesis is tested and compared to each other.\n",
      "However, I do have some concerns about the paper: 1. The authors took the liberty to change the font size and the line spacing of the abstract, enabling them to have a longer abstract and to fit the content into the 8 pages requirement.\n",
      "2. I don't think this paper fits the tagging, chunking, parsing area, as it is more an information extraction problem.\n",
      "3. I have difficulties to see why some annotations such as sombody in Fig. 1 are related to a product.\n",
      "4. The basic results are very basic indeed and - with all the tools available nowadays in NLP -, I am sure that it would have been possible to have more elaborate baselines without too much extra work.\n",
      "5. Domain adaptation experiments corroborate what we already know about user-generated data where two forums on video games, e.g., may have different types of users (age, gender, etc.) leading to very different texts. So this does not give new highlights on this specific problem. \n",
      "-------------------\n",
      "The paper describes a novel application of mostly existing representations, features sets, and methods: namely, detecting Mild Cognitive Impairment (MCI)  in speech narratives. The nature of the problem, datasets, and domain are thoroughly described. While missing some detail, the proposed solution and experiments sound reasonable. Overall, I found the study interesting and informative.\n",
      "In terms of drawbacks, the paper needs some considerable editing to improve readability. Details on some key concepts appear to be missing. For example,  details on the multi-view learning used are omitted; the set of “linguistic features” needs to be clarified; it is not entirely clear what datasets were used to generate the word embeddings (presumably the 3 datasets described in the paper, which appear to be too small for that purpose…). It is also not clear why disfluencies (filled pauses, false starts, repetitions, etc.) were removed from the dataset. One might suggest that they are important features in the context of MCI. It is also not clear why the most popular tf-idf weighting scheme was not used for the BoW classifications. In addition, tests for significance are not provided to substantiate the conclusions from the experiments. Lastly, the related work is described a bit superficially.  Detailed comments are provided below: Abstract: The abstract needs to be shortened. See detailed notes below.\n",
      "Lines 22,23 need rephrasing.            “However, MCI disfluencies produce agrammatical speech impacting in parsing results” → impacting the parsing results?\n",
      "Lines 24,25: You mean correct grammatical errors in transcripts manually? It is not clear why this should be performed, doesn’t the fact that grammatical errors are present indicate MCI? … Only after reading the Introduction and Related Work sections it becomes clear what you mean. Perhaps include some examples of disfluencies.\n",
      "Lines 29,30 need rephrasing: “as it a lightweight and language  independent representation” Lines 34-38 need rephrasing: it is not immediately clear which exactly are the 3 datasets. Maybe: “the other two: Cinderella and … “             Line 70: “15% a year” → Not sure what exactly “per year” means… Line 73 needs rephrasing.\n",
      "Lines 115 - 117: It is not obvious why BoW will also have problems with disfluencies, some explanation will be helpful.\n",
      "Lines 147 - 149: What do you mean by “the best scenario”?\n",
      "Line 157: “in public corpora of Dementia Bank” → a link or citation to Dementia Bank will be helpful.  Line 162: A link or citation describing the “Picnic picture of the Western Aphasia Battery” will be helpful.\n",
      "Line 170: An explanation as to what the WML subtest is will be helpful.\n",
      "Line 172 is missing citations.\n",
      "Lines 166 - 182: This appears to be the core of the related work and it is described a bit superficially. For example, it will be helpful to know precisely what methods were used to achieve these tasks and how they compare to this study.\n",
      "Line 185: Please refer to the conference citation guidelines. I believe they are something along these lines: “Aluisio et al. (2016)  used…” Line 188: The definition of “PLN” appears to be missing.\n",
      "Lines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a last step in text pre-processing and normalization, in fact there are also additional common normalization/preprocessing steps omitted.  Lines 290-299: Did you create the word embeddings using the MCI datasets or external datasets?\n",
      "Line 322: consisted of → consist of Lines 323: 332 need to be rewritten. ... “ manually segmented of the DementiaBank and Cinderella” →  What do you mean by segmented, segmented into sentences? Why weren’t all datasets automatically segmented?; “ ABCD” is not defined; You itemized the datasets in i) and ii), but subsequently  you refer to 3 dataset, which is a bit confusing. Maybe one could explicitly name the datasets, as opposed to referring to them as “first”, “second”, “third”.\n",
      "Table 1 Caption: The demographic information is present, but there are no any additional statistics of the dataset, as described.\n",
      "Lines 375 - 388:  It is not clear why filled pauses, false starts, repetitions, etc. were removed. One might suggest that they are important features in the context of MCI ….\n",
      "Line 399: … multidisciplinary team with psychiatrists ... → consisting of psychiatrists… Lines 340-440: A link or citation describing the transcription norms will be helpful.\n",
      "Section 4.2.1. It is not clear what dataset was used to generate the word embeddings.  Line 560. The shortest path as defined in feature 6?\n",
      "Section “4.2.2 Linguistic Features” needs to be significantly expanded for clarity. Also, please check the conference guidelines regarding additional pages (“Supplementary Material”).\n",
      "Line 620: “In this work term frequency was …” → “In this work, term frequency was …” Also, why not tf-idf, as it seems to be the most common weighting scheme?  The sentence on lines 641-645 needs to be rewritten.\n",
      "Line 662: What do you mean by “the threshold parameter”? The threshold for the word embedding cosine distance?\n",
      "Line 735 is missing a period.\n",
      "Section 4.3 Classification Algorithms: Details on exactly what scheme of multi-view learning was used are entirely omitted. Statistical significance of result differences is not provided. \n",
      "-------------------\n",
      "The paper introduces an extension of the entity grid model. A convolutional neural network is used to learn sequences of entity transitions indicating coherence, permitting better generalisation over longer sequences of entities than the direct estimates of transition probabilities in the original model.\n",
      "This is a nice and well-written paper. Instead of proposing a fully neural approach, the authors build on existing work and just use a neural network to overcome specific issues in one step. This is a valid approach, but it would be useful to expand the comparison to the existing neural coherence model of Li and Hovy. The authors admit being surprised by the very low score the Li and Hovy model achieves on their task. This makes the reader wonder if there was an error in the experimental setup, if the other model's low performance is corpus-dependent and, if so, what results the model proposed in this paper would achieve on a corpus or task where the other model is more successful. A deeper investigation of these factors would strengthen the argument considerably.\n",
      "In general the paper is very fluent and readable, but in many places definite articles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and probably more). I would suggest proofreading the paper specifically with article usage in mind. The expression \"...limits the model to do X...\", which is used repeatedly, sounds a bit unusual. Maybe \"limits the model's capacity to do X\" or \"stops the model from doing X\" would be clearer.\n",
      "-------------- Final recommendation adjusted to 4 after considering the author response. I agree that objective difficulties running other people's software shouldn't be held against the present authors. The efforts made to test the Li and Hovy system, and the problems encountered in doing so, should be documented in the paper. I would also suggest that the authors try to reproduce the results of Li and Hovy on their original data sets as a sanity check (unless they have already done so), just to see if that works for them. \n",
      "-------------------\n",
      "The paper proposes a convolutional neural network approach to model the coherence of texts. The model is based on the well-known entity grid representation for coherence, but puts a CNN on top of it.  The approach is well motivated and described, I especially appreciate the clear discussion of the intuitions behind certain design decisions (e.g. why CNN and the section titled 'Why it works').\n",
      "There is an extensive evaluation on several tasks, which shows that the proposed approach beats previous methods. It is however strange that one previous result could not be reproduced: the results on Li/Hovy (2014) suggest an implementation or modelling error that should be addressed.\n",
      "Still, the model is a relatively simple 'neuralization' of the entity grid model. I didn't understand why 100-dimensional vectors are necessary to represent a four-dimensional grid entry (or a few more in the case of the extended grid). How does this help? I can see that optimizing directly for coherence ranking would help learn a better model, but the difference of transition chains for up to k=3 sentences vs. k=6 might not make such a big difference, especially since many WSJ articles may be very short.\n",
      "The writing seemed a bit lengthy, the paper repeats certain parts in several places, for example the introduction to entity grids. In particular, section 2 also presents related work, thus the first 2/3 of section 6 are a repetition and should be deleted (or worked into section 2 where necessary). The rest of section 6 should probably be added in section 2 under a subsection (then rename section 2 as related work).\n",
      "Overall this seems like a solid implementation of applying a neural network model to entity-grid-based coherence. But considering the proposed consolidation of the previous work, I would expect a bit more from a full paper, such as innovations in the representations (other features?) or tasks.\n",
      "minor points: - this paper may benefit from proof-reading by a native speaker: there are articles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ... toolkit' (2x), etc.\n",
      "- p.1 bottom left column: 'Figure 2' -> 'Figure 1' - p.1 Firstly/Secondly -> First, Second - p.1 'limits the model to' -> 'prevents the model from considering ...' ?\n",
      "- Consider removing the 'standard' final paragraph in section 1, since it is not necessary to follow such a short paper. \n",
      "-------------------\n",
      "26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge This paper presents an approach for factoid question answering over a knowledge graph (Freebase), by using a neural model that attempts to learn a semantic correlation/correspondence between various \"aspects\" of the candidate answer (e.g., answer type, relation to question entity, answer semantic, etc.) and a subset of words of the question. A separate correspondence component is learned for each \"aspect\" of the candidate answers. The two key contributions of this work are: (1) the creation of separate components to capture different aspects of the candidate answer, rather than relying on a single semantic representation, and (2) incorporating global context (from the KB) of the candidate answers.\n",
      "The most interesting aspect of this work, in my opinion, is the separation of candidate answer representation into distinct aspects, which gives us (the neural model developer) a little more control over guiding the NN models towards information that would be more beneficial in its decision making. It sort of harkens to the more traditional algorithms that rely on feature engineering. But in this case the \"feature engineering\" (i.e., aspects) is more subtle, and less onerous. I encourage the authors to continue refining this system along these lines.\n",
      "While the high-level idea is fairly clear to a reasonably informed reader, the devil in the details would make it hard for some audience to immediately grasp key insights from this work. Some parts of the paper could benefit from more explanation... Specifically: (1) Context aspect of candidate answers (e_c) is not clearly explained in the paper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n",
      "(2) Mention of OOV in the abstract and introduction need more explanation. As such, I think the current exposition in the paper assumes a deep understanding of prior work by the reader.\n",
      "(3) The experiments conducted in this paper restrict comparison to IR-based system -- and the reasoning behind this decision is reasonable. But it is not clear then why the work of Yang et al. (2014) -- which is described to be SP-based -- is part of the comparison. While, I am all for including more systems in the comparison, there seem to be some inconsistencies in what should and should not be compared. Additionally, I see not harm in also mentioning the comparable performance numbers for the best SP-based systems.\n",
      "I observe in the paper that the embeddings are learned entirely from the training data. I wonder how much impact the random initialization of these embeddings has on the end performance. It would be interesting to determine (and list) the variance if any. Additionally, if we were to start with pre-trained embeddings (e.g., from word2vec) instead of the randomly initialized ones, would that have any impact?\n",
      "As I read the paper, one possible direction of future work that occurred to me was to possibly include structured queries (from SP-based methods) as part of the cross-attention mechanism. In other words, in addition to using the various aspects of the candidate answers as features, one could include structured queries that generate the produce that candidate answer as an additional aspect of the candidate answer. An attention mechanism could then also focus on various parts of the structured query, and its (semantic) matches to the input question as an additional signal for the NN model. Just a thought.\n",
      "Some notes regarding the positioning of the paper: I hesitate to call the model proposed here \"attention\" models, because (per my admittedly limited understanding) attention mechanisms apply to \"encoder-decoder\" situations, where semantics expressed in one structured form (e.g., image, sentence in one language, natural language question, etc.) are encoded into an abstract representation, and then generated into another structured form (e.g., caption, sentence in another language, structured query, etc.). The attention mechanism allows the \"encoder\" to jump around and attend to different parts of the input (instead of sequentially) as the output is being generated by the decoder. This paper does not appear to fit this notion, and may be confusing to a broader audience.\n",
      "------ Thank you for clarifications in the author response. \n",
      "-------------------\n",
      "The paper proposes a method to recognize time expressions from text. It is a simple rule-based method, which is a strong advantage as an analysis tool since time expression recognition should be a basic process in applications. \n",
      "Experiments results show that the proposed method outperforms the state-of-the-art rule-based methods and machine learning based method for time expression recognition.  It is great, but my concern is generality of the method. The rules in the method were designed based on observations of corpora that are used for evaluation as well. Hence I’m afraid that the rules over-fit to these corpora. Similarly, domains of these corpora may have affected the rule design. \n",
      "There is no statistic nor discussion to show overlaps in time expressions in the observed corpora. If it was shown that time expressions in these corpora are mostly overlap, the fact should have supported generality of the rules.  Anyway, it was better that the experiments have been conducted using a new corpus that was distinct from rule design process in order to show that the proposed method is widely effective. \n",
      "-------------------\n",
      "This paper proposes a method for generating datasets of pictures from simple building blocks, as well as corresponding logical forms and language descriptions. \n",
      "The goal seems to be to have a method where the complexity of pictures and corresponding desciptions can be controlled and parametrized.   - The biggest downside seems to be that the maximally achievable complexity is very limited, and way below the complexity typically faced with image-captioning and other multimodal tasks. \n",
      " - The relative simplicity is also a big difference to the referenced bAbI tasks (which cover the whole qualitative spectrum of easy-to-hard reasoning tasks), whereas in the proposed method a (qualitatively) easy image reconition task can only be quantitatively made harder, by increasing the number of objects, noise etc in unnatural ways. \n",
      " - This is also reflected in the experimental section. Whenever the experimental performance results are not satisfying, these cases seem like basic over/underfitting issues that may easily be tackled by restricting/extending the capacity of the networks or using more data. It is hard for me to spot any other qualitative insight. \n",
      " - In the introduction it is stated that the \"goal is not too achieve optimal performance\" but to find out whether \"architectures are able to successfully demonstrate the desired understanding\" - there is a fundamental contradiction here, in that the proposed task on the one side is meant to provide a measure as to whether architectures demontrate \"understanding\", on the other hand the score is not supposed to be taken as meaningful/seriously.\n",
      "General comments: The general approach should be made more tangible earlier (i.e. in the introction rather than in section 3) \n",
      "-------------------\n",
      "This paper details a method of achieving translation from morphologically impoverished languages (e.g. Chinese) to morphologically rich ones (e.g. Spanish) in a two-step process. First, a system translates into a simplified version of the target language. Second, a system chooses morphological features for each generated target word, and inflects the words based on those features.\n",
      "While I wish the authors would apply the work to more than one language pair, I believe the issue addressed by this work is one of the most important and under-addressed problems with current MT systems. The approach taken by the authors is very different than many modern approaches based on BPE and character-level models, and instead harkens back to approaches such as \"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into Morphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013), both of which are unfortunately uncited.\n",
      "I am also rather suspicious of the fact that the authors present only METEOR results and no BLEU or qualitative improvements. If BLEU scores do not rise, perhaps the authors could argue why they believe their approach is still a net plus, and back the claim up with METEOR and example sentences.\n",
      "Furthermore, the authors repeatedly talk about gender and number as the two linguistic features they seek to correctly handle, but seem to completely overlook person. Perhaps this is because first and second person pronouns and verbs rarely occur in news, but certainly this point at least merits brief discussion. I would also like to see some discussion of why rescoring hurts with gender. If the accuracy is very good, shouldn the reranker learn to just keep the 1-best?\n",
      "Finally, while the content of this paper is good overall, it has a huge amount of spelling, grammar, word choice, and style errors that render it unfit for publication in its current form. Below is dump of some errors that I found.\n",
      "Overall, I would like to this work in a future conference, hopefully with more than one language pair, more evaluation metrics, and after further proofreading.\n",
      "General error dump: Line 062: Zhand --> Zhang Line 122: CFR --> CRF Whole related work section: consistent use of \\cite when \\newcite is appropriate It feels like there's a lot of filler: \"it is important to mention that\", \"it is worth mentioning that\", etc Line 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in general? \n",
      "Line 191: \"a software\" Line 196: \"academic and commercial level\" -- this should definitely be pluralized, but are these even levels? \n",
      "Line 210: \"a morphology-based simplified target\" makes it sound like this simplified target uses morphology. Perhaps the authors mean \"a morphologically simplified target\"? \n",
      "Line 217: \"decide on the morphological simplifications\"? \n",
      "Table 1: extra space in \"cuestión\" on the first line and \"titulado\" in the last line. \n",
      "Table 1: Perhaps highlight differences between lines in this table somehow? \n",
      "How is the simplification carried out? Is this simplifier hand written by the authors, or does it use an existing tool? \n",
      "Line 290: i.e. --> e.g. Line 294: \"train on\" or \"train for\" Line 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires our architecture\" Line 324: drop this comma Line 338: This equation makes it look like all words share the same word vector W Line 422: This could also be \"casas blancas\", right? How does the system choose between the sg. and pl. forms? Remind the reader of the source side conditioning here. \n",
      "Line 445: This graph is just a lattice, or perhaps more specifically a \"sausage lattice\" Line 499: Insert \"e.g.\" or similiar: (e.g. producirse) Line 500: misspelled \"syllable\" Line 500/503: I'd like some examples or further clarity on what palabras llanas and palabras estrújulas are and how you handle all three of these special cases. \n",
      "Line 570: \"and sentences longer than 50 words\" Line 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\" Line 574: are you sure this is an \"and\" and not an \"or\"? \n",
      "Line 596: \"trained for\" instead of \"trained on\" Line 597: corpus --> copora Line 604: size is --> sizes are Line 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable training times. \n",
      "Line 615: \"seven and five being the best values\" Line 617: Why 70? Increased from what to 70? \n",
      "Table 3: These are hyperparameters and not just ordinary parameters of the model Line 650: \"coverage exceeds 99%\"? \n",
      "Line 653: \"descending\" Line 666: \"quadratic\" Line 668: space before \\cites Line 676: \"by far\" or \"by a large margin\" instead of \"by large\" Line 716: below Line 729: \"The standard phrase-based ...\" zh-seg citation lists the year as 2016, but the tool actually was released in 2009 \n",
      "-------------------\n",
      "The paper describes a method for improving two-step translation using deep learning. Results are presented for Chinese->Spanish translation, but the approach seems to be largely language-independent.\n",
      "The setting is fairly typical for two-step MT. The first step translates into a morphologically underspecified version of the target language. The second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms (using a morphological generator). The main novelty of this work is the choice of deep NNs as classifiers in the second step. The authors also propose a rescoring step which uses a LM to select the best variant.\n",
      "Overall, this is solid work with good empirical results: the classifier models reach a high accuracy (clearly outperforming baselines such as SVMs) and the improvement is apparent even in the final translation quality.\n",
      "My main problem with the paper is the lack of a comparison with some straightforward deep-learning baselines. Specifically, you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step. ( Unless I misunderstood the approach.) But this is a sequence labeling task which RNNs are well suited for. How would e.g. a bidirectional LSTM network do when trained and used in the standard sequence labeling setting? After reading the author response, I still think that baselines (including the standard LSTM) are run in the same framework, i.e. independently for each local label. If that's not the case, it should have been clarified better in the response. This is a problem because you're not using the RNNs in the standard way and yet you don't justify why your way is better or compare the two approaches.\n",
      "The final re-scoring step is not entirely clear to me. Do you rescore n-best sentences? What features do you use? Or are you searching a weighted graph for the single optimal path? This needs to be explained more clearly in the paper. \n",
      "(My current impression is that you produce a graph, then look for K best paths in it, generate the inflected sentences from these K paths and *then* use a LM -- and nothing else -- to select the best variant. But I'm not sure from reading the paper.) This was not addressed in the response.\n",
      "You report that larger word embeddings lead to a longer training time. Do they also influence the final results?\n",
      "Can you attempt to explain why adding information from the source sentence hurts? This seems a bit counter-intuitive -- does e.g. the number information not get entirely lost sometimes because of this? I would appreciate a more thorough discussion on this in the final version, perhaps with a couple of convincing examples.\n",
      "The paper contains a number of typos and the general level of English may not be sufficient for presentation at ACL.\n",
      "Minor corrections: context of the application of MT -> context of application for MT In this cases, MT is faced in two-steps -> In this case, MT is divided into two steps markov -> Markov CFR -> CRF task was based on a direct translation -> task was based on direct translation task provided corpus -> task provided corpora the phrase-based system has dramatically -> the phrase-based approach... investigated different set of features -> ...sets of features words as source of information -> words as the source... correspondant -> corresponding Classes for gender classifier -> Classes for the... for number classifier -> for the... This layer's input consists in -> ...consists of to extract most relevant -> ...the most... Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer would produce (-1, 1).\n",
      "information of a word consists in itself -> ...of itself this $A$ set -> the set $A$ empty sentences and longer than 50 words -> empty sentences and sentences longer than... classifier is trained on -> classifier is trained in aproximately -> approximately coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand) in descendant order -> in descending order cuadratic -> quadratic (in multiple places) but best results -> but the best results Rescoring step improves -> The rescoring step... are not be comparable -> are not comparable \n",
      "-------------------\n",
      "This paper presents a method for generating morphology, focusing on gender and number, using deep learning techniques. From a morphologically simplified Spanish text, the proposed approach uses a classifier to reassign the gender and number for each token, when necessary. The authors compared their approach with other learning algorithms, and evaluated it in machine translation on the Chinese-to-Spanish (Zh->Es) translation direction.\n",
      "Recently, the task of generating gender and number has been rarely tackled, morphology generation methods usually target, and are evaluated on, morphologically-rich languages like German or Finnish. \n",
      "However, calling the work presented in this paper “morphology generation“ is a bit overselling as the proposed method clearly deals only with gender and number. And given the fact that some rules are handcrafted for this specific task, I do not think this method can be straightforwardly applied to do more complex morphology generation for morphologically-rich languages.\n",
      "This paper is relatively clear in the sections presenting the proposed method. \n",
      "A lot of work has been done to design the method and I think it can have some interesting impact on various NLP tasks. However the evaluation part of this work is barely understandable as many details of what is done, or why it is done, are missing. From this evaluation, we cannot know if the proposed method brings improvements over state-of-the-art methods while the experiments cannot be replicated. Furthermore, no analysis of the results obtained is provided. Since half a page is still available, there was the possibility to provide more information to make more clear the evaluation. This work lacks of motivation. Why do you think deep learning can especially improve gender and number generation over state-of-the-art methods?\n",
      "In your paper, the word “contribution“ should be used more wisely, as it is now in the paper, it is not obvious what are the real contributions (more details below).  abstract: what do you mean by unbalanced languages?\n",
      "section 1: You claim that your main contribution is the use of deep learning. Just the use of deep learning in some NLP task is not a contribution.\n",
      "section 2: You claim that neural machine translation (NMT), mentioned as “neural approximations“,  does not achieve state-of-the-art results for Zh->Es. I recommend to remove this claim from the paper, or to discuss it more, since Junczys-Dowmunt et al. (2016), during the last IWSLT, presented some results for Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10 BLEU points.\n",
      "section 5.1: You wrote that using the Zh->Es language pair is one of your main contributions. Just using a language pair is not a contribution. Nonetheless, I think it is nice to see a paper on machine translation that does not focus of improving machine translation for English. \n",
      "The numbers provided in Table 2 were computed before or after preprocessing? \n",
      "Why did you remove the sentences longer than 50 tokens? \n",
      "Precise how did you obtain development and test sets, or provide them. Your experiments are currently no replicable especially because of that.\n",
      "section 5.2: You wrote that you used Moses and its default parameters, but the default parameters of Moses are not the same depending on the version, so you should provide the number of the version used.\n",
      "section 5.3: What do you mean by “hardware cost“? \n",
      "Table 3: more details should be provided regarding how did you obtain these values. You chose these values given the classifier accuracy, but how precisely and on what data did you train and test the classifiers? On the same data used in section 6? \n",
      "If I understood the experiments properly, you used simplified Spanish. But I cannot find in the text how do you simplify Spanish. And how do you use it to train the classifier and the SMT system?  section 6: Your method is better than other classification algorithms, but it says nothing about how it performs compared to the state-of-the-art methods. You should at least precise why you chose these classifications algorithms for comparison. Furthermore, how your rules impact these results? And more generally, how do you explain such a high accuracy for you method? \n",
      "Did you implement all these classification algorithms by yourselves? If not, you must provide the URL or cite the framework you used. \n",
      "For the SMT experiments, I guess you trained your phrase table on simplified Spanish. You must precise it. \n",
      "You chose METEOR over other metrics like BLEU to evaluate your results. You must provide some explanation for this choice. I particularly appreciate when I see a MT paper that does not use BLEU for evaluation, but if you use METEOR, you must mention which version you used. METEOR has largely changed since 2005. \n",
      "You cited the paper of 2005, did you use the 2005 version? Or did you use the last one with paraphrases? \n",
      "Are your METEOR scores statistically significant?\n",
      "section 7: As future work you mentioned “further simplify morphology“. In this paper, you do not present any simplification of morphology, so I think that choosing the word “further“ is misleading.\n",
      "some typos: femenine ensambling cuadratic style: plain text citations should be rewritten like this: “(Toutanova et al, 2008) built “ should be “Toutanova et al. (2008) built “ place the caption of your tables below the table and not above, and with more space between the table and its caption. \n",
      "You used the ACL 2016 template. You must use the new one prepared for ACL 2017. \n",
      "More generally, I suggest that you read again the FAQ and the submission instructions provided on the ACL 2017 website. It will greatly help you to improve the paper. There are also important information regarding references: you must provide DOI or URL of all ACL papers in your references.\n",
      "----------------------- After authors response: Thank you for your response.\n",
      "You wrote that rules are added just as post-processing, but does it mean that you do not apply them to compute your classification results? Or if you do apply them before computing these results, I'm still wondering about their impact on these results.\n",
      "You wrote that Spanish is simplified as shown in Table 1, but it does not answer my question: how did you obtain these simplifications exactly? ( rules? \n",
      "software? etc.) The reader need to now that to reproduce your approach.\n",
      "The classification algorithms presented in Table 5 are not state-of-the-art, or if they are you need to cite some paper. Furthermore, this table only tells that deep learning gives the best results for classification, but it does not tell at all if your approach is better than state-of-the-art approach for machine translation. You need to compare your approach with other state-of-the-art morphology generation approaches (described in related work) designed for machine translation. If you do that your paper will be much more convincing in my opinion. \n",
      "-------------------\n",
      "This work showed that word representation learning can benefit from sememes when used in an appropriate attention scheme. Authors hypothesized that sememes can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL model which detects word senses and learn representations simultaneously. \n",
      "Though experimental results indicate that WRL benefits, exact gains for WSI are unclear since a qualitative case study of a couple of examples has only been done. Overall, paper is well-written and well-structured.\n",
      "In the last paragraph of introduction section, authors tried to tell three contributions of this work. ( 1) and (2) are more of novelties of the work rather than contributions. I see the main contribution of the work to be the results which show that we can learn better word representations (unsure about WSI) by modeling sememe information than other competitive baselines. ( 3) is neither a contribution nor a novelty.\n",
      "The three strategies tried for SE-WRL modeling makes sense and can be intuitively ranked in terms of how well they will work. Authors did a good job explaining that and experimental results supported the intuition but the reviewer also sees MST as a fourth strategy rather than a baseline inspired by Chen et al. 2014 (many WSI systems assume one sense per word given a context). \n",
      "MST many times performed better than SSA and SAC. Unless authors missed to clarify otherwise, MST seems to be exactly like SAT with a difference that target word is represented by the most probable sense rather than taking an attention weighted average over all its senses. MST is still an attention based scheme where sense with maximum attention weight is chosen though it has not been clearly mentioned if target word is represented by chosen sense embedding or some function of it.\n",
      "Authors did not explain the selection of datasets for training and evaluation tasks. Reference page to Sogou-T text corpus did not help as reviewer does not know Chinese language. It was unclear which exact dataset was used as there are several datasets mentioned on that page. Why two word similarity datasets were used and how they are different  (like does one has more rare words than another) since different models performed differently on these datasets. The choice of these datasets did not allow evaluating against results of other works which makes the reviewer wonder about next question.\n",
      "Are proposed SAT model results state of the art for Chinese word similarity? \n",
      "E.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by using CBOW word embeddings.\n",
      "Reviewer needs clarification on some model parameters like vocabulary sizes for words (Does Sogou-T contains 2.7 billion unique words) and word senses (how many word types from HowNet). Because of the notation used it is not clear if embeddings for senses and sememes for different words were shared. Reviewer hopes that is the case but then why 200 dimensional embeddings were used for only 1889 sememes. It would be better if complexity of model parameters can also be discussed.\n",
      "May be due to lack of space but experiment results discussion lack insight into observations other than SAT performing the best. Also, authors claimed that words with lower frequency were learned better with sememes without evaluating on a rare words dataset.\n",
      "I have read author's response. \n",
      "-------------------\n",
      "Review, ACL 2017, paper 256: This paper extends the line of work which models generation in dialogue as a sequence to sequence generation problem, where the past N-1 utterances (the ‘dialogue context’) are encoded into a context vector (plus potential other, hand-crafted features), which is then decoded into a response: the Nth turn in the dialogue. As it stands, such models tend to suffer from lack of diversity, specificity and local coherence in the kinds of response they tend to produce when trained over large dialogue datasets containing many topics (e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce diverse responses using the decoder, e.g. through word-by-word beam search (which has been shown not to work very well, even lose crucial information about grammar and valid sequences), or via a different objective function (such as in Li et. al.’s work) the authors introduce a latent variable, z, over which a probability distribution is induced as part of the network. At prediction time, after encoding utterances 1 to k, a context z is sampled, and the decoder is greedily used to generate a response from this. The evaluation shows small improvements in BLEU scores over a vanilla seq2seq model that does not involve learning a probability distribution over contexts and sampling from this.\n",
      "The paper is certainly impressive from a technical point of view, i.e. in the application of deep learning methods, specifically conditioned variational auto encoders, to the problem of response generation, and its attendant difficulties in training such models. Their use of Information-Retrieval techniques to get more than one reference response is also interesting.  I have some conceptual comments on the introduction and the motivations behind the work, some on the model architecture, and the evaluation which I write below in turn: Comments on the introduction and motivations….  The authors seem not fully aware of the long history of this field, and its various facets, whether from a theoretical perspective, or from an applied one.\n",
      "1. “[ the dialogue manager] typically takes a new utterance and the dialogue context as input, and generates discourse level decisions.”          This is not accurate. Traditionally at least, the job of the dialogue manager is to select actions (dialogue acts) in a particular dialogue context. \n",
      "The                    action chosen is then passed to a separate generation module for realisation. Dialogue management is usually done in the context of task-based systems which are goal driven. The dialogue manager is to choose actions which are optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few steps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer and colleagues, and various publications from Steve Young, Milica Gasic and colleagues for an overview of the large literature on Reinforcement Learning and MDP models for task-based dialogue systems.\n",
      "2. The authors need to make a clear distinction between task-based, goal-oriented dialogue, and chatbots/social bots, the latter being usually no more than a language model, albeit a sophisticated one (though see Wen et. al. 2016). What is required from these two types of system is usually distinct. \n",
      "Whereas the former is required to complete a task, the latter is, perhaps only required to keep the user engaged. Indeed the data-driven methods that have been used to build such systems are usually very different. \n",
      "3. The authors refer to ‘open-domain’ conversation. I would suggest that there is no such thing as open-domain conversation - conversation is always in the context of some activity and for doing/achieving something specific in the world. And it is this overarching goal, the overarching activity, this overarching genre, which determines the outward shape of dialogues and determines what sorts of dialogue structure are coherent. Coherence itself is activity/context-specific. Indeed a human is not capable of open-domain dialogue: if they are faced with a conversational topic or genre that they have never participated in, they would embarrass themselves with utterances that would look incoherent and out of place to others already familiar with it. \n",
      "(think of a random person on the street trying to follow the conversations at some coffee break at ACL). This is the fundamental problem I see with systems that attempt to use data from an EXTREMELY DIVERSE, open-ended set of conversational genres (e.g. movie subtitles) in order to train one model, mushing everything together so that what emerges at the other end is just very good grammatical structure. Or very generic responses.  Comments on the model architecture: Rather than generate from a single encoded context, the authors induce a distribution over possible contexts, sample from this, and generate greedily with the decoder. It seems to me that this general model is counter intuitive, and goes against evidence from the Linguistic/Psycholinguistic literature on dialogue: this literature shows that people tend to resolve potential problems in understanding and acceptance very locally - i.e. make sure they agree on what the context of the conversation is - and only then move on with the rest of the conversation, so that at any given point, there is little uncertainty about the current context of the conversation. The massive diversity one sees results from the diversity in what the conversation is actually trying to achieve (see above), diversity in topics and contexts etc, so that in a given, fixed context, there is a multitude of possible next actions, all coherent, but leading the conversation down a different path.\n",
      "It therefore seems strange to me at least to shift the burden of explaining diversity and coherence in follow-up actions to that of the linguistic/verbal/surface contexts in which they are uttered, though of course, uncertainty here can also arise as a result of mismatches in vocabulary, grammars, concepts, people’s backgrounds etc. But this probably wouldn’t explain much of the variation in follow-up response.  In fact, at least as far as task-based Dialogue systems are concerned, the challenge is to capture synonymy of contexts, i.e. dialogues that are distinct on the surface, but lead to the same or similar context, either in virtue of interactional and syntactic equivalence relations, or synonymy relations that might hold in a particular domain between words or sequences of words (e.g. “what is your destination?” = “where would you like to go?” in a flight booking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon, 2016 - the latter use a grammar to cluster semantically similar dialogues.\n",
      "Comments on the evaluation: The authors seek to show that their model can generate more coherent, and more diverse responses. The evaluation method, though very interesting, seems to address coherence but not diversity, despite what they say in section 5.2: The precision and recall metrics measure distance between ground truth utterances and the ones the model generates, but not that between the generated utterances themselves (unless I’m misunderstanding the evaluation method). \n",
      "See e.g. Li et al. who measure diversity by counting the number distinct n-grams in the generated responses.\n",
      "Furthermore, I’m not sure that the increase in BLEU scores are meaningful: they are very small. In the qualitative assessment of the generated responses, one certainly sees more diversity, and more contentful utterances in the examples provided. But I can’t see how frequent such cases in fact are.\n",
      "Also, it would have made for a stronger, more meaningful paper if the authors had compared their results with other work, (e.g. Li et. al) that use very different methods to promote diversity (e.g. by using a different objective function). The authors in fact do not mention this, or characterise it properly, despite actually referring to Li et. al. 2015. \n",
      "-------------------\n",
      "This paper presents a neural sequence-to-sequence model for encoding dialog contexts followed by decoding system responses in open-domain conversations. \n",
      "The authors introduced conditional variational autoencoder (CVAE) which is a deep neural network-based generative model to learn the latent variables for describing responses conditioning dialog contexts and dialog acts. \n",
      "The proposed models achieved better performances than the baseline based on RNN encoder-decoder without latent variables in both quantitative and qualitative evaluations.\n",
      "This paper is well written with clear descriptions, theoretically sound ideas, reasonable comparisons, and also detailed analysis. \n",
      "I have just a few minor comments as follows: - Would it be possible to provide statistical significance of the results from the proposed models compared to the baseline in quantitative evaluation? The differences don't seem that much for some metrics.\n",
      "- Considering the importance of dialog act in kgCVAE model, the DA tagging performances should affect the quality of the final results. Would it be there any possibility to achieve further improvement by using better DA tagger? \n",
      "Recently, deep learning models have achieved better performances than SVM also in DA tagging.\n",
      "- What do you think about doing human evaluation as a part of qualitative analysis? It could be costly, but worth a try to analyze the results in more pragmatic perspective.\n",
      "- As a future direction, it could be also interesting if kgCVAE model is applied to more task-oriented human-machine conversations which usually have much richer linguistic features available than open conversation.\n",
      "- In Table 1, 'BLUE-1 recall' needs to be corrected to 'BLEU-1 recall'. \n",
      "-------------------\n",
      "This is a nice paper on morphological segmentation utilizing word  embeddings. The paper presents a system which uses word embeddings to  both measure local semantic similarity of word pairs with a potential  morphological relation, and global information about the semantic validity of potential morphological segment types. The paper is well written and  represents a nice extension to earlier approaches on semantically driven  morphological segmentation.\n",
      "The authors present experiments on Morpho Challenge data for three  languages: English, Turkish and Finnish. These languages exhibit varying  degrees of morphological complexity. All systems are trained on Wikipedia  text.  The authors show that the proposed MORSE system delivers clear  improvements w.r.t. F1-score for English and Turkish compared to the well  known Morfessor system which was used as baseline. The system fails to  reach the performance of Morfessor for Finnish. As the authors note, this  is probably a result of the richness of Finnish morphology which leads to  data sparsity and, therefore, reduced quality of word embeddings. To  improve the performance for Finnish and other languages with a similar  degree of morphological complexity, the authors could consider word  embeddings which take into account sub-word information. For example, @article{DBLP:journals/corr/CaoR16,   author    = {Kris Cao and                Marek Rei},   title     = {A Joint Model for Word Embedding and Word Morphology},   journal   = {CoRR},   volume    = {abs/1606.02601},   year                  = {2016},   url                 = {http://arxiv.org/abs/1606.02601},   timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},   bibsource = {dblp computer science bibliography, http://dblp.org} } @article{DBLP:journals/corr/BojanowskiGJM16,   author    = {Piotr Bojanowski and                Edouard Grave and                Armand Joulin and                Tomas Mikolov},   title     = {Enriching Word Vectors with Subword Information},   journal   = {CoRR},   volume    = {abs/1607.04606},   year                  = {2016},   url                 = {http://arxiv.org/abs/1607.04606},   timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},   bibsource = {dblp computer science bibliography, http://dblp.org} } The authors critique the existing Morpho Challenge data sets. \n",
      "For example, there are many instances of incorrectly segmented words in  the material. Moreover, the authors note that, while some segmentations  in the the data set may be historically valid (for example the  segmentation of business into busi-ness), these segmentations are no  longer semantically motivated. The authors provide a new data set  consisting of 2000 semantically motivated segmentation of English word  forms from the English Wikipedia. They show that MORSE deliver highly  substantial improvements compared to Morfessor on this data set.\n",
      "In conclusion, I think this is a well written paper which presents  competitive results on the interesting task of semantically driven  morphological segmentation. The authors accompany the submission with  code and a new data set which definitely add to the value of the  submission. \n",
      "-------------------\n",
      "The authors use self-training to train a seq2seq-based AMR parser using a small annotated corpus and large amounts of unlabeled data. They then train a similar, seq2seq-based AMR-to-text generator using the annotated corpus and automatic AMRs produced by their parser from the unlabeled data. They use careful delexicalization for named entities in both tasks to avoid data sparsity. This is the first sucessful application of seq2seq models to AMR parsing and generation, and for generation, it most probably improves upon state-of-the art.\n",
      "In general, I really liked the approach as well as the experiments and the final performance analysis. \n",
      "The methods used are not revolutionary, but they are cleverly combined to achieve practial results. \n",
      "The description of the approach is quite detailed, and I believe that it is possible to reproduce the experiments without significant problems. \n",
      "The approach still requires some handcrafting, but I believe that this can be overcome in the future and that the authors are taking a good direction.\n",
      "(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another reviewer of a data overlap in the Gigaword and the Semeval 2016 dataset. This is potentially a very serious problem -- if there is a significant overlap in the test set, this would invalidate the results for generation (which are the main achievemnt of the paper). Unless the authors made sure that no test set sentences made their way to training through Gigaword, I cannot accept their results.\n",
      "(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer, which I fully agree with, is the  5.4 point claim when comparing to a system tested on an earlier version of the AMR dataset. The paper could probably still claim improvement over state-of-the art, but I am not sure I can accept the 5.4 points claim in a direct comparison to Pourdamghani et al. -- why haven't the authors also tested their system on the older dataset version (or obtained Pourdamghani et al.'s scores for the newer version)?\n",
      "Otherwise I just have two minor comments to experiments:  - Statistical significance tests would be advisable (even if the performance difference is very big for generation).\n",
      "- The linearization order experiment should be repeated with several times with different random seeds to overcome the bias of the particular random order chosen.\n",
      "The form of the paper definitely could be improved. \n",
      "The paper is very dense at some points and proofreading by an independent person (preferably an English native speaker) would be advisable. \n",
      "The model (especially the improvements over Luong et al., 2015) could be explained in more detail; consider adding a figure. The experiment description is missing the vocabulary size used. \n",
      "Most importantly, I missed a formal conclusion very much -- the paper ends abruptly after qualitative results are described, and it doesn't give a final overview of the work or future work notes.\n",
      "Minor factual notes: - Make it clear that you use the JAMR aligner, not the whole parser (at 361-364). Also, do you not use the recorded mappings also when testing the parser (366-367)?\n",
      "- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1 points, not 5.4 (at 578).\n",
      "- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\n",
      "Minor writing notes: - Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385, 650-659, 683, 694-695.\n",
      "- Inter-sentitial punctuation is sometimes confusing and does not correspond to my experience with English syntax. There are lots of excessive as well as missing commas.\n",
      "- There are a few typos (e.g., 375, 615), some footnotes are missing full stops.\n",
      "- The linearization description is redundant at 429-433 and could just refer to Sect. 3.3.\n",
      "- When refering to the algorithm or figures (e.g., near 529, 538, 621-623), enclose the references in brackets rather than commas.\n",
      "- I think it would be nice to provide a reference for AMR itself and for the multi-BLEU script.\n",
      "- Also mention that you remove AMR variables in Footnote 3.\n",
      "- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n",
      "- The order in Tables 1 and 2 seems a bit confusing to me, especially when your systems are not explicitly marked (I would expect your systems at the bottom). \n",
      "Also, Table 1 apparently lists development set scores even though its description says otherwise.\n",
      "- The labels in Table 3 are a bit confusing (when you read the table before reading the text).\n",
      "- In Figure 2, it's not entirely visible that you distinguish month names from month numbers, as you state at 376.\n",
      "- Bibliography lacks proper capitalization in paper titles, abbreviations and proper names should be capitalized (use curly braces to prevent BibTeX from lowercasing everything).\n",
      "- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually four authors.\n",
      "*** Summary: The paper presents first competitive results for neural AMR parsing and probably new state-of-the-art for AMR generation, using seq2seq models with clever preprocessing and exploiting large a unlabelled corpus. Even though revisions to the text are advisable, I liked the paper and would like to see it at the conference.  (RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with previous state-of-the-art on generation is entirely sound, and most importantly, whether the good results are not actually caused by data overlap of Gigaword (additional training set) with the test set.\n",
      "*** Comments after the authors' response: I thank the authors for addressing both of the major problems I had with the paper. I am happy with their explanation, and I raised my scores assuming that the authors will reflect our discussion in the final paper. \n",
      "-------------------\n",
      "This paper addresses the task of lexical entailment detection in context, e.g. is \"chess\" a kind of \"game\" given a sentence containing each of the words -- relevant for QA. The major contributions are: (1) a new dataset derived from WordNet using synset exemplar sentences, and  (2) a \"context relevance mask\" for a word vector, accomplished by elementwise multiplication with feature vectors derived from the context sentence. Fed to a logistic regression classifier, the masked word vectors just beat state of the art on entailment prediction on a PPDB-derived dataset from previous literature. Combined with other existing features, they beat state of the art by a few points. They also beats the baseline on the new WN-derived dataset, although the best-scoring method on that dataset doesn't use the masked representations.\n",
      "The paper also introduces some simple word similarity features (cosine, euclidean distance) which accompany other cross-context similarity features from previous literature. All of the similarity features, together, improve the classification results by a large amount, but the features in the present paper are a relatively small contribution.\n",
      "The task is interesting, and the work seems to be correct as far as it goes, but incremental. The method of producing the mask vectors is taken from existing literature on encoding variable-length sequences into min/max/mean vectors, but I don't think they've been used as masks before, so this is novel. \n",
      "However, excluding the PPDB features it looks like the best result does not use the representation introduced in the paper.\n",
      "A few more specific points: In the creation of the new Context-WN dataset, are there a lot of false negatives resulting from similar synsets in the \"permuted\" examples? If you take word w, with synsets i and j, is it guaranteed that the exemplar context for a hypernym synset of j is a bad entailment context for i? What if i and j are semantically close?\n",
      "Why does the masked representation hurt classification with the context-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well? \n",
      "Wouldn't the classifier learn to ignore the context-agnostic features?\n",
      "The paper should make clearer which similarity measures are new and which are from previous literature. It currently says that previous lit used the \"most salient\" similarity features, but that's not informative to the reader.\n",
      "The paper should be clearer about the contribution of the masked vectors vs the similarity features. It seems like similarity is doing most of the work.\n",
      "I don't understand the intuition behind the Macro-F1 measure, or how it relates to \"how sensitive are our models to changes in context\" -- what changes? How do we expect Macro-F1 to compare with F1?\n",
      "The cross-language task is not well motivated.\n",
      "Missing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms. \n",
      "Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING 2014.\n",
      "== I have read the author response. As noted in the original reviews, a quick examination of the tables shows that the similarity features make the largest contribution to the improvement in F-score on the two datasets (aside from PPDB features). The author response makes the point that similarities include contextualized representations. However, the similarity features are a mixed bag, including both contextualized and non-contextualized representations. This would need to be teased out more (as acknowledged in the response).\n",
      "Neither Table 3 nor 4 gives results using only the masked representations without the similarity features. This makes the contribution of the masked representations difficult to isolate. \n",
      "-------------------\n",
      "This paper introduces UCCA as a target representation for semantic parsing and also describes a quite successful transition-based parser for inference into that representation. I liked this paper a lot. I believe there is a lot of value simply in the introduction of UCCA (not new, but I believe relatively new to this community), which has the potential to spark new thinking about semantic representations of text. I also think the model was well thought out. \n",
      "While the model itself was fairly derivative of existing transition-based schemes, the extensions the authors introduced to make the model applicable in this domain were reasonable and well-explained, at what I believe to be an appropriate level of detail.\n",
      "The empirical evaluation was pretty convincing -- the results were good, as compared to several credible baselines, and the authors demonstrated this performance in multiple domains. My biggest complaint about this paper is the lack of multilingual evaluation, especially given that the formalism being experimented with is exactly one that is supposed to be fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in fact, I think the \"20k leagues\" corpus used in this paper is one such), so it would be good to see results in a language other than English.\n",
      "One minor point: in section 6, the authors refer to their model as \"grammarless\", which strikes me as not quite correct. It's true that the UCCA representation isn't derived from linguistic notions of syntax, but it still defines a way to construct a compositional abstract symbolic representation of text, which to me, is precisely a grammar. ( This is clearly a quibble, and I don't know why it irked me enough that I feel compelled to address it, but it did.)\n",
      "Edited to add: Thanks to the authors for their response. \n",
      "-------------------\n",
      "This paper presents the first parser to UCCA, a recently proposed meaning representation. The parser is transition based, and uses a new transition set designed to recover challenging discontinuous structures with reentrancies. \n",
      "Experiments demonstrate that the parser works well, and that it is not easy to build these representation on top of existing parsing approaches.  This is a well written and interesting paper on an important problem. The transition system is well motivated and seems to work well for the problem. The authors also did a very thorough experimental evaluation, including both varying the classifier for the base parser (neural, linear model, etc.) and also comparing to the best output you could get from other existing, but less expressive, parsing formulations. This paper sets a strong standard to UCCA parsing, and should also be interesting to researchers working with other expressive meaning representations or complex transition systems.  My only open question is the extent to which this new parser subsumes all of the other transition based parsers for AMR, SDP, etc. Could the UCCA transition scheme be used in these cases (which heuristic alignments if necessary), and would it just learn to not use the extra transitions for non-terminals, etc. \n",
      "Would it reduce to an existing algorithm, or perhaps work better? Answering this question isn’t crucial, the paper is very strong as is, but it would add to the overall understanding and could point to interesting areas for future work.\n",
      "---- I read the author response and agree with everything they say. \n",
      "-------------------\n",
      "This paper presents a new state-of-the-art deep learning model for semantic role labeling (SRL) that is a natural extension of the previous state-of-the-art system (Zhou and Xu, 2015) with recent best practices for initialization and regularization in the deep learning literature. \n",
      "The model gives a 10% relative error reduction which is a big gain on this task. The paper also gives in-depth empirical analyses to reveal the strengths and the remaining issues, that give a quite valuable information to the researchers in this field.  Even though I understand that the improvement of 3 point in F1 measure is a quite meaningful result from the engineering point of view, I think the main contribution of the paper is on the extensive analysis in the experiment section and a further in-depth investigation on analysis section. The detailed analyses shown in Section 4 are performed in a quite reasonable way and give both comparable results in SRL literature and novel information such as relation between accuracies in syntactic parsing and SRL. This type of analysis had often been omitted in recent papers. However, it is definitely important for further improvement.\n",
      "The paper is well-written and well-structured. \n",
      "I really enjoyed the paper and would like to see it accepted. \n",
      "-------------------\n",
      "This paper proposed a macro discourse structure scheme. The authors carried out a pilot study annotating a corpus consisting of 97 news articles from Chinese treebank 8.0. They then built a model to recognize the primary-secondary relations and 5 discourse relations (joint, elaboration, sequence, background, cause-result) in this corpus.\n",
      "The paper is poorly written and I have difficulties to follow it. I strongly suggest that the authors should find a native English speaker to carefully proofread the paper. Regarding the content, I have several concerns:  1 The logic of the paper is not clear and justifiable:  1) what are \"logical semantics\" and \"pragmatic function\"(line 115-116)? I'd prefer the authors to define them properly.\n",
      "2) macro discourse structure: there are some conflicts of the definition between macro structure and micro structure. Figure 4 demonstrates the combination of macro discourse structure and micro discourse structure. There, the micro discourse structure is presented *within paragraphs*. However, in the specific example of micro discourse structure shown in Figure 6, the micro-level discourse structure is *beyond the paragraph boundary* and captures the discourse relations across paragraphs. This kind of micro-level discourse structure is indeed similar to the macro structure proposed by the authors in Figure 5, and it's also genre independent. So, why can't we just use the structure in Figure 6? What's the advantage of macro discourse structure proposed in Figure 5? For me, it's genre dependent and doesn't provide richer information compared to Figure 6.\n",
      "By the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it because they are subtitles? But sentence 12 which is a subtitle is present there.\n",
      "2 Corpus construction (section 4) is not informative enough: without a detailed example, it's hard to know the meaning of \"discourse topic, lead, abstract, paragraph topics (line 627-629)\". And you were saying you \"explore the relationships between micro-structure and macro-structure\", but I can't find the correspondent part.\n",
      "Table 4 is about agreement study The authors claimed \"Its very difficult to achieve high consistence because the judgments of relation and structure are very subjective. Our measurement data is only taken on the layer of leaf nodes.\"--------> First, what are the leaf nodes? In the macro-level, they are paragraphs; in the micro-level, they are EDUs. Should we report the agreement study for macro-level and micro-level separately? Second, it seems for me that the authors only take a subset of data to measure the agreement. This doesn't reflect the overall quality of the whole corpus, i.e., high agreement on the leaf nodes annotation doesn't ensure that we will get high agreement on the non-leaf nodes annotation.\n",
      "Some other unclear parts in section 4: Table 4: \"discourse structure, discourse relation\" are not clear, what is discourse structure and what is discourse relation? \n",
      "Table 5: \"amount of macro discourse relations\", still not clear to me, you mean the discourse relations between paragraphs? But in Figure 6, these relations can exist both between sentences and between paragraphs.\n",
      "3 Experiments: since the main purpose of the paper is to provide richer discourse structure (both on macro and micro level), I would expect to see some initial results in this direction. The current experiment is not very convincing: a) no strong baselines; b) features are not clearly described and motivated; c) I don't understand why only a sub set of discourse relations from Table 6 is chosen to perform the experiment of discourse relation recognition.\n",
      "In general, I think the paper needs major improvement and currently it is not ready for acceptance. \n",
      "-------------------\n",
      "This paper presents a unified annotation that combines macrostructures and RST structure in Chinese news articles. Essentially, RST structure is adopted for each paragraph and macrostructure is adopted on top of the paragraphs. \n",
      "While the view that nuclearity should not depend on the relation label itself but also on the context is appealing, I find the paper having major issues in the annotation and the experiments, detailed below: - The notion of “primary-secondary” relationship is advocated much in the paper, but later in the paper that it became clear this is essentially the notion of nuclearity, extended to macrostructure and making it context-dependent instead of relation-dependent. Even then, the status nuclear-nuclear, nuclear-satellite, satellite-nuclear are “redefined” as new concepts.\n",
      "- Descriptions of established theories in discourse are often incorrect. For example, there is rich existing work on pragmatic functions of text but it is claimed to be something little studied. There are errors in the related work section, e.g., treating RST and the Chinese Dependency Discourse Treebank as different as coherence and cohesion; the computational approach subsection lacking any reference to work after 2013; the performance table of nuclearity classification confusing prior work for sentence-level and document-level parsing.\n",
      "- For the annotation, I find the macro structure annotation description confusing; furthermore, statistics for the macro labels are not listed/reported. The agreement calculation is also problematic; the paper stated that \"Our measurement data is only taken on the layer of leaf nodes\". I don't think this can verify the validity of the annotation. There are multiple mentions in the annotation procedure that says “prelim experiments show this is a good approach”, but how? Finally it is unclear how the kappa values are calculated since this is a structured task; is this the same calculation as RST discourse treebank?\n",
      "- It is said in the paper that nuclearity status closely associates with the relation label itself. So what is the baseline performance that just uses the relation label? Note that some features are not explained at all (e.g., what are “hierarchical characteristics”?)\n",
      "- The main contribution of the paper is the combination of macro and micro structure. However, in the experiments only relations at the micro level are evaluated; even so, only among 5 handpicked ones. I don't see how this evaluation can be used to verify the macro side hence supporting the paper.\n",
      "- The paper contains numerous grammatical errors. Also, there is no text displayed in Figure 7 to illustrate the example. \n",
      "-------------------\n",
      "This paper focuses on interpreting sarcasm written in Twitter identifying sentiment words and then using a machine translation engine to find an equivalent not sarcastic tweet.  EDIT: Thank you for your answers, I appreaciate it. I added one line commenting about it.\n",
      "- Strengths: Among the positive aspects of your work, I would like to mention the parallel corpus you presented. I think it will be very useful for other researchers in the area for identifying and interpreting sarcasm in social media. An important contribution is also the attempt to evaluate the parallel corpora using existing measures such as the ones used in MT tasks. But also because you used human judgements to evaluate the corpora in 3 aspects: fluency, adequacy and equivalent sentiment.\n",
      "- Room for improvement: Tackling the problem of interpretation as a monolingual machine translations task is interesting, while I do appreciate the intent to compare the MT with two architectures, I think that due the relatively small dataset (needed for RNN) used it was predictable that the “Neural interpretation” is performing worse than “moses interpretation”. You came to the same conclusion after seeing the results in Table3. In addition to comparing with this architecture, I would've liked to see other configuration of the MT used with moses. Or at least, you should provide some explanation of why you use the configuration described in lines 433 through 442; to me this choice is not justified. \n",
      "  - thank you for your response, I understand it is difficult to write down all the details but I hope you include a line with some of your answer in the paper, I believe this could add valuable information.\n",
      "When you presented SING, it is clear that you evaluate some of its components beforehand, i.e. the MT. But other important components are not evaluated, particularly, the clustering you used of positive and negative words. While you did said you used k-means as a clustering algorithm it is not clear to me why you wanted to create clusters with 10 words. Why not test with other number of k, instead of 7 and 16, for positive and negative words respectively. Also you could try another algorithm beside kmeans, for instance, the star clustering algorithm (Aslam et al. 2004), that do not require a k parameter. \n",
      "   - thanks for clarifying.\n",
      "You say that SIGN searches the tweet for sentiment words if it found one it changes it for the cluster ID that contain that word. I am assuming that there is not a limit for the number of sentiment words found, and the MT decides by itself how many sentiment words to change. For example, for the tweet provided in Section 9: “Constantly being irritated, anxious and depressed is a great feeling” the clustering stage of SIGN should do something like “Constantly being cluster-i, cluster-j and cluster-k is a cluster-h feeling”, Is that correct? If not, please explain what SIGN do. \n",
      "    - Thanks for clarifying - Minor comments: In line 704, section 7, you said: “SIGN-context’s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which come closer to the 73.8% in the gold standard human interpretations.” This means that 25% of the human interpretations are the same as the original tweet? Do you have any idea why is that?\n",
      "In section 6, line 539 you could eliminate the footnote 7 by adding “its cluster ID” or “its cluster number”.\n",
      "References: Aslam, Javed A., Pelekhov, Ekaterina, and Rus, Daniela. \" The star clustering algorithm for static and dynamic information organization..\" Journal of Graph Algorithms and Applications 8.1 (2004): 95-129. <http://eudml.org/doc/51529>. \n",
      "-------------------\n",
      "This paper presents a purpose-built neural network architecture for textual entailment/NLI based on a three step process of encoding, attention-based matching, and aggregation. The model has two variants, one based on TreeRNNs and the other based on sequential BiLSTMs. The sequential model outperforms all published results, and an ensemble with the tree model does better still.\n",
      "The paper is clear, the model is well motivated, and the results are impressive. Everything in the paper is solidly incremental, but I nonetheless recommend acceptance.  Major issues that I'd like discussed in the response: – You suggest several times that your system can serve as a new baseline for future work on NLI. This isn't an especially helpful or meaningful claim—it could be said of just about any model for any task. You could argue that your model is unusually simple or elegant, but I don't think that's really a major selling point of the model. \n",
      "– Your model architecture is symmetric in some ways that seem like overkill—you compute attention across sentences in both directions, and run a separate inference composition (aggregation) network for each direction. This presumably nearly doubles the run time of your model. Is this really necessary for the very asymmetric task of NLI? Have you done ablation studies on this?** \n",
      "– You present results for the full sequential model (ESIM) and the ensemble of that model and the tree-based model (HIM). Why don't you present results for the tree-based model on its own?**\n",
      "Minor issues: – I don't think the Barker and Jacobson quote means quite what you want it to mean. In context, it's making a specific and not-settled point about *direct* compositionality in formal grammar. You'd probably be better off with a more general claim about the widely accepted principle of compositionality. \n",
      "– The vector difference feature that you use (which has also appeared in prior work) is a bit odd, since it gives the model redundant parameters. Any model that takes vectors a, b, and (a - b) as input to some matrix multiplication is exactly equivalent to some other model that takes in just a and b and has a different matrix parameter. There may be learning-related reasons why using this feature still makes sense, but it's worth commenting on. \n",
      "– How do you implement the tree-structured components of your model? Are there major issues with speed or scalability there? \n",
      "\u001f– Typo: (Klein and D. Manning, 2003)  – Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much more readable parse trees without crossing lines. I'd suggest using them.\n",
      "--- Thanks for the response! I still solidly support publication. This work is not groundbreaking, but it's novel in places, and the results are surprising enough to bring some value to the conference. \n",
      "-------------------\n",
      "This paper introduces new configurations and training objectives for neural sequence models in a multi-task setting. As the authors describe well, the multi-task setting is important because some tasks have shared information and in some scenarios learning many tasks can improve overall performance.\n",
      "The methods section is relatively clear and logical, and I like where it ended up, though it could be slightly better organized. The organization that I realized after reading is that there are two problems: 1) shared features end up in the private feature space, and 2) private features end up in the  shared space. There is one novel method for each problem. That organization up front would make the methods more cohesive. In any case, they introduce one  method that keeps task-specific features out of shared representation (adversarial loss) and another to keep shared features out of task-specific representations (orthogonality constraints). My only point of confusion is the adversarial system. \n",
      "After LSTM output there is another layer, D(s^k_T, \\theta_D), relying on parameters U and b. This output is considered a probability distribution which is compared against the actual. This means it is possible it will just learn U and b that effectively mask task-specific information from  the LSTM outputs, and doesn't  seem like it can guarantee task-specific information is removed.\n",
      "Before I read the evaluation section I wrote down what I hoped the experiments would look like and it did most of it. This is an interesting idea and there are  a lot more experiments one can imagine but I think here they have the basics to show the validity of their methods. It would be helpful to have best known results on these tasks.\n",
      "My primary concern with this paper is the lack of deeper motivation for the  approach. I think it is easy to understand that in a totally shared model there will be problems due to conflicts in feature space. The extension to  partially shared features seems like a reaction to that issue -- one would  expect that the useful shared information is in the shared latent space and  each task-specific space would learn features for that space. Maybe this works and maybe it doesn't, but the logic is clear to me. In contrast, the authors seem to start from the assumption that this \"shared-private\" model has this issue. I expected the argument flow to be 1) Fully-shared obviously has this problem; 2) shared-private seems to address this; 3) in practice shared-private does not fully address this issue for reasons a,b,c.; 4) we introduce a method that more effectively constrains the spaces. \n",
      "Table 4 helped me to partially understand what's going wrong with shared-private and what your methods do; some terms are _usually_ one connotation or another, and that general trend can probably get them into the shared feature space. This simple explanation, an example, and a more logical argument flow would help the introduction and make this a really nice reading paper.\n",
      "Finally, I think this research ties into some other uncited MTL work [1], which does deep hierarchical MTL - supervised POS tagging at a lower level, chunking at the next level up, ccg tagging higher, etc. They then discuss at the end some of the qualities that make MTL possible and conclude that MTL only works \"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this previous work because potentially this model could learn what sufficiently similar is -- i.e., if two tasks are not sufficiently similar the shared model would learn nothing and it would fall back to learning two independent systems, as compared to a shared-private model baseline that might overfit and perform poorly.\n",
      "[1] @inproceedings{sogaard2016deep,   title={Deep multi-task learning with low level tasks supervised at lower layers},   author={S{\\o}gaard, Anders and Goldberg, Yoav},   booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},   volume={2},   pages={231--235},   year={2016},   organization={Association for Computational Linguistics} } \n",
      "-------------------\n",
      "This paper presents a graph-based approach for producing sense-disambiguated synonym sets from a collection of undisambiguated synonym sets.  The authors evaluate their approach by inducing these synonym sets from Wiktionary and from a collection of Russian dictionaries, and then comparing pairwise synonymy relations (using precision, recall, and F1) against WordNet and BabelNet (for the English synonym sets) or RuThes and Yet Another RussNet (for the Russian synonym sets).\n",
      "The paper is very well written and structured.              The experiments and evaluations (or at least the prose parts) are very easy to follow.              The methodology is sensible and the analysis of the results cogent.  I was happy to observe that the objections I had when reading the paper (such as the mismatch in vocabulary between the synonym dictionaries and gold standards) ended up being resolved, or at least addressed, in the final pages.\n",
      "The one thing about the paper that concerns me is that the authors do not seem to have properly understood the previous work, which undercuts the stated motivation for this paper.\n",
      "The first instance of this misunderstanding is in the paragraph beginning on line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a discussion of resources that are \"not formally structured\" and that contain \"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the other two resources by using a formal structure (a relational database) based on word senses rather than orthographic forms.              Translations, synonyms, and other semantic annotations in OmegaWiki are therefore unambiguous.\n",
      "The second, and more serious, misunderstanding comes in the three paragraphs beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet and UBY \"rely on English WordNet as a pivot for mapping of existing resources\" and criticizes this mapping as being \"error-prone\".  Though it is true that BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a general-purpose specification for the representation of lexical-semantic resources and of links between them.  It exists independently of any given lexical-semantic resource (including WordNet) and of any given alignment between resources (including ones based on \"similarity of dictionary definitions\" or \"cross-lingual links\").  Its maintainers have made available various databases adhering to the UBY spec; these contain a variety of lexical-semantic resources which have been aligned with a variety of different methods.  A given UBY database can be *queried* for synsets, but UBY itself does not *generate* those synsets.  Users are free to produce their own databases by importing whatever lexical-semantic resources and alignments thereof are best suited to their purposes.  The three criticisms of UBY on lines 120 to 125 are therefore entirely misplaced.\n",
      "In fact, I think at least one of the criticisms is not appropriate even with respect to BabelNet.  The authors claim that Watset may be superior to BabelNet because BabelNet's mapping and use of machine translation are error-prone.  The implication here is that Watset's method is error-free, or at least significantly less error-prone.  This is a very grandiose claim that I do not believe is supported by what the authors ought to have known in advance about their similarity-based sense linking algorithms and graph clustering algorithms, let alone by the results of their study.  I think this criticism ought to be moderated.              Also, I think the third criticism (BabelNet's reliance on WordNet as a pivot) somewhat misses the point -- surely the most important issue to highlight isn't the fact that the pivot is English, but rather that its synsets are already manually sense-annotated.\n",
      "I think the last paragraph of §1 and the first two paragraphs of §2 should be extensively revised. They should focus on the *general* problem of generating synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016 for a survey), rather than particularly on BabelNet (which uses certain particular methods) and UBY (which doesn't use any particular methods, but can aggregate the results of existing ones).  It may be helpful to point out somewhere that although alignment/translation methods *can* be used to produce synsets or to enrich existing ones, that's not always an explicit goal of the process.  Sometimes it's just a serendipitous (if noisy) side-effect of aligning/translating resources with differing granularities.\n",
      "Finally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI of JoBimText are criticized for including too many words that are hypernyms, co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear in the output of Watset?  (We can get only a very vague idea of this from comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset really is better at filtering out words with other semantic relations, then it would be nice to see some quantitative evidence of this.\n",
      "Some further relatively minor points that should nonetheless be fixed: - Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather useless.  Why bother mentioning their analysis if you're not going to tell us what they found?\n",
      "- Line 091: It took me a long time to figure out how \"wat\" has any relation to \"discover the correct word sense\".  I suppose this is supposed to be a pun on \"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at least consider rewording the sentence to better explain the pun.\n",
      "- Figure 2 is practically illegible owing to the microscopic font.  Please increase the text size!\n",
      "- Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use a larger font.              To save space, consider abbreviating the headers (\"P, \"R\", \"F1\") and maybe reporting scores in the range 0–100 instead of 0–1, which will eliminate a leading 0 from each column.\n",
      "- Lines 517–522: Wiktionary is a moving target.  To help others replicate or compare against your work, please indicate the date of the Wiktionary database dump you used.\n",
      "- Throughout: The constant switching between Times and Computer Modern is distracting.  The root of this problem is a longstanding design flaw in the ACL 2017 LaTeX style file, but it's exacerbated by the authors' decision to occasionally set numbers in math mode, even in running text.  Please fix this by removing \\usepackage{times} from the preamble and replacing it with either \\usepackage{newtxtext} \\usepackage{newtxmath} or \\usepackage{mathptmx} References: I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan & Claypool.\n",
      "---- I have read the author response. \n",
      "-------------------\n",
      "The work describes a joint neural approach to argumentation mining. There are several approaches explored including:  1) casting the problem as a dependency parsing problem (trying several different parsers)  2) casting the problem as a sequence labeling problem 3) multi task learning (based on sequence labeling model underneath) 4) an out of the box neural model for labeling entities and relations (LSTM-ER) 5) ILP based state-of-the art models All the approaches are evaluated using F1 defined on concepts and relations. \n",
      "Dependency based solutions do not work well, seq. labeling solutions are effective. \n",
      "The out-of-the-box LSTM-ER model performs very well. Especially on paragraph level. \n",
      "The Seq. labeling and LSTM-ER models both outperform the ILP approach. \n",
      "A very comprehensive supplement was given, with all the technicalities of training the models, optimizing hyper-parameters etc. \n",
      "It was also shown that sequence labeling models can be greatly improved by the multitask approach (with the claim task helping more than the relation task). \n",
      "The aper  is a very thorough investigation of neural based approaches to end-to-end argumentation mining.\n",
      "- Major remarks     - my one concern is with the data set, i'm wondering if it's a problem that essays in the train set and in the test set might    be on the same topics, consequently writers might use the same or similar arguments in both essays, leading to information    leakage from the train to the test set. In turn, this might give overly optimistic performance estimates. Though, i think the same    issues are present for the ILP models, so your model does not have an unfair advantage. Still, this may be something to discuss.\n",
      "  - my other concern is that one of your best models LSTM-ER is acutally just a an out-of-the box application of a model from related     work. However, given the relative success of sequence based models and all the experiments and useful lessons learned, I think this      work deserves to be published.\n",
      "- Minor remarks and questions: 222 - 226 - i guess you are arguing that it's possible to reconstruct the full graph once you get a tree as output? Still, this part is not quite clear. \n",
      "443-444 The ordering in this section is seq. tagging -> dependency based -> MTL using seq. tagging, it would be much easier to follow if the order of the first two were                   reversed (by the time I got here i'd forgotten what STag_T stood for) 455 - What does it mean that it de-couples them but jointly models them (isn't coupling them required to jointly model them?) \n",
      "         - i checked Miwa and Bansal and I couldn't find it 477 - 479 -  It's confusing when you say your system de-couples relation info from entity info, my best guess is that you mean it                         learns some tasks as \"the edges of the tree\" and some other tasks as \"the labels on those edges\", thus decoupling them. \n",
      "                        In any case,  I recommend you make this part clearer Are the F1 scores in the paragraph and essay settings comparable? In particular for the relation tasks. I'm wondering if paragraph based  models might miss some cross paragraph relations by default, because they will never consider them. \n",
      "-------------------\n",
      "This paper presents a text classification method based on pre-training technique using both labeled and unlabeled data. The authors reported experimental results with several benchmark data sets including TREC data, and showed that the method improved overall performance compared to other comparative methods.\n",
      "I think the approach using pre-training and fine-tuning itself is not a novel one, but the originality is the use of both labeled and unlabeled data in the pre-training step. \n",
      "The authors compare their results against three baselines, i.e. without pre-training and a deep learning with unsupervised pre-training using deep autoencoders, but I think that I would be interesting to compare the method against other methods presented in the introduction section. \n",
      "-------------------\n",
      "The paper models the relation extraction problem as reading comprehension and extends a previously proposed reading comprehension (RC) model to extract unseen relations. The approach has two main components: 1. Queryfication: Converting a relation into natural question. Authors use crowdsourcing for this part.\n",
      "2. Applying RC model on the generated questions and sentences to get the answer spans. Authors extend a previously proposed approach to accommodate situations where there is no correct answer in the sentence.\n",
      "My comments: 1. The paper reads very well and the approach is clearly explained.\n",
      "2. In my opinion, though the idea of using RC for relation extraction is interesting and novel, the approach is not novel. A part of the approach is crowdsourced and the other part is taken directly from a previous work, as I mention above.\n",
      "3. Relation extraction is a well studied problem and there are plenty of recently published works on the problem. However, authors do not compare their methods against any of the previous works. This raises suspicion on the effectiveness of the approach. As seen from Table 2, the performance numbers of the proposed method on the core task are not very convincing. However, this maybe because of the dataset used in the paper. Hence, a comparison with previous methods would actually help assess how the current method stands with the state-of-the-art.\n",
      "4. Slot-filling data preparation: You say \"we took the first sentence s in D to contain both e and a\". How can you get the answer sentence for (all) the relations of an entity from the first sentence of the entity's Wikipedia article? Please clarify this. See the following paper. They have a set of rules to locate (answer) sentences corresponding to an entity property in its Wikipedia page: Wu, Fei, and Daniel S. Weld. \" Open information extraction using Wikipedia.\" \n",
      "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.\n",
      "Overall, I think the paper presents an interesting approach. However, unless the effectiveness of the approach is demonstrated by comparing it against recent works on relation extraction, the paper is not ready for publication. \n",
      "-------------------\n",
      "This paper delves into the mathematical properties of the skip-gram model, explaining the reason for its success on the analogy task and for the general superiority of additive composition models. It also establishes a link between skip-gram and Sufficient Dimensionality Reduction.\n",
      "I liked the focus of this paper on explaining the properties of skip-gram, and generally found it inspiring to read. I very much appreciate the effort to understand the assumptions of the model, and the way it affects (or is affected by) the composition operations that it is used to perform. In that respect, I think it is a very worthwhile read for the community.\n",
      "My main criticism is however that the paper is linguistically rather naive. The authors' use of 'compositionality' (as an operation that takes a set of words and returns another with the same meaning) is extremely strange. Two words can of course be composed and produce a vector that is a) far away from both; b) does not correspond to any other concept in the space; c) still has meaning (productivity wouldn't exist otherwise!) Compositionality in linguistic terms simply refers to the process of combining linguistic constituents to produce higher-level constructs. It does not assume any further constraint, apart from some vague (and debatable) notion of semantic transparency. The paper's implication (l254) that composition takes place over sets is also wrong: ordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a well-known shortcoming of additive composition.  Another important aspect is that there are pragmatic factors that make humans prefer certain phrases to single words in particular contexts (and the opposite), naturally changing the underlying distribution of words in a large corpus. For instance, talking of a 'male royalty' rather than a 'king' or 'prince' usually has implications with regard to the intent of the speaker (here, perhaps highlighting a gender difference). This means that the equation in l258 (or for that matter the KL-divergence modification) does not hold, not because of noise in the data, but because of fundamental linguistic processes. \n",
      "This point may be addressed by the section on SDR, but I am not completely sure (see my comments below).\n",
      "In a nutshell, I think the way that the authors present composition is flawed, but the paper convinces me that this is indeed what happens in skip-gram, and I think this is an interesting contribution.  The part about Sufficient Dimensionality Reduction seems a little disconnected from the previous argument as it stands. I'm afraid I wasn't able to fully follow the argument, and I would be grateful for some clarification in the authors' response. If I understand it well, the argument is that skip-gram produces a model where a word's neighbours follow some exponential parametrisation of a categorical distribution, but it is unclear whether this actually reflects the distribution of the corpus (as opposed to what happens in, say, a pure count-based model). The fact that skip-gram performs well despite not reflecting the data is that it implements some form of SDR, which does not need to make any assumption about the underlying form of the data. But then, is it fair to say that the resulting representations are optimised for tasks where geometrical regularities are important, regardless of the actual pattern of the data? I.e. there some kind of denoising going on?\n",
      "Minor comments: - The abstract is unusually long and could, I think, be shortened.\n",
      "- para starting l71: I think it would be misconstrued to see circularity here. \n",
      "Firth observed that co-occurrence effects were correlated with similarity judgements, but those judgements are the very cognitive processes that we are trying to model with statistical methods. Co-occurrence effects and vector space word representations are in some sense 'the same thing', modelling an underlying linguistic process we do not have direct observations for. So pair-wise similarity is not there to break any circularity, it is there because it better models the kind of judgements humans known to make.\n",
      "- l296: I think 'paraphrase' would be a better word than 'synonym' here, given that we are comparing a set of words with a unique lexical item.\n",
      "- para starting l322: this is interesting, and actually, a lot of the zipfian distribution (the long tail) is fairly uniform.\n",
      "- l336: it is probably worth pointing out that the analogy relation does not hold so well in practice and requires to 'ignore' the first returned neighbour of the analogy computation (which is usually one of the observed terms).\n",
      "- para starting l343: I don't find it so intuitive to say that 'man' would be a synonym/paraphrase of anything involving 'woman'. The subtraction involved in the analogy computation is precisely not a straightforward composition operation, as it involves an implicit negation.  - A last, tiny general comment. It is usual to write p(w|c) to mean the probability of a word given a context, but in the paper 'w' is actually the context and 'c' the target word. It makes reading a little bit harder... Perhaps change the notation?\n",
      "Literature: The claim that Arora (2016) is the only work to try and understand vector composition is a bit strong. For instance, see the work by Paperno & Baroni on explaining the success of addition as a composition method over PMI-weighted vectors: D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its parts: How composition affects PMI values in distributional semantic vectors. \n",
      "Computational Linguistics 42(2): 345-350.\n",
      "*** I thank the authors for their response and hope to see this paper accepted. \n",
      "-------------------\n",
      "This paper presents a neural network-based framework for dialogue state tracking. \n",
      "The main contribution of this work is on learning representations of user utterances, system outputs, and also ontology entries, all of which are based on pre-trained word vectors. \n",
      "Particularly for the utterance representation, the authors compared two different neural network models: NBT-DNN and NBT-CNN. \n",
      "The learned representations are combined with each other and finally used in the downstream network to make binary decision for a given slot value pair. \n",
      "The experiment shows that the proposed framework achieved significant performance improvements compared to the baseline with the delexicalized approach.\n",
      "It's generally a quality work with clear goal, reasonable idea, and improved results from previous studies. \n",
      "But the paper itself doesn't seem to be very well organized to effectively deliver the details especially to readers who are not familiar with this area.\n",
      "First of all, more formal definition of DST needs to be given at the beginning of this paper. \n",
      "It is not clear enough and could be more confusing after coupling with SLU. \n",
      "My suggestion is to provide a general architecture of dialogue system described in Section 1 rather than Section 2, followed by the problem definition of DST focusing on its relationships to other components including ASR, SLU, and policy learning.\n",
      "And it would also help to improve the readability if all the notations used throughout the paper are defined in an earlier section. \n",
      "Some symbols (e.g. t_q, t_s, t_v) are used much earlier than their descriptions.\n",
      "Below are other comments or questions: - Would it be possible to perform the separate SLU with this model? If no, the term 'joint' could be misleading that this model is able to handle both tasks.\n",
      "- Could you please provide some statistics about how many errors were corrected from the original DSTC2 dataset? \n",
      "If it is not very huge, the experiment could include the comparisons also with other published work including DSTC2 entries using the same dataset.\n",
      "- What do you think about using RNNs or LSTMs to learn the sequential aspects in learning utterance representations? \n",
      "Considering the recent successes of these recurrent networks in SLU problems, it could be effective to DST as well.\n",
      "- Some more details about the semantic dictionary used with the baseline would help to imply the cost for building this kind of resources manually.\n",
      "- It would be great if you could give some samples which were not correctly predicted by the baseline but solved with your proposed models. \n",
      "-------------------\n",
      "This paper proposed to explore discourse structure, as defined by Rhetorical Structure Theory (RST) to improve text categorization. A RNN with attention mechanism is employed to compute a representation of text. The experiments on various of dataset shows the effectiveness of the proposed method. Below are my comments: (1) From Table 2, it shows that “UNLABELED” model performs better on four out of five datasets than the “FULL” model. The authors should explain more about this, because intuitively, incorporating additional relation labels should bring some benefits. Is the performance of relation labelling so bad and it hurts the performance instead?\n",
      "(2) The paper also transforms the RST tree into a dependency structure as a pre-process step. Instead of transforming, how about keep the original tree structure and train a hierarchical model on that?\n",
      "(3) For the experimental datasets, instead of comparing with only one dataset with each of the previous work, the authors may want to run experiments on more common datasets used by previous work. \n",
      "-------------------\n",
      "This paper describes interesting and ambitious work: the automated conversion of Universal Dependency grammar structures into [what the paper calls] semantic logical form representations.  In essence, each UD construct is assigned a target construction in logical form, and a procedure is defined to effect the conversion, working ‘inside-out’ using an intermediate form to ensure proper nesting of substructures into encapsulating ones.  Two evaluations are carried out: comparing the results to gold-standard lambda structures and measuring the effectiveness of the resulting lambda expressions in actually delivering the answers to questions from two QA sets.   It is impossible to describe all this adequately in the space provided.  The authors have taken some care to cover all principal parts, but there are still many missing details.  I would love to see a longer version of the paper! \n",
      "Particularly the QA results are short-changed; it would have been nice to learn which types of question are not handled, and which are not answered correctly, and why not.  This information would have been useful to gaining better insight into the limitations of the logical form representations.   That leads to my main concern/objection.  This logical form representation is not in fact a ‘real’ semantic one.                          It is, essentially, a rather close rewrite of the dependency structure of the input, with some (good) steps toward ‘semanticization’, including the insertion of lambda operators, the explicit inclusion of dropped arguments (via the enhancement operation), and the introduction of appropriate types/units for such constructions as eventive adjectives and nouns like “running horse” and “president in 2009”.  But many (even simple) aspects of semantic are either not present (at least, not in the paper) and/or simply wrong.  Missing: quantification (as in “every” or “all”); numbers (as in “20” or “just over 1000”); various forms of reference (as in “he”, “that man”, “what I said before”); negation and modals, which change the semantics in interesting ways; inter-event relationships (as in the subevent relationship between the events in “the vacation was nice, but traveling was a pain”; etc. etc.  To add them one can easily cheat, by treating these items as if they were just unusual words and defining obvious and simple lambda formulas for them.  But they in fact require specific treatment; for example, a number requires the creation of a separate set object in the representation, with its own canonical variable (allowing later text to refer to “one of them” and bind the variable properly).  For another example, Person A’s model of an event may differ from Person B’s, so one needs two representation symbols for the event, plus a coupling and mapping between them.  For another example, one has to be able to handle time, even if simply by temporally indexing events and states.  None of this is here, and it is not immediately obvious how this would be added.  In some cases, as DRT shows, quantifier and referential scoping is not trivial.   It is easy to point to missing things, and unfair to the paper in some sense; you can’t be expected to do it all.  But you cannot be allowed to make obvious errors.  Very disturbing is the assignment of event relations strictly in parallel with the verb’s (or noun’s) syntactic roles.  No-one can claim seriously that “he broke the window” and “the window broke” has “he” and “the window” filling the same semantic role for “break”. \n",
      "That’s simply not correct, and one cannot dismiss the problem, as the paper does, to some nebulous subsequent semantic processing.                          This really needs adequate treatment, even in this paper.  This is to my mind the principal shortcoming of this work; for me this is the make-or-break point as to whether I would fight to have the paper accepted in the conference.  (I would have been far happier if the authors had simply acknowledged that this aspect is wrong and will be worked on in future, with a sketch saying how: perhaps by reference to FrameNet and semantic filler requirements.)                           Independent of the representation, the notation conversion procedure is reasonably clear.  I like the facts that it is rather cleaner and simpler than its predecessor (based on Stanford dependencies), and also that the authors have the courage of submitting non-neural work to the ACL in these days of unbridled and giddy enthusiasm for anything neural. \n",
      "-------------------\n",
      "This paper describes a novel approach for learning multi-sense word representations using reinforcement learning. A CBOW-like architecture is used for sense selection, computing a score for each sense based on the dot product between the sum of word embeddings in the current context and the corresponding sense vector. A second module based on the skip-gram model is used to train sense representations, given results from the sense selection module. In order to train these two modules, the authors apply Q-Learning, where the Q-value is provided by the CBOW-based sense selection module. The reward is given by the skip-gram negative sampling likelihood. Additionally, the authors propose an approach for determining the number of senses for each word non-parametrically, by creating new senses when the Q-values for existing scores have a score under 0.5.\n",
      "The resulting approach achieves good results under the \"MaxSimC\" metric, and results comparable to previous approaches under \"AvgSimC\". The authors suggest that their approach could be used to improve the performance for downstream tasks by replacing word embeddings with their most probable sense embedding. It would have been nice to see this claim explored, perhaps in a sequential labeling task such as POS-tagging or NER, especially in light of previous work questioning the usefulness of multi-sense representations in downstream tasks. \n",
      "I found it somewhat misleading to suggest that relying on MaxSimC could reduce overhead in a real world application, as the sense disambiguation step (with associated parameters) would still be required, in addition to the sense embeddings. A clustering-based approach using a weighted average of sense representations would have similar overhead. The claims about improving over word2vec using 1/100 of the data are also not particularly surprising on SCWS. \n",
      "These are misleading contributions, as they do not advance/differ much from previous work.\n",
      "The modular quality of their approach results in a flexibility that I think could have been explored further. The sense disambiguation module uses a vector averaging (CBOW) approach. A positive aspect of their model is that they should be able to substitute other context composition approaches (using alternative neural architecture composition techniques) relatively easily.\n",
      "The paper applies an interesting approach to a problem that has been explored now in many ways. The results on standard benchmarks are comparable to previous work, but not particularly surprising/interesting. However, the approach goes beyond a simple extension of the skip-gram model for multi-sense representation learning by providing a modular framework based on reinforcement learning. \n",
      "Ideally, this aspect would be explored further. But overall, the approach itself may be interesting enough on its own to be considered for acceptance, as it could help move research in this area forward.\n",
      "- There are a number of typos that should be addressed (line 190--representations*, 331--selects*, 492--3/4th*).\n",
      "NOTE: Thank you to the authors for their response. \n",
      "-------------------\n",
      "The authors present a new formula for assessing readability of Vietnamese texts. The formula is developed based on a multiple regression analysis with three features. Furthermore, the authors have developed and annotated a new text corpus with three readability classes (easy, middle, hard).\n",
      "Research on languages other than English is interesting and important, especially when it comes to low-resource languages. Therefore, the corpus might be a nice additional resource for research (but it seems that the authors will not publish it - is that right?). However, I don't think the paper is convincing in its current shape or will influence future research. Here are my reasons: - The authors provide no reasons why there is a need for delevoping a new formula for readability assessments, given that there already exist two formulas for Vietnamese with almost the same features. What are the disadvantages of those formulas and why is the new formula presented in this paper better?\n",
      "- In general, the experimental section lacks comparisons with previous work and analysis of results. The authors claim that the accuracy of their formula (81% on their corpus) is \"good and can be applied in practice\". What would be the accuracy of other formulas that already exist and what are the pros and cons of those existing formulas compared to the new one?\n",
      "- As mentioned before, an analysis of results is missing, e.g. which word / sentence lengths / number of difficult words are considered as easy/middle/hard by their model?\n",
      "- A few examples how their formula could be applied in a practical application would be nice as well.\n",
      "- The related work section is rather a \"background\" section since it only presents previously published formulas. What I'm missing is a more general discussion of related work. There are some papers that might be interesting for that, e.g., DuBay 2004: \"The principles of readability\", or Rabin 1988: \"Determining difficulty levels of text written in languages other than English\" - Since Vietnamese is syllable-based and not word-based, I'm wondering how the authors get \"words\" in their study. Do they use a particular approach for merging syllables? And if yes, which approach do they use and what's the accuracy of the approach?\n",
      "- All in all, the content of the paper (experiments, comparisons, analysis, discussion, related work) is not enough for a long paper.\n",
      "Additional remarks: - The language needs improvements - Equations: The usage of parentheses and multiplying operators is inconsistent - Related works section: The usage of capitalized first letters is inconsistent \n",
      "-------------------\n",
      "The paper introduces a general method for improving NLP tasks using embeddings from language models. Context independent word representations have been very useful, and this paper proposes a nice extension by using context-dependent word representations obtained from the hidden states of neural language models. \n",
      "They show significant improvements in tagging and chunking tasks from including embeddings from large language models. There is also interesting analysis which answers several natural questions.\n",
      "Overall this is a very good paper, but I have several suggestions: - Too many experiments are carried out on the test set. Please change Tables 5 and 6 to use development data - It would be really nice to see results on some more tasks - NER tagging and chunking don't have many interesting long range dependencies, and the language model might really help in those cases. I'd love to see results on SRL or CCG supertagging.\n",
      "- The paper claims that using a task specific RNN is necessary because a CRF on top of language model embeddings performs poorly. It wasn't clear to me if they were backpropagating into the language model in this experiment - but if not, it certainly seems like there is potential for that to make a task specific RNN unnecessary. \n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "raw_reviews_path =  '../data/reviewer2/ACL/ACL/ACL_2017/ACL_2017_review'\n",
    "\n",
    "reviews = []\n",
    "valid_cnt = 0\n",
    "total_cnt = 0\n",
    "## Iterate over files in the directory and load them into a pandas dataframe\n",
    "for root, dirs, files in os.walk(raw_reviews_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file_path.endswith('.json'):\n",
    "            json_data = pd.read_json(file_path)\n",
    "            for i,review in json_data.iterrows():\n",
    "                total_cnt += 1\n",
    "                # print(review.keys())\n",
    "                # break\n",
    "                review_text = review['report']['main']\n",
    "                lower_review_text = review_text.lower()\n",
    "                weakness_pos = lower_review_text.find('weakness') or lower_review_text.find('weaknesses')\n",
    "                if weakness_pos != -1:\n",
    "                    main_text = review_text[weakness_pos:]\n",
    "                    reviews.append([file,main_text])\n",
    "                    valid_cnt += 1\n",
    "                else:\n",
    "                    print(review_text)\n",
    "                    print('-------------------')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(reviews, columns=['filename', 'review'])\n",
    "\n",
    "df.to_csv('/home/boda/mbzuai/peerq-generation/data/processed/acl2017_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews:  272\n",
      "Valid reviews:  205\n"
     ]
    }
   ],
   "source": [
    "print('Total reviews: ', total_cnt)\n",
    "print('Valid reviews: ', valid_cnt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

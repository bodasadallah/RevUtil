paped_id,paper_summary,summary_of_strengths,summary_of_weaknesses,"comments,_suggestions_and_typos",score_overall,score_best_paper,score_datasets,score_software,author_identity_guess,confidence,score_replicability,ethical_concerns,focused_review
ARR_2022_20_review,"This paper aims to find the best strategy to select training examples for few-shot transfer. They evaluated 5 data selection methods (data cross-entropy, predictive entropy, gradient embedding, loss embedding) on 3 tasks (POS tagging, NER, NLI) from 20 languages (categorized into 3 groups) . Their final results showed embedding based data selection methods consistently outperforms other strategies. ",Comprehensive empirical studies. Experiments seems to be very solid. Paper is clearly written. ,"Although the experiments are very comprehensive, this paper lacks technical novelty. The optimal data selection strategy also seems to offer limited improvement over random data selection. ","Can you provide more insights about why loss embedding based data selection is better for sequential labeling task while gradient embedding is better for classification task? Why embedding based methods are better than other selection methods?
For method selection, details should be included in the main paper rather than appendix (2.1 and 2.2 all refer to Appendix B). ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"Although the experiments are very comprehensive, this paper lacks technical novelty. The optimal data selection strategy also seems to offer limited improvement over random data selection. 
Can you provide more insights about why loss embedding based data selection is better for sequential labeling task while gradient embedding is better for classification task? Why embedding based methods are better than other selection methods?
For method selection, details should be included in the main paper rather than appendix (2.1 and 2.2 all refer to Appendix B). "
ARR_2022_99_review,"This paper investigates whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. The extensive analysis found that Transformer-based architectures are competitive with the E-Z Reader, but only when computing attention flow scores. ","- The paper is well written and easy to follow.
- The experiment results are solid. The authors conduct extensive experiments with different variants of attention weights, and they compare those results with traditional cognitive models named E-Z Reader. The experiment results are solid enough to show that attention flow scores are competitive with the E-Z Reader.
- As mentioned in this paper, it is the first work that compares fixations between natural reading and task-specific reading on classical NLP tasks including relation extraction and sentiment classification. ","- The paper is not so clear as the introduction part does not show much background about the cognitive models. The authors may need to refine the writing.  - This paper shows lots of analyses about the attention weights of the Transformer models. Although it presents an in-depth analysis, how to utilize these conclusions is still unclear. If the authors can present a more powerful attention mechanism which correlates better with human attention and improves task performance compared with those baselines, I will be more convinced of the conclusions. ","Comments:  1.  If a model correlates better with human attention compared to other models, does it show better task performance?
 2.  Does all the experiments keep the numbers of the parameters of the compared models the same?
Typos: Line 255 / Line 259:  p < .05 -->  p < 0.05 Line 266:  Correlations grouped by sentence length shows stable values around .6  (SST) and .4-.6 (Wikipedia) except for shorter sen-    tences where correlations fluctuate. -- > 0.6  0.4 0.6 ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The paper is not so clear as the introduction part does not show much background about the cognitive models. The authors may need to refine the writing.  - This paper shows lots of analyses about the attention weights of the Transformer models. Although it presents an in-depth analysis, how to utilize these conclusions is still unclear. If the authors can present a more powerful attention mechanism which correlates better with human attention and improves task performance compared with those baselines, I will be more convinced of the conclusions. 
Comments:  1.  If a model correlates better with human attention compared to other models, does it show better task performance?
 2.  Does all the experiments keep the numbers of the parameters of the compared models the same?
Typos: Line 255 / Line 259:  p < .05 -->  p < 0.05 Line 266:  Correlations grouped by sentence length shows stable values around .6  (SST) and .4-.6 (Wikipedia) except for shorter sen-    tences where correlations fluctuate. -- > 0.6  0.4 0.6 "
ARR_2022_241_review,"This work evaluates 4 different ASR models under the single-speaker endangered languages scenario.  - The 4 models are HMM-GMM, Persephone (LSTM), conformer/LF-MMI, and the pretrained wav2vec 2.0 model. The first 3 are trained from scratch and the last one is fine-tuned.
- Each model is evaluated using an open-sourced pipeline authors developed, the evaluation is done extensively across 1 public dataset and 1 private dataset (11 languages in total) - Authors demonstrate that fine-tuning the pretrained wav2vec model gives significantly better performance except for 1 language.
- Authors also investigate the performance of using different sizes of the training sets: they show that dropping training set under 99 mins degrade performance significantly. They conclude larger dataset than 99 mins could yield PER below 10% and be useful for language documentation. ","- The scenario investigated is indeed very useful as many endangered languages corpus is usually a single-speaker corpus. The conclusion that we might want a larger corpus than 99 min to train a model also is very practical.
- The pipeline authors have open-sourced on GitHub seem to be well-documented and easy to reproduce - Authors also consider the training time/hardware constraints for each model ","- the main contribution of fine-tuning a multilingual wav2vec model is not very novel.  - If I understand correctly, only the wav2vec is fine-tuned but the other 3 models are trained from scratch, therefore the comparison across the 4 models might not be very meaningful as it is easy to guess that the fine-tuned one would perform best.
- detailed comparison and analysis across models are not shown (e.g: what errors does each model make? is there any particular error trend made by each model?) ","- One point I would like to read more about is why the srs language fails on the XLSR (wav2vec) model, authors mention this is due to an unusually fast convergence but does not explain why this happens. Is it due to the quality of srs dataset or the XLSR model? Shedding more light on this might give us more hints on when we should use wav2vec and when we should refrain from using it.
- As mentioned in the previous section, it would be interesting if authors can discuss more details of each model (e.g: what error each model make and when should we use each model) ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- the main contribution of fine-tuning a multilingual wav2vec model is not very novel.  - If I understand correctly, only the wav2vec is fine-tuned but the other 3 models are trained from scratch, therefore the comparison across the 4 models might not be very meaningful as it is easy to guess that the fine-tuned one would perform best.
- detailed comparison and analysis across models are not shown (e.g: what errors does each model make? is there any particular error trend made by each model?) 
- One point I would like to read more about is why the srs language fails on the XLSR (wav2vec) model, authors mention this is due to an unusually fast convergence but does not explain why this happens. Is it due to the quality of srs dataset or the XLSR model? Shedding more light on this might give us more hints on when we should use wav2vec and when we should refrain from using it.
- As mentioned in the previous section, it would be interesting if authors can discuss more details of each model (e.g: what error each model make and when should we use each model) "
ARR_2022_241_review,"This paper is about the design of an automatic phoneme transcription system for transcription assistance. 
In particular, it targets endangered languages with only one speaker data, and the goal is to reduce the cost of transcription such languages. 
With all due respect to previous research, the authors note that each of them uses a different system, and that each of them has been tested on a different language. 
The authors also propose a model that is retrained from pre-trained models in multiple languages, and hypothesize that this will be effective for speech recognition with small amounts of data, such as for endangered languages. 
The authors designed a unified experimental setup, the STP test bed, and used it to compare 4 different models under 11 different languages. 
The experiments showed that the system with the multilingual pre-trained model performed better in many languages. 
The authors also estimated that there is a boundary where the recognition rate drops significantly around 90 minutes of training data. ","The strength of this paper is that it uses a unified experimental setup to conduct experiments on endangered language automatic transcriptions, which have traditionally been conducted with different models and different languages. 
The discussion of the experiment, for example, whether it is suitable for fieldwork or not, is given from a humanistic perspective as well, so that it can be considered as a contribution not only to technology but also to the humanities. 
In addition, the authors have published a container for reproducing some of the experiments, which is expected to have a significant impact not only on the paper itself but also on future research in this field. ","The motivation, purpose, and selection of models for the study are appropriate. 
However, I have some concerns about the experiment.
1. Due to the characteristics of endangered language evaluation, I feel that the test data is very limited. 
The authors split the data 9:1 between training and testing, but for example, with 90 minutes of data, there are only 10 minutes of test data. 
Of course I understand that this is unavoidable due to data limitations, but I think some approach or support for this is needed. 
For example, cross-validation can be considered. ( Note that I am not referring to the cross-validation set commonly employed in neural net training.) 
Other possibilities include showing that the distribution of phonemes in the test data is not significantly different from the overall distribution, or calculating perplexity. 
Averaging the languages with the same weights is also anxious in terms of the reliability of the test set mentioned above. 
For experiments with such a small amount of data, I think a confidence interval should be shown.
2. As the authors describe at the end of their discussion, the number of speakers is very small. 
Therefore, I feel that it is not possible to distinguish whether the experimental results are speaker-dependent or language-dependent. 
Of course I understand the difficulty of the experiments with endangered languages, but this is not a reason to relax the experimental conditions for generalization. 
For example, I think the authors could conduct a quasi-limited experiment using a European language for which a large amount of data is available. 
If it is inappropriate in those languages, the authors should explain why.
I am very sympathetic to the philosophy of this paper and understand its importance. 
However, I believe that the experimental setup should be very carefully designed, as this study could be a baseline for future work in this field. ","Even if I take into account the convenience of fieldwork, there seems to be little need to compare training times. 
And if it only takes 24 hours, it seems acceptable. 
If you want to describe the training time, I encourage you to discuss it more. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The motivation, purpose, and selection of models for the study are appropriate. 
However, I have some concerns about the experiment.
1. Due to the characteristics of endangered language evaluation, I feel that the test data is very limited. 
The authors split the data 9:1 between training and testing, but for example, with 90 minutes of data, there are only 10 minutes of test data. 
Of course I understand that this is unavoidable due to data limitations, but I think some approach or support for this is needed. 
For example, cross-validation can be considered. ( Note that I am not referring to the cross-validation set commonly employed in neural net training.) 
Other possibilities include showing that the distribution of phonemes in the test data is not significantly different from the overall distribution, or calculating perplexity. 
Averaging the languages with the same weights is also anxious in terms of the reliability of the test set mentioned above. 
For experiments with such a small amount of data, I think a confidence interval should be shown.
2. As the authors describe at the end of their discussion, the number of speakers is very small. 
Therefore, I feel that it is not possible to distinguish whether the experimental results are speaker-dependent or language-dependent. 
Of course I understand the difficulty of the experiments with endangered languages, but this is not a reason to relax the experimental conditions for generalization. 
For example, I think the authors could conduct a quasi-limited experiment using a European language for which a large amount of data is available. 
If it is inappropriate in those languages, the authors should explain why.
I am very sympathetic to the philosophy of this paper and understand its importance. 
However, I believe that the experimental setup should be very carefully designed, as this study could be a baseline for future work in this field. 
Even if I take into account the convenience of fieldwork, there seems to be little need to compare training times. 
And if it only takes 24 hours, it seems acceptable. 
If you want to describe the training time, I encourage you to discuss it more. "
ARR_2022_241_review,This paper discusses an important task for endangered language documentation -- i.e. phoneme transcription. This typically involves building a phoneme recognition model on a language where the amount of data is in minutes and the number of speakers are often just 1. The authors show that this approach is quite different from the typical ASR task where the models need to be speaker independent. They then compare and contrast the recent ASR architectures towards this scenario and try to understand them in the single speaker scenario. Finally the authors also provide their toolkit for furthering the research towards this important task. ,"The paper addresses a very real and challenging problem of phoneme transcription for endangered language documentation. The challenge is 2 fold in terms of being single speaker and also being limited in terms of amount of training data. This is quite different from the typical ASR task where the models need to be speaker independent. The paper is well written and also contributes their software.  Among the models that the authors choose, there is a variety in their -   1) Architecture - HMM/DNN vs CTC based models vs LF-MMI models    2) Encoders - Transformer, Conformer and LSTM based encoding   3) Training regime - Pretrained vs Training from scratch Which can help answer the importance of each of them towards this task. I have some reservations with the presentation of the results and feel it can be better. I have provided some detailed comments regarding that in the other sections. ","I feel the choice of models used by the authors are different on 1 important aspect. It seems like except XLSR-53 all other models were trained from scratch. In my opinion there are 2 important factors in place - (1) single speaker (2) limited data on an endangered language. This can then lead to a variety of questions -  1) Which models (HMM/DNN , CTC , Enc-Dec) is better? 
2) Using pre-trained or training from scratch which is better? 
3) If using pre-trained is it better to use pre-trained model on many languages or just 1 language but multiple speakers is enough.  The results presented by the authors seem to answer these questions but only to some extent. This limits the contribution of this work. It would be great if they can add an additional ""from scratch"" vs ""from pretrained"" results for each of the model choices. ","1) Could you provide the source of the g2p rules. It's important for readers to replicate your results as the results are provided in PER and PER is also directly dependent on the ground truth phonemes generated by the g2p rule. 
2) Have you considered using a phone based model like Li et. al. 2020 and using that to then convert to phoneme based rules for the endangered language? 
3) XLSR-53 is a pre-trained acoustic model which is trained in a self-supervised manner. However there are many large multilingual or english models available that can be fine-tuned. It would be interesting to see how they work compared to XLSR-53. For example using the KALDI ASPIRE model, or models built in Dalmia et. al 2018, or multilingual models in ESPNET toolkit, or the hidden representations of the Allosaurus model in Li et. al. 2020; as done in -  https://arxiv.org/pdf/2104.01624.pdf. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I feel the choice of models used by the authors are different on 1 important aspect. It seems like except XLSR-53 all other models were trained from scratch. In my opinion there are 2 important factors in place - (1) single speaker (2) limited data on an endangered language. This can then lead to a variety of questions -  1) Which models (HMM/DNN , CTC , Enc-Dec) is better? 
2) Using pre-trained or training from scratch which is better? 
3) If using pre-trained is it better to use pre-trained model on many languages or just 1 language but multiple speakers is enough.  The results presented by the authors seem to answer these questions but only to some extent. This limits the contribution of this work. It would be great if they can add an additional ""from scratch"" vs ""from pretrained"" results for each of the model choices. 
1) Could you provide the source of the g2p rules. It's important for readers to replicate your results as the results are provided in PER and PER is also directly dependent on the ground truth phonemes generated by the g2p rule. 
2) Have you considered using a phone based model like Li et. al. 2020 and using that to then convert to phoneme based rules for the endangered language? 
3) XLSR-53 is a pre-trained acoustic model which is trained in a self-supervised manner. However there are many large multilingual or english models available that can be fine-tuned. It would be interesting to see how they work compared to XLSR-53. For example using the KALDI ASPIRE model, or models built in Dalmia et. al 2018, or multilingual models in ESPNET toolkit, or the hidden representations of the Allosaurus model in Li et. al. 2020; as done in -  https://arxiv.org/pdf/2104.01624.pdf. "
ARR_2022_96_review,"This paper investigates the effectiveness of an early stopping method that is designed to be effective in scenarios where there is a limited amount of labelled data where the prediction accuracy fluctuates highly during training. The early stopping method being presented is based on two considerations: the probabilities of the predicted class label, and the similarity of the output class distribution to the true class distribution.
Experiments are presented involving 5  document classification datasets that show the extent to which the method being proposed is effective. Three of the datasets concern sentiment analysis and the other two involve classification on the basis of topic. ","The scenario that this paper applies to (i.e. a situation where there is a limited quantity of labelled data) is an important one that arises frequently in many practical applications of NLP.
The overall method being proposed is sensible and the method is instantiated in a way that addresses a number of the complexities that arise.
Two types of document classification are considered: classification based on sentiment, and classification based on topic.
The paper is generally well written. ","The major weakness of this paper is the fact that the proposed method does not appear to out-perform pre-existing methods by a particularly significant margin. Indeed, there is no discussion of the statistical significance of the differences that are observed. ","Some of the LaTeX could be improved in the mathematical formulas. In particular, situations where the names of functions involve multiple characters. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The major weakness of this paper is the fact that the proposed method does not appear to out-perform pre-existing methods by a particularly significant margin. Indeed, there is no discussion of the statistical significance of the differences that are observed. 
Some of the LaTeX could be improved in the mathematical formulas. In particular, situations where the names of functions involve multiple characters. "
ARR_2022_320_review,"This work focuses on span-based Nested NER task. For this task, this paper has first identified a few heterogeneous factors that influence the detection of nested named entities. 
The factors are tokens, boundaries, labels and related spans. Then it proposes a span-based method with a novel triaffine attention and scoring mechanism to fuse the identified heterogeneous factors for span representation and classification.
The proposed method is evaluated on four nested NER datasets: ACE2004, ACE2005, GENIA, and KBP2017. Extensive comparative experiments were performed to show that the proposed method outperforms existing methods. Furthermore, ablation analysis was performed to show the importance of all the components. Results on the long and nested entities indicate the importance of use of triaffine transformations compared to biaffine transformation. ","1. A very relevant and challenging task. It has an immense downstream impact in domains like BioNER. 
2. Novel triaffine attention and scoring mechanism exploiting heterogeneous factors. Identification of the heterogeneous factors and utilizing them appropriately. 
3. Well-designed experiments. Comparison with existing methods is quite extensive, i.e., several existing methods were considered for comparison. Ablation analysis and case studies were also performed. 
4. Comprehensive study and well written ","1. A minor weakness of the paper is a convoluted affine mechanism which may not be easy to interpret.
2. Some more clarity is required in the method description. Please see the Comments section. ","1. Line no 84:- ""...may be..."" -> may have been 2. Line no 218:- are the given references for the text encoding the first ones to use such encoding? If not, then please give the appropriate reference. 
3. Line no 203:- Define MLP to avoid any confusion 4. Line no 761:- Rephrase ""We specific seeds"" 5. Another minor issue is it is not clear how the different spans and boundaries are detected ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. A minor weakness of the paper is a convoluted affine mechanism which may not be easy to interpret.
2. Some more clarity is required in the method description. Please see the Comments section. 
1. Line no 84:- ""...may be..."" -> may have been 2. Line no 218:- are the given references for the text encoding the first ones to use such encoding? If not, then please give the appropriate reference. 
3. Line no 203:- Define MLP to avoid any confusion 4. Line no 761:- Rephrase ""We specific seeds"" 5. Another minor issue is it is not clear how the different spans and boundaries are detected "
ARR_2022_320_review,"This work improves the span-based methods of nested NER.  The authors propose to use the triaffine scoring function to fuse heterogeneous factors, i.e. tokens, boundaries, labels, other spans, to obtain span representations and label scores.  Experiments and ablation studies confirm the effectiveness. ","- The paper is well-written and easy to follow.
- Ablation studies and analyses are thorough. ","- The authors claimed SOTA in L114-115, however, as acknowledged by authors in L388-389, the proposed method underperforms several SOTA when both using BERT as the encoder.  Other approaches may obtain higher results when using ALBERT,  so the comparison is not fair and I think the claim is somewhat exaggerated.
-  I do not think Eq. (11) and Eq. (13) are equivalent. Note that in Eq(1-3), every input of the Triaffine function would be fed into an MLP layer firstly, which is NONLINEAR.  Hence the coefficients \beta_{i, j, g, r} cannot be extracted to the front (in Eq. 13).  The authors said that Eq. (11) can be decomposed to Eq. (12-13) in L277-278, which is WRONG, and so do Eq.(23-28) in Appendix B.  As a consequence, some ablation studies may not faithfully reflect the advantage of the Triaffine function as the decomposition does not hold mathematically - Triaffine mechanisms are widely used. Extending them to heterogeneous factors is somewhat trivial.  Hence technical contributions and novelties are limited.  First, I do not think boundary tokens are heterogeneous to inside tokens. Using an attention mechanism to compute the weight of each token within the span and using their weighted sum to represent the span, known as attention-pooling [1], has been explored before. [ 2] systematically compare different span representations in different tasks. They find that attention-pooling span representation performs the best in NER ( better than end-point representation, i.e. the baseline model of this work), but has a lower performance in many other tasks. So the result of this work is not surprising, since the authors also use attention-pooling to tackle (nested) NER, except that they replace the naive attention with the triaffine attention mechanism. After obtaining the span representation, it is often to concatenate the boundary token representations to the span representation and feed them to an MLP classifier for labeling. The authors use another triaffine instead. The really exciting part of this work is to use triaffine to fuse information from other related spans. To overcome the large computational complexity, the authors propose to (1) prune span, which is often used. ( 2) use decomposition, but the derivation is wrong as I discussed before. Finally, as other reviewers mentioned, this paper does not raise novel issues.
[1] What do you learn from context? Probing for sentence structure in contextualized word representations. In ICLR 2019.
[2] A Cross-Task Analysis of Text Span Representations. ","Obtaining powerful span representations is crucial to many span-relation tasks [1, 2]. The authors could evaluate the proposed Triaffine mechanism on these tasks to make it more influential [1] Jiang, et al ""Generalizing Natural Language Analysis through Span-relation Representations"" In ACL2020. 
[2] Toshniwal, et al ""A Cross-Task Analysis of Text Span Representations"" In RepL4NLP 2020 ",2.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",None ,"- The authors claimed SOTA in L114-115, however, as acknowledged by authors in L388-389, the proposed method underperforms several SOTA when both using BERT as the encoder.  Other approaches may obtain higher results when using ALBERT,  so the comparison is not fair and I think the claim is somewhat exaggerated.
-  I do not think Eq. (11) and Eq. (13) are equivalent. Note that in Eq(1-3), every input of the Triaffine function would be fed into an MLP layer firstly, which is NONLINEAR.  Hence the coefficients \beta_{i, j, g, r} cannot be extracted to the front (in Eq. 13).  The authors said that Eq. (11) can be decomposed to Eq. (12-13) in L277-278, which is WRONG, and so do Eq.(23-28) in Appendix B.  As a consequence, some ablation studies may not faithfully reflect the advantage of the Triaffine function as the decomposition does not hold mathematically - Triaffine mechanisms are widely used. Extending them to heterogeneous factors is somewhat trivial.  Hence technical contributions and novelties are limited.  First, I do not think boundary tokens are heterogeneous to inside tokens. Using an attention mechanism to compute the weight of each token within the span and using their weighted sum to represent the span, known as attention-pooling [1], has been explored before. [ 2] systematically compare different span representations in different tasks. They find that attention-pooling span representation performs the best in NER ( better than end-point representation, i.e. the baseline model of this work), but has a lower performance in many other tasks. So the result of this work is not surprising, since the authors also use attention-pooling to tackle (nested) NER, except that they replace the naive attention with the triaffine attention mechanism. After obtaining the span representation, it is often to concatenate the boundary token representations to the span representation and feed them to an MLP classifier for labeling. The authors use another triaffine instead. The really exciting part of this work is to use triaffine to fuse information from other related spans. To overcome the large computational complexity, the authors propose to (1) prune span, which is often used. ( 2) use decomposition, but the derivation is wrong as I discussed before. Finally, as other reviewers mentioned, this paper does not raise novel issues.
[1] What do you learn from context? Probing for sentence structure in contextualized word representations. In ICLR 2019.
[2] A Cross-Task Analysis of Text Span Representations. 
Obtaining powerful span representations is crucial to many span-relation tasks [1, 2]. The authors could evaluate the proposed Triaffine mechanism on these tasks to make it more influential [1] Jiang, et al ""Generalizing Natural Language Analysis through Span-relation Representations"" In ACL2020. 
[2] Toshniwal, et al ""A Cross-Task Analysis of Text Span Representations"" In RepL4NLP 2020 "
ARR_2022_42_review,"The paper introduces GlobalWoZ, a  mutllingual task-oriented dataset built from translating MultiWoZ dataset to 20 other languages. The paper first provides deep analysis on 3 use cases for ToD (E&F, F&E, F&F) aside of English-speaker in English-speaking country. The dataset is built by generating template, translating the dialogue, and template filling with local entities based on a crawled ontology.
The paper shows and extensive experiments on utilizing the datasets by only including single use-case (SUC), Bi-lingual Bi-use-case (BBUC), Multilingual Bi-use-case (MBUC),Multilingual multi-use-cases (MMUC). The experiment shows the effectiveness of using the translated data on all 3 use-cases in both zero-shot and few-shot settings by applying SUC, BBUC, MBUC, and further show generalization on all use-cases using a single MMUC model.  The paper shows the importance of code-switching use-cases (i.e., E&F and F&E) and further provides interesting discussion on overestimation of translate-train baseline which is commonly used in building a practical setting of a multilingual ToD system especially on local entities. Additional experiment on the impact of training on data is also analyzed in the paper. Lastly, the paper shows that machine translated data can be a good proxy for a machine translated data with an additional human curation step, and further provide translated MultiWoZ dataset for 17 other languages. ","• The paper introduces GlobalWoZ, a translated multilingual ToD dataset covering 20 languages with 3 practical use-cases of a multilingual ToD system • The paper shows extensive analysis on all 3 use-cases which provides an empirical proof on the effectiveness of the GlobalWoZ dataset and performing joint training on all the use-cases to achieve better generalization for a multilingual ToD system.
• The paper shows a performance gap in the translate-train model (a common baseline in multilingual ToD) when the data contains local context.
• The paper shows that a machine translated data can be a proxy for a human curated data ","• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor. ","• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor.
• Line 131 “another” -> “other” ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor. 
• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor.
• Line 131 “another” -> “other” "
ARR_2022_42_review,"The paper explores three unexplored usecases on multilingual task-oriented dialogue (ToD) systems where the speaker where a foreign language speaker uses ToD in the foreign-language country (F&F) or an English country (F&E), and an English speaker uses ToD in a foreign-language country (E&F). They introduced a curation method by creating a template, create the ontology, translate using machine translation, and then ask annotators to edit the translations. This method is claimed to be cost-effective. ","- A new multilingual use case for ToD that can be useful for investigate the behavior of speakers living in different locations.  - The curation method is cost-effective. It requires a limited amount of post-editing efforts for a test set.
- Extensive experiments on different scenarios. ","- It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
- The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
- The authors do not explore the performance drop when using local entities in the experiments. ","Comments: 1. Please add more descriptions as the authors undergo the human annotation process. 
2. It would be better to add examples for each setting.
Questions: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage? 
2. Why did the significant performance drop occur when using local entities in the experiments? 
3. Did you manage to measure the quality of the samples you collected using the proposed method? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
- The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
- The authors do not explore the performance drop when using local entities in the experiments. 
Comments: 1. Please add more descriptions as the authors undergo the human annotation process. 
2. It would be better to add examples for each setting.
Questions: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage? 
2. Why did the significant performance drop occur when using local entities in the experiments? 
3. Did you manage to measure the quality of the samples you collected using the proposed method? "
ARR_2022_180_review,"The paper ""Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation"" proposes to transfer the benefits of advances in general domain LMs, such as fewer parameters or more accurate predictions, to domain-specific LMs via knowledge distillation. For this, a general-domain student model is trained to mimic the neuron activation patterns of a calibrated domain-specific teacher model. The authors show that that this domain adapatation scheme transfers the benefits of the general-domain student models: fewer parameters with same performance for ALBERT and improved performance over BERT for RoBERTa. ","Strengths: - The problem of adapting advanced general-domain PLMs to specific domains without further LM-style pretraining is an important topic, because if successful it would allow researchers in specialized domains without access to large GPU clusters to reap the benefits of advances in LM research.
- The proposed solution of using knowledge distillation is sound and the results are promising, as they suggest that advanced PLMs can be domain adapted while retaining their improvements over BERT.
- Code is available ","Weaknesses: I have two major conerns with the work: - The first is whether the proposed method generalizes to other contexts. The experiments are limited to one domain (biomedical), one task (text classification) and a single teacher model (BioBERT) and it is not clear what happens if any of these parameters are changed. It would be especially important to report results on NER datasets as this is a standard benchmark for biomedical PLMs.    - The second concern is whether the reported experiments actually allow to conclude that the proposed method actually works as intended. Couldn't there be other reasons for the reported improvements of the domain-adapted models over the finetuned ones (-ft vs DoKTra in Table 2)? For instance, it would seem important to compare to using a general-domain teacher to make sure the improvements are actually due to transferred domain knowledge and not because of other factors related to distillation. Relatedly, what would happen when using an ALBERT-sized BERT model as student? This would be important to confirm that the improvements are actually because architectural improvements of the student were utilized. Also, it is mentioned that the student model is also finetuned with a calibration regularizer. It would be important to include ALEBRT-ft + calibration regularizer in the ablation study to make sure that the observed improvements do not stem from the calibration regularizer.
Additionally, I have also one minor comment. A comparison to domain-specific PLMs adapted with continued pretraining would be informative, as it would allow researchers to determine how much performance could be gained if they invested the resources to do that. For this, e.g. the RoBERTa model of [1] could be used. More generally, a comparison with state of the art on the used datasets is missing.
[1]: Gururangan, Suchin, et al. ""Don't stop pretraining: adapt language models to domains and tasks."" ",Is final classification performed after linear layer that equalizes dimensions or before? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Weaknesses: I have two major conerns with the work: - The first is whether the proposed method generalizes to other contexts. The experiments are limited to one domain (biomedical), one task (text classification) and a single teacher model (BioBERT) and it is not clear what happens if any of these parameters are changed. It would be especially important to report results on NER datasets as this is a standard benchmark for biomedical PLMs.    - The second concern is whether the reported experiments actually allow to conclude that the proposed method actually works as intended. Couldn't there be other reasons for the reported improvements of the domain-adapted models over the finetuned ones (-ft vs DoKTra in Table 2)? For instance, it would seem important to compare to using a general-domain teacher to make sure the improvements are actually due to transferred domain knowledge and not because of other factors related to distillation. Relatedly, what would happen when using an ALBERT-sized BERT model as student? This would be important to confirm that the improvements are actually because architectural improvements of the student were utilized. Also, it is mentioned that the student model is also finetuned with a calibration regularizer. It would be important to include ALEBRT-ft + calibration regularizer in the ablation study to make sure that the observed improvements do not stem from the calibration regularizer.
Additionally, I have also one minor comment. A comparison to domain-specific PLMs adapted with continued pretraining would be informative, as it would allow researchers to determine how much performance could be gained if they invested the resources to do that. For this, e.g. the RoBERTa model of [1] could be used. More generally, a comparison with state of the art on the used datasets is missing.
[1]: Gururangan, Suchin, et al. ""Don't stop pretraining: adapt language models to domains and tasks."" 
Is final classification performed after linear layer that equalizes dimensions or before? "
ARR_2022_295_review,"This paper provides a large new dataset written in Hanja, an old Korean language not fully understood yet by modern Korean speakers. This dataset comprises three datasets: AJD, DRS, DRRI (DRRI was proposed in this work). The authors investigate the performance of two pretrained language models (PLMs) based on BERT on this data by first continuing the pretraining of the PLMs and later finetuning them to perform a series of supervised tasks. 
The authors considered two such models: mBERT and AnchiBERT (pretrained on ancient Chinese data). As for the tasks, the authors propose extracting labels from the dataset structure and realizing supervised learning for king prediction (era prediction), topic classification, NER, and summary retrieval. 
The paper provides a thoughtful set of experiments evaluating the performance of these models with and without finetuning on Hanja data, including zero-shot experiments on DRRI. Later, the authors also investigate the impact of time and historical events using the best performant model (finetuned AnchiBERT). ","- The paper provides a large new dataset for a low-resource language (Hanja) - The paper does an excellent job at using state-of-the-art approaches for investigating linguistic structures of the datasets, including the impact of name entities and document ages on classification - The presentation of the results is simple and yet comprehensive ","- The paper would be easy to follow with an English-proofreading even though the overall idea is still understandable.
- The new proposed dataset, DRRI, could have been explored more in the paper.
- It is not clear how named entities were extracted from the datasets. ",An English-proofreading would significantly improve the readability of the paper. ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"- The paper would be easy to follow with an English-proofreading even though the overall idea is still understandable.
- The new proposed dataset, DRRI, could have been explored more in the paper.
- It is not clear how named entities were extracted from the datasets. 
An English-proofreading would significantly improve the readability of the paper. "
ARR_2022_295_review,"This paper presents a 'HUE' dataset and pre-trained Hanja language models, which aim to help to analyze the Korean historical documents written in Hanja, an extinct language based on Chinese characters.
As a great amount of the ancient Korean documents were written in Hanja, that is hard to understand now. The author(s) thus provide a  dataset comprising three corpora of records written in Hanja during the Joseon dynasty; four models for four tasks (King prediction, topic classification, named entity recognition, summary retrieval); as well as a language model pretrained on the Hanja documents.
They then conduct experiments on HUE corpora with the LM model, taking BERT as the baseline. 
Overall, models pretrained on time-specific specific corpus data achieve the best performance in these four tasks. By providing additional information as input, the extended experiments show further improvement on KP, TC and NER tasks. Finally, a zero-shot experiment is conducted to demonstrate the effectiveness of the models on information extraction from unseen data. ","The proposed HUE would be the first resource for the processing and understanding of ancient Korean texts in Hanja. In light of combining with advanced language models, this approach has a lot of potential in computational classical languages and scripts. ","I understand that Section 6 tries to getting more factors involved in the interpretation of the modal's capability. However, the target theses seem reasonable (to get a quick yes answer) yet require much more pages. They are two of the big questions in digital humanities, and I don't see any convincing arguments/reviews on the extended analysis that contributes. ","In general, I think this is a great pioneering work and could be fed into a much more interesting resources in the future. 
It would be better to introduce more about the background for readers who are not familiar with (ancient) Korean. For instance,  why is Hanja an extinct 'language' instead of a 'script'? If ancient Korean only wrote Hanja, why do we call it a 'language'? 
In addition, Table 4 would need more explanation which might really help classical humanities studies, e.g., why naive AnchiBERT only outperforms others on NER? etc. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,no ethical concerns in this paper. ,"I understand that Section 6 tries to getting more factors involved in the interpretation of the modal's capability. However, the target theses seem reasonable (to get a quick yes answer) yet require much more pages. They are two of the big questions in digital humanities, and I don't see any convincing arguments/reviews on the extended analysis that contributes. 
In general, I think this is a great pioneering work and could be fed into a much more interesting resources in the future. 
It would be better to introduce more about the background for readers who are not familiar with (ancient) Korean. For instance,  why is Hanja an extinct 'language' instead of a 'script'? If ancient Korean only wrote Hanja, why do we call it a 'language'? 
In addition, Table 4 would need more explanation which might really help classical humanities studies, e.g., why naive AnchiBERT only outperforms others on NER? etc. "
ARR_2022_342_review,"The paper focuses on exploring the role of context in determining the humans’ (annotators) perception of hate or counter-hate in social media comments. Further, the authors also investigate if the context helps improve the models that detect hate speech or counter hate. The authors aggregate a corpus of 6,846 text pairs containing — (a) the context referred as “parent” and the comment to be classified referred to as “target”, and (b) annotations indicating whether each target is hate/counter-hate/neutral. Experiments demonstrate the benefits of incorporating context and training context-aware neural models towards better detection of hate speech in text. ","The paper is written well. 
The paper introduces an annotated dataset which could be useful for future research. 
The authors perform a detailed analysis of the collected data and the results as well. ","1. The importance of context is well known and well-established in several prior work related to hate speech. While the paper cites works such as Gao and Huang, 2017 and Vidgen, et al., it just mentions that they don’t identify the role of context in annotation or modeling. The former definitely considers its role for modeling and the latter incorporates it in the annotation phase. Though this work performs analysis of corpus and model to study the role of the context, the claim of being the first work to establish the importance of context may be a little stretched. 
2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well? 
3. Though MACE can be used to assess the competent annotators and eliminate redundant annotators, it could be challenging to use when it involves most ambiguous content. 
4.  Some of the analysis and results discussed  (for eg. section 6) might be specific to the tested Roberta model. More experiments using different architectures are needed to determine if the findings and errors that arise are consistent across different models and different settings. ","Claim of being the first one to recognize the importance of context might be too stretched. 
More experiments with multiple runs with different random seeds for the dataset split will help report the mean score and the standard deviation. This will help us understand the sensitivity of the model to the data split or order of training etc. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The importance of context is well known and well-established in several prior work related to hate speech. While the paper cites works such as Gao and Huang, 2017 and Vidgen, et al., it just mentions that they don’t identify the role of context in annotation or modeling. The former definitely considers its role for modeling and the latter incorporates it in the annotation phase. Though this work performs analysis of corpus and model to study the role of the context, the claim of being the first work to establish the importance of context may be a little stretched. 
2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well? 
3. Though MACE can be used to assess the competent annotators and eliminate redundant annotators, it could be challenging to use when it involves most ambiguous content. 
4.  Some of the analysis and results discussed  (for eg. section 6) might be specific to the tested Roberta model. More experiments using different architectures are needed to determine if the findings and errors that arise are consistent across different models and different settings. 
Claim of being the first one to recognize the importance of context might be too stretched. 
More experiments with multiple runs with different random seeds for the dataset split will help report the mean score and the standard deviation. This will help us understand the sensitivity of the model to the data split or order of training etc. "
ARR_2022_342_review,"This paper provides a new dataset of about 6900 English comments (Parent-Target pairs) collected from Reddit annotated according to already existing definitions of hate speech and counter hate. In addition to describing the corpus creation, the authors conduct preliminary experiments which show that taking into consideration the Parent comment improves the model's performance, highlighting the importance of considering the conversational context. ","The paper is reasonably strong. The general framing of the task is conducted well. 
The error analysis is very interesting as it highlights how prevalent each kind of mistake are, which is useful for future research and would allow one to prioritize which kind of misclassification to focus on next. ","Regarding the dataset construction: the approach is generally sound, but the purposive (i.e., keyword-search oriented) method regarding representative keywords potentially provides a heavy bias. Reflection on how to mitigate this bias, or discussion of machine learning/other unsupervised approaches to reasonably expanding this list, would be welcomed. ",Could the authors please comment more on their decision of including only one comment for context? ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Regarding the dataset construction: the approach is generally sound, but the purposive (i.e., keyword-search oriented) method regarding representative keywords potentially provides a heavy bias. Reflection on how to mitigate this bias, or discussion of machine learning/other unsupervised approaches to reasonably expanding this list, would be welcomed. 
Could the authors please comment more on their decision of including only one comment for context? "
ARR_2022_342_review,"This paper presents an English Reddit corpus which consists of Parent/Target comments to explore whether the conversational context is important to identify a message as Hate, Counter, or Neutral considering both human judgments and system predictions. Authors conduct an extensive analysis based on the annotation by humans as wells as the automatic system's decision. Moreover, an error analysis of the best systems is presented. ","S1: The paper is well-written and structured. The authors define clearly the contributions, motivation, and experiments conducted.  S2: Novelty of the work lies in the creation of a Reddit corpus to explore whether the conversational context affects identifying a comment as hate speech, counter, or neutral.  S3: Extensive experiment results are conducted. Both, the main results, as well as ablation results, are strong.
S4: The experiments in the paper confirm the hypothesis of the authors about the utility of using the conversational context to identify hate speech or counter-narratives. The author shows it affects both human judgments and system prediction. ","W1: In the process of collecting the Redding messages, a pair (parent, target) is considered, but a better justification of this decision is needed to explain why authors have not considered more response messages.
W2: In the process of annotation analysis, five annotators label each message. An explanation of this number is necessary.  W3: The analysis of the results is complete but didactic examples while explaining each case would help to interpret the discussion of the authors.  W4: An analysis of the difficulty of the annotators in identifying each class is not presented. Would be important to see in which cases the annotators do not agree and why.
W5: I believe that the annotation guide may be too short for annotators who are not familiar with this type of task. Have examples been provided to the annotators? ","- The word ""context"" in the title of the paper could be more specified. I suggest the authors replace it with ""Conversational context"", otherwise could be confused by the context of the sentence.  - What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task.  - How the authors have resolved the possible doubts of the annotators while the annotating process in MTurk?  - Please, mention the language you focus on to retrieve the Reddit posts.  - Please, move Table 1 to page 2.  - The link of the code is not available but the authors in a footnote affirm ""Code and data available at anonymous_GitHub_link"" - A dot is missing after the ""Label distribution and linguistic insights"" paragraph title.
- In paragraph 481, please show didactic examples where the ""Parent"" helps to identify the class of the ""Target"".  - Please, explain what do you mean by ""Intricate text"" in Table 8. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"W1: In the process of collecting the Redding messages, a pair (parent, target) is considered, but a better justification of this decision is needed to explain why authors have not considered more response messages.
W2: In the process of annotation analysis, five annotators label each message. An explanation of this number is necessary.  W3: The analysis of the results is complete but didactic examples while explaining each case would help to interpret the discussion of the authors.  W4: An analysis of the difficulty of the annotators in identifying each class is not presented. Would be important to see in which cases the annotators do not agree and why.
W5: I believe that the annotation guide may be too short for annotators who are not familiar with this type of task. Have examples been provided to the annotators? 
- The word ""context"" in the title of the paper could be more specified. I suggest the authors replace it with ""Conversational context"", otherwise could be confused by the context of the sentence.  - What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task.  - How the authors have resolved the possible doubts of the annotators while the annotating process in MTurk?  - Please, mention the language you focus on to retrieve the Reddit posts.  - Please, move Table 1 to page 2.  - The link of the code is not available but the authors in a footnote affirm ""Code and data available at anonymous_GitHub_link"" - A dot is missing after the ""Label distribution and linguistic insights"" paragraph title.
- In paragraph 481, please show didactic examples where the ""Parent"" helps to identify the class of the ""Target"".  - Please, explain what do you mean by ""Intricate text"" in Table 8. "
ARR_2022_136_review,"The paper ""CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark"" presents a benchmark of eight diverse tasks to benchmark models for BioNLP in Chinese. 
The authors report results of 11 different PLMs and compare them to human performance. 
The results show that there is still a performance gap between humans and NLP models. ","- The authors present a diverse benchmark for BioNLP in Chinese, comprising eight different data sets.  - The benchmark comes with a rich ecosystem of baselines, a submission system and a leaderboard.
- The authors present an extensive evaluation of different PLMs and give human baseline results. ","- I am not quite clear on which datasets were produced by the authors and which were prior work. I feel this should be described in more detail in the paper.  - What is the source on the amount of Chinese speakers? statista claims it's 16% of the world's population (https://www.statista.com/chart/12868/the-worlds-most-spoken-languages/).
- References to the supplementary material/appendix are missing in the main text and should be inserted in the appropriate locations. This is especially important for the data set details, as these are missing from the main text. ","- L236: Acronym IRB was not introduced - L460: specficanti-toxin -> specific anti-toxin - Alibaba QUAKE is sometimes spelled Alibaba KUAKE, is this on purpose? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",The paper is not very clear on the anonymisation process. I think this should be expanded in greater detail to make sure that deanonymisation is not possible. ,"- I am not quite clear on which datasets were produced by the authors and which were prior work. I feel this should be described in more detail in the paper.  - What is the source on the amount of Chinese speakers? statista claims it's 16% of the world's population (https://www.statista.com/chart/12868/the-worlds-most-spoken-languages/).
- References to the supplementary material/appendix are missing in the main text and should be inserted in the appropriate locations. This is especially important for the data set details, as these are missing from the main text. 
- L236: Acronym IRB was not introduced - L460: specficanti-toxin -> specific anti-toxin - Alibaba QUAKE is sometimes spelled Alibaba KUAKE, is this on purpose? "
ARR_2022_223_review,"The paper investigates the transferability of ""soft prompts"" across tasks and models. It shows that initializing with prompts from different tasks is helpful in terms of both quality and convergence speed. For transferring prompts across models, the results are mainly negative, showing that cross-model transfer is still a hard problem. Finally, the paper shows that similarity of induced neural activation is a better measure of prompt similarity than simpler embedding-based metrics. ","This is a well-structured and useful study of prompt transferability. It adds to recent findings from Vu et al. 2021 that soft prompts can effectively be transferred between related tasks. There are several novel contributions, including (1) that prompt transfer can speed up prompt tuning, and (2) that induced neural activations are a strong measure of prompt similarity. ","The methodology for cross-model transfer (section 5) was less clear, and seemed less well-motivated than the rest of the paper. If I'm understanding correctly, for Distance Minimizing, the projector is trained on a single datapoint (??), which seems unlikely to work well, and the Task Tuning approach doesn't seem preferable to just training a prompt on the target model directly. Regardless, covering more details around training and evaluation would be useful. This could include for training: # of training datapoints, batch size, steps, and for evaluation: specifying whether reported numbers are on dev or test sets.
More direct comparison to Vu et al. 2021 would be useful, as the goals and methods are very similar. One point of comparison could be pointing out that overlap of neuron activation works better than Vu et al.'s embedding-based similarity. ","=== General comments === Is cross-model prompt transfer something that would be useful in practice? If so, it would be helpful to add some motivation for why this is an interesting problem to solve.
Lester et al. 2021 found the original T5 checkpoints didn't work well for prompt tuning, and used LM-adapted checkpoints instead. Is this work using the original checkpoints? If so, it might be interesting to comment on why they might be working well here but not there. A related question: Is the evaluation of T5 for classification tasks doing unconstrained generation, or is it a ""ranking"" approach, just scoring the likelihood of the legal labels?
Vu et al. 2021 is mentioned as ""concurrent"", but was online 3 months before the ARR Jan deadline, so should be considered ""recent"", not concurrent. See the Citation section here: https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation Vu et al. 2021 seems very similar: (1) investigates prompt transfer through initialization, (2) tests different transferability indicators, (3) discusses building a ""prompt warehouse"" for retrieving related tasks and improving prompt tuning. It would be useful to include a bit more description of their approach and the differences/similarities with this work.
=== Specific comments === p.2 -- It's not clear what TPT stands for when the term is introduced.
p.2,3 -- In introducing ""soft prompts"" (p.2) and prompt tuning (section 3.1), the Li and Liang 2021 paper is presented as just tuning input embedding vectors. However Li and Liang's core results are using ""prefix tuning"" which overwrites activations in intermediate layers as well. This should be clarified. Lester et al. 2021 may be a better reference for tuning prompts that only affect the input layer.
p.3 ""In this work, we show that prompt transfer can remedy, improve the effectiveness to some extent with knowledge transfer, and empirically analyze the transferability of prompts across tasks and PLMs."" -- Grammar seems off here.
p.4 I'm confused why Fig 3b laptop => laptop is 99. Shouldn't the diagonal be 100 by definition? If not, it would be useful to explain why not.
p.4 ""Considering RoBERTa can only predict a single token"" -- I'm not understanding why RoBERTa can't be used for extractive QA tasks. Isn't this typically done by adding a QA head on top of BERT type models? Would that not work here? If not, it would be useful to explain why not.
p.5 ""a larger and heterogeneous target PLM"" -- I'm not sure what heterogeneous means here.
p.5 ""In the experiments, we select laptop and MNLI for the projector learning."" -- Could you say why these two tasks were selected?
p.5 The description of Distance Minimizing and Task Tuning could be clearer. From Table 2, it looks like these methods are only training the projector on one task at a time? For Distance Minimizing, does this mean you're training a perceptron on a single datapoint? Does the loss go to 0 in this case? I'm surprised the laptop=>laptop and MNLI=>MNLI transfer scores aren't 100%. Could you say more about how many datapoints you're training on and how many steps of optimization you do?
p.6 For Task Tuning, training the projector directly on the training task examples seems very similar to prompt tuning itself. Is there any added value to training the projector weights as opposed to just optimizing the projector's output (the prompt) directly?   p.6 Are C_concat and C_average the same prompt similarity metrics used in Vu et al. 2021? If so it would be useful to mention this.
p.8 Is there a reason why T5 XL size is left out of Figure 4? ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The methodology for cross-model transfer (section 5) was less clear, and seemed less well-motivated than the rest of the paper. If I'm understanding correctly, for Distance Minimizing, the projector is trained on a single datapoint (??), which seems unlikely to work well, and the Task Tuning approach doesn't seem preferable to just training a prompt on the target model directly. Regardless, covering more details around training and evaluation would be useful. This could include for training: # of training datapoints, batch size, steps, and for evaluation: specifying whether reported numbers are on dev or test sets.
More direct comparison to Vu et al. 2021 would be useful, as the goals and methods are very similar. One point of comparison could be pointing out that overlap of neuron activation works better than Vu et al.'s embedding-based similarity. 
=== General comments === Is cross-model prompt transfer something that would be useful in practice? If so, it would be helpful to add some motivation for why this is an interesting problem to solve.
Lester et al. 2021 found the original T5 checkpoints didn't work well for prompt tuning, and used LM-adapted checkpoints instead. Is this work using the original checkpoints? If so, it might be interesting to comment on why they might be working well here but not there. A related question: Is the evaluation of T5 for classification tasks doing unconstrained generation, or is it a ""ranking"" approach, just scoring the likelihood of the legal labels?
Vu et al. 2021 is mentioned as ""concurrent"", but was online 3 months before the ARR Jan deadline, so should be considered ""recent"", not concurrent. See the Citation section here: https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation Vu et al. 2021 seems very similar: (1) investigates prompt transfer through initialization, (2) tests different transferability indicators, (3) discusses building a ""prompt warehouse"" for retrieving related tasks and improving prompt tuning. It would be useful to include a bit more description of their approach and the differences/similarities with this work.
=== Specific comments === p.2 -- It's not clear what TPT stands for when the term is introduced.
p.2,3 -- In introducing ""soft prompts"" (p.2) and prompt tuning (section 3.1), the Li and Liang 2021 paper is presented as just tuning input embedding vectors. However Li and Liang's core results are using ""prefix tuning"" which overwrites activations in intermediate layers as well. This should be clarified. Lester et al. 2021 may be a better reference for tuning prompts that only affect the input layer.
p.3 ""In this work, we show that prompt transfer can remedy, improve the effectiveness to some extent with knowledge transfer, and empirically analyze the transferability of prompts across tasks and PLMs."" -- Grammar seems off here.
p.4 I'm confused why Fig 3b laptop => laptop is 99. Shouldn't the diagonal be 100 by definition? If not, it would be useful to explain why not.
p.4 ""Considering RoBERTa can only predict a single token"" -- I'm not understanding why RoBERTa can't be used for extractive QA tasks. Isn't this typically done by adding a QA head on top of BERT type models? Would that not work here? If not, it would be useful to explain why not.
p.5 ""a larger and heterogeneous target PLM"" -- I'm not sure what heterogeneous means here.
p.5 ""In the experiments, we select laptop and MNLI for the projector learning."" -- Could you say why these two tasks were selected?
p.5 The description of Distance Minimizing and Task Tuning could be clearer. From Table 2, it looks like these methods are only training the projector on one task at a time? For Distance Minimizing, does this mean you're training a perceptron on a single datapoint? Does the loss go to 0 in this case? I'm surprised the laptop=>laptop and MNLI=>MNLI transfer scores aren't 100%. Could you say more about how many datapoints you're training on and how many steps of optimization you do?
p.6 For Task Tuning, training the projector directly on the training task examples seems very similar to prompt tuning itself. Is there any added value to training the projector weights as opposed to just optimizing the projector's output (the prompt) directly?   p.6 Are C_concat and C_average the same prompt similarity metrics used in Vu et al. 2021? If so it would be useful to mention this.
p.8 Is there a reason why T5 XL size is left out of Figure 4? "
ARR_2022_223_review,"Motivated by empirical findings that training models with Prompt Tuning can achieve the same performance as fully fine-tuning a model, but the training takes much longer to reach the same performance, they explore ways to exploit knowledge from already trained prompts.
They explore using already trained prompts to transfer knowledge between tasks (using the same frozen model) and also the transfer of prompt between _different_ frozen models. For between task transfer, they either directly re-use a prompt from a source task on the target task or they use the prompt learned from the source task as the initialization point for the target task.
For between model transfer, they uses these same methods but include a learnable `projector` (a small, 2 layer neural-network) that maps the prompts from one frozen model to another be using the projected prompt in one of the methods mentioned above. They have two methods for learning this `projector`. In the first method, which they call _Distance Minimization_, they minimize the $L_2$ distance between a projected source prompt (trained on the source frozen model) and a target prompt (a prompt trained on the same task using the target model). In the second method (_Task Tuning_) they learn the `projector` via backpropagation. In this case they take a prompt trained on a source task $P_s$, project it ($Proj(P_s)$) and then use that when prompt tuning the target model. Gradient updates are only applied to the projector.
They also look at several methods of prompt similarity and use them to predict prompt transferability. They main methods are Cosine and Euclidean distances between prompt tokens and their novel model activation similarity where prompts are fed into frozen models and the activations of the feed-forward layers are recorded. The call this method _ON_. ### Results Their first results look at the performance of directly re-using a prompt trained on a source task for a downstream task. They find that this can produce strong performance (measured in relative performance, the direct source to target prompt transfer performance divided by the performance researched from directly training on the target task) within clusters of similar tasks.
Their second results look at the performance of using a prompt learned on a source task to initialize the prompt for a target task and then doing Prompt Tuning. They find that this method can give consistent gains in terms of task performance as well as speed of convergence.
Their third set results examine transfer across models. They find that direct re-use of a prompt projected by the `projector` learned via the _Distance Minimization_ method results in poor performance, especially within the Sentiment tasks. They find that direct reuse of a prompt projected by a `projector` learned with their _Task Tuning_ method does better especially when the tasks are within the same cluster. They also look at how using a _Task Tuning_ prompt to initialize training of a new prompt performs and finds that it can lead to some improvements in task performance and small improvements in convergence speed.
Their final set of results examine use prompt similarity methods to predict prompt transferablity (in the context of direct prompt reuse). They find that all methods are able to distinguish between multiple prompts (created by training with different random seeds) trained for the same task from prompts trained for other tasks. They also find that _ON_ produces a ranking of similar prompts that best correlate with direct reuse performance (using Spearman's rank correlation scores). They also find that the correlation decreases as the size of the frozen model grows. ","The strengths of the paper include:  * Experiments on many different and diverse datasets, 17 with a good mixture of sentiment, NLI, EJ, Paraphrase detection, and Question answers. 
  * Experiments across many model sizes and architectures, including encoder-only models like RoBERTa instead of just the encoder-decoder and decoder-only models we see else where. 
  * The inclusion of small motivating experiments like the convergence speed are a great way to establish the importance of the work and the impact it would have. 
   * The use of the same methods (direct reuse of prompts and using prompts as initialization) in different settings (cross task transfer with the same model and cross model transfer with the same task) and similar results in each demonstrate the robustness of the method. 
   * Not only does their novel prompt similarity method (_ON_ based on model activations when processing the prompt) work great at predicting direct use similarity, it also captures the non-linear way the model interacts with the prompt in a way that simple methods like token similarity can. ","The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.
1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot  smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning. 
2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation. 
3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels? 
4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation? 
5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important  when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance.  The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice. 
6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is. 
7. The majority of Model  transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim. 
8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like ""our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures"" ","1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks. 
2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't. 
3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts. 
4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White. 
5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained. 
6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained. 
7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy. 
8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels. 
9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt. 
10. Line 074: This sentence is confusing. Perhaps something like ""Thus"" over ""Hence only""? 
11. Line 165: Remove ""remedy,"" ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)","4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.
1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot  smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning. 
2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation. 
3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels? 
4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation? 
5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important  when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance.  The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice. 
6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is. 
7. The majority of Model  transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim. 
8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like ""our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures"" 
1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks. 
2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't. 
3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts. 
4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White. 
5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained. 
6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained. 
7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy. 
8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels. 
9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt. 
10. Line 074: This sentence is confusing. Perhaps something like ""Thus"" over ""Hence only""? 
11. Line 165: Remove ""remedy,"" "
ARR_2022_139_review,"This work models the Knowledge Graph Completion task and Question Answering task into a sequence-to-sequence paradigm. The authors propose two easy but effective training and inference methods for the two tasks, respectively.  KGT5 outperforms the current compositional KGE models on the link prediction task. Moreover, the proposed method also achieved outstanding performance on KGQA over incomplete KGs. ","(1) The new training and inference strategies for sequence-to-sequence method are easy but effective. 
(2) The proposed method outperforms the current compositional KGE models on the link prediction task. In addition, it is demonstrated to be complementary to the SOTA KGE methods. 
(3) the proposed method has an outstanding performance on KGQA tasks over incomplete KGs. ","(1) The performances of the KGT5 on KGQA are only evaluated with 50% incomplete KG settings. The effectiveness of the proposed methods is not demonstrated under the complete KG situation, which is necessary for KGQA evaluation. 
(2) The performance of KGT5 on KGQA tasks could take advantage of the n-to-one mapping between an entity/relation and its textual representation. The experiments should be performed in a more fair setting. ",The performance on KGQA with the complete KG setting should be provided. ,3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"(1) The performances of the KGT5 on KGQA are only evaluated with 50% incomplete KG settings. The effectiveness of the proposed methods is not demonstrated under the complete KG situation, which is necessary for KGQA evaluation. 
(2) The performance of KGT5 on KGQA tasks could take advantage of the n-to-one mapping between an entity/relation and its textual representation. The experiments should be performed in a more fair setting. 
The performance on KGQA with the complete KG setting should be provided. "
ARR_2022_139_review,"This paper proposes a simple but powerful approach that uses a single Transformer architecture to tackle KG link prediction and question answering treated as sequence-to-sequence tasks. This approach can reduce the model size up to 90% compared to conventional Knowledge graph embedding (KGE) models, and the performance of this approach is best among small-sized baseline models. ","1. 	This paper uses the Transformer structure for KG link prediction and question answering tasks, and this simple approach seems powerful. 
2. 	This paper conducts a large number of experiments on multiple datasets and analyzes the experimental results. ",Minor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later. ,"Specific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA. 
2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Minor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later. 
Specific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA. 
2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice. "
ARR_2022_285_review,"The present paper investigates the Word-in-Context task in a few-shot setting. Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings. Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks. ","The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting. The paper shows that the methodology is applicable to some other few-shot tasks as well. Claims are substantiated and backed by evidence. ","The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below. 
Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue. ","The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)
- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.  - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?
- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.
- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting ""matched"" for ""not matched"" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below. 
Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue. 
The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)
- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.  - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?
- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.
- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting ""matched"" for ""not matched"" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions. "
ARR_2022_52_review,"This paper argues that existing benchmarks for the text-to-SQL task have failed to capture the out-of-domain problem on column operations, matching domain-specific phrases to composite operation over columns. To tackle this problem, the authors propose a synthetic dataset together with a repartitioned train/test SQUALL dataset. On the methodology, prior domain knowledge is injected via column expansion templates, which requires a small amount of manual effort. For example, a ""time_span"" column can be expanded as ""start_time"" and ""end_time"" columns. Schema pruning is also proposed to drop irrelevant columns. In this way, column operations can be reduced to regular column matching. Empirical results on the two datasets show that the proposed method can significantly outperform two strong baseline parsers, seq2seq of Shi et al. (2020) and SmBop of Rubin and Berant (2021). ","1. This paper reveals a new perspective of out-of-domain generalization on the text-to-SQL parsing, which I feel is a practical problem and valuable to the community. 
2. A synthetic dataset covering three domains (finance, sports, and science) has been newly created. The existing SQUALL has been repartitioned considering column operations such as field accessors or arithmetic operations. Both datasets target the domain generalization of column operation. 
3. The proposed method may seem to be engineering, where a small amount of manual effort is needed to design templates for schema expansion. But it works well on the two proposed datasets. The improvement against the baseline parsers is significant. And it helps the parser to relieve the burden of reasoning and reduces the column operations to a column matching problem. Schema pruning is also proposed to boost performance. 
4. The experimental study, as well as the discussion, is comprehensive. ","1. My biggest concern is whether the templates are domain or dataset-specific. The motivation is to address the domain generalization on column operations, but it is implemented by manually crafted ad-hoc templates that could be tailored for the datasets. I understand this paper is the first step to tackle this problem. The authors also say that improvement does exist and automatic schema expansion is worth future study. 
2. I have not extensively studied the benchmarks in the text-to-SQL task. So I'm not sure whether repartitioning the SQUALL is the best choice or any other resources are also available. ","1. I have a question regarding the schema expansion for the synthetic dataset. According to my understanding,  Table 2 shows the templates used for the SQUALL. What are the templates used for the synthetic dataset? Are they the same as the formulas used in the dataset construction (Table 5)? If so, will this setting make the synthetic dataset easier? The figures in Table 3 demonstrates that the performance on the synthetic dataset is much better than that on the repartitioned SQUALL. 
2. I haven't seen any typos. The paper is clearly written. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. My biggest concern is whether the templates are domain or dataset-specific. The motivation is to address the domain generalization on column operations, but it is implemented by manually crafted ad-hoc templates that could be tailored for the datasets. I understand this paper is the first step to tackle this problem. The authors also say that improvement does exist and automatic schema expansion is worth future study. 
2. I have not extensively studied the benchmarks in the text-to-SQL task. So I'm not sure whether repartitioning the SQUALL is the best choice or any other resources are also available. 
1. I have a question regarding the schema expansion for the synthetic dataset. According to my understanding,  Table 2 shows the templates used for the SQUALL. What are the templates used for the synthetic dataset? Are they the same as the formulas used in the dataset construction (Table 5)? If so, will this setting make the synthetic dataset easier? The figures in Table 3 demonstrates that the performance on the synthetic dataset is much better than that on the repartitioned SQUALL. 
2. I haven't seen any typos. The paper is clearly written. "
ARR_2022_52_review,"Paper deals with a particular problem in designing parser for Text-to-SQL, i.e., mapping phrases in the input question to composite operations over table columns. Paper hypothesize that problem of column operations is challenging for SOTA Text-to-SQL parsers. To verify the hypothesis paper proposes two benchmark datasets: 1) a synthetic dataset, 2)  a different train/test partitioning of the existing SQUALL dataset. The paper shows two SOTA parsers SEQ2SEQ (Shi et al. 2020) and SMBOP (Rubin and Berant2021), struggle on the proposed benchmarks. Paper design two modules: schema expansion and schema pruning which can be added to these SOTA parsers to handle the problem of column operations. ","1. Paper shows the weakness of two SOTA Text-to-SQL parsers in handling column operations by providing two benchmark datasets.
2. Paper design two simple modules schema expansion and schema pruning which utilizes the domain knowledge about different column operations to improve the performance of SOTA Text-to-SQL parsers.
3. Report 5.1% absolute performance improvement on new SQUALL data split using the domain knowledge. ","1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, ""What was <column> in <year>?"".
2. The paper assumes strong domain knowledge about the column types and assumes a domain developer first creates a set of templates based on column types. With the help of these column templates, I think many approaches (parsers) can easily solve the problem. For example, parsers utilizing the SQL grammar to generate the output SQL can use these templates to add new rules that can be used while generating the output. Few such works are 1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. ","1. It will good if the authors can learn the templates for schema expansion from source domain data. 
2. Compare the proposed approach with methods which uses domain knowledge in the form of grammar. Comparing with below methods will show generality of ideas proposed in the paper in a much better way.
1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, ""What was <column> in <year>?"".
2. The paper assumes strong domain knowledge about the column types and assumes a domain developer first creates a set of templates based on column types. With the help of these column templates, I think many approaches (parsers) can easily solve the problem. For example, parsers utilizing the SQL grammar to generate the output SQL can use these templates to add new rules that can be used while generating the output. Few such works are 1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. 
1. It will good if the authors can learn the templates for schema expansion from source domain data. 
2. Compare the proposed approach with methods which uses domain knowledge in the form of grammar. Comparing with below methods will show generality of ideas proposed in the paper in a much better way.
1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. "
ARR_2022_52_review,"This paper identifies an important and challenging problem for out-of-domain generalization of text-to-SQL parsers, which is the alignment between composite column operations and their corresponding domain-specific mentions in natural language. The paper proposes a synthetic dataset to study this, and then a method based on schema expansion and pruning to alleviate the issue.   Though the technical contribution from this work is limited, the identified problem along with the thorough studies using both synthetic and real datasets provide many interesting observations on the out-of-domain generalization of text-to-SQL parsing. I believe this work would be valuable to the community of people working on text-to-SQL parsing. ","This work identifies an under-explored problem that is important for out-of-domain generalization of text-to-SQL parsing. Moreover, it provides a simple yet effective expand-then-prune pipeline to address the problem. ","As mentioned in the paper, the process of expanding columns is quite heuristic, and pruning is arguably tricky to train and tune. ","Maybe the authors can talk a bit about what the findings of this work imply from the perspective of the creator/maintainers of databases. For example, are the template used for schema expansion helpful for creating a ""parsing-friendly"" schema? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"As mentioned in the paper, the process of expanding columns is quite heuristic, and pruning is arguably tricky to train and tune. 
Maybe the authors can talk a bit about what the findings of this work imply from the perspective of the creator/maintainers of databases. For example, are the template used for schema expansion helpful for creating a ""parsing-friendly"" schema? "
ARR_2022_233_review,"The paper proposes a new benchmark for the task of analogical reasoning, framing it as two tasks over a new corpus. The tasks consist of a multiple-choice question answering and the generation of explanations related to the query and each candidate answer. In the experimental part, the benchmark is used to test a set of neural language models, whose performances are not satisfactory and highlight the difficulty of this benchmark.
The paper is well written and easy to understand. The background section provides all the necessary material to understand and contextualize the task. 
The task itself is very interesting, well-motivated, and offers a new difficult challenge to the NLP community. 
The experimental part and the analysis of the result are satisfactory.
The dataset creation process seems to have some weak points but it is still acceptable if some additional details are provided. It is not clear whether the authors plan to release this dataset publicly or whether it will remain private. ",The task is well designed and motivated. It provides a difficult challenge that sets a high bar for current models and that will be very useful to evaluate future NLP-based reasoning systems. ,Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released. ,"1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them.  First of all, will the author release the dataset or will it remain private? 
Are the guidelines used to train the annotators publicly available? 
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines. 
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported. 
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available? 
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released. 
1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them.  First of all, will the author release the dataset or will it remain private? 
Are the guidelines used to train the annotators publicly available? 
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines. 
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported. 
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available? 
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances. "
ARR_2022_129_review,"This work collects a large-scale RC dataset for educational purposes. The dataset is annotated by annotators with knowledge in education and similar areas. The curated questions cover different elements of a story, such as the identification of characters and the causal relationship among events, testing children’s RC skills.  Based on the newly collected dataset, the author proposes a pipeline-based system of question-answer pair generation (QAG) conditioned on given stories. The pipeline contains three components, including a heuristic-based answer generation model, a BART-based question generation model, and a BERT-based QA-pairs ranking model. The answer generation model extracts noun phrases and description of events with an off-the-shelf POS tagging model and a SRL model, respectively, as answer candidates. The QG model takes as input the concatenation of QA-pairs. The ranking model scores a list of QA-pairs.
The author conducts automatic and human evaluations. The proposed QAG system achieves higher ROUGE-L than the adopted baseline systems. Moreover, human evaluators verify that the QA pairs from the proposed system have higher readability and relevance with respect to the stories. ","The collected dataset is a significant contribution for both the QA community and the fields of NLP applications for education. Almost all of the existing work on automatic QA focuses on training models to answer questions. This work investigates how to construct QA tasks to test children’s RC skills. As far as I know, this field is very underexplored though it is vital. ","1. Despite the good motivation, there is a lack of empirical justification for the collected dataset. As mentioned in the paper, there are some underlying principles adopted for data collection. It would be more convincing to present some evidences showing that the new dataset really achieves that goal, comprehensively evaluating children’s RC skills, in comparison to existing datasets (e.g. NarrativeQA).  2. The evaluation of QAG methods is problematic. The author adopted the ROUGE-L metric for evaluation, but there is no justification that this metric is reliable in the addressed task: How likely a QA-pair with high ROUGE score can faithfully evaluate children’s RC skills? Although they conduct human evaluation, the evaluation metrics are not closely related to the educational purposes. 
The author also mentions a user study: “A preliminary user study with 12 pairs of parents and children between the ages of 3-8 suggests that this application powered by our QAG system can successfully maintain engaging conversations with children about the story content. In addition, both parents and children found the system useful, enjoyable, and easy to use”. But no details are included, and hence it is hard to determine whether the user study successfully proves the claims in the paper.  3. There are lots of details of data collection that are missing in the paper. What are the sources of stories? What are the coding templates used? ","“we developed a new RC dataset targeting students from kindergarten to eighth grade” Isn’t this range too broad? Are they expected to process the same or similar levels of RC skills?  You have observed that the questions asked by education experts are open-ended. Why does the designed QAG system extracts answer candidates from stories directly?
Typos: Line 271: “question generation module (QG) module” Line 303: Missing citation of the Spacy library ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Despite the good motivation, there is a lack of empirical justification for the collected dataset. As mentioned in the paper, there are some underlying principles adopted for data collection. It would be more convincing to present some evidences showing that the new dataset really achieves that goal, comprehensively evaluating children’s RC skills, in comparison to existing datasets (e.g. NarrativeQA).  2. The evaluation of QAG methods is problematic. The author adopted the ROUGE-L metric for evaluation, but there is no justification that this metric is reliable in the addressed task: How likely a QA-pair with high ROUGE score can faithfully evaluate children’s RC skills? Although they conduct human evaluation, the evaluation metrics are not closely related to the educational purposes. 
The author also mentions a user study: “A preliminary user study with 12 pairs of parents and children between the ages of 3-8 suggests that this application powered by our QAG system can successfully maintain engaging conversations with children about the story content. In addition, both parents and children found the system useful, enjoyable, and easy to use”. But no details are included, and hence it is hard to determine whether the user study successfully proves the claims in the paper.  3. There are lots of details of data collection that are missing in the paper. What are the sources of stories? What are the coding templates used? 
“we developed a new RC dataset targeting students from kindergarten to eighth grade” Isn’t this range too broad? Are they expected to process the same or similar levels of RC skills?  You have observed that the questions asked by education experts are open-ended. Why does the designed QAG system extracts answer candidates from stories directly?
Typos: Line 271: “question generation module (QG) module” Line 303: Missing citation of the Spacy library "
ARR_2022_129_review,"This paper introduces a new dataset FAIRYTALEQA with text extracted from fairy tales, claimed to be appropriate for K-8th grade, with human annotations of QA pairs. A hybrid approach is proposed for generating QA pairs. Heuristic rules are applied to the design of the model for answer extraction (e.g., extracting named entities and noun chunks as potential answers and semantic roles of verbs as annotated in Propbank). These heuristics help in identifying answers of a specific type appropriate for narratives (identifying characters, setting, plot etc). Then a model is fine tuned on this dataset to generate questions. The model is compared with models fine-tuned with other datasets. The generated QA pairs are ranked via a classification task that decides if a QA pair is more similar to a ground truth pair or not. Automated evaluation metrics include calculating a ROUGE-L score. Human evaluation metrics are reported on three evaluation questions for 70QA pairs. ","Overall this paper is tackling the issue of domain specific QA generation, specifically comprehension of narrative fictional passages for K-8.  This is an important area with a high potential for impact and problem has been adequately been described. Below I list the major strengths of the paper. However, there are sever issues with the paper that need to be addressed.  + Substantial effort for creating new dataset of fairytales annotated with QA pairs. 
+ Interdisciplinary work with education experts which leads to introducing a specific set of comprehension questions appropriate for the fiction narrative genre. 
+ Human annotation and evaluation ","-p. 3 the authors point out that general purpose QA are not appropriate for the education domain and that they solve this problem by developing a new dataset targeting students K-8th grade. This is a big range of grades where fundamental reading comprehension strategies and abilities apply. Reading comprehension questions for Kindergarteners are vastly different from 8th grade middle schoolers. 
-The authors overgeneralize when making claims about the problem they solve in many places. I advise the authors to be precise with their claims.  The current work does not solve the problem of QA in education. It makes a contribution for asking specific type of basic reading comprehension questions for the genre  fiction fairy tale genre. 
-It is not clear if the improved results reported in Table 3 are statistically significant. It would be useful to add a discussion as to whether the reported improvement was within expectation given the effort for annotating a specialized dataset. 
-p.5 It is not clear how the proposed classification task can be extended for ranking QA pairs beyond the specific dataset. The method relies on having available ground truth pairs. ","p.3 Casual Relationship --> Causal Relationship p.7/8 The human evaluation questions that are reported are 3  (readability, question relevancy, answer relevancy) but the intercoder reliability is evaluated on 4 dimensions. Which is the fourth dimension. Also, can you provide more details about how you computed Krippendoff's alpha? The reported agreement for 5 annotators on questions of relevancy is surprisingly high. Maybe it would be helpful to explain why you didn't choose answer accuracy. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,"5 = From a violation of the anonymity-window or other double-blind-submission rules, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,I would like to use this space to provide some further clarification regarding the breaking of anonymity. The paper references a dataset that I was not familiar with: FAIRYTALQA. Searching up this dataset revealed a) that the paper is in arxiv since Sept 2021 https://arxiv.org/abs/2109.03423 b) there is another paper that introduces the dataset that is in openreview and has overlapping experiments: https://openreview.net/forum?id=o-LWIbKagke. ,"-p. 3 the authors point out that general purpose QA are not appropriate for the education domain and that they solve this problem by developing a new dataset targeting students K-8th grade. This is a big range of grades where fundamental reading comprehension strategies and abilities apply. Reading comprehension questions for Kindergarteners are vastly different from 8th grade middle schoolers. 
-The authors overgeneralize when making claims about the problem they solve in many places. I advise the authors to be precise with their claims.  The current work does not solve the problem of QA in education. It makes a contribution for asking specific type of basic reading comprehension questions for the genre  fiction fairy tale genre. 
-It is not clear if the improved results reported in Table 3 are statistically significant. It would be useful to add a discussion as to whether the reported improvement was within expectation given the effort for annotating a specialized dataset. 
-p.5 It is not clear how the proposed classification task can be extended for ranking QA pairs beyond the specific dataset. The method relies on having available ground truth pairs. 
p.3 Casual Relationship --> Causal Relationship p.7/8 The human evaluation questions that are reported are 3  (readability, question relevancy, answer relevancy) but the intercoder reliability is evaluated on 4 dimensions. Which is the fourth dimension. Also, can you provide more details about how you computed Krippendoff's alpha? The reported agreement for 5 annotators on questions of relevancy is surprisingly high. Maybe it would be helpful to explain why you didn't choose answer accuracy. "
ARR_2022_352_review,"The authors compress generalized models using established quantization methods. Finding these fail, they evaluate the failure and to rectify it introduce a novel quantization framework that offers improvements leaps and bounds better than prior methods. Their token-level contrastive distillation approach is elegant, simple, and easy to comprehend. ","The authors introduce a well-researched, effective, and scalable method for quantizing generalized models. They implement competitive baselines,  evaluate the failure cases. Using the discovery that quantization produces homogenous embeddings they create their contrastive token distillation method which is shown to improve the generator model performance heavily. ",The only weakness I see in this paper is a lack of description of the incurred cost for token-level conservative loss. Without being able to see the associated code and run the experiments it is difficult to understand what negative impact using token level contrastive learning has. How much slower does it make the training? How does it affect GPU memory usage? ,It would be beneficial to the community to release the software and associated models along with exporting the models to an engine that can take advantage of lower precision for increased inference speed. ,4.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No Ethical concerns as their approaches focus on making models more efficient. ,"The only weakness I see in this paper is a lack of description of the incurred cost for token-level conservative loss. Without being able to see the associated code and run the experiments it is difficult to understand what negative impact using token level contrastive learning has. How much slower does it make the training? How does it affect GPU memory usage? 
It would be beneficial to the community to release the software and associated models along with exporting the models to an engine that can take advantage of lower precision for increased inference speed. "
ARR_2022_352_review,"Paper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Empirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4× and 13.4× compression rates on GPT-2 and BART, respectively. ","Paper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Lot of earlier work focusses on BERT based model. Thorough analysis and proposal to mitigate gaps for generative models makes compelling case for future research where the problem is harder than the BERT counter-part and opens avenues for generative tasks by quantizing large PLMs.
Empirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4× and 13.4× compression rates on GPT-2 and BART, respectively. Thorough ablation studies for the two proposed directions. ","Lot of interesting ideas presented int he paper. It would be useful to incorporate latency aspects of the proposed methods. Additionally, it seems that GPT is used for utterance and BART for summarization but it will be good to add results for both the models across the tasks to see their impact. ",There is lot of content and tables. Perhaps moving Related work higher might be useful for readers. Additionally adding results for both QuantGPT and QuantBART for both the tasks for completeness. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Yes,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Lot of interesting ideas presented int he paper. It would be useful to incorporate latency aspects of the proposed methods. Additionally, it seems that GPT is used for utterance and BART for summarization but it will be good to add results for both the models across the tasks to see their impact. 
There is lot of content and tables. Perhaps moving Related work higher might be useful for readers. Additionally adding results for both QuantGPT and QuantBART for both the tasks for completeness. "
ARR_2022_126_review,"This paper introduces a dataset intended for structured multi-document summarisation, based on annotated meta-reviews from recent ICLR conferences along with the corresponding reviews and ratings of the papers. The data is thoroughly described and analysed, and a series of experiments are carried out comparing a range of baselines to a system based on Transformers. The paper presents the results from a particular configuration, while the appendix gives more details of the parameter space and other settings that were explored. A human evaluation was also carried out, which indicated that the proposed method produced output that was rated more highly than the baseline systems. The authors intend to release the data and code when the paper is published. ","The main strength of this paper is the quality of the dataset, which is of a good size and appears to have high-quality, reliable annotations. This dataset, together with the code for the baseline and Transformer-based systems, can provide an extremely useful resource for other researchers working in this and similar areas.
The paper itself is well written throughout, and the inclusion of the appendix giving details of the other settings that were explored is a useful extra. ","The ""Related Work"" section is quite brief: it cites a number of papers, but mostly only in a list of references with very little context. It would help to give a more thorough comparison, particularly to the most similar approaches such as the Bhatia et al. (2020) paper which also addressed meta-reviews. ","See above for my main comments.
The font in Table 7 in particular is EXTREMELY small, particularly with the highlighting which makes it even harder to read. Similar issues also apply to the axes of the graph in Figure 5. ",4.5 ,Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,I don't know the policies of openreview.net -- is it acceptable to scrape reviews in this way? ,"The ""Related Work"" section is quite brief: it cites a number of papers, but mostly only in a list of references with very little context. It would help to give a more thorough comparison, particularly to the most similar approaches such as the Bhatia et al. (2020) paper which also addressed meta-reviews. 
See above for my main comments.
The font in Table 7 in particular is EXTREMELY small, particularly with the highlighting which makes it even harder to read. Similar issues also apply to the axes of the graph in Figure 5. "
ARR_2022_126_review,"The authors collect the meta reviews of ICLR and label the sentences with 9 categories of sentence types: abstract, strength, weakness, rating summary, AC disagreement, rebuttal process, suggestion, decision, and misc.. With these sentence labels, the authors introduce a task of controlled generation focused on the macro structure of the review. They show some control methods to begin with this task. ","- The meta review dataset looks very interesting, and many researchers (including me) will be interested in playing with models trained on the dataset. ","- The experiment section is not informative to the readers, because it is stating obvious results such as Transformer models being more “capable of capturing content-specific information”.
- The case study section is also not very informative. The authors evaluate whether the model assigns bigger attention weights to the appropriate control token when generating relevant sentences. However, they only show a single sample as an illustration. Such a single sample illustration is very vulnerable to cherry-picking and biases. Moreover, interpreting bigger attention weights as being more important is somewhat controversial in the field. It would be more helpful to the readers if the authors show diverse quantitative analyses.
- The authors show how different control sequences affect the generated outputs with only 3 examples, which is extremely limited to see whether the model can be controlled. ",- It will be interesting to drop the control sequences from the gold labels (or replace them with negative ones) one after one and see how the automatic metric changes. This will show whether the control tokens really do contribute to the model’s performance. ,"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The experiment section is not informative to the readers, because it is stating obvious results such as Transformer models being more “capable of capturing content-specific information”.
- The case study section is also not very informative. The authors evaluate whether the model assigns bigger attention weights to the appropriate control token when generating relevant sentences. However, they only show a single sample as an illustration. Such a single sample illustration is very vulnerable to cherry-picking and biases. Moreover, interpreting bigger attention weights as being more important is somewhat controversial in the field. It would be more helpful to the readers if the authors show diverse quantitative analyses.
- The authors show how different control sequences affect the generated outputs with only 3 examples, which is extremely limited to see whether the model can be controlled. 
- It will be interesting to drop the control sequences from the gold labels (or replace them with negative ones) one after one and see how the automatic metric changes. This will show whether the control tokens really do contribute to the model’s performance. "
ARR_2022_126_review,"This paper proposes a Meta-Review Dataset of annotated ICLR reviews and a new task of controlled generation with passage macro structures. The annotation is done by attributing each sentence to one of the 9 categories (abstract, strength, weakness, ... ). It also builds some baselines for this task. ",- proposed a task of controlled text generation based on passage macro structure using sentence labels - collected and annotate a meta review dataset - good analysis about the properties of the dataset ,- The type of annotation that labels each sentence into 1 of 9 category is very specific to this task and does not transfer to broader control text generation problem. It has some value as a niche research benchmark but lacks broader impact on research/application ,"Referring BART as Transfermers in table 5 is misleading, could just write BART to indicate it is pretrained. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The type of annotation that labels each sentence into 1 of 9 category is very specific to this task and does not transfer to broader control text generation problem. It has some value as a niche research benchmark but lacks broader impact on research/application 
Referring BART as Transfermers in table 5 is misleading, could just write BART to indicate it is pretrained. "
ARR_2022_30_review,"This paper compares two figures, Firth and Harris, who are often cited as foundational in modern computational linguistics but who are rarely actually read, perhaps even not by the people who cite them. It does a deep dive into their work and takes an opinionated stance that Harris was “introverted”, focused on language as a system in isolation and Firth was extroverted, focusing on language as it exists in the world. ","This is an interesting paper, of a type rarely seen at ACL venues: intellectual history with an opinionated thesis. I genuinely enjoyed reading it and learned from it. I imagine the same would be true for many folks in the ACL community.
There is some real scholarship here, as it investigates the works of these mid-20th c scholars, derives an opinionated synthesis, and applies it to modern NLP.
Given that NLP as a field is extremely forward-looking, often considering something even a year or two old to be ancient history, this is a valuable perspective. ","The paper points out that Firth’s work is somewhat scattered and hard to get a clear grip on. Yet it ends up coming down at times in a way that feels to me a bit too much “Harris bad, Firth good”. The claim is that Firth’s views are well aligned with a strand of thought, currently popular in NLP (and well articulated in Bender & Koller’s position piece and Bisk et al.) that “you can’t learn language on the radio” and that language meaning needs to be in embedded context in a way that is heavily socially mediated. The argument is that, by contrast, Harris misses the boat on this.
I wasn’t quite convinced on this point. It makes for an interesting contrast for the two thinkers, but it also seems to me to be a bit unfair to Harris since it’s hard to counterfactually reason about how Harris would have reacted to the current state of NLP. And I could imagine a variety of other arguments about the relevance of his work in NLP today. Firth’s positions are, according to the paper, admittedly sometimes murky and not always spelled out, which means it is easy to attribute a wider variety of perspectives to him. So I think there should be some caution in that framing.
It also seems possible that a “radically distributional”, like the kind attributed to Harris, could in fact capture a wide range of rich social contexts and individual variation. For instance, GPT-3 which is trained as if it’s trained on a single monolithic dialect, can be a quite effective code-switcher when prompted with different registers.
I’ll mention one other thing, which isn’t really a weakness but is more of a meta-concern: One potential pitfall of submitting and publishing this kind of work in an ACL venue is that the reviewers (like me) and audience are not necessarily going to be experts in this methodology and so care should be taken to make sure it is well reviewed by people who have the relevant expertise. An example of the way in which ACL is not necessarily set up for this kind of work is that I have to select whether the work is reproducible: I picked ""1 = They would not be able to reproduce the results here no matter how hard they tried."" since it's hard to imagine some other set of authors deciding to read Harris and Firth and writing the same paper :-).  But the broader point is that some of this veers methodologically into intellectual history, which I’m certainly not an expert in, and the ACL reviewing process is not necessarily set up to review a paper with this method. That's not a reason not to publish it! In fact, it's all the more reason to give it serious consdieration. But I think there should be some thought given to make sure the work is well evaluated. ",-The paper says that computational linguists routinely cite Harris and Firth. This is true of textbooks and big review papers. But my impression is that many in the ACL community do not engage with them at all. ,"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,none ,"The paper points out that Firth’s work is somewhat scattered and hard to get a clear grip on. Yet it ends up coming down at times in a way that feels to me a bit too much “Harris bad, Firth good”. The claim is that Firth’s views are well aligned with a strand of thought, currently popular in NLP (and well articulated in Bender & Koller’s position piece and Bisk et al.) that “you can’t learn language on the radio” and that language meaning needs to be in embedded context in a way that is heavily socially mediated. The argument is that, by contrast, Harris misses the boat on this.
I wasn’t quite convinced on this point. It makes for an interesting contrast for the two thinkers, but it also seems to me to be a bit unfair to Harris since it’s hard to counterfactually reason about how Harris would have reacted to the current state of NLP. And I could imagine a variety of other arguments about the relevance of his work in NLP today. Firth’s positions are, according to the paper, admittedly sometimes murky and not always spelled out, which means it is easy to attribute a wider variety of perspectives to him. So I think there should be some caution in that framing.
It also seems possible that a “radically distributional”, like the kind attributed to Harris, could in fact capture a wide range of rich social contexts and individual variation. For instance, GPT-3 which is trained as if it’s trained on a single monolithic dialect, can be a quite effective code-switcher when prompted with different registers.
I’ll mention one other thing, which isn’t really a weakness but is more of a meta-concern: One potential pitfall of submitting and publishing this kind of work in an ACL venue is that the reviewers (like me) and audience are not necessarily going to be experts in this methodology and so care should be taken to make sure it is well reviewed by people who have the relevant expertise. An example of the way in which ACL is not necessarily set up for this kind of work is that I have to select whether the work is reproducible: I picked ""1 = They would not be able to reproduce the results here no matter how hard they tried."" since it's hard to imagine some other set of authors deciding to read Harris and Firth and writing the same paper :-).  But the broader point is that some of this veers methodologically into intellectual history, which I’m certainly not an expert in, and the ACL reviewing process is not necessarily set up to review a paper with this method. That's not a reason not to publish it! In fact, it's all the more reason to give it serious consdieration. But I think there should be some thought given to make sure the work is well evaluated. 
-The paper says that computational linguists routinely cite Harris and Firth. This is true of textbooks and big review papers. But my impression is that many in the ACL community do not engage with them at all. "
ARR_2022_330_review,"This paper deals with intriguing and important issues of answer uncertainty and unanswerability in multiple-choice MRC. 
If the evaluation is conducted under a negative marking scheme, it is sensible to choose an answer or abstain from giving an answer if the system is not very confident with any of the answer options. 
Or, if the QA setting includes the none-of-the-above (NOA) option, how to choose this option may pose a problem: the system may rely on the confidence to select one of the non-NOA options, or it could try to detect the unanswerability of the question explicitly. 
To investigate these issues, this paper proposes to use an expected entropy as the measure of predictive uncertainty. 
In addition, the authors devised a dedicated benchmark dataset by modifying the ReClor corpus while removing the correct answer from a subset of possible answers. 
This paper presents empirical results from the extensive experiments using this dataset and concludes that the uncertainty measure could be used to detect questions that the system is not confident about. 
Moreover, the experimental results show that this approach is better than a system explicitly trained with unanswerable examples. ","- This paper investigates the issues of answer uncertainty and unanswerability in multiple-choice MRC, which are intriguing from the perspective of language understanding yet crucial in developing an MRC/QA system that works in more realistic scenarios than experimental settings.
- This paper empirically demonstrated that the expected entropy could be better used to determine answer-or-not-to-answer (in a negative marking scheme) and choose the NOA option (if it is included in the QA setting).
- These main results and other convincing trends are obtained from well-organized experiments that exploit the benchmark dataset devised by altering the ReClor corpus. ","- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results. ","- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results. 
- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained. "
ARR_2022_330_review,"Summary:     This paper aims to explore unanswerability in multiple-choice MRC. Specifically, they aim to use uncertainty measures to identify unanswerable questions based on the probability distribution of the answer choices provided. They use ReColr MRC dataset for their experiments. First, they create a data variation that replaces one of the choices with 'None of the Above' to create a dataset with 25% unanswerable questions. Then, they perform experiments with Electra PLM using expected entropy over the answer choices as the metric that measures uncertainty. First, they show that under a harsh negative-penalty (3:5) for wrong answers, abstaining from answering a certain amount of questions improves the overall performance. They observe that it doesn't really help in case of more common/usual negative penalty schemes such as 3:1. Then, they use the data manipulation to train explicit and implicit models to see if the uncertainty measure would help the implicit model perform reasonably in comparison to the explicit model. They observe that the implicit model shows some meaningful gains over a weaker MAP model.  Contributions: 1) Exploring uncertainty measures in detecting unanswerable questions in MRC 2) Demonstrating the use of explicit entropy uncertainty measure for this use case ",Strengths: 1) Detecting unanswerability in MRC questions is an important research question and using uncertainty measures over output probability distribution is a valid experimental endeavor 2) The insights obtained via the analysis in the paper are interesting ,"Weaknesses: 1) The paper doesn't present any clear hypothesis and experimental set-up to validate their hypothesis. Neither do the authors explore the issue of unanswerability in MRC in exhaustive detail. They fail to account for previous works on this task (https://ojs.aaai.org/index.php/AAAI/article/download/4619/4497) 2) The claims made in the abstract are unsupported in the results. The authors claim that ""it is shown that uncertainty outperforms a system explicitly built with an NOA option"". But, the results of the Implicit system are inferior to that of the Explicit system. The explicit: option A system is not a meaningful system in itself. 
3) The results of the negative penalty scheme show that the proposed uncertainty measure is not that useful in common cases such as 3:1. The authors fail to explore reasons for this, neither do they attempt to experiment with other MRC datasets to see if this is ubiquitous. This makes the exploration incomplete. 
4) The writing of the paper is quite convoluted. The paper is not divided into motivation, hypothesis, proposal, experimental design, results, discussion and related work clearly. Most of the sections are a mix of several of these componenets. This undermines the readability of the paper significantly. ","1) Kindly include the details of the expected entropy calculations in the main paper to make the paper self-contained. 
2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"Weaknesses: 1) The paper doesn't present any clear hypothesis and experimental set-up to validate their hypothesis. Neither do the authors explore the issue of unanswerability in MRC in exhaustive detail. They fail to account for previous works on this task (https://ojs.aaai.org/index.php/AAAI/article/download/4619/4497) 2) The claims made in the abstract are unsupported in the results. The authors claim that ""it is shown that uncertainty outperforms a system explicitly built with an NOA option"". But, the results of the Implicit system are inferior to that of the Explicit system. The explicit: option A system is not a meaningful system in itself. 
3) The results of the negative penalty scheme show that the proposed uncertainty measure is not that useful in common cases such as 3:1. The authors fail to explore reasons for this, neither do they attempt to experiment with other MRC datasets to see if this is ubiquitous. This makes the exploration incomplete. 
4) The writing of the paper is quite convoluted. The paper is not divided into motivation, hypothesis, proposal, experimental design, results, discussion and related work clearly. Most of the sections are a mix of several of these componenets. This undermines the readability of the paper significantly. 
1) Kindly include the details of the expected entropy calculations in the main paper to make the paper self-contained. 
2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore. "
ARR_2022_330_review,"The paper investigates the problem of identifying unanswerable questions in multiple choice MRC. It proposes two ways of tackling this problem: Firstly, by explicitly augmenting training data with unanswerable examples and secondly, by thresholding on (estimated) prediction uncertainty. The paper goes on to show that this can help to identify and abstain from (falsely) predicting hard examples and to identify unanswerable questions on a constructed dataset. ","The stregth of the paper is that it touches upon a topic that appears under-explored in the literature. The paper is written well and the results are presented clearly. Evaluation metrics are well motivated and discussed in detail, which is important as they deviate from the usual F1/Accuracy measures used for evaluating MC-MRC. ","I have identified some weaknesses in the paper:  - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned. 
 - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). ","What follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.
- What is ""Fraction unanswerable"" in Figure 5 and how is it obtained?
- I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I have identified some weaknesses in the paper:  - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned. 
 - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). 
What follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.
- What is ""Fraction unanswerable"" in Figure 5 and how is it obtained?
- I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25. "
ARR_2022_330_review,"This paper describes modifications made to the ReClor data set and ELECTRA, a system built to perform machine reading comprehension (MRC) on that data set, so that both the data and the system handle uncertainty in the selection of multiple-choice answers and abstain from providing an answer when the question posed is unanswerable.  ReClor is augmented to contain four variations of the same content/question/answer-choice triplets but with three versions of the same triplet containing a ""None of the above"" answer choice.  Then ELECTRA is modified to produce confidence scores for its answer selections using one of several metrics, the best-performing of which is expected entropy. ","The authors of this paper are correct in that most multiple-choice data sets like these don't include a ""None of the above"" option, and that most standardized tests, especially those that are adaptive (e.g. GRE) don't penalize test-takers for not answering a given question.  In order to mimic that, data sets need uncertainty built into them, which can be provided using the augmentation strategy here, if needed. ","I think many members of our community will find this work rather niche.  It might be better suited for BEA.
The authors build on top of a system called ELECTRA which is a shared task entry that has not yet been released, or even published.  Therefore, it's difficult to understand the modifications made to ELECTRA described in this paper.
The authors should consider doing evaluations using other data sets besides ReClor to see if their augmentation strategy generalizes well. ","Your abstract is way too long.  You can shorten it starting at the sentence, ""This paper investigates both of these issues by making use of predictive uncertainty"", remove everything until you get to ""It is shown"", then replace ""uncertainty. It is shown"" with ""uncertainty, and shows"".  Also, in the last sentence of your abstract, ""it is shown that"" should be ""we show that"".
On line 050, replace ""consistently observed"" with ""been used to create"".
On lines 066-067, remove "", and of course..."" to the end of the sentence.
On lines 074-076, you can remove everything prior to ""answer uncertainty"" and begin this sentence with ""Answer uncertainty"" for clarity.
On line 192, insert ""give"" between ""and"" and ""no"".
On line 194, ""discourages"" should just be ""discourage"".
On line 234, ""increase demand"" should be ""increased demand"".
On line 273, ""ensembled-based"" should just be ""ensemble-based"".
On lines 338-339, change ""with unanswerable examples too"" to ""that also contain unanswerable examples"".
In Table 2, can you clarify what ""Paper"" and ""Others"" are?  Which papers do these come from?
On line 466, insert ""and"" in front of ""only"".
Can you provide some examples content/question/answer-choice triplets from ReClor?  What is ReClor's domain? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I think many members of our community will find this work rather niche.  It might be better suited for BEA.
The authors build on top of a system called ELECTRA which is a shared task entry that has not yet been released, or even published.  Therefore, it's difficult to understand the modifications made to ELECTRA described in this paper.
The authors should consider doing evaluations using other data sets besides ReClor to see if their augmentation strategy generalizes well. 
Your abstract is way too long.  You can shorten it starting at the sentence, ""This paper investigates both of these issues by making use of predictive uncertainty"", remove everything until you get to ""It is shown"", then replace ""uncertainty. It is shown"" with ""uncertainty, and shows"".  Also, in the last sentence of your abstract, ""it is shown that"" should be ""we show that"".
On line 050, replace ""consistently observed"" with ""been used to create"".
On lines 066-067, remove "", and of course..."" to the end of the sentence.
On lines 074-076, you can remove everything prior to ""answer uncertainty"" and begin this sentence with ""Answer uncertainty"" for clarity.
On line 192, insert ""give"" between ""and"" and ""no"".
On line 194, ""discourages"" should just be ""discourage"".
On line 234, ""increase demand"" should be ""increased demand"".
On line 273, ""ensembled-based"" should just be ""ensemble-based"".
On lines 338-339, change ""with unanswerable examples too"" to ""that also contain unanswerable examples"".
In Table 2, can you clarify what ""Paper"" and ""Others"" are?  Which papers do these come from?
On line 466, insert ""and"" in front of ""only"".
Can you provide some examples content/question/answer-choice triplets from ReClor?  What is ReClor's domain? "
ARR_2022_144_review,"This paper investigates the effects of moral framing on the persuasiveness of arguments relating to controversial topics. The work is comprised of three main parts: development of a technique for mining argumentative statements which correspond to a given moral position from text; integration with IBM Project Debater’s narrative generation API to generate arguments pro/con a given topic which match a given moral position; and, a detailed user-study assessing the impact of morally-framed arguments on audiences of different political ideologies.
For mining of morally framed statements, a BERT classifier based on data collected automatically using distant supervision was used. This approach is clearly explained and produces good results, with the proposed model outperforming both a lexicon-based baseline and existing work for four of the five moral foundations (for 'care' the lexicon-based approach performed better than the other two).
The user study described was carefully planned and executed. Although the results were not 100% conclusive, they suggest a clear impact of moral framing on both liberal and conservative audiences, with both being more affected by moral arguments, and liberals in particular finding arguments with relevant moral framing more effective. ","Overall the paper is well written and clear, providing interesting insights on the effects of moral framing on persuasiveness. The implementation work is well described and highly reproducible. The BERT classifier used for identifying morally framed statements shows good results, which although not perfect, show a solid improvement over existing work.
Although previous work in the field has addressed belief-based claim generation, there has been little to no work previously assessing the persuasiveness of such claims on an audience. As such, the user study included in this paper is extremely welcome and offers good indication that moral framing can impact how arguments are received. ","The only real concern here may be how well other factors are controlled for in the user study. With the method described for extracting morally framed argument components on a given topic, it is possible that other differences may also be present. Some of these may be connected to the moral position -- certain moral positions may bring with them certain phrasing or aspects of the topic that are more appealing to a given audience. And some of these may arise by chance given the relatively small sample size. I do not view this as a major weakness, but it may benefit from some further consideration. ","The assignment of a single moral label to a sentence seems quite definitive. It may be more often the case that a sentence corresponds somewhat to two or more moral foundations, and this might be better modelled as a score for each foundation. Is this something that has been considered?
The sentence starting on line 279 (""Having considerable higher effectiveness on average signals the advantage of using the proposed dataset."") seems incomplete? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,No ethical concerns. The Ethical Statement in Section 8 of the paper covers this well. ,"The only real concern here may be how well other factors are controlled for in the user study. With the method described for extracting morally framed argument components on a given topic, it is possible that other differences may also be present. Some of these may be connected to the moral position -- certain moral positions may bring with them certain phrasing or aspects of the topic that are more appealing to a given audience. And some of these may arise by chance given the relatively small sample size. I do not view this as a major weakness, but it may benefit from some further consideration. 
The assignment of a single moral label to a sentence seems quite definitive. It may be more often the case that a sentence corresponds somewhat to two or more moral foundations, and this might be better modelled as a score for each foundation. Is this something that has been considered?
The sentence starting on line 279 (""Having considerable higher effectiveness on average signals the advantage of using the proposed dataset."") seems incomplete? "
ARR_2022_144_review,"This is a very-well written paper that manipulates (more persuasive) argument generation by filtering moral arguments for the given audience before letting the IBM API generate the final output. Although it heavily relies on IBM API, it poses its own research question (moral arguments for audiences), builds a distantly-supervised classifier for moral arguments, and conducts a nicely designed user study. ","- very clear writing, easy to understand key points - interesting application of distant supervision for detecting moral arguments (better than few-shot fine-tuning) - interesting user study with conservative and liberal audiences - code submitted along with the paper ","- I'm afraid the final number of participants in the user study (3 + 3) makes any drawn conclusions rather unsubstantiated; One would expect some confidence intervals (classical stats approach) or high-density intervals (with Bayesian approach) to see, how much we can trust the study in Table 5 and 6 - Not sure, how much IBM API is stable/reproducible. See a similar argument by Kilgarriff, A. (2006). Googleology is bad science. Computational Linguistics, 33(1):147–151 ","Missing reference to Lukin et al. 2017 (EACL) How exactly do you formulate queries (378)?
Regarding the Ethical statement: Well... this is super weak argumentation: you rely on IBM's black box but at the same time claim only correct solution is to aim for being transparent to the user. So you weren't but still did the study. What should we take from it? 
On the other hand, I think the ethical statement section is mostly non-sense anyway, and I wouldn't know what to write here myself, if I were the author. There is no guarantee that transparency leads to non-abusive technology, to begin with. So I'm just going to ignore this section completely. ",3.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"- I'm afraid the final number of participants in the user study (3 + 3) makes any drawn conclusions rather unsubstantiated; One would expect some confidence intervals (classical stats approach) or high-density intervals (with Bayesian approach) to see, how much we can trust the study in Table 5 and 6 - Not sure, how much IBM API is stable/reproducible. See a similar argument by Kilgarriff, A. (2006). Googleology is bad science. Computational Linguistics, 33(1):147–151 
Missing reference to Lukin et al. 2017 (EACL) How exactly do you formulate queries (378)?
Regarding the Ethical statement: Well... this is super weak argumentation: you rely on IBM's black box but at the same time claim only correct solution is to aim for being transparent to the user. So you weren't but still did the study. What should we take from it? 
On the other hand, I think the ethical statement section is mostly non-sense anyway, and I wouldn't know what to write here myself, if I were the author. There is no guarantee that transparency leads to non-abusive technology, to begin with. So I'm just going to ignore this section completely. "
ARR_2022_86_review,"This paper proposes to use the synonyms of the ICD codes to enrich the code representations. A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding. Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.  The idea of using synonyms of the ICD codes is simple and effective. The paper is well-written and concise. The experimental results show promising improvement over previous work. I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete. Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness. I think the material presented is well-suited for a short paper. ","- The idea of using synonyms of the ICD codes is simple.
- The paper is well-written and concise.
- The experimental results show promising improvement over previous work. ","I have some questions regarding the results, see Questions below ","- The proposed method obtains larger gains on the top-50 setting. My intuition was that the top codes would be benefited less since we have enough training data for those codes. Why do you think this happens?
- For the full setting, AUC and precision improved a lot, but not F1. Do you have any observation or explanation for this? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"I have some questions regarding the results, see Questions below 
- The proposed method obtains larger gains on the top-50 setting. My intuition was that the top codes would be benefited less since we have enough training data for those codes. Why do you think this happens?
- For the full setting, AUC and precision improved a lot, but not F1. Do you have any observation or explanation for this? "
ARR_2022_86_review,"This paper addresses a text classification task: assigning ICD-9-CM codes to a clinical text.  This is a difficult task because only short spans of the input text are relevant to a given target code, hence the use of attention mechanisms in recent work.  In the present work, multiple synonyms are used in the attention mechanism in order to attend to all descriptions of a code.  Furthermore, a biaffine transformation is learnt to help compare synonym representations to the representation of the input text for a given code. 
That combination of using multiple synonyms in the attention mechanism and a biaffine transformation in the architecture is novel to the best of my knowledge.  On the MIMIC-III ICD9 coding dataset (full and top-50 codes), both sub-methods are shown to improve results over using a single term per code or learning independent code representations as in previous work. 
The paper finally explores the space of synonym representations. ","- The introduction of multiple synonyms and of the biaffine transformation in the architecture is motivated.
- Each are shown to bring improvements.
- Improves over previous state of the art, represented by five published systems.
- Short exploration of the resulting synonym representation space. ","- Experiments are performed only once, with no test of multiple random seeds.  Performing experiments N times and reporting mean and standard deviation is good practice.
- Tests should be performed to check whether the observed differences are significant. ","- The basic principle of using multiple synonyms of ICD terms is quite common in earlier work: using the presence of ICD in the UMLS makes it straightforward to obtain more terms for the same ICD code, and terms can then be used to help concept extraction and normalization from text.  In a way, it is therefore surprising that recent work using deep learning methods does not use this simple resource. 
  - Note that in the related task of entity linking, for instance BioSyn (Sung et al., ACL 2020) uses multiple synonyms for each concept.
- How did you sample the synonyms when not all of them are selected: fully randomly? 
  - what if less synonyms are available than the required number $m$?  Does this happen in the performed experiments?
- More synonyms probably increase the memory and time requirements: can you say more about the complexity of the method relative to the number of synonyms?
- ""Deep learning methods usually treat this task as a multi-label classification problem"": this is the case more generally of supervised machine learning methods, and is not specific to deep learning methods.
- ""A d-layer bi-directional LSTM layer with output size h /is followed by/ word embeddings to obtain text hidden representations H."" -> I assume that the LSTM takes word embeddings as input instead?
- ""We notice that the macro F1 has large variance in MIMIC-III full setting"": please explain what you mean by variance here.  This does not seem to be visible in Table 1.
@inproceedings{Sung:ACL2020,     title = ""Biomedical Entity Representations with Synonym Marginalization"",     author = ""Sung, Mujeen  and       Jeon, Hwisang  and       Lee, Jinhyuk  and       Kang, Jaewoo"",     booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",     month = jul,     year = 2020,     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2020.acl-main.335"",     doi = ""10.18653/v1/2020.acl-main.335"",     pages = ""3641--3650"" } ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Experiments are performed only once, with no test of multiple random seeds.  Performing experiments N times and reporting mean and standard deviation is good practice.
- Tests should be performed to check whether the observed differences are significant. 
- The basic principle of using multiple synonyms of ICD terms is quite common in earlier work: using the presence of ICD in the UMLS makes it straightforward to obtain more terms for the same ICD code, and terms can then be used to help concept extraction and normalization from text.  In a way, it is therefore surprising that recent work using deep learning methods does not use this simple resource. 
  - Note that in the related task of entity linking, for instance BioSyn (Sung et al., ACL 2020) uses multiple synonyms for each concept.
- How did you sample the synonyms when not all of them are selected: fully randomly? 
  - what if less synonyms are available than the required number $m$?  Does this happen in the performed experiments?
- More synonyms probably increase the memory and time requirements: can you say more about the complexity of the method relative to the number of synonyms?
- ""Deep learning methods usually treat this task as a multi-label classification problem"": this is the case more generally of supervised machine learning methods, and is not specific to deep learning methods.
- ""A d-layer bi-directional LSTM layer with output size h /is followed by/ word embeddings to obtain text hidden representations H."" -> I assume that the LSTM takes word embeddings as input instead?
- ""We notice that the macro F1 has large variance in MIMIC-III full setting"": please explain what you mean by variance here.  This does not seem to be visible in Table 1.
@inproceedings{Sung:ACL2020,     title = ""Biomedical Entity Representations with Synonym Marginalization"",     author = ""Sung, Mujeen  and       Jeon, Hwisang  and       Lee, Jinhyuk  and       Kang, Jaewoo"",     booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",     month = jul,     year = 2020,     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2020.acl-main.335"",     doi = ""10.18653/v1/2020.acl-main.335"",     pages = ""3641--3650"" } "
ARR_2022_251_review,This paper proposes a trainable subgraph retriever (SR) that acts as a plug-in module to enhance subgraph-oriented knowledge base question answering (KBQA) frameworks. The authors decouple the subgraph retriever from the reasoner based on the probabilistic formalization of KBQA. The subgraph retriever constructs the subgraph by expanding paths from topic entities and merging the trees. The authors explore weak supervision and distant supervision for training the subgraph retriever and also propose an end-to-end training method to jointly learn the retriever and reasoner. Extensive experiments show that the proposed method shows significant improvement over existing subgraph-oriented KBQA models. ,"1. 	The proposed method is very novel. The decoupled retriever and reasoner are substantially different from existing works. Different training strategies and an end-to-end variant are also studied. 
2. 	The proposed method shows significant improvement over previous methods. 
3. 	The experiments are very comprehensive and solid. 
4. 	The paper is well-written and easy to follow. 
5. 	Code is provided and well-organized. ","1. 	While the proposed method is more effective than the previous methods, the efficiency (e.g. training and inference speed) is not compared. ","Question: 1. 	For the end-to-end variant, are the retrieving and the reasoning also intertwined as in previous works? Any intuition on why the proposed method is better?
Typos and Formatting Issues: 1. 	Line 53: the publication year is missing from the citation for WebQSP. 
2. 	The caption of the table should be put on top. 
3. 	Line 435: a space is missing between “statement.” and “SR”. 
4. 	Line 576: proves -> prove. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. 	While the proposed method is more effective than the previous methods, the efficiency (e.g. training and inference speed) is not compared. 
Question: 1. 	For the end-to-end variant, are the retrieving and the reasoning also intertwined as in previous works? Any intuition on why the proposed method is better?
Typos and Formatting Issues: 1. 	Line 53: the publication year is missing from the citation for WebQSP. 
2. 	The caption of the table should be put on top. 
3. 	Line 435: a space is missing between “statement.” and “SR”. 
4. 	Line 576: proves -> prove. "
ARR_2022_251_review,"In this paper, the authors propose a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. The paper also propose a weakly supervised pre-training for SR and a unsupervised pre-training method for the retriever. Extensive experiments are conducted on two KBQA datasets: WebQSP and CWQ. 
The experimental results show the proposed subgraph retriever and training strategy significantly improve retrieval perfomance and achieve the SOTA performance.
The paper is a standard ACL-level paper and I tend to recommend to accept it. ","1. The intuition that decouple subgraph retriever and reasoner seems reasonable. 
2. The paper gives clear problem deﬁnition and formalized discussion, which makes the proposed method more convincing. 
3. Extensive experiments, clear and significant improvements. 
3. Good writting. ","1. The method is too complicated, may be hard to follow. ( but the authors public their code) 2. The  proposed weakly supervised pre-training method and unsupervised pre-training method seem not very related to the main contribution of the paper. ",It's better to also append the results of proposed weakly supervised pre-training method and unsupervised pre-training method  to the main table (Table 2). ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The method is too complicated, may be hard to follow. ( but the authors public their code) 2. The  proposed weakly supervised pre-training method and unsupervised pre-training method seem not very related to the main contribution of the paper. 
It's better to also append the results of proposed weakly supervised pre-training method and unsupervised pre-training method  to the main table (Table 2). "
ARR_2022_251_review,"This paper proposes a trainable subgraph retrieving (SR) approach for KBQA, which can be combined with existing reasoning modules.  The SR approach uses a dual-encoder to expand paths to induce the subgraph and stop the expansion automatically. It can be trained in a weakly supervised way or in an unsupervised way and be fine-tuned in an end-to-end manner. This paper conducts experiments on two benchmark datasets with the proposed SR module.  Experiments show that the SR module leads to smaller subgraphs with higher answer recall. When combined with existing reasoning modules such as GraftNet and NSM, the approach achieves SOTA performance. ","1. The paper points out a promising direction of improving retrieving subgraphs for KBQA, which receives little attention currently.
2. The weakly supervised and unsupervised pre-training strategies can alleviate the problem of scarcity in training data, which is an important issue in KBQA.
3. The proposed trainable SR approach is effective in retrieving subgraphs with higher coverage and smaller sizes. The performance on two KBQA datasets is impressive. ","1. The proposed path expanding method might not involve much novelty. It is very similar to the UHop model in [1], which also adopts an iterative style to extract reasoning path until reaching the END, and updates question representation with historical expanded relations.  2. The trainable SR module is based on RoBERTa, while NSM and GraftNet do not employ large pretrained language models. Since RoBERTa is generally considered more powerful than LSTM and MLP, it is likely that the high performance largely results from RoBERTa rather than the trainable SR strategy itself. Then the comparison between the setting with and without SR may be unfair. Also, it is questionable whether it is worth the much larger computation resources to use a RoBERTa-based SR module instead of the simple and effective PRR.  3. There may be some problems with the mathematical formulas. In equation 6, when the similarity s(q,r) is larger, the probability of r given q should be larger rather than smaller.
[1] UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering. NAACL’19 ","1. I recommend citing the UHop paper, which is closely related to the proposed SR method.
2. In the experiments of this paper, it is assumed that the golden topic entities are given (as indicated in lines 187-188). So it is better that you mark whether the baseline models use golden topic entities or entity linking results in the result table, which is a variable need to be controlled when we compare the performance of KBQA models. For example, BAMNet, GraftNet, and PullNet use entity linking results on WebQSP, according to original paper.
3. The EmbedKGQA reports 66.4 Hits@1 in the original paper while Table 2 shows 46.0 Hits@1, which is much lower. Is there any explanation for this contradiction? ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The proposed path expanding method might not involve much novelty. It is very similar to the UHop model in [1], which also adopts an iterative style to extract reasoning path until reaching the END, and updates question representation with historical expanded relations.  2. The trainable SR module is based on RoBERTa, while NSM and GraftNet do not employ large pretrained language models. Since RoBERTa is generally considered more powerful than LSTM and MLP, it is likely that the high performance largely results from RoBERTa rather than the trainable SR strategy itself. Then the comparison between the setting with and without SR may be unfair. Also, it is questionable whether it is worth the much larger computation resources to use a RoBERTa-based SR module instead of the simple and effective PRR.  3. There may be some problems with the mathematical formulas. In equation 6, when the similarity s(q,r) is larger, the probability of r given q should be larger rather than smaller.
[1] UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering. NAACL’19 
1. I recommend citing the UHop paper, which is closely related to the proposed SR method.
2. In the experiments of this paper, it is assumed that the golden topic entities are given (as indicated in lines 187-188). So it is better that you mark whether the baseline models use golden topic entities or entity linking results in the result table, which is a variable need to be controlled when we compare the performance of KBQA models. For example, BAMNet, GraftNet, and PullNet use entity linking results on WebQSP, according to original paper.
3. The EmbedKGQA reports 66.4 Hits@1 in the original paper while Table 2 shows 46.0 Hits@1, which is much lower. Is there any explanation for this contradiction? "
ARR_2022_89_review,"The paper focuses on the intent understanding of textual TODO notes. The authors propose a weakly supervised approach based on multi-task learning to pre-train better vector representations for the former task. The auxiliary tasks include: (1) autocompletion, MLM objective based on hand-crafted rules, (2) pre-action (what is needed) and goal generation (pseudo intent) and (3) action arguments prediction, using labels from FrameNet. The proposed approach outperforms strong (sentence) embedding baseline models in zero-shot setting. ","1. The paper is clearly written and mostly easy to follow, the proposed approach is well motivated. 
2. The task is interesting, especially from a practical point of view. 
3. The presented results are convincing and I think the evaluated baseline models are good enough. One question left unanswered is -- if the selected similarity functions are the best choice for these models, additional ablation is welcome. 
4. Extensive ablation study of the proposed tasks. ","1. The experiments are held on a private datasets and the exact setup is impossible to reproduce. 
2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well. 
3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE? ","I urge the authors to release at least some part of the dataset to the wider public, or under some  end user-agreement.
Comments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task. 
2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The experiments are held on a private datasets and the exact setup is impossible to reproduce. 
2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well. 
3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE? 
I urge the authors to release at least some part of the dataset to the wider public, or under some  end user-agreement.
Comments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task. 
2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations. "
ARR_2022_109_review,"This paper presents a new point of view that the success of neural text matching models lies in learning the length divergence bias on datasets. To this end, the paper constructs adversarial test sets to verify this conclusion. The results show that the performance of TM models on such test sets is worse. To address it, the paper conducts the same method to construct adversarial training sets and retrain TM models on them. ",Verify that adding adversarial data can improve the robustness of the text matching models. ,"1. This paper seems to reduce the impact of semantic noise on text matching to ""length divergence bias"". From the first example in Table 1, it can be seen that what affects the model's prediction should be that T2 contains more other semantics (noise) such as ""Canadian"", ""University of Waterloo grads Kaheer Suleman and Sam Pasupalak"", which may lead to semantic representation drifting and then affects the TM models. In my opinion, the ""length divergence bias"" should only have a discrepancy in the length of the textual expressions rather than introducing additional semantics. 
2. Lack of details and intuitive examples of the constructed adversarial datasets. ","- Line 136-138: ""... we down-sample each category to align with the average PosRatio of the whole test set"". Could the authors provide more details about the adversarial data construction?
- Could the authors use some specific examples to analyze why the constructed adversarial training sets can improve the robustness of the TM models? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. This paper seems to reduce the impact of semantic noise on text matching to ""length divergence bias"". From the first example in Table 1, it can be seen that what affects the model's prediction should be that T2 contains more other semantics (noise) such as ""Canadian"", ""University of Waterloo grads Kaheer Suleman and Sam Pasupalak"", which may lead to semantic representation drifting and then affects the TM models. In my opinion, the ""length divergence bias"" should only have a discrepancy in the length of the textual expressions rather than introducing additional semantics. 
2. Lack of details and intuitive examples of the constructed adversarial datasets. 
- Line 136-138: ""... we down-sample each category to align with the average PosRatio of the whole test set"". Could the authors provide more details about the adversarial data construction?
- Could the authors use some specific examples to analyze why the constructed adversarial training sets can improve the robustness of the TM models? "
ARR_2022_109_review,"The paper discusses length bias for textual matching models, shows that models are prone to this bias, gives an explanation why, and proposes a solution to correct it. ","- Identification of a new bias - Solution (adversarial training) to address it - Investigation of IR and ""NLP"" model settings ","- This is the (n+1)st paper on discussing biases in models and datasets and it's not clear to me whether this specific bias hasn't been discovered before - not a single 2021 paper cited - models investigated (ESIM, etc.) are a bit old, no novel model is included - the explanation via probing is a bit trivial (this probing has been criticized btw., as unreliable [1]); it's also not clear to me why BERT performs best in the probing, but is least prone to the length diversion bias, questioning the apparent explanation - in l.237, authors write ""except for one combination"", but there are quite a few negative signs in Table 3, indicating that adversarial training is less often helpful - the losses from length bias are often small, e.g., 1 percentage point for BERT [1] https://aclanthology.org/2020.conll-1.8.pdf ","- Textual matching is also relevant for evaluation metrics [2], where similar biases (e.g., lexical overlap) are discovered. It would be interesting to extend this analysis and also consider models from such communities [2] https://aclanthology.org/2021.emnlp-main.701/ ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"- This is the (n+1)st paper on discussing biases in models and datasets and it's not clear to me whether this specific bias hasn't been discovered before - not a single 2021 paper cited - models investigated (ESIM, etc.) are a bit old, no novel model is included - the explanation via probing is a bit trivial (this probing has been criticized btw., as unreliable [1]); it's also not clear to me why BERT performs best in the probing, but is least prone to the length diversion bias, questioning the apparent explanation - in l.237, authors write ""except for one combination"", but there are quite a few negative signs in Table 3, indicating that adversarial training is less often helpful - the losses from length bias are often small, e.g., 1 percentage point for BERT [1] https://aclanthology.org/2020.conll-1.8.pdf 
- Textual matching is also relevant for evaluation metrics [2], where similar biases (e.g., lexical overlap) are discovered. It would be interesting to extend this analysis and also consider models from such communities [2] https://aclanthology.org/2021.emnlp-main.701/ "
ARR_2022_213_review,"This paper proposes an in-depth study of the impact of translationese on machine translation performance from a causal point-of-view, i.e. how the alignment of direction (from originally authored to translated text and vice-versa) of the model and data involved in the translation process influences BLEU scores. This differs from previous work which mostly focused on the alignment direction between the translation model and the test set.
The authors built an annotated corpus based on Europarl to control for variables such as original or translationese, content in terms of topics, and sentence length, and extracted comparable sub-corpora of aligned and unaligned directions to empirically show the following: aligned training and test directions perform best, alignment direction is also relevant when producing synthetic data with supervised training and back-translation, but the impact of data and model alignment varies among language pairs and translations when considering other factors.  The evaluation is conducted on five language pairs in ten translation directions with a test set sampled from the corpus built by the authors, as well as on the newstest2014 from WMT in two language pairs with four translation directions. ","Emerging from the presence of various directions (mixed set) in WMT test sets, recent studies on the impact of translationese on machine translation were mostly limited to the test-to-model alignment impact.  The authors of this paper, however, aim at providing a deeper understanding of previously observed results, due to the fact that the relation between training and test directions alignment as measured by automatic metrics is still unclear. This is especially true with the large amount of synthetic data produced and used in machine translation nowadays.  An additional contribution, backed by further experiments, is the exploration of causal and anticausal learning and their impact on translation performances (based on an automatic metric) when various data directions are used, which appears to be positive or negative depending on the translation direction and language pair.  Finally, the claims made by the authors throughout the paper are supported by a solid set of experiments conducted on various translation directions, the paper is very well written and straightforward to follow. ","Due to the difficulty to build a corpus as employed in this study, the amount of data used to train the machine translation systems is relatively small when compared to publicly available corpora used for the same language pairs by the research community. Thus, the conclusions drawn by the authors might be limited to low or average data settings.
Also, a popular method usually employed to mitigate the alignment mismatch between train and test directions is to add a tag per sample specifying the direction. Commonly used when adding back-translations to the training data, this method could contrast with the current set of experiments and provide an interesting comparison point. The question would be, is the machine translation system able to model the direction and potentially abstract from train-test or data-model alignment mismatch. ","Because speakers at the European Parliament might not be native speakers of the language in which the transcription are written in, wouldn’t the Europarl corpus contain text annotated as originals but from non-native speakers?
The information about the passive voice checker in English is missing.
Dark colors in tables tend to make the numbers difficult to read. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Due to the difficulty to build a corpus as employed in this study, the amount of data used to train the machine translation systems is relatively small when compared to publicly available corpora used for the same language pairs by the research community. Thus, the conclusions drawn by the authors might be limited to low or average data settings.
Also, a popular method usually employed to mitigate the alignment mismatch between train and test directions is to add a tag per sample specifying the direction. Commonly used when adding back-translations to the training data, this method could contrast with the current set of experiments and provide an interesting comparison point. The question would be, is the machine translation system able to model the direction and potentially abstract from train-test or data-model alignment mismatch. 
Because speakers at the European Parliament might not be native speakers of the language in which the transcription are written in, wouldn’t the Europarl corpus contain text annotated as originals but from non-native speakers?
The information about the passive voice checker in English is missing.
Dark colors in tables tend to make the numbers difficult to read. "
ARR_2022_213_review,"This paper studies translationese effects (differences between original and translations in the same language) in machine translation. Since one cannot have parallel corpora with originals in the 2 languages simultaneously, the authors analyse which is the best configuration in data and model alignments for maximising machine translation quality. Recent literature has studied the effect that translationese has in the test sets for the evaluation, but training data and models have been less explored. For this work, the authors first compile a corpus tagged for translationese in five language pairs (causalMT) and then run several experiments on different combinations of the data to see the effect on BLEU scores. ","- The number of experiments is comprehensive - The conclusions might help MT researchers in designing their systems. This is true for language pairs where translationese information is available, but also in general when one uses back-translation or self-training ","- Given the nature of the work, evaluation is important. Authors argue that they cannot perform human evaluation, but at least the combination of other metrics would be relevant. COMET for instance correlates better with human judgements than BLEU [1] and goes further string matching - A couple of relevant references are missing and it would be nice to see them included in the revised version and emphasize your contributions after their works: [2] which create a similar corpus from Europarl with the addition of information about the possibility of pivot translation (which might translationese effects) and [3] which also studied the relevance of the train-test alignment in SMT translation quality (see the references in the Comments section) ","- Section 4, lines 404-411. 
I don't understand this part, I'm not able to reproduce you Corr column in Table 4 from the numbers in Table 2. Table 4's caption helps a bit, but I think the explanation here is not clear enough. Also, the term correlation is confusing (at least to me). I understand your point in the definition but, still, when I read ""correlation"" I expect something between -1 and 1 that shows me how much two variables relate to each other.
- Still in Table 4, the 3r column shouldn't be named ATE? If I understood properly Corr is also Cau-Ant.
- Causal effect results. How does (1) and the last part of (2) match? 
""The data-model alignment is a clear cause for MT performance."" vs ""For other language pairs, the data-model alignment can sometimes have a distinct positive impact and can also sometimes have a negative impact.""
- Related to the previous point: it makes a lot of sense to me that data-model alignment affects MT performance. The fact that it does not always happen couldn't be the effect of more ""others"" in Eq.3 besides length and content?
- I think it would be good to include some of the appendices in the main text. Some parts of the main text are duplicated (Section 6 is already found in pieces in previous sections) on the other hand some relevant information is ignored here. 
For instance, I found Appendix E1 especially interesting as the performance quality of method used to find topic and length equivalents might affect the final quality of the generated corpora.
References [1] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, Arul Menezes. To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. WMT@EMNLP 2021: 478-494 [2] Kwabena Amponsah-Kaakyire, Daria Pylypenko, Cristina España-Bonet and Josef van Genabith. Do not Rely on Relay Translations: Multilingual Parallel Direct Europarl. Proceedings of the Workshop on Modelling Translation: Translatology in the Digital Age (MoTra21), pages 1-7, Iceland (online), May 2021.
[3] Sylwia Ozdowska, and Andy Way. Optimal Bilingual Data for French-English PB-SMT. Annual Conference of the European Association for Machine Translation EAMT (2009). ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Given the nature of the work, evaluation is important. Authors argue that they cannot perform human evaluation, but at least the combination of other metrics would be relevant. COMET for instance correlates better with human judgements than BLEU [1] and goes further string matching - A couple of relevant references are missing and it would be nice to see them included in the revised version and emphasize your contributions after their works: [2] which create a similar corpus from Europarl with the addition of information about the possibility of pivot translation (which might translationese effects) and [3] which also studied the relevance of the train-test alignment in SMT translation quality (see the references in the Comments section) 
- Section 4, lines 404-411. 
I don't understand this part, I'm not able to reproduce you Corr column in Table 4 from the numbers in Table 2. Table 4's caption helps a bit, but I think the explanation here is not clear enough. Also, the term correlation is confusing (at least to me). I understand your point in the definition but, still, when I read ""correlation"" I expect something between -1 and 1 that shows me how much two variables relate to each other.
- Still in Table 4, the 3r column shouldn't be named ATE? If I understood properly Corr is also Cau-Ant.
- Causal effect results. How does (1) and the last part of (2) match? 
""The data-model alignment is a clear cause for MT performance."" vs ""For other language pairs, the data-model alignment can sometimes have a distinct positive impact and can also sometimes have a negative impact.""
- Related to the previous point: it makes a lot of sense to me that data-model alignment affects MT performance. The fact that it does not always happen couldn't be the effect of more ""others"" in Eq.3 besides length and content?
- I think it would be good to include some of the appendices in the main text. Some parts of the main text are duplicated (Section 6 is already found in pieces in previous sections) on the other hand some relevant information is ignored here. 
For instance, I found Appendix E1 especially interesting as the performance quality of method used to find topic and length equivalents might affect the final quality of the generated corpora.
References [1] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, Arul Menezes. To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. WMT@EMNLP 2021: 478-494 [2] Kwabena Amponsah-Kaakyire, Daria Pylypenko, Cristina España-Bonet and Josef van Genabith. Do not Rely on Relay Translations: Multilingual Parallel Direct Europarl. Proceedings of the Workshop on Modelling Translation: Translatology in the Digital Age (MoTra21), pages 1-7, Iceland (online), May 2021.
[3] Sylwia Ozdowska, and Andy Way. Optimal Bilingual Data for French-English PB-SMT. Annual Conference of the European Association for Machine Translation EAMT (2009). "
ARR_2022_72_review,"This paper proposes a new Consistency and Robustness Evaluative Test Suite (CARETS) for the VQA task.  This test suite differs from previous work by balancing question generation to create pairs of instances to test models and focusing on six capabilities, rephrasing, ontological, order, visual obfuscation, attribute antonym, and negation.
The paper conducts many experiments in some state-of-the-art VQA systems in the proposed dataset to reveal existing major issues. ","1. The motivation of this paper is clear and novel, as well as easy to follow.
2. The new dataset can be very useful for the VQA community and greatly promote progress in this field. ","1. This paper mainly compares some experimental results of existing models on proposed new data, I would suggest exploring a potential solution based on the proposed new data.
2. How do the templates generate? ","Please check some typo before publication: Line435-436: ""their weight initailiza-tions"" -> initailizations Line 613: ""their comprehension of different visuo-linguistic"" -> visual-linguistic ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. This paper mainly compares some experimental results of existing models on proposed new data, I would suggest exploring a potential solution based on the proposed new data.
2. How do the templates generate? 
Please check some typo before publication: Line435-436: ""their weight initailiza-tions"" -> initailizations Line 613: ""their comprehension of different visuo-linguistic"" -> visual-linguistic "
ARR_2022_271_review,"In this paper, the authors challenge the basis of the seq2seq-based ABSA framework and propose the simple yet effective Seq2Path framework. By carefully designing the output format and discriminator tokens, the Seq2Path could generate multiple semantic tuples from the input sentences. The training methods and the decoding algorithm are reasonable and sound. Extensive experimental results demonstrate its superiority over the previous baselines. ","- the problem of the previous Seq2Seq model for the ABSA model is reasonable   - experimental results are rich to prove the effectiveness of the proposed Seq2Path framework. The baselines are very detailed. 
  - Seq2Path is simple yet effective for the ABSA tasks. ","It makes me confused to understand the meaning of Figure 2. In Figure2, why do the searched results point to the augmented dataset? ",What is the purpose of your paragraphing in the abstract section? ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"It makes me confused to understand the meaning of Figure 2. In Figure2, why do the searched results point to the augmented dataset? 
What is the purpose of your paragraphing in the abstract section? "
ARR_2022_9_review,"Below is a copy from the previous review: >In this paper the authors proposed a new fairness metric, accumulated prediction sensitivity. The authors formulate the metric and establish its properties in relationship with group fairness and individual fairness. Interestingly, the authors measure the correlation of the proposed metric with a human judgment of fairness. Since the proposed metric requires a choice of the way of computing two hyperparameter vectors, the authors experiment with different choices and show that this choice matters quite a lot. ",Below is a copy from the previous review: > - Important direction of research > - Relatively good correlation with human judgment > - The authors evaluated several choices of the metric > - Intuitive formulation of the metric > - Clearly written and easy to follow paper ,"Below is a copy from the previous review: > - Need of choice of the two hyperparameters vectors w and v > - This choice, as evident from the authors experiments, is very important > - Process of obtaining v can be quite involved > - The proposed metric works only or gradient-based models > - Although the formulation is intuitive, the metric values themselves can be hard to interpret. ",The authors addressed most of the reviewers' comments made for the previous submission. This is an important direction of research and I believe in the current state the paper can be accepted for publication. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Below is a copy from the previous review: > - Need of choice of the two hyperparameters vectors w and v > - This choice, as evident from the authors experiments, is very important > - Process of obtaining v can be quite involved > - The proposed metric works only or gradient-based models > - Although the formulation is intuitive, the metric values themselves can be hard to interpret. 
The authors addressed most of the reviewers' comments made for the previous submission. This is an important direction of research and I believe in the current state the paper can be accepted for publication. "
ARR_2022_164_review,"This paper is about determining the syntactic ability of two Dutch variants of transformer based language model BERT: BERTje and RobBERT. The authors use a Multiple Context Free Grammar (MCFG) formalism to model two patterns of Dutch syntax: control verb nesting and verb raising. These rule-based grammatical models are used to generate a test set which is limited by a bound on recursion depth and populated from a lexicon. For evaluation, each verb occurrence garners a prediction of which referential noun phrase is selected by it, and the resulting accuracy is reported. The authors show results that demonstrate drastically worse performance as recursive depth and number of noun phrases increase, and conclude that the models have not properly learned the underlying syntax of the linguistic phenomena they describe; ie discontinuous constituents/cross-serial dependencies. ","As someone unfamiliar with Dutch and with this area of research, I felt this paper did an excellent job of motivating their reasoning for their research and of describing the ways that Dutch syntax is different from English. Figures and examples were clear and well-done. The article was clearly and concisely written, and appears to be a valuable contribution that adds counter-evidence to claims about how much syntax BERT-based models actually “know”. Authors are careful not to exaggerate the consequences of their findings and make suggestions for how this work could be expanded with other languages or other tasks. ","I was unable to get the provided code to work. I tried both on my Macbook and on a Linux-based computing cluster. To be fair, I did not try for very long (< 15 minutes), and I also did not have access to a GPU so I tried to run it on a CPU. It’s possible that was the problem, but it wasn’t stated that that was a requirement. It seems that if I understood the instructions in the readme properly, there were a few __init__ files missing. However, even after changing those, I ran into a number of other errors. The readme was also a bit sparse, ie “Play around with the results as you see fit”. I commend the authors for including the code and data with the submission, but I would have liked to see a script included already (i.e. not just a snippet in the readme) along with a brief description of any dependencies required beyond the requirements.txt and what one might expect when running the script.
Another weakness I felt, was a lack of description of previous/related work. They mention in the very beginning that ""Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT"", but didn't reference other work to provide similar counter evidence to the consensus they referenced in Rogers et al. (2020). As someone not familiar with this area of research, maybe there is not so much to cite here, but if that is the case, I feel it should be mentioned why there is no related work section. ","Citation should be formatted like so: ""The consensus points to BERT-like models having some capacity for syntactic understanding (Rogers et al., 2020)."" ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I was unable to get the provided code to work. I tried both on my Macbook and on a Linux-based computing cluster. To be fair, I did not try for very long (< 15 minutes), and I also did not have access to a GPU so I tried to run it on a CPU. It’s possible that was the problem, but it wasn’t stated that that was a requirement. It seems that if I understood the instructions in the readme properly, there were a few __init__ files missing. However, even after changing those, I ran into a number of other errors. The readme was also a bit sparse, ie “Play around with the results as you see fit”. I commend the authors for including the code and data with the submission, but I would have liked to see a script included already (i.e. not just a snippet in the readme) along with a brief description of any dependencies required beyond the requirements.txt and what one might expect when running the script.
Another weakness I felt, was a lack of description of previous/related work. They mention in the very beginning that ""Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT"", but didn't reference other work to provide similar counter evidence to the consensus they referenced in Rogers et al. (2020). As someone not familiar with this area of research, maybe there is not so much to cite here, but if that is the case, I feel it should be mentioned why there is no related work section. 
Citation should be formatted like so: ""The consensus points to BERT-like models having some capacity for syntactic understanding (Rogers et al., 2020)."" "
ARR_2022_310_review,The paper presents a text-infilling method for interactive machine translation. The idea enables human interaction in the form of lexical constraints while asking the NMT model to fill the remaining parts of the translation. ,"The paper is well written and easy to follow. 
The experimental work is well defined and allows to correctly test the the presented method in different language pairs and data conditions. 
The method proposed allows to fast and accurately reuse human constraints to boost post-edition performance. ","A human evaluation of the ""gaps"" filled by the model would have been a plus for the paper. ","A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on: - How often invalid constraints are finally hypothesized by the model?
- How grammatical are the ""filled gaps"" proposed by the model given that in principle the model does not modifies human constraints?
- Which is the impact of using different ratios of raw/augmented sentences for training?
Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs?
The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation ""are"" used to guarantee that all ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"A human evaluation of the ""gaps"" filled by the model would have been a plus for the paper. 
A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on: - How often invalid constraints are finally hypothesized by the model?
- How grammatical are the ""filled gaps"" proposed by the model given that in principle the model does not modifies human constraints?
- Which is the impact of using different ratios of raw/augmented sentences for training?
Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs?
The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation ""are"" used to guarantee that all "
ARR_2022_310_review,"This paper proposes BiTIIMT which is the Bilingual Text-infilling (BiTI) task, extending text-infilling from monolingual setting to bilingual setting and aiming to fill in missing segments in a revised translation for a given source sentence. The way to do that is simply to cast the bilingual text-infilling task as a sequence-to-sequence task and then employ the standard NMT model to perform this task. Experiments on WMT14 En-De, WMT14 En-Fr and Zh-En tasks show that the proposed model performs better than LCD in terms of translation quality and efficiency. ",1. Proposes the bilingual text-infilling task and regard it as a seq-seq task; 2. Builds an improved IMT system based on the bilingual text-infilling task. ,1. I didn't see obvious weakness of this paper. The method proposed is simple but effective. ,"1 The paper is well written and easy to follow. 
2 In terms of the automatic evaluation, can authors use more metrics except BLUE to verify the results? 
3 Please do the significance test for all results. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. I didn't see obvious weakness of this paper. The method proposed is simple but effective. 
1 The paper is well written and easy to follow. 
2 In terms of the automatic evaluation, can authors use more metrics except BLUE to verify the results? 
3 Please do the significance test for all results. "
ARR_2022_310_review,"This paper discusses methods to improve MT for Interactive machine translation but not in a standard sense. In IMT, usually MT predicts the next set of tokens given the prefix that has been post-edited by translators. In this paper, the authors tackle another modified use case where translators are first provided a full translation and then they are asked to post-edit portions of the translated text. Usually constrained decoding is used for such a task where model has to regenerate the beam according to the post-edited content.  Overall, I liked reading the paper because everything was stitched and motivated well throughout the paper. It does look like the paper was proof-read multiple times because I did not really find any grammatical errors. Having said that, I do think there's scope of improvement in the paper which I laid down in Weakness section. ",The task of IMT is tough and authors did a good job at providing full context in the problem and how they tackle a modified IMT problem.  Latency is definitely a major component in adoption of this technology. I liked the way this paper outlined the metrics and provided motivation for it.  The approach is quite simple and very effective in terms of quality and latency. ,"Usage of data and experiment design: WMT data contain human generated references, any reason why post-edited content was not used in the paper for experiments? Post-edited content would have been much closer to what your task is and there's plenty of available data out there which is post-edited.   This task of sequence refinement has been studied well in  1. Relation to Neural Automatic Post editing models  2. Sequence Refinement part of Levenshtein Transformer (LevT) by Gu. et. al. 2019 is not discussed when it is almost the same thing although the use case is different (for Automatic Post Editing). 
3. Why was there no discussion on non-autoregressive models? Text infilling approach could indeed solved with a non-autoregressive model like LevT. What are the other methods of text infilling, why aren't those compared with your cross-lingual text-infilling approach?
I am left with some other questions after reading the paper, I am writing them as weakness here but they are essentially points where this paper can improve.  L246: ""Then we combine the sampled data D and the trivial data D as the augmented data to train the model P in our experiments."" what is the ratio of this combination, is it 1:1, if so are you not giving more weight text infilling problem than translation? An ablation study on this combination would have been good.  In the real-world scenario, given that the sample size is low (200), is 65.79 (BiTIIMT) significantly better than 64.05 (LCD)? Same question for edit distance cost? ","L273: ""In other words, we employ the standard beam search algorithm to yield Yb, which leads to a valid Y, without explicitly imposing the constraint during decoding"" Is this always the case in all languages? If not, can you mention the accuracy here? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"Usage of data and experiment design: WMT data contain human generated references, any reason why post-edited content was not used in the paper for experiments? Post-edited content would have been much closer to what your task is and there's plenty of available data out there which is post-edited.   This task of sequence refinement has been studied well in  1. Relation to Neural Automatic Post editing models  2. Sequence Refinement part of Levenshtein Transformer (LevT) by Gu. et. al. 2019 is not discussed when it is almost the same thing although the use case is different (for Automatic Post Editing). 
3. Why was there no discussion on non-autoregressive models? Text infilling approach could indeed solved with a non-autoregressive model like LevT. What are the other methods of text infilling, why aren't those compared with your cross-lingual text-infilling approach?
I am left with some other questions after reading the paper, I am writing them as weakness here but they are essentially points where this paper can improve.  L246: ""Then we combine the sampled data D and the trivial data D as the augmented data to train the model P in our experiments."" what is the ratio of this combination, is it 1:1, if so are you not giving more weight text infilling problem than translation? An ablation study on this combination would have been good.  In the real-world scenario, given that the sample size is low (200), is 65.79 (BiTIIMT) significantly better than 64.05 (LCD)? Same question for edit distance cost? 
L273: ""In other words, we employ the standard beam search algorithm to yield Yb, which leads to a valid Y, without explicitly imposing the constraint during decoding"" Is this always the case in all languages? If not, can you mention the accuracy here? "
ARR_2022_6_review,"This paper gives a comprehensive analysis of the Square One Bias in NLP. Through the statistics of ACL conference papers in recent years, the authors find that the current researchers are encouraged to go beyond the prototype experiment (optimizing accuracy/F1 on an English dataset) in only a single dimension. The problems resulting from the Square One Bias, the recommendations to overcome this, the examples   (one-dimensional research) and counter-examples (multi-dimensional research) are also discussed in the paper. ","1. The paper is well-written and gives a systematic discussion on the Square One Bias in NLP. 
2. I found the examples in the paper to demonstrate the Square One Biases (including architecture biases, annotation biases, selection biases, protocol biases, and organizational biases) pretty precise and helpful. 
3. From the perspective of this paper, one can easily notice unexplored areas of the research manifold, thus I believe this paper is helpful to the community to become more diverse. Especially, this paper would be inspiring for researchers who are interested in non-standard circumstances (e.g. non-English, non-standard architectures, non-accuracy/F1 metrics, etc.). ","Although this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. ","The following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Although this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. 
The following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. "
ARR_2022_10_review,"The paper is essentially an exploration of the design space for seq2seq architectures that produce code from natural language. The paper explores the use of pretrained language models for the encoder as well as the use of an output vocabulary that guarantees grammatically valid code after a mapping is applied, but using Early parser actions as the output. The latter is one of the key contributions of the paper, allowing the comparison of different architectures with direct decoding into code and decoding into grammatical operations.
The paper is valuable as a replication and validation of previous work. It finds that the use of copying directly from the language input is the biggest contributor to performance, with grammatical outputs being another strong factor. ",This paper validates previous work and makes it clear which components are important to pay attention to when building a text to code model. This will be valuable guidance for future research and implementation. The paper has a detailed discussion of the different settings including a qualitative discussion (which unfortunately had to be moved into the appendix). The paper makes very good use of detailed examples to make it clear what is being done. ,"- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.
- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?
- The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details. ","It would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are. 
If you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?
Typos: - Page 1: ""set of value"" -> ""set of values"" ""For instance, Orlanski and Gittens (2021) fine-tunes BART"" -> ""fine-tune"" - Page 2: ""Non determinism"" -> ""Non-Determinism"" ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.
- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?
- The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details. 
It would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are. 
If you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?
Typos: - Page 1: ""set of value"" -> ""set of values"" ""For instance, Orlanski and Gittens (2021) fine-tunes BART"" -> ""fine-tune"" - Page 2: ""Non determinism"" -> ""Non-Determinism"" "
ARR_2022_174_review,"This paper proposes VHED, 1) a dataset comprising human judgments over story pairs from the VIST dataset, and 2) Vrank, a reference-free metric for VIST. To motivate the new metric contribution, this work provides rich insights regarding the evaluation protocols being used in VIST. 
Experiments demonstrate Vrank superior capacity to align with human judgments when compared with existing automatic metrics. ","1. The contributed dataset is valuable for the field and can help researchers working on the task of visual storytelling.
2. The conducted analysis on the annotated story pairs brings relevant insights to this task, such as the fact that in some cases, machine-generated stories are better than references. Such observations not only deliver a strong motivation for the work, but also contribute directly to better understanding the task at its evaluation protocols.
3. The Appendix provides extra experiments that complement the main experimental section.
4. Paper manuscript is well written and structured. ","1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes.  2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide.  In the Appendix, ""Model Design"" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?
3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? ","In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes.  2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide.  In the Appendix, ""Model Design"" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?
3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? 
In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar "
ARR_2022_174_review,"The authors propose a new learned metric for ranking the quality of generated visual stories. Their model/metric, VRANK, is trained directly on pairwise human judgments. Compared to metrics traditionally used for caption generation evaluation like METEOR and ROUGE-L, the authors new metric performs better. They also evaluate on a non-visual corpus, and demonstrate that their model also works to evaluate in that case. ","I liked this work! I suspect that some controversy because the author's metric is i) model-based; and ii) trained directly on human ratings, but the authors make a compelling case that our current metrics simply are not working for this setting. In some sense, the results are not surprising: of course the learned metric does better in the evaluation setting that it was trained in. But, the fact that it also does well in a unimodal setting suggests that the model has not overfit to particular spurious factors in the corpus, which is promising. Overall --- I think the authors make a convincing case for their model-based metric. And, the fine-grained evaluations in table 5 provide some perspective on where the metric could be improved, e.g., accounting more for grammatically and irrelevant nouns. ","Of course, training on human judgments directly is not a panacea. There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become ""stale""? 
That being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.
The author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.
One small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4. My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct). ","Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation. While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of ""staleness""/""neural nets are uninterpretable as metrics"" that could apply in this setting hold less weight. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Of course, training on human judgments directly is not a panacea. There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become ""stale""? 
That being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.
The author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.
One small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4. My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct). 
Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation. While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of ""staleness""/""neural nets are uninterpretable as metrics"" that could apply in this setting hold less weight. "
ARR_2022_300_review,"This paper suggests a solution to tackle the practical challenges of using active learning (AL) for 2 NLP tasks. Specifically, the authors (1) propose to use a distilled version of a PLM during data collection and then leverage a larger model to annotate unlabeled examples for training on the (presumably more complex) successor model.  (2) propose UPS to reduce candidates for labeling. ","1. The paper introduces an important problem: the inefficiency of data acquisition for AL and proposed a useful method. 
2. The author conducts experiments on different tasks/datasets. 
3. The paper is clear and easy to follow. 
4. I appreciate the authors for the efforts of adding extra experiments and writing the response letter. ","1. In the previous rounds, the reviewer qXqD mentioned that there are no more explanations for the claim `PLASM can generalize to models that are not aligned with DistilBERT/BERT'.  The authors added experiments on XLNet, which partially solves this issue. But I expect the authors to give a deeper analysis for it. Also, it is better for authors to include textCNN/LSTM (1 of them is enough) for text classification, which can show whether non-pretrained models can be used under PLASM.
2. The authors seems to leverage some existing techniques for AL methods, thus the technical novelty is not very high. ","- What's the time complexity of Tracin operation?
- It would be even better to evaluate more datasets for text classification (e.g. SST-2 or TREC-6).  - In PLASM, the authors propose to use pseudo labeling to mitigate the ASM problem. Maybe I do not understand correctly, but using pseudo-labeling for AL is not a new concept. The are many works that leverage this technique for AL. Some relevant papers includes: (1) Semi-supervised active learning for sequence labeling; (2) Rethinking deep active learning: Using unlabeled data at model training; (3) ATM:  An Active self-training framework for label-efficient text classification.  The author could probably elaborate more on the details of pseudo-labeling. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. In the previous rounds, the reviewer qXqD mentioned that there are no more explanations for the claim `PLASM can generalize to models that are not aligned with DistilBERT/BERT'.  The authors added experiments on XLNet, which partially solves this issue. But I expect the authors to give a deeper analysis for it. Also, it is better for authors to include textCNN/LSTM (1 of them is enough) for text classification, which can show whether non-pretrained models can be used under PLASM.
2. The authors seems to leverage some existing techniques for AL methods, thus the technical novelty is not very high. 
- What's the time complexity of Tracin operation?
- It would be even better to evaluate more datasets for text classification (e.g. SST-2 or TREC-6).  - In PLASM, the authors propose to use pseudo labeling to mitigate the ASM problem. Maybe I do not understand correctly, but using pseudo-labeling for AL is not a new concept. The are many works that leverage this technique for AL. Some relevant papers includes: (1) Semi-supervised active learning for sequence labeling; (2) Rethinking deep active learning: Using unlabeled data at model training; (3) ATM:  An Active self-training framework for label-efficient text classification.  The author could probably elaborate more on the details of pseudo-labeling. "
ARR_2022_261_review,"- This paper works on zero-shot relation extraction. Note that the authors assume that ground-truth entities in each sentence are given as input. In other words, this task does not require identifying entities.
- The problematic issue this paper focuses on is the difficulty of distinguishing similar but different-class relations, called “similar relations” and “similar entities” (see the details in line 71-84 and Table 1 shows some examples).
- To mitigate this issue, the authors propose a new relation contrastive learning framework (RCL). The reason why they applied contrastive learning (particularly, instance-wise contrastive learning) to zero-shot relation extraction is that some existing studies reported its remarkable effectiveness for representation learning (see the details in line. 84-91).
- Their proposed method achieved better performance (Table 2). Also, using dropout for augmentation is more effective than other augmentation techniques (Table 3), which is consistent with the result of Guo et al. (2021). ",- Their method consistently achieves performance improvements (Table 2).  - Their proposed method is so simple that readers can reimplement their method. ,"- Their claim is not properly supported. In Section 1 (in line 71-84 and 121-124), the authors introduce the problem they want to solve, and they state as follows: “It effectively mitigates two types of similar problems: similar relations and similar entities by learning representations jointly optimized with contrastive loss and classification loss.”  It is true that they show some actual examples their method solved in Appendix (Figure 6). However, there is no quantitative evidence for supporting that their proposed method solves or mitigates the problem. Thus, readers cannot judge whether their method can mitigate the “similar relations and similar entities” problem or not. Readers would appreciate if the authors could provide quantitative results for supporting the claim.
- The authors adopt different evaluation metrics (B^3 F1, NMI, ARI) from the standard metric (F1 score) used in many existing papers on zero-shot RE, such as Chen and Li (2021) and Levy et al. (2017). In terms of the three metrics, the state-of-the-art method, ZS-BERT, is inferior to other methods, such as Att-BiLSTM, which surprises many readers. Although it is okay to adopt the different metrics, many readers are likely to see the performance comparisons in terms of the standard F1 score as well as the three metrics.
- The proposed method is not so new. It is true that the proposed method includes a few simple extensions for relation extraction (e.g., Softmax Layer and Concat Layer), the key idea and most parts of their method are based on existing contrastive learning methods, such as SimCSE proposed by Gao et al. (2021). So, readers would be happy if the authors specify which parts are their original extensions more clearly.  - Minor point: Some notations are confusing and undefined. Please see “Comments, Suggestions And Typos” in detail. ","- How about comparing your proposed method with existing ones by using the standard F1 score?
- Missing references: (1) For Open Relation Extraction (in line 156), it might be better to cite the pioneering work, Bank et al. “Open Information Extraction from the Web” in Proc. of IJCAI2007. For contrastive learning in NLP, there are some existing studies. ( 2) For sequence labeling, Wiseman and Stratos “Label-Agnostic Sequence Labeling by Copying Nearest Neighbors” in Proc. of ACL2019. ( 3) For span classification such as NER, Ouchi et al. “Instance-Based Learning of Span Representations” in Proc. of ACL2020. They do not call their methods ""contrastive learning,"" but their methods are a type of instance-wise contrastive learning.
- Notations: (1) In line 247, the subscript $i$ (in $X_i$) is not defined. ( 2) In Equation 1, the symbol $i$ is used for both the subscript and the superscript, such as $X^i_i$, which is a little bit confusing. It might be better to use different symbols. ( 3) Also, in Equation 1, the authors assume that the first entity span is $(i, j-1)$ and the second one $(k, l-1)$. It might be better to specify the entity spans and the notations. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Their claim is not properly supported. In Section 1 (in line 71-84 and 121-124), the authors introduce the problem they want to solve, and they state as follows: “It effectively mitigates two types of similar problems: similar relations and similar entities by learning representations jointly optimized with contrastive loss and classification loss.”  It is true that they show some actual examples their method solved in Appendix (Figure 6). However, there is no quantitative evidence for supporting that their proposed method solves or mitigates the problem. Thus, readers cannot judge whether their method can mitigate the “similar relations and similar entities” problem or not. Readers would appreciate if the authors could provide quantitative results for supporting the claim.
- The authors adopt different evaluation metrics (B^3 F1, NMI, ARI) from the standard metric (F1 score) used in many existing papers on zero-shot RE, such as Chen and Li (2021) and Levy et al. (2017). In terms of the three metrics, the state-of-the-art method, ZS-BERT, is inferior to other methods, such as Att-BiLSTM, which surprises many readers. Although it is okay to adopt the different metrics, many readers are likely to see the performance comparisons in terms of the standard F1 score as well as the three metrics.
- The proposed method is not so new. It is true that the proposed method includes a few simple extensions for relation extraction (e.g., Softmax Layer and Concat Layer), the key idea and most parts of their method are based on existing contrastive learning methods, such as SimCSE proposed by Gao et al. (2021). So, readers would be happy if the authors specify which parts are their original extensions more clearly.  - Minor point: Some notations are confusing and undefined. Please see “Comments, Suggestions And Typos” in detail. 
- How about comparing your proposed method with existing ones by using the standard F1 score?
- Missing references: (1) For Open Relation Extraction (in line 156), it might be better to cite the pioneering work, Bank et al. “Open Information Extraction from the Web” in Proc. of IJCAI2007. For contrastive learning in NLP, there are some existing studies. ( 2) For sequence labeling, Wiseman and Stratos “Label-Agnostic Sequence Labeling by Copying Nearest Neighbors” in Proc. of ACL2019. ( 3) For span classification such as NER, Ouchi et al. “Instance-Based Learning of Span Representations” in Proc. of ACL2020. They do not call their methods ""contrastive learning,"" but their methods are a type of instance-wise contrastive learning.
- Notations: (1) In line 247, the subscript $i$ (in $X_i$) is not defined. ( 2) In Equation 1, the symbol $i$ is used for both the subscript and the superscript, such as $X^i_i$, which is a little bit confusing. It might be better to use different symbols. ( 3) Also, in Equation 1, the authors assume that the first entity span is $(i, j-1)$ and the second one $(k, l-1)$. It might be better to specify the entity spans and the notations. "
ARR_2022_119_review,"This paper proposes a text-to-SQL parser that can better utilize given alignments between text and SQL queries. Empirically, the proposed parser achieves the new state-of-the-art performance on the SQUALL dataset. ","The usage of alignments as contexts is creative and coupled well with BERT-based encoding. The training method of noisy alignment augmentation is also interesting, and the way I view this is that it's an efficient way of ""marginalizing"" over all possible noisy alignments. ","My main complaints are 1) the presentation of this work is a bit misleading to me, 2) I'm not convinced by the main arguments of this work.
For 1), it's not completely clear from early sections that this work focus on the setting where the gold alignments are given. I don't think this setting is the default text-to-SQL in the current literature, so it'd be better to clarify this as early as possible.  For 2), the main arguments of the proposed system are that attention cannot capture alignments well because it's at the token level. But isn't the sequence tagging model used is also at the token level?  The second argument is that attention is prone to overfitting. I'm not convinced that we should attribute the overfitting of current parsers to attention mechanisms, and this paper does not provide any justification.  Maybe the authors should emphasize their way of incorporating alignments (alignments as context, and alignment augmentation), which I think are the main contribution of this work. ","The two-stage process is very related to [1]. In general, the paper perhaps should also mention the line of work that treat alignments as latent variables, e.g., [2, 3] - [1] Compositional Generalization via Semantic Tagging - [2] Span-based semantic parsing for compositional generalization - [3] Learning semantic parsers from denotations with latent structured alignments and abstract programs ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"My main complaints are 1) the presentation of this work is a bit misleading to me, 2) I'm not convinced by the main arguments of this work.
For 1), it's not completely clear from early sections that this work focus on the setting where the gold alignments are given. I don't think this setting is the default text-to-SQL in the current literature, so it'd be better to clarify this as early as possible.  For 2), the main arguments of the proposed system are that attention cannot capture alignments well because it's at the token level. But isn't the sequence tagging model used is also at the token level?  The second argument is that attention is prone to overfitting. I'm not convinced that we should attribute the overfitting of current parsers to attention mechanisms, and this paper does not provide any justification.  Maybe the authors should emphasize their way of incorporating alignments (alignments as context, and alignment augmentation), which I think are the main contribution of this work. 
The two-stage process is very related to [1]. In general, the paper perhaps should also mention the line of work that treat alignments as latent variables, e.g., [2, 3] - [1] Compositional Generalization via Semantic Tagging - [2] Span-based semantic parsing for compositional generalization - [3] Learning semantic parsers from denotations with latent structured alignments and abstract programs "
ARR_2022_203_review,"The paper deals with multi-task learning. The authors find that having separate networks to learn separate tasks would lead to good performance, but requires a large memory. Using MT-DNN would save memory, but the results are not satisfying. The authors thus propose an approach that saves memory and at the same time achieves good results on GLUE.
Figure 1 gives a good summary. First, train task-specific models (but for each model, only finetune the top n layers). Second, do knowledge distillation, so that we can compress the n layers into a smaller number of layers. Third, merge all the models together as shown in Figure 1(c). The GLUE performance is comparable to full fine-tuning (i.e., tuning k models separately where k is the number of tasks), and the proposed approach saves 2/3 of memory. ","- Writing is clear.
- Many baseline experiments are performed, including DistillBERT, BERT-of-Theseus, MT-DNN, and many others.  - The observation about MT-DNN’s degradation on QQP (line 237) is interesting. This observation reminds me of the intermediate training empirical paper, where in Table 1 they find that QQP is a very different task compared to others: https://arxiv.org/pdf/2005.00628.pdf ","Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary?  Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. ","Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).
Minor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary?  Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. 
Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).
Minor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. "
ARR_2022_116_review,"Authors present  a novel finetuning method, BitFit, where instead of finetuning the entire model the authors finetune only the `bias’ terms and the final task-specific classification layer. The authors compare BitFit to other parameter-efficient finetuning methods such as adapters and sparse params and report that BitFit performs better than previous approaches and even better than full finetuning on certain tasks. Additional analysis on the change in values of layer wise bias terms reveals that finetuning a specific subset of bias terms can also lead to marginally lower performance as compared to finetuning all bias terms. For low to medium data regimes, BitFit performs better than full finetuning. ","1. Simple and effective method of finetuning that performs better than competitive baselines. For low to medium data, BitFit has better performance than fine-tuning the entire transformer model. 
2. Author perform additional analysis on the change of bias terms and find that tuning a subset of bias terms also lead to marginally lower performance than finetuning all bias terms. ","1. Sensitivity of results to random seeds is not clear. Would be helpful to report variance for finetuning under different random seed settings. 
2. The actual strengths or limitations of BitFit are unclear. To elaborate, in addition to reporting empirical numbers on multiple tasks an analysis of the kind of examples where BitFit performs well should be added. It would be helpful for the community to understand if there is a certain type of tasks/nature of examples,  where BitFit performs better or is comparable to full fine-tuning. ",Minor typo: Line 69: hard reason --> hard to reason ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Sensitivity of results to random seeds is not clear. Would be helpful to report variance for finetuning under different random seed settings. 
2. The actual strengths or limitations of BitFit are unclear. To elaborate, in addition to reporting empirical numbers on multiple tasks an analysis of the kind of examples where BitFit performs well should be added. It would be helpful for the community to understand if there is a certain type of tasks/nature of examples,  where BitFit performs better or is comparable to full fine-tuning. 
Minor typo: Line 69: hard reason --> hard to reason "
ARR_2022_116_review,"The paper proposes a simple and efficient method, BitFit, for fine-tuning language models on tasks with small-or-low training data. BitFit fine-tunes only the bias-terms of the language model and shows comparable performance to full fine-tuning on some tasks in GLUE.  The bias terms constitute very few parameters of a pre-trained LM and so BitFit is more efficient than DiffPrune and Adapters in terms of the number of parameters fine-tuned. ",- The paper introduces a simple technique for fine-tuning language models and comparisons with other techniques show that the method is parameter efficient on the GLUE benchmark. ,"- The paper provides some unsubstantiated conjectures about the ""fine-tuning as exposure of existing capabilities in LMs"". Without adequate reasoning or references, it is hard to comprehend what the author(s) refer to in this case.
- Generalization gap: No reference to a graph or table that the Generalization gap talks about. However, I did understand later that the generalization gap referred to here is from validation to test in Table 1. The difference between full fine-tuning validation and test also seems to be within 3% from this table and given that experiments haven't been run over multiple seeds, the conclusion about the lower generalization gap is unformed. On QQP, both full fine-tuning and BitFit suffer very similar drops. 
    - Have the author(s) run the codes for multiple seeds and reported the mean? 
    - If not run for multiple seeds, is the <1% difference a significant result?
Overall, while this paper proposes an interesting empirical result, it makes some unsubstantiated claims and some of the conclusions from the experiments are not adequately justified. The paper could benefit from a more substantial discussion of fewer results in the main paper with a more detailed discussion deferred to the Appendix for other experiments. ","- Perhaps this point is not explicitly mentioned, but it seems like BitFit also has the added advantage of *not* introducing any new parameters to the model. This would have been a nice positive to stress on.
- Is there any reason why the paper stresses on Masked Language Models in particular? Did experiments not work out with an autoregressive LM? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The paper provides some unsubstantiated conjectures about the ""fine-tuning as exposure of existing capabilities in LMs"". Without adequate reasoning or references, it is hard to comprehend what the author(s) refer to in this case.
- Generalization gap: No reference to a graph or table that the Generalization gap talks about. However, I did understand later that the generalization gap referred to here is from validation to test in Table 1. The difference between full fine-tuning validation and test also seems to be within 3% from this table and given that experiments haven't been run over multiple seeds, the conclusion about the lower generalization gap is unformed. On QQP, both full fine-tuning and BitFit suffer very similar drops. 
    - Have the author(s) run the codes for multiple seeds and reported the mean? 
    - If not run for multiple seeds, is the <1% difference a significant result?
Overall, while this paper proposes an interesting empirical result, it makes some unsubstantiated claims and some of the conclusions from the experiments are not adequately justified. The paper could benefit from a more substantial discussion of fewer results in the main paper with a more detailed discussion deferred to the Appendix for other experiments. 
- Perhaps this point is not explicitly mentioned, but it seems like BitFit also has the added advantage of *not* introducing any new parameters to the model. This would have been a nice positive to stress on.
- Is there any reason why the paper stresses on Masked Language Models in particular? Did experiments not work out with an autoregressive LM? "
ARR_2022_116_review,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model. The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning). The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters. ","- The paper is well written and easy to understand.
- The proposed method (BitFit) is neat and novel.
- The authors show strong empirical results on GLUE benchmark. ",I do not have any concerns about this paper. ,"It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5). But current version is good enough for a short paper. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"
It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5). But current version is good enough for a short paper. "
ARR_2022_362_review,"The authors perform an investigation on the effects of cross-domain classification of moral values from text. Given the specificity of the moral values  This involves experiments that evaluate a multi-label moral value classifier conducted under different cross-domain training settings that analyze the following scenarios — (a) ideal all-data training scenario, (b) generalizability, (c) transferability, and (d) catastrophic forgetting. Some of the key findings include: (i) the ability of a value classifier to generalize to novel domains when trained on multiple domains combined with a small amount of data from the novel domain, (ii) fine-tuning on a novel domain causes catastrophic forgetting of the pre-trained domains. The authors also analyze the correspondence between model predictions and the annotators agreement. The authors report that the majority of classification errors are not severe as at least one annotator agreed with the model prediction in all evaluation settings. ","1. The paper is well-written and easy to understand. 
2. The paper performs a comprehensive set of evaluations for cross-domain classification and provides good explanation of all the different experimental settings . ","1. While the paper focuses on generalizability and transferability of value classifier, however,  there needs more clarity on domain specificity. Though the paper talks briefly about the moral value expressions taking different forms in various domains, the BERT model used here doesn’t seem to take into account the domain information. Hence, the value classifier might emit the same moral value label for a moral value expression irrespective of the domain. 
2. Though the objective is not to compare classifiers but evaluate the cross-domain capability, the observation of catastrophic forgetting, despite several other reasons, could also be a result of lack of domain information leading to more misclassification errors. 
3. The evaluation sections reports results over 10 runs, but the paper doesn’t report the standard deviation that potentially provides more information about the sensitivity of the model to order or domains trained vs domains tested. 
4. The findings of the paper pretty much align with earlier understanding of how training on larger amount of data  from different or same domain helps improve generalizability or transferability. Though it is important to validate them in the moral value applications (which is limited),  it will be better if the paper clarifies what its core contributions are and how it varies from other general approaches/findings in domain adaptation or multi-task learning methods. ","More clarity on specific contributions of the paper. 
Explaining the limitations/assumptions and potential reasons behind specific findings would be useful. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. While the paper focuses on generalizability and transferability of value classifier, however,  there needs more clarity on domain specificity. Though the paper talks briefly about the moral value expressions taking different forms in various domains, the BERT model used here doesn’t seem to take into account the domain information. Hence, the value classifier might emit the same moral value label for a moral value expression irrespective of the domain. 
2. Though the objective is not to compare classifiers but evaluate the cross-domain capability, the observation of catastrophic forgetting, despite several other reasons, could also be a result of lack of domain information leading to more misclassification errors. 
3. The evaluation sections reports results over 10 runs, but the paper doesn’t report the standard deviation that potentially provides more information about the sensitivity of the model to order or domains trained vs domains tested. 
4. The findings of the paper pretty much align with earlier understanding of how training on larger amount of data  from different or same domain helps improve generalizability or transferability. Though it is important to validate them in the moral value applications (which is limited),  it will be better if the paper clarifies what its core contributions are and how it varies from other general approaches/findings in domain adaptation or multi-task learning methods. 
More clarity on specific contributions of the paper. 
Explaining the limitations/assumptions and potential reasons behind specific findings would be useful. "
ARR_2022_62_review,"This paper focus on table pretraining via spreadsheet formula to realize numerical reasoning. Concretely, it proposes two effective pretraining tasks: numerical reference prediction and numerical calculation prediction. At the same time, formula mask language model is also employed to get better representation of the spreadsheet formula. Experimental results show that the numerical-reasoning-aware pretraining make the TUTA (tree-based transformer for table pretraining) outperforms SOTA methods on formula prediction, question answering and cell ","1. The paper is well-written and easy to follow.
2. The numerical-reasoning-aware pretraining outperforms SOTA by large margin in three representative tasks.
3. Analysis is very detailed to further clarify the effectiveness of pretraining. ","Most of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task,  it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. ","Questions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? 
2. In Table 2, what about the result of  SpreadsheetCoder under 20% train set? 
3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?
Suggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. 
2. The order of terms in L288-L290 should be aligned with Figure 2. 
3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example. 
4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely.  Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Most of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task,  it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. 
Questions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? 
2. In Table 2, what about the result of  SpreadsheetCoder under 20% train set? 
3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?
Suggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. 
2. The order of terms in L288-L290 should be aligned with Figure 2. 
3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example. 
4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely.  Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 "
ARR_2022_62_review,"This paper presents a technique for numerical reasoning over tables by leveraging formulas based on the pre-trained model TUTA specifically designed for tables.  FORTAP, a FORmula-driven TAble pre-training model, is proposed in this paper, which incorporates numerical reasoning capabilities based on large-scale spreadsheet formulas.
The main contributions are two complementary/auxiliary objective functions:   * Numerical Reference Prediction,   * Numerical Calculation Prediction.  They take relationships between cells into consideration and predict all operators using cell embeddings.  Experimental results show their method achieves better performance over several datasets when compared to both baseline and SOTA models. ","- Show the potential of leveraging formulas in table pretraining.
- Good results based on comprehensive experimental settings. Apart from that, the authors provide very detailed explanations and discussions.  - Very clear structure and good organization. ","- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.
- To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. ","- More clarification and novelty is needed for contributions in Sec 1.
- As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?
- TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?
- Is there any deficiency/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.
- To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. 
- More clarification and novelty is needed for contributions in Sec 1.
- As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?
- TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?
- Is there any deficiency/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? "
ARR_2022_50_review,"The paper described a set of approaches from unsupervised to weakly supervised to segment sentences into words and morphemes under an extremely low-resource setting. The approaches are tested on two languages currently (still) being documented.
It would be very interesting to follow the developments, should a system such as the presented one be implemented and used in actual fieldwork. Two of the points I can think of are the feedback from fieldworkers as well as the (hopefully) incremental precision of the system. It would also be extremely interesting to test how much input is needed to reach acceptable levels of automated segmentation. ","The authors work in a low-resource setting, and their work is very valuable and could greatly help linguistic fieldworkers with (semi-)automatic segmentation of either recorded speech or transcripts.  The authors convincingly show that the chosen approach outperforms two not-so-simple baselines (SentencePiece and Morfessor).
Overall, the language is mostly clear and the structure logical.
In response to one of the earlier reviews, the authors have added experiments with Morfessor, which is also highly laudable. ","Unfortunately, the paper is very difficult to follow in parts particularly because of the extensive use of abbreviations.
Regarding word vs morpheme, what exactly is the conclusion reached? It is not entirely clear, yet it seems to be one of the main questions in the paper.
The final sentence in the paper claims that the proposed algorithm may help speed up annotation processes, and as such must run fast. That holds only true if the algorithm is run on-the-fly, and if human annotators are used to incrementally teach the algorithm. Otherwise the algorithm can be run independently of the annotators; one could pre-segment all data automatically and then present it to annotators for correction. ","In line 165, you don't explain ""DP"", although it is recoverable through the context. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Unfortunately, the paper is very difficult to follow in parts particularly because of the extensive use of abbreviations.
Regarding word vs morpheme, what exactly is the conclusion reached? It is not entirely clear, yet it seems to be one of the main questions in the paper.
The final sentence in the paper claims that the proposed algorithm may help speed up annotation processes, and as such must run fast. That holds only true if the algorithm is run on-the-fly, and if human annotators are used to incrementally teach the algorithm. Otherwise the algorithm can be run independently of the annotators; one could pre-segment all data automatically and then present it to annotators for correction. 
In line 165, you don't explain ""DP"", although it is recoverable through the context. "
ARR_2022_50_review,"This paper takes advantage of data (utterances, annotations) that is produced as part of language documentation to use for automatic linguistic segmentation, mainly at the word level. The authors worked with two languages that are still being documented Mboshi and Japhug. Both languages have data available but at different levels of granularity. They used Bayesian non-parametric approaches with different variations of the n-gram models and data used in addition to an incremental data training approach. The evaluation is reported in terms of F1 on three different segmentation levels (morphological boundary level, token level, and type-level). The discussion points out that the weakly supervised performs better than fully supervised and of course better than unsupervised. On the other hand, morpheme-based segmentation benefits more from underserviced training. ","- The paper is well written and easy to follow.
- The languages that were targeted are still being documented and they resemble a real low-resource setting.
- The core approach is easy to follow and replicate. It lends itself to being easily explainable in terms of behavior and performance.
- This effort can be helpful as an enabling technology for the language documentation process. ","There are no major weaknesses in this version of the paper. 
The weaknesses mentioned in the previous review have been addressed in the current version for the most part. ",I appreciate the authors taking the time to improve upon the paper. I believe this work would definitely facilitate the documentation process whether in progress or as a post-processing step. It would be really useful for a future version of this work to include a use case study on using this process and reporting how useful it was. This would definitely increase the value of this work among both NLP and language documentation communities. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"There are no major weaknesses in this version of the paper. 
The weaknesses mentioned in the previous review have been addressed in the current version for the most part. 
I appreciate the authors taking the time to improve upon the paper. I believe this work would definitely facilitate the documentation process whether in progress or as a post-processing step. It would be really useful for a future version of this work to include a use case study on using this process and reporting how useful it was. This would definitely increase the value of this work among both NLP and language documentation communities. "
ARR_2022_192_review,"In this resubmitted paper, the authors propose a framework that generates tables of contents for long documents based on different personas. The authors start with dataset introduction. And then the authors present the details of the framework, including aspect detection, persona mapping, intelligent navigation via question generation. The authors also demonstrate the experiments on different subtasks in the framework. The authors also present the human evaluation that demonstrates the successful performance of the pipeline. ","In comparison with the previous version, the authors successfully improved the paper. 
1. With regard to novelty: Although the paper generally describes the same framework as the one proposed in the previous version, I think the authors clearly highlight their contribution in the improvement of TOC generation, including algorithmic results and human assessment. 
2. With regard to details and organization: The authors re-organized the paper, and clearly stated the details in the proposed framework and dataset. 
3. With regard to experiments: In this version, the authors provide detailed analysis of the proposed framework, Figure 4 & 5 clearly show the some examples from results of different aspect detection methods ",I am still a little concerned about the Figure 2. This still requires a high-res display and zoom-in. I suggest the authors use supplementary pages or materials to show a clearer and detailed version of the table. ,"No questions. 
I think the authors have polished the paper and achieved a good status. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,No. No concerns. ,"I am still a little concerned about the Figure 2. This still requires a high-res display and zoom-in. I suggest the authors use supplementary pages or materials to show a clearer and detailed version of the table. 
No questions. 
I think the authors have polished the paper and achieved a good status. "
ARR_2022_192_review,"A dynamic table of content-based navigator called DYNAMICTOC has been proposed for persona-based document consumption that highlights sections of interest. DYNAMICTOC is also augmented with short questions generated using deep reinforcement learning techniques to assist the users in understanding the underlying content of persona-clustered paragraphs. In addition to automatic evaluations, human evaluation is also conducted to demonstrate the effectiveness of DYNAMICTOC. ","1. A novel dynamic table of content-based navigator has been proposed. 
2. Good amount of experiments were conducted to validate the effectiveness of the proposed model. 
3. The paper is Well-written. ","1. Does not evaluate the performance of the end-to-end model using any automatic metrics. 
2. Only a small human evaluation has been conducted. 
3. Should use more baseline models to investigate the effectiveness of various techniques. ","1. Figure 2 should be presented better. 
2. The performance difference between the clusters in Figure 4 (a) and Figure 4 (b) should be demonstrated better. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. Does not evaluate the performance of the end-to-end model using any automatic metrics. 
2. Only a small human evaluation has been conducted. 
3. Should use more baseline models to investigate the effectiveness of various techniques. 
1. Figure 2 should be presented better. 
2. The performance difference between the clusters in Figure 4 (a) and Figure 4 (b) should be demonstrated better. "
ARR_2022_288_review,"This paper introduces a domain-specific model for analyzing political conflict and violence. They evaluate this model in 18 tasks and show that ConfliBERT performs better than BERT overall, particularly in settings with less annotated data. ","- The arguments for having a domain-specific BERT model for political conflict and violence is well-motivated and supported with references to benefits of domain-specific models  - Discussion of different vocabularies and tokenization in 3.1 and comparisons in the task performance results was interesting - Discussion of model details and training time was good for replicability - Thorough comparisons of ConfliBERT models pretrained from scratch and continual pretraining, as well as with BERT - The curated data and model will be useful for future NLP and political science research (cross-disciplinary impact) ","- Unclear how mainstream news was filtered to be relevant for political conflict. It could be useful to have some more data filtering details and evaluation task descriptions (e.g. example input, output classes, etc) in the appendix - [minor weakness] Beyond the vocabulary and model performance tables, I want even more out of the comparison between ConfliBERT and BERT because the model is the main contribution (there's no new theoretical contribution, major methodological contribution, or dataset). [ suggestion] Is it possible to do a manual analysis of cases where BERT misclassifies a text and ConfliBERT does it correctly, or both models still struggle? Why does ConfliBERT have large improvements for some tasks but not others? ","Since ConfliBERT’s main gains seems to be from political conflict-relevant words in the vocabulary, how well will it generalize in a rapidly-evolving space? Would it need to be retrained as different political groups and organizations emerge? ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Unclear how mainstream news was filtered to be relevant for political conflict. It could be useful to have some more data filtering details and evaluation task descriptions (e.g. example input, output classes, etc) in the appendix - [minor weakness] Beyond the vocabulary and model performance tables, I want even more out of the comparison between ConfliBERT and BERT because the model is the main contribution (there's no new theoretical contribution, major methodological contribution, or dataset). [ suggestion] Is it possible to do a manual analysis of cases where BERT misclassifies a text and ConfliBERT does it correctly, or both models still struggle? Why does ConfliBERT have large improvements for some tasks but not others? 
Since ConfliBERT’s main gains seems to be from political conflict-relevant words in the vocabulary, how well will it generalize in a rapidly-evolving space? Would it need to be retrained as different political groups and organizations emerge? "
ARR_2022_288_review,"This paper presents a new domain-specific language model for the area of political violence and conflicts. General-purpose language models, like BERT, do not cover important domain-specific vocabularies, as a result, those models do not perform well in many downstream tasks. This paper addresses this limitation and proposes ConfliBERT, which is trained on text data from the area of social unrest, political violence, etc. ConfliBERT has two variations – (1) built from scratch, and (2) trained on top of pre-trained BERT. Experimental results show that either version performs better than BERT in several tasks related to this domain. ","+ This paper proposes a new language model for a domain. As the authors claim there are not any previous language models specific to this area. While this may boost research in this area, this new model can also help in developing new research questions. 
+ There is a lack of a large text corpus specific to this area. Addressing this issue, the authors develop this model by merging several existing datasets and creating a large dataset that is suitable to train a model like BERT. This corpus will be an important resource to conduct text data-driven research in this area. 
+ The paper evaluates its claims through a variety of experiments. The experiments are designed to include several types of problems, such as binary and multiclass classification, sequence labeling, on the other hand, each problem has been tested on several datasets. In all these experiments, the ConfliBERT has consistently outperformed the performance of the BERT model. These results demonstrate the power of this model across a diverse set of problems and corpora (across the world) and the need for a domain-specific language model in this context. 
+ The work is reproducible, where the paper furnishes details about the implementation, datasets, and hyperparameters and also the system configuration for training. ","-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case.  The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. ","The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case.  The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. 
The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? "
ARR_2022_287_review,The paper presents a novel dataset for the task of stereotype detection. The authors also propose a multi-task model that uses neighbor tasks such as hate speech detection and offensive language detection to improve the classification on stereotypes. They further introduce a reinforcement learning agent that can identify the most informative examples in multi-task learning setup. An ablation study analyzes the importance of different factors in the classification process. ,- detecting stereotypes and biases in the pre-trained language model is an important problem in contemporary NLP - the proposed dataset appears to be of better quality than existing resources and contain real-world examples rather than artificially constructed ones - using multi-task approach is justified in the presented situation - various experiments and ablation study give a good overview of the advantages of the model ,"- the writing of the paper can be improved in some sections (Introduction/Related work) to accommodate readers that are not experts on the task - missing details on corpus creation (instructions, agreement) ","- the paragraph from l90 to l116 is very unclear. It is explained in later paragraphs, but on first reading it is somewhat confusing - l121 - ``consists'' -> ``consists of'' or ``contains'' - can you include more details on the selection criteria for candidate examples from reddit (section 3.1). E.g.: what are the topics, what are the stereotype targets, what kind of filtering do you apply?
- can you include agreement statistics for the corpus in section 3.1 or 3.2?
- while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for ""selecting relevant examples"", in order to justify the use of RL in that setup over a simpler selection method ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- the writing of the paper can be improved in some sections (Introduction/Related work) to accommodate readers that are not experts on the task - missing details on corpus creation (instructions, agreement) 
- the paragraph from l90 to l116 is very unclear. It is explained in later paragraphs, but on first reading it is somewhat confusing - l121 - ``consists'' -> ``consists of'' or ``contains'' - can you include more details on the selection criteria for candidate examples from reddit (section 3.1). E.g.: what are the topics, what are the stereotype targets, what kind of filtering do you apply?
- can you include agreement statistics for the corpus in section 3.1 or 3.2?
- while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for ""selecting relevant examples"", in order to justify the use of RL in that setup over a simpler selection method "
ARR_2022_287_review,"This paper proposes presents a multi-task learning approach to improve low-resource stereotype recognition performance. The authors first critically analyse the state of the art in stereotype detection. Based on the resulting insights, they propose a new 4-way classification setup that is more suitable for the task and that guarantees a more systematic analysis of language phenomena in real-world data. Due to the limits of previously defined datasets, they propose a data collection to derive a crowd-sourced dataset based on Reddit content. Additionally, they use the same annotation protocol to analyse previous datasets as well. However, stereotype detection is a task with very few accurate data points. Therefore, they rely on the intuition that other related offensive language datasets could be used to boost performance. This is mostly because offensive language phrases are typically relying on specific stereotypes. In their evaluation, they use 6 offensive language datasets in a multi-task learning regime. This multi-task learning regime can be further enhanced with a policy that decides which data points to consider for training based on the classification performance. The policy is trained via an actor-critic reinforcement learning algorithm to optimise the F1 measure of the classification task. The authors tried this approach with several large-scale language models showing substantial performance improvement over the corresponding baselines. ",1. The authors completed an analysis of the literature which they use to define a data collection procedure based on a novel annotation scheme for stereotype detection; 2. The authors propose to combine several offensive language detection to boost performance in the stereotype classification setup; 3. The authors describe how to use a state of the art RL technique to combine multiple tasks together; 4. The paper clearly describes the problem that the authors are trying to solve and provide a very good evaluation to support their approach. ,"1. The multi-task learning approach defined in Section 4.1 is not described in detail. There are several ways of performing multi-task learning and the authors should be a little bit more precise in the description of their model. For a more systematic description of MTL approaches for Deep learning models, please refer to https://arxiv.org/abs/1706.05098. 
2. Multi-task learning generally struggles when tasks of different complexities are combined together (e.g., see GradNorm for a possible solution: https://arxiv.org/abs/1711.02257). On the other hand, the authors assume that multi-task learning is going to work without putting much effort into studying how each task affects the others. 
3. The authors perform an ablation where the model is trained using each neighbour dataset together with the reference dataset. However, to show the power of using multiple related tasks at the same time, it would be beneficial to see the effect on the overall performance, of different combinations of datasets. ","Regarding the issue 1) and 2), I believe the authors should better describe their MTL approach and properly localise it in the literature. Additionally, the authors should provide more details about the training regime and what is the effect of combining different tasks at once. In this sense, weakness 3) can be useful to solve all the others. In particular, I would suggest the authors take their `bert-base` classifier, and train using MTL with different dataset combinations. For instance, assuming that their reference task is S and the related tasks are T_1, T_2, ..., T_6, I would evaluate the following combinations: 1. T_1 + S 2. T_1 + T_2 + S 3. (...) 
4. T_1 + T_2 + ... + T6 + S (this is the version with all the datasets that is currently implemented in the evaluation) In this sense, Table 4 goes in this direction. However, I believe the evaluation should focus on the performance of the stereotype detection task only because is the reference task for this paper. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The multi-task learning approach defined in Section 4.1 is not described in detail. There are several ways of performing multi-task learning and the authors should be a little bit more precise in the description of their model. For a more systematic description of MTL approaches for Deep learning models, please refer to https://arxiv.org/abs/1706.05098. 
2. Multi-task learning generally struggles when tasks of different complexities are combined together (e.g., see GradNorm for a possible solution: https://arxiv.org/abs/1711.02257). On the other hand, the authors assume that multi-task learning is going to work without putting much effort into studying how each task affects the others. 
3. The authors perform an ablation where the model is trained using each neighbour dataset together with the reference dataset. However, to show the power of using multiple related tasks at the same time, it would be beneficial to see the effect on the overall performance, of different combinations of datasets. 
Regarding the issue 1) and 2), I believe the authors should better describe their MTL approach and properly localise it in the literature. Additionally, the authors should provide more details about the training regime and what is the effect of combining different tasks at once. In this sense, weakness 3) can be useful to solve all the others. In particular, I would suggest the authors take their `bert-base` classifier, and train using MTL with different dataset combinations. For instance, assuming that their reference task is S and the related tasks are T_1, T_2, ..., T_6, I would evaluate the following combinations: 1. T_1 + S 2. T_1 + T_2 + S 3. (...) 
4. T_1 + T_2 + ... + T6 + S (this is the version with all the datasets that is currently implemented in the evaluation) In this sense, Table 4 goes in this direction. However, I believe the evaluation should focus on the performance of the stereotype detection task only because is the reference task for this paper. "
ARR_2022_287_review,"This paper presented a fine-grained stereotype detection set and proposed a reinforcement-learning based multi-task framework for stereotype detection. Different from previous datasets that focus more on the explicit stereotype, the constructed dataset considers explicit, implicit and non-stereotypes. Experimental results show that the proposed framework significantly improves the base model on different datasets. ","1. This paper presented a fine-grained evaluation set for stereotype detection, which aims to alleviate the conceptual issue of various stereotypes.
2. The proposed reinforcement-learning based multi-task framework shows superior performance over the base model. ","1. The authors presented a fine-grained evaluation set in this paper. However, the anti-stereotype that appears in previous datasets is missing in the constructed dataset. In addition, details of annotations are missing in this paper. Since stereotype detection is quite challenging, it would be important to discuss how to guarantee the annotation quality and whether annotators can reach an agreement on collected corpus.
2. Missing related baselines. Only PLMs are considered in this paper and other task-related baselines are missing.
3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set? ","1. More discussions about dataset construction should be provided, e.g., the time range of data collection, preprocessing strategies and quality control.
2. All ""table"" and ""figure"" in the context should be capitalized. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The authors presented a fine-grained evaluation set in this paper. However, the anti-stereotype that appears in previous datasets is missing in the constructed dataset. In addition, details of annotations are missing in this paper. Since stereotype detection is quite challenging, it would be important to discuss how to guarantee the annotation quality and whether annotators can reach an agreement on collected corpus.
2. Missing related baselines. Only PLMs are considered in this paper and other task-related baselines are missing.
3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set? 
1. More discussions about dataset construction should be provided, e.g., the time range of data collection, preprocessing strategies and quality control.
2. All ""table"" and ""figure"" in the context should be capitalized. "
ARR_2022_350_review,"This paper presents a cross-lingual information retrieval approach using knowledge distillation. The underlying model is ColBERT with XLM-R as the pretained language model. The approach makes use of a teacher model based on query translation and monolingual IR in English. The student model is trained with two objectives. One is an IR objective to match the teacher model's query-passage relevance predictions. The second objective is to learn a representation of the non-english text that most closely matches the teacher's representation at the token level. This relies on a cross lingual token alignment based on greedily aligning tokens with the highest cosine similarity. The authors do abalations of their two objectives and find they are both useful and also compare against fine-tuning ColBERT directly on cross lingual data. On the XOR-TyDi leaderboard, one of this paper's models is the current best. ",- Novel approach that does cross lingual IR where the resulting model does not use MT  - New cross lingual token alignment based on multilingual pretrained langauge model - Good abalations and comparisons with fine-tuning on cross lingual data - Strong performance on zero-shot settings as well - The paper has best performance on XOR-TyDi ,No major weaknesses ,"line 62-64 asks whether a high performance CLIR model can be trained that can be operate without having to rely on MT. But the training process still relies on MT, so this approach does still rely on MT, right? I guess the point is that it only relies on MT at training time and not at evaluation / inference. It might be possible to try to make this clearer. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"
line 62-64 asks whether a high performance CLIR model can be trained that can be operate without having to rely on MT. But the training process still relies on MT, so this approach does still rely on MT, right? I guess the point is that it only relies on MT at training time and not at evaluation / inference. It might be possible to try to make this clearer. "
ARR_2022_350_review,"The authors describe a novel approach to Cross-Language Information Retrieval (CLIR). They propose a single-model-based approach. That is, in contrast with classical MT-based CLIR systems, where we can clearly distinguish two phases/stages/subprocesses involved (translation + monolingual retrieval), this is now solved altogether. For simplicity they have used English as the target language for their experiments. The results obtained show state-of-the-art performane according to their metrics.
Their proposal takes as basis a ColBERT IR architecture (with proved state-of-the-art performance) and a pre-trained multilingual masked language model (XML-RoBERTa in their experiments). The idea consists on using knowledge distillation (KD) to teach/fine-tune the underlying multilingual model to ""imitate"" the behavior (and, theoretically, the performance) of a monolingual IR system in the target language, even when the CLIR system is taking as input queries in source language, not the target language. This fine-tuning/training process is done in two phases.  A first KD process is focused on teaching the CLIR system to select and rank relevant documents in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to obtain the same output results as a monolingual model. In other words, we are teaching the system how to solve the retrieval part of the process. For this purpose, a regular CLIR dataset is needed (parallel queries and their corresponding relevant documents).
The second KD process is focused on teaching the CLIR system to encode (internally) a source-language text (e.g. the input query to the CLIR system) in the same way as the target-language monolingual system. Ideally, the resulting fine-tuned [cross-language] model should be able to generate the same internal representation of a text as the monolingual model even when they are taking as input texts (queries) in different languages (but being mutual translations, of course). In other words, we are teaching the system how to solve the translation part of the process. For this purpose, a parallel corpora is needed.  Logically, there are still two issues involved in the joint CLIR process (translation + retrieval), but they are now solved at a time after having teached the model how to do it.   The XOR-TyDi CLIR dataset (7 languages + English) is used for in-domain experiments, and the MKQA dataset (5 common languages with the previous one + English) is used for zero-shot experiments. For simplicity, English is always used as the target language. The following baselines, in increasing performance order, are used: (1) [bottom] ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with English data       (2) ColBERT IR model + XML-RoBERTa multilingual masked language model, fine-tuned with both English and cross-language data  (3) [top] classical approach: MT + ColBERT IR model, fine-tuned with English data  The results obtained are clearly positive, notably outperforming the lower baselines and approaching the top monolingual state-of-the-art approach. An ablation study is also performed, proving that both KD processes actually contribute to the results. ","- Novelty.
- State-of-the-art results.
- Quite complete experiments (given the length of the article).
- Well-written. ","- I miss the use of more metrics - Given the length of the main article, 5 pages, 3 pages of Appendix may be a bit excessive. ","- When reading the first part of the article, the authors tend to associate the term ""CLIR"" mainly with theirs and similar approaches, in contrast with the classical MT+monolingual IR approach. However, both of them are CLIR systems and this creates some confusion in the reader.  - Sect. 1 ""Introduction"" contains too many redundancies with respect to the rest of the paper. Taking into account the length of the paper and the Appendix, a reduction of Sect.1 through the removal of unnecessary redundancies would provide space enough to move part of the appendix back to tha main paper, thus making it more complete and self-contained.
- l.166-181: No data are given here about the parallel corpus used for the term-level alignment. This is explained later, in the appendix, but no clue is given to the reader at this point. The authors should move that part back to the main paper or, at least, indicate that those data are provided in the Appendix. Apart from that, nothing is said about the size of the resultant dictionary.
- Figure 2: It has not been useful at all, at least for me.
- l.182-195: According to this, it seems that the resulting CLIR system proposed by the authors would be also able to work not only with queries in the source-language (i.e. non-English), but also with queries in the target-language (i.e. English). Is this the case or just a confusion?
- Table 1: The reader must be able to easily identify which configuration described in the text corresponds with each raw, and this is not the case. This must be corrected.
- I have some concerns about the (only) use of R@5kt as metric. Why no Precision-based metric is used? This information would be very useful to have a better view of the performance of the approach. Moreover, which criteria is used to count the tokens? Do you join the text of all the snippets retrieved and, then, you count 5000 tokens?
- l.266-268: The authors are surprised about the fact that the contribution of the KD with parallel corpus is greater than that of IR triples. I think it makes much sense since you're using a Recall-based metric. Since it is a Recall-based metric, the important thing is that you retrieve the document, regardless of its ranking. In a CLIR context, a more accurate translation (e.g. being able to translate one more term of the input query) allows new matchings, thus improving recall, regardless of the ranking position where the new documents are retrieved.
- References: They contain the usual missing-uppercases errors.
- footnotes 3,4: When a reference to a footnote is inserted immediately before a punctuation mark, they should switch positions; e.g. ""(...) OPUS^3. The (...)"" --> ""(...) OPUS.^3 The (...)""  - l.479-480: How many cases have been analyzed? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"- I miss the use of more metrics - Given the length of the main article, 5 pages, 3 pages of Appendix may be a bit excessive. 
- When reading the first part of the article, the authors tend to associate the term ""CLIR"" mainly with theirs and similar approaches, in contrast with the classical MT+monolingual IR approach. However, both of them are CLIR systems and this creates some confusion in the reader.  - Sect. 1 ""Introduction"" contains too many redundancies with respect to the rest of the paper. Taking into account the length of the paper and the Appendix, a reduction of Sect.1 through the removal of unnecessary redundancies would provide space enough to move part of the appendix back to tha main paper, thus making it more complete and self-contained.
- l.166-181: No data are given here about the parallel corpus used for the term-level alignment. This is explained later, in the appendix, but no clue is given to the reader at this point. The authors should move that part back to the main paper or, at least, indicate that those data are provided in the Appendix. Apart from that, nothing is said about the size of the resultant dictionary.
- Figure 2: It has not been useful at all, at least for me.
- l.182-195: According to this, it seems that the resulting CLIR system proposed by the authors would be also able to work not only with queries in the source-language (i.e. non-English), but also with queries in the target-language (i.e. English). Is this the case or just a confusion?
- Table 1: The reader must be able to easily identify which configuration described in the text corresponds with each raw, and this is not the case. This must be corrected.
- I have some concerns about the (only) use of R@5kt as metric. Why no Precision-based metric is used? This information would be very useful to have a better view of the performance of the approach. Moreover, which criteria is used to count the tokens? Do you join the text of all the snippets retrieved and, then, you count 5000 tokens?
- l.266-268: The authors are surprised about the fact that the contribution of the KD with parallel corpus is greater than that of IR triples. I think it makes much sense since you're using a Recall-based metric. Since it is a Recall-based metric, the important thing is that you retrieve the document, regardless of its ranking. In a CLIR context, a more accurate translation (e.g. being able to translate one more term of the input query) allows new matchings, thus improving recall, regardless of the ranking position where the new documents are retrieved.
- References: They contain the usual missing-uppercases errors.
- footnotes 3,4: When a reference to a footnote is inserted immediately before a punctuation mark, they should switch positions; e.g. ""(...) OPUS^3. The (...)"" --> ""(...) OPUS.^3 The (...)""  - l.479-480: How many cases have been analyzed? "
ARR_2022_231_review,"This paper presents a knowledge distillation method based on MAML. The paper's main aim is to address two drawbacks of existing knowledge distillation methods: (1) teacher being unaware of the capacity of the student, and (2) teacher not optimised for distillation.
The paper assumes the teacher network is trainable and proposes a simple technique that, in the MAML framework, applies ""pilot updates"" in the inner loop to make the teacher and student networks more compatible. Pilot updates are essentially MAML's original inner loop, but applied twice, the first time to update the teacher and the second time to update the student.
Experiments are performed on distilling BERT and evaluated on a number of tasks/datasets in the GLUE benchmark. The proposed method consistently outperforms the compared baselines on almost all tasks.
On the CV side, the proposed method is evaluated on distilling ResNet and VGG. The method achieves comparable performance with SOTA model CRD. ","- Addresses knowledge distillation, an important technique with wide application in machine learning, natural language processing and computer vision.
- The proposed method achieves SOTA performance on a large number of tasks in NLP, in distilling BERT.
- The presentation is clear, logical and easy to follow.
- A comparative analysis on the tradeoff between model performance and computational costs. ","- The proposed method is quite simple. Its essence is the ""pilot update"" mechanism, which basically applies the inner loop of MAML twice, once to update the teacher, and once to update the student.
- The achieved performance boost is moderate.
- The comparative analysis in Table 2 shows that, with significantly more computational costs than the two compared baselines (PKD and ProKT), the model achieves a modest performance gain of about 0.5 absolute F1 points. This analysis, though very welcome, demonstrates that the proposed method suffers a large computational penalty for a small accuracy/F1 gain. ","This paper is a resubmission from ARR August, and it has not been significantly improved. The only major addition is Table 2 that describes the tradeoff between computational cost and model performance. Therefore, the majority of reviewers' comments from ARR August still holds (I was one of the original reviewers). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The proposed method is quite simple. Its essence is the ""pilot update"" mechanism, which basically applies the inner loop of MAML twice, once to update the teacher, and once to update the student.
- The achieved performance boost is moderate.
- The comparative analysis in Table 2 shows that, with significantly more computational costs than the two compared baselines (PKD and ProKT), the model achieves a modest performance gain of about 0.5 absolute F1 points. This analysis, though very welcome, demonstrates that the proposed method suffers a large computational penalty for a small accuracy/F1 gain. 
This paper is a resubmission from ARR August, and it has not been significantly improved. The only major addition is Table 2 that describes the tradeoff between computational cost and model performance. Therefore, the majority of reviewers' comments from ARR August still holds (I was one of the original reviewers). "
ARR_2022_32_review,"The paper studies the benefits of introducing a Bayesian perspective to abstractive summarization. The authors run the MC dropout method on two pre-trained summarization models, sampling different summarization texts according to specific dropout filters. They use BLEUVarN as a metric of uncertainty for a possible summarization, showing the variations across the summary samples. The authors conduct experiments on three datasets on the correlation between the uncertainty and summarization performances, and show that the performance of the summarization can slightly improve by selecting the ""median"" summary across the pool of sampled ones. ","- To the extent of my knowledge, it is the first work that study model uncertainty (in the particular form of the variability of generated summaries) in abstractive summarization.  - The paper provides an analyses on three collections, showing the (cor)relations between the metric of summarization uncertainty (or in fact summarization variability) and ROUGH. They observe that in general the higher the uncertainty score of a summary, the lower its ROUGH score.
- The work shows that the performance of summarization can be slightly improved by selecting the summary that lays in the ""centroid"" of the pool of generated summaries. ","My main concerns are the lack of novelty and proper comparison with a previous study.
- As correctly mentioned in the paper, the work of Xu et al. is not based on MC dropout. However, that work still provide a metric of uncertainty over a generated summary. In fact, the metric of Xu et al. (namely the entropy of the generation distributions) comes with no or little extra computational costs, while the MC dropout of 10 or 20 introduces considerably large feedforward overheads. I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.
- There is no specific novelty in the method. The observation regarding the correlation between uncertainty and performance is in fact an expected one, and has already observed in several previous studies (also in the context of language generation), like: Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rekabsaz, Carsten Eickhoff In proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)- - The reported improvement is marginal, while achieved with the large overhead of MC sampling. My guess is that the improvement is only due to the effect of ensembling, inherent in MC droupout. ","As mentioned above: - I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.
- More evidences regarding the performance improvement, showing that it is not only due to the effect of ensembling.
- Studying more efficient and recent Bayesian approaches, such as: Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"My main concerns are the lack of novelty and proper comparison with a previous study.
- As correctly mentioned in the paper, the work of Xu et al. is not based on MC dropout. However, that work still provide a metric of uncertainty over a generated summary. In fact, the metric of Xu et al. (namely the entropy of the generation distributions) comes with no or little extra computational costs, while the MC dropout of 10 or 20 introduces considerably large feedforward overheads. I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.
- There is no specific novelty in the method. The observation regarding the correlation between uncertainty and performance is in fact an expected one, and has already observed in several previous studies (also in the context of language generation), like: Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rekabsaz, Carsten Eickhoff In proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)- - The reported improvement is marginal, while achieved with the large overhead of MC sampling. My guess is that the improvement is only due to the effect of ensembling, inherent in MC droupout. 
As mentioned above: - I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.
- More evidences regarding the performance improvement, showing that it is not only due to the effect of ensembling.
- Studying more efficient and recent Bayesian approaches, such as: Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR "
ARR_2022_32_review,"The paper proposes to use Monte Carlo dropout to introduce the notion of uncertainty in text summarization models, which is basically performing multiple forward passes with different dropout masks. By sampling multiple summaries from the models, the authors also propose the use of a ""predictive mean"" which is basically selecting the summary with the lowest disagreement, as the output summary. This method improves the performance of both BART and PEGASUS models. ","1. Trustworthy in abstractive text summarization is a very important topic. The method proposed in the paper would help evaluate whether or not to use and deploy summarization systems, and to introduce an ""unanswerability"" function when the system is unsure about the summary that it produced.
2. The proposed method also improves the performance, which is an added benefit to introduce uncertainty. ","1. Since the focus of the paper is model trustworthiness, it would be nice to have more analyses and less (or no) focus on the model performance. That is, it would be nice to have more model comparisons (aside from BART and PEGASUS and including state-of-the-art models), more datasets (different domains, multi-document vs single-document, etc.), and most importantly factuality evaluations.
2. The novelty of the proposed method is also very limited. This work is basically an application of the MC dropout to text summarization, and I find that there isn't a technical difficulty to apply such in text generation models. This also relates to the first weakness -- since the novelty is limited, readers would expect more analyses with regards to trustworthiness in summarization systems. Also, as stated in the paper, [1] already have used MC dropout in MT, which is also a text generation problem similar to text summarization. In fact, Section 3.2 is inspired from [1] and is only extended very marginally.
3. The paper mentions [2] as related work that focused on uncertainty for summarization. However, no comparison is made between this and the proposed method on the effectiveness to measure uncertainty. Moreover, Section 5.1 and Figure 1 do not evaluate the proposed method's effectiveness to measure uncertainty. There should be a baseline method, such as [2], and a metric that measure such effectiveness. Instead, Figure 1 just shows if the summarization systems are more certain than the other.
4. Finally, in Section 3.3, the ""predictive mean"", which is to select the summary with the lowest disagreement, seems to be very simple. Simple is not bad, but I think there can be smarter ways to ensemble multiple predictions from a summarization system. For example, the method used in [3] is one possible way. There should have been an exploration (or at least an ablation) to compare different possible techniques to do ""predictive mean"", especially since selecting a summary is far from taking a mean.
[1] Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers. BDL 2019 [2] Understanding Neural Abstractive Summarization Models via Uncertainty. EMNLP 2020.
[3] A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation. ACL 2016. ","Please mention which system/s are used to generate the summaries in Table 3 and 4. Also, it is interesting to see that summaries in Table 3 are very different from each other, while the same cannot be said for Table 4. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No ethical concerns ,"1. Since the focus of the paper is model trustworthiness, it would be nice to have more analyses and less (or no) focus on the model performance. That is, it would be nice to have more model comparisons (aside from BART and PEGASUS and including state-of-the-art models), more datasets (different domains, multi-document vs single-document, etc.), and most importantly factuality evaluations.
2. The novelty of the proposed method is also very limited. This work is basically an application of the MC dropout to text summarization, and I find that there isn't a technical difficulty to apply such in text generation models. This also relates to the first weakness -- since the novelty is limited, readers would expect more analyses with regards to trustworthiness in summarization systems. Also, as stated in the paper, [1] already have used MC dropout in MT, which is also a text generation problem similar to text summarization. In fact, Section 3.2 is inspired from [1] and is only extended very marginally.
3. The paper mentions [2] as related work that focused on uncertainty for summarization. However, no comparison is made between this and the proposed method on the effectiveness to measure uncertainty. Moreover, Section 5.1 and Figure 1 do not evaluate the proposed method's effectiveness to measure uncertainty. There should be a baseline method, such as [2], and a metric that measure such effectiveness. Instead, Figure 1 just shows if the summarization systems are more certain than the other.
4. Finally, in Section 3.3, the ""predictive mean"", which is to select the summary with the lowest disagreement, seems to be very simple. Simple is not bad, but I think there can be smarter ways to ensemble multiple predictions from a summarization system. For example, the method used in [3] is one possible way. There should have been an exploration (or at least an ablation) to compare different possible techniques to do ""predictive mean"", especially since selecting a summary is far from taking a mean.
[1] Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers. BDL 2019 [2] Understanding Neural Abstractive Summarization Models via Uncertainty. EMNLP 2020.
[3] A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation. ACL 2016. 
Please mention which system/s are used to generate the summaries in Table 3 and 4. Also, it is interesting to see that summaries in Table 3 are very different from each other, while the same cannot be said for Table 4. "
ARR_2022_253_review,"This paper study the learned embeddings' distribution from the CharacterBERT model using a new analysis tool (i.e., information axis). It argues that the currently used character sequence are limited and there are many distorted character sequences that are popular in speech analysis, vocal, pseudowords are not considered. In experiments,  the analysis and results based on the information axis demonstrate the embedding of deep models learned is far apart from the extant character sequences. ","The paper is enjoyable to read, which makes it easy to understand the main idea. The new analytical tool (information axis along with UMAP) extends the current analysis over extant words/subwords. The authors give a solid analysis and result on analyzing those ""garble"" (sub-), paralanguage, pseudowords and morphology of words. It is a good practice for ExplainAI that try to understand the learned character-sequence distribution in the high-dimensional space. ",no major weakness is observed. ,"- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
- It would be better to define some notations and give a clear definition of the ""information axis"", ""word concreteness"" and also ""Markov chain information content"".
- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"
- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
- It would be better to define some notations and give a clear definition of the ""information axis"", ""word concreteness"" and also ""Markov chain information content"".
- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE. "
ARR_2022_149_review,"The paper investigates the problem of detecting implicit offensive comments in dialogue, where multiple interpretative steps may be needed to uncover the offensive nature of the comment. Such interpretative steps may include, among others, the use of common sense knowledge or stereotypes. Along with describing the task and arguing for its importance, the paper presents a new dataset of implicit offensive comments (annotated with the ""reasoning steps"" required to go from the original implicit comment to its explicit equivalent). Finally, the paper shows how how entailment models can be exploited to determine which reasoning step is most likely at each stage. ","- The paper addresses an interesting and important question - It is also very clear, especially in its discussion of the task and its relation with previous work - It comes along with a new dataset ","The models described in the evaluation rely on a number of strong assumptions, as detailed by the authors in the discussion. Although entailment models can be applied to estimate the probability of a given step in the reasoning chain, the paper does not really propose a full model that would apply such steps one by one, using some type of decoding process. ",The authors have done a very good job at addressing the comments from the previous reviewing round. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",the ethical concerns are well covered in the paper ,"The models described in the evaluation rely on a number of strong assumptions, as detailed by the authors in the discussion. Although entailment models can be applied to estimate the probability of a given step in the reasoning chain, the paper does not really propose a full model that would apply such steps one by one, using some type of decoding process. 
The authors have done a very good job at addressing the comments from the previous reviewing round. "
ARR_2022_149_review,This paper propose a method to automatically identify offensive instances in natural language when the offensiveness is implicit. The method aims at constructing a chain of logical inference that links the original utterance to an offensive statement. ,"The proposal is novel and i aims at building a more transparent system, where the reasoning behind the final offensiveness label is actually explicitly defined as a sequence of steps. ","Several aspects of the proposal are not clear from how it is presented in this paper. The method is based on a highly rich and complex manually-built dataset, which appears to be hard to adapt or scale to other scenarios, domains, abusive phenomena, targets, languages, and so on. The actual effort in terms of time and money is not known, and a discussion on the scalability of the resource is needed.
The annotation task also seems to be particularly subjective, dealing with individual perception of what is offensive and what is supposedly common ground knowledge. While the authors' claim ""a person's identity or attributes have not played a critical role in existing OTD research"", there are actually recent works on this direction, e.g. (taken from the perspective data manifesto): https://arxiv.org/pdf/2112.04030.pdf https://ojs.aaai.org/index.php/HCOMP/article/view/7473/7260 https://aclanthology.org/2021.emnlp-main.822.pdf Another, more fundamental issue, that I'd like to see discussed is that reasoning in this work is treated as a seq2seq problem and approached by neutral networks. This feels like a step back with respect to the large literature on semantic parsing and logical-based inference. With the abundance of ontologies, knowledge graphs, and automatic reasoning tools, it is odd that none of them is even mentioned, while the task is reduced to a series of machine learning steps, hindering the transparence of the process. ","Eleven pages of Appendix are an indication of a great effort that went into this paper. Perhaps parts of it would be better positioned into the main text, in particular the annotators' guidelines. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"Several aspects of the proposal are not clear from how it is presented in this paper. The method is based on a highly rich and complex manually-built dataset, which appears to be hard to adapt or scale to other scenarios, domains, abusive phenomena, targets, languages, and so on. The actual effort in terms of time and money is not known, and a discussion on the scalability of the resource is needed.
The annotation task also seems to be particularly subjective, dealing with individual perception of what is offensive and what is supposedly common ground knowledge. While the authors' claim ""a person's identity or attributes have not played a critical role in existing OTD research"", there are actually recent works on this direction, e.g. (taken from the perspective data manifesto): https://arxiv.org/pdf/2112.04030.pdf https://ojs.aaai.org/index.php/HCOMP/article/view/7473/7260 https://aclanthology.org/2021.emnlp-main.822.pdf Another, more fundamental issue, that I'd like to see discussed is that reasoning in this work is treated as a seq2seq problem and approached by neutral networks. This feels like a step back with respect to the large literature on semantic parsing and logical-based inference. With the abundance of ontologies, knowledge graphs, and automatic reasoning tools, it is odd that none of them is even mentioned, while the task is reduced to a series of machine learning steps, hindering the transparence of the process. 
Eleven pages of Appendix are an indication of a great effort that went into this paper. Perhaps parts of it would be better positioned into the main text, in particular the annotators' guidelines. "
ARR_2022_149_review,"This paper formulates implicit offensive text detection (OTD) as a multi-hop reasoning problem that includes the chain of reasoning, generating a dataset that consists of explicit and implicit offensive texts, to demonstrate how the detection is difficult and should be handled.  In detail, the authors assume the circumstance where a personal attribute of reader or listener is pre-defined, attained from PERSON-CHAT corpus. Then, three kinds of statements, namely implicitly offensive, explicitly offensive, and non-offensive ones are created concerning the attribute. Afterwards, a reasoning chain that explains the flow from implicit to an explicitly offensive statement, is augmented. The construction phase is depicted in detail, along with several post-processing rules for chain generation.
Finally, the authors propose the dataset, Mh-RIOT, and show that the conventional pretrained language models with a simple classification strategy cannot well discern the implicit offensive text, exhibiting about 15% accuracy at maximum. This suggests the new challenge provided by this dataset, but seems to be influential as a future direction of OTD. It is also presented that KIR enhances the overall performance of SOTA OTD models, implying the importance of reasoning-based inference in OTD. ","- This paper well formulates the implicit OTD as a multi-hop reasoning problem, constructing a dataset to check the capability of reasoning of SOTA OTD models.
- The experiments are conducted to examine the use of entailment of models as a part of the multi-hop reasoning approach for implicit OTD, showing that such entailment helps the model find an appropriate reasoning chain to detect implicit offensive texts.
- The authors distribute the built corpus and knowledge base for future research. ","- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. ","Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349 ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,"I think it should be described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restrictions. ","- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. 
Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349 "
ARR_2022_84_review,This paper presents a contrastive learning framework for unsupervised sentence representation learning. The proposed framework can alleviate the sampling bias in the random negative example sampling in contrastive learning. It achieves the debiased sampling by 1) adding per-example noise-based negatives and iteratively improving them by maximizing the non-uniform objective; 2) adding a similarity based filtering (based on a separate model) to remove false negatives. Empirical evaluations show the the proposed method outperforms competitive baselines on 7 semantic textual similarity tasks. ,"- The proposed method is intuitive and achieves good results comparing to existing methods.
- The paper is well written and easy to follow.
- The proposed method also achieves good performance in the few-shot setting, which makes it more useful in real practice. ","One key motivation in the paper is to improve the quality of the sentence representations via improving uniformity. However, it is still a bit unclear to me if there is a strong correlation between uniformity and representation quality. Maybe we can perform some additional analyses to reveal this by varying negative proportion k? Note that Figure 6(b) seems to show that k does not have a high quality impact on some datasets. ","1. Figure 1: although it is very intuitive to use one random example to demonstrate the distribution of false negative examples, it would also be good to show the statistics over the whole dataset or a particular slice of the dataset. 
2. Line 414: it would be good to show the ratio of false negative examples that are filtered out. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"One key motivation in the paper is to improve the quality of the sentence representations via improving uniformity. However, it is still a bit unclear to me if there is a strong correlation between uniformity and representation quality. Maybe we can perform some additional analyses to reveal this by varying negative proportion k? Note that Figure 6(b) seems to show that k does not have a high quality impact on some datasets. 
1. Figure 1: although it is very intuitive to use one random example to demonstrate the distribution of false negative examples, it would also be good to show the statistics over the whole dataset or a particular slice of the dataset. 
2. Line 414: it would be good to show the ratio of false negative examples that are filtered out. "
ARR_2022_84_review,"**What is the task?**
Debiased contrastive learning framework for unsupervised sentence representation learning **What has been done before?**
Previous works (contrastive learning based baselines) mostly utilize in-batch negatives to learn the uniformity, but the randomly negative sampling strategy may lead to sampling bias, such as false negatives and anisotropy representations. Different from these methods, the proposed framework adopts an in-stance weighting method for punishing false negatives and a gradient-based algorithm for generating noise-based negatives towards the most nonuniform points. In this way, the influence of false negatives can be alleviated and the model can better learn the uniformity. It finally reduces the sampling bias and improves the model performance.
**What are the main contributions of the paper? How many of them are novel?**
- First attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.
- Presented a new framework DCLR to alleviate the influence of sampling bias.
- Experiments on 7 semantic textual similarity tasks show that our approach is more effective than competitive baselines on semantic textual similarity (STS) tasks using BERT and RoBERTa **What are the key techniques used to tackle this task?**
The core idea is to improve the random negative sampling strategy for alleviating the sampling bias problem.
- A noise-based negatives generation strategy to reduce the bias caused by the anisotropy PLM-derived representations - initialize new negatives based on a Gaussian distribution and iteratively update these negatives by non-uniformity maximization.
- An instance weighting method to reduce the bias caused by false negatives - similarity between original sentence and each negative **What are the main results? Are they significant?**
Experiments on 7 semantic textual similarity tasks show that our approach is more effective than competitive baselines on semantic textual similarity (STS) tasks using BERT and RoBERTa ","- First attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.
- Well-written and easy to read paper - Claims well-supported by ablation analysis and experimental results ",See comments below ,"- It is unclear which evaluation set(s) were used for Figure 3 - In section 6.1, it is unclear why and how negative sampling (DCLR) has to change. It seems DCLR is independent of the positive data augmentation strategy used.
- Why was STS-B and SICK-R chosen for figure 4-6 and not STS-avg ?
- How does DCLR compare with SimCSE on figure 5? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"
- It is unclear which evaluation set(s) were used for Figure 3 - In section 6.1, it is unclear why and how negative sampling (DCLR) has to change. It seems DCLR is independent of the positive data augmentation strategy used.
- Why was STS-B and SICK-R chosen for figure 4-6 and not STS-avg ?
- How does DCLR compare with SimCSE on figure 5? "
ARR_2022_84_review,"The paper tries to address a new and important problem: sampling bias in contrastive learning that improper negatives (e.g. false negatives and anisotropy representation) will hurt the uniformity of the representation space. To tackle this problem, they propose DCLR framework to alleviate the influence sampling bias. Specifically, they employ 1) weighting method to punish false negatives 2) generate noise-based negatives to guarantee the uniformity of the representation space. They demonstrated the effectiveness of their method on 7 semantic textual similarity tasks. Overall, the paper is well-written and the results are encouraging. However, I am not fully convinced of the way that the authors use to identify false negatives. The authors can also do a better job at explaining why optimizing equation (3) and (4) can lead the noise-based negatives into more non-uniform semantic space. ",The paper is overall well-written. The problem that the paper tries to address seems new and important. The authors  propose two methods to remediate the sampling bias issue which lead to encouraging results (main results Table 1) compared with previous approaches. ,"I am not fully convinced about the way the false negative samples are identified. In the paper, the authors proposed to use SimCSE as a complementary model and identify false negatives as negatives that have higher semantic similarity over a threshold using SimCSE representation, and punish those “false negatives” with a 0 weighting. I would argue that using similarity score alone could not identify false negatives. It only identify negatives that are very close to the original sentence semantically. In fact, those are “hard negatives” which has been shown in many contrastive works as essential in learning a high-quality representation. 
The authors can also do a better job at explaining why starting with a Gaussian negatives and optimizing equation (3) and (4) can lead to negatives sampled from non-uniform semantic space. Any previous work or evidence to support that? ","1) In Figure 1, what is the corpus that the authors use to sample sentence pairs? 
		2) In addition to STS tasks, the authors could also explore a number of downstream tasks as implemented in SentEval to evaluate the quality of sentence representations. 
		3) It would be good to add statical analysis to the results in Table 1 to show significance. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",n/a ,"I am not fully convinced about the way the false negative samples are identified. In the paper, the authors proposed to use SimCSE as a complementary model and identify false negatives as negatives that have higher semantic similarity over a threshold using SimCSE representation, and punish those “false negatives” with a 0 weighting. I would argue that using similarity score alone could not identify false negatives. It only identify negatives that are very close to the original sentence semantically. In fact, those are “hard negatives” which has been shown in many contrastive works as essential in learning a high-quality representation. 
The authors can also do a better job at explaining why starting with a Gaussian negatives and optimizing equation (3) and (4) can lead to negatives sampled from non-uniform semantic space. Any previous work or evidence to support that? 
1) In Figure 1, what is the corpus that the authors use to sample sentence pairs? 
		2) In addition to STS tasks, the authors could also explore a number of downstream tasks as implemented in SentEval to evaluate the quality of sentence representations. 
		3) It would be good to add statical analysis to the results in Table 1 to show significance. "
ARR_2022_332_review,"This paper introduces a new image analysis task aiming to identify the time and location of a given image. The authors demonstrate that solving the task requires extracting visual and textual information and searching in a knowledge base. The details of the data collecting process, and an analysis of the collected dataset is provided. The authors study human performance and the performances of several CLIP-based models and show that there is a large gap between machine performance and human performance. ","Both the task and the dataset are new. The task requires extracting and reasoning with visual and textual information in the input image, and it is a non-trivial task to collect the dataset. The large gap between human and machine performance suggests that this is a challenge task to solve. ","- The usefulness of the proposed task is not clearly justified. In the abstract it is briefly mentioned that the proposed task helps with subsequent tasks, but the authors did not provide further discussions and references. Also, it seems that time and location can only be inferred for certain types of images (e.g., news images), and therefore it is not clear how this can be a generally useful task.
- The authors demonstrated that the proposed task requires some reasoning capability to solve, but they only evaluated basic vision-language retrieval models that do not seem to have the corresponding reasoning capability. There is also not too much analysis of the experiment result, especially on the result that shows time prediction is completely off. ","- The authors need to provide a compelling argument about the practical value of the proposed task and demonstrate that incorporating the corresponding reasoning capability improves the model performance.
- It might be useful to provide the chance accuracy so that the readers can have a better idea of how well the baselines perform.
- Considering that the task can also be formulated as a classification problem, it might be useful to consider a standard softmax classifier. This will also give us an idea about how much performance one can get without transfer learning. Also, the time prediction seems to be completely off; the softmax classifier might be useful for diagnosing the problem. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The usefulness of the proposed task is not clearly justified. In the abstract it is briefly mentioned that the proposed task helps with subsequent tasks, but the authors did not provide further discussions and references. Also, it seems that time and location can only be inferred for certain types of images (e.g., news images), and therefore it is not clear how this can be a generally useful task.
- The authors demonstrated that the proposed task requires some reasoning capability to solve, but they only evaluated basic vision-language retrieval models that do not seem to have the corresponding reasoning capability. There is also not too much analysis of the experiment result, especially on the result that shows time prediction is completely off. 
- The authors need to provide a compelling argument about the practical value of the proposed task and demonstrate that incorporating the corresponding reasoning capability improves the model performance.
- It might be useful to provide the chance accuracy so that the readers can have a better idea of how well the baselines perform.
- Considering that the task can also be formulated as a classification problem, it might be useful to consider a standard softmax classifier. This will also give us an idea about how much performance one can get without transfer learning. Also, the time prediction seems to be completely off; the softmax classifier might be useful for diagnosing the problem. "
ARR_2022_332_review,"In this paper the authors introduce their TARA dataset, which is designed to enable high level reasoning from an image which needs open world knowledge grounded in the visual content. The dataset 16k images from the NYT articles and an additional 61k weakly supervised samples from the wikipedia dataset. The dataset is extremely challenging for the present state of the art models to solve with an extremely wide gap between human and mode performance. They present strong baseline models based on the recent CLIP model and extend it further to adapt it to the present task. An extensive annotation and data cleaning process is followed to ensure that the quality of the data is maintained while collecting a sufficient number of samples. ","The paper introduces a very novel and very relevant problem to the area and presents a dataset for the same. The problem of combining open real world knowledge to make inferences about the visual content in an image is something which can find a variety of uses in real life and is something that comes easily to humans, but is extremely hard for our present models to solve making it an interesting problem to enhance the capabilities of present day computer vision systems to take them closer to the goal of AGI. This dataset is a useful step in that direction.
The data cleaning and annotation process is very thorough and the process is very clearly described with the use of very relevant and self explanatory visuals making the paper easy to read and follow.  Strong baselines are proposed and evaluated against and a very relevant and useful evaluation metric of example-F1 is used to compare model performance while allowing for accounting for multiple different hierarchies in the labels. ","1. Some data annotations steps can possibly lead to some spurious labels like the use of geopy for location annotations can introduce annotation errors which could throw off models trained/evaluated on the dataset 2. example-F1 is an unweighted measure and hence weighs the prediction of the century and decade the same as month and date. Maybe using a weighted version of the same to penalize the harder to get labels could help make the evaluation more fair. 
3. Dataset size is a little limited for the use of the more recent and heavy multimodal models which might be needed to reason over a vast amount of open ended data and visual images 4. Simpler baselines like random guess, a simple guess based on a captioning model, more recent visual transformers for feature extraction etc can be added as more baselines ","1. Try using weighted example F1 to assign different weights to easier prediction labels like the century and harder ones like month and date. Similarly distinguish between location and time prediction  2. Try incorporating more diverse and larger data sources to expand the data beyond NYT and also enhance it in size 3. line 365: size larger than 50, should be defined on a reference image size since it could mean widely different things based on how big the image itself is. 
4. Add more baselines including simpler ones as reference ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Some data annotations steps can possibly lead to some spurious labels like the use of geopy for location annotations can introduce annotation errors which could throw off models trained/evaluated on the dataset 2. example-F1 is an unweighted measure and hence weighs the prediction of the century and decade the same as month and date. Maybe using a weighted version of the same to penalize the harder to get labels could help make the evaluation more fair. 
3. Dataset size is a little limited for the use of the more recent and heavy multimodal models which might be needed to reason over a vast amount of open ended data and visual images 4. Simpler baselines like random guess, a simple guess based on a captioning model, more recent visual transformers for feature extraction etc can be added as more baselines 
1. Try using weighted example F1 to assign different weights to easier prediction labels like the century and harder ones like month and date. Similarly distinguish between location and time prediction  2. Try incorporating more diverse and larger data sources to expand the data beyond NYT and also enhance it in size 3. line 365: size larger than 50, should be defined on a reference image size since it could mean widely different things based on how big the image itself is. 
4. Add more baselines including simpler ones as reference "
ARR_2022_146_review,"The paper proposes a pipeline for zero-shot data-to-text generation. 
The framework follows traditional data-to-text diagram. It contains 4 steps in general. 1) Template verbalization with  2) ordering module, then the 3) aggregation and 4) sentence compression. 
To enable training the pipeline model, the paper contributes a large dataset named Wikifluent corpus. The data-text pairs are collected from Wikipedia dump. It also applies some split-and-repharase and coreference models on corresponding texts.  The framework proposed in this paper is similar to rule-based data-to-text generation. The main difference of the proposed method compared to traditional methods, each module is trained using neural networks by leveraging recent pre-trained generation models like BART. The novelty of this paper is limited. The neural pipeline method not entirely new, some of previous papers combine important modules such as planning and ordering of traditional data-to-text generation into an end-to-end neural network. [ 1,2]. 
The paper show improvements of the proposed method on two public datasets with COPY baseline in automatic evaluation. In the manual evaluation, the paper fails to compare with existing baseline method, leaving the evaluation incomplete. The overall paper structure is messy. A lot of detailed model descriptions are described in experiments, which is quite confusing.  [1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. 
[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. ",1. The paper contributes a large scale data-to-text generation dataset WikiFluent.  2. The paper proposes a neural pipeline method leveraging recent pre-trained language models. ,"1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper. 
2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline. 
3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY). 
4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods. ","In sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching?  There are some missing references in this paper. 
[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. 
[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. 
[3] An architecture for data-to-text systems. Reiter et al., 2007.  Typos Line 409 we adopt BART-base for -> we adopt BART-base model for ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper. 
2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline. 
3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY). 
4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods. 
In sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching?  There are some missing references in this paper. 
[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. 
[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. 
[3] An architecture for data-to-text systems. Reiter et al., 2007.  Typos Line 409 we adopt BART-base for -> we adopt BART-base model for "
ARR_2022_146_review,"This paper focuses on data to text generation from RDF triples and attribute value pairs. The task involves generating a couple of sentences from a set of triples. As far as the traditional generation pipeline is concerned, it does not include any content selection or document planning. All the triples must be verbalized although their order must be determined and which ones should be grouped (aggregated together) to give rise to a single sentence. Most state of the art models are trained or fine-tuned on datasets developed specifically for this task. The authors argue that training on in-domain datasets leads to overfitting. They propose to render the RDF triples into natural language, aka facts, using domain-specific templates, and then develop general modules which perform the text rewriting operations (ordering, aggregation) required to render the clunky fact-based sentences into fluent text. The rewriting modules are trained on a synthetic corpus which the authors create from the English Wikipedia. ","- The paper is well written, and the proposed approach intuitive and conceptually simply.
- The Wikifluent corpus will be of use for other text-to-text tasks, such as simplification or even document compression.
- The results are rather encouraging for a zero-shot model.
- Evaluation is through, and includes ablations and experiments with automatic metrics and humans ","I am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all.  For this reason, I would have liked to see an assessment of how noisy these intermediate steps are. ","- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).  - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit).  -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results  - ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all.  For this reason, I would have liked to see an assessment of how noisy these intermediate steps are. 
- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).  - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit).  -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results  - "
ARR_2022_322_review,"This paper digs into the evaluation of Non-AutoRegressive machine translation model and reveals several flaws in standard evaluation methodology, to provide a better understanding of the merits and weakness of NAR model.
A more fair and detailed comparison shows that the speed gap between AR and NAR models quickly shrink in more realistic usage conditions (larger batch or multi-core CPUs), which is kind of against the common belief that NAR model is significantly faster. ","This paper shows a bigger picture in the speed evaluation of NAR models, with more settings and test-sets, we can get a better understanding of the merits and flaws of NAR models. I think it will have a positive impact on the research of NAR models.  And the experiment for comparison is solid and fair. ","My major concern of this paper is the innovativeness.  I think the contribution of the paper lies in more on the engineering side: results on more test-sets to avoid overfitting, speed comparison not only on 1-batch GPU setting but also on x-batch and multi-cpu settings, etc. However, while a lot of content is on engineering side, there is very few new findings on algorithm or model side: the CTC model is not new, and no new evaluation metrics is proposed. 
So I think the paper would be better to appear in related workshop, rather than a ACL long paper. ","1. In Table 3, the COMET score is added without enough explanation: why choose this metric and how it fits into this evaluation?   And with the observation of substantial difference in COMET and BLEU score, the author suggest that NAR models may rank poorly in human evaluation, why not add the human evaluation in the paper?  If it does rank poorly, it may reveal another important feature of NAR model. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"My major concern of this paper is the innovativeness.  I think the contribution of the paper lies in more on the engineering side: results on more test-sets to avoid overfitting, speed comparison not only on 1-batch GPU setting but also on x-batch and multi-cpu settings, etc. However, while a lot of content is on engineering side, there is very few new findings on algorithm or model side: the CTC model is not new, and no new evaluation metrics is proposed. 
So I think the paper would be better to appear in related workshop, rather than a ACL long paper. 
1. In Table 3, the COMET score is added without enough explanation: why choose this metric and how it fits into this evaluation?   And with the observation of substantial difference in COMET and BLEU score, the author suggest that NAR models may rank poorly in human evaluation, why not add the human evaluation in the paper?  If it does rank poorly, it may reveal another important feature of NAR model. "
ARR_2022_156_review,"The authors propose a new method for the KG-to-text generation task. Similar to earlier work, their method S-OSC leverages pre-trained LMs (PLMs) and models the generation task as a sequence-to-sequence task by linearizing the KG triples. However, unlike previous PLM based methods, S-OSC pays specific attention to two things: (1) order of triples in the linearized sequence should match the ordering in GT sentence (2) part-of-speech tags can help the model know when to copy and when to generate new words. By having additional models/training procedures for these sub-goals, S-OSC is able to outperform the state-of-the-art on 2 public KG-to-text datasets (WebNLG and DART). ","1. Through well-motivated and relatively simple modifications to the vanilla PLM-based KG-to-text models, the authors are able to achieve SOTA results on 2 large public datasets (WebNLG and DART). 
2. The authors perform extensive ablation studies and show how each of their modifications impacts model performance. Along with the use of multiple automated metrics (BLEU, ROUGE, METEOR), human evaluation is also carried out. ","1. The results on WebNLG seem only marginally better compared to the second best model by Li et al [1] (BLEU-4 61.88 -> 61.90, Chrf++ 79.1 -> 79.7, CIDEr score is lower). For DART, there seems to be a significant jump compared to the baselines; however, the model by Li et al [1] is absent from this comparison (Table 2). 
2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear.
[1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 ","1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template. 
2. The original numbers in [2] for BART-* and T5-* do not exactly match the ones reported in table 1. Why is that?
[2] Investigating Pretrained Language Models for Graph-to-Text Generation (https://github.com/UKPLab/plms-graph2text) ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The results on WebNLG seem only marginally better compared to the second best model by Li et al [1] (BLEU-4 61.88 -> 61.90, Chrf++ 79.1 -> 79.7, CIDEr score is lower). For DART, there seems to be a significant jump compared to the baselines; however, the model by Li et al [1] is absent from this comparison (Table 2). 
2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear.
[1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 
1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template. 
2. The original numbers in [2] for BART-* and T5-* do not exactly match the ones reported in table 1. Why is that?
[2] Investigating Pretrained Language Models for Graph-to-Text Generation (https://github.com/UKPLab/plms-graph2text) "
ARR_2022_94_review,"The authors present a strong method for unsupervised constituency parsing that relies on RoBERTa and constituent classification based on an initial set of constituents / distuents. Throughout training, this labeled set is updated with the models own predictions. Extensive results and analysis show the competitiveness of this approach with other unsupervised parsing methods. That being said, some details needed for reproducibility are needed, and claims about the initial set being a “minor” source of supervision are contradicted throughout the text.
To clarify the last part above, the issue may be that “minor” is underspecified. Clearly their supervision is relatively easy to acquire, although it appears their approach is sensitive to choice of initial labeled set. ","- Simple yet effective method for unsupervised constituency parsing that is EM-like. Competitive with existing models.
- Provides additional value through interesting ablations and extensive analysis. ","- It is not clear what effect the initial set of candidate constituents and distuents has on final performance. Multiple times the authors claim this form of supervision is “minimal”, and although it is certainly simple to use, perhaps different initial sets would yield different results. Based on Appendix A.1 it does seem like several ways of exploring initial set were explored, although no results reported. There are some results with this respect in section 5.3 showing that left-branching and random strategies to build initial seed set yield very low performance, and authors also state “the manner in which we perform the initial classification has a strong impact on the final tree structure” here. It would greatly help to be more consistent about the effect of the initial seed set throughout the text.
- It’s not clear how h_in(i, j) and h_out(i, j) are computed. For h_in, two likely possibilities are: a) RoBERTa is run over only the tokens i:j and the last vector is used to represent the phrase, or b) RoBERTa is run over an entire sentence x, and two or more output vectors are concatenated to get phrase vectors. If (a) is used, this seems clean although a bit expensive. If (b) is used, it seems like this would have trouble distinguishing between inside and outside. In either case, the text should be more clear about how this is done otherwise reproducibility would be quite difficult. ","- Do you think that validation using performance on the dev set (using early stopping or hyperparam selection) would be helpful? There are many reasons why people are interested in methods that could benefit from validation (even for unsupervised learning) because it can help find a model with a good inductive bias that does not overfit to given labels. I suppose by reporting three settings (inside, inside w/ self-training, inside + outside) you are doing a type of validation over hyperparams.  - Do you think your method of seed bootstrapping would be generally applicable to other types of unsupervised parsing models?
- What does it mean to re-normalize class probability scores in Fig 3?
- How much does the set of labels grow each epoch?
- How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- It is not clear what effect the initial set of candidate constituents and distuents has on final performance. Multiple times the authors claim this form of supervision is “minimal”, and although it is certainly simple to use, perhaps different initial sets would yield different results. Based on Appendix A.1 it does seem like several ways of exploring initial set were explored, although no results reported. There are some results with this respect in section 5.3 showing that left-branching and random strategies to build initial seed set yield very low performance, and authors also state “the manner in which we perform the initial classification has a strong impact on the final tree structure” here. It would greatly help to be more consistent about the effect of the initial seed set throughout the text.
- It’s not clear how h_in(i, j) and h_out(i, j) are computed. For h_in, two likely possibilities are: a) RoBERTa is run over only the tokens i:j and the last vector is used to represent the phrase, or b) RoBERTa is run over an entire sentence x, and two or more output vectors are concatenated to get phrase vectors. If (a) is used, this seems clean although a bit expensive. If (b) is used, it seems like this would have trouble distinguishing between inside and outside. In either case, the text should be more clear about how this is done otherwise reproducibility would be quite difficult. 
- Do you think that validation using performance on the dev set (using early stopping or hyperparam selection) would be helpful? There are many reasons why people are interested in methods that could benefit from validation (even for unsupervised learning) because it can help find a model with a good inductive bias that does not overfit to given labels. I suppose by reporting three settings (inside, inside w/ self-training, inside + outside) you are doing a type of validation over hyperparams.  - Do you think your method of seed bootstrapping would be generally applicable to other types of unsupervised parsing models?
- What does it mean to re-normalize class probability scores in Fig 3?
- How much does the set of labels grow each epoch?
- How did you choose min/max threshold values, and what are these values? Also, how did you decide 1:10 ratio of constituents to distituents? "
ARR_2022_159_review,"This paper dealt with a interchange intervention training with a distillation technique. They propose to train a student model to become a causal abstraction of a teacher model, which is faithful with a simpler causal structure. Their experimental results show that the proposed method improved some tasks compared with a standard distillation method. ","+ Distillation method with a causal abstraction is novel approach. 
+ They experimented the proposed method with various types of tasks. ","+ They did not show the detailed method (of distillation training procedure) in the paper. They should include them in main text not in appendix. 
+ They should explain why the proposed model can deal with a causal abstraction. At least they should show some samples and analyze them. 
+ They should compare among the proposed methods and analyze these results. ","+ Performance of each teacher model should be shown in Table 1. 
+ They can use the following paper: Knowledge Distillation from Internal Representations [Aguilar, AAAI20] ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"+ They did not show the detailed method (of distillation training procedure) in the paper. They should include them in main text not in appendix. 
+ They should explain why the proposed model can deal with a causal abstraction. At least they should show some samples and analyze them. 
+ They should compare among the proposed methods and analyze these results. 
+ Performance of each teacher model should be shown in Table 1. 
+ They can use the following paper: Knowledge Distillation from Internal Representations [Aguilar, AAAI20] "
ARR_2022_297_review,"The paper creates a corpus of verbal negations and adapts QA-SRL to collect questions and answers regarding the arguments of the affirmative counterpart of a negated predicate, and manipulate them to generate the affirmative interpretation. They use the curated dataset to pose two challenges - classifying entailment and neutral relationship between the negation and its affirmative counterpart, and generating the affirmative interpretation directly. Experiments show that current models are unable to perform reasonably on the generation task, which is the more realistic setting. ","The authors adapt a question answering approach to create an annotation scheme to create a corpus of verbal negations and their affirmative interpretations. The paper describes the full annotation process clearly, and the authors are careful to manually verify significant random samples of the corpus. The annotation process is designed to guide annotators to fill in template questions before answering them factually. These answers are used to create the affirmative interpretations. Further, they only retain the records where annotators are confident about their judgment. The authors perform well-designed experiments to establish benchmarks for the tasks they propose on their corpus. These experiments show that standard models are unable to perform the generation task as well as humans do. ","1. While it is fair to say that two annotators might have different answers to the same question and both might be correct, it would be better to verify that the answers provided are all valid. The authors manually validate a small subset of the dataset but, for a high quality dataset, it would be better to validate all of it. 
2. The paper should include automatic metrics for the generation task. While the metrics have their own problems, it would be a good way to compare systems without expensive human evaluation. ","1. It would be good if you expand on the importance of the order of wh-words used for question generation. 
2. Line 112: Consider the ‘second’ example actually refers to the first example in Figure 1. 
3. I agree with the authors about the framing of the classification task, that it isn’t a realistic one. Maybe the paper would be better without it. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. While it is fair to say that two annotators might have different answers to the same question and both might be correct, it would be better to verify that the answers provided are all valid. The authors manually validate a small subset of the dataset but, for a high quality dataset, it would be better to validate all of it. 
2. The paper should include automatic metrics for the generation task. While the metrics have their own problems, it would be a good way to compare systems without expensive human evaluation. 
1. It would be good if you expand on the importance of the order of wh-words used for question generation. 
2. Line 112: Consider the ‘second’ example actually refers to the first example in Figure 1. 
3. I agree with the authors about the framing of the classification task, that it isn’t a realistic one. Maybe the paper would be better without it. "
ARR_2022_182_review,"This paper addresses the problem of how to weight task-specific losses in multi-task learning and presents an algorithm to adapt weights of loss functions, each of which corresponds to a task, during training.
The key idea is to update loss weights so that the weighted sum of task-specific gradients moves model parameters to the direction which reduces all task-specific losses computed on held-out data.
The proposed algorithm is expected to (a) avoid undesirable task performance trade-offs (i.e, sacrificing performance on one task to improve on another task) and (b) facilitate better generalization.
The experiments showed that the proposed method achieved meaningful improvements in most of the tested individual tasks, and also achieved significantly higher performances on average. ","1. The proposed method directly and naturally addresses the trade-off problem of multi-task learning. The design motivation is easy to understand thus can facilitate  further, deeper research easily.
2. The presented experiments clearly demonstrated the performance superiority in two different types of text classification tasks. Thus it is arguable that the proposed method will have broader applications. ","1. The experimental settings may be in favor of the proposed method and the actual performance gap in the averaged loss may not be as significant as presented in the paper. The loss function of the “Uniform” baseline uses a constant weight across tasks, which matches the weights used in the evaluation of average performance. Thus I expect that the baseline’s performance is not very much behind the other methods including the proposed method *for the averaged score*. However Figure 2 and 3 shows it performed poorly. This may suggest that the baseline, and other baselines,  might overfit to the training data. There is no mention of the criteria to stop training so I cannot tell whether the authors did early stopping using held-out data. Since the proposed method uses a held-out dataset (called “query” in the paper) to control training, I think it is fair to use such a dataset to control training of baseline methods.
2. The paper doesn’t provide insights about the adapted weights except they looked to keep adapted during training. I think it is of general interest whether the proposed method actually controlled task weights nicely and/or the other baseline methods assigned task weights poorly and resulted in worse performances and due to that. The scaling of Figure 8 and 9 makes it hard to compare weights between compared methods, especially MGDA and GradNorm, because these absolute values are much smaller than others and see *relative magnitudes” between tasks. ( For task weighting, absolute magnitudes don’t matter.) I think the paper doesn’t provide strong evidence that the observed good performances were actually due to better task weighting.
3. Related to #2, no reason is provided to support why the baselines were chosen to compare. The readers who follow the technical area closely might understand how informative contrasting the proposed method and the baselines are, but for others like me, they look like random choices. Explanations are required to describe why they were chosen. ","1. The theoretical analysis doesn’t provide much insight beyond that a loss computed on a held-out dataset (= the query set) is a better estimator of the expected loss than that computed on the training data, which is well known. It could be fully removed to make spaces for other discussions.
2. Scatter plots in Figure 2 and 3 are not very suitable because it is hard to see how each run is distributed when they overlap. Box plots like in Figure 4 and 5 would be better. ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The experimental settings may be in favor of the proposed method and the actual performance gap in the averaged loss may not be as significant as presented in the paper. The loss function of the “Uniform” baseline uses a constant weight across tasks, which matches the weights used in the evaluation of average performance. Thus I expect that the baseline’s performance is not very much behind the other methods including the proposed method *for the averaged score*. However Figure 2 and 3 shows it performed poorly. This may suggest that the baseline, and other baselines,  might overfit to the training data. There is no mention of the criteria to stop training so I cannot tell whether the authors did early stopping using held-out data. Since the proposed method uses a held-out dataset (called “query” in the paper) to control training, I think it is fair to use such a dataset to control training of baseline methods.
2. The paper doesn’t provide insights about the adapted weights except they looked to keep adapted during training. I think it is of general interest whether the proposed method actually controlled task weights nicely and/or the other baseline methods assigned task weights poorly and resulted in worse performances and due to that. The scaling of Figure 8 and 9 makes it hard to compare weights between compared methods, especially MGDA and GradNorm, because these absolute values are much smaller than others and see *relative magnitudes” between tasks. ( For task weighting, absolute magnitudes don’t matter.) I think the paper doesn’t provide strong evidence that the observed good performances were actually due to better task weighting.
3. Related to #2, no reason is provided to support why the baselines were chosen to compare. The readers who follow the technical area closely might understand how informative contrasting the proposed method and the baselines are, but for others like me, they look like random choices. Explanations are required to describe why they were chosen. 
1. The theoretical analysis doesn’t provide much insight beyond that a loss computed on a held-out dataset (= the query set) is a better estimator of the expected loss than that computed on the training data, which is well known. It could be fully removed to make spaces for other discussions.
2. Scatter plots in Figure 2 and 3 are not very suitable because it is hard to see how each run is distributed when they overlap. Box plots like in Figure 4 and 5 would be better. "
ARR_2022_298_review,"This paper proposes an approach for few-shot text classification. The authors propose a Siamese Network to encode the input text and a textual representation of a label. Once encoded, the cosine similarity of the two encodings is computed and it's used in a Textual Entailment like task to decide the label of the input. The main contribution of the paper is in the Label Tuning technique, which allows to 1) re-use the architecture for multiple tasks and ii) to save computational cost at inference time. ","The paper is in a well-know area in Computational Linguistics which is gaining attention in the past years. The paper introduces an interesting perspective on the usage of Cross Attention models for few-shot settings, demonstrating that a much simpler (and faster approach) can achieve a comparable accuracy in many different tasks/datasets. The authors show multiple results in English and multi-lingual settings that seems convincing. ",The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section). ,"- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.
- Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention ""no significant"" difference, which instead is referring to the statistical significance. I would suggest to revise this part.
- It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?
- On which data is fine-tuned the model for the ""Knowledge Distillation"" - Please, add an intro paragraph to section 4.
- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?
- In line 303 the authors mention ""sentence transformers"". Why are the authors mentioning this? Is it possible to add a citation?
- There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.
- I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? 
- ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section). 
- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.
- Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention ""no significant"" difference, which instead is referring to the statistical significance. I would suggest to revise this part.
- It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?
- On which data is fine-tuned the model for the ""Knowledge Distillation"" - Please, add an intro paragraph to section 4.
- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?
- In line 303 the authors mention ""sentence transformers"". Why are the authors mentioning this? Is it possible to add a citation?
- There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.
- I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? 
- "
ARR_2022_298_review,"This paper presents new methods for zero/few-shot classification focusing on improving inference and training time costs. Few-shot classification is a challenging setting where very little training data is available (5-100 examples per label). Recent work has seen that entailment models are effective at zero/few-shot classification, where the input sentence is used as the premise and label strings are used as hypothesis. Inference from these types of approaches typically need a number of forward passes (equal to the number of labels) through a large pretrained model, which involves an expensive cross-attention step between the input sentence and hypothesis.
The first method proposed in the paper uses a dual-encoder / Siamese network to separately encode the premise and hypothesis. This is much more efficient at inference, since label vectors can be cached and only the input sentences need to be encoded once (rather than as many times as the number of labels). The paper further proposes ""label tuning"", where the premise encoder is kept fixed, and only the label embeddings are tuned. This helps significantly reduce training time cost, and allows deployment of a single shared premise encoder for input sentences across various different tasks (saving memory costs).
The authors validate their approach on several classification datasets in English, German and Spanish. Siamese Networks perform similar to their cross-attention counter-parts. Label tuning is not as effective, but a lot of performance can be recovered using knowledge distillation. ","1. Zero/few-shot classification is a very important and practically relevant problem. This paper is well-grounded into state-of-the-art research in few-shot learning and has an excellent literature survey.
2. The paper identifies an important limitation of current few-shot methods using entailment approaches (slow inference time), and does a thorough analysis comparing the proposed more efficient Siamese networks with the expensive cross-attention models, finding little difference in performance. Moreover, the authors have an interesting finding that simply using the label strings themsleves (instead of hand-crafted prompts / strings) are equally effective.
3. The authors further improve training/inference time time/memory efficiency using label tuning, where the input encoder parameters are kept fixed, and only label vectors are finetuned. Since this leads to a performance drop compared to approaches which fine-tune the encoder network, the authors propose using knowledge distillation from the fine-tuned model to recover part of the performance.
4. The paper has several experiments in both English and multilingual settings. A total of 16 datasets are studied, and results tables comprehensively compare different variants of the models. The paper also has good analysis experiments studying inference time speed, performance by token length, the effect of negation markers. ","No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work. ","1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently. 
2. Any reason you did not consider the other GLUE / SUPERGLUE tasks? 
3. I think you should rename ""null hypothesis"" to something else, since ""null hypothesis"" could have a different meaning and is not specific to label verbalization. 
4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work. 
1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently. 
2. Any reason you did not consider the other GLUE / SUPERGLUE tasks? 
3. I think you should rename ""null hypothesis"" to something else, since ""null hypothesis"" could have a different meaning and is not specific to label verbalization. 
4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph. "
ARR_2022_40_review,"This paper describes a self-generation framework for training ODD and TOD systems, as well as the prediction of the transition turns from ODD to TOD turns. The proposed framework makes use of self-chatting approaches using a SotA system, as well as fine-tuned versions of the chatbot to play the role of a salesman and a user. In addition, an interesting approach for predicting the best turn where to insert the transition and use of zero-shot approaches for intent detection complete the work done by the authors. Finally, human evaluations show the feasibility of the proposed approach. ",- The generated dataset is interesting for people working on dialogue evaluation - The proposed framework for self-generating the dataset is very interesting and the results are feasible - The proposed method for intent detection and transition turns are also correct and interesting - The paper is well written and clear to follow. Experimentation and human evaluation is correct ,"- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.
- Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset.  - Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results - Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses.  - It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses  - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?
- Line 216: How many paraphrases were created for each question, and what was their quality rate?
- Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?
- Line 254-257: How many templates were manually created?  - Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear)  - Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses - Line 324: how the repeated dialogues are detected?  - Line 356: how and how many sentences are finally selected from the 120 generated sentences?
- Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones? ","- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker) ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.
- Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset.  - Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results - Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses.  - It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses  - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?
- Line 216: How many paraphrases were created for each question, and what was their quality rate?
- Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?
- Line 254-257: How many templates were manually created?  - Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear)  - Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses - Line 324: how the repeated dialogues are detected?  - Line 356: how and how many sentences are finally selected from the 120 generated sentences?
- Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones? 
- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker) "
ARR_2022_40_review,The paper proposes a novel and automatic way of transitioning from chit-chat to task-oriented (TOD) mode. They leverate BlenderBot to automatically generate the dialogs with transitions and evaluare their approach using human evaluation.  They automate the process using a series of ML modeling - 1) QA based intent detection for automatically transitioning from chit-chat to task-oriented mode 2) T5 model to generate the transition turn 3) BlenderBot model to simulate both chit-chat turns and TOD turns. ,"- The proposed dataset could be useful for the broader research dialog research community since there is a dearth of datasets blending chit-chat and task-oriented dialogs.  - The paper is easy to follow and well-written.
- The method seems general enough to be applicable to other task-oriented datasets with intent annotations. ","- More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?
- Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?
- The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). ","During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?
- Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?
- The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). 
During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ? "
ARR_2022_40_review,"This paper focuses on transitioning from chit to task-oriented dialogues. This is important for triggering business opportunities.
Specifically, the authors propose a framework to automatically generate dialogues, which start from open-domain social chatting and then gradually transit to task-oriented dialogs.
The human evaluation shows that the automatically generated dialogues have a reasonable quality with natural conversation ﬂows from a business point of view. ","1. Combining Chit-Chat and Task-Oriented Dialogues is a less studied direction. 
2. Generating dialogs automatically can be useful in the industry. ","1. From the perspective of research, since the released dataset is automatically generated without further manual revision or annotation, it is hard to say that this work proposes a new research task. Furthermore, the two difficult problems (implicit intent detection and transition utterance generation) deserve a more closer study. 
2. From the perspective of models, the proposed method consists of a list of sub-models. The whole process is too trivial. ","The authors target an important problem that tries to combine the open-domain dialog and task-oriented dialog. Specifically, they provide a  clear objective for this combination, i.e. triggering the business opportunities. There are at least three basic sub-problems. 
(1) How to capture the interaction between the two types of dialogs? 
(2) How to determine the turning point? 
(3) How to transit properly?
All of these sub-problems need to be treated more systematically and carefully.  Other suggestions. 
(1) The introduction is a little wordy and sometimes confusing. For example, in line 78, ""the conversation starts without any speciﬁc goal"". This is confusing, since if a user has no specific goal, why does he or she chat with a salesperson? 
(2) This paper is highly related to dialog recommendation. In line 505-512, the authors claim ""such systems is to only make entity recommendations instead of tasks..."". From my point of view, there is considerable overlap between ""make entity recommendations"" and ""transferring from chit-chat to task-oriented dialogues and completing a task the user may want"". Specifically, in Liu 2020, they have also combined chitchat and task-oriented dialog, and achieved chit to task-oriented transition. Then, the only difference lies is complete the task, which is not the focus of this paper. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. From the perspective of research, since the released dataset is automatically generated without further manual revision or annotation, it is hard to say that this work proposes a new research task. Furthermore, the two difficult problems (implicit intent detection and transition utterance generation) deserve a more closer study. 
2. From the perspective of models, the proposed method consists of a list of sub-models. The whole process is too trivial. 
The authors target an important problem that tries to combine the open-domain dialog and task-oriented dialog. Specifically, they provide a  clear objective for this combination, i.e. triggering the business opportunities. There are at least three basic sub-problems. 
(1) How to capture the interaction between the two types of dialogs? 
(2) How to determine the turning point? 
(3) How to transit properly?
All of these sub-problems need to be treated more systematically and carefully.  Other suggestions. 
(1) The introduction is a little wordy and sometimes confusing. For example, in line 78, ""the conversation starts without any speciﬁc goal"". This is confusing, since if a user has no specific goal, why does he or she chat with a salesperson? 
(2) This paper is highly related to dialog recommendation. In line 505-512, the authors claim ""such systems is to only make entity recommendations instead of tasks..."". From my point of view, there is considerable overlap between ""make entity recommendations"" and ""transferring from chit-chat to task-oriented dialogues and completing a task the user may want"". Specifically, in Liu 2020, they have also combined chitchat and task-oriented dialog, and achieved chit to task-oriented transition. Then, the only difference lies is complete the task, which is not the focus of this paper. "
ARR_2022_221_review,"In an effort to advance research for question answering and natural language generation on tables, the authors introduce a new dataset HiTab (Hierarchical Tables), a collection of 3,597 complex tables (w/ several levels of headers for either rows or columns) from 25+ domains with manually-derived fine-grained annotations of quantity and entity alignments between the table cells and the natural language text descriptions that accompany the tables (in their original reports). Entity alignments map entities mentioned in text to corresponding row/column headers/subheaders and quantities are mapped to with single-cell quantities or to multiple cell via an operator that should be applied to the multiple cell values (e.g., average, difference, etc.). Moreover, QA pairs are created from these source declarative sentences and are included in the dataset. ","- the dataset is compared to similar datasets - in addition to creating the dataset, baseline performance is established using existing models as-is or adapted for the new dataset; along with some error analysis.
- for Table QA, the means of representing the table's header hierarchy in a logical-form is novel.
- data and software made available to the research community ","- while this dataset format/type is not mainstream, it is relevant for Table QA and NLG tasks ","The paper is well-written. I like the use of the initial example throughout the paper. The order of the Tables/Figures shown in the Appendices is somewhat confusing, having to jump back and forth across pages (although this may be due to their size and latex's automatic layout). Appendix B.3 is not referenced in the main paper.
Please revisit the following sentences for readability: line 137, 301, 498, 550 For Table 1, where examples are shown, why is the % not aligned as well for the 1st example (to E3/G3) while is is aligned for the 3rd (where proportion aligns to E3, shouldn't this be G3)? ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"- while this dataset format/type is not mainstream, it is relevant for Table QA and NLG tasks 
The paper is well-written. I like the use of the initial example throughout the paper. The order of the Tables/Figures shown in the Appendices is somewhat confusing, having to jump back and forth across pages (although this may be due to their size and latex's automatic layout). Appendix B.3 is not referenced in the main paper.
Please revisit the following sentences for readability: line 137, 301, 498, 550 For Table 1, where examples are shown, why is the % not aligned as well for the 1st example (to E3/G3) while is is aligned for the 3rd (where proportion aligns to E3, shouldn't this be G3)? "
ARR_2022_221_review,"This paper presents a new dataset namely HiTab with 3,597 hierarchical tables and 10,686 questions. They construct this dataset by collecting a wealth of statistical reports and Wikipedia pages, and then asking human annotators to convert sentence descriptions into question-answer pairs. Nearly all tables in the dataset are hierarchical, with fine-grained annotations of quantity and entity alignment. They evaluate two tasks on this dataset: table QA and natural language generation. They show that complex hierarchical structures increase the difficulty of both table QA and text generation. ","In general, I think there are some unique features to make this HiTab dataset valuable to the NLP community, including 1) this is the first dataset that focuses on hierarchical tables, 2) it contains a detailed annotation of entity and quantity alignment for each table-question pair, and 3) instead of crowd-sourcing, HiTab obtain the questions from real sentence descriptions of each table. This avoids annotation artifacts and bias to some extent.  There are some other strengths as follows:  - Good efforts are spent to build this dataset, with 18 annotators and 2400 working hours. Besides the hierarchical tables,  - Automatic Evaluation (Section 2.4) and Human Inspections (Section 2.5) are conducted in the dataset construction process to ensure the data quality.  - The details of dataset construction are well-presented in the paper and the Appendix. The authors also provide a good comparison with existing Table QA datasets.  - The dataset and codes are provided alongside with the paper. I randomly checked some data samples, and they are in high quality. ","It is still not quite obvious to me what additional challenges are posed by the hierarchical table structure in table QA. Although the authors mentioned three challenges in the introduction, it seems that they are not well-reflected in the experiments. I think Appendix B.4 is a good example to empirically show hierarchical structures indeed increases the difficulty of table QA. I suggest the authors put this part into the main paper, and I expect to see more results and analysis towards this direction to reveal how SOTA QA models fail in facing with complex hierarchical structures. ","I am not an expert of table QA. But to me, the authors seem to miss a straightforward baseline, that is to firstly flat the table into a text sequence like Figure 7 and then train a text-based QA model on it. For example, we can fine-tune the pretrained QA models like SpanBERT for table QA after flatting the table into a text sequence. In this case, we might be able to learn the hierarchical information through the self-attention mechanism built in Transformers. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"It is still not quite obvious to me what additional challenges are posed by the hierarchical table structure in table QA. Although the authors mentioned three challenges in the introduction, it seems that they are not well-reflected in the experiments. I think Appendix B.4 is a good example to empirically show hierarchical structures indeed increases the difficulty of table QA. I suggest the authors put this part into the main paper, and I expect to see more results and analysis towards this direction to reveal how SOTA QA models fail in facing with complex hierarchical structures. 
I am not an expert of table QA. But to me, the authors seem to miss a straightforward baseline, that is to firstly flat the table into a text sequence like Figure 7 and then train a text-based QA model on it. For example, we can fine-tune the pretrained QA models like SpanBERT for table QA after flatting the table into a text sequence. In this case, we might be able to learn the hierarchical information through the self-attention mechanism built in Transformers. "
ARR_2022_340_review,"This paper describes an advancement in counterfactual probes that test whether neural nets make use syntactic information. The goal of counterfactual probes is show that models actually use syntactic info rather than just encoding it. The paper identifies a potential problem in previous work (summarized neatly in Fig. 1), namely that information may be encoded redundantly, and the probe may attend to the wrong redunant representation relative to the model itself, thus underestimating the presence/utility of the information. They solve this by introducing a dropout probe. ","The paper is overall clear in identifying the problem, though I wish they had described what a counterfactual probe was more clearly and earlier than they did (2.2), and I believe from the paper that they have solved the particular problem that they set out to: the issue of probes missing redundantly encoded information. ","My main concern is that they haven't quite demonstrated enough to validate the claim that these are demonstrating a causal role for syntactic knowledge. Two crticisms in particular: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion.  2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to ""cheat"" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure.
Finally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information.
Really, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. ","I got garden pathed reading line 331 ""One could define other Z1 and Z2..."" The author seems to really like the bigram ""prior art."" ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"My main concern is that they haven't quite demonstrated enough to validate the claim that these are demonstrating a causal role for syntactic knowledge. Two crticisms in particular: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion.  2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to ""cheat"" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure.
Finally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information.
Really, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. 
I got garden pathed reading line 331 ""One could define other Z1 and Z2..."" The author seems to really like the bigram ""prior art."" "
ARR_2022_340_review,"The authors entertain the hypothesis that syntactic information is redundantly encoded in neural language models, which has two important implications for probing and causal analysis: 1. Probes may arbitrarily choose one place a syntactic property is encoded, and the model may choose another. 
2. Changing a representation based on a probe may therefore only partially change whether the property is encoded in the model, and might not influence the model decision, even if the model is relying on syntax.
For contributions, the authors show first that syntactic information is redundantly encoded in language models, based on estimates of mutual information between different parts of the embeddings. They then propose a simple method to make probing more robustly rely on all the information in the representation, by adding dropout to the probe. They show that this leads to altered representations that are more influential on changing the model's behavior. ","1. Well-written and clear hypothesis: redundant encoding of syntactic information poses a problem for drawing conclusions about whether models rely on the syntactic information encoded by probes.
2. The authors show that different parts of the hidden representations in different trained networks redundantly encode information correlated with syntax.
3. The dropout method for addressing this issue is both simple and motivated, which makes it appealing.
4. The experimental results, in the case of the QA models, tell a different story than past work that suffered from the redundancy vulnerability. Specifically, this work finds evidence that QA models do rely on syntactic information, while past work suggested they didn't. In the case of NLI, this work agrees with past works in finding a negative result. ","1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.
2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.
3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. ","It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?
417: why is “as” included along with “were” and “are”?
295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within” ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.
2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.
3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. 
It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?
417: why is “as” included along with “were” and “are”?
295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within” "
ARR_2022_179_review,"This short paper presents an alternative to label smoothing for neural MT.  In one variant (""weighted""), the amount of training-time probability assigned to every vocab item except the correct one at each time step in the decoder is varied based on whether the item appears in the source-side vocabulary, target-side vocabulary, or both.  The more fully explored variant (""masked"") just sets the source-side vocabulary weight to 0 and distributes the smoothed weight evenly over the vocabulary items in the other two classes.  Both variants of label smoothing are motivated by a noticed drop in BLEU scores whenever label smoothing is done with source/target vocabulary sharing also turned on.  Label smoothing and vocabulary sharing each improve scores over the Transformer-base, but using the two together hurts performance.
Masked label smoothing (with vocab sharing) is tested against Transformer-base, vocab sharing alone, label smoothing alone, and vocab sharing + label smoothing together.  Evaluation is mainly by BLEU score on a variety of data sets (IWSLT, WMT, CASIA) and language pairs.  The score differences are not large -- I think never more than 0.5 BLEU) -- but the authors report that using masked label smoothing statistically outperforms regular label smoothing when vocab selection is also turned on.
The short paper fits in four pages as required.  There is a one-page appendix.  There's also a small software package with a README as part of the uploaded submission -- but it's not mentioned in the text and so I almost missed it. ","- Focused, straightforward idea with a clear intuitive and practical motivation.  I think people will be interested in this paper's observations and proposed solution.  I expect that the details necessary for re-implementation are relatively clear.
- Evaluation on several different language pairs, which includes some diversity of character set / writing system.  Both into and out of English is covered. ","- The use of the appendix is a little more integral than I think would be ideal.  You have to read the appendix in order to know the conclusions of some analysis; the main text only says that such-and-such ""matters,"" but now how.
- It's not always clear which results are meaningful.  Some rows of the main table have statistical significance tests, but other metrics, analysis, and data points do not.  Since the score differences are overall quite small, this detracts from the reliability of the results.  Readers might be inclined to decide that the techniques aren't worth trying. ","SUGGESTIONS - It's great to have at least some statistical significance tests.  Since they were added only after the first revision of the paper, I felt like they weren't fully integrated into the main text or all of the results tables (like Table 5).  For example, the beginning of Section 5.2 (line 201) says that LS ""outperforms"" VS+LS... but we do not have any significance tests between these two configurations.  These differences of 0.1 BLEU may actually be ties, in which case ""outperforms"" is incorrect.
- In Table 3(b), is it really the case that 23.19 BLEU for RO-EN is statistically better than 23.15, or is this a typo?  I've never seen significance at p < 0.01 for such a tiny score change.
- Even without a formal significance test, can we have some intuition about this ECE score in Section 6.1 / Table 4?  How is this score computed and what properties of MT training does it reflect?  I've never heard of this metric before, so I don't really understand what it is or whether an improvement of 0.7 is meaningful.  (Whereas I know quite well how much intuitive importance to give 0.7 BLEU.)
- In Table 5, I would think that one intuitive configuration is to devalue the source vocabulary without shutting it off completely, via parameter settings like 0.5-0.5-0.25.  If your original motivation is correct, I would expect a configuration like that to do better than your current best result, 0.5-0-0.5.  Doesn't the current result indicate that the source-side vocabulary is somehow quite helpful -- the opposite of your initial claim?
OTHER TYPOS OR CONFUSION - Why don't the numbers at the top of Table 5 (for baseline label smoothing) match the scores given in Table 3 for LS?  Aren't these identical configurations?
- How do you decide when model training is over?  Is it a fixed number of epochs or updates, or after perplexity on the dev set converges (for some definition), etc.  Appendix B doesn't say.  Since your experiments modify the kind of signal that the model receives during training, I was wondering if that has a confounding effect on the results.  (For example, if MLS makes the training last 25 percent more updates than the baseline, then is the increased performance partially due to the mere fact of training longer?)
- The WLS parameters ""1-1-0"" in Section 4.2 (line 175) directly contradict the claim in Section 4.1 (line 150) that these three values must sum to 1. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No ethical concerns. ,"- The use of the appendix is a little more integral than I think would be ideal.  You have to read the appendix in order to know the conclusions of some analysis; the main text only says that such-and-such ""matters,"" but now how.
- It's not always clear which results are meaningful.  Some rows of the main table have statistical significance tests, but other metrics, analysis, and data points do not.  Since the score differences are overall quite small, this detracts from the reliability of the results.  Readers might be inclined to decide that the techniques aren't worth trying. 
SUGGESTIONS - It's great to have at least some statistical significance tests.  Since they were added only after the first revision of the paper, I felt like they weren't fully integrated into the main text or all of the results tables (like Table 5).  For example, the beginning of Section 5.2 (line 201) says that LS ""outperforms"" VS+LS... but we do not have any significance tests between these two configurations.  These differences of 0.1 BLEU may actually be ties, in which case ""outperforms"" is incorrect.
- In Table 3(b), is it really the case that 23.19 BLEU for RO-EN is statistically better than 23.15, or is this a typo?  I've never seen significance at p < 0.01 for such a tiny score change.
- Even without a formal significance test, can we have some intuition about this ECE score in Section 6.1 / Table 4?  How is this score computed and what properties of MT training does it reflect?  I've never heard of this metric before, so I don't really understand what it is or whether an improvement of 0.7 is meaningful.  (Whereas I know quite well how much intuitive importance to give 0.7 BLEU.)
- In Table 5, I would think that one intuitive configuration is to devalue the source vocabulary without shutting it off completely, via parameter settings like 0.5-0.5-0.25.  If your original motivation is correct, I would expect a configuration like that to do better than your current best result, 0.5-0-0.5.  Doesn't the current result indicate that the source-side vocabulary is somehow quite helpful -- the opposite of your initial claim?
OTHER TYPOS OR CONFUSION - Why don't the numbers at the top of Table 5 (for baseline label smoothing) match the scores given in Table 3 for LS?  Aren't these identical configurations?
- How do you decide when model training is over?  Is it a fixed number of epochs or updates, or after perplexity on the dev set converges (for some definition), etc.  Appendix B doesn't say.  Since your experiments modify the kind of signal that the model receives during training, I was wondering if that has a confounding effect on the results.  (For example, if MLS makes the training last 25 percent more updates than the baseline, then is the increased performance partially due to the mere fact of training longer?)
- The WLS parameters ""1-1-0"" in Section 4.2 (line 175) directly contradict the claim in Section 4.1 (line 150) that these three values must sum to 1. "
ARR_2022_179_review,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting. This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence. They propose to use a very simple approach (Masked label smoothing) that addresses the conflict. Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.  Overall the conflict is interesting but the result is very mixed. ,The paper presents an interesting conflict between these two approaches. I personally never thought about this before. ,"Cons: From translation accuracy perspective, the improvement from fixing the conflict is very incremental. This is clearly shown as the translation improvement is only +0.47 at its best. In other cases (e.g. in the specific case (EN-RO - Table 5)), the improvement is +0.04 (from 23.15 to 23.19), which I would say it does not imply anything that is particularly meaningful.  able 5 also shows that assigning B_s to 0 seems does not matter that much (i.e. it does not hurt the performance that much). This shows that despite the intuition, the conflict actually does not matter in reality.
There might be improvement in model calibration, but with the current paper it is not clear from Table 4 the improvement means big thing or not. ","Typo + writing - as shown in Table 6,7 for both BLEU 207 and chrF -> 6, 7 - As shown in Table 3 , MLS 212 achieves consistent improvement over the origi213 nal label smoothing in both the original and the 214 balanced multilingual translation dataset under all 215 translation directions -> Table 3, MLS - About ECE score: There should be an explanation from the paper.
- Writing style: the authors use the word outperform a lot in the paper. Clearly the improvement does not reflect an ""outperforming"" by anymeans. I suggest the authors lower the use of word... ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Cons: From translation accuracy perspective, the improvement from fixing the conflict is very incremental. This is clearly shown as the translation improvement is only +0.47 at its best. In other cases (e.g. in the specific case (EN-RO - Table 5)), the improvement is +0.04 (from 23.15 to 23.19), which I would say it does not imply anything that is particularly meaningful.  able 5 also shows that assigning B_s to 0 seems does not matter that much (i.e. it does not hurt the performance that much). This shows that despite the intuition, the conflict actually does not matter in reality.
There might be improvement in model calibration, but with the current paper it is not clear from Table 4 the improvement means big thing or not. 
Typo + writing - as shown in Table 6,7 for both BLEU 207 and chrF -> 6, 7 - As shown in Table 3 , MLS 212 achieves consistent improvement over the origi213 nal label smoothing in both the original and the 214 balanced multilingual translation dataset under all 215 translation directions -> Table 3, MLS - About ECE score: There should be an explanation from the paper.
- Writing style: the authors use the word outperform a lot in the paper. Clearly the improvement does not reflect an ""outperforming"" by anymeans. I suggest the authors lower the use of word... "
ARR_2022_263_review,"This paper addresses cross lingual summarization task. This task is generating a summary in a target language from a source document in a source language. In other words, the cross lingual summarization is the combination of machine translation and summarization.
Some previous studies regarded the cross lingual summarization as the combination task of machine translation and summarization, and proposed multi-task learning methods. 
In contrast, this study regards the cross lingual summarization as a super-task of machine translation and summarization, and proposes a hierarchical model to control them. The proposed method uses two kinds of latent variables: global and local. A global corresponds to cross lingual summarization, and a local corresponds to machine translation and summarization. This paper applies conditional variational auto encoder to treat these latent variables.
Experimental results show that the proposed method achieved better performance than previous methods in automatic and human evaluation. ","The core idea, that regards cross lingual summarization as a super-task of machine translation and summarization, seems reasonable. In addition, it is a reasonable approach to apply conditional variational auto encoder to the cross lingual summarization.
The proposed method achieved better performance than previous methods in automatic and human evaluation. ","Since explanations on experiments contain unclear points, I'm not sure whether the proposed method achieved better performance than previous methods in a fair comparison. 
For training datasets, the proposed method seems to be using only cross lingual summarization data but MS-CLS, MT-CLS, and MT-MS-CLS use machine translation, summarization, or both datasets based on descriptions in 4.2. 
I checked Appendix A but it does not contain details explicitly such as the number of sentences for each task.
In addition, the parameter sizes of each method might be different from each other. The proposed method share parameters of an encoder but this paper does not explain other methods explicitly. Thus, I cannot judge whether the proposed training strategy improves the performance or sharing parameters improves the performance (or other reasons for improvements). 
I recommend explaining the number of sentences and parameter sizes for each method explicitly.
Moreover, I suppose that the proposed method requires much more training time than previous methods. If so, the authors should discuss the relation between training time and performance because a view on Green AI is important.
For the human evaluation, the authors used only 25 samples but the number of samples are too small. I doubt that we can conclude the effectiveness based on such small instances. ","I'm not sure why the authors describe pipeline methods in Table 1. 
Zhu et al., 2019 indicated that end-to-end methods achieved better performance than pipeline methods. 
I think that the results in Table 1 are copied from their paper because scores are identical to ones of them. 
Thus, I cannot understand why authors re-compare pipeline methods (with reported scores) with end-to-end methods.
In addition, I doubt that the above pipeline methods consist of weakly methods. 
If the authors conduct a fair comparison, the authors should prepare strong methods for machine translation and summarization. 
For example, initializing each model with mBART and fine-tuning them on training data of each task.
For explanations in Section 4.3, the authors should list each method for readability such as using \item. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Since explanations on experiments contain unclear points, I'm not sure whether the proposed method achieved better performance than previous methods in a fair comparison. 
For training datasets, the proposed method seems to be using only cross lingual summarization data but MS-CLS, MT-CLS, and MT-MS-CLS use machine translation, summarization, or both datasets based on descriptions in 4.2. 
I checked Appendix A but it does not contain details explicitly such as the number of sentences for each task.
In addition, the parameter sizes of each method might be different from each other. The proposed method share parameters of an encoder but this paper does not explain other methods explicitly. Thus, I cannot judge whether the proposed training strategy improves the performance or sharing parameters improves the performance (or other reasons for improvements). 
I recommend explaining the number of sentences and parameter sizes for each method explicitly.
Moreover, I suppose that the proposed method requires much more training time than previous methods. If so, the authors should discuss the relation between training time and performance because a view on Green AI is important.
For the human evaluation, the authors used only 25 samples but the number of samples are too small. I doubt that we can conclude the effectiveness based on such small instances. 
I'm not sure why the authors describe pipeline methods in Table 1. 
Zhu et al., 2019 indicated that end-to-end methods achieved better performance than pipeline methods. 
I think that the results in Table 1 are copied from their paper because scores are identical to ones of them. 
Thus, I cannot understand why authors re-compare pipeline methods (with reported scores) with end-to-end methods.
In addition, I doubt that the above pipeline methods consist of weakly methods. 
If the authors conduct a fair comparison, the authors should prepare strong methods for machine translation and summarization. 
For example, initializing each model with mBART and fine-tuning them on training data of each task.
For explanations in Section 4.3, the authors should list each method for readability such as using \item. "
ARR_2022_176_review,The paper presents a novel way of using annotator rationales. The main novelty lies in introducing the rationales though a ranking constraint. The newly introduced model is empirically compared with several others from prior work. The new model outperforms those baselines when little data is available. ,The idea of a ranking constraint is very interesting and as far as I'm aware is novel. The comparison with other work is sensible as is the choice of baseline tasks.  The paper is easy to read. ,"The experimental setup is a bit weak in my view. The choice of baselines covers related work, but it doesn't cover possible variations of the model, such as a non-BERT ranking baseline. The SVM and LR baselines both use the ""old"" style of using representations. I would honestly expect this kind of investigation from a paper that emphasizes the role of the ranking constraint and even puts it in the title.  In addition, the presentation of the main results makes it difficult to see the strengths of each model. The graphs are cut off at around 250 training documents even though some datasets contain much more. It appears as if the rivaling models start to converge after a while. Yet the analysis in table 1 suggest that the new model indeed promises overall improvements that the others cannot achieve (for example 85% accuracy on ASRS). ",It would be helpful for me to address my two concerns above. This would strengthen the results vastly. ,2.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"The experimental setup is a bit weak in my view. The choice of baselines covers related work, but it doesn't cover possible variations of the model, such as a non-BERT ranking baseline. The SVM and LR baselines both use the ""old"" style of using representations. I would honestly expect this kind of investigation from a paper that emphasizes the role of the ranking constraint and even puts it in the title.  In addition, the presentation of the main results makes it difficult to see the strengths of each model. The graphs are cut off at around 250 training documents even though some datasets contain much more. It appears as if the rivaling models start to converge after a while. Yet the analysis in table 1 suggest that the new model indeed promises overall improvements that the others cannot achieve (for example 85% accuracy on ASRS). 
It would be helpful for me to address my two concerns above. This would strengthen the results vastly. "
ARR_2022_176_review,"This paper proposes a novel approach that jointly utilizes annotator information, that labels and elicited rationales are used to speed up the training of deep learning models with limited training data. The ranking constraint is the most important mechanism introduced. Performance evaluation shows that the proposed method achieves state-of-the-art performance with higher efficiency and lower demand for data. ","1. The proposed approach is well-designed and novel. This paper derives the constraint formulas in a rigorous logical flow. 
2. This paper contributes a new text classification dataset with rationales and makes it publicly available. 
3. The experiments are sufficiently and reasonably conducted to verify the effectiveness of this method. ","1. Experimental part is less convincing. The LR and SVM based baselines are too weak compared with deep learning approaches. 
2. Figure 2 does not show complete trends w.r.t the training documents. It seems that LwR-RC might be outperformed with RB-WAVG or even Lw/oR-BERT. Moreover, analysis of this trend is insufficiently discussed. ","1. Please provide some statistics of three benchmarks as truncation has been used in both sentence length and sentence number for each instance. 
2. Please reorganize the section of performance evaluation to highlight the strengths of the proposed methods. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. Experimental part is less convincing. The LR and SVM based baselines are too weak compared with deep learning approaches. 
2. Figure 2 does not show complete trends w.r.t the training documents. It seems that LwR-RC might be outperformed with RB-WAVG or even Lw/oR-BERT. Moreover, analysis of this trend is insufficiently discussed. 
1. Please provide some statistics of three benchmarks as truncation has been used in both sentence length and sentence number for each instance. 
2. Please reorganize the section of performance evaluation to highlight the strengths of the proposed methods. "
ARR_2022_302_review,"The paper presents experiments where mBERT models along with appropriate task-specific layers on top are fine-tuned on multiple languages simultaneously. Moreover, active learning is used across all training languages based on low confidence criteria.  The tasks addressed are tagging, NER, and dependency parsing. experiments are done on English, Spanish, Japanese, German and Dutch.
The paper seems to presume that zero-shot transfer of individual monolingual models has been the only method of choice when tackling multilingual tasks. However, there is a sizable body of work that aims solely on producing a single model for various languages, seeking to reduce the number of parameters while retaining performance. Here are some examples: https://proceedings.neurips.cc/paper/2021/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf https://arxiv.org/pdf/1602.01595.pdf ",A fair comparison between various mono and multilingual setups and active learning regimen. ,"No comparison with past works that produce higher quality multilingual models.  In fact, no comparison with past work is made. ","The paper needs more innovation and comparison with past work.
AL in ML setting could be a line to pursue further. ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"No comparison with past works that produce higher quality multilingual models.  In fact, no comparison with past work is made. 
The paper needs more innovation and comparison with past work.
AL in ML setting could be a line to pursue further. "
ARR_2022_114_review,"The performance of structured prediction models can be greatly improved by scaling to larger state spaces, yet the inference complexity of these models scales poorly w.r.t. the size of the state space. The goal of this work is to reduce the inference complexity of structured models by factorizing the clique potentials using low-rank tensor decompositions and performing message passing in an induced rank space instead of the original state space.
This work makes three contributions: 1. Using the language of factor graph grammars, this work unifies previous low-rank tensor decomposition works such as Yang et al 2021b  and Chiu et al 2021. This work shows that those works are essentially performing message passing on a factor graph with two types of nodes: the original state nodes and auxiliary rank nodes induced by the low-rank tensor decomposition. 
2. On a sub-family of factor graph grammars which subsume most commonly-used structured prediction models such as HMMs, HSMMs, and PCFGs, this work proposes to marginalize the state nodes first and only perform inference in the induced rank nodes, which reduces the complexity by replacing a factor of the state size by a factor of the rank size which is usually smaller. 
3. Empirically this work scales HMMs and PCFGs to very large state spaces and achieves strong performance. ","1. This work is insightful in pointing out that by performing message passing only in the rank space after marginalizing the original state nodes (which is a one-time cost), a factor of the number of states in the total complexity can be replaced by a factor of the rank size. This idea is generally applicable to a large family of factor graph grammars that have one external node per hypergraph fragment, and it might enable scaling many structured prediction models. 
2. This work gets strong empirical performance by scaling to very large state spaces when compared to previous structured prediction works. In particular, this work trains the largest-ever PCFG in the task of unsupervised parsing on PTB (to my knowledge) and establishes a new state-of-the-art performance in this particular task. 
3. This work confirms findings of previous works such as Chiu and Rush 2020 that scaling structured prediction models can improve performance. For example, Figure 6 (b) suggests that scaling PCFGs to beyond 10k pre-terminals might further improve modeling performance. ","By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials).  For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.
As another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\in \mathcal{R}^{r \times r}$ is parameterized as $V U^T$ where $U,V\in \mathcal{R}^{r \times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\mathcal{R}^{r \times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here? ","In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared. 
2. For each HMM with rank r, add a baseline smaller HMM with state size being r. ","5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.",Maybe,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials).  For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.
As another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\in \mathcal{R}^{r \times r}$ is parameterized as $V U^T$ where $U,V\in \mathcal{R}^{r \times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\mathcal{R}^{r \times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here? 
In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared. 
2. For each HMM with rank r, add a baseline smaller HMM with state size being r. "
ARR_2022_360_review,"This paper is about phonology of sign languages. It introduces the task of phonological property prediction, as well as a novel resource, which combines two existing data sets into a data set that can be used for training large-scale neural network models for phonological property prediction. Finally, the study presents a set of experiments, in which various models trained to perform the specified task on that data set. The results show that the task is feasible in principle, but the existing models do not reach the optimal performance. ","This is paper in an area severely underexplored in the NLP community, even though sign languages are commonly used. The study seems to be well-conducted, and the paper is easy to read, I think it would be useful to include the paper in the conference. ","It wasn't obvious to me why attention-based architectures weren't used for the task. I can imagine the NLP community would be interested in that, but that doesn't make the paper's contribution less interesting. Also, it wasn't clear to me whether the data would can be released, as this is one of the important contributions of the paper. ","- It took me a while to understand that ""sign"" and ""gloss"" refer to the same thing. Or am I still misunderstanding it?
- I couldn't understand why a multiclass multilabel approach couldn't be presented in parallel. Was it just left for future work? I assume it would be necessary for the tokenization task that the authors mention in the conclusion, would it not? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"It wasn't obvious to me why attention-based architectures weren't used for the task. I can imagine the NLP community would be interested in that, but that doesn't make the paper's contribution less interesting. Also, it wasn't clear to me whether the data would can be released, as this is one of the important contributions of the paper. 
- It took me a while to understand that ""sign"" and ""gloss"" refer to the same thing. Or am I still misunderstanding it?
- I couldn't understand why a multiclass multilabel approach couldn't be presented in parallel. Was it just left for future work? I assume it would be necessary for the tokenization task that the authors mention in the conclusion, would it not? "
ARR_2022_360_review,This paper aims at addressing the phonological property recognition task for sign languages. The authors take advantage of the existing datasets and annotate the data with six phonological properties. Then they train neural models based on the dataset and perform a classification task to find an optimized automatic recognition approach for these properties. Preliminary results show that neural networks present various performances towards different properties. ,"1. The paper structure is well organized, and the illustration is clear and cohesive. 
2. From this work, they contribute a dataset that can be further used. ","1. When conducting the evaluation, do you consider using the F1 score instead of accuracy to have a more comprehensive understanding of model performance? 
2. I notice there is a severe imbalance with the data you annotated. I think a balanced-data experiment is needed. 
3. I would expect a more detailed error analysis in the discussion part. ",I think those properties will interact with each other so the models may easily be influenced by them together. Do you consider generating or extracting contrastive pairs during model training and testing? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. When conducting the evaluation, do you consider using the F1 score instead of accuracy to have a more comprehensive understanding of model performance? 
2. I notice there is a severe imbalance with the data you annotated. I think a balanced-data experiment is needed. 
3. I would expect a more detailed error analysis in the discussion part. 
I think those properties will interact with each other so the models may easily be influenced by them together. Do you consider generating or extracting contrastive pairs during model training and testing? "
ARR_2022_201_review,"This paper introduces a method to improve both interpretations and inference by training an end-to-end model to simultaneously generate explanations and labels for the NLI and CQA tasks. Compared to previous work which either produces explanations that are used to infer the label or vice-versa, this work introduces a framework that trains both tasks jointly. This is done by iteratively updating the predicted label at each explanation decoding step. The representations of the label and the explanation tokens are combined through gating mechanisms and used as input for the next timestep, enabling the model to use the explanation generated thus far to predict the label and the predicted label to guide the generation of the next explanation token. To improve the correlation between the inferred label and the generated interpretation, an additional adversarial regularization is used. Experiments show that this model shows improvements in both inference and interpretation compared to baselines. ","1. The idea of jointly training inference and interpretation is interesting. It is intuitive that the inference and interpretation should guide each other and be dependent. 
2. Several experimental results are presented to show that the model improves over both generated explanations and predicted labels. 
3. Ablation results are presented to highlight the contributions of each component. 
4. Evaluation is conducted over out-of-domain data, and a metric is introduced to evaluate the fidelity between the inference predictions and generated interpretations. Visual analysis shows how the predicted label changes with the generated interpretation. Human evaluation is also conducted. ","1. The Methodology section is very hard to follow. The model architecture description is rather confusing and sometimes uses inconsistent notation. For example, Section 2.2 introduces $v^p_{t-1}$ in the description which does not appear in the equations. Some of the notation pertaining to the labels ($l_0$, $l_{t-1}$) initially gives the impression that a sequence of tokens are being generated as the label, which is not the case. 
2. It is not clear why the baseline model considered for the NLI task is based on an older work (Conneau et al., 2017) and not the work by Kumar and Talukdar (2020), which seems more relevant to this work and is also cited in the paper. In addition, all the comparisons in the result section (and the delta numbers in Table 1) are made against a baseline vanilla Transformer model. 
3. Some of the numbers presented in Table 1 are confusing. It is not clear how a BLEU score of 22.51 is obtained by evaluating the ground truth dataset against itself for the NLI task. It is also not explained how the perplexity results are obtained. Are the numbers averaged across the dataset? Would that be a valid way to present these results? 
4. Some details like whether the results are averaged across multiple runs, and what the inter-annotator agreement was in the human evaluation are missing. ","- In Section 2.1, the Transformer model is declared to have been established as the dominant approach for test generation. The next sentence immediately proceeds to the model description. This paragraph would flow better if there were a sentence linking the two parts and making it explicit that the Transformer model is being used. For example, ""We therefore adopt the Transformer model as our base model"" or something to that effect.
- The description of the model architecture needs to be rephrased for clarity. See (1) under Weaknesses for some notation inconsistencies that can be improved as well.
- In Section 2.3, Line 266 the description states $L$, $R$ are conditioned on $X$. Presumably it is meant to be $L$, $E$ since no $R$ is introduced anywhere.
- In Section 3.2, the description of the Transformer baseline states that an MLP layer is added for generating sentence-level interpretations. This was perhaps meant to be predicted labels and not the interpretations, unless the decoder here is generating the labels - this needs clarification or correction.
- In the results under ""Interpretation Promotion"", it is stated that ""most of the explanations generated by our method are reasonable even though the BLEU scores are low"". It might help to add an example here to make this more convincing.
- Line 043: comprehensibleness -> comprehensibility  - Line 044: human -> humans - Line 046: which nevertheless -> which are nevertheless - Line 059: With the annotated -> With annotated - Line 135: method achieve -> method achieves - Line 163: dataset annotated by human -> human-annotated dataset - Line 169: a MLP -> an MLP - Line 233: to better inference -> to do better inference/to infer better - Line 264: two distribution -> two distributions - Line 303: maximum the negative likelihood -> maximize the negative likelihood - Line 493--494: the opening quotes are incorrectly formatted - Line 509: exapmle -> example ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"1. The Methodology section is very hard to follow. The model architecture description is rather confusing and sometimes uses inconsistent notation. For example, Section 2.2 introduces $v^p_{t-1}$ in the description which does not appear in the equations. Some of the notation pertaining to the labels ($l_0$, $l_{t-1}$) initially gives the impression that a sequence of tokens are being generated as the label, which is not the case. 
2. It is not clear why the baseline model considered for the NLI task is based on an older work (Conneau et al., 2017) and not the work by Kumar and Talukdar (2020), which seems more relevant to this work and is also cited in the paper. In addition, all the comparisons in the result section (and the delta numbers in Table 1) are made against a baseline vanilla Transformer model. 
3. Some of the numbers presented in Table 1 are confusing. It is not clear how a BLEU score of 22.51 is obtained by evaluating the ground truth dataset against itself for the NLI task. It is also not explained how the perplexity results are obtained. Are the numbers averaged across the dataset? Would that be a valid way to present these results? 
4. Some details like whether the results are averaged across multiple runs, and what the inter-annotator agreement was in the human evaluation are missing. 
- In Section 2.1, the Transformer model is declared to have been established as the dominant approach for test generation. The next sentence immediately proceeds to the model description. This paragraph would flow better if there were a sentence linking the two parts and making it explicit that the Transformer model is being used. For example, ""We therefore adopt the Transformer model as our base model"" or something to that effect.
- The description of the model architecture needs to be rephrased for clarity. See (1) under Weaknesses for some notation inconsistencies that can be improved as well.
- In Section 2.3, Line 266 the description states $L$, $R$ are conditioned on $X$. Presumably it is meant to be $L$, $E$ since no $R$ is introduced anywhere.
- In Section 3.2, the description of the Transformer baseline states that an MLP layer is added for generating sentence-level interpretations. This was perhaps meant to be predicted labels and not the interpretations, unless the decoder here is generating the labels - this needs clarification or correction.
- In the results under ""Interpretation Promotion"", it is stated that ""most of the explanations generated by our method are reasonable even though the BLEU scores are low"". It might help to add an example here to make this more convincing.
- Line 043: comprehensibleness -> comprehensibility  - Line 044: human -> humans - Line 046: which nevertheless -> which are nevertheless - Line 059: With the annotated -> With annotated - Line 135: method achieve -> method achieves - Line 163: dataset annotated by human -> human-annotated dataset - Line 169: a MLP -> an MLP - Line 233: to better inference -> to do better inference/to infer better - Line 264: two distribution -> two distributions - Line 303: maximum the negative likelihood -> maximize the negative likelihood - Line 493--494: the opening quotes are incorrectly formatted - Line 509: exapmle -> example "
ARR_2022_201_review,"Prior work has used interpretation to improve inference, while ignoring “using inference logic to enhance interpretation.” This work deals with “mutual promotion” where they “promote” in both directions. Specifically, the “mutual promotion” is done using stepwise integration mechanism (SIM; Section 2.2). Additionally, adversarial fidelity regularization is used to further “improve the fidelity of inference and interpretation” (Section 2.3).   Essentially, during training, the model generates explanation first, and at the last time-step, generates the prediction. Therefore, the explanation and prediction are using largely the same parameters, and gradient updates would influence both the explanation and prediction.
The authors experiment on NLI and conversational QA tasks. The explanation is scored against the gold-standard human-provided evaluation results in e-SNLI and CoS-E datasets. ","Interpretation is an important problem.  The figures are well-designed and help readers understand the algorithms.
The method performs well on out-of-domain datasets (MNLI and SICK-E).
The mutual information discussion is well-motivated. ","I’m not convinced that AFiRe (the adversarial regularization) brings significant improvement, especially because - BLEU improvements are small (e.g., 27.93->28.64; would humans be able to identify the differences?)
- Hyperparameter details are missing.
- Human evaluation protocols, payment, etc. are all missing. Who are the raters? How are they ""educated"" and how do the authors ensure the raters provide good-faith annotations? What is the agreeement?
Other baselines are not compared against. For example, what if we just treat the explanation as a latent variable as in Zhou et al. (2021)? https://arxiv.org/pdf/2011.05268.pdf A few other points that are not fatal: - Gold-standard human explanation datasets are necessary, given the objective in line 307.  - Does it mean that inference gets slowed down drastically, and there’s no way to only do inference (i.e., predict the label)? I don’t think this is fatal though.  What’s the coefficient of the p(L, E | X) term in line 307? Why is it 1?  Hyperparamter details are missing, so it’s not clear whether baselines are well-tuned, and whether ablation studies provide confident results.  The writing is not careful, and often impedes understanding.
- Line 229: What’s t?
- Line 230: What’s n?
- Line 273: having X in the equation without defining it is a bit weird; should there be an expectation over X?
- Sometimes, the X is not bolded and not italicized (line 262). Sometimes, the X is not bolded but italicized (line 273). Sometimes, the X is bolded but not italicized (line 156).  - Line 296: L and E should be defined in the immediate vicinity. Again, sometimes L, E are italicized (line 296) and sometimes not (line 302).
- Line 187: It’s best to treat Emb as a function. Having l’ and e’ as superscripts is confusing.
- In Table 4, why sometimes there are punctuations and sometimes there are no punctuations? ","- Perplexity does not necessarily measure fluency. For example, an overly small perplexity may correspond to repeating common n-grams. But it’s okay to use it as a coarse approximation of fluency.
- Line 191: \cdot should be used instead of regular dot Section 2.1: It would be best to define the dimensionalities of everything.
- Line 182: A bit confusing what the superscript p means.
- Line 229: What’s t?
- Line 230: What’s n?  - Line 255: Comma should not start the line. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"I’m not convinced that AFiRe (the adversarial regularization) brings significant improvement, especially because - BLEU improvements are small (e.g., 27.93->28.64; would humans be able to identify the differences?)
- Hyperparameter details are missing.
- Human evaluation protocols, payment, etc. are all missing. Who are the raters? How are they ""educated"" and how do the authors ensure the raters provide good-faith annotations? What is the agreeement?
Other baselines are not compared against. For example, what if we just treat the explanation as a latent variable as in Zhou et al. (2021)? https://arxiv.org/pdf/2011.05268.pdf A few other points that are not fatal: - Gold-standard human explanation datasets are necessary, given the objective in line 307.  - Does it mean that inference gets slowed down drastically, and there’s no way to only do inference (i.e., predict the label)? I don’t think this is fatal though.  What’s the coefficient of the p(L, E | X) term in line 307? Why is it 1?  Hyperparamter details are missing, so it’s not clear whether baselines are well-tuned, and whether ablation studies provide confident results.  The writing is not careful, and often impedes understanding.
- Line 229: What’s t?
- Line 230: What’s n?
- Line 273: having X in the equation without defining it is a bit weird; should there be an expectation over X?
- Sometimes, the X is not bolded and not italicized (line 262). Sometimes, the X is not bolded but italicized (line 273). Sometimes, the X is bolded but not italicized (line 156).  - Line 296: L and E should be defined in the immediate vicinity. Again, sometimes L, E are italicized (line 296) and sometimes not (line 302).
- Line 187: It’s best to treat Emb as a function. Having l’ and e’ as superscripts is confusing.
- In Table 4, why sometimes there are punctuations and sometimes there are no punctuations? 
- Perplexity does not necessarily measure fluency. For example, an overly small perplexity may correspond to repeating common n-grams. But it’s okay to use it as a coarse approximation of fluency.
- Line 191: \cdot should be used instead of regular dot Section 2.1: It would be best to define the dimensionalities of everything.
- Line 182: A bit confusing what the superscript p means.
- Line 229: What’s t?
- Line 230: What’s n?  - Line 255: Comma should not start the line. "
ARR_2022_60_review,"This paper contributes two experiments to investigate how annotator’s backgrounds and sociopolitical attitudes affect their perception of toxic language. The paper finds that having certain characteristics is correlated with perceptions towards racism, for example, believing in freedom of speech, predisposes one to be more lax in annotating anti-Black toxicity. The paper also finds that the widely used toxicity detection tool, Perspective API, mimics the conservative attitudinal profile when it comes anti-Black toxicity. ","- Important and valuable work - Going beyond demographic characteristics of annotators, but also including sociopolitical attitudes - Carefully thought-out experiments - Detailed description of some metrics and results - Well-described and contextualized implications of disregarding annotator background and beliefs when creating ",- Underdefined and conflation of concepts - Several important details missing - Lack of clarity in how datasets were curated prevents one from assessing their validity - Too many results which are not fully justified or explained ,"This is a very important, interesting, and valuable paper with many positives. First and foremost, annotators’ backgrounds are an important factor and should be taken into consideration when designing datasets for hate speech, toxicity, or related phenomena. The paper not only accounts for demographic variables as done in previous work but other attitudinal covariates like attitude towards free speech that are well-chosen. The paper presents two well-thought out experiments and presents results in a clear manner which contain several important findings.
It is precisely because of the great potential and impact of this paper, I think the current manuscript requires more consideration and fine-tuning before it can reach its final stage. At this point, there seems to be a lack of important details that prevent me from fully gauging the paper’s findings and claims. Generally: - There were too many missing details (for example, what is the distribution of people with ‘free off speech’ attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.
- Certain researcher choices and experiment design choices were not justified (for example, why were these particular scales used?)
- The explanation of the creation of the breadth-of-posts was confusing. How accurate was the classification of AAE dialect and vulgarity?  - The toxicity experiment was intriguing but there was too little space to be meaningful.
More concretely, - With regard to terminology and concepts, toxicity and hate speech may be related but are not the same thing. The instructions to the annotators seem to conflate both. The paper also doesn’t present a concrete definition of either. While it might seem redundant or trivial, the wording to annotators plays an important role and can confound the results presented here.
- Why were the particular scales chosen for obtaining attitudes? Particularly, for empathy there are several scale items [1], so why choose the Interpersonal Reactivity Index?
- What was the distribution of the annotator’s background with respect to the attitudes? For example, if there are too few ‘free of speech’ annotators, then the results shown in Table 3, 4, etc are underpowered.  - What were the correlations of the chosen attitudinal scale item for the breadth-of-posts study with the toxicity in the breadth-of-workers study?
- How accurate are the automated classification in the breadth-of-posts experiment, i.e., how well does the states technique differentiate identity vs non-identity vulgarity or AAE language for that particular dataset. Particularly, how can it be ascertained whether the n-word was used as a reclaimed slur or not?  - In that line, Section 6 discusses perceptions of vulgarity, but there are too many confounds here. Using b*tch in a sentence can be an indication of vulgarity and toxicity (due to sexism).
- In my opinion, the perspective API experiment was interesting but rather shallow. My suggestion would be to follow up on it in more detail in a new paper rather than include it in this one. The newly created space could be used to enter the missing details mentioned in the review.   - Finally, given that the paper notes that MTurk tends to be predominantly liberal and the authors (commendably) took several steps to ensure greater participation from conservatives, I was wondering if ‘typical’ hate speech datasets are annotated by more homogenous annotators compared to the sample in this paper. What could be the implications of this? Do this paper's findings then hold for existing hate speech datasets?
Besides these, I also note some ethical issues in the ‘Ethical Concerns’ section. To conclude, while my rating might seem quite harsh, I believe this work has great potential and I hope to see it enriched with the required experimental details.
References: [1] Gerdes, Karen E., Cynthia A. Lietz, and Elizabeth A. Segal. "" Measuring empathy in the 21st century: Development of an empathy index rooted in social cognitive neuroscience and social justice."" Social Work Research 35, no. 2 (2011): 83-93. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,"There is no limitations section, unlike what the authors state in the responsible NLP research checklist. There are however many expositional and ethical limitations in this paper. I do not believe they are grounds for rejection, but I expect that the paper acknowledges and engages with them.
- The biggest problem, as is the case for abusive language detection in general, is that there is no reflection on the harm to annotators.  - Second, given that the whole idea of the paper is to consider annotator backgrounds, I am left wondering what the backgrounds of the undergraduate annotators for the 15 posts were. Speaking of backgrounds, there is also no reflection on the background of the authors themselves and how that might color their perspectives in this paper (see Sweet, Paige L. ""Who Knows? Reflexivity in Feminist Standpoint Theory and Bourdieu."" Gender & Society 34, no. 6 (2020): 922-950.) ","- Underdefined and conflation of concepts - Several important details missing - Lack of clarity in how datasets were curated prevents one from assessing their validity - Too many results which are not fully justified or explained 
This is a very important, interesting, and valuable paper with many positives. First and foremost, annotators’ backgrounds are an important factor and should be taken into consideration when designing datasets for hate speech, toxicity, or related phenomena. The paper not only accounts for demographic variables as done in previous work but other attitudinal covariates like attitude towards free speech that are well-chosen. The paper presents two well-thought out experiments and presents results in a clear manner which contain several important findings.
It is precisely because of the great potential and impact of this paper, I think the current manuscript requires more consideration and fine-tuning before it can reach its final stage. At this point, there seems to be a lack of important details that prevent me from fully gauging the paper’s findings and claims. Generally: - There were too many missing details (for example, what is the distribution of people with ‘free off speech’ attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.
- Certain researcher choices and experiment design choices were not justified (for example, why were these particular scales used?)
- The explanation of the creation of the breadth-of-posts was confusing. How accurate was the classification of AAE dialect and vulgarity?  - The toxicity experiment was intriguing but there was too little space to be meaningful.
More concretely, - With regard to terminology and concepts, toxicity and hate speech may be related but are not the same thing. The instructions to the annotators seem to conflate both. The paper also doesn’t present a concrete definition of either. While it might seem redundant or trivial, the wording to annotators plays an important role and can confound the results presented here.
- Why were the particular scales chosen for obtaining attitudes? Particularly, for empathy there are several scale items [1], so why choose the Interpersonal Reactivity Index?
- What was the distribution of the annotator’s background with respect to the attitudes? For example, if there are too few ‘free of speech’ annotators, then the results shown in Table 3, 4, etc are underpowered.  - What were the correlations of the chosen attitudinal scale item for the breadth-of-posts study with the toxicity in the breadth-of-workers study?
- How accurate are the automated classification in the breadth-of-posts experiment, i.e., how well does the states technique differentiate identity vs non-identity vulgarity or AAE language for that particular dataset. Particularly, how can it be ascertained whether the n-word was used as a reclaimed slur or not?  - In that line, Section 6 discusses perceptions of vulgarity, but there are too many confounds here. Using b*tch in a sentence can be an indication of vulgarity and toxicity (due to sexism).
- In my opinion, the perspective API experiment was interesting but rather shallow. My suggestion would be to follow up on it in more detail in a new paper rather than include it in this one. The newly created space could be used to enter the missing details mentioned in the review.   - Finally, given that the paper notes that MTurk tends to be predominantly liberal and the authors (commendably) took several steps to ensure greater participation from conservatives, I was wondering if ‘typical’ hate speech datasets are annotated by more homogenous annotators compared to the sample in this paper. What could be the implications of this? Do this paper's findings then hold for existing hate speech datasets?
Besides these, I also note some ethical issues in the ‘Ethical Concerns’ section. To conclude, while my rating might seem quite harsh, I believe this work has great potential and I hope to see it enriched with the required experimental details.
References: [1] Gerdes, Karen E., Cynthia A. Lietz, and Elizabeth A. Segal. "" Measuring empathy in the 21st century: Development of an empathy index rooted in social cognitive neuroscience and social justice."" Social Work Research 35, no. 2 (2011): 83-93. "
ARR_2022_211_review,"The paper proposes a core-set based token selection algorithm to reduce the computation overhead of Transformer models on sequence-level classification tasks. The main contribution of the paper is to propose a practical core-set selection algorithm which effectively find a $\delta$-cover of the token embeddings, and thus reduces redundant tokens. Similar to previous work, the sequence-length is gradually reduced from bottom to top, thus reducing the redundancy among tokens. The paper also presents positive results on GLUE and Long-Range-Arena benchmark. ","1. A core-set based token selection algorithm is proposed, with a derivation of the error bound. One of the main advantages of core-set based token selection compared to other local pooling/selection strategies is that core-set based token selection allows removing long-range duplications.
2. A new sequence-length configuration is proposed for gradually reducing the sequence length in the pipeline of encoders, showing its advantages over other alternatives. ","1. In the experiment on GLUE, there is no comparison of the proposed core-set based method with a simple local-pooling based method. Since this paper is inspired and follows this line of previous work, such a comparison is necessary.
2. The derivation of the optimization objective is a bit messy (Equation 1-3). For example, the model weights $\mathbf{w}$ are important optimization variables, but they are totally omitted in the derivation. Therefore, the derived objective for token selection (i.e., Equation 3) is not mathematically reasonable. For the revised version or resubmission, I would suggest the authors re-think or re-write this part. ","Comments: 1. The proposed method is a model acceleration technique rather than model compression technique (which reduce the model parameters or size). Instead of model compression techniques, I believe other acceleration techniques such as [1-2] should be cited and discussed.
[1]: Dynamic Early Exiting for Accelerating BERT Inference [2]: BERT Loses Patience: Fast and Robust Inference with Early Exit Questions: 1. How do the authors form clusters for tokens in Figure 1? It's unclear in Section 3.
2. In the experiments, did the authors preserve the [CLS] token for other baseline methods (i.e., Att, Rand)?
Typos: line 304: \mathcal{X} \cdot \mathcal{Y} ==> \mathcal{X} \times \mathcal{Y} line 309: |S| = \ell_j ==> |S_j| = \ell_j line 323: \mathcal{L}(x, y, \tilde{S}, S] ==>\mathcal{L}(x, y, \tilde{S}, S)] ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. In the experiment on GLUE, there is no comparison of the proposed core-set based method with a simple local-pooling based method. Since this paper is inspired and follows this line of previous work, such a comparison is necessary.
2. The derivation of the optimization objective is a bit messy (Equation 1-3). For example, the model weights $\mathbf{w}$ are important optimization variables, but they are totally omitted in the derivation. Therefore, the derived objective for token selection (i.e., Equation 3) is not mathematically reasonable. For the revised version or resubmission, I would suggest the authors re-think or re-write this part. 
Comments: 1. The proposed method is a model acceleration technique rather than model compression technique (which reduce the model parameters or size). Instead of model compression techniques, I believe other acceleration techniques such as [1-2] should be cited and discussed.
[1]: Dynamic Early Exiting for Accelerating BERT Inference [2]: BERT Loses Patience: Fast and Robust Inference with Early Exit Questions: 1. How do the authors form clusters for tokens in Figure 1? It's unclear in Section 3.
2. In the experiments, did the authors preserve the [CLS] token for other baseline methods (i.e., Att, Rand)?
Typos: line 304: \mathcal{X} \cdot \mathcal{Y} ==> \mathcal{X} \times \mathcal{Y} line 309: |S| = \ell_j ==> |S_j| = \ell_j line 323: \mathcal{L}(x, y, \tilde{S}, S] ==>\mathcal{L}(x, y, \tilde{S}, S)] "
ARR_2022_211_review,"The authors present a method for implementing transformers for long sequences.
The method is based on the finding that tokens in deeper layers of a transformer tend to have similar embeddings. This means that in cases where only the CLS token is used (classification or sentence embedding) tokens can be pruned from the deeper layers without much decrease in model quality.
The algorithm used for pruning the tokens is called core-set token selection.
The paper has a theorem that states that the loss is bound by the delta-cover core token selection algorithm in that the loss goes to zero as delta goes to zero.
However, it remains somewhat unclear what this means in theory and practise. 
What guarantees does this give us in theory? 
How does the actual implementation differ from the theoretical algorithm? 
It seems that two potentially important aspects differ: That is the authors use fine-tuning instead of weighing the self attention and use a greedy approximation for the k-Center problem.
In general, I find the theorem hard to follow for an average reader of an ACL-like conference. 
Basic concepts such as lambda-Lipschitz should be introduced at least with the basic idea.
Some parts could also be justified a bit more. For example, the phrase “and the generalization error of models like BERT is known to be small” should be justified with a citation or some explanation.
The method is evaluated using the GLUE and LRA benchmarks.
For GLUE, the author shows that their method out-performs various baselines at the same speed-up factor and stays behind 1.5 points in accuracy of an unpruned Bert-base model.
The authors also show that they can achieve the same average performance as the unpruned baseline at a 1.5X speed up.
Some ensemble statistics (interquartile range or std deviation) would have been good in the tables to give the reader some idea of what the meaningful differences are in the experiments. 
On LRA, the authors show that their method outperforms some of the baselines on three of the LRA tasks. Their method stays behind the unpruned model by 0.6 points in accuracy when using Big Bird and out-performs the unpruned baseline when using a Performers model.
The authors acknowledge that speed ups in these cases are insignificant because of the shallow nature of the networks used.
This makes one wonder if LRA is indeed a good evaluation setup for their method.
There is also a sentence that was a bit unclear to me:  L555: “Thus, we only reduce sequence length in the input layer, which is before the first encoder”. Is the statement only true for the baselines or also for the core set selection algorithm? ",- The paper is overall well written and well motivated - Novel interesting method in a relevant area of NLP - Strong experimental results on GLUE (1.5X speed-up at no loss in quality (on average) and higher speed ups at reasonable drops in quality) - Theoretical justification of the method ,"- A bit unclear if LRA is actually a good evaluation setup for the method (no speed up are shown and the models used are too shallow) - The presentation of the theorem will be hard to follow for the average audience of an ACL conference. Key concepts should be introduced  - Unclear what the theorem means in theory and practice (what does the theorem gives us in practical terms, is it still valid in practice given the differences of the actual implementation) ",- Introduce key concepts in the discussion of the theorem - Explain what the theorem gives us in practical terms - Justify the differences from theoretical algorithm and practical implementation ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- A bit unclear if LRA is actually a good evaluation setup for the method (no speed up are shown and the models used are too shallow) - The presentation of the theorem will be hard to follow for the average audience of an ACL conference. Key concepts should be introduced  - Unclear what the theorem means in theory and practice (what does the theorem gives us in practical terms, is it still valid in practice given the differences of the actual implementation) 
- Introduce key concepts in the discussion of the theorem - Explain what the theorem gives us in practical terms - Justify the differences from theoretical algorithm and practical implementation "
ARR_2022_211_review,"In this paper, the authors proposes a new intermediate vector selection algorithm for BERT models, attempting to reduce the inference latency. The proposed core-set selection algorithm works right before the FF layer, which reduces the number of vectors in the input set according to a pre-defined length configuration. The intuition of of the algorithm is similar to DBSCAN with iterations, it starts with a core set containing only CLS embedding. Then it expands the core set by selecting m closest vectors to the core set. The expansion ends when the number of core-set reaching a certain number.
In experiments, the authors compare with the attention based method and first K selection method. The experiment results show that by fixing the speedup ratio at 3x, the proposed core-set selection algorithm shows advantage over other competitors.
Overall, my opinion leans toward an acception. ",- This authors put significant amount of content on theoretical justification for the proposed core-set selection algorithm - Results on LRA test set demonstrate that the proposed method can be used to further reduce the space complexity with big bird or performers ,"- Given this research topic, my first question would be: what is the upper bound / oracle performance for ""Select"", say, at the 3X speed up. If we have an oracle selection algorithm, can the performance be fully recovered?  In Table 1, the base average performance is 81.5%, and CS-opt achieves 80%. If the oracle performance is 80%, then we know a performance drop is inevitable, and this ""Select"" problem is fully solved by this paper. If the oracle performance is still 81.5%, then we can think there is still potential for further improvement. To me, such an study gives most valuable insights for understanding the evaluation results.
- In table 1 and table 2, we can see CS-k-1 has a very close performance comparing to CS-opt. The only evaluation datapoints where CS-k-1 is significantly behind CS-opt is the SST-2 task in Table 1. However, if we go to Table 2, there is no difference between CS-k-1 and CS-opt results on SST-2. If the drop in Table 1 is caused by noise, then indeed CS-k-1 can be a much easier and simpler algorithm in terms of implementation.
- In Table 4, the dataset that we can see significant gap between CS-opt and CS-k-1 is CIFAR-10 sequence-based classification task. However, I don't think it's proper to draw any key conclusion based on such a non-standard task. It itself is still a good toy task.
- Overall, the intuition of the algorithm is to keep vectors close to the CLS embedding. I'm worrying that in some complex problems (e.g, QA domain), it might be really difficult for the model to determine the CLS embedding until the last layer. Selection in the first layer based on a noisy CLS embedding may cause the performance to degrade significantly. ","Line 247 ""However, if two or more tokens are exact duplicates of each other, then one can easily remove the duplicates from the input and modify the self-attention appropriately to get the same CLS embedding at the top layer, and hence the same prediction.""
Is this true? As the transformer has positional embedding and the self-attention layer is multi-head. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Given this research topic, my first question would be: what is the upper bound / oracle performance for ""Select"", say, at the 3X speed up. If we have an oracle selection algorithm, can the performance be fully recovered?  In Table 1, the base average performance is 81.5%, and CS-opt achieves 80%. If the oracle performance is 80%, then we know a performance drop is inevitable, and this ""Select"" problem is fully solved by this paper. If the oracle performance is still 81.5%, then we can think there is still potential for further improvement. To me, such an study gives most valuable insights for understanding the evaluation results.
- In table 1 and table 2, we can see CS-k-1 has a very close performance comparing to CS-opt. The only evaluation datapoints where CS-k-1 is significantly behind CS-opt is the SST-2 task in Table 1. However, if we go to Table 2, there is no difference between CS-k-1 and CS-opt results on SST-2. If the drop in Table 1 is caused by noise, then indeed CS-k-1 can be a much easier and simpler algorithm in terms of implementation.
- In Table 4, the dataset that we can see significant gap between CS-opt and CS-k-1 is CIFAR-10 sequence-based classification task. However, I don't think it's proper to draw any key conclusion based on such a non-standard task. It itself is still a good toy task.
- Overall, the intuition of the algorithm is to keep vectors close to the CLS embedding. I'm worrying that in some complex problems (e.g, QA domain), it might be really difficult for the model to determine the CLS embedding until the last layer. Selection in the first layer based on a noisy CLS embedding may cause the performance to degrade significantly. 
Line 247 ""However, if two or more tokens are exact duplicates of each other, then one can easily remove the duplicates from the input and modify the self-attention appropriately to get the same CLS embedding at the top layer, and hence the same prediction.""
Is this true? As the transformer has positional embedding and the self-attention layer is multi-head. "
ARR_2022_211_review,"This paper proposes a novel pyramid-BERT to achieve the sequence length reduction across different encoder layers, which benefits the memory and decoding time reduction for downstream classification and ranking tasks. This method is based on the core-set based token selection method which is justified by theoretical results. Experiments on GLUE benchmarks and Long Range Arena datasets demonstrate the effectiveness of the proposed method. ","- This paper designs a novel speedup method with core-set based token selection which is justified by theoretical results.
- Nice experiment results on GLUE benchmarks and Long Range Arena datasets. ","- This paper lacks experiment comparisons with some very similar approaches, such as centroid transformers and representation pooling, although the authors claim that a thorough comparison is not required.
- The proposed method seems not to improve the training speed of traditional BERT in the pre-training stage. It means that we only apply this method in the downstream tasks. I wonder about the effectiveness of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). ","This paper is well written and easy to follow. The authors explore to speed up BERT by selecting core-set tokens and reducing the sequence length across different encoder layers. This idea is interesting, and the motivation is reasonable.  My main concern is the experiment comparisons with similar approaches, such as centroid transformers and representation pooling. Although these methods do not release the code, I think this comparison could strengthen the effectiveness of the proposed method. 
Besides, this method seems to only fit downstream classification and ranking tasks. Thus, I wonder about the performance of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). If we adopt the way of dynamic early exiting, the sequence length reduction across different layers seems to be marginal somehow.
The token selection algorithm used in the paper also reminds me of another way that combines the selective encoding method and Gumbel-Softmax for BERT.  Each token predicts the probability of surviving at each layer and adopt the Gumbel-Softmax to obtain the final token that feeds to the next layer.  This method may enhance the robustness of the proposed method, since it traverses more different combinations. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- This paper lacks experiment comparisons with some very similar approaches, such as centroid transformers and representation pooling, although the authors claim that a thorough comparison is not required.
- The proposed method seems not to improve the training speed of traditional BERT in the pre-training stage. It means that we only apply this method in the downstream tasks. I wonder about the effectiveness of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). 
This paper is well written and easy to follow. The authors explore to speed up BERT by selecting core-set tokens and reducing the sequence length across different encoder layers. This idea is interesting, and the motivation is reasonable.  My main concern is the experiment comparisons with similar approaches, such as centroid transformers and representation pooling. Although these methods do not release the code, I think this comparison could strengthen the effectiveness of the proposed method. 
Besides, this method seems to only fit downstream classification and ranking tasks. Thus, I wonder about the performance of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). If we adopt the way of dynamic early exiting, the sequence length reduction across different layers seems to be marginal somehow.
The token selection algorithm used in the paper also reminds me of another way that combines the selective encoding method and Gumbel-Softmax for BERT.  Each token predicts the probability of surviving at each layer and adopt the Gumbel-Softmax to obtain the final token that feeds to the next layer.  This method may enhance the robustness of the proposed method, since it traverses more different combinations. "
ARR_2022_104_review,"This paper presents 2 approaches to removing multiplication computation in the popular Transformer attention: 1) binary quantization of attention inputs, and 2) the use of L1 norm to compare between queries and keys. Experiments show that their approach can lead to lower energy consumption in the attention module with some losses in performance. For its weaknesses: 1. it utilizes quantization technology but does not compare with the other quantization approaches. 
2. there is still a softmax computation left in the presented approach. 
3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. 
4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. 
5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). 
6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. ","This paper presents 2 approaches to removing multiplication computation in the popular Transformer attention: 1) binary quantization of attention inputs, and 2) the use of L1 norm to compare between queries and keys. Experiments show significantly lower energy consumption with few losses in performance. ","1. it utilizes quantization technology but does not compare with the other quantization approaches. 
2. there is still a softmax computation left in the presented approach. 
3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. 
4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. 
5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). 
6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. ","It's recommended to report the overall energy reduction of the full model, conduct experiments on more language pairs and with the big setting on a few of them. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. it utilizes quantization technology but does not compare with the other quantization approaches. 
2. there is still a softmax computation left in the presented approach. 
3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. 
4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. 
5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). 
6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. 
It's recommended to report the overall energy reduction of the full model, conduct experiments on more language pairs and with the big setting on a few of them. "
ARR_2022_166_review,"The paper presents a new corpus with anaphoric relations, annotating recipes for coreference and bridging. The utility of the corpus is demonstrated by training a ML classifier. ",The new corpus can benefit others studying anaphora beyond identify. The paper is well written. ,"- Transfer learning does not look to bring significant improvements. Looking at the variance, the results with and without transfer learning overlap. ","Can you please clarify/expand the evaluation methodology, e.g., given gold anaphors, you only evaluate the antecedents? At least you should state why the standard evaluation metrics fail in this case, e.g., they cannot score plurals, i.e., split-antecedent references Typo: line 513 'pretrained' ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"- Transfer learning does not look to bring significant improvements. Looking at the variance, the results with and without transfer learning overlap. 
Can you please clarify/expand the evaluation methodology, e.g., given gold anaphors, you only evaluate the antecedents? At least you should state why the standard evaluation metrics fail in this case, e.g., they cannot score plurals, i.e., split-antecedent references Typo: line 513 'pretrained' "
ARR_2022_166_review,"The main point of the paper is a corpus of baking recipes annotated for coreference and bridging relations (both 1-to-1 and many-to-1 transformations of ingredients) and experiments to learn coreference resolution on this genre by transfer from either just unsupervised pretraining (GloVe, ELMo) or including supervised training on an existing larger chemical corpus and subsequent transfer using a state-of-the-art model for span linking coreference. The authors show that both joint training of coreference and bridging and transfer learning from the larger annotated corpus help the performance of the model. ",- the authors present a new corpus with coreference and bridging annotation which provides an interesting dataset for exploring this task - the authors provide a nice survey of the work especially concerning bridging corpora and resolution - the authors present some experiments showing that transfer learning can be beneficial for learning bridging resolution ,"- while the focus of the paper is on bridging resolution and the authors claim that transfer learning allows the model to incorporate procedural knowledge, this claim is not backed up by any kind of error analysis or qualitative examples that would show what the model learns exactly - the part that is novel wrt Feng et al 2021 is pretty much just the extension towards ingredient transformation and the transfer learning study (which is however less elaborate as the one in Xia&vanDurme) - while the models are reasonably state of the art (based on ELMo and in line with Lee et al 2018 and Feng et al 2021), newer research such as the Xia&vanDurme paper use more recent language models such as XLM-R as the base model, which may lead to better performance overall ","Xia and van Durme has been published at EMNLP 2021 and can be cited as such.
Section 4 413: ""For evaluation, we use precision, recall and F1"" - since MUC and B3 measures also define P/R/F1, it's good to mention here that it's P/R/F1 on individual (coreference or bridging) links. 
Section 6 555: ""Table 4 shows ..."" - the structure of Table 3 and Table 4 isn't very intuitive, since there is partial overlap in conditions and metrics but essentially it's one big collection of results (baseline, +joint, +transfer, +joint+transfer). It's not clear to me why the ""overall"" figure isn't computed based on the separate baseline classifiers ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- while the focus of the paper is on bridging resolution and the authors claim that transfer learning allows the model to incorporate procedural knowledge, this claim is not backed up by any kind of error analysis or qualitative examples that would show what the model learns exactly - the part that is novel wrt Feng et al 2021 is pretty much just the extension towards ingredient transformation and the transfer learning study (which is however less elaborate as the one in Xia&vanDurme) - while the models are reasonably state of the art (based on ELMo and in line with Lee et al 2018 and Feng et al 2021), newer research such as the Xia&vanDurme paper use more recent language models such as XLM-R as the base model, which may lead to better performance overall 
Xia and van Durme has been published at EMNLP 2021 and can be cited as such.
Section 4 413: ""For evaluation, we use precision, recall and F1"" - since MUC and B3 measures also define P/R/F1, it's good to mention here that it's P/R/F1 on individual (coreference or bridging) links. 
Section 6 555: ""Table 4 shows ..."" - the structure of Table 3 and Table 4 isn't very intuitive, since there is partial overlap in conditions and metrics but essentially it's one big collection of results (baseline, +joint, +transfer, +joint+transfer). It's not clear to me why the ""overall"" figure isn't computed based on the separate baseline classifiers "
ARR_2022_312_review,"This paper introduces coherence boosting, in which a weight is used at inference time to mix the log-probabilities generated by a language model on long- and short-term contexts.  The approach is evaluated on a large number of NLP tasks/datasets, with convincing results. ","- The idea is elegant in it’s simplicity, yet impactful.  This is reminiscent of “Improving Neural Language Models with a Continuous Cache” (https://arxiv.org/abs/1612.04426) which is also applied at inference time. 
	* The paper is very well written and easy to follow. 
	* Illustrations clearly demonstrate the problem and solution. 
	* The mathematical development of the model is concise and easy to follow -- enough detail is provide for re-implementation. 
	* State-of-the-art results are achieved on the LAMBADA dataset, and the approach is additive on a number of other tasks. 
	* A number of novel metrics are introduced. ","- There approach may have limitations w/r/t to model scale (e.g., \alpha approaching zero in Figure 2) -- although the approach is still additive on GPT-3 (one of the largest language models available) in Table 1. 
	* It is not clear that results are durable w/r/t to non-vanilla models (e.g., results in Table 4 are far short of SOTA on these tasks) -- is coherence boosting still additive for more sophisticated approaches. ","I liked this paper and believe that the NLP community would benefit from knowing about this simple, yet impactful approach.  It would be great to see if context boosting is still impactful versus more sophisticated approaches.  Two questions: 1.  Should Section 1.1 be Section 2.0 instead? 
2.  How does Table 3 in this paper relate to Tables 2 of Zhang 2020b (re: DigloGPT,Beam 345 seems to perform better)? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- There approach may have limitations w/r/t to model scale (e.g., \alpha approaching zero in Figure 2) -- although the approach is still additive on GPT-3 (one of the largest language models available) in Table 1. 
	* It is not clear that results are durable w/r/t to non-vanilla models (e.g., results in Table 4 are far short of SOTA on these tasks) -- is coherence boosting still additive for more sophisticated approaches. 
I liked this paper and believe that the NLP community would benefit from knowing about this simple, yet impactful approach.  It would be great to see if context boosting is still impactful versus more sophisticated approaches.  Two questions: 1.  Should Section 1.1 be Section 2.0 instead? 
2.  How does Table 3 in this paper relate to Tables 2 of Zhang 2020b (re: DigloGPT,Beam 345 seems to perform better)? "
ARR_2022_4_review,"This paper studies the research problem of Adversarial Attack on text data. Specifically, it focused on fooling stock prediction models over the Tweet data. It proposes a Hierarchical perturbation to generate attack and formulates the attack generation problem as a combinatorial optimization problem. Experiments are conducted on real world datasets and the results over 3 prediction models are comprehensive. ","S1. The research problem is important and useful in real world applications.
S2. The experiment design is good.
S3. The results are comprehensive. ",Technical contribution is not so significant. ,I am aware that this paper is a re-submission of a paper that I have reviewed before. This time the presentation improves a lot and more details are added as attachment. So I insist on my previous positive rating. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"
I am aware that this paper is a re-submission of a paper that I have reviewed before. This time the presentation improves a lot and more details are added as attachment. So I insist on my previous positive rating. "
ARR_2022_169_review,This paper deals with the Cross-Lingual Event Detection (CLED) task where only source language data is available for training but the model is then tested on a target language. The paper claims that their intuition is using unlabelled data in training (line 097-101). They use adversarial language adaptation (ALA) to learn language-invariant representations out of unlabeled bilingual text data. The main contribution of the paper is it proposes to use Optimal Transport (OT) for selecting samples for ALA training. Introducing ALA into CLED itself can be seen as a minor contribution. ,1. The paper proposes to leverage Optimal Transport (OT) to find similar samples. This is new knowledge to me. Previously I only know Optimal Transport (OT) or Wasserstein Distance is used in Wasserstein GAN (WGAN) which aligns real and fake image spaces. I did not see papers that use OT to search similar data samples before. I also appreciate the way that the paper draw samples in equation (3) and calculate p(s) and p(t) in 2.4.2 which is clever.  2. The main results in 3.2 looks promising. ,"1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure ""unlabelled"" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived.  To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels.  2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1.  3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples?  4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT.  5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that ""it would be beneficial for the LD to be trained with examples containing events"". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training. ","1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043. 
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method. 
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT. 
4. In equation (4), L2 distance is used. In OT,  earth mover's distance is more common. What is the benefit of L2 distance? 
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,N/A ,"1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure ""unlabelled"" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived.  To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels.  2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1.  3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples?  4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT.  5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that ""it would be beneficial for the LD to be trained with examples containing events"". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training. 
1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043. 
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method. 
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT. 
4. In equation (4), L2 distance is used. In OT,  earth mover's distance is more common. What is the benefit of L2 distance? 
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern. "
ARR_2022_12_review,"In this paper authors propose a neural singing voice beautifier (NSVB), which is based on conditional variational autoencoder (CVAE). It leverages shape-aware DTW (SADTW) for pitch correction, and latent-mapping for vocal tone improvement. NSVB supports semi-supervised learning, and a new dataset (PopBuTFy) is built. ","This paper is technically sound, and it's clear written overall, with good experimental design and ablation study presented. ",I feel the design of NVSB and some experimental results need more explanation (more information in the section below). ,"1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input? 
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below). 
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input? 
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI. "
ARR_2022_12_review,"This paper proposes a novel task called singing voice beautifying, the task of improving of quality of amuteur singing performance. It includes two subtasks, automatic pitch correction and vocal tone modification. To solve this challenge, they propose a conditional variational autoencoder with adversarial training to perform singing beautifying. During inference, they also propose shape-aware dynamitc time warping to better align parallel pitch contours. To validate the model, two datasets in both Chinese and English songs were collected from professional singers.  While most neural singing models focus on automatic pitch correction, the authors also extend their model to modify vocal tone and achieves good performance. This is a contribution to the field of modeling singing. The singing dataset in two languages, if released, will be helpful for singing researchers, as singing datasets are rare. In my opinion, this is a solid work.
The writing is mostly clear and concise. While some sections are technical and dense, they are accessible to researchers in speech processing in general. The model details are thoroughly described and the actual parameters can be found in source code. ","- The task is novel and the proposed solution is effective. Based on the evaluation results and the given audio samples, the proposed method significantly improves the quality of amuteur singing. Furthermore, the model has been evaluated in two languages, Chinese and English, strengthening their arguments.  - The authors have performed extensive evaluation using a variety of metrics, including both objective measures and subjective measures from trained singers.  - This study is open and transparent. The authors have provided code and singing samples, which is helpful for understanding and assessing the proposed method. They also promise to release the data later. ","I see no major weaknesses. But there are some minor issues that could be improved.
-  The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this. 
 - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results? ","- Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.  - The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be ""if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?"" ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,"The authors have provided a list of ethical considerations in the Appendix. Based on their provided answers, I do not see any ethical concerns. ","I see no major weaknesses. But there are some minor issues that could be improved.
-  The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this. 
 - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results? 
- Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.  - The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be ""if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?"" "
ARR_2022_79_review,"This paper augments the standard cross-modal retrieval framework with an additional codebook. The motivation is that with the codebook, the model's behavior can be interpretable. The proposed approach improves the performance over the baseline approach by a margin. ","1. I like the idea of using codebook for augmenting the cross-modal representation learning. Interpretabliity is one of the major issues in the cross-modal learning. I am glad that the author tackles this problem. 
2. The codebook update rules/policies are straightforward. It is interesting to observe that the codebook is aligned with actions in Fig. 3. ","1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework? 
2. It is unclear to me how the gradient back-prop in the equation of L165? 
3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size? 
4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity? 
5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding? ",I think this paper is well-written and easy to follow. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework? 
2. It is unclear to me how the gradient back-prop in the equation of L165? 
3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size? 
4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity? 
5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding? 
I think this paper is well-written and easy to follow. "
ARR_2022_76_review,"This paper addresses some key issues of the Entity Set Expansion (ESE) problem; from the evaluation metrics and dataset perspective. The authors mentioned that existing datasets do not consider entities that belong to multiple concepts, are non-named and vaguely attached to the concepts they belong to. They show how their benchmark datasets capture these properties. They also argue that MAE at top-20, which is a standard evaluation metric for ESE approaches, is not sufficient to understand their effectiveness. They propose MAE. at gold-k as an alternative. ",The paper proposes three benchmark datasets that are helpful for evaluating ESE approaches. The authors focus on three key issues of ESE datasets and address those in constructing their own datasets. ,The study is not well motivated. The paper does not consider the application perspective of ESE approaches and the arguments about evaluation metrics is not strong. The description of the dataset construction process is inadequate. ,"What is the motivation behind applying ESE methods on user-generated text? Why would someone apply set expansion approaches to construct a dictionary from customer reviews? For example, a customer might mention that the room type of a hotel is small. How does that help in constructing a dictionary for the room type concept?   “We first select concepts for the seed by referring to the features on the corresponding websites, to ensure their relevance for immediate downstream tasks.” - This is how the authors define the relevance for immediate downstream tasks. But, I worry if ESE has application in such scenarios. In fact, ESE is a useful mechanism when a concept is complicated enough to be expressed as a query. The concepts selected by the authors in this paper are well-defined by the site owners.  The argument about the new metric MAP at gold-k is not solid. From the retrieval perspective MAP at top-20 still makes sense. If someone is interested to retrieve all the entities belonging to a concept, they can look at the top-20 retrieved entities, annotate them, and then get more seeds to train a model to retrieve more. Based on this iterative process, one can find all the relevant entities belonging to a concept. There is a Total Recall track at the Text Retrieval Conference (TREC) that tries to achieve this. That’s why we should not disregard MAP at top-20, but we can explore the performance of a method at gold-k for reference. How does the map at gold-k makes sense from the application perspective? If an ESE approach returns a ranked list of entities, at which depth a user should go to find all the relevant entities? The user does not have any idea about gold-k and could only go to some pre-defined depth such as 20. Different depths could be explored considering how much effort a user is interested to put in, but MAP at top-20 should never be ignored.  “Existing evaluation metrics tend to overestimate the real-world performance of ESE methods and may be unreliable for evaluating concepts with large entity sets.” — I do not agree with this takeaway because of the above statement.  The effectiveness of ensemble approaches is not surprising.  The performance difference between CGExpan and LM-Base (proposed by the authors) is striking even if the underlying techniques are very similar. Can the authors provide more insight into this? Why does LM-base perform so well compared to CGExpan in user-generated datasets? I wonder if there could be hyperparameters for CGExpan that are not well-tuned.  I find the dataset description to be inadequate. Specifically, the source of the data is not clearly mentioned. ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The study is not well motivated. The paper does not consider the application perspective of ESE approaches and the arguments about evaluation metrics is not strong. The description of the dataset construction process is inadequate. 
What is the motivation behind applying ESE methods on user-generated text? Why would someone apply set expansion approaches to construct a dictionary from customer reviews? For example, a customer might mention that the room type of a hotel is small. How does that help in constructing a dictionary for the room type concept?   “We first select concepts for the seed by referring to the features on the corresponding websites, to ensure their relevance for immediate downstream tasks.” - This is how the authors define the relevance for immediate downstream tasks. But, I worry if ESE has application in such scenarios. In fact, ESE is a useful mechanism when a concept is complicated enough to be expressed as a query. The concepts selected by the authors in this paper are well-defined by the site owners.  The argument about the new metric MAP at gold-k is not solid. From the retrieval perspective MAP at top-20 still makes sense. If someone is interested to retrieve all the entities belonging to a concept, they can look at the top-20 retrieved entities, annotate them, and then get more seeds to train a model to retrieve more. Based on this iterative process, one can find all the relevant entities belonging to a concept. There is a Total Recall track at the Text Retrieval Conference (TREC) that tries to achieve this. That’s why we should not disregard MAP at top-20, but we can explore the performance of a method at gold-k for reference. How does the map at gold-k makes sense from the application perspective? If an ESE approach returns a ranked list of entities, at which depth a user should go to find all the relevant entities? The user does not have any idea about gold-k and could only go to some pre-defined depth such as 20. Different depths could be explored considering how much effort a user is interested to put in, but MAP at top-20 should never be ignored.  “Existing evaluation metrics tend to overestimate the real-world performance of ESE methods and may be unreliable for evaluating concepts with large entity sets.” — I do not agree with this takeaway because of the above statement.  The effectiveness of ensemble approaches is not surprising.  The performance difference between CGExpan and LM-Base (proposed by the authors) is striking even if the underlying techniques are very similar. Can the authors provide more insight into this? Why does LM-base perform so well compared to CGExpan in user-generated datasets? I wonder if there could be hyperparameters for CGExpan that are not well-tuned.  I find the dataset description to be inadequate. Specifically, the source of the data is not clearly mentioned. "
ARR_2022_218_review,"The paper measures the out-of-domain faithfulness of post-hoc explanations and the explanations of select-then-predict models. For post-hoc explanations, results show that in many cases, sufficiency and comprehensiveness are better for out-of-domain than for in-domain explanations. This leads to the suggestions,that these models generalize better than other models. Select-then-predict models also perform surprisingly well in out-of-domain settings. However, the authors find that using sufficiency and comprehensiveness for evaluation of faithfulness can be misleading. They hence suggest comparing the faithfulness of post-hoc explanations to a random attribution baseline to ensure robustness. ","The paper is well written and easy to understand. It follows a logical structure which makes it easy to follow the experiments and line of argumentation. There are only few grammatical mistakes. A very detailed evaluation is given for both normalized sufficiency and normalized comprehensiveness, as well as full-text F1. The models were trained and evaluated on multiple datasets and domains to ensure unbiased results. A zip file has been attached to reproduce results. The file contains Python code, along with required packages and a detailed README.md. References are given where necessary. All experiments are well explained, and details for datasets, models, and analyses are given in the appendix. ","There are quite a few punctuation errors and grammatical errors (l. 41, 137, 457), as well as orthographic inconsistencies (e.g., versions of “full-text” l. 95, 343, Table 3, 459, etc.). Some of the findings are printed in italic letters in an inconsistent way (e.g., 391-396, 427-438, 555-561, but not 567-581), also, this makes reading a little inconvenient. Spaces between tables, formulas, and text are also inconsistent (e.g., l. 285, 372). ","Please correct the punctuation errors and review the consistency of words (orthography with or without dashes, upper and lowercase letters). Remove italic conclusions to ensure better readability. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"There are quite a few punctuation errors and grammatical errors (l. 41, 137, 457), as well as orthographic inconsistencies (e.g., versions of “full-text” l. 95, 343, Table 3, 459, etc.). Some of the findings are printed in italic letters in an inconsistent way (e.g., 391-396, 427-438, 555-561, but not 567-581), also, this makes reading a little inconvenient. Spaces between tables, formulas, and text are also inconsistent (e.g., l. 285, 372). 
Please correct the punctuation errors and review the consistency of words (orthography with or without dashes, upper and lowercase letters). Remove italic conclusions to ensure better readability. "
ARR_2022_14_review,"This paper proposes a framework for training and extract-the-generate model for the long document summarization task. The premise is that the extractor should pass on important information from the source to the generator for producing abstractive summaries. To train the entire network, there have been three losses defined (generator, oracle, and consistency), of which generator is the decoder loss, oracle is used to optimize the extractor with oracle labels, consistency is used to marginalize dynamic attention with extractor distribution. Results are promising on two long summarization datasets (GovReport and QMSum) and competitive on arXiv. ","-The paper is easy-to-follow and understandable in most parts. 
-The problem is well-approached, although it has not been motivated much in the paper. 
-Results outperform the prior SOTA by a large margin on two long datasets (GovReport and QMSum). ","-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.  - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. ","In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.  - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. "
ARR_2022_314_review,"To analyze the attribution from a token to another token in transformer networks more accurately than previous studies, the authors proposed a new method named GlobEnc. Locally (for each layer), they extended the method of Kobayashi et al., 2021 by considering the contribution of residual connection #2 and layer normalization #2 (Figure 1). Globally (for the whole network), they corrected for the heuristic in the method of Abnar and Zuidema 2020.
Experimental results showed that the proposed method is a faithful index that correlates well with the gradient x input method (Kindermans et al., 2016). In addition, they confirmed that the vector norm-based method (Kobayashi et al. 2020) and the consideration of residual connection (Kobayashi et al., 2021), which were proposed as local analysis methods, contribute to global faithfulness. They also pointed out that considering only layer normalization #1 (Kobayashi et al., 2021) is insufficient, and layer normalization #2 should be considered simultaneously. ","1. Transformer networks are now a fundamental tool used not only in NLP but also in a wide range of data sciences, and improving analysis methods is an important research direction. 
1. Well written and easy to understand 1. Clearly mentioned the connections to related literatures 1. Thorough comparison with abundant previous methods in Experiments 1. Provide the insightful result that two Layer normalization in the layer counteract each other. ","1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa). 
1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results? 
1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well? 
1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks. 
    - Title: ""Incorporating the whole encoder layer""     - l.08:  ""incorporates all the components"" ","- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.
- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa). 
1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results? 
1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well? 
1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks. 
    - Title: ""Incorporating the whole encoder layer""     - l.08:  ""incorporates all the components"" 
- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.
- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving. "
ARR_2022_314_review,"The paper introduces a token attribution analysis method for transformer based models building upon previous norm-based and rollout aggregation methods. The proposed approach incorporates all the components of the encoder block except feed forward layers to compute contribution of each token in a sentence. The work provides a detailed quantitative analysis of each component's role, layer-wise evaluation and achieves better results than previous methods on three classification datasets. ","1. Paper is well written and easy to follow in general.
2. Related work is adequately covered.
3. The background section builds the analysis method quite well citing previous methods wherever necessary.
4. The major strength and novelty of the paper lies in the detailed analysis, analyzing role of each component and comparison with weight-based, norm-based and gradient based methods. Sub-sections in section 4.5 can be quite relevant for the interpretability research area in general and relevant downstream application areas. ","1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.
2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020)) ","1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?
2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is ""incorporates all components"", line 73 also says the ""whole encoder"", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.
3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.
4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.
2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020)) 
1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?
2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is ""incorporates all components"", line 73 also says the ""whole encoder"", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.
3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.
4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research. "
ARR_2022_314_review,"Motivated by the assumption that all the components in the encoder block, in general, are meaningful for token attribution analysis, this work proposes a GlobEnc, which incorporates all the components in the encoder block in the model. ","- The proposed method, GlobEnc, expands the scope of analysis from attention block in Transformers to the whole encoder.
- This paper shares intriguing insights from various experiments.
- Writing quality is high and validates the proposed method on many different criteria to investigate the role of each component in the encoder. ","- In token attribution analysis task, most researchers will intuitively agree that self-attention plays a primary role in Transformer architecture. However, why is the sub-network on top of the self-attention block important, and what role the authors expect from them are not well-motivated. A qualitative experiment comparing the token attribution analysis with and without subsequent sub-network (i.e., FFN, residual connection #2, and output LN) would be helpful to elucidate the role of remaining components other than self-attention block.
- The work appears to be incremental: As mentioned in the paper, Kobayashi et al. (2020, 2021) proposed the analysis method leveraging attention block. While the proposed method extends the existing approach to the entire encoder block, GlobEnc seems to reuse the tactics used in the previous method without any distinctive improvement. ","- In Equation 6, it would be better to note what γ and ⊙ (element-wise product) represent.
- In Figure 1, it would be better to compare the attribution map of GlobEnc with the attribution map of other comparable methods (e.g., N_{res}). ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- In token attribution analysis task, most researchers will intuitively agree that self-attention plays a primary role in Transformer architecture. However, why is the sub-network on top of the self-attention block important, and what role the authors expect from them are not well-motivated. A qualitative experiment comparing the token attribution analysis with and without subsequent sub-network (i.e., FFN, residual connection #2, and output LN) would be helpful to elucidate the role of remaining components other than self-attention block.
- The work appears to be incremental: As mentioned in the paper, Kobayashi et al. (2020, 2021) proposed the analysis method leveraging attention block. While the proposed method extends the existing approach to the entire encoder block, GlobEnc seems to reuse the tactics used in the previous method without any distinctive improvement. 
- In Equation 6, it would be better to note what γ and ⊙ (element-wise product) represent.
- In Figure 1, it would be better to compare the attribution map of GlobEnc with the attribution map of other comparable methods (e.g., N_{res}). "
ARR_2022_2_review,"This paper introduces a new dataset and a different problem setting for automatic fact-checking. The authors argue that it makes more sense to formulate automatic fact-checking as an entailment problem in which the veracity of the claim is evaluated against existing evidence. Authors demonstrate that by modifying a recent dense passage retrieval method proposed by Karpukhin et. al. (2020), they can improve over a TF-IDF baseline. ","Authors offer a more realistic problem setting for automated fact-checking using existing data sources.
Evaluation framework (prequential evaluation) used in this study is the more appropriate evaluation framework that takes into account the temporal aspect of fact-checking, than static accuracy and F1 scores. Aside from evaluation, the paper also follows a sound research design in general. ","This dataset is very similar to the FEVER formulation of fact-checking, it's just data from a different and more noisy domain, which is of course a more realistic problem setting for automated fact-checking. The approach the authors propose is still useful but not very novel.  At least some version of this data has been previously released in other publications. See https://aclanthology.org/2021.acl-short.86/ , https://aclanthology.org/2020.emnlp-main.623/ .
Briefly at the end of the abstract and In 5.1 (Stage-1 Results) in detail, you mention that your dense passage retrieval approach improves over a TF-IDF baseline. Why is that improvement significant enough to be published? Since you are only comparing with a simple baseline, It is difficult to tell how much improvement is good improvement.
The negative pairs used for training the dense passage retrieval model appear to be selected at random. This presents a known issue about negative sampling into the problem setting: because it's easier to distinguish negative example pairs than positive ones, your final model ends up learning to do the easier task. Since learning the discrepancy between irrelevant and randomly selected <claim, article sentence> pairs is much simpler than learning the similarity between relevant <claim, article sentence> pair, the model ends up not learning very much. ","How do you handle false negatives in your problem formulation? I couldn't find any references to automatic or manual deduplication of the claims in your data, and this data clearly may contain different versions of the same claim fact-checked multiple times. How do you make sure that the false negatives in your dataset do not disturb your evaluation? i.e., what if model A is performing better than model B in your task, but because model A is picking more on those false negatives and you only count performance based on true positives, you end up showing model B superior, where in reality model A is superior but your evaluation is noisy? ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"This dataset is very similar to the FEVER formulation of fact-checking, it's just data from a different and more noisy domain, which is of course a more realistic problem setting for automated fact-checking. The approach the authors propose is still useful but not very novel.  At least some version of this data has been previously released in other publications. See https://aclanthology.org/2021.acl-short.86/ , https://aclanthology.org/2020.emnlp-main.623/ .
Briefly at the end of the abstract and In 5.1 (Stage-1 Results) in detail, you mention that your dense passage retrieval approach improves over a TF-IDF baseline. Why is that improvement significant enough to be published? Since you are only comparing with a simple baseline, It is difficult to tell how much improvement is good improvement.
The negative pairs used for training the dense passage retrieval model appear to be selected at random. This presents a known issue about negative sampling into the problem setting: because it's easier to distinguish negative example pairs than positive ones, your final model ends up learning to do the easier task. Since learning the discrepancy between irrelevant and randomly selected <claim, article sentence> pairs is much simpler than learning the similarity between relevant <claim, article sentence> pair, the model ends up not learning very much. 
How do you handle false negatives in your problem formulation? I couldn't find any references to automatic or manual deduplication of the claims in your data, and this data clearly may contain different versions of the same claim fact-checked multiple times. How do you make sure that the false negatives in your dataset do not disturb your evaluation? i.e., what if model A is performing better than model B in your task, but because model A is picking more on those false negatives and you only count performance based on true positives, you end up showing model B superior, where in reality model A is superior but your evaluation is noisy? "
ARR_2022_2_review,"The paper provides a benchmark dataset that can be used for training & evaluation of automated fact checking systems. The major contribution of this paper is that they provide a large collection of 33,697 claims with associated review articles and premise articles. In the experiment, this work presents a two-stage detection framework, including evidence sentence extraction and claim veracity inference.  LSTM-based baselines and RoBERTa-based baselines are included and compared. ","1. The idea of using premises articles for claim inference in automated fact checking is interesting. 
2. The paper is overall well-structured and the methods are explained clearly. ","1. The methods are not novel as they are largely borrowing from existing work. 
2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. 
3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. 
4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. ","1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. 
2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? 
3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The methods are not novel as they are largely borrowing from existing work. 
2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. 
3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. 
4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. 
1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. 
2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? 
3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. "
ARR_2022_2_review,"The paper introduces a new fact-checking dataset, containing claims from a variety of fact-checking websites. No annotations are performed. Instead, the dataset combines each claim with its review written by a human fact-checker (working for the respective fact-checking site) and all background articles mentioned in the review. The main task to be solved using the dataset is claim verification based on background articles. The full dataset contains over 30k claims.
The authors experiment with various baselines models for solving this task in a two-step approach. The first is sentence retrieval with the goal to retrieve those sentences from all background articles that belong to background articles of a claim. The authors compare TF-IDF with a BERT architecture, where the latter shows better performance. In a second step, the top retrieved sentences are used for veracity prediction. Here, a Bi-LSTMs, hierarchical attention networks, and RoBERTa are compared, with the RoBERTa model outperforming the others.
The authors also show that there is a time-dependent topic shift in the dataset, which affects performance. That is, if the model was to be applied to new data collected in the future, a drop in performance is to be expected due to the topic shift. ","-	A large new dataset for fact-checking is introduced that will be made publicly available. 
-	A two-step approach with various different baseline models is evaluated on the new dataset. 
-	An analysis regarding data-shift is performed, which is very relevant for the real-world application of models trained on the dataset. 
-	The paper is well written and easy to follow. ","-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? ","-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? 
-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. "
ARR_2022_275_review,"This paper proposes a new pinyin input method with Chinese GPT. 
Their method tries to output Chinese characters given perfect pinyin, which consists of both initials and finals, or abbreviated pinyin, which only uses characters' initials. 
Abbreviated pinyin input is user-friendly since the number of typing characters is fewer. Still, it is a more complex problem since the model has to search for the correct output from more candidates than perfect pinyin.
Their method uses Chinese GPT and adapts it by concatenating both contexts of Chinese characters and pinyin (PinyinGPT-Concat) or by using the embeddings of characters and pinyin (PinyinGPT-Embed). 
They also construct a dataset called WD Dataset for evaluating their input methods.
Their experiments show that the Chinese GPT given perfect pinyin can output the correct characters even if the model is not fine-tuned for the task and surpasses previous baselines. 
However, their analysis shows that the vanilla GPT does not work well with the abbreviated pinyin. 
Their proposed PinyinGPT-Concat method surpasses the vanilla GPT baseline on abbreviated pinyin settings. ","1. They adapt Chinese GPT for the pinyin input method and the results show their method works well compared to the baselines.
2. They created a new pinyin IME evaluation dataset called WD Dataset and it would be useful for further researches. ","1. Their experiments compared methods trained with different datasets and different model parameters. It is still difficult to distinguish this model architecture is nice, or the larger dataset/parameters are good.
2. They only compare the previous method in the prefect pinyin settings though several works on abbreviated pinyin IMEs are mentioned in the related work section. 
It would be better to add a comparison of these.
3. I could not fully understand why people want higher pinyin IME accuracy with full use of V100 GPU. It might be possible to run this on the standard laptops in the future, though. ","l.299, please add the explanation of the abbreviation of ""precision at top-K""-> ""precision at top-k (P@K)"". ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No ethical concerns. ,"1. Their experiments compared methods trained with different datasets and different model parameters. It is still difficult to distinguish this model architecture is nice, or the larger dataset/parameters are good.
2. They only compare the previous method in the prefect pinyin settings though several works on abbreviated pinyin IMEs are mentioned in the related work section. 
It would be better to add a comparison of these.
3. I could not fully understand why people want higher pinyin IME accuracy with full use of V100 GPU. It might be possible to run this on the standard laptops in the future, though. 
l.299, please add the explanation of the abbreviation of ""precision at top-K""-> ""precision at top-k (P@K)"". "
ARR_2022_275_review,"This paper explores adapting the Chinese GPT for pinyin input.  Their task settings are that the input of the model includes a sequence of Chinese characters as the context and a sequence of pinyin (perfect pinyin or abbreviated pinyin), and the output is a sequence of Chinese characters. The number and the pronunciation of output characters should be consistent with the input pinyin, and the meaning of output should fit the input context.
To adapt the Chinese GPT for pinyin input, they propose two methods. The first one is concatenating pinyin input to the context of  Chinese characters. The second one is adding a pinyin embedding layer at the bottom of GPT. Since the model needs to select the best one from characters pronounced with the same pinyin in the inference stage, they propose pinyin-constrained training.
For datasets, they use the common benchmark PD dataset for training and evaluation. They also propose a new WD dataset that contains 15 domains. Their results show that their approach improves the performance of abbreviated pinyin across all domains. They also conducted a series of additional experiments to understand the importance of pinyin context and pinyin constrained training. ","This paper is well-written and well-organized; the readers could easily understand their methods and statements. The additional experiments are sufficient, which help the readers understand their method deeply. Moreover, the new evaluating dataset proposed in this paper is also very useful; it enables future work to evaluate their models across several domains. ","The main weakness is that the novelty of the proposed method is somewhat limited. The pinyin-enhanced pre-trained models and processing the abbreviated pinyin have existed in previous works, and this paper only adapted them on a new pre-trained model. ","I am not sure why the paper used a probability of 50% to sample a target sequence with less than five words (in line 318). Is that because people tend to input short sequences in real-world situations? If so, it would be better to explain the reasons. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weakness is that the novelty of the proposed method is somewhat limited. The pinyin-enhanced pre-trained models and processing the abbreviated pinyin have existed in previous works, and this paper only adapted them on a new pre-trained model. 
I am not sure why the paper used a probability of 50% to sample a target sequence with less than five words (in line 318). Is that because people tend to input short sequences in real-world situations? If so, it would be better to explain the reasons. "
ARR_2022_275_review,"This paper addresses a Chinese pinyin input method. Pinyin represents a pronunciation of a corresponding Chinese character. The Chinese pinyin input method converts an input pinyin sequence into a corresponding Chinese characters. This paper assumes two configurations for inputs. One receives all pinyin representations (perfect pinyin) and the other receives the initial characters of pinyin (abbreviated pinyin). 
This paper indicates that the GPT model trained on Chinese corpora achieved good performance on the perfect pinyin configuration. In addition, this paper proposes two input strategies of pinyin, and the one is useful in the abbreviation pinyin configuration when the authors fine-tuned GPT parameters with the input strategy. ","This paper reported state-of-the-art performance on the Chinese pinyin input method. 
Although the technical novelty is limited in my understanding, this paper might have a potential of good paper as a demo paper. ","In my understanding, this paper doesn't contain any novel findings essentially. 
This paper reports that a neural language model trained on a large amount of data can achieve better performance when we fine-tune the model on a target task. Recent studies [Peters et al., 18, Devlin et al., 19, Lewis et al., 20] have demonstrated that this pre-trained and fine-tuning strategy is effective in various tasks.
This paper applies the method to incorporate pinyin to the standard language modeling task, i.e., next word prediction task. This paper proposed two strategies. One is attaching the pinyin sequence to the input sequence. The other is combining the embeddings of pinyin with the input embeddings of characters. The former is a standard (straightforward) way, and the latter uses the idea in previous studies. For example, in abstractive summarization tasks, previous studies proposed the method which combines additional information such as length embeddings to embeddings of input tokens [Kikuchi et al., 16, Takase et al., 19].
In other words, this paper applies the exiting strategies to the Chinese pinyin input method but doesn't contain technical novelty.
Moreover, this paper addresses the abbreviation pinyin configuration but I doubt this configuration is well justified. 
This paper should describe whether human can solve this task because the abbreviation pinyin configuration seems too difficult. In addition, this paper should describe the importance of this configuration such as ""how frequent this configuration occurs"".
Peters et al., 18: Deep Contextualized Word Representations.
Devlin et al., 19: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
Lewis et al., 20: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
Kikuchi et al., 16: Controlling Output Length in Neural Encoder-Decoders.
Takase et al., 19: Positional Encoding to Control Output Sequence Length. ","The equation (2) is inconsistent with the task definition described in Section 2. 
Section 2 explains that the pinyin sequence starts from n+1 but the equation (2) uses pinyin sequence from 1 to n. To solve this inconsistency, the authors should prepare another variable such as N instead of using n+k in the equation (2). ",1.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"In my understanding, this paper doesn't contain any novel findings essentially. 
This paper reports that a neural language model trained on a large amount of data can achieve better performance when we fine-tune the model on a target task. Recent studies [Peters et al., 18, Devlin et al., 19, Lewis et al., 20] have demonstrated that this pre-trained and fine-tuning strategy is effective in various tasks.
This paper applies the method to incorporate pinyin to the standard language modeling task, i.e., next word prediction task. This paper proposed two strategies. One is attaching the pinyin sequence to the input sequence. The other is combining the embeddings of pinyin with the input embeddings of characters. The former is a standard (straightforward) way, and the latter uses the idea in previous studies. For example, in abstractive summarization tasks, previous studies proposed the method which combines additional information such as length embeddings to embeddings of input tokens [Kikuchi et al., 16, Takase et al., 19].
In other words, this paper applies the exiting strategies to the Chinese pinyin input method but doesn't contain technical novelty.
Moreover, this paper addresses the abbreviation pinyin configuration but I doubt this configuration is well justified. 
This paper should describe whether human can solve this task because the abbreviation pinyin configuration seems too difficult. In addition, this paper should describe the importance of this configuration such as ""how frequent this configuration occurs"".
Peters et al., 18: Deep Contextualized Word Representations.
Devlin et al., 19: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
Lewis et al., 20: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.
Kikuchi et al., 16: Controlling Output Length in Neural Encoder-Decoders.
Takase et al., 19: Positional Encoding to Control Output Sequence Length. 
The equation (2) is inconsistent with the task definition described in Section 2. 
Section 2 explains that the pinyin sequence starts from n+1 but the equation (2) uses pinyin sequence from 1 to n. To solve this inconsistency, the authors should prepare another variable such as N instead of using n+k in the equation (2). "
ARR_2022_265_review,"This paper investigates an interesting problem of automatically generating high-cognitive-demand educational questions for children’s storybooks. To this end, this paper proposes a three-stage framework: 1) learn to predict the question type distribution, 2) extract salient question-worthy events from the text, and 3) generate educational questions based on the outputs from steps 1) and 2). Both automatic and human evaluation validates the effectiveness of the proposed method. ","- A well-motivated task worthy of study by the QG community. It is practically meaningful to generate educational questions for children's storybooks and it could be applied to many real-world educational applications.  - The proposed framework is straightforward but logically sound.  - The authors conducted comprehensive evaluation, including the automatic evaluation and human evaluation.  - The paper is fairly easy to follow. ","- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. ","Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: ""We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth."" Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. 
  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. 
  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. 
Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: ""We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth."" Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. 
  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. 
  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. "
ARR_2022_170_review,"This paper introduced LimitedInk, a self-explaining model that outperforms the existing baseline on both end-task predictions and human-annotated rationale agreement. More importantly, the paper shows that unlike the finding from the previous studies, the shortest rationales can not present the best explanation for human understanding which call for more careful design on evaluating human rationales. ",The paper conducts a human study on rationales. The finding contradicts the typical intuition about rationales that the shortest rationales are the best to explain human understanding. This would encourage a fundamental rethinking and revolution of rationale research. ,I guess that is due to content limited. It would be better to see some qualitative analysis on why 10-30% rationales can already give decent performance whereas human needs at least 40% rationales. It seems to indicates that the downstream model is making predictions in a way that is very different from how human make predictions. ,I would suggest adding some qualitative analysis to explain the title of the paper in later version. ,4.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I guess that is due to content limited. It would be better to see some qualitative analysis on why 10-30% rationales can already give decent performance whereas human needs at least 40% rationales. It seems to indicates that the downstream model is making predictions in a way that is very different from how human make predictions. 
I would suggest adding some qualitative analysis to explain the title of the paper in later version. "
ARR_2022_170_review,"Existing self-explaining models mostly generate the short rationales with the assumption that short rationales are more intuitive to humans, while this work discusses the question that whether the shortest rationale is the most understandable for humans. In this work, the authors design a self-explaining model, LIMITEDINK, that can take controls on rationale length by incorporating contextual information and supporting flexibly extracting rationales at any target length. By generating rationales at different length levels, the authors study how much rationale would be sufficient for humans to confidently make predictions. Experiments on various tasks demonstrate that the proposed method outperforms most prior works, and meanwhile show that the shortest rationales are not the best for human understanding. ","1. The method proposed in this work is effective and can outperform several strong baselines on the performance of both label predictions and rationale predictions.
2. The problem, the effect of the rationales at different length levels, discussed in this work is meaningful and the conclusions may serve as good guidance for further research in this field. ","1. Although this work points out that shortest rationales are largely not the best for human understanding, the appropriate lengths are still subject to the datasets or even the instances. The length of meaningful rationales may largely depend on the density of the information related to the task. As pointed in Section 5, a more rigorous evaluation is needed to better understand what is a good rationale explanation.
2. This work does not report how ""short"" the rationales generated by prior works are. As shown in Section 1, recent works agree that good rationales should be ""shortest yet sufficient"", while this work seems to simply focus more on ""shortest"". This brings out the concern that whether the main question discussed in this work can really stand for the trend of current works on this task. 
         (a). I think one potential solution to handle this concern is that - by extending or shortening the golden rationales and see whether such perturbations outperform or underperform the original one. ","1. I would like to see some examples of the generated rationales at different length levels from the proposed methods, as well as the rationales generated by the baselines. Such examples can help the readers to better understand the influence of rationale lengths. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Although this work points out that shortest rationales are largely not the best for human understanding, the appropriate lengths are still subject to the datasets or even the instances. The length of meaningful rationales may largely depend on the density of the information related to the task. As pointed in Section 5, a more rigorous evaluation is needed to better understand what is a good rationale explanation.
2. This work does not report how ""short"" the rationales generated by prior works are. As shown in Section 1, recent works agree that good rationales should be ""shortest yet sufficient"", while this work seems to simply focus more on ""shortest"". This brings out the concern that whether the main question discussed in this work can really stand for the trend of current works on this task. 
         (a). I think one potential solution to handle this concern is that - by extending or shortening the golden rationales and see whether such perturbations outperform or underperform the original one. 
1. I would like to see some examples of the generated rationales at different length levels from the proposed methods, as well as the rationales generated by the baselines. Such examples can help the readers to better understand the influence of rationale lengths. "
ARR_2022_304_review,"The goal of the presented research was to improve results for NLP tasks requiring semantic understanding. Bert-like models while performing very well in many NLP tasks give much lower results when the differences in semantics, e.g. negation, are tested. Some solutions for improving handling negation were presented before, here the authors looked also into another semantics related task – synonyms and antonyms detection. They propose a new solution – intermediate training on tasks related to meaning matching.   The authors propose three tasks: masked knowledge retrieval on negated queries (MKR-NQ), masked word retrieval (MWR), and synonym/antonym recognition (SAR). In the first one, they negated the LAMA dataset  - for sentences with one verb the negation was added (or removed) and the different answers were picked.  For the MWR task, queries, like “happy is the synonym of [MASK]”, were used. The number of wrong predictions, i.e. words which are in a different relation to a  given word at the top of the list of the most probable predictions were counted.   In the last task, the relation between a  pair of words  - synonymy or antonymy was predicted. To create this set data from ConceptNet was used.  To illustrate the lack of information of negation and lexical semantics in  Bert models, all three tasks were first solved using several (BERT, RoBERTa, ALBERT, Electra in small and large versions) already trained models;  only for the SAR task the additional training was performed. In general, the results of all the models measured by the hit rate metrics were not good. It looked like the synonymy/antonymy relations cannot be retrieved.  For MKR-NQ and MWR tasks large models were generally worse than small ones. For the SAR task, fine-tuning improved the results significantly.  To encode the needed information in language models, the authors proposed the approach with the intermediate model training on the newly defined meaning matching task. The training set consisted of word-sentence pairs. The same three tasks were then solved using the resulting models and generally, there was a significant increase in the results for the large models. However,  for the Elektra model, the results were worse. 
 The authors also checked if the additional training step can negatively influence the results obtained on the other tasks. The results obtained for 8 tasks from the GLUE benchmark did not change much. So the authors conclude that their approach (IM2) is of benefit to learning lexical-semantic information and the meaning of negated expressions. ","The authors address the important problem of language models based on distributional hypothesis – words which occur in the same context may be of the same category but semantically significantly different, e.g. ‘cat’/ ‘dog’, ’long’/ ‘short’. It is also hard to differentiate negated and not negated sequences. They propose a method that helps in recognizing negation in a more efficient way than it was proposed in BERTNOT and at the same time helps in differentiating synonyms and antonyms.  The series of experiments are well planned and sufficiently described. ",The performance for some tasks and some models increased significantly but this is not true in all cases. The fact was explored by the authors who hypothesised that a leading cause is catastrophic forgetting where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task and confirmed that small models were changed more.  But it does not explain all results (like Bert small better than large even after intermediate training). The authors did also not comment on the difference of behaviour of BERT and Roberta models. ,One table with all the results - before and after intermediate training would help readers to observe differences. ,"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The performance for some tasks and some models increased significantly but this is not true in all cases. The fact was explored by the authors who hypothesised that a leading cause is catastrophic forgetting where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task and confirmed that small models were changed more.  But it does not explain all results (like Bert small better than large even after intermediate training). The authors did also not comment on the difference of behaviour of BERT and Roberta models. 
One table with all the results - before and after intermediate training would help readers to observe differences. "
ARR_2022_66_review,"This review is for a resubmission that I reviewed earlier. Hence, I will be focusing more on the changes from the last submitted version.
First of all, I would like to thank the reviewers for their detailed responses to my, and other reviewers', concerns.
For the previous version of this submission, I had two major concerns. First, one of the datasets used by the authors is not public. The authors have committed to releasing the dataset, pending approvals from the concerned organizations. I really appreciate the authors' willingness to do so and look forward to the valuable resource being available for the research community.
My other concern was that the job representations learned by the proposed approach were not used in downstream tasks that could illustrate the utility of the representations and how the representations fare with respect to other off-the-shelf representations (say using BERT). The authors have presented results on the next-job prediction task. ","- Well-written paper, tackles an interesting application - The authors will be making a new dataset public which would be a useful resource for the community ","I thank the authors for conducting additional experiments on testing the utility of learned job title representations for the next job prediction task. However, the results reported in the updated draft have reinforced my concerns regarding the effectiveness of the learned job title representations. As Table 3 shows, BERT-based representations outperform on the task when compared with representations learned by the proposed solution under various settings. Even on the classification task, the performance of BERT and Word2Vec methods is reasonably well. Thus, it is not clear what, if any, are the advantages of using the proposed solution for learning job title representations. Some gains in performance, in some settings, could certainly be achieved but at the cost of significantly more computationally expensive solution compared to using off-the-shelf BERT or W2V representations. ","In sum, I would keep my earlier scores unchanged. While I appreciate the efforts undertaken by the authors to test on a new task, the results, unfortunately, are not infavor of the proposed solution. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"I thank the authors for conducting additional experiments on testing the utility of learned job title representations for the next job prediction task. However, the results reported in the updated draft have reinforced my concerns regarding the effectiveness of the learned job title representations. As Table 3 shows, BERT-based representations outperform on the task when compared with representations learned by the proposed solution under various settings. Even on the classification task, the performance of BERT and Word2Vec methods is reasonably well. Thus, it is not clear what, if any, are the advantages of using the proposed solution for learning job title representations. Some gains in performance, in some settings, could certainly be achieved but at the cost of significantly more computationally expensive solution compared to using off-the-shelf BERT or W2V representations. 
In sum, I would keep my earlier scores unchanged. While I appreciate the efforts undertaken by the authors to test on a new task, the results, unfortunately, are not infavor of the proposed solution. "
ARR_2022_69_review,"Most behavioral LM analysis work focuses on introducing a particular challenge dataset, and only evaluates a handful of LMs on that dataset. This article reports on a much more extensive experiment: the authors evalute a relatively large number of models (all transformer-based). This make it possible to draw helpful comparative conclusions: for example, model size does not predict performance, and all models fail on some zero-shot tasks, including the ones included in oLMpics which to this human seem quite simple. It also shows that some pessimistic conclusions based on a single model (e.g. Ettinger's negation dataset) are not quite as severe for other models. Overall, it is hard to discern a clear predictor of success (model size, training data size, etc, do not have a consistent effect). ","The experiments and post-hoc analyses of the results are straightforward, well-motivated and clearly described. I would like to see this paper published. ","Nothing serious comes to mind. This isn't exactly a groundbreaking paper in its methodology, but it's important to have this sort of experimental empirical work in the literature. ","Another study that showed limited correlation between model size and ""zero-shot"" performance is Hu et al 2020 ACL, A Systematic Assessment of Syntactic Generalization in Neural Language Models, https://arxiv.org/abs/2005.03692 . More generally, the paper could engage a bit more with the syntactic evaluation literature, much of which temporally precedes the semantics and commonsense evaluations included in the current paper.
The related work section conflates approaches that train a classifier on a model's internal representations (such as Tenney et al) ""zero-shot"" targeted behavioral evaluations such as Ettinger's and Goldberg's that use the model's original objective (word prediction), calling both of them ""probing"". It would be helpful to be clearer about this distinction and clarify which analyses in the paper follow which approach.
One suggestion to the authors is to change the title to something that is less focused on BERT, captures some of the more noteworthy empirical findings, and avoid ""muppet"" as a generic term for transformer-based language models.
l. 331: ""same approach"" - which one? Both?
l. 439: that that -> that Section 4.7: Consider reminding the reader what the age comparison task is -- I got a little confused here and had to search for the PDF for word ""age"". ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Nothing serious comes to mind. This isn't exactly a groundbreaking paper in its methodology, but it's important to have this sort of experimental empirical work in the literature. 
Another study that showed limited correlation between model size and ""zero-shot"" performance is Hu et al 2020 ACL, A Systematic Assessment of Syntactic Generalization in Neural Language Models, https://arxiv.org/abs/2005.03692 . More generally, the paper could engage a bit more with the syntactic evaluation literature, much of which temporally precedes the semantics and commonsense evaluations included in the current paper.
The related work section conflates approaches that train a classifier on a model's internal representations (such as Tenney et al) ""zero-shot"" targeted behavioral evaluations such as Ettinger's and Goldberg's that use the model's original objective (word prediction), calling both of them ""probing"". It would be helpful to be clearer about this distinction and clarify which analyses in the paper follow which approach.
One suggestion to the authors is to change the title to something that is less focused on BERT, captures some of the more noteworthy empirical findings, and avoid ""muppet"" as a generic term for transformer-based language models.
l. 331: ""same approach"" - which one? Both?
l. 439: that that -> that Section 4.7: Consider reminding the reader what the age comparison task is -- I got a little confused here and had to search for the PDF for word ""age"". "
ARR_2022_112_review,"The paper introduces SkillSpan, the first open-source, expert-annotated dataset for skill extraction, which is essentially sequential labeling of skill and knowledge in job postings. Authors experimented with BERT-based models with various tweaks and found that pretraining language models on job postings yields better performance. ","- Commercial motivation for automatic skill extraction is understandable and open-sourcing an expert-annotated dataset thus will be beneficial for the community.
- This is a very solid paper with thorough description on the annotation and experiment processes.
- The paper is well written and easy to follow. ","- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
-  Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix. ","L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about ""Type of Skills"" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of ""skill"", ""knowledge"" and ""attitude"" correspond to ""hard"" and ""soft"" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider ""skill"" and ""knowledge"" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: ""Pre-training"" and ""pretraining"" are not spelled consistently. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
-  Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix. 
L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about ""Type of Skills"" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of ""skill"", ""knowledge"" and ""attitude"" correspond to ""hard"" and ""soft"" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider ""skill"" and ""knowledge"" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: ""Pre-training"" and ""pretraining"" are not spelled consistently. "
ARR_2022_208_review,"The authors present a method of calibrating learned multiclass classification models during training, that is, improving model calibration (in other words, pushing accuracy-versus-model-confidence graphs towards the identity line---well-calibrated models have confidence perfectly reflecting prediction accuracy).
The proposed method is done during training, rather than being a post-hoc model which recalibrates a learned model to maximize performance on a held-out calibration set. This has the benefit of allowing the model to use the whole train set (i.e. not requiring a held-out calibration set), in addition to ideally leveraging the entire train set to do calibration. In this sense it follows a small number of published methods.
Overall the core writeup of the method (sec 3) was difficult to follow. The core design decisions of the method were difficult to find the motivation of. I believe the decisions pivot around an increased sample-efficiency claim (line 316), but this claim was not stated precisely (the free variable $\epsilon$ bounds ""calibration error,"" which I do not believe is defined), and the claim does not have a proof, so the relationship between the system setup decisions and sample efficiency claims is not at all clear.
Generally, the calibrated systems' plots do not seem obviously notably superior to the baseline calibrations (Fig 2, Fig 1(top)), though the methods do indeed achieve superior ECE (expected calibration error) performance across some of the tasks (xSLUE, a suite of multiclass classification NLP tasks, Table 1) and, somewhat surprisingly, superior top-level accuracy/F1 (table 1).
Given the importance of proper calibration to so many applications of NLP, I suspect these methods would be of interest to practitioners in the field even if they do not provide an across-the-board calibration improvement, but it is not clear from the writeup when they work and when they do not.  That is, this writeup does not really address the question ""when should a practitioner use this very complex method of calibration, rather than the much conceptually simpler post-hoc Platt scaling, given that the curves in Fig 2 and Fig 1 seem to hint that the calibration benefits of this method seem to be very noisy."" Is there a property of the dataset or predictor where we can expect this method to really help?
Generally, I think with notable revisions this paper would be of interest to practitioners, but it is somewhat difficult to follow the method exactly and know how we should expect it to affect performance on a given novel task. ","- The authors present a novel calibration technique in multiclass classification setups that leverages the entire train set (rather than a held-out calibration set). This method learns both a Platt-scaling transform (an affine transform on logits, basically), in addition to iteratively adaptively binning the train set based on this learned transform, I believe so that we can calculate the ECE without a held-out set.
- The proposed methods perform superior to baselines with respect to calibration error on more tasks than they perform worse.
- The proposed modifications improve against the baseline MLE systems in terms of accuracy/F1 across many tasks; that is, the calibration term appears to act as an effective model regularizer in some setups. ","- The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in ""comments"" below, written as I read the presentation.
- There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example:   - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand? 
  - The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods? 
  - In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?
- Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw. ","- ""These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator"" this is a pretty over-the-top claim! "" Perfect balance?"" This doesn seem like isn't an ""ideal calibrator,"" it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable - tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other - What is Fig 2's dataset? the average across all the tasks? Just one task?
- The definition of perfect calibration in the info is perhaps a bit confusing as-is (namely, $P$ has to be a joint over the covariates $x$ and the predictions $f$ right so you need some sort of metric over both, is that right? Or perhaps you just require $f$ to be Borel-measurable and deterministic or something. ( Unrelatedly, I also realize that if $f$ is itself nondeterministic, then you probably need an expectation operator right?). Anyways, it might be helpful to add a short sentence to the text here explaining this eq in words, no need to be too precise.
- Is the output of step (3) differentiable? Do you need subgradients or something to get the loss? I guess this is why it's helpful to know exactly what $\beta$ is.
- A nit but i might use something other than ""distance"" to describe $d$ in line 231, since it's not in general a valid distance metric, maybe ""calibration mismatch"" or something.
- Also maybe change $q$ to $\hat q$ in the eq on line 232? Since the thing you're minimizing isn't the true value $q$, which we don't have access to even in principle.
- I'm a bit perplexed by the discrepancy between the matrix $Q$, which is essentially a function of the predictor network, and the $q$ in 232, which is not. Does $Q$ as given suffice to allow us to calculate arbitrary distance functionals between $p$ and $q$, as described in line 231?
- You haven't defined calibration error but instantiate it via $\epsilon$ in line 254. It's abs(p - q) to use the terminology of line (232), is that right? ECE is usually given with the expectation randomness integrating over the simplex $D_K$ right, do we have to do that here? Not sure if this is essential to get all the details precise here but probably it would be good to define what ""calibration error"" is at the least.
- Don't think you define what the Platt-scaling set $G$ is ranged over by the argmin in step 1.
- What is the set $\beta$ in line 311?
- It's not clear to me why step (3) of the algorithm should be necessary at all. It seems like this wouldn't change the calibration but allows us to estimate the ECE? Is that right? Or does this adaptive binning + discretization (essentially isotonic regression, right?) actually affect calibration in expectation?
- Line (7) of alg 1's pseucode is referenced in the text but the fig doesn't have line numbers, is it possible to add them ""the achieved reduction in ECE as compared to all baselines is significant"" what does this mean? Paired t-test? p < 0.05? Either describe the test or remove this significance claim - do you have a proof of the sample-efficiency claim in line 317? would be good to put this into an appendix - Is it possible to compare to MCDropout? It would be really nice to see that comparison, but this isn't crucial. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in ""comments"" below, written as I read the presentation.
- There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example:   - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand? 
  - The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods? 
  - In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?
- Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw. 
- ""These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator"" this is a pretty over-the-top claim! "" Perfect balance?"" This doesn seem like isn't an ""ideal calibrator,"" it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable - tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other - What is Fig 2's dataset? the average across all the tasks? Just one task?
- The definition of perfect calibration in the info is perhaps a bit confusing as-is (namely, $P$ has to be a joint over the covariates $x$ and the predictions $f$ right so you need some sort of metric over both, is that right? Or perhaps you just require $f$ to be Borel-measurable and deterministic or something. ( Unrelatedly, I also realize that if $f$ is itself nondeterministic, then you probably need an expectation operator right?). Anyways, it might be helpful to add a short sentence to the text here explaining this eq in words, no need to be too precise.
- Is the output of step (3) differentiable? Do you need subgradients or something to get the loss? I guess this is why it's helpful to know exactly what $\beta$ is.
- A nit but i might use something other than ""distance"" to describe $d$ in line 231, since it's not in general a valid distance metric, maybe ""calibration mismatch"" or something.
- Also maybe change $q$ to $\hat q$ in the eq on line 232? Since the thing you're minimizing isn't the true value $q$, which we don't have access to even in principle.
- I'm a bit perplexed by the discrepancy between the matrix $Q$, which is essentially a function of the predictor network, and the $q$ in 232, which is not. Does $Q$ as given suffice to allow us to calculate arbitrary distance functionals between $p$ and $q$, as described in line 231?
- You haven't defined calibration error but instantiate it via $\epsilon$ in line 254. It's abs(p - q) to use the terminology of line (232), is that right? ECE is usually given with the expectation randomness integrating over the simplex $D_K$ right, do we have to do that here? Not sure if this is essential to get all the details precise here but probably it would be good to define what ""calibration error"" is at the least.
- Don't think you define what the Platt-scaling set $G$ is ranged over by the argmin in step 1.
- What is the set $\beta$ in line 311?
- It's not clear to me why step (3) of the algorithm should be necessary at all. It seems like this wouldn't change the calibration but allows us to estimate the ECE? Is that right? Or does this adaptive binning + discretization (essentially isotonic regression, right?) actually affect calibration in expectation?
- Line (7) of alg 1's pseucode is referenced in the text but the fig doesn't have line numbers, is it possible to add them ""the achieved reduction in ECE as compared to all baselines is significant"" what does this mean? Paired t-test? p < 0.05? Either describe the test or remove this significance claim - do you have a proof of the sample-efficiency claim in line 317? would be good to put this into an appendix - Is it possible to compare to MCDropout? It would be really nice to see that comparison, but this isn't crucial. "
ARR_2022_92_review,"The general motivation of this paper is to provide tools that analyze pairs of review and rebuttal in the peer-review process of scientific work and assess Area Chairs in making sense of this discussion to reach better-informed decisions of acceptance or rejection. For this goal, the main contribution of this paper is a dataset of peer-reviews annotated with discourse labels and relations between the rebuttal and review pairs. The proposed discourse scheme attempts to unify several other discourse labels proposed in previous works. In my opinion, this scheme is complete and well organized on different levels of granularity. Although limited, the paper additionally presents some descriptive analyses of the collected annotations. ","- The task is relevant to the community, and efforts towards easing the life of area chairs are appreciated.
- The constructed discourse labels scheme is nicely detailed and complete.
- The annotated dataset could benefit future research on discourse analysis of the peer review process ","- The analysis section is limited and confusing in some parts: 	- Section 4.1 describes only the annotation results, without any further discussion of the implications.
	- In Section 4.2, based on Figure 2, the authors conclude that rebuttal sentences are not aligned with review sentences. However, what I see is completely different. The majority of review-rebuttal pairs happen to have a positive correlation of more than 50% unless I am missing something here!
	- Section 4.3 wasn't clear for me. I didn't understand how the figure supported the claim that authors often interpret reviews in a way that supports their argumentative goals. ","- There is missing work of Cheng et al. 2021 (Argument Pair Extraction via Attention guided Multi-Layer Multi-Cross Encoding) - It wasn't clear to me why the authors needed to build their own software for the annotation task. I think the task is similar to any other discourse labeling and relation identification task, and there are a lot of open-source tools that can be used for such annotations.
- In the introduction, the authors mention something about annotators' training and calibration process, but this wasn't elaborated upon later. I guess some details on this process should be presented in the paper.
- In Table 6, the header contains abbreviations. It is good to point out what do they mean. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"- The analysis section is limited and confusing in some parts: 	- Section 4.1 describes only the annotation results, without any further discussion of the implications.
	- In Section 4.2, based on Figure 2, the authors conclude that rebuttal sentences are not aligned with review sentences. However, what I see is completely different. The majority of review-rebuttal pairs happen to have a positive correlation of more than 50% unless I am missing something here!
	- Section 4.3 wasn't clear for me. I didn't understand how the figure supported the claim that authors often interpret reviews in a way that supports their argumentative goals. 
- There is missing work of Cheng et al. 2021 (Argument Pair Extraction via Attention guided Multi-Layer Multi-Cross Encoding) - It wasn't clear to me why the authors needed to build their own software for the annotation task. I think the task is similar to any other discourse labeling and relation identification task, and there are a lot of open-source tools that can be used for such annotations.
- In the introduction, the authors mention something about annotators' training and calibration process, but this wasn't elaborated upon later. I guess some details on this process should be presented in the paper.
- In Table 6, the header contains abbreviations. It is good to point out what do they mean. "
ARR_2022_92_review,"This paper presents a new dataset - DISAPERE, which includes discourse related annotations over scientific peer reviews and rebuttals. Each review is paired with the first rebuttal text. The authors develop four levels for review annotation: (1) review-action, (2) aspect, (3) polarity, and (4) fine-review action; and two levels for rebuttals: (1) argumentative and (2) non-argumentative ones. This dataset could help better characterize the intentions and interactions between reviewers and authors, which in turn can assist decision making for area chair.
The authors also tested two machine learning tasks: (1) sentence classification for the proposed schema, and (2) sentence ranking to determine the context mapping from rebuttal to review. Preliminary results show that pre-trained transformer models achieve moderate performance, therefore leaving room for future work. ","1. This work releases a new dataset of 506 review-rebuttal pairs with sentence level annotation for discourse related aspects. 
1. The proposed taxonomy is comprehensive and captures various aspects of peer review text. 
1. The authors framed practical machine learning tasks over the dataset, and benchmarked performance of baseline transformer models. 
1. In the updated draft, the authors have accounted for the majority of the comments in the previous review. Overall the paper is more clear, and certain minor mistakes have been rectified. ",N/A ,- The hyperlink for footnote 3 and 4 do not seem to work.  - Line 172: an argument level -> on argument level ,"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"
- The hyperlink for footnote 3 and 4 do not seem to work.  - Line 172: an argument level -> on argument level "
ARR_2022_324_review,"Authors propose augmenting the Knowledge Grounded Conversation (KGC) models by incorporating personal memory to produce persona-consistent responses in given context. Grounding responses in knowledge and personal memory makes those responses knowledgeable and personalized which is desirable.
Authors follow the standard approach of knowledge/memory selection, followed by response generation. In this paper they focus on knowledge/memory selection. They posit that knowledge selection is dependent on memory selection and memory is selected based on the context. They introduce two latent mappings: 1. Forward mapping from personal memory to external knowledge and 2. Inverse mapping of knowledge to personal memory. The authors present a variational inference based method to learn these mappings without access to explicit supervision. They set up a dual learning problem where the forward and inverse mappings are additionally informed by the produced response and learn from each other.
Authors also collected an appropriate corpus for this task which will be released to the community. They have conducted automatic and human evals and compared the proposed approach to existing baselines in both tasks (i.e., personalized dialogue response generation and knowledge grounded response generation). The proposed approach is shown to be significantly better than the baselines. Overall evaluation is well conducted and has meaningful comparison although a more intuitive explanation of differences (specifically compared to KnowledGPT + M) would be helpful.
Authors have also presented ablation studies that prove the value of each component. It’s interesting to see that modeling knowledge selection as a dependent process on memory selection results in significantly improved results. Although, the same results question the utility of dual learning and it should be discussed in more detail. ","Authors show that the proposed approach for knowledge/memory selection can be trained without explicit supervision by employing a dual learning method based on variational inference.
The proposed knowledge/memory selection indeed leads to better response generation and establishes a new SOTA in personalized KGC as evaluated by automatic and human evals.
Authors also promise to release the corpus they collected further establishing this task as benchmark in personalized KGC. This should be valuable to the research community. ","The paper presentation could benefit from a rewriting that focuses on introducing the problem of personalized KGC and motivating it. 
Authors should also discuss why it’s not enough to treat personal memory as just some additional knowledge candidates.
The corpus creation described in section 4.1 does not give enough details to understand exactly how knowledge and memory candidates are created. Authors should also give examples of the data that was collected.
The distillation process of closing the loop in the dual learning setup should be discussed in more detail. E.g., where is equation 17 used in Algorithm 1? ","There are many sentences that begin with “And …”. Just remove the ‘and’ and rewrite. ( e.g., line 92, 98).
Line 149 and line 163, remove repetition.
Line 173, cases Line 205, “Auxiliary task” This is later called a dual task in line 336. Are these the same?
Overall in the manuscript for random variables and equations it will be better to avoid brevity and be explicit with all variables being conditioned on. ( e.g., line 321-325). Also in line 216 the variable should be conditioned on the sampled $\widetilde{Z}^p$. Also in figure 2, It should be $q(Z^p | C, R)$ without $Z^k$, right? Please verify all variables and carefully review it all. Please don't leave this as an exercise for the user.
Line 248-268, Standard BERT description could be avoided to make space for discussing points more specific to your contribution.
Why is the pseudo knowledge label named {$P\bar}? Is this accurate?
Please explain the dataset collection with examples of how knowledge and memory is compiled. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The paper presentation could benefit from a rewriting that focuses on introducing the problem of personalized KGC and motivating it. 
Authors should also discuss why it’s not enough to treat personal memory as just some additional knowledge candidates.
The corpus creation described in section 4.1 does not give enough details to understand exactly how knowledge and memory candidates are created. Authors should also give examples of the data that was collected.
The distillation process of closing the loop in the dual learning setup should be discussed in more detail. E.g., where is equation 17 used in Algorithm 1? 
There are many sentences that begin with “And …”. Just remove the ‘and’ and rewrite. ( e.g., line 92, 98).
Line 149 and line 163, remove repetition.
Line 173, cases Line 205, “Auxiliary task” This is later called a dual task in line 336. Are these the same?
Overall in the manuscript for random variables and equations it will be better to avoid brevity and be explicit with all variables being conditioned on. ( e.g., line 321-325). Also in line 216 the variable should be conditioned on the sampled $\widetilde{Z}^p$. Also in figure 2, It should be $q(Z^p | C, R)$ without $Z^k$, right? Please verify all variables and carefully review it all. Please don't leave this as an exercise for the user.
Line 248-268, Standard BERT description could be avoided to make space for discussing points more specific to your contribution.
Why is the pseudo knowledge label named {$P\bar}? Is this accurate?
Please explain the dataset collection with examples of how knowledge and memory is compiled. "
ARR_2022_324_review,"This paper describes a novel technique for improving knowledge-grounded conversational systems by incorporating *personal memory* into the process.  Personal memory, defined abstractly as the influence of previous personal experience, preference, and values when interpreting dialogue context and selecting follow-up dialogue, is represented in this work as user-specific utterance histories under multiple types of context.  The paper's key contributions include: - A new dataset, scraped from Reddit, providing user-specific personal memory to function as a training and testbed for this task - The first approach for introducing personal memory into knowledge-grounded conversational systems The approach, a variational method that models interdependencies between persona and knowledge using latent variables and employs dual learning to jointly optimize the relationship between dialogue context, personal memory, and knowledge, achieves strong performance on the new dataset relative to competitive baselines under both automated and human evaluation paradigms. ","The paper is for the most part clearly written, and tackles an interesting topic that is likely to be of interest to the conversational AI community.  Key strengths include: - The proposed methods are described thoroughly.  They seem to be sound and well-suited for the intended purposes.
- A strong evaluation is conducted, comparing the proposed model to a suite of competitive baselines including a Generative Profile Memory Network (Zhang et al., 2018), Transformer Memory Network (Dinan et al., 2019), transfer learning-based Transformer model (Wolf et al., 2019), Sequential Knowledge Transformer (Kim et al., 2020), KnowledGPT (Zhao et al., 2020), a custom variation of KnowledGPT designed to incorporate personal memory, a transmiter-receiver based framework (Liu et al., 2020), and a recently published persona-based baseline (Song et al., 2021).
- Further analyses are conducted to establish the individual contributions of model components and additional hyperparameters. ","Despite its strengths, there are also several areas that offer opportunities for improvement in future revisions: - In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward).  Revising this section could benefit the overall clarity of findings and subsequent takeaway points.
- Some aspects of the dataset are unclear.  For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?
- Some of the presented results seem very close, particularly in the ablation study.  It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings. ",Two typos that could be addressed: - Line 278: The word *the* is repeated - Line 321-323: The text seems to indicate that $p_{\theta}(Z^{p})$ is shorthand for $p_{\theta}(Z^{p})$.  Should the second instance of this be something else? ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","The paper discusses a new dataset comprised of publicly available Reddit posts, but does not mention whether IRB (or other external review board) approval was obtained for collecting and using the data. ","Despite its strengths, there are also several areas that offer opportunities for improvement in future revisions: - In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward).  Revising this section could benefit the overall clarity of findings and subsequent takeaway points.
- Some aspects of the dataset are unclear.  For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?
- Some of the presented results seem very close, particularly in the ablation study.  It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings. 
Two typos that could be addressed: - Line 278: The word *the* is repeated - Line 321-323: The text seems to indicate that $p_{\theta}(Z^{p})$ is shorthand for $p_{\theta}(Z^{p})$.  Should the second instance of this be something else? "
ARR_2022_324_review,"This paper works on the problem of personalization in knowledge grounded conversation (KGC). To develop a benchmark, the authors collected a new KGC dataset based on Reddit containing personalized information (e.g. user profile and dialogue history). The authors propose a probabilistic model for utterance generation conditioned on both personalized profile (personal memory) and personal knowledge. Dual learning is employed to better learn the unconstrained relation between personal memory $Z^m$ and knowledge $Z^k$ , and variational method is proposed to approximately marginalize out $Z^m$ and $Z^k$ during inference. The results with automatic evaluation show promising improvement and human evaluation also validates this. Finally, various ablation studies are conducted to reveal the contribution of each model component. ","- The problem of personalization in KGC is a relatively overlooked yet important problem. The authors developed a promising method and benchmark for this new challenge.
- The idea of incorporating dual learning to link personalized sources (e.g. personal memory and knowledge) is very interesting and convincing. I’d like to see follow-up works comparing the ideas against this paper’s.
- The improvement in automatic evaluation is significant (though not fully reliable, as the author’s acknowledge in line 522). Human evaluation also corroborates the proposed model’s superiority, though the improvement becomes less significant. ","- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?
- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). ","- In lines 198-220, explanation of $\phi$, $\psi$ and $\pi$ is not clear. Can they be better explained or incorporated in Figure 2?
- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.
- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?
- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). 
- In lines 198-220, explanation of $\phi$, $\psi$ and $\pi$ is not clear. Can they be better explained or incorporated in Figure 2?
- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.
- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper. "
ARR_2022_150_review,This paper deals with the problem of table-based verification. It proposes learning how to mix the outcomes of three experts. Each of these experts focuses on a different kind of information (table reasoning). Each of these expert is also based on different technologies as a means of maximizing its performance. ,"Overall, I liked the paper and the approach. 
The paper can be understood with just one read. 
The idea is clear and technically sounds.
I do not see any technical flaw in this work (considering the restrictions), and I like it. I think it is a competitive paper for a top-tier conference. ","The main weakness of this paper is that it is unnecessarily cryptic. Although it does not have hard maths, there are things that could be explained only with words easily (page 3 for instance).
In the empirical section, I would add error analysis, significance tests between different configuration, especially in sections 6.1-6.3 ","a) ""Table-based verification is more challenging than unstructured-text-based"" I would suggest to you that the right thing to say is ""Table-based verification faces different challenges than unstructured-text-based...."" Text analysis has been around for quite a long time, while [web] table analysis for around 15 years.
b) ""Section 2 ""Task Formulation"" I would re-design it as a ""Research Question(s)"" where you say mostly the same thing but in a clearer way, including the main finding. ""
c) How do you know that a row in a table is actually true? :)
d) What hardware did you require?
e) Some history of table mining forerunners would be nice in the extra page.
f) What is the trigger-word set??? Did I overlook something? That second point was unclear to me. I think I needed to make my way until section 5.3 to get it. ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)","3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The main weakness of this paper is that it is unnecessarily cryptic. Although it does not have hard maths, there are things that could be explained only with words easily (page 3 for instance).
In the empirical section, I would add error analysis, significance tests between different configuration, especially in sections 6.1-6.3 
a) ""Table-based verification is more challenging than unstructured-text-based"" I would suggest to you that the right thing to say is ""Table-based verification faces different challenges than unstructured-text-based...."" Text analysis has been around for quite a long time, while [web] table analysis for around 15 years.
b) ""Section 2 ""Task Formulation"" I would re-design it as a ""Research Question(s)"" where you say mostly the same thing but in a clearer way, including the main finding. ""
c) How do you know that a row in a table is actually true? :)
d) What hardware did you require?
e) Some history of table mining forerunners would be nice in the extra page.
f) What is the trigger-word set??? Did I overlook something? That second point was unclear to me. I think I needed to make my way until section 5.3 to get it. "
ARR_2022_245_review,"The paper presents a new task called: Spoken Conversational Question Answering (SCQA). The proposed dataset consists of multi-turn conversations and passages in both text and speech form. The authors claim that SCQA is more challenging than only text-based tasks; and indeed, the performance of existing SOTA models such as BERT-based and ALBERT-based degrade. The paper suggests a dual attention technique to tackle the underperformance and shows that indeed the performance increases. The main idea of the dual attention is that model can benefit from learning on two language modalities (in this case, text and speech) ","•	The paper is well written; the experiments are sound, and the task seem interesting. 
•	I really like the idea of exploiting the two language modalities to improve existing models. I am not familiar with this kind of usage, so the novelty here is good. Also, I like the teacher-student approach (called Knowledge Distillation) which enable them to overcome the noise that came from ASR. 
•	The mathematical formulation of the task and the different approaches are nice and clear •	The baseline comparisons are also good ","The authors use the term “significant” several times in the paper but in fact, did not use any statistical test to check that the results are indeed significantly decreed. 
This problem gets bigger when looking at Tables 5 and 6; it is hard to see that the suggested techniques truly improve the models. Personally, I am not convinced that the Dual attention and knowledge distillation technique, suggested by the authors, improved the models (see for example lines 5 and 8 in table 5, the change in F1 score is ~2 points) I am wondering if you had a perfect ASR, was it still beneficial to use the two language modalities? I am not an expert on speech embedding but does it capture things like tone, speed, and nervousness? If the answers is negative, I think that this is a major weakness ","I think that the main strength of the paper is the use of the two language modalities. The dataset\task is nice, but it is not the main contribution in my opinion. 
I think that if the paper will be accepted then it will be required to perform the right statistical test in order to prove that indeed the methods works ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The authors use the term “significant” several times in the paper but in fact, did not use any statistical test to check that the results are indeed significantly decreed. 
This problem gets bigger when looking at Tables 5 and 6; it is hard to see that the suggested techniques truly improve the models. Personally, I am not convinced that the Dual attention and knowledge distillation technique, suggested by the authors, improved the models (see for example lines 5 and 8 in table 5, the change in F1 score is ~2 points) I am wondering if you had a perfect ASR, was it still beneficial to use the two language modalities? I am not an expert on speech embedding but does it capture things like tone, speed, and nervousness? If the answers is negative, I think that this is a major weakness 
I think that the main strength of the paper is the use of the two language modalities. The dataset\task is nice, but it is not the main contribution in my opinion. 
I think that if the paper will be accepted then it will be required to perform the right statistical test in order to prove that indeed the methods works "
ARR_2022_24_review,"This paper is similar to ""Learning Music Helps You Read"", that is, it tests the transferability of structure from synthetic pretraining data. It uses similar synthetic data to this prior work, but adds a dependency dataset. This dependency dataset is similar to the parenthetical dataset that exists, but it uses different symbols for opening and closing, rather than the same symbol. Their results are mostly similar to the existing results, but they find that introducing different opening and closing symbols does make the nested structure important for transfer, which is different from the previous results.  They then go over results after pretraining on masked language model objectives and then fine tuning on dependency parsing. ","I was very interested in the result that a simple and well-justified modification to the parenthetical language actually reveals nested structure to be valuable.
They run multiple seeds for the pretraining objective, allowing them to perform statistical tests over pretrained parameters. This kind of rigor is unfortunately often lacking in many papers on transfer learning, and I was pleased to see it here. ","I'm not convinced that this should be an entire long paper, given that it is largely a replication of existing results.  There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.
""We adopt sentence-level modeling [over between-sentences] because we would like to focus on the learning of sentence structures and also simplify the task setting."" 
This isn’t entirely clear. Do they mean that the prior work was using multiple sentences as input at a time and they are using shorter inputs? And if so, does this actually generalize to a more typical setting, which uses fixed length multiple sentence inputs? If you are making a significant change, justifying it verbally is not as good as testing the effect of it.
The authors describe the Transformer performance as evidence that ""Transformer encoders exhibit surprisingly good transferability to other languages"". I disagree with this characterization. It looks from the plot as though the random weights have much better performance in the Transformer model, indicating that the Transformer is not necessarily transferring the data better, but is just better at learning the fine tuned task generally. Even the authors seem to disagree with their own characterization, in lines 494-496. ","A few citations on existing synthetic training: https://arxiv.org/pdf/2106.01044.pdf and https://cs.stanford.edu/~nfliu/papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf both of which look at the inductive biases through synthetic training data.
Be clear whether the language modeling task in section 4 is a causal model.  418: s/is/are/ There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.
Figure 2a ""unifrom"" To what degree does TILT actually differ from model stitching https://arxiv.org/abs/2106.07682 procedures? I guess it doesn’t have the stitching layer?
Where are the results training on the parenthetical data in section 5? Only the dependency data is there. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I'm not convinced that this should be an entire long paper, given that it is largely a replication of existing results.  There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.
""We adopt sentence-level modeling [over between-sentences] because we would like to focus on the learning of sentence structures and also simplify the task setting."" 
This isn’t entirely clear. Do they mean that the prior work was using multiple sentences as input at a time and they are using shorter inputs? And if so, does this actually generalize to a more typical setting, which uses fixed length multiple sentence inputs? If you are making a significant change, justifying it verbally is not as good as testing the effect of it.
The authors describe the Transformer performance as evidence that ""Transformer encoders exhibit surprisingly good transferability to other languages"". I disagree with this characterization. It looks from the plot as though the random weights have much better performance in the Transformer model, indicating that the Transformer is not necessarily transferring the data better, but is just better at learning the fine tuned task generally. Even the authors seem to disagree with their own characterization, in lines 494-496. 
A few citations on existing synthetic training: https://arxiv.org/pdf/2106.01044.pdf and https://cs.stanford.edu/~nfliu/papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf both of which look at the inductive biases through synthetic training data.
Be clear whether the language modeling task in section 4 is a causal model.  418: s/is/are/ There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.
Figure 2a ""unifrom"" To what degree does TILT actually differ from model stitching https://arxiv.org/abs/2106.07682 procedures? I guess it doesn’t have the stitching layer?
Where are the results training on the parenthetical data in section 5? Only the dependency data is there. "
ARR_2022_24_review,"This paper investigates what kind of linguistic structural knowledge is transferable during language model pre-training. It explicitly probes certain knowledge/properties by designing several synthetic languages, i.e., (1) tokens are sampled independently and uniformly; (2) tokens are sampled independently but following the Zipfian distribution; (3) tokens are sampled depending on the sentence topic; (4) flat or nesting parenthesis language (Papadimitriou and Jurafsky, 2020); (5) flat or nesting dependency language. Experimental results show that for a causal LM task, conditioned token sampling and nested dependencies provide useful information, especially for LSTM LMs. Nested dependencies are most useful to pre-train a masked LM for a dependency parsing task. ","- This paper provides a novel and interesting setting to demonstrate that synthetic conditioned token sampling and synthetic nested dependencies contain knowledge that can be transferred to real language modeling or dependency parsing.
- Many follow-up ideas could be tested along this direction. ","- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. ","- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. "
ARR_2022_227_review,"The paper addresses a fundamental shortcoming in evaluating conversational question answering (CQA) models based on the gold-annotated question-answer pairs. The existing evaluation procedure ignores the impact of the model’s output on upcoming exchanges, thereby detracting from real-world scenarios. Specifically, when a model flounders, the conversation is likely to be progressed differently than what is already annotated. The current evaluation mechanism does not account for these cases.
The paper first highlights this problem by using human annotators to converse with 4 CQA models, trained on QuAC. After collecting QA conversations, another group of annotators check the validity of conversations by (1) checking whether the question at each turn is valid and answerable, and (2) verifying whether the models’ answers are correct. This experiment reveals that human evaluation substantially differs from gold-answer evaluation when the relative performance of the 4 models is compared using each evaluation method.
The paper finds that using predicted answers while retaining existing questions suffers from 3 drawbacks: unresolved coreference, incoherence, and change in the correct answer. Since the most frequent issue was related to coreference, the paper proposes a question-rewrite method to amend references by replacing them with the original entities, mentioned in the history. This method is empirically shown to be most aligned with human evaluation when compared to other evaluation methods.
Finally, the paper suggests 3 critical areas that future models should focus on: (1) ability to detect question dependencies, (2) ability to detect unanswered questions, (3) evaluation under real-world settings. ","- The paper focuses on an interesting problem and the insights provided in this work can be of interest to the community and spark future research.
- The analysis of CQA models is much-needed in the literature. Once we understood where our models fall short, we can make meaningful progress in building more effective CQA models.
Overall, I think the paper is a solid work. ","- My main concern with this paper is the proposed question-rewrite strategy. I’m not convinced this method covers a sufficiently large number of questions because out of 100 passages (or \~3.7K questions based on Table 4 in the appendix), 23% become invalid (\~850 questions), out of which 44% are attributed to unresolved coreference (\~375 questions). As reported at L373, the proposed method rewrites 12% of the questions for all models (\~45 questions per model), which hardly has a significant effect.
- I don’t understand why generating question-in-context lags behind the question-rewrite method. The question-rewrite method may make a question ungrammatical (as pointed out in $\S$B) because the original entity can be a clause or the coreference resolution tool may make mistakes. On the other hand, the question-replace model is trained to generate contextualized questions and its output is more likely to be grammatically correct. ",L238-L241: The sentence seems to be repeated but with different agreements. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- My main concern with this paper is the proposed question-rewrite strategy. I’m not convinced this method covers a sufficiently large number of questions because out of 100 passages (or \~3.7K questions based on Table 4 in the appendix), 23% become invalid (\~850 questions), out of which 44% are attributed to unresolved coreference (\~375 questions). As reported at L373, the proposed method rewrites 12% of the questions for all models (\~45 questions per model), which hardly has a significant effect.
- I don’t understand why generating question-in-context lags behind the question-rewrite method. The question-rewrite method may make a question ungrammatical (as pointed out in $\S$B) because the original entity can be a clause or the coreference resolution tool may make mistakes. On the other hand, the question-replace model is trained to generate contextualized questions and its output is more likely to be grammatically correct. 
L238-L241: The sentence seems to be repeated but with different agreements. "
ARR_2022_227_review,"This paper focuses on the task of Conversational Question Answering (CQA), which envisions a back-and-forth question answering scenario with an information-seeking goal. The work shows how the most common way of automatically evaluating CQA systems (on a popular instantiation of the task with one specific English dataset) is empirically different from how humans would evaluate the systems in an actual real-world scenario. The models use a dataset of human-human conversations and evaluation is commonly done by using the 'gold-standard' conversational history. But the human-machine conversations, which would result when the models are deployed or used, represent a fundamentally different distribution from human-human conversations. This means the current evaluation paradigm does not accurately capture the use-case and therefore, tracking modeling improvement with that evaluation paradigm will not be meaningful. The work's primary contribution is showing this discrepancy by conducting a human evaluation study where CQA systems get to interact with humans - the result human-machine conversational QA dataset is also a contribution. Further analysis shows how automated evaluation differs from human evaluation, and what human evaluation tells about current CQA modeling strategies including insights for future work. The work also puts forth a new automated evaluation paradigm that mirrors human evaluation results more closely than the existing evaluation strategy of using the gold answers. ","- The paper highlights a fundamental discrepancy between the dominant automated evaluation paradigm and human judgment of CQA models when situated in the real-world scenario - this exposition is meaningful for the CQA community in order to take stock of whether modeling improvements can be claimed with an evaluation strategy that seems to be quite broken.  - Further, the exposition of this problem in automated evaluation is carried out rigorously by conducting a user study where humans interact with different CQA models - and the resulting human-machine dataset is another important contribution. The work does also shows the different ways in which the problem with the current automated evaluation paradigm manifests itself.  - The difference in the distribution of human-human and human-machine conversations is made clear, and what that means for current automated model evaluation (such as models being ranked differently in terms of automated and human evaluation) is explained well.  - The insights derived for different modeling strategies and how they fare on human evaluation can help better understand differences and relative strengths and weaknesses of different CQA models. These insights can inform future work on CQA modeling and some solid recommendations are indeed put forth in this work.  - The paper is written clearly and organized well. The main ideas and contributions, and their importance, are presented early and quite clearly. ","1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy -      a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs.      b) gets to the fundamental problem with automated evaluation raised in the paper, which is that ""when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage."" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case.  In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion. ","Questions to the authors (which also act as suggestions):  Q1. - Line 151: ""four representative CQA models"" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense.  Q2. Line 196: ""We noticed that the annotators are biased when evaluating the correctness of answers"" - are any statistics on this available?  Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (""With Little Power Comes Great Responsibility."") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: ""The gap between HAM and ExCorD is significant in Auto-Gold"" - how is significance measured here?
Q5. Lines 360-364: ""We determine whether e∗_j = e_j by checking if F1(s∗_{j,1}, s_{j,1}) > 0 ....  .... as long as their first mentions have word overlap."" Two questions here -      5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid? 
    5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten.  Comments, suggestions, typos:  - Line 031: ""has the promise to revolutionize"" - this should be substantiated further, seems quite vague.  - Line 048: ""extremely competitive performance of"" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague.  - The abstract is written well and invokes intrigue early - could potentially be made even better if, for ""evaluating with gold answers is inconsistent with human evaluation"" - an example of the inconsistency, such as models get ranked differently is also given there.  - Line 033: ""With recent development of large-scale datasets"" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself.  - Line 147: ""more modeling work has been done than in free-form question answering"" - potential typo, maybe it should be ""maybe more modeling work has been done 'in that'"" - where that refers to extractive QA?
- Line 222: ""In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs"" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could.  - Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart.  - Lines 250-252: ""the absolute numbers of human evaluation are much higher than those of automatic evaluations"" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not.  - Line 348: ""background 4, S∗ - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - ""background,\footnote{} ..."" in latex.
- Footnote 4: 'empirically helpful' - should have a cite or something to back that there.  - Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy -      a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs.      b) gets to the fundamental problem with automated evaluation raised in the paper, which is that ""when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage."" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case.  In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion. 
Questions to the authors (which also act as suggestions):  Q1. - Line 151: ""four representative CQA models"" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense.  Q2. Line 196: ""We noticed that the annotators are biased when evaluating the correctness of answers"" - are any statistics on this available?  Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (""With Little Power Comes Great Responsibility."") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: ""The gap between HAM and ExCorD is significant in Auto-Gold"" - how is significance measured here?
Q5. Lines 360-364: ""We determine whether e∗_j = e_j by checking if F1(s∗_{j,1}, s_{j,1}) > 0 ....  .... as long as their first mentions have word overlap."" Two questions here -      5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid? 
    5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten.  Comments, suggestions, typos:  - Line 031: ""has the promise to revolutionize"" - this should be substantiated further, seems quite vague.  - Line 048: ""extremely competitive performance of"" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague.  - The abstract is written well and invokes intrigue early - could potentially be made even better if, for ""evaluating with gold answers is inconsistent with human evaluation"" - an example of the inconsistency, such as models get ranked differently is also given there.  - Line 033: ""With recent development of large-scale datasets"" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself.  - Line 147: ""more modeling work has been done than in free-form question answering"" - potential typo, maybe it should be ""maybe more modeling work has been done 'in that'"" - where that refers to extractive QA?
- Line 222: ""In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs"" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could.  - Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart.  - Lines 250-252: ""the absolute numbers of human evaluation are much higher than those of automatic evaluations"" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not.  - Line 348: ""background 4, S∗ - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - ""background,\footnote{} ..."" in latex.
- Footnote 4: 'empirically helpful' - should have a cite or something to back that there.  - Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting. "
ARR_2022_227_review,"The paper conducts a large-scale human evaluation of human-machine conversations using four state-of-the-art CQA systems on the QuAC dataset. The motivation behind this study is that using gold history does not generalize well to real-world applications. In terms of model ranking, the study shows disagreement between human and gold history evaluations, which is expected, given that humans are interactive and will have followup questions based on model's outputs. An intuitive solution is using predicated history, however, this invalidates some questions. To this end, the paper also proposes a simple question re-writing mechanism based on co-reference resolution to correct invalid questions when the predicted history is used. The paper reports many interesting findings. ","The paper is well-written and easy to follow.
The motivation is clear and the experimental setup is sound The human evaluation of human-machine conversations and the findings in the paper are insightful and would improve how CQA models are evaluated, and how future CQA datasets are constructed. ","It seems that the proposed re-writing approach makes re-written questions context-independent and self-contained (i.e., easier for the models). For example, ""What songs were in Parade"" could be answered without history information. Moreover, the proposed mechanism seems to kill the naturality of a conversation. Can you please elaborate on this?
With human evaluation (shown in Table 2), the number of unanswerable questions is higher across all models. I wonder if this is an artefact of humans conversing with models or the fact that humans (annotators) did not have access to the full passage. ","As a followup work, it would be insightful to conduct a similar study on different CQA datasets ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"It seems that the proposed re-writing approach makes re-written questions context-independent and self-contained (i.e., easier for the models). For example, ""What songs were in Parade"" could be answered without history information. Moreover, the proposed mechanism seems to kill the naturality of a conversation. Can you please elaborate on this?
With human evaluation (shown in Table 2), the number of unanswerable questions is higher across all models. I wonder if this is an artefact of humans conversing with models or the fact that humans (annotators) did not have access to the full passage. 
As a followup work, it would be insightful to conduct a similar study on different CQA datasets "
ARR_2022_349_review,"This paper introduces a suffix identification dataset ChapterBreak to evaluate the long-range understanding ability of long-range Transformer language models. This dataset contains complex types of chapter transitions that require long-range context understanding. The authors evaluate previous long-range language models on the challenge dataset, such as Big-Bird and Routing Transformer. They observe all models perform poorly compared to a segment-based RoBERT model trained on the suffix identification task. The authors assume the accuracy of suffix identification is a more useful metric than PPL for long-range context understanding evaluation. ","1. A new task (suffix identification) is proposed for evaluating the long-range context understanding ability of language models. 
2. Some previous typical models are evaluated on the new dataset, which can facilitate future works. ","1. The previous LRLMs perform poorly on suffix identification compared to the RoBERTa-based model, which is trained with this task. The LRLMs may have better results when also trained with this task. The authors need to show their task is more useful than vanilla language modeling task on more downstream tasks, such as QA or summarization. ","(1) The authors claim that the ChapterBreak (suffix identification) is more chanllenge on long-range understanding tasks. However, in order to show the effectiveness of the suffix identification task task, the long-range Transformer model pretrained with the task need to have powerful performance on the downstream understanding tasks than vanilla language modeling. The authors can give more results about that, following previous works (Big-Bird or Longformer)? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"1. The previous LRLMs perform poorly on suffix identification compared to the RoBERTa-based model, which is trained with this task. The LRLMs may have better results when also trained with this task. The authors need to show their task is more useful than vanilla language modeling task on more downstream tasks, such as QA or summarization. 
(1) The authors claim that the ChapterBreak (suffix identification) is more chanllenge on long-range understanding tasks. However, in order to show the effectiveness of the suffix identification task task, the long-range Transformer model pretrained with the task need to have powerful performance on the downstream understanding tasks than vanilla language modeling. The authors can give more results about that, following previous works (Big-Bird or Longformer)? "
ARR_2022_346_review,This paper studies the discourse structure of long-form answers by analyzing two datasets: ELI5 and Natural Questions (NQ). The authors define six sentence-level discourse roles for long-form answers and ask human annotators to label a total of 1.3K examples. The authors further conduct experiments on two tasks based on the annotated corpus: automatic discourse analysis and summarizing long-form answers. ,"- To my belief, this is the first work that analyzes the discourse structure of long-form answers. It calls our attention that better understanding discourse structure might benefit long-form QA.  - The data annotation process is described in detail and inter-annotator agreements are fully revealed.  - It did some initial studies on how machine-generated answers and human-written answers differ in discourse structure.  - The paper is fairly easy to follow. ","However, the work leaves a few things to be desired, let me order them from most to least important.
- I am not clear about the goals of the paper. The authors argue that a better understanding of the discourse structure of long-form answers can benefit the long-form QA systems or the evaluation of long-form answers. However, this end goal is not covered in the paper. The authors did not perform further experiments to show whether a long-form QA system can benefit from that discourse-level information to generate better answers.  - The six sentence-level discourse roles defined by the authors are still coarse-grained to me. It is also not surprising that most long-form answers are consist of answer summary, auxiliary information, and examples. I did not find many motivating insights from the analysis that could potentially benefit the future work of long-form QA.  - Section 5 is good to me. It analyzes the weaknesses of current long-form QA systems in discourse structure. However, the authors only analyzed 52 examples. Such a small sample size may not be enough to make a statistically significant observation. ","Putting together all the strengths and weaknesses I believe the authors proposed a new angle to study long-form QA and the NLP community will benefit from the annotated corpus. However, there are some problems that do not allow me to accept the current version. I think there are two directions the authors can work on to improve the paper:  - Conduct more in-depth or fine-grained analysis on the discourse structure of long-form answers, which hopefully would provide more insightful observations that can benefit the long-form QA community. For example, analyzing how the discourse structure of long-form answers differs in different domains and in different question types.  - Make this paper a closed loop. That is to show the additional discourse structure information can benefit the long-form QA system in terms of either improving its performance or facilitating the evaluation of long-form answers. ",2.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"However, the work leaves a few things to be desired, let me order them from most to least important.
- I am not clear about the goals of the paper. The authors argue that a better understanding of the discourse structure of long-form answers can benefit the long-form QA systems or the evaluation of long-form answers. However, this end goal is not covered in the paper. The authors did not perform further experiments to show whether a long-form QA system can benefit from that discourse-level information to generate better answers.  - The six sentence-level discourse roles defined by the authors are still coarse-grained to me. It is also not surprising that most long-form answers are consist of answer summary, auxiliary information, and examples. I did not find many motivating insights from the analysis that could potentially benefit the future work of long-form QA.  - Section 5 is good to me. It analyzes the weaknesses of current long-form QA systems in discourse structure. However, the authors only analyzed 52 examples. Such a small sample size may not be enough to make a statistically significant observation. 
Putting together all the strengths and weaknesses I believe the authors proposed a new angle to study long-form QA and the NLP community will benefit from the annotated corpus. However, there are some problems that do not allow me to accept the current version. I think there are two directions the authors can work on to improve the paper:  - Conduct more in-depth or fine-grained analysis on the discourse structure of long-form answers, which hopefully would provide more insightful observations that can benefit the long-form QA community. For example, analyzing how the discourse structure of long-form answers differs in different domains and in different question types.  - Make this paper a closed loop. That is to show the additional discourse structure information can benefit the long-form QA system in terms of either improving its performance or facilitating the evaluation of long-form answers. "
ARR_2022_132_review,"This paper focuses on generating word definitions written in plain and familiar words for language learners. 
The task of simple definition generation proposed in this paper is challenging due to the lack of training data unlike ordinary definition generation. 
The proposed method tackles this problem by combining two decoders that share their parameters---the one is for generating a definition for a given word and the other is for generating a simple sentence. 
The experimental results show that the proposed method outperforms pipeline approaches and the effectiveness of the parameter sharing scheme. ","1) This paper proposes a novel challeging task of simple definition generation.
2) The proposed style transfer approach using parameter sharing outperforms strong pipeline approaches. ","3) The explanation for baselines lacks the details of training settings, resulting in difficulty in reproducing. 
How to train the LOG-CaD and MASS models for complex definitions and the ACCESS and MUSS models for simple definitions? 
Which datasets did the authors use for those models?
4) It is hard to say that the performance improvements by the proposed method are significant. 
From the ablation study, SimpDefiner trained without LM and TR must be identical to the baseline MASS model. 
However, the baseline MASS model achieves 24.00 BLEU score on Complex whereas SimpDefiner trained only with definition generation achieves 25.02. 
The authors should explain this difference. 
Furthermore, the output of the baseline MASS model should be evaluated on Simple without applying ACCESS or MUSS. 
If the baseline MASS model achieves 25.02 BLEU score on Complex, it would also achieve 13.66 BLEU score on Simple as shown in Table 6, which is higher than the performances of pipelines on Table 3. ","5) The explanation for the inference should be described in Section 3, although Section 1 implies that SimpDefiner produces not only a simple definition but also a complex one. 
Also, Section 5.1 should indicate that complex definitions from Gen. Decoder are evaluated on Complex (from the OD test set) and simple definitions from Rec. Decoder are evaluated on Simple (from the OALD test set).
6) The expression of ""fully unsupervised"" in Section 1 is a bit misleading, because an alternative standard dictionary containing complex definitions can be used for training.
7) Conducting multiple runs with different seeds is preferable. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"3) The explanation for baselines lacks the details of training settings, resulting in difficulty in reproducing. 
How to train the LOG-CaD and MASS models for complex definitions and the ACCESS and MUSS models for simple definitions? 
Which datasets did the authors use for those models?
4) It is hard to say that the performance improvements by the proposed method are significant. 
From the ablation study, SimpDefiner trained without LM and TR must be identical to the baseline MASS model. 
However, the baseline MASS model achieves 24.00 BLEU score on Complex whereas SimpDefiner trained only with definition generation achieves 25.02. 
The authors should explain this difference. 
Furthermore, the output of the baseline MASS model should be evaluated on Simple without applying ACCESS or MUSS. 
If the baseline MASS model achieves 25.02 BLEU score on Complex, it would also achieve 13.66 BLEU score on Simple as shown in Table 6, which is higher than the performances of pipelines on Table 3. 
5) The explanation for the inference should be described in Section 3, although Section 1 implies that SimpDefiner produces not only a simple definition but also a complex one. 
Also, Section 5.1 should indicate that complex definitions from Gen. Decoder are evaluated on Complex (from the OD test set) and simple definitions from Rec. Decoder are evaluated on Simple (from the OALD test set).
6) The expression of ""fully unsupervised"" in Section 1 is a bit misleading, because an alternative standard dictionary containing complex definitions can be used for training.
7) Conducting multiple runs with different seeds is preferable. "
ARR_2022_132_review,"This paper extends the definition generation task to generate simple definitions instead of complex ones. The authors propose a multitasking framework that consists of complex definition generation, text reconstruction and language modeling tasks to indirectly learn the probability of a simple definition, given the word and its context. The authors experiment on English and Chinese datasets and use a set of measures such as BLEU, SemanticSimilarity, SARI and HSK level (for Chinese); and also compare to the baseline pipelines that consists of definition generation and a simplifier. They report improved scores for all cases. ","The paper is generally well-written and the task is well motivated. The parameter-sharing scheme is interesting and could be further investigated to disentangle other factors from text. The experimental design is well-thought, i.e., contains suitable baselines and measures, as well as manual evaluation (in some cases). ","- The proposed method is not explained with adequate details (more below).
- The results (Table 3, 4) seem to be from single runs, hence are not trustworthy. They should be given as the average of multiple runs with different random seeds. ","- 006-007: The sentence sounds weird. Maybe delete the ""better"" at the end - 017: You can hint at which components you refer to - 017: jointly - 060: how do you define a difficult word?
- 091-93: it just repeats the same sentence with no clarification. then you jump into the tiny details.   - 131: capable of learning - 135: space before ( - 152-155: more suitable then what? e.g., isn't T5 also suitable?
- 188-191: why would you need to generate both of them? isn't simplification the point of this task?
- 225: in -> of - 250-252: what is the hypothesis? why do you think sharing decoders among two tasks would help? the motivation is missing - 267: focusd -> focused - 285-287: repeating the same sentence - 304: Don't you only have the corrupted text as the input for the reconstruction (also as shown in Fig 2)? But this equation tells you do language modeling on the original text?   - In general for Sec 3.2: you mention that only some parameters are shared, but then you use the same \theta as the model parameters for each sub task. This is confusing. Please separate the parameters for clarity.  - 317: dived -> divided - 340: I'm very confused how the complexity ""c"". what's the range of c? how is it defined? is it learned during training without supervision and how? how do you control for it?
- 350: what exactly is a query? the output vector from the cross-attention?
- 433: The model is heavily based on MASS, but it is not described properly.   - 440-441: Why don't you tune your lambda parameters on the dev test for fair comparison? Please also add how you initialize the parameters that are not part of MASS, e.g., the ones in Eq. 8.
- 547: sharing 571: extra space before comma ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The proposed method is not explained with adequate details (more below).
- The results (Table 3, 4) seem to be from single runs, hence are not trustworthy. They should be given as the average of multiple runs with different random seeds. 
- 006-007: The sentence sounds weird. Maybe delete the ""better"" at the end - 017: You can hint at which components you refer to - 017: jointly - 060: how do you define a difficult word?
- 091-93: it just repeats the same sentence with no clarification. then you jump into the tiny details.   - 131: capable of learning - 135: space before ( - 152-155: more suitable then what? e.g., isn't T5 also suitable?
- 188-191: why would you need to generate both of them? isn't simplification the point of this task?
- 225: in -> of - 250-252: what is the hypothesis? why do you think sharing decoders among two tasks would help? the motivation is missing - 267: focusd -> focused - 285-287: repeating the same sentence - 304: Don't you only have the corrupted text as the input for the reconstruction (also as shown in Fig 2)? But this equation tells you do language modeling on the original text?   - In general for Sec 3.2: you mention that only some parameters are shared, but then you use the same \theta as the model parameters for each sub task. This is confusing. Please separate the parameters for clarity.  - 317: dived -> divided - 340: I'm very confused how the complexity ""c"". what's the range of c? how is it defined? is it learned during training without supervision and how? how do you control for it?
- 350: what exactly is a query? the output vector from the cross-attention?
- 433: The model is heavily based on MASS, but it is not described properly.   - 440-441: Why don't you tune your lambda parameters on the dev test for fair comparison? Please also add how you initialize the parameters that are not part of MASS, e.g., the ones in Eq. 8.
- 547: sharing 571: extra space before comma "
ARR_2022_49_review,The authors state that prior work encode passages for distantly supervised relation extraction exclusively one-by-one. They propose a simple BERT based architecture that processes several passages simultaneously and thus allows for inter-sentence information flow during encoding. They provide results that show that their method outperforms prior work. ,"1) The experimental execution of the paper is of high quality. Results are presented across four datasets and compared against several relevant/recent baselines. Further, meaningful ablation studies are presented.  2) The proposed architecture seems to work and yields performance improvements.  3) Overall the paper is well-written and easy to follow. The scope and contributions fit my expectation of a short paper. ","1) I miss an analysis for which specific examples encoding multiple passages help. Adding an analysis where models that encode passages independently of each other fail compared to the proposed approach would be helpful. Similarly, this is a weak point in the motivation of the paper: it is not immediately clear to me why encoding passages simultaneously should be beneficial. You remain very vague here (e.g., line 046) and do not provide convincing arguments.  2) A central argument of the paper is that encoding passages simultaneously is helpful. Why do you restrict yourself to a maximum sequence length of 512 by using BERT? There is pretrained language models e.g., based on Longformer where you could encode more passages. Providing more insights how the number of passages influence performance (beyond what is mentioned in paragraph around line 243) would make the paper stronger. ","1) Why do you use uncased PLMs?  2) Potentially relevant but not cited work: Chu et al. 2021 (https://openreview.net/pdf?id=8smkJ2ekBRC), Wang et al. 2019 (https://aclanthology.org/P19-1132.pdf) 3) Please fix the bibliography and cite published versions rather than arxiv versions (e.g., Loshchilov et al. 2017). ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1) I miss an analysis for which specific examples encoding multiple passages help. Adding an analysis where models that encode passages independently of each other fail compared to the proposed approach would be helpful. Similarly, this is a weak point in the motivation of the paper: it is not immediately clear to me why encoding passages simultaneously should be beneficial. You remain very vague here (e.g., line 046) and do not provide convincing arguments.  2) A central argument of the paper is that encoding passages simultaneously is helpful. Why do you restrict yourself to a maximum sequence length of 512 by using BERT? There is pretrained language models e.g., based on Longformer where you could encode more passages. Providing more insights how the number of passages influence performance (beyond what is mentioned in paragraph around line 243) would make the paper stronger. 
1) Why do you use uncased PLMs?  2) Potentially relevant but not cited work: Chu et al. 2021 (https://openreview.net/pdf?id=8smkJ2ekBRC), Wang et al. 2019 (https://aclanthology.org/P19-1132.pdf) 3) Please fix the bibliography and cite published versions rather than arxiv versions (e.g., Loshchilov et al. 2017). "
ARR_2022_291_review,"This paper proposes DAR, which improves dense retrieval models with MixUp interpolations and dropout perturbation. MixUp (Zhang+ 2018) was initially proposed for image models, which creates augmented examples by linearly interpolating both the representations and labels between positive-negative sample pairs.  This work applies this technique to dense  text retrieval, and augments the original contrastive loss with soft binary cross-entropy losses. Furthermore, this paper also explores perturbing the representations with 0.1 dropout rate, and this technique can be combined with the MixUp strategy.
Experiment on the DPR framework shows that DAR significantly outperforms vanilla DPR and its combinations with different data augmentation strategies (QA,DA,AR). The improvement is more prominent on unlabeled documents (not seen during training).  Oblation study shows that perturbation is only marginally helpful by itself, but a lot more helpful when combined with MuxUp.  Experiment on the ANCE framework shows that combining ANCE with DAR significantly improves MRR, and marginally improves R@1k. ","This work is well motivated from the need to generalize unseen documents, and shows good empirical results. ","However, I find it less motivated by the computational cost of other data augmentation approaches, which are not really that expensive as shown in Table 7. ","It seems that the proposed document augmentation approach is not mutually exclusive to other approaches such as query generation (QA, DA, AR in Table 1), and we should evaluate the combined performance of them with DAR. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"However, I find it less motivated by the computational cost of other data augmentation approaches, which are not really that expensive as shown in Table 7. 
It seems that the proposed document augmentation approach is not mutually exclusive to other approaches such as query generation (QA, DA, AR in Table 1), and we should evaluate the combined performance of them with DAR. "
ARR_2022_291_review,"The paper proposed two document augmentation methods for training data augmentation, namely, the interpolation with mixup and the stochastic perturbation with dropout. In the interpolation, a dummy document is created by interpolating the positive and negative documents which is associated with a soft label; In the dropout, multiple dummy document representations are  generated by masking the document representation with stochastic masks. ","- The description of the motivation, and method are clear and easy to follow - The proposed methods are intriguing - The reported results are promising. Especially, the boost in terms of the success rate @ 20, 5, 1 on NQ looks good. ","- The biggest concerns/confusions during the reading are the lack of implementation details of the proposed methods. They should have been described in the implementation details in Section 4.1.   1) For the interpolation method, how the \lambda has been set? 
2) For the dropout, thru the reading of the response letter, my understanding is that multiple stochastic masks (w/ 0 and 1) are applied to a document presentation from an encoder. Herein, what is the dropping rate? How many masks have been generated?
- I am not sure TQA is a good enough benchmark for the proposed method.  It can be seen from Table 1, none of the competing deep models could outperform BM25. Though the proposed DAR could boost DPR, I am not sure if such gain is meaningful. ",- The proposed methods are novel and intriguing. The confusion mostly comes from the implementation details and the results. Why not extend the efforts to a full paper and properly add all of the details given the extra space? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The biggest concerns/confusions during the reading are the lack of implementation details of the proposed methods. They should have been described in the implementation details in Section 4.1.   1) For the interpolation method, how the \lambda has been set? 
2) For the dropout, thru the reading of the response letter, my understanding is that multiple stochastic masks (w/ 0 and 1) are applied to a document presentation from an encoder. Herein, what is the dropping rate? How many masks have been generated?
- I am not sure TQA is a good enough benchmark for the proposed method.  It can be seen from Table 1, none of the competing deep models could outperform BM25. Though the proposed DAR could boost DPR, I am not sure if such gain is meaningful. 
- The proposed methods are novel and intriguing. The confusion mostly comes from the implementation details and the results. Why not extend the efforts to a full paper and properly add all of the details given the extra space? "
ARR_2022_46_review,"This paper advocates the importance of cultural awareness when designing an NLP solution. The authors of this paper identify four dimensions of culture associated with the NLP systems, i.e., linguistic form, common ground, aboutness, and values. Then, the authors propose the strategies to work on cross-cultural NLP, which cover three areas, namely data collection, model training, and translation. ","Several reasons to accept this paper in ACL special theme track: + Understanding the challenges of cross-cultural and multicultural NLP can improve the NLP systems to serve the users in a more inclusive world. This paper raises the issues, surveys existing strategies, and highlights directions for the future development of cross-cultural NLP. 
+ When discussing the four cultural factors, the authors cite the various examples from different culture around the world (not Western centric only). For example: Latin America, Middle East (section 2); India (section 4); and Africa (section 6).    + Most recommendations, regarding data curation, annotation, translation, and model bulding, are sensible and formulated in systematic way. ","- The author describes that the four components of culture are represented orthogonally (Figure 1). In fact, most of these components depend on each other. ","Line 338, ""..towards shifting the current culture into a hopefully more equitable one"". What is meant by ""current culture""?
Line 642, ""..more diverse cross-cultural representation"". What is the concrete implementation of this idea when building an NLP model?
In which NLP tasks is cultural bias a prominent issue and needs to be prioritized to address? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The author describes that the four components of culture are represented orthogonally (Figure 1). In fact, most of these components depend on each other. 
Line 338, ""..towards shifting the current culture into a hopefully more equitable one"". What is meant by ""current culture""?
Line 642, ""..more diverse cross-cultural representation"". What is the concrete implementation of this idea when building an NLP model?
In which NLP tasks is cultural bias a prominent issue and needs to be prioritized to address? "
ARR_2022_46_review,"The paper proposes to trace an agenda for cross-cultural NLP, first by identifying four axes of variation for cultures (linguistic form and style, common ground, aboutness, objectives and values) and describing the challenges related to them; then by highlighting possible strategies for mitigating cultural biases in modern NLP in the areas of data collection, model training and translation.
I am not an expert on the issues under discussion in this paper, so it is not easy for me to provide a balanced evaluation. I enjoyed reading it, especially for the attempt of brining awareness on cultural issues, which are often neglected in current NLP practices. I think the authors did a very extensive and comprehensive work on the bibliography, which could be useful for correcting biases.
On the other hand, I feel that is often difficult to disentangle between linguistic and cultural dimension, and in many cases  the problems/solutions are overlapping with the typical low-resource language scenarios (e.g. where a particular language/dialect/sociolect can be associated with a specific cultural group), and in such cases the general heuristic could be described as ""the more cultural-specific data we can acquire, the better""; while in other cases it is difficult to propose any conclusive solution (e.g. see the case for model training in Section 6.2, where methods balancing for bias would require access to demographic attributes, but the problem would persist because one cannot really be sure that such attributes adequately reflect culture).  I guess this is a general problematic when one has to deal with something like culture, which is notoriously difficult to define. 
At the end of my reading, I still feel that I did not get what are, specifically, the directions indicated by the authors (this might well be a limitation of mine) ... ","The topic is highly relevant for the special theme of this year, and the issue of cross-cultural NLP is a new and interesting perspective. 
The survey takes into account a wide body of references that might be useful for NLP researchers. ","The difficulty in defining culture and in disentangling between linguistic and cultural dimension make it also difficult, in many of the cases exemplified by the authors, to trace a clear roadmap for a cross-cultural NLP. ",l. 141 varies --> vary l. 466 datasts --> datasets ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The difficulty in defining culture and in disentangling between linguistic and cultural dimension make it also difficult, in many of the cases exemplified by the authors, to trace a clear roadmap for a cross-cultural NLP. 
l. 141 varies --> vary l. 466 datasts --> datasets "
ARR_2022_238_review,"This paper proposed to use linguistic (phonetic/phonological) features, rather than phoneme identities, for training TTS system. The experiments demonstrate how this method can be useful for low-resource settings in a meta-learning scenario. Another important contribution is the proposed adaptation of the model-agnostic meta-learning (MAML) to TTS, resulting in a novel procedure that the authors call LAML (language-agnostic meta-learning). The study shows that the proposed methods achieve good TTS results (in terms of both intelligibiligy and naturalness) with little training data. ","The idea of using articulatory features is rather simple, but results in substantial improvements in terms of training time (in case of training on a single language) compared to the ""traditional"" training mode, and, more importantly, it enables the use of meta-learning for low-resource TTS settings. This seems to me like an important step in developing low-resource TTS, but I have very little knowledge of state-of-the-art TTS system and cannot provide a good evaluation. ","Some methodological details seem to be rather underspecified in the paper: e.g., what are ""similar results"" in line 216, or what is ""much more difficulto to train"" in line 221, or how many is actually ""many updates"" in line 284, and what is ""subjective assessment"" in line 394?
The paper only presents an evaluation on German and may or may not work in the actual low-resource settings. In particular, what are the exact features used? As far as I know, IPA-based features may be tailored towards English or other Indo-European languages, which may be a problem for low-resource languages. ","Line 517: typo - ""improveD"". 
Line 542: I assume Dutch and Finnish were removed to remove the phonemes that are also present in German, but it would be useful to say that explicitly. Also, isn't there overlap between German and the other training languages? ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Some methodological details seem to be rather underspecified in the paper: e.g., what are ""similar results"" in line 216, or what is ""much more difficulto to train"" in line 221, or how many is actually ""many updates"" in line 284, and what is ""subjective assessment"" in line 394?
The paper only presents an evaluation on German and may or may not work in the actual low-resource settings. In particular, what are the exact features used? As far as I know, IPA-based features may be tailored towards English or other Indo-European languages, which may be a problem for low-resource languages. 
Line 517: typo - ""improveD"". 
Line 542: I assume Dutch and Finnish were removed to remove the phonemes that are also present in German, but it would be useful to say that explicitly. Also, isn't there overlap between German and the other training languages? "
ARR_2022_238_review,The paper targets low-resource text-to-speech by using the articulation features instead of phoneme identity. The model is feasible to perform text-to-speech with 30 minutes of training data. Three contributions are (1) articulatory features for TTS; (2) LAML which is a modified version of MAML; (3) Interesting analysis over the framework could be very helpful for practical usage. ,"(1) The paper proposes a training procedure adopted from MAML so as to fine-tune a TTS model for low-resource scenarios.
(2) The paper verifies the feasibility of using articulate features for TTS training and its extension for low-resource scenarios.
(3) Experiments are with exciting discussions over the framework's capacity. ","The main story is convincing, but some experiments are lacking or incomplete.
(1) In line 96, the authors state that articulatory features could be beneficial than phoneme identities. While in the experimental section, the authors discuss the results on embedding similarities (articulatory features v.s. phoneme identities) and the convergence time with alignment examples. Indeed, attention alignment could reveal the results, but the reviewer would more want to see the results from your subjective results and also the convergence time for a fully trained model with specific features.
(2) In section 4.2.1, It would be good to show when the low-resource model could have a matched performance as the baseline. ","(1) It's not clear why the author would use the phoneme embedding from a tacotron 2 model as gold data, which seems contradictory to the paper's statement. Suppose the articulatory features are learned to be very similar to phoneme embedding. In that case, it's difficult to say the learned feature could be better than phoneme embedding (discussed in 4.1.2).
(2) The reviewer is interested in how the LAML works in settings where the data size for each language is imbalanced. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The main story is convincing, but some experiments are lacking or incomplete.
(1) In line 96, the authors state that articulatory features could be beneficial than phoneme identities. While in the experimental section, the authors discuss the results on embedding similarities (articulatory features v.s. phoneme identities) and the convergence time with alignment examples. Indeed, attention alignment could reveal the results, but the reviewer would more want to see the results from your subjective results and also the convergence time for a fully trained model with specific features.
(2) In section 4.2.1, It would be good to show when the low-resource model could have a matched performance as the baseline. 
(1) It's not clear why the author would use the phoneme embedding from a tacotron 2 model as gold data, which seems contradictory to the paper's statement. Suppose the articulatory features are learned to be very similar to phoneme embedding. In that case, it's difficult to say the learned feature could be better than phoneme embedding (discussed in 4.1.2).
(2) The reviewer is interested in how the LAML works in settings where the data size for each language is imbalanced. "
ARR_2022_356_review,"This paper presents a hate speech dataset XTREMESPEECH containing 20,297 social media passages from Brazil, Germany, India and Kenya. Different from most existing datasets, XTREMESPEECH was collected and annotated by fact-checkers who are both independent workers in local communities and targets of online extreme speech themselves. The passages were annotated as derogatory, exclusionary or dangerous, in which the last two requires removal. Annotators also annotated the targets of extreme speech. The paper further establishes XTREMESPEECH baselines on 4 tasks: predicting the extremity of speech, if a passage should be removed, hate target identification, and zero-shot cross-country classification.  The paper is easy to read. While I do like the distinction between different types (levels) of extreme speech, I would expect to see more clarifications with examples among the 3 types extreme speech and if there are culture differences in the annotations across countries. ","1. The authors put a huge work in dataset curation. The experiments appear to be reasonable. 
2. A collection of cross-country and multilingual hate speech is provided, in principal, reflecting the current social and political situation in a country. This dataset will be useful for addressing hate speech from a multilingual perspective. ","1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558). 
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria? ","1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation? 
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant. 
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558). 
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria? 
1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation? 
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant. 
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching). "
ARR_2022_122_review,The authors proposed a Locally Aggregated Feature Attribution method and claimed that this is a novel gradient-based feature attribution method for NLP models. ,The authors proposed a Locally Aggregated Feature Attribution method ,Results are varying so much on two different datasets ,"Did you use an attention mechanism? If yes, what are the significant changes you observed between the two approaches? If not, could you please check a performance comparison with any attention mechanism? 
 Why are the results varying so much on two different datasets? Is your model biased on a particular data? Did you check the disparity and fairness of data? ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"
Did you use an attention mechanism? If yes, what are the significant changes you observed between the two approaches? If not, could you please check a performance comparison with any attention mechanism? 
 Why are the results varying so much on two different datasets? Is your model biased on a particular data? Did you check the disparity and fairness of data? "
ARR_2022_359_review,"This paper proposes a unified SpeechT5 framework, which models various spoken language processing tasks as a speech/text to speech/text format based on the encoder-decoder architecture. For input speech or text, the modal-specific pre-net processes it into a shared space. Then the shared encoder-decoder network performs the sequence-to-sequence conversion. Finally, the modal-specific post-nets generate the corresponding output. For learning cross-modal alignment, a vector quantization method is proposed. Experiments on several spoken language processing tasks demonstrate the effectiveness. ","1. 	This paper proposes a new framework, which treats various spoken language processing tasks as a unified format. With the help of the task-specific pre/post-nets, the encoder-decoder network is shared across all tasks. This makes the efficient training. 
2. 	This paper proposes a vector quantized method for cross-modal alignment. This technique may inspire related studies about cross-modal learning. 
3. 	The proposed SpeechT5 achieves multiply state-of-the-art results on spoken language processing tasks. The released models will promote the development of the related community. ","Although this paper proposes a unified framework, the design of the pre/post-nets for each task is not novel. In addition, the quantized method has been explored in the previous work. The primary contribution of this paper is to integrate the existed strategies into a unified framework. ","The amount of the training data and the model size are important for pre-training. The dominant pre-trained methods usually report the results and release the models with the different settings. I suggest the authors do it, which will improve the influence of the work. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Although this paper proposes a unified framework, the design of the pre/post-nets for each task is not novel. In addition, the quantized method has been explored in the previous work. The primary contribution of this paper is to integrate the existed strategies into a unified framework. 
The amount of the training data and the model size are important for pre-training. The dominant pre-trained methods usually report the results and release the models with the different settings. I suggest the authors do it, which will improve the influence of the work. "
ARR_2022_359_review,"This paper proposes SpeechT5, a multimodal transformer model that pretrains on a variety of speech/text related tasks, including speech recognition, speech translation, speech synthesis and voice conversion, etc. The authors adapts T5 (an encoder-decoder transformer) to speech pretraining by jointly learning speech and texts. The joint learning are said to improve the performance of downstream tasks. While evaluated on a variety of speech related tasks, SpeechT5 demonstrates impressive performance compared to other pretrained models (Wav2Vec2 & HuBERT).   This paper contains non-trivial contributions. As the authors noted, this is the first model that try to combine speech recognition and generation tasks in speech pre-training. This alone will be an important contribution to the field of speech processing, as previous pretrained models like Wav2Vec2 & HuBERT models do not focus on speech generation tasks. This could be a step towards uniting the pipeline of speech processing. ","- A novel appriach to combine speech recognition and generation through jointly training on speech & text.
- Exhaustive evaluation and strong performance across a variety of speech related tasks.  - Good documentation of experiments.
- The writing is clear and easy to follow. ","- I do not see any major weaknesses of this paper but there are a minor points.
- The authors evaluated the model on speech-to-text, text-to-speech and speech-to-speech tasks but did not include text-to-text tasks in the evaluation. Yet this evaluation could be important for understanding the advantages/limitations of SpeechT5. ","- Vased on what is written in this paper, it seems that the authors do not state any plans to release the code and the pre-trained weights. SpeechT5 is a complex model with several pretraining tasks and ultilizes a lot of computing resources. It might not be easy to replicate the model for a significant number of researchers. If the authors can consider releasing the model, it will benefit more of the research community.  - As speech and texts are projected into the same latent space, it will be interesting to see an analysis of the joint embedding space to better understand how the model learns. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- I do not see any major weaknesses of this paper but there are a minor points.
- The authors evaluated the model on speech-to-text, text-to-speech and speech-to-speech tasks but did not include text-to-text tasks in the evaluation. Yet this evaluation could be important for understanding the advantages/limitations of SpeechT5. 
- Vased on what is written in this paper, it seems that the authors do not state any plans to release the code and the pre-trained weights. SpeechT5 is a complex model with several pretraining tasks and ultilizes a lot of computing resources. It might not be easy to replicate the model for a significant number of researchers. If the authors can consider releasing the model, it will benefit more of the research community.  - As speech and texts are projected into the same latent space, it will be interesting to see an analysis of the joint embedding space to better understand how the model learns. "
ARR_2022_237_review,"The paper proposes a method for relation extraction by taking advantage of interactions between relations instead of only entities, by constructing both relation and entity embeddings. 
These representations are further refined via attention mechanisms over the entity-context interactions. 
In addition, Tucker decomposition is employed in order to align representations of entities and relations in a joint space, predicting if a triple exists or not.
Experiments are conducted on three datasets, one for document-level relation extraction (DocRED) and two for joint entity and relation extraction (NYT, WebNLG). Comparison with prior works shows some improvement while ablation studies indicate that alignment of entity and relation embeddings contributes to performance. ",Strengths of the paper include:  - A new fusion and alignment approach between relation and entity embeddings for relation extraction - Improvement in document-level and sentence-level relation extraction compared with prior work ,"Weaknesses of the paper include: - The introduction of relation embeddings for relation extraction is not new, for example look at all Knowledge graph completion approaches that explicitly model relation embeddings or works on distantly supervised relation extraction. However, an interesting experiment would be to show the impact that such embeddings can have by comparing with a simple baseline that does not take advantage of those.
- Improvements are incremental across datasets, with the exception of WebNLG. Why mean and standard deviation are not shown for the test set of DocRED?
- It is not clear if the benefit of the method is just performance-wise. Could this particular alignment of entity and relation embeddings (that gives the most in performance) offer some interpretability? ( perhaps this could be shown with a t-SNE plot, i.e. check that their embeddings are close in space). ","Comments/Suggestions: - Lines 26-27: Multiple entities typically exist in both sentences and documents and this is the case even for relation classification, not only document-level RE or joint entity and relation extraction.
- Lines 39-42: Point to figure 1 for this particular example.
- Lines 97-98: Rephrase the sentence ""one that searches for ... objects"" as it is currently confusing - Line 181, Equations 4: $H^s$, $E^s$, $E^o$, etc are never explained.
- Could you show ablations on EPO and SEO? You mention in the Appendix that the proposed method is able to solve all those cases but you don't show if your method is better than others.
- It would be interesting to also show how the method performs when different number of triples reside in the input sequence. Would the method help more sequences with more triples?
Questions:  - Improvement still be observed with a better encoder, e.g. RoBERTa-base, instead of BERT?
- How many seeds did you use to report mean and stdev on the development set?
- For DocRED, did you consider the documents as an entire sentence? How do you deal with concepts (multiple entity mentions referring to the same entity)? This information is currently missing from the manuscript. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Weaknesses of the paper include: - The introduction of relation embeddings for relation extraction is not new, for example look at all Knowledge graph completion approaches that explicitly model relation embeddings or works on distantly supervised relation extraction. However, an interesting experiment would be to show the impact that such embeddings can have by comparing with a simple baseline that does not take advantage of those.
- Improvements are incremental across datasets, with the exception of WebNLG. Why mean and standard deviation are not shown for the test set of DocRED?
- It is not clear if the benefit of the method is just performance-wise. Could this particular alignment of entity and relation embeddings (that gives the most in performance) offer some interpretability? ( perhaps this could be shown with a t-SNE plot, i.e. check that their embeddings are close in space). 
Comments/Suggestions: - Lines 26-27: Multiple entities typically exist in both sentences and documents and this is the case even for relation classification, not only document-level RE or joint entity and relation extraction.
- Lines 39-42: Point to figure 1 for this particular example.
- Lines 97-98: Rephrase the sentence ""one that searches for ... objects"" as it is currently confusing - Line 181, Equations 4: $H^s$, $E^s$, $E^o$, etc are never explained.
- Could you show ablations on EPO and SEO? You mention in the Appendix that the proposed method is able to solve all those cases but you don't show if your method is better than others.
- It would be interesting to also show how the method performs when different number of triples reside in the input sequence. Would the method help more sequences with more triples?
Questions:  - Improvement still be observed with a better encoder, e.g. RoBERTa-base, instead of BERT?
- How many seeds did you use to report mean and stdev on the development set?
- For DocRED, did you consider the documents as an entire sentence? How do you deal with concepts (multiple entity mentions referring to the same entity)? This information is currently missing from the manuscript. "
ARR_2022_194_review,"This paper proposes NoisyTune, which is a method to add noise using the standard deviation of matrix-wise parameters during fine-tuning pretrained language models. The main motivation of using NoisyTune is in line with other work (e.g., by Chen et al. 2020 and Lee et al. 2020) on avoiding large deviation of parameters from the pretrained model to avoid overfitting during finetuning. Experimental results on XTREME and GLEU benchmark datasets suggest that NoisyTune gives small improvement consistently on various settings. ",- Simplicity of the method: A matrix-wise standard deviation of parameters is only necessary to use this method - Comparison with a simple baseline of adding a global noise - Parameter matrix-wise distribution is useful for adding noise - Experimental results with four different pretrained language models ,"- The performance gain by using NoisyTune on XTREME and GLEU is very small (less than 1 point in accuracy or F1 score).  - No standard deviation scores are reported, and the comparison of the performance gain vs. standard deviation is not reported.
- The methods compared in the experiments are only with using global noise. ","### Comments - I suggest further experimenting with various hyperparameters and would the addition of NoisyTune be complimentary to hyperparameter tuning.
- Since the motivation of this paper is avoiding overfitting during fine-tuning, I would be very curious to see NoisyTune in few-shot settings, where overfitting is more of an issue.
- [Minor] For Figure 4, preferable to have legends outside the plot.
### Questions to the Authors - Are different hyperparameters (e.g., drop out rate) explored? Would it be possible that tuning one of the hyperparameters leads to larger gains than using NoisyTune?
- What is the standard deviation across multiple runs for e.g., Table 1 and Figure 2? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- The performance gain by using NoisyTune on XTREME and GLEU is very small (less than 1 point in accuracy or F1 score).  - No standard deviation scores are reported, and the comparison of the performance gain vs. standard deviation is not reported.
- The methods compared in the experiments are only with using global noise. 
### Comments - I suggest further experimenting with various hyperparameters and would the addition of NoisyTune be complimentary to hyperparameter tuning.
- Since the motivation of this paper is avoiding overfitting during fine-tuning, I would be very curious to see NoisyTune in few-shot settings, where overfitting is more of an issue.
- [Minor] For Figure 4, preferable to have legends outside the plot.
### Questions to the Authors - Are different hyperparameters (e.g., drop out rate) explored? Would it be possible that tuning one of the hyperparameters leads to larger gains than using NoisyTune?
- What is the standard deviation across multiple runs for e.g., Table 1 and Figure 2? "
ARR_2022_56_review,This paper proposes a contrastive learning framework to learn phrase representations in an unsupervised way. Cluster-assisted contrastive learning is proposed to reduce noisy in-batch negatives by selecting negatives from clusters. Extensive experiments on entity clustering and topical phrase mining show the effectiveness of the proposed methods. Case studies are also provided that demonstrate coherent and diverse topical phrases can be founded by UCTopic without supervision. ,"- The paper is well-written and clearly presented.  - The proposed cluster-assisted contrastive learning objective is well-motivated and effective when finetuning the encoder on the target task further. Extensive experimental results are provided to show the significance of the proposed UCTopic versus baseline methods.  - Detailed discussion is also provided for different datasets to further analyze the effectiveness of the proposed method with respect to informativeness, diversity of the constructed phrases, and the source of phrase semantics. ",- The details of how to apply K-Means methods to obtain the pseudo labels when using the CCL and how the number of clusters will affect the final performance is missing. ,"- Given that sentence length might affect in Table 1, additional statistics of the pre-trained sentence length versus the might be good to provide.  - Could the author provide a more detailed description of how to construct the pre-training phrases and how many pre-training phrases are present and how these phrases overlapped with the downstream tasks? ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The details of how to apply K-Means methods to obtain the pseudo labels when using the CCL and how the number of clusters will affect the final performance is missing. 
- Given that sentence length might affect in Table 1, additional statistics of the pre-trained sentence length versus the might be good to provide.  - Could the author provide a more detailed description of how to construct the pre-training phrases and how many pre-training phrases are present and how these phrases overlapped with the downstream tasks? "
ARR_2022_56_review,The paper proposes a contrastive learning-based method for phrase representations and topic mining. Results show that the method achieves the best performance on topic mining and phrase representations. The proposed model can extract more diverse phrases. ,"1. The paper applies unsupervised contrastive learning to topic modeling, which is suitable for the unsupervised task. Results show that the proposed model can extract more diverse phrases. 
2. The authors also find that in the finetuning process, in-batch negative samples have a bad influence on the performance. So they propose a topic-assist contrastive learning method to reduce noises and turn the original finetuning process into a topic-specific finetuning process. 
3. Experiemnts results show that the proposed model achieves good performance on several datasets. ","1. It seems that the biggest contribution of the paper is to apply contrastive learning to topic modeling, which is of limited novelty. 
2. Batch is a sampling method, so what is the major difference between batch and the proposed one? This limits the performance significantly. ","Suggestions: 1. It is a little bit confusing in the assumptions in section 1. First of all, “The phrase semantics are determined by their context.”. As for the examples in Figure 1, the semantic of phrase “ United States” is fixed and not influenced by the context. I think the writers want to express: if we mask “United States”, we can still infer the mask phrase by its context. 
2. Writing should be strengthened. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. It seems that the biggest contribution of the paper is to apply contrastive learning to topic modeling, which is of limited novelty. 
2. Batch is a sampling method, so what is the major difference between batch and the proposed one? This limits the performance significantly. 
Suggestions: 1. It is a little bit confusing in the assumptions in section 1. First of all, “The phrase semantics are determined by their context.”. As for the examples in Figure 1, the semantic of phrase “ United States” is fixed and not influenced by the context. I think the writers want to express: if we mask “United States”, we can still infer the mask phrase by its context. 
2. Writing should be strengthened. "
ARR_2022_281_review,"This paper presents the idea of creating domain confused examples paired with contrastive learning for unsupervised domain adaptation. The authors propose identifying these puzzles in the representation space by utilizing adversarial examples---the method searches for an extreme direction that would shift a data point to the direction of the other domain. Then, the contrastive learning setup encodes these original and domain confused examples to be closer to one another, thus pulling examples from different domains closer to the domain discrimination boundary, encouraging models to learn domain invariant representations in the process. The authors further compare this method against other data generation schemes, such as back translation, finding that learning with domain confused examples provides far greater benefit. Further experiments show that the proposed method outperforms several strong baselines on two Amazon reviews benchmarks for sentiment analysis. I think this paper offers valuable contributions, both methodical as well as in the insights that are shared alongside quantitative results. ~I've also read the submitted previous reviews and feel confident in the author response. Overall, I think this paper could be a strong add to an ACL venue but I also have some concerns that I have laid below. I firmly believe that addressing these concerns would strengthen this paper to a great extent.~ I am confident that the authors have addressed my concerns to the extent possible and would recommend acceptance. I have reflected changes to my last review by striking the weaknesses that have been addressed satisfactorily. ","- The paper is well motivated, and addresses key limitations posed by both adversarial training for domain adaptation (DANN) and -contrastive learning. Additionally, the connections to and differences from prior work in computer vision are well documented as well.
- The experimental setup is well designed and the results suggest that the method clearly outperforms several strong baselines on various domain adaptation setups. The ablation studies presented by the authors alongside these results further help establish the efficacy of the proposed approach.
- The paper is well written and easy to follow. The exposition is clear, which makes the experiments easy to replicate for a reader. ","- ~I was a bit disappointed to see no error analysis presented in this paper. It would be nice to see some trends and insights into cases when this proposed approach fails to perform well.~ - ~The experimental results seem to be missing significance testing. In absence of it, it's rather unclear whether some small absolute performance improvements are statistically significant. The paper would also benefit from a greater discussion of these close cases. Such a discussion is currently missing.~ - ~A qualitative analysis of the generated domain confused examples would help the reader better understand what these examples could be. Such an analysis is also missing from the paper.~ ","~How does the issue of instability of DANN relate to the solution proposed by [1] to learn an asymmetrically relaxed alignment? My feeling is that their method would address some of this, plus it's a stronger baseline regardless as it also takes label shift into consideration. Perhaps, try and include it as a baseline?~ ~[1] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. "" Domain adaptation with asymmetrically-relaxed distribution alignment."" In International Conference on Machine Learning, pp. 6872-6881. PMLR, 2019.~ It might still be nice to include this in the camera ready. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- ~I was a bit disappointed to see no error analysis presented in this paper. It would be nice to see some trends and insights into cases when this proposed approach fails to perform well.~ - ~The experimental results seem to be missing significance testing. In absence of it, it's rather unclear whether some small absolute performance improvements are statistically significant. The paper would also benefit from a greater discussion of these close cases. Such a discussion is currently missing.~ - ~A qualitative analysis of the generated domain confused examples would help the reader better understand what these examples could be. Such an analysis is also missing from the paper.~ 
~How does the issue of instability of DANN relate to the solution proposed by [1] to learn an asymmetrically relaxed alignment? My feeling is that their method would address some of this, plus it's a stronger baseline regardless as it also takes label shift into consideration. Perhaps, try and include it as a baseline?~ ~[1] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. "" Domain adaptation with asymmetrically-relaxed distribution alignment."" In International Conference on Machine Learning, pp. 6872-6881. PMLR, 2019.~ It might still be nice to include this in the camera ready. "
ARR_2022_59_review,"The paper mainly proposes an approximation approach for transformer model inference so that it can apply homomorphic encryption. First, the author introduces the current practical homomorphic encryptions and transformer-based model. Second, the author proposes an approximation workflow by separating the fine-tuning stage into several subphases. Third, the author evaluates the approximation performance on different NLP tasks and datasets. ","Homomorphic Encryption is widely used for security and privacy protection and this problem is meaningful for real-world applications.  The experiment results in table 1 and table 2 are promising.
The paper is well-organized and easy to follow. ","The author only evaluates tinyBERT and doesn't consider the larger model BERT_base and BERT_large. When the number of encoders increases, the performance loss could drop significantly. ","in Figure 4, the blue line and orange line are almost overlapping, it is difficult to distinguish. Increasing the width of the line can help. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The author only evaluates tinyBERT and doesn't consider the larger model BERT_base and BERT_large. When the number of encoders increases, the performance loss could drop significantly. 
in Figure 4, the blue line and orange line are almost overlapping, it is difficult to distinguish. Increasing the width of the line can help. "
ARR_2022_59_review,"This paper focuses on alleviating two challenges of privacy-preserving inference of pre-trained models. The first is how to protect users’ plain text data from access by third-party service providers, and the second one is the performance issue. The method of THE-X converts a fine-tuned model into a cloud service and processes users' data without peaking the data itself. And in inference, the text is anonymous and the results are computed in the ciphertext.  Homomorphic encryption (HE) is used to guarantee the solution and multiple approximation components in the transformer model are designed. Empirical results show that THE-X could conduct privacy-preserving inference with promising results. ","- This is a pioneering work to explore the privacy-preserving transformer inference with HE. The paper presents a  whole pipeline with multiple approximation methods to support HE operations and conduct such privacy-preserving inference. The problem is worth exploring and the methods are reasonable.  - The paper is well-organized and well-written, it is easy to follow and understand the workflow and the contributions.
- The paper analyzes the components and different schedules of approximation workflow of THE-X. ","- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds.  - Another concern is that the method may not be practical. In fine-tuning, THE-X firstly drops the pooler of the pre-trained model and replaces softmax and GeLU, then conducts standard fine-tuning. For the fine-tuned model, they add LayerNorm approximation and d distill knowledge from original LN layers. Next, they drop the original LN and convert the model into fully HE-supported ops. The pipeline is too complicated and the knowledge distillation may not be easy to control.  - Only evaluating the approach on BERTtiny is also not convincing although I understand that there are other existing papers that may do the same thing. For example, a BiLSTM-CRF could yield a 91.03 F1-score and a BERT-base could achieve 92.8. Although computation efficiency and energy-saving are important, it is necessary to comprehensively evaluate the proposed approach. ","- The LayerNorm approximation seems to have a non-negligible impact on the performances for several tasks. I think it is an important issue that is worth exploring.
- I am willing to see other reviews of this paper and the response of the authors.  - Line #069: it possible -> it is possible? ",2.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",NA ,"- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds.  - Another concern is that the method may not be practical. In fine-tuning, THE-X firstly drops the pooler of the pre-trained model and replaces softmax and GeLU, then conducts standard fine-tuning. For the fine-tuned model, they add LayerNorm approximation and d distill knowledge from original LN layers. Next, they drop the original LN and convert the model into fully HE-supported ops. The pipeline is too complicated and the knowledge distillation may not be easy to control.  - Only evaluating the approach on BERTtiny is also not convincing although I understand that there are other existing papers that may do the same thing. For example, a BiLSTM-CRF could yield a 91.03 F1-score and a BERT-base could achieve 92.8. Although computation efficiency and energy-saving are important, it is necessary to comprehensively evaluate the proposed approach. 
- The LayerNorm approximation seems to have a non-negligible impact on the performances for several tasks. I think it is an important issue that is worth exploring.
- I am willing to see other reviews of this paper and the response of the authors.  - Line #069: it possible -> it is possible? "
ARR_2022_334_review,"This paper describes a methodology for collecting parallel data for the task of detoxification. This is the task of taking an offensive sentence and paraphrasing it into a so-called “neutral sentence”. The methodology is introduced by example, and by doing so creates two parallel-data datasets for detoxification. These are according to the authors the first of their kind. Along with the description of the methodology is a breakdown of the monetary costs of conducting such a resource project, which sums to a cost of 811.55$ for ~20.000 toxic → neural paraphrase samples - of these, 12.000 are unique toxic sentences. The authors train several SOTA systems on their parallel dataset and show significant improvements over unsupervised systems, emphasizing the benefits of focusing efforts on creating parallel data, as opposed utilizing non-parallel data. ","1. Useful parallel dataset for the detoxification community which has no parallel datasets. 
2. Simple and reproducible methodology/pipeline for creating similar datasets. 
3. Strong and simple evidence in favor of creating parallel resources to improve detoxification systems performance as opposed to relying on non-parallel data or unsupervised systems. 
4. Informative insights into the efficacy of the pipeline. Namely, the amount of good vs bad samples is an interesting insight ","1. Subtle biases as a result of rejecting samples:     - fetched toxic sentences for rewriting (line 215),     - toxicity classifier (line 218)     - annotator bias     These choices introduce biases into the datasets (unavoidable). The paper would benefit from attempting to quantify how useful a model trained on ParaDetox is in a more general setting. e.g. performance on other (non-parallel) toxicity datasets. 
     2. I’m uncertain about the generality of the methodology. The pricing and application are platform-specific (toloka.yandex.com) and language-specific (English). Would this be possible using another crowdsourcing platform? Or in another language? 
3. The experimental details, for the classifier and proposed Bart-model are unspecified. In particular, details regarding data splits, fine-tuning configuration, and evaluation are missing. ","### Comments and questions - How many annotators were involved in the annotation processes?
- How many samples are enough to get reasonable results with the data produced using your pipeline? You show results on the subsets of ParaNMT, but how does performance scale with the samples in ParaDetox?
- In the name of reproducibility: What embeddings are used to compute the cosine similarity?
- In the paragraph on line 267, it is described that samples that the classifier scores above the threshold of 0.8 are included. How much does this affect the precision of the classifier? W.r.t. the $F_1$ score of 0.76.
- Do you intend to share non-detoxifiable samples? This to me seems like a highly useful resource that could benefit the community. Knowing when something can’t be detoxified isn’t something that can be inherently modeled by the proposed models.
### Style I found the nested parentheses on lines 041-043 hard to read. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Subtle biases as a result of rejecting samples:     - fetched toxic sentences for rewriting (line 215),     - toxicity classifier (line 218)     - annotator bias     These choices introduce biases into the datasets (unavoidable). The paper would benefit from attempting to quantify how useful a model trained on ParaDetox is in a more general setting. e.g. performance on other (non-parallel) toxicity datasets. 
     2. I’m uncertain about the generality of the methodology. The pricing and application are platform-specific (toloka.yandex.com) and language-specific (English). Would this be possible using another crowdsourcing platform? Or in another language? 
3. The experimental details, for the classifier and proposed Bart-model are unspecified. In particular, details regarding data splits, fine-tuning configuration, and evaluation are missing. 
### Comments and questions - How many annotators were involved in the annotation processes?
- How many samples are enough to get reasonable results with the data produced using your pipeline? You show results on the subsets of ParaNMT, but how does performance scale with the samples in ParaDetox?
- In the name of reproducibility: What embeddings are used to compute the cosine similarity?
- In the paragraph on line 267, it is described that samples that the classifier scores above the threshold of 0.8 are included. How much does this affect the precision of the classifier? W.r.t. the $F_1$ score of 0.76.
- Do you intend to share non-detoxifiable samples? This to me seems like a highly useful resource that could benefit the community. Knowing when something can’t be detoxified isn’t something that can be inherently modeled by the proposed models.
### Style I found the nested parentheses on lines 041-043 hard to read. "
ARR_2022_334_review,"The authors present a new crowdsourcing pipeline to collect parallel data for the task of detoxification (i.e., a style-transfer task to convert toxic text into its non-toxic variant). The authors used their proposed pipeline to collect the first parallel English detoxification dataset (ParaDetox). ParaDetox has almost 12,000 toxic sentences where each sentence is paired with 1-3 non-toxic paraphrases. Moreover, the authors also show that their pipeline could be used to retrieve toxic and non-toxic sentence pairs from existing paraphrasing parallel datasets (e.g., ParaNMT). Furthermore, the authors demonstrated the effectiveness of their created parallel data by training various detoxification seq2seq BART-based models and showing that the trained models outperform current STOA methods. ","- A novel dataset that will encourage research and development on the task of detoxification.
- A useful pipeline that could be used to create new parallel datasets for detoxification. The pipeline also provides a faster and cheaper way to take advantage of existing parallel paraphrasing datasets to retrieve toxic/non-toxic sentence pairs.
- Clear description of the crowdsourcing pipeline.   * Nice examples of what the data looks like. I found it very useful to understand the task. ","- There are some technical aspects of the paper that weren't clear to me:   * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
  * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
     * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. ","I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- There are some technical aspects of the paper that weren't clear to me:   * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
  * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
     * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) "
ARR_2022_140_review,The paper addressed the knowledge graph completion task and proposed a commonsense-aware knowledge embedding method to learn the distributed representations of the entities and relations in the KG. The main contributions have two folds. The first is to extract commonsense to guide the negative sampling. And the second is to develop a multi-view link prediction mechanism. The experimental results on four datasets show the effectiveness of the proposed method. ,"Employing the extracted commonsense knowledge to guide the negative sampling is somewhat novel and interesting.
The experimental results show that adding the proposed negative sampling method to different kinds of knowledge graph completion methods is useful. ","The proposed method seems to be only useful for ""N-1"" and ""1-N"" relation types. How it works for ""1-1"" and ""N-N"" situations. The authors should show the detailed performance of different relation types. ","How does the performance goes when we tune the parameter alpha in equation (6) and (7)? How to determine the value of such a parameter?
When the method extracts commonsense, there is a hierarchy structure among concepts. Therefore, how to determine the head concept and tail concept? Do the method extract grandpa nodes? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The proposed method seems to be only useful for ""N-1"" and ""1-N"" relation types. How it works for ""1-1"" and ""N-N"" situations. The authors should show the detailed performance of different relation types. 
How does the performance goes when we tune the parameter alpha in equation (6) and (7)? How to determine the value of such a parameter?
When the method extracts commonsense, there is a hierarchy structure among concepts. Therefore, how to determine the head concept and tail concept? Do the method extract grandpa nodes? "
ARR_2022_82_review,"Recent works have demonstrated the knowledge-recalling capacities (robin, is a, bird) of pre-trained language models based on transformers, such as BERT, etc. In light of these findings, the authors of this paper isolate and attribute the knowledge-recalling tendencies of such models to a few neurons in the feed-forward networks (FFN) of the BERT architecture. They do so by applying the Integrated Gradients method to the hidden layers of the FFNs in BERT-base, specifically when the model receives inputs about relational knowledge. 
To ensure that the isolated knowledge-neurons consistently capture relational knowledge, the authors use several formulations of a given knowledge triple -- for instance, using {“X was produced by Y”, “X is a product of Y”, “Y and its product X”} for the relation: (X, manufacturer, Y). As a baseline attribution score, the authors choose regular, unmodified activation scores of the neurons.
The authors validate their findings by proposing two main experiments: First, they ascertain the knowledge-sensitivity of the attributed neurons (for a given relation) by comparing their activations on sentences where the head (X) and the tail (Y) words mentioned in sentences that express the relation  vs. those that do not (and instead express different facts about X and Y). 
Second, the authors demonstrate how the identified neurons can be numerically modified to update and delete relational information, showcasing notable changes in model behavior for each modification.
The paper’s auxiliary findings include notable improvements against the chosen baseline and  greater percentage of knowledge-neurons in deeper--as opposed to shallow--layers. ","The paper makes a desirable contribution to the field of ‘BlackBox NLP’ by conducting analyses into a model that has famously been shown to recall factual knowledge, moving beyond just analyzing its outputs and instead focusing on its internal mechanistic units. In general, the paper shows innovations in: 1. Its usage of variably-phrased prompts for relational information -- to me, this truly encourages robustness in isolating neuronal-capacities. 
2. Its evaluation of the robustness of the identified neurons by differentiating knowledge-expressing sentences from lexical and syntactic co-occurrence. 
2. Its demonstration of knowledge update (and erasure) via simple modifications to the knowledge neurons, further ascertaining their sensitivity to relational information. ","Though the paper excels as a result of its extensive analyses and evaluations, there is generally a lack of clarity in many areas: 1. It is not straightforwardly clear exactly which layer in the FFN is being subject to the experiments -- the authors do mention the intermediate layer in line 161 and Figure 2, but I think this should be made precisely clear that it is the value after applying the non-linear GELU function to input $H$ multiplied by weight matrix $W_1$. 2. Their attribution and isolation technique involves a variety of different parameters that seem to have been vaguely chosen (threshold t, prompt percentage p, and more importantly the ‘desired’ number of neurons). If the authors indeed conducted ablations for these values, they should at-least mention it in some form or the other. 
3. Going deeper in the choice of [2,5] neurons: I am unsure if having a fixed range of desired neurons (without any hypothesis) is entirely appropriate. I hope to see some sort of a justification or reasoning behind restricting this range, especially since the rest of the parameters depend upon it, i.e.,  the authors choose t and p so that the final set of neurons lie in [2, 5]. 
4. The knowledge-update equation in section 5.1 is a bit confusing: $\mathrm{FFN}_i^{(val)}$ seems like a scalar value (the value of a single neuron in the layer), whereas the value it is assigned is composed of vectors ($\mathbf{t}$ and $\mathbf{t}^{\prime}$). ","Questions to the authors: Would the knowledge-neurons remain robust when the words in a given relational prompt are mentioned but not in a knowledge-expressing manner? For instance, if one uses lexical primes in a prompt not meant for the relation, e,g., 'capital. X is a city in Y', do the neurons still robustly show low activations?
Suggestions: Line 393-395: Having an example here would be nice. It is also not clear how the choice of $\mathbf{t}^{\prime}$ is being made.
Line 505-507: “... have pointed out that the feed-forward network module also matters to Transformer” needs a little elaboration -- matters in what way? ",3.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",3 = Potentially useful: Someone might find the new software useful for their work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Though the paper excels as a result of its extensive analyses and evaluations, there is generally a lack of clarity in many areas: 1. It is not straightforwardly clear exactly which layer in the FFN is being subject to the experiments -- the authors do mention the intermediate layer in line 161 and Figure 2, but I think this should be made precisely clear that it is the value after applying the non-linear GELU function to input $H$ multiplied by weight matrix $W_1$. 2. Their attribution and isolation technique involves a variety of different parameters that seem to have been vaguely chosen (threshold t, prompt percentage p, and more importantly the ‘desired’ number of neurons). If the authors indeed conducted ablations for these values, they should at-least mention it in some form or the other. 
3. Going deeper in the choice of [2,5] neurons: I am unsure if having a fixed range of desired neurons (without any hypothesis) is entirely appropriate. I hope to see some sort of a justification or reasoning behind restricting this range, especially since the rest of the parameters depend upon it, i.e.,  the authors choose t and p so that the final set of neurons lie in [2, 5]. 
4. The knowledge-update equation in section 5.1 is a bit confusing: $\mathrm{FFN}_i^{(val)}$ seems like a scalar value (the value of a single neuron in the layer), whereas the value it is assigned is composed of vectors ($\mathbf{t}$ and $\mathbf{t}^{\prime}$). 
Questions to the authors: Would the knowledge-neurons remain robust when the words in a given relational prompt are mentioned but not in a knowledge-expressing manner? For instance, if one uses lexical primes in a prompt not meant for the relation, e,g., 'capital. X is a city in Y', do the neurons still robustly show low activations?
Suggestions: Line 393-395: Having an example here would be nice. It is also not clear how the choice of $\mathbf{t}^{\prime}$ is being made.
Line 505-507: “... have pointed out that the feed-forward network module also matters to Transformer” needs a little elaboration -- matters in what way? "
ARR_2022_82_review,"The paper studies how factual knowledge is stored inside the pre-trained transformer model, and presents a knowledge attribution method to identify “knowledge neurons” in feed-forward layers. The method is based on integrated gradients and several refining steps. They apply their method on the fill-in-the-blank cloze task for BERT. By comparing to a baseline (which takes activation values as the attribution score), the authors show their identified neurons are specific to the input fact and not shared with other facts, indicating the neurons are more likely to express the specific factual knowledge. Based on the identified knowledge neurons, the paper demonstrates that the factual knowledge can be updated or erased by modifying the activation values. ","- The paper is well-written and easy to follow.
- The approach of identifying knowledge neurons makes sense to me and the experiments (e.g., suppressing or amplifying the knowledge neurons) seem well-designed to show the effectiveness of the approach.
- The paper provides insightful studies on how knowledge is stored in pre-trained language models. Given the recent works on using pre-trained LMs to extract facts or to answer questions in a closed-book scheme, this paper can be of interest to the NLP community.
- The paper demonstrates a new method to modify the factual knowledge in Section 5.1. The empirical results (e.g., success rates) look strong. This is an interesting addition to the related literature in this field.
- Section 4.6 is interesting and the results are potentially useful to determine whether a sentence describes a relation between two entities or not, which might help improve auto-labeling quality in other NLP tasks based on distant supervision. ","- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer).  - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper). ","- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer).  - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper). 
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion. "
ARR_2022_34_review,"This paper reviews recent state of NLP in South Asia with special focus on historical comparative linguistics. The authors argue that the most fundamental problem of South Asian NLP is data scatteredness instead of data scarcity. The data is available to extract even for endangered languages, while the challenges are how to wrangle this data and to digitise existing non machine readable texts. The authors then present three ongoing language resource projects they are working on, i.e. dependency treebanks, etymological database, and historical linguistic analysis. In the future, the authors plan to work on text digitisation and OCR. ",Several research works presented in this paper adds to the horizon of readers' knowledge about NLP research in South Asia. ,"- This paper does not provide a clear and explicit definition of historical-comparative linguistics. Not all *ACL audiences familiarize with this area, except the member of special working group in limited workshop scope.
- It is hard to follow what are the opinion piece the authors want to share with the readers through this paper. The paper is more likely reporting what the authors are currently do and what they plan to do, but still lacks the reflection points. ",It is better if the authors can relate the discussion points with the more general context. Does the linguistics issues discussing in this paper applied only for the languages in South Asia? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- This paper does not provide a clear and explicit definition of historical-comparative linguistics. Not all *ACL audiences familiarize with this area, except the member of special working group in limited workshop scope.
- It is hard to follow what are the opinion piece the authors want to share with the readers through this paper. The paper is more likely reporting what the authors are currently do and what they plan to do, but still lacks the reflection points. 
It is better if the authors can relate the discussion points with the more general context. Does the linguistics issues discussing in this paper applied only for the languages in South Asia? "
ARR_2022_34_review,"The paper surveys the state of NLP for South Asian languages with a focus on historical linguistics. The authors first describe prior efforts in NLP and linguistics research for South Asian languages and show that most languages in South Asia are severely underrepresented. They argue that the main issue is not data scarcity; data is theoretically available, it just needs to be consolidated and used. Further, the authors highlight a number of ongoing projects and discuss potential future work. For me, the paper mainly serves as an entry point for researchers seeking to get involved in NLP and comparative linguistics for South Asian (or related) languages. ","- Addresses an important issue (underrepresentation of South-Asian languages in NLP) - Well-written with a clear structure - Provides an excellent overview of prior work on NLP and comparative linguistics for South Asian languages.
- Highlights ongoing work in this area of research and provides multiple concrete suggestions for future work, which will help interested people get involved themselves. ","Although one of the main arguments made in the paper is that the biggest obstacle for NLP for the South Asian languages is data scatteredness rather than data scarcity, these two concepts seem to be mixed in some places. E.g., the introduction first mentions “This data scarcity persists despite long native traditions of linguistic description, [...]”, but then it says “the the most basic problem in NLP/CL work on South Asian languages is not data scarcity, but data scatteredness”. Why mention the data scarcity issue then, rather than going more into detail on the scatteredness aspect? Similarly, section 3 begins with “Having established the issue of data scarcity”. I wish the distinction between these two concepts would be more clear throughout the paper. ",Typos: l. 287: serveral → several l. 315: missing space ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"Although one of the main arguments made in the paper is that the biggest obstacle for NLP for the South Asian languages is data scatteredness rather than data scarcity, these two concepts seem to be mixed in some places. E.g., the introduction first mentions “This data scarcity persists despite long native traditions of linguistic description, [...]”, but then it says “the the most basic problem in NLP/CL work on South Asian languages is not data scarcity, but data scatteredness”. Why mention the data scarcity issue then, rather than going more into detail on the scatteredness aspect? Similarly, section 3 begins with “Having established the issue of data scarcity”. I wish the distinction between these two concepts would be more clear throughout the paper. 
Typos: l. 287: serveral → several l. 315: missing space "
ARR_2022_172_review,"The main contribution area of the authors is studying the impact of Trojan tracks on language models internals depicting the following specific contributions: Analyzing the effect of Trojan attacks on attention behaviors of BERT models and highlighting the attention focus drifting effect on Trojaned models. 
Proposing a technique to detect the Trojaned BERT models using attention behavior analysis called Attention-based Trojan Detector Preparing and sharing a repository of the Trojaned and clean models with corresponding data and triggers. ","Authors have formulated the attention focus drifting ( in page 4) which helps a lot in shaping a mathematical base to understand the attention focus drifting problem they are working on. 
Authors also have done an organized investigation and framed a methodology for quantifying the effect of attention focus drifting for Trojaned models and classifying different token types. This provides a strong base to build both the Trojaned model detectors and possible defense mechanisms. 
The analysis (table 2) showing the significance of different attention drifting problems for both Trojaned and clean models provides an meaningful insight on quantification and understating  of the attention drifting behavior. ","Attention attribution and entropy terms in section 3.2.1 needs a little more context or definition or example. 
Difference between adversarial perturbation and Triggers is not clear in section 4 and why perturbations cannot cause attention focus drifting while can change the classification changes. 
Why Phrase Candidate Generator and Non-Phrase Candidate Generator are required is not clear. 
Seems the section 3.3 be not very relevant or required to both attention monitor and Trojan perturbation generation on section 4.1. The space can be used to add more clarity and more examples for section 4.1. 
Also needs more clarity on how the thresholds/ stats detailed in section 3 on quantifying the attention drifting have been utilized in section 4.1 for attention monitor. More examples and references to those attention drifting will provide much stronger utilizing them in shaping the architecture of the attention monitor. 
Section 4.1 as the main section to generate Triggers and Trojan detection needs some examples for clarification or at least in Appendix. 
Section 4.1: It is not clear whether all possible perturbation candidates are used for all clean samples or a subset of perturbations? Applying all neutral words to a full dataset can make the whole pipeline of Figure 6 not feasible, or maybe I missed anything? Or maybe by reaching the beta=15 you will stop searching for each sample? Does any under/overfitting problem can happen for Trojan detection when evaluating the ASR on unseen Trojan perturbation?
Regarding table 5: Is not clear how the ASR is calculated. For example 96% for IMDB shows that the we had a posinoied/purturbed dataset ( with injected triggers) and trained our model ( to have a Trojaned model and then evaluated that model with another unseen poisoned test dataset and the result is called ASR ( same for clean) ? The percentages are for all data samples, 96% of how many data samples? More clarity is very useful here. 
Regarding Table 6: It is also not clear to what performance the numbers ( acc or auc) refer to? Data samples, Trojaned models? ",More examples on section 4 in comparison to attention focus formulating efforts done in section 3 will add extra visibility and application to the entire research ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"Attention attribution and entropy terms in section 3.2.1 needs a little more context or definition or example. 
Difference between adversarial perturbation and Triggers is not clear in section 4 and why perturbations cannot cause attention focus drifting while can change the classification changes. 
Why Phrase Candidate Generator and Non-Phrase Candidate Generator are required is not clear. 
Seems the section 3.3 be not very relevant or required to both attention monitor and Trojan perturbation generation on section 4.1. The space can be used to add more clarity and more examples for section 4.1. 
Also needs more clarity on how the thresholds/ stats detailed in section 3 on quantifying the attention drifting have been utilized in section 4.1 for attention monitor. More examples and references to those attention drifting will provide much stronger utilizing them in shaping the architecture of the attention monitor. 
Section 4.1 as the main section to generate Triggers and Trojan detection needs some examples for clarification or at least in Appendix. 
Section 4.1: It is not clear whether all possible perturbation candidates are used for all clean samples or a subset of perturbations? Applying all neutral words to a full dataset can make the whole pipeline of Figure 6 not feasible, or maybe I missed anything? Or maybe by reaching the beta=15 you will stop searching for each sample? Does any under/overfitting problem can happen for Trojan detection when evaluating the ASR on unseen Trojan perturbation?
Regarding table 5: Is not clear how the ASR is calculated. For example 96% for IMDB shows that the we had a posinoied/purturbed dataset ( with injected triggers) and trained our model ( to have a Trojaned model and then evaluated that model with another unseen poisoned test dataset and the result is called ASR ( same for clean) ? The percentages are for all data samples, 96% of how many data samples? More clarity is very useful here. 
Regarding Table 6: It is also not clear to what performance the numbers ( acc or auc) refer to? Data samples, Trojaned models? 
More examples on section 4 in comparison to attention focus formulating efforts done in section 3 will add extra visibility and application to the entire research "
ARR_2022_306_review,"This work has created a benchmark dataset for multi-task learning for biomedical datasets. Based on this new benchmark dataset, this work has proposed instruction learning based multi-task learning, which has shown to outperform single-task learning as well as vallina multi-task learning. ","1. This work has newly aggregated more than 20 biomedical datasets in 9 categories into a new multi-task paradigm and formalize them into a text to text format so that we can build one unified model for all different tasks. 
2. This work has proposed using manually created instructions for multi-task learning so that the model can be instructed to perform each task without confusion. And this method has been shown to outperform a lot the vanilla multi-task learning and also outperform single-task learning in some cases. ","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. ","1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. ",2.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. "
ARR_2022_268_review,"This work methods for role-based dialogue summarization. They argue that information for other role’s utterances and summaries can improve the informativeness and quality of dialogue summaries. They propose two enhancements which could be applied to existing methods: a cross attention mechanism to to acquire other role’s information supported by a attention divergence loss to match the attention distribution between different roles. And a decoder self-attention-based interaction mechanism to share other role’s summary information. On two role-based dialogue summarization datasets, the authors perform automatic and human evaluation to show improvements over base architectures. ",The paper is clearly written and easy to understand. The two mechanisms proposed are individually motivated with ablated model variations showing advantage of both. Extensive experiments are performed to show the advantage of the proposed models. ,"•	It is not clear why does the user decoder at time step t uses only the information till time step t from the agent decoder and why not use the information from all the time steps?
•	The motivation of applying the attention divergence loss to force attention similarity is still not clear to me. What happens if att^a_u is made equal to att^u_u . I couldn’t find any model ablation which justifies this loss as well.  •	The human evaluation is not clearly able to identify if the model improvements actually help as the results are close and not consistent across models (although reasoning has been provided). ",Paper is well written and organized. Additional experiments to justify the the attention divergence loss can help the paper. More examples in case studies can better help in understanding the contributions. ,3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"•	It is not clear why does the user decoder at time step t uses only the information till time step t from the agent decoder and why not use the information from all the time steps?
•	The motivation of applying the attention divergence loss to force attention similarity is still not clear to me. What happens if att^a_u is made equal to att^u_u . I couldn’t find any model ablation which justifies this loss as well.  •	The human evaluation is not clearly able to identify if the model improvements actually help as the results are close and not consistent across models (although reasoning has been provided). 
Paper is well written and organized. Additional experiments to justify the the attention divergence loss can help the paper. More examples in case studies can better help in understanding the contributions. "
ARR_2022_267_review,"The authors proposed an improved version of the $\alpha$-entmax which does not require sorting during computation, which aims to reduce the latency of the operation. The proposed computation method is based on $\alpha$-RELU, which replaces the normalization variable in $\alpha$-entmax to a fixed value. Experiments show that the resultant operator is faster than 1.5-entmax on WMT14 En-De and WMT13 En-Ru translation tasks. ","- The proposed method shows that even with a potentially unnormalized distribution, it doesn't seem to harm the decoding performance of the generation model - Experiment results show that the proposed method is constantly faster than $\alpha$-entmax and the performance is not dropping ","The authors claim that the alpha entmax is slow because it requires sorting. I checked the code of  $\alpha$-entmax implemented in https://github.com/deep-spin/entmax and find that the top-k approximation is the default option with K=100 as the default setting. In this case, `torch.topk` is called in the code, and only K numbers are sorted. Therefore, if the author position the proposed method as a fast approximation to the original  $\alpha$-entmax, I think the authors shall perform more systematical comparison with the top-k approximation.
In the paper, the only comparison with top-k approximation is Figure 2 (bottom, center). However, it's actually difficult to make a conclusion for comparing performance and speed based on this figure. It seems that the proposed 1.5-ReLU is still faster in the first 20 hours of training, then the gap closes. Also, as a key performance indicator of the proposed method, it is desirable to have the actual numbers. For example, averaged seconds per batch for training and averaged seconds per 1k decoding requests for inference. It will be even better to profile just the computation time of softmax /  $\alpha$-entmax /  $\alpha$-ReLU operators.
I hope to see more detailed comparison with the top-k variant of $\alpha$-entmax on (1) translation performance (at least WMT14 En-De) and (2) execution speed (can be seconds per 1k batch, will be better if reporting the running time of just the $\alpha$-entmax/$\alpha$-ReLU layer). I might change my assessment according to the results.
Another concern is on the distribution, in Figure 7, authors showed that the sums are still concentrating to a certain value. However, the deviation is not small according to this figure. Will this impact the correctness of the language model scores (log p)? Some applications are relying on the scores for reranking purposes. ","- For evaluation, the datasets that the authors use are pretty old datasets. I'm not against using WMT14 En-De, however, WMT13 En-Ru is rare in the research community recently. ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The authors claim that the alpha entmax is slow because it requires sorting. I checked the code of  $\alpha$-entmax implemented in https://github.com/deep-spin/entmax and find that the top-k approximation is the default option with K=100 as the default setting. In this case, `torch.topk` is called in the code, and only K numbers are sorted. Therefore, if the author position the proposed method as a fast approximation to the original  $\alpha$-entmax, I think the authors shall perform more systematical comparison with the top-k approximation.
In the paper, the only comparison with top-k approximation is Figure 2 (bottom, center). However, it's actually difficult to make a conclusion for comparing performance and speed based on this figure. It seems that the proposed 1.5-ReLU is still faster in the first 20 hours of training, then the gap closes. Also, as a key performance indicator of the proposed method, it is desirable to have the actual numbers. For example, averaged seconds per batch for training and averaged seconds per 1k decoding requests for inference. It will be even better to profile just the computation time of softmax /  $\alpha$-entmax /  $\alpha$-ReLU operators.
I hope to see more detailed comparison with the top-k variant of $\alpha$-entmax on (1) translation performance (at least WMT14 En-De) and (2) execution speed (can be seconds per 1k batch, will be better if reporting the running time of just the $\alpha$-entmax/$\alpha$-ReLU layer). I might change my assessment according to the results.
Another concern is on the distribution, in Figure 7, authors showed that the sums are still concentrating to a certain value. However, the deviation is not small according to this figure. Will this impact the correctness of the language model scores (log p)? Some applications are relying on the scores for reranking purposes. 
- For evaluation, the datasets that the authors use are pretty old datasets. I'm not against using WMT14 En-De, however, WMT13 En-Ru is rare in the research community recently. "
ARR_2022_64_review,"This paper investigates the effectiveness of entity representations in multilingual language models. The proposed mLUKE model exhibits strong empirical results with the word inputs (mLUKE-W), it also also shows even better performance with the entity representations (mLUKE-E) in cross-lingual transfer tasks. The authors' analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks. Extensive experimental results suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. ","1. The authors explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. They train a multilingual language model with 24 languages with entity representations and show mLUKE model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. 
2. The authors show that a cloze-prompt-style fact completion task can effectively be solved with the query and answer space in the entity vocabulary. 
3. The results show that entity-based prompt elicits correct factual knowledge more likely than using only word representations. ","Most of languages in LAMA are rich-resourced languages indeed, the authors may need to test mLUKE on some low-resourced languages. ","This paper has done a solid work on Multilingual Pretrained Language Models. 
This paper is well written and easy to read. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Most of languages in LAMA are rich-resourced languages indeed, the authors may need to test mLUKE on some low-resourced languages. 
This paper has done a solid work on Multilingual Pretrained Language Models. 
This paper is well written and easy to read. "
ARR_2022_64_review,"This paper proposed to extend the monolingual entity representation model (LUKE) to the multilingual version and further conduct experiments on several cross-lingual tasks. The experimental results show the superiority of incorporating pre-trained multilingual entity representations. Besides, the authors also had detailed analysis of the benefits of proposed methods. Overall the work is solid with simple ideas and good results. I believe the released entity representation model would be useful for multilingual knowledge-related tasks. ","1. Well-written paper and good presentation for the method, tasks as well as analysis.
2. The method is effective by incorporating entity representations and the performance got improved for various cross-lingual transfer tasks.  3. If the pre-trained model is released, it would benefit more research about utilizing multilingual entity knowledge by either feature extraction or extra entity features. ","1. The idea is a bit incremental and simply the extension of previous monolingual LUKE. 
2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations. ",The authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The idea is a bit incremental and simply the extension of previous monolingual LUKE. 
2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations. 
The authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones. "
ARR_2022_64_review,"This paper investigates the effectiveness of entity representations in multilingual language models. They argue that using entity representations facilitates cross-lingual transfer by providing language-independent features and present a multilingual extension of LUKE. They investigate two ways of using the entity representations in cross-lingual transfer tasks: (1) perform entity linking for the input text, and append the detected entity tokens to the input sequence;  (2) use the entity [MASK] token from the MEP task as a language-independent feature extractor. 
The experimental results show that these entity-based approaches consistently outperform word-based baselines, and entity representations provide more language-agnostic features to solve the downstream tasks. 
Overall, this work has conducted a series of solid experiments to demonstrate the effectiveness of entity representations in multilingual tasks and provided in-depth analysis on entity representations. ","- investigated the effectiveness of entity representations in multilingual language models.
- present a multilingual extension of LUKE - conducted a series of solid experiments to demonstrate the effectiveness of entity representations in multilingual tasks and provided in-depth analysis on entity representations.
- suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. ","- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:     - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction     - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. ","In line ""544"", I still quite understand why using entity representations can reduce language bias. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:     - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction     - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line ""544"", I still quite understand why using entity representations can reduce language bias. "
ARR_2022_205_review,"The paper introduces an approach for easily incorporating both lexical and syntactic constraints to a model for sentence paraphrasing. Lexical constraints include words that are desirable to appear in the paraphrase, while syntactic constraints involve desired surface forms. Under CGPG, constraints are represented as text sequences that are concatenated to the source sentence. These concatenated inputs are then used to fine-tune pre-trained sequence-to-sequence models (e.g. ProphetNet, BART). A challenging aspect in this task is to obtain training data with constraint information, since datasets for paraphrasing only include <sentence, paraphrase> pairs. The authors experiment with different methods for constraint generation, following previous work. For syntactic constraints, in particular, the authors also propose a new method for extracting exemplars based on syntax similarity, called SSE. An exemplar is a natural sentence with a desired surface form for the output paraphrase. At training time, previous work generates these exemplars by replacing words in the target sentence. In contrast, the authors propose to use other natural sentences in the training set that have similar a syntactic structure to the target sentence. Automatic evaluation shows that GCPG + SSE achieves better scores than previous work on two paraphrasing datasets. The authors also conduct some human evaluation experiments, and analyse some of the characteristics of the output paraphrases to show the benefits of their proposed model. ","1. The proposed GCPG approach obtains significant better results than previous work, and appears to be simple to implement. This could motivate its application in other text-to-text generation tasks.
2. The proposed SSE method for extracting syntactic constraints seems reasonable, and it could be implemented using bigger and more diverse datasets (if I understand the approach correctly), expanding its applicability.
3. The methodology described is mostly appropriate, which makes the conclusions more reliable. I particularly appreciate that the authors conducted a human evaluation study, and that they attempted to analyse quantitatively some characteristics of the outputs that make them better than previous work. ","1. More emphasis could be given to explaining what the approach is actually doing better than previous ones. The model’s outputs do obtain higher scores using automatic metrics. However, I think the paper would benefit from more in-depth analysis of where this improvement comes from. Section 4.4 goes in the right direction by showing that models trained using GCPG suffer less from he copying problem and are able to generate more diverse ngrams than simply using ProphetNet or ParafraGPT. However, this does not show if the outputs actually better follow the constraints provided: how frequently and correctly do the outputs do incorporate the lexical items indicated as constraints better than previous work? How frequently and correctly do the outputs follow the syntactic structure of the exemplar (for example) better than previous work?
2. Automatic evaluation shows (according to Table 2) that GCPG outperforms previous work in almost all metrics, but TED. Do the authors have an intuition (or better yet, evidence) of why this happens? As far as I understand, TED intends to measure how close the syntactic trees of both reference and outputs are, which is indicative of how much the model actually followed the syntactic condition. If that’s the case, then GCPG models are not better than CGEN and SGCP for this important criteria. However, this only happens in the ParaNMT dataset and not in QQP-Pos. Could the authors suggest an explanation for this? Is it related to the metric itself or maybe to some characteristics of the datasets?
3. In NLG tasks, human evaluation is especially important. As such, I would request the authors to provide more details about the methodology they followed. For instance, it would be helpful to know: (a) how was the quality of the data collected ensured considering how noisy crowdsourced scores can be? what type of quality controls were implemented; (b) what was the interannotator agreement/reliability for the data collected?; ( c) what were the exact instructions provided to the crowdworkers? The authors provide very short explanations for loyalty (meaning preservation? adequacy?), fluency and, specially, syntax similarity. For this last one, in particular, it is important to know how the annotators were instructed (maybe trained?) to assess it.
4. It is curious that, in the human evaluation study, the authors did not include results for GCPG-S only (Table 4). This would have helped support the statements that combining lexical and syntax constraints contribute to obtain better paraphrases, rather than applying them independently, which is somewhat suggested by the automatic evaluation. ","1. I strongly recommend to explicitly state from the beginning of the paper the language that is being studied. On why that is important, I would suggest reading **Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science** (Bender and Friedman, 2018) or https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ 2. I think readers unfamiliar with the area could appreciate more motivation for controllability in paraphrasing. For instance, in lines 43-44, the authors state that “lacking control might result in undesirable results”. Perhaps mentioning a couple of such “undesirable” outcomes could help with explaining why this type of controllability is important/necessary. Same with line 46, where the authors state “To obtain desirable surface forms..”. It could help to provide an example of a real-world situation/application where a user would want to indicate a desirable syntactic form for the output paraphrase. For example, in Text Simplification, one might know that a certain target audience benefits from reading sentences expressed in active voice. So, one could attempt to control the paraphrasing based on that syntactic structure.
3. In lines 70-71, the authors state: “Since sentential exemplar are only available for testing, …”. At this point, the reader does not know that this is due to the way the available datasets for paraphrasing have been created (explained later in the paper). I think it would help to briefly explain why this happens, or to link the reader to the section of the paper where this is explained.
4. Lines 211-214: The authors indicate that the conditions are concatenated to the source sentences via the [SEP] token. At this point, by looking at Figure 2, it would seem that more than one syntactic constraint can be concatenated at the time. However, by looking at the Experiments section, that’s not the case. Maybe it would be helpful to clarify that tin the Figure caption. In addition, how are syntactic and lexical conditions combined before being concatenated to the source sentence? Are they also separated by a [SEP] token among themselves?
5. The Exemplar Dictionary plays a key role in generating training instances. As such, it would be beneficial for reproducibility to provide more details on how it was constructed. My guess is that for every sentence in the training set, its truncated LCT was obtained, and then a dictionary was created with these LCT being keys. If any other further post-processing or filtering was performed, then I think it should be stated. In addition, could the authors provide more information about the size of the dictionary and the average number of natural sentences for each LCT? Finally, according to my understanding, any dataset could be used to create this Exemplar Dictionary since the LCTs are obtained automatically from any sentence. Is there any specific reason why the authors decided to use the training set and not a perhaps bigger dataset that would provide more variability to the exemplars?
6.  Lines 301-328. This section explains how the Lexical Conditions are extracted for both the training and the test sets. However, it is confusing, mainly for the test set. At first, the authors state that two strategies are used: one extracting keywords from the references, and another using a sequence-to-sequence model to predict them from the source sentence. Then the authors say that they used three methods: TF-IDF, TextRank and KeyBERT. As far as I know, neither of those three are sequence to sequence models. So, was that second strategy from (Liu et al.,  2020a) actually used? Looking at Table 3, it does not appear anywhere either.
7. In Table 2, the authors use “B-R” (BLEU-R), shouldn’t it be BLEU-4?
8. While the paper can be easily read and understood, here I provide a few suggestions for replacing some words or phrases for clarity: - Lines 76-77: there lacks —> there is no - Line 118: significant performances —> very good/high/strong performances - Line 139: provide —> introduce/develop - Line 229: most simple —> simplest - Line 256: use \citep instead of \citet for citing But et al. (2021).
- Line 508: For the lack of space —> Due to space constraints - In several instances (167, 181, 460, etc.) the word “besides” should be replaced by “also” or “in addition”. Please makes a few more passes on the paper to double-check this. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. More emphasis could be given to explaining what the approach is actually doing better than previous ones. The model’s outputs do obtain higher scores using automatic metrics. However, I think the paper would benefit from more in-depth analysis of where this improvement comes from. Section 4.4 goes in the right direction by showing that models trained using GCPG suffer less from he copying problem and are able to generate more diverse ngrams than simply using ProphetNet or ParafraGPT. However, this does not show if the outputs actually better follow the constraints provided: how frequently and correctly do the outputs do incorporate the lexical items indicated as constraints better than previous work? How frequently and correctly do the outputs follow the syntactic structure of the exemplar (for example) better than previous work?
2. Automatic evaluation shows (according to Table 2) that GCPG outperforms previous work in almost all metrics, but TED. Do the authors have an intuition (or better yet, evidence) of why this happens? As far as I understand, TED intends to measure how close the syntactic trees of both reference and outputs are, which is indicative of how much the model actually followed the syntactic condition. If that’s the case, then GCPG models are not better than CGEN and SGCP for this important criteria. However, this only happens in the ParaNMT dataset and not in QQP-Pos. Could the authors suggest an explanation for this? Is it related to the metric itself or maybe to some characteristics of the datasets?
3. In NLG tasks, human evaluation is especially important. As such, I would request the authors to provide more details about the methodology they followed. For instance, it would be helpful to know: (a) how was the quality of the data collected ensured considering how noisy crowdsourced scores can be? what type of quality controls were implemented; (b) what was the interannotator agreement/reliability for the data collected?; ( c) what were the exact instructions provided to the crowdworkers? The authors provide very short explanations for loyalty (meaning preservation? adequacy?), fluency and, specially, syntax similarity. For this last one, in particular, it is important to know how the annotators were instructed (maybe trained?) to assess it.
4. It is curious that, in the human evaluation study, the authors did not include results for GCPG-S only (Table 4). This would have helped support the statements that combining lexical and syntax constraints contribute to obtain better paraphrases, rather than applying them independently, which is somewhat suggested by the automatic evaluation. 
1. I strongly recommend to explicitly state from the beginning of the paper the language that is being studied. On why that is important, I would suggest reading **Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science** (Bender and Friedman, 2018) or https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ 2. I think readers unfamiliar with the area could appreciate more motivation for controllability in paraphrasing. For instance, in lines 43-44, the authors state that “lacking control might result in undesirable results”. Perhaps mentioning a couple of such “undesirable” outcomes could help with explaining why this type of controllability is important/necessary. Same with line 46, where the authors state “To obtain desirable surface forms..”. It could help to provide an example of a real-world situation/application where a user would want to indicate a desirable syntactic form for the output paraphrase. For example, in Text Simplification, one might know that a certain target audience benefits from reading sentences expressed in active voice. So, one could attempt to control the paraphrasing based on that syntactic structure.
3. In lines 70-71, the authors state: “Since sentential exemplar are only available for testing, …”. At this point, the reader does not know that this is due to the way the available datasets for paraphrasing have been created (explained later in the paper). I think it would help to briefly explain why this happens, or to link the reader to the section of the paper where this is explained.
4. Lines 211-214: The authors indicate that the conditions are concatenated to the source sentences via the [SEP] token. At this point, by looking at Figure 2, it would seem that more than one syntactic constraint can be concatenated at the time. However, by looking at the Experiments section, that’s not the case. Maybe it would be helpful to clarify that tin the Figure caption. In addition, how are syntactic and lexical conditions combined before being concatenated to the source sentence? Are they also separated by a [SEP] token among themselves?
5. The Exemplar Dictionary plays a key role in generating training instances. As such, it would be beneficial for reproducibility to provide more details on how it was constructed. My guess is that for every sentence in the training set, its truncated LCT was obtained, and then a dictionary was created with these LCT being keys. If any other further post-processing or filtering was performed, then I think it should be stated. In addition, could the authors provide more information about the size of the dictionary and the average number of natural sentences for each LCT? Finally, according to my understanding, any dataset could be used to create this Exemplar Dictionary since the LCTs are obtained automatically from any sentence. Is there any specific reason why the authors decided to use the training set and not a perhaps bigger dataset that would provide more variability to the exemplars?
6.  Lines 301-328. This section explains how the Lexical Conditions are extracted for both the training and the test sets. However, it is confusing, mainly for the test set. At first, the authors state that two strategies are used: one extracting keywords from the references, and another using a sequence-to-sequence model to predict them from the source sentence. Then the authors say that they used three methods: TF-IDF, TextRank and KeyBERT. As far as I know, neither of those three are sequence to sequence models. So, was that second strategy from (Liu et al.,  2020a) actually used? Looking at Table 3, it does not appear anywhere either.
7. In Table 2, the authors use “B-R” (BLEU-R), shouldn’t it be BLEU-4?
8. While the paper can be easily read and understood, here I provide a few suggestions for replacing some words or phrases for clarity: - Lines 76-77: there lacks —> there is no - Line 118: significant performances —> very good/high/strong performances - Line 139: provide —> introduce/develop - Line 229: most simple —> simplest - Line 256: use \citep instead of \citet for citing But et al. (2021).
- Line 508: For the lack of space —> Due to space constraints - In several instances (167, 181, 460, etc.) the word “besides” should be replaced by “also” or “in addition”. Please makes a few more passes on the paper to double-check this. "
ARR_2022_205_review,"The paper proposed a unified framework to exert both syntactic and keyword control for paraphrase generation. The paper proposes to convert both syntactic controlled generation (target syntax can be represented by either masked sequence, linearized constituency tree, masked template, or exemplar sentences) and keyword control to a sequence-to-sequence task formulation. 
The paper makes the following main contributions: 1. Their new seq-2se1 formulation allows use of pre-trained language models for controlled paraphrase generation. The paper reports impressive gains (>10 R-1 improvement) over prior work for two paraphrase datasets. 
2. For the exemplar-based syntax control: the paper proposes a new method of creating training exemplars. This is different from prior work that only had exemplar availability at test time. 
3. The paper shows that the proposed approach can be simultaneously used for both syntactic and lexical control. ","1. This work creates a new SOTA for syntax-controlled paraphrase generation. While my impression is that most of this gain is due to use of pre-trained transformer models (almost all prior work in this direction uses LSTM-based models), this work unifies this prior work into a seq-2-seq framework. 
2. This work is the first to explore joint syntax + lexically controlled paraphrase generation. ","- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.  - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings. ","- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better.  Missing References: Syntactically controlled paraphrase generation:  Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control  <-- this is a contemporaneous work, but would be nice to cite in next version.  Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.  - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings. 
- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better.  Missing References: Syntactically controlled paraphrase generation:  Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control  <-- this is a contemporaneous work, but would be nice to cite in next version.  Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization "
ARR_2022_110_review,"This paper is about the task of vision-and-language navigation (VLN), with the aim of solving two challenges: 1. utilizing multilingual instructions for improved instruction-path grounding 2. navigating through unseen environments.
To do so, an agent is trained to learn shared and visually-aligned cross lingual representations (English, Hindi, Telugu), and an environment agnostic visual representation.  Two major claims are: 1. improve SOTA performance when generalizing to unseen environments (Room-across-room dataset) 2. learned representations can be successfully transferred to other datasets. ","1. The work takes the VLN task into a very interesting and practical dimension of understanding multilingual instructions; this is especially intriguing for Indic languages (such as Hindi and Telugu) which colloquially often employ code-switching between different Indic languages or between Indic languages and English (for example the word English word ""sink"" is used in the Hindi instruction in Figure 1, and a transliteration of ""wash basin"" has been used in Telugu). An agent that understands such phenomena could be quite useful in such situations. 
2. The work is well motivated -- different languages may contain different forms of instructions and learning jointly could lead to more robust representations. 
3. Overfitting is a significant challenge in reinforcement learning methods in navigation.  The proposed solution addresses that issue by means of contrastive learning between semantically aligned paths. 
4. Experimental settings seem to be sound, and benchmarking is exhaustive. 
5. Gains w.r.t. baselines are non-trivial on multiple evaluation metrics. ","1. My biggest concern is that the analysis doesn't provide insights on WHICH language benefits WHICH other language in a multilingual setup. The only comparison provided is mono-vs-tri-lingual, but we should also compare vs bi-lingual (For example -- comparing En+Hi, En+Te, Hi+Te vs En+Hi+Te).  This analysis may point to interesting observations on which language combination is more effective. I understand that the authors want to show us that multi-lingual representations improve performance, but understanding the relationship between different languages in this context of VLN is an equally important aspect -- the experimental setting of this paper is perfectly poised to provide this analysis, but the results are tucked away in the Appendix -- this should be moved to the main paper. 
2. [ presentation of results] Table 2 should also include the best result from Shen et al. 2021 (one of the baselines), so that we can compare the finegrained results on each language and metric. 
3. There seems to be some discrepancy between the scores for RxR in Table 1 and the scores reported by Shen et al for RxR (see Table 4 in Shen et al.) -- could you elaborate why? 
4. Sec 6.3 results are provided inline, but not as a figure/table.  It is difficult to understand the improvements easily. I would strongly recommend making this a table, and also providing SOTA methods for R2R and CVDN in that table (not just the baselines). This will tell us how far away the proposed transferred model fares vs fully supervised models on the respective datasets. 
5. Sec 6.3: The definition of ""transfer"" is unclear.  Does it mean that only the navigation step is trained on R2R and CVDN while the representation step is used from RxR? Or are both steps retrained (in that case it isn't a transfer).  What about zero-shot transfer from RxR to R2R? How does the CLEAR approach fare on zero-shot? 
6. Sec N of the appendix includes another baseline (SimCSE, Gao et al) -- but this is missing from the tables in the main paper. Why? 
7. Are LSTMs a better choice for the navigation step? Would a transformer/attention mechanism be better suited for learning longer sequences? This architectural choice should be justified. 
8. In Related Work (Sec 2), other datasets/methods for V&L with multilinguality are mentioned (VQA, captioning, retrieval...) -- is your method inspired by/related to any of these methods? If not, how is it different? ","1. Sec 6.3 is titled ""Generalization to other V&L tasks"" -- this is misleading since both eval datasets are also VLN datasets. Other tasks has the connotation of other V&L tasks such as VQA, captioning, ...  2. There seems to be a related cross-lingual VLN dataset (English-Chinese) based on R2R -- https://arxiv.org/abs/1910.11301 .  You might want to test CLEAR on this benchmark if possible -- I'm not sure how practical it is, so I've not included this under ""Weaknesses"".  At the very least, it would be prudent to include this in related work. 
3. Overall, I would encourage the authors to re-think which analysis should go into the main paper and which in the appendix. In my opinion, every study that directly contributed to the 2 claims from the abstract, should be moved to the main paper.  The main contribution of this paper seems to be the multi-lingual training -- as such ablation studies about this part of the algorithm are more important that the ablation study about visual features.  (Table 9 for example) This paper makes a step in a very interesting direction -- however, I would like to see (1) re-organization of the analysis (between main and appendix), (2) focused and precise analysis of multi-lingual training, and (3) more details and exhaustive experiments in the transfer learning setup. My current score of 3.5 reflects these weaknesses + other unclear details that I've mentioned in ""Weaknesses"" above.   In case the paper doesn't go through to your ideal *ACL venue, please address these issues before resubmitting and I'll be happy to continue as a reviewer of this paper. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. My biggest concern is that the analysis doesn't provide insights on WHICH language benefits WHICH other language in a multilingual setup. The only comparison provided is mono-vs-tri-lingual, but we should also compare vs bi-lingual (For example -- comparing En+Hi, En+Te, Hi+Te vs En+Hi+Te).  This analysis may point to interesting observations on which language combination is more effective. I understand that the authors want to show us that multi-lingual representations improve performance, but understanding the relationship between different languages in this context of VLN is an equally important aspect -- the experimental setting of this paper is perfectly poised to provide this analysis, but the results are tucked away in the Appendix -- this should be moved to the main paper. 
2. [ presentation of results] Table 2 should also include the best result from Shen et al. 2021 (one of the baselines), so that we can compare the finegrained results on each language and metric. 
3. There seems to be some discrepancy between the scores for RxR in Table 1 and the scores reported by Shen et al for RxR (see Table 4 in Shen et al.) -- could you elaborate why? 
4. Sec 6.3 results are provided inline, but not as a figure/table.  It is difficult to understand the improvements easily. I would strongly recommend making this a table, and also providing SOTA methods for R2R and CVDN in that table (not just the baselines). This will tell us how far away the proposed transferred model fares vs fully supervised models on the respective datasets. 
5. Sec 6.3: The definition of ""transfer"" is unclear.  Does it mean that only the navigation step is trained on R2R and CVDN while the representation step is used from RxR? Or are both steps retrained (in that case it isn't a transfer).  What about zero-shot transfer from RxR to R2R? How does the CLEAR approach fare on zero-shot? 
6. Sec N of the appendix includes another baseline (SimCSE, Gao et al) -- but this is missing from the tables in the main paper. Why? 
7. Are LSTMs a better choice for the navigation step? Would a transformer/attention mechanism be better suited for learning longer sequences? This architectural choice should be justified. 
8. In Related Work (Sec 2), other datasets/methods for V&L with multilinguality are mentioned (VQA, captioning, retrieval...) -- is your method inspired by/related to any of these methods? If not, how is it different? 
1. Sec 6.3 is titled ""Generalization to other V&L tasks"" -- this is misleading since both eval datasets are also VLN datasets. Other tasks has the connotation of other V&L tasks such as VQA, captioning, ...  2. There seems to be a related cross-lingual VLN dataset (English-Chinese) based on R2R -- https://arxiv.org/abs/1910.11301 .  You might want to test CLEAR on this benchmark if possible -- I'm not sure how practical it is, so I've not included this under ""Weaknesses"".  At the very least, it would be prudent to include this in related work. 
3. Overall, I would encourage the authors to re-think which analysis should go into the main paper and which in the appendix. In my opinion, every study that directly contributed to the 2 claims from the abstract, should be moved to the main paper.  The main contribution of this paper seems to be the multi-lingual training -- as such ablation studies about this part of the algorithm are more important that the ablation study about visual features.  (Table 9 for example) This paper makes a step in a very interesting direction -- however, I would like to see (1) re-organization of the analysis (between main and appendix), (2) focused and precise analysis of multi-lingual training, and (3) more details and exhaustive experiments in the transfer learning setup. My current score of 3.5 reflects these weaknesses + other unclear details that I've mentioned in ""Weaknesses"" above.   In case the paper doesn't go through to your ideal *ACL venue, please address these issues before resubmitting and I'll be happy to continue as a reviewer of this paper. "
ARR_2022_74_review,"The paper explores the task of detecting of adversarial examples in the space of NLP problems, as opposed to the more common types of defence against them (e.g. adversarial training).  Models for this task require large sets of adversarial examples, and the paper notes as a contribution that it provides a dataset of such examples from “four popular attack methods on three datasets and four NLP models to encourage further research in this field”.  The paper also proposes a model for the task, which it compares against a range of baseline methods. ","- This is an important task that has been underexplored in the NLP space.  It has been investigated more in the image processing space, where a distinct set of techniques has been proposed, argued for, and contrasted with “pro-active” methods like adversarial training.
- It’s a good, effective proposed detection method.  It’s described as a “competitive baseline”; that feels a bit like underselling it, as there’s already an existing baseline (FGWS) in addition to the common-sense PPL, and the proposed method is well motivated, well investigated (e.g. in the analysis of error bounds in App A.2) and performs well with respect to these existing baselines.
- There are a lot of detailed method definitions and experimental results provided in the main paper and in the supplementary material, including ablation studies etc, helpfully pre-empting a lot of potential questions.  (But not all: see below.) ","- This isn’t a major weakness, but the paper could do a better job at the start of characterizing what the innovation of the approach to the task is.  I’d suggest the authorsnote up front in Sec 1 the FGWS method used as a comparator as an existing detection method: the current intro conveys the idea that existing methods are more limited than they are, and this is only rectified in Sec 5.  In particular, I’d characterise the differences with respect to FGWS – at a high level they have the similar idea of detecting low frequency samples (Fig 1 caption of present paper), but the approach is quite different.  Also, DISP as a key baseline for the FGWS work.
- The pre-generation of adversarial examples doesn’t feel like a major contribution, although it’s true it will be help for others developing adversarial example detection techniques.
- Again, not a major weakness, but the MLE baseline is completely unclear to me.  Line 363 says that MLE denotes the technique of Lee et al (2018) from image processing, but I don’t know which technique from that paper, and I don’t know how it’s adapted for the tasks in this paper.  In Lee et al (2018), their core technique is based on a Mahalanobis distance-based score, which doesn't sound like the brief description of MLE here; neither of their baselines sound like MLE either.  Shortly after that, a list of “two variants of our method” contains three items, including MLE.  I don’t know whether there was just some bad editing around this section, but I really don’t know what the MLE baseline method is supposed to be. ","- line 69: adversrial - line 121: Should “a” begin a new sentence?  Is there something missing?
- line 456-458: Maybe the reason FGWS works best on PWWS is that they’re closely aligned: both use word-based notions of weighted frequency-related characteristics.  It would be interesting to know if FGWS still works best against a version of PWWS that doesn’t align quite so closely.
- Sec 5 on related work is reasonable, although could include the most recent work from the image processing space, e.g. [CSG20].
[CSG20] Gilad Cohen, Guillermo Sapiro, Raja Giryes (2020). Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors. Proc. CVPR. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- This isn’t a major weakness, but the paper could do a better job at the start of characterizing what the innovation of the approach to the task is.  I’d suggest the authorsnote up front in Sec 1 the FGWS method used as a comparator as an existing detection method: the current intro conveys the idea that existing methods are more limited than they are, and this is only rectified in Sec 5.  In particular, I’d characterise the differences with respect to FGWS – at a high level they have the similar idea of detecting low frequency samples (Fig 1 caption of present paper), but the approach is quite different.  Also, DISP as a key baseline for the FGWS work.
- The pre-generation of adversarial examples doesn’t feel like a major contribution, although it’s true it will be help for others developing adversarial example detection techniques.
- Again, not a major weakness, but the MLE baseline is completely unclear to me.  Line 363 says that MLE denotes the technique of Lee et al (2018) from image processing, but I don’t know which technique from that paper, and I don’t know how it’s adapted for the tasks in this paper.  In Lee et al (2018), their core technique is based on a Mahalanobis distance-based score, which doesn't sound like the brief description of MLE here; neither of their baselines sound like MLE either.  Shortly after that, a list of “two variants of our method” contains three items, including MLE.  I don’t know whether there was just some bad editing around this section, but I really don’t know what the MLE baseline method is supposed to be. 
- line 69: adversrial - line 121: Should “a” begin a new sentence?  Is there something missing?
- line 456-458: Maybe the reason FGWS works best on PWWS is that they’re closely aligned: both use word-based notions of weighted frequency-related characteristics.  It would be interesting to know if FGWS still works best against a version of PWWS that doesn’t align quite so closely.
- Sec 5 on related work is reasonable, although could include the most recent work from the image processing space, e.g. [CSG20].
[CSG20] Gilad Cohen, Guillermo Sapiro, Raja Giryes (2020). Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors. Proc. CVPR. "
ARR_2022_74_review,"The authors released a dataset of adversarial examples generated with four NLP models for three text classification datasets under four attack algorithms. The dataset was created to evaluate how well a model can discriminate adversarial examples from clean ones. A baseline for detecting adversarial examples also has been proposed in which the authors assume that the distributions of the clean example’s features follow a multivariate Gaussian distribution and apply kPCA to reduce the feature dimensionality and MCD to remove the outliers in the training set. However, it is not very useful to statically detect the adversarial examples generated by a particular attack algorithm because, in the real situation, an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown in advance. To me, it seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is not sufficient for such modeling. ","This paper is generally well written and easy to follow. 
The proposed baseline based on density estimation achieved the highest AUC on 21 out of dataset-attack-model combinations. ","(1) It is not very useful to detect the adversarial examples generated in advance by a particular attack algorithm because an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown. 
(2) It seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is normally not sufficient for such modeling. 
(3) It is unrealistic to assume that a defender knows which attack algorithm is used unless the authors show that the adversarial examples generated by different attack algorithms share some common features that can be leveraged to detect them. 
(4) The method used to create the dataset seems very simple. 
(5) In the experimentation, the authors mentioned that ""… our method is relatively susceptible to PWWS"". But the authors did not give any reasonable explanation. ","The authors would better evaluate the performance of the defense method combined with the proposed detection model compared to existing defense methods.
Some typos: (1) counter measure --> countermeasure (2) relatively little efforts --> relatively few efforts (3) be harmful for --> be harmful to (4) detecting adversrial misspellings --> detecting adversarial misspellings (5) require training nor validation --> require training or validation ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"(1) It is not very useful to detect the adversarial examples generated in advance by a particular attack algorithm because an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown. 
(2) It seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is normally not sufficient for such modeling. 
(3) It is unrealistic to assume that a defender knows which attack algorithm is used unless the authors show that the adversarial examples generated by different attack algorithms share some common features that can be leveraged to detect them. 
(4) The method used to create the dataset seems very simple. 
(5) In the experimentation, the authors mentioned that ""… our method is relatively susceptible to PWWS"". But the authors did not give any reasonable explanation. 
The authors would better evaluate the performance of the defense method combined with the proposed detection model compared to existing defense methods.
Some typos: (1) counter measure --> countermeasure (2) relatively little efforts --> relatively few efforts (3) be harmful for --> be harmful to (4) detecting adversrial misspellings --> detecting adversarial misspellings (5) require training nor validation --> require training or validation "
ARR_2022_100_review,"The paper extrinsically studies different methods to transfer visual knowledge to language models (LMs) for commonsense reasoning tasks assumed to require physical/visual knowledge. 
The methods differ in  (i) the domain of the data for pre-training (e.g., pre-training LMs on text of the visual domain (MSCOCO captions), and training visual-linguistic models using MSCOCO's images-caption pairs), as well as in  (ii) the pre-training objectives (voken classification, MLM, cross-modal contrastive learning (CMCL), cross-modal knowledge distillation).  Models: The authors experimentally compare text-only BERT-base and a range of existing cross-modal models, pretrained with the different objectives, as far as applicable.   Experiments are conducted on 5 downstream tasks (PIQA, Visual Paraphrasing, OBQA, Metaphora on RiddleSense). In both, the low-resource and fully supervised setting, using contrastive learning (CL) seems to be the only beneficial cross-modal pre-training method (ii) applied for the visual-linguistic models (and only with additional sampling strategies in the case of pre-training on *image*-caption pairs)---these models slightly improve over the cross-modal baseline models (MLM) as well as the text-based LM (BERT).  The authors further demonstrate that for general language understanding (GLUE benchmark), the visual-linguistic models overall are less effective than text-only BERT. ","- A comparison of a range of pretraining objectives and data for cross-modal learning towards representations/models that capture commonsense knowledge - Experiments on multiple downstream tasks, suggesting a  benefit of cross-modal contrastive learning with additional sampling strategies - Overal well written and easy to follow ","- What do we learn overall? That visual information, from captions or images, together with contrastive learning, is slightly beneficial for downstream tasks underlying visual commonsense? I am  missing insights into       - why the other pretraining strategies fail           - why there is, overall, basically no difference in performance between pre-training on captions or on images (shouldn't the latter even be more informative than captions?) 
         - why pre-training on visual-linguistic data is harmful for general NLU instead of just not beneficial. 
     - It is positive that such a range of different visual-linguistic models have been examined. However, from their descriptions in Section 3 I am not sure I can assess whether these models are fairly comparable.  - Text-based comparison baseline: BERT-base is a weak model compared to BERT-large or RoBERTa -- would the conclusion that visual information can be integrated and leads to better commonsense reasoning behaviour hold when these stronger purely text-based LMs were used? ","- l222: *the* fully - l224f: *the* low-resource, *the* train data, *set* the size - l281: call *it* - l483: This *suggests* - Equ. ( 2): double-check i vs. j in denominator ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- What do we learn overall? That visual information, from captions or images, together with contrastive learning, is slightly beneficial for downstream tasks underlying visual commonsense? I am  missing insights into       - why the other pretraining strategies fail           - why there is, overall, basically no difference in performance between pre-training on captions or on images (shouldn't the latter even be more informative than captions?) 
         - why pre-training on visual-linguistic data is harmful for general NLU instead of just not beneficial. 
     - It is positive that such a range of different visual-linguistic models have been examined. However, from their descriptions in Section 3 I am not sure I can assess whether these models are fairly comparable.  - Text-based comparison baseline: BERT-base is a weak model compared to BERT-large or RoBERTa -- would the conclusion that visual information can be integrated and leads to better commonsense reasoning behaviour hold when these stronger purely text-based LMs were used? 
- l222: *the* fully - l224f: *the* low-resource, *the* train data, *set* the size - l281: call *it* - l483: This *suggests* - Equ. ( 2): double-check i vs. j in denominator "
ARR_2022_100_review,"This paper explores whether intermediate pre-training on visual knowledge can improve performance on commonsense reasoning tasks. The visual knowledge can be categorized into textual knowledge from captions and knowledge from image-caption pairs. Authors first study which types of knowledge can benefit which tasks. Besides, the authors also explore which knowledge transfer methods are beneficial for the downstream tasks. Experimental results show that visual knowledge helps tasks related to physical commonsense, especially in few-shot settings. Also, models can be further improved with the help of knowledge distillation and contrastive learning methods. ","1. Meaningful analysis: It is a natural idea to study how vision modality can help textual tasks. And intermediate pre-training on visual knowledge is a great choice to evaluate the role of visual knowledge in the process. 
2. Comprehensive experiments: Authors attempt to reproduce the most recent methods including VidlanKD and Vokenization, and perform analysis on various aspects which can influence the transferability. I'm impressed by the hard work of authors. ","1. The effect of different corpus: The sizes of different text corpus are different. But in the current experimental setup, authors didn't consider this factor. I'm also a bit confusing about why the authors choose MSCOCO as the caption dataset instead of the larger Conceptual Captions. 
2. Lack of knowledge probing tasks: Since the paper mainly studies how to evaluate the ""knowledge"" transfer, it is natural to consider adopting knowledge probing task such as LAMA (Petroni et al., 2019) to evaluate whether models already study the knowledge. ",Most comments are mentioned in the weakness part. I suggest authors add the knowledge probing tasks in the updated version and conduct some qualitative analysis to show if the models indeed learn the visual knowledge and which kinds of knowledge they learn more. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. The effect of different corpus: The sizes of different text corpus are different. But in the current experimental setup, authors didn't consider this factor. I'm also a bit confusing about why the authors choose MSCOCO as the caption dataset instead of the larger Conceptual Captions. 
2. Lack of knowledge probing tasks: Since the paper mainly studies how to evaluate the ""knowledge"" transfer, it is natural to consider adopting knowledge probing task such as LAMA (Petroni et al., 2019) to evaluate whether models already study the knowledge. 
Most comments are mentioned in the weakness part. I suggest authors add the knowledge probing tasks in the updated version and conduct some qualitative analysis to show if the models indeed learn the visual knowledge and which kinds of knowledge they learn more. "
ARR_2022_215_review,"In this paper the authors expand on the CrowS-Pairs dataset from Nangia et al. 2020 for evaluating stereotyping in large language models by constructing a parallel version of the corpus in French, including both translated examples and some novel examples specific to the French context. The authors catalog numerous aspects of the process of doing so, including identifying some issues with the original dataset which they work to remedy. They test multiple BERT variants on the new corpora and show that multilingual BERT exhibits the lowest bias. ","The paper is clear and well written. The annotation and corpus development process followed is well-described, and seems incredibly careful and thoughtful. Readers will learn a lot about stereotyping phenomena in language as well as the crucial details of corpus development from reading this paper. I see few flaws; it simply seems like careful, interesting work. ","The primary one I would mention is when I read the abstract and start of the paper, I was excited about and hoping this paper would include analysis that attempted to quantitatively compare the English and French examples to extract instances of differing bias. The paper does this very well qualitatively and with careful human analysis, but not the sort of quantitative comparison I was expecting - I think this would be an exciting area for future work.
Also, is there a notion of statistical significance that could be applied here to understand whether the biases we see are greater than expected by chance? ","For clarity I would add significantly to captions; for instance, Table 4 should specify the scale of the scores and that e.g. a score of 50 indicates an absence of bias. Also double-check some of your numbers. For instance, the text says ""Multilingual BERT is the model with the lowest degree of bias, with a score of 53 for English and 50.17 for French,"" but the table says 52.9 for English, so why the difference in significant figures?
This is likely a topic for future work, but a core question that arose for me is why multilingual BERT displayed so much less bias than the other models. Is this something that could be explored further? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The primary one I would mention is when I read the abstract and start of the paper, I was excited about and hoping this paper would include analysis that attempted to quantitatively compare the English and French examples to extract instances of differing bias. The paper does this very well qualitatively and with careful human analysis, but not the sort of quantitative comparison I was expecting - I think this would be an exciting area for future work.
Also, is there a notion of statistical significance that could be applied here to understand whether the biases we see are greater than expected by chance? 
For clarity I would add significantly to captions; for instance, Table 4 should specify the scale of the scores and that e.g. a score of 50 indicates an absence of bias. Also double-check some of your numbers. For instance, the text says ""Multilingual BERT is the model with the lowest degree of bias, with a score of 53 for English and 50.17 for French,"" but the table says 52.9 for English, so why the difference in significant figures?
This is likely a topic for future work, but a core question that arose for me is why multilingual BERT displayed so much less bias than the other models. Is this something that could be explored further? "
ARR_2022_215_review,"This paper presents an extension of the CrowS-pairs dataset (Nangia et al., 2016) for French. CrowS-pairs consists of about ~1500 English sentence pairs, where one sentence of each pair shows a certain type of social bias, and the other is an unbiased version of it. In the extension, each sentence has been translated by two authors of the paper, taking care of adjusting the bias properly to French contexts/culture. Moreover, 212 new French sentence pairs have been added in a crowdsourcing process, in which also the translations and the bias types were checked.
In experiments on the dataset, the paper analyzes how much bias different BERT-based language models show for French (in comparison to English). Several insights into the problems and solutions of translating biased sentences are given afterwards. ","1. Social bias has largely been studied for English only so far. The dataset contributes to enabling multilingual social bias research, and it seems to be constructed thoroughly in different respects. I have little doubt that the data is of reasonable quality. The dataset is promised to be made available.
2. The paper's discussion convinced me of the quality of the translation, even though I was skeptical at first whether translation can work here. The examples and statistics in Tables 1 and 2, along with the descriptions in the text, emphasize the effort that was put into obtaining sentences that make sense for French (such as adjusting names, culture-related concepts, etc.).
3. The experimental results give useful insights into the behavior of the language models with respect to social bias in French text. With my given but limited knowledge of French, I was able to follow the discussion well, also because the examples are well-chosen and not too complex. Without any knowledge of French, some details may be hard to assess for readers, but I don't think this anyhow avoidable for such a topic. ","1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.  2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better. ","- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.  - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).  - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first. ",3.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",Nothing of concern. ,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.  2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better. 
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability.  - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column).  - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first. "
ARR_2022_19_review,"This paper presents a transformer-based model to generate open close tests for language learning and assessment. It is based on ELECTRA and makes use of two loss functions, one of which is standard token classification loss and the other is a language-based loss function that makes sure that a potential gap is not too open-ended. The model shows good performance, evaluated by automatic comparison with gold-standard gaps and also by human expert judgment. The paper will make the dataset available, which may be greatly helpful for future research in education-related NLP. ","- The paper provides a good benchmark for a task that has not been studied much before.
- I appreciate human evaluation in addition to automatic evaluation and qualitative evaluation.
- The paper is well-written and easy to understand. ","- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only. ",- Including an example created by BERT and comparing them with one created by the proposed model may be helpful. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- An ablation study with the two loss objectives may have been better than a BERT baseline. Instead of comparing an ELECTRA model with the two loss functions with a BERT with one of them, the authors could have compared their model with two ELECTRA models with one of the loss functions only. 
- Including an example created by BERT and comparing them with one created by the proposed model may be helpful. "
ARR_2022_19_review,"The paper explore the ELECTRA on constructing open cloze tests. They use ELECTRA's RTE head to do the gap/non gap (gap means the token should be masked in open cloze tests) token-level classification and use ELECTRA's MLM head to do the MLM auxiliary task. Meanwhile, they propose two heuristic methods: 1. a loss forcing gaps to be far away; 2. a post-processing method involving grap's repetition and PoS information. The paper conducts experiments on their collected datasets. They conduct automatic evaluation and human evaluation. Their methods outperformance baselines including: random selection, Exercise Maker ( a rule based method) and BERT. ","1. The paper is the first one to explore the ELECTRA on constructing open cloze tests, which seems reasonable. 
2. The evaluation adequate, including automatic evaluation and human evaluation. ","1. The post-processing seems complicated. 
2. lacks of ablation study of two loss. 
3. Maybe can conduct experiments on other datasets. 
4. Method not novel. ","1. I wonder the label mapping of  ELECTRA's binary classification head (RTE). In other words, the positive gap label corresponds to the positive RTE label, or not? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The post-processing seems complicated. 
2. lacks of ablation study of two loss. 
3. Maybe can conduct experiments on other datasets. 
4. Method not novel. 
1. I wonder the label mapping of  ELECTRA's binary classification head (RTE). In other words, the positive gap label corresponds to the positive RTE label, or not? "
ARR_2022_16_review,"This paper addresses training models to be robust to adversarial perturbations for sentiment analysis tasks. The authors first identify two limitations that exists with prior works: (i) adversarial perturbations for text often require a large number of search steps; and (ii) while expanding the search space for adversarial perturbations to augment the training set may improve robustness of the trained model to such perturbations, it could also hurt model performance on the clean data. To address this issue, the paper introduces friendly adversarial data augmentation (FADA) and geometry-aware adversarial training (GAT). The key idea behind FADA is to create adversarial examples in a way that takes them closer to the decision boundary but does not push them over it (as that would harm performance on original test set). Following creation of such adversarial examples in one go (rather than the dynamic approach adopted in prior work), the authors optimize the model using GAT, making it computationally efficient. Experiments on IMDb and SST-2 datasets show that adversarially trained models using GAT outperforms several strong baselines against five attack methods (TextFooler, TextBugger, BAE, PSO, and FastGA). The authors also present several ablation studies to identify the effect of each component in GAT, finding positive contribution from each. Additional analysis on step size, number of steps, training epochs, etc. is also presented in the paper. Overall, I think it is a very good paper that merits acceptance to an ACL venue. ","- The paper is well motivated and well written (except some minor issues I've laid below). It provides enough background to a first time reader and is very easy to follow. With motivation from the toy example, going into the experiments on SST-2 and IMDb datasets, the results are very interesting and provide good evidence in support of the efficacy of the proposed method. I do think the paper could be restructured in a way that highlights the contributions better than they have been in the current version.
- The authors complement the experiments with a thorough analysis to determine the effect of several choices made in constructing adversarial augmentations. The ablation studies also provide a good insight into the effectiveness of GAT.
- There are limitations of the proposed method and the paper, but to the authors' credit they offer a discussion of some of these limitations in the paper itself (thus warranting a mention in Strengths from my perspective). ","- There seems to be a slight disconnect between the motivation and the actual experiments. While the toy setup used to motivate the paper shows dramatic performance decline on clean data, that drop isn't as dramatic on the actual dataset as shown in the experiments.
- The experiments are limited to only sentiment analysis as a task. I would have liked to see some ""more difficult"" tasks as well, such as NLI.
- I don't understand why the results on RoBERTa and DeBERTa are restricted to only SST-2. I would have liked to see RoBERTa or DeBERTa be the default model for experiments on both SST-2 and IMDb over BERT as we know from prior work that these two methods perform better than BERT.
- The paper lacks any error analysis to see cases where prior methods fail but this method works well and vice versa. Aside from a practical utility offered by this method, I'm not sure if I am gaining any insights from the paper. I'd like to see how the accuracy sees significant shifts by just changing one word, some qualitative analysis (or examples to highlight this) would have supported the argument better. Right now, it does not pop up clearly to the reader. ","Comments: - In Section 4.2, why does sentence length influence percentage of perturbed words? The fact that its percentage of the tokens in a sentence and not an absolute number should remove the dependence on sentence length, no?
- Please specify the maximum sentence length kept for BERT/RoBERTa/DeBERTa in Section 4.4 - Lines 458-460: I don't think that we can conclude anything based on experiments on one dataset.
------------------------------------------ Suggestions: - Please include an error analysis as described above. Furthermore, it would be nice to also see some examples of friendly adversarial data, before and after they cross the decision boundary.
- Provide a greater discussion of why almost all defense techniques work similarly well against BAE on SST-2, and how a vanilla DeBERTa base outperforms adversarially trained methods against BAE on SST-2.
---------------------------------------- Typos and other presentation comments: - General comment: Specify upfront that when you say robustness, you mean robustness to adversarial perturbations and when you say accuracy, you mean accuracy on the original held out test set.
- Lines 25, 26: ""outperform humans on many natural language processing (NLP) tasks"". Please do not use such hyperbole. These methods are great but iid performance on one dataset for a task does not suggest that we've outperformed expert humans on that task.
- Line 28: ""However, recent studies..."" Which recent studies? Provide citations.
- Lines 337, 338: Footnotes should appear after the period. The only time a footnote should appear before a punctuation (if there is a punctuation right after where you want to place a footnote) is if the punctuation is a dash. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- There seems to be a slight disconnect between the motivation and the actual experiments. While the toy setup used to motivate the paper shows dramatic performance decline on clean data, that drop isn't as dramatic on the actual dataset as shown in the experiments.
- The experiments are limited to only sentiment analysis as a task. I would have liked to see some ""more difficult"" tasks as well, such as NLI.
- I don't understand why the results on RoBERTa and DeBERTa are restricted to only SST-2. I would have liked to see RoBERTa or DeBERTa be the default model for experiments on both SST-2 and IMDb over BERT as we know from prior work that these two methods perform better than BERT.
- The paper lacks any error analysis to see cases where prior methods fail but this method works well and vice versa. Aside from a practical utility offered by this method, I'm not sure if I am gaining any insights from the paper. I'd like to see how the accuracy sees significant shifts by just changing one word, some qualitative analysis (or examples to highlight this) would have supported the argument better. Right now, it does not pop up clearly to the reader. 
Comments: - In Section 4.2, why does sentence length influence percentage of perturbed words? The fact that its percentage of the tokens in a sentence and not an absolute number should remove the dependence on sentence length, no?
- Please specify the maximum sentence length kept for BERT/RoBERTa/DeBERTa in Section 4.4 - Lines 458-460: I don't think that we can conclude anything based on experiments on one dataset.
------------------------------------------ Suggestions: - Please include an error analysis as described above. Furthermore, it would be nice to also see some examples of friendly adversarial data, before and after they cross the decision boundary.
- Provide a greater discussion of why almost all defense techniques work similarly well against BAE on SST-2, and how a vanilla DeBERTa base outperforms adversarially trained methods against BAE on SST-2.
---------------------------------------- Typos and other presentation comments: - General comment: Specify upfront that when you say robustness, you mean robustness to adversarial perturbations and when you say accuracy, you mean accuracy on the original held out test set.
- Lines 25, 26: ""outperform humans on many natural language processing (NLP) tasks"". Please do not use such hyperbole. These methods are great but iid performance on one dataset for a task does not suggest that we've outperformed expert humans on that task.
- Line 28: ""However, recent studies..."" Which recent studies? Provide citations.
- Lines 337, 338: Footnotes should appear after the period. The only time a footnote should appear before a punctuation (if there is a punctuation right after where you want to place a footnote) is if the punctuation is a dash. "
ARR_2022_16_review,"The paper aims to investigate the robustness of pre-trained language models from a geometry-aware perspective. The authors argue that adversarial examples across the decision boundary, in fact, are harmful to models' performance, which is supported by a controlled experiment. These findings motivate the introduction of a so-called geometry-aware adversarial training (GAT) equipped with proposed ""friendly"" adversarial data augmentation (FADA). Evaluated on SST-2 and IMDb, language models fine-tuned with this method show stronger robustness. A thoughtful ablation study follows to analyze the impact of some related factors (e.g., search steps, step size, training epochs). ","1). Borrowing the concept of geometry, it is an interesting research topic to analyze the quality of generated adversarial examples against the decision boundary. 2). It is a nice try to introduce the motivation of this work by an experiment on a toy set at the very beginning. 3). To promote the robustness of pre-trained language models, GAT with FADA is proposed and achieves better defense performance on SST-2 and IMDb against three wide-used attacks. ","1). Although the hypothesis is quite interesting, it is not well verified by the designed experiment. As pointed out in Section 3.1, models in conventional methods are trained on the original training set in addition to the generated adversarial examples. In contrast, the base model is trained on the adversarial set only. It is better to compare the model trained on the original dataset with that trained on the mixture so as to highlight the impact of the augmented adversarial examples. Since this experiment serves as the motivation throughout this work, it is critical to make it more convincing. 2). The statement (title or scope) needs more extensive experiments to support. The overall goal of this work is to improve the robustness of pre-trained language models, while RoBERT and DeBERTa are only examined on SST-2, and large versions are not touched. Evaluation on only SST-2 and IMDb ( two classification tasks) seems weak. 3). It is encouraged to introduce the motivation by a lightweight experiment, but the Introduction is not the appropriate place for it. It is said ""achieved with ADA, FADA, and the original training set"" in the caption of Figure 1, but I can only see two pairs of learning curves. FADA should be defined formally in Section 3.1. The experiments conducted in line 207-216 is unclear without proper reference to either tables or figures. Is it the same as the one in the Introduction? I also try to find the definition of evaluation metrics in Section 4, but finally, they are in the caption of Table 2. I believe huge efforts have been made in this work, but the paper structure needs reorganization to underline the motivation and then highlight the contributions with detailed settings. ","Also see the comments above. 
+ An appendix is recommended to involve more details regarding experimental setup (e.g., hyper-parameters exploration) and case studies (e.g., “friendly” examples). 
+ Please avoid using words like ""friendly"" and FGM in the Abstract without further definition. 
+ Line 021 – less steps + Line 228 – Take … ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",There are no ethical concerns arising from this paper. ,"1). Although the hypothesis is quite interesting, it is not well verified by the designed experiment. As pointed out in Section 3.1, models in conventional methods are trained on the original training set in addition to the generated adversarial examples. In contrast, the base model is trained on the adversarial set only. It is better to compare the model trained on the original dataset with that trained on the mixture so as to highlight the impact of the augmented adversarial examples. Since this experiment serves as the motivation throughout this work, it is critical to make it more convincing. 2). The statement (title or scope) needs more extensive experiments to support. The overall goal of this work is to improve the robustness of pre-trained language models, while RoBERT and DeBERTa are only examined on SST-2, and large versions are not touched. Evaluation on only SST-2 and IMDb ( two classification tasks) seems weak. 3). It is encouraged to introduce the motivation by a lightweight experiment, but the Introduction is not the appropriate place for it. It is said ""achieved with ADA, FADA, and the original training set"" in the caption of Figure 1, but I can only see two pairs of learning curves. FADA should be defined formally in Section 3.1. The experiments conducted in line 207-216 is unclear without proper reference to either tables or figures. Is it the same as the one in the Introduction? I also try to find the definition of evaluation metrics in Section 4, but finally, they are in the caption of Table 2. I believe huge efforts have been made in this work, but the paper structure needs reorganization to underline the motivation and then highlight the contributions with detailed settings. 
Also see the comments above. 
+ An appendix is recommended to involve more details regarding experimental setup (e.g., hyper-parameters exploration) and case studies (e.g., “friendly” examples). 
+ Please avoid using words like ""friendly"" and FGM in the Abstract without further definition. 
+ Line 021 – less steps + Line 228 – Take … "
ARR_2022_319_review,"This paper presents several logic failures in the way that attribution methods are evaluated. For each failure they identify, the authors discuss why it will prevent proper identification of attribution quality, and design experiments that crystallize this potential failure. The authors survey the attribution evaluation literature, and argue that such logic failures hinder the advancements of better, more reliable attribution methods. ","This is a well written paper that discusses an important and very timely subject. It highlights important failures that are often overlooked in the model interpretability literature. Each failure the authors identify is clearly explained and motivated, and the experiments that demonstrate it are neat and clear. I believe that this paper can improve the way we evaluate attribution methods, and can be beneficial for many in the community. ","While I have a generally favorable opinion of this paper, I have two issues with it that if addressed would improve the paper in my opinion.
First, I agree with the reasoning and premise, but I think the paper is missing a quite obvious connection to causal inference. Another way of explaining the current limitation of attribution evaluation is that these methods are almost never compared against counterfactuals (the only counterexamples I can think of are the causal model explanation papers). That is, we need benchmarks that include counterfactual examples where we know what is the type of manipulation that was done, and compare model predictions between the original examples and the counterfactual examples.
More broadly, we can think of model explanation as a causal estimation problem, and evaluate methods as we do in causal inference (sensitivity analysis and the likes). While this paper is not the place to invent new evaluation methods, I think that it’s worthwhile to highlight the potential connections between all these failures and the lack of causal thinking in the model interpretability literature.
Second, I think that the discussion points are a bit too vague and are not as thought out as the described logic failures. Especially in 3.3, I agree that we should want models that are robust to perturbations, but it is not a problem of evaluation methods, it’s a problem of model robustness. We do want our attribution methods to capture such inconsistency if it exists, this is exactly why we use it. ","The paper is generally very well-written, but may benefit from an additional quick round of editing.  Some typos for example:  Line 616: “make it more robustness” ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"While I have a generally favorable opinion of this paper, I have two issues with it that if addressed would improve the paper in my opinion.
First, I agree with the reasoning and premise, but I think the paper is missing a quite obvious connection to causal inference. Another way of explaining the current limitation of attribution evaluation is that these methods are almost never compared against counterfactuals (the only counterexamples I can think of are the causal model explanation papers). That is, we need benchmarks that include counterfactual examples where we know what is the type of manipulation that was done, and compare model predictions between the original examples and the counterfactual examples.
More broadly, we can think of model explanation as a causal estimation problem, and evaluate methods as we do in causal inference (sensitivity analysis and the likes). While this paper is not the place to invent new evaluation methods, I think that it’s worthwhile to highlight the potential connections between all these failures and the lack of causal thinking in the model interpretability literature.
Second, I think that the discussion points are a bit too vague and are not as thought out as the described logic failures. Especially in 3.3, I agree that we should want models that are robust to perturbations, but it is not a problem of evaluation methods, it’s a problem of model robustness. We do want our attribution methods to capture such inconsistency if it exists, this is exactly why we use it. 
The paper is generally very well-written, but may benefit from an additional quick round of editing.  Some typos for example:  Line 616: “make it more robustness” "
ARR_2022_319_review,"The paper uncovers critical issues around interpretability and explainability of the reasoning and performance of modern deep learning models. Using several theoretical and experimental analysis, it specifically reveals the weaknesses of existing attribution methods designed to qualify and support research propositions by assessing model predictions. Because of their deceptive nature i.e. easily disregarded or even missed, the paper presents these weaknesses as logic traps. For example, if input features responsible for certain predictions are perturbed, scores provided by attribution methods should reflect the significant change or difference in the model’s predictions given the perturbations. The paper highlights the potential damage of neglecting these traps such as inaccurate evaluation, unfair comparison, unmerited pressure to re-use unreliable evaluation techniques etc. In its analysis, the paper shows factual information of how attribution methods can be misleading when they approve of a model’s prediction countering the actual norm or expectation.  Two helpful examples of logic traps include, (1) the assumption that a humans' decision making process is equivalent to that of a neural network. In a question answering task, BERT base maintained its test set performance despite replacing development set samples with empty strings. Ideally, the performance would drop because of the perturbations, but it did not, therefore clearly misleading human decision making. Moreover, the paper empirically proves this error by showing a drop in the model's confidence in its prediction on unchanged samples. ( 2) A second example of a logic trap is using arribution methods as ground truth to evaluate target attribution method. In an evaluation based on meaningful perturbation, such as Area over the perturbation curve (AOPC) which is precisely the average difference between probabilities predicted with original input feature and those predicted with perturbed input features. While the norm expects this AOPC to be significant or at least representative of the degree of perturbation, the paper illustrates that the perturbation (modification) strategy will dictate the eventual AOPC value, i.e. it varies with respect to the modification strategy. Scores of different attribution methods when inputs are perturbed by token deletion are inconsistent with their scores when inputs are perturbed by token replacement. The paper concludes with suggestions of limiting the impact of logic traps such as the ones discussed. Enhancing the target model as well as Excluding predictions with lower confidence. ","Precise and concise abstract.
The paper is organised and well written.
The subject addressed is crucial for the deep learning community. Redirecting attention to the appropriately selecting attribution methods (during research task evaluation stages) can subtly reduce the immense effort and focus researchers have in outperforming previous works which in any case can potentially be premised on unreliable evaluation methods.
A sufficient number of examples to illustrate the logic traps is used, which is very helpful especially because they are deceptively obvious. ","Lines 179-182 Surprisingly, the trained MRC model maintained the original prediction on 64.0% of the test set samples (68.4% on correctly answered samples and 55.4% on wrongly answered samples).
It’s clear that the MRC model surprisingly maintains prediction accuracy when evaluated on 64% of the test samples, however, what follows in the brackets is unclear i.e. ""68.4% on correctly answered samples and 55.4% on wrongly answered samples"" is an unclear statement.  Lines 183-185 Moreover, we analyze the model confidence change in these unchanged samples, where the probability on the predicted label is used as the confidence score.
What unchanged samples are you referring to? Did you replace all the development samples with empty strings or was it just a portion you replaced with empty strings hence retaining a few that you refer to as unchanged? If not, are the unchanged samples in the test set or?
Evaluation 3 and Logic trap 3.
You use a hard to follow example to illustrate the logic trap you define as  “the change in attribution scores is brought about by the model reasoning process rather than the attribution method unreliability”. Because you indicate that deep models are vulnerable to adversarial samples, which indeed is right and therefore you would expect attribution scores to be faithful to the shift caused by the attack.
The argument feels more like, the change in attribution scores is with respect to the change in samples which eventually will meet a different model reasoning process?
For the results you discuss and summarize, Is the claim that the original evaluation method correctly obtains a low similarity in the F1 scores of the adversarial sample subset and the original sample subset, whereas the attribution method says otherwise?
A rewrite of this section particularly the experiment and its results to add clarity or rather use of a different example would improve the work. ","Lines 088-092 Last, the over-belief in existing evaluation metrics encourages efforts to propose more accurate attribution methods, notwithstanding the evaluation system is unreliable.
The statement above looks more like you intended to say discourages rather than encourages. Please have a look.
Lines 276 and 328 AOPC rather than APOC Lines 528 With no overlap between the two subsets, there is no way we can hypothesis the adversarial samples share similar model reasoning to the original samples.
hypothesise rather than hypothesis in the above sentence.
You can probably do away with some repetitions of long sentences such as what is in the introduction as well as in the conclusion. “ Though strictly accurate evaluation metrics for attribution methods might be a “unicorn” which will likely never be found, we should not just ignore logic traps in existing evaluation methods and draw conclusions recklessly.” ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Lines 179-182 Surprisingly, the trained MRC model maintained the original prediction on 64.0% of the test set samples (68.4% on correctly answered samples and 55.4% on wrongly answered samples).
It’s clear that the MRC model surprisingly maintains prediction accuracy when evaluated on 64% of the test samples, however, what follows in the brackets is unclear i.e. ""68.4% on correctly answered samples and 55.4% on wrongly answered samples"" is an unclear statement.  Lines 183-185 Moreover, we analyze the model confidence change in these unchanged samples, where the probability on the predicted label is used as the confidence score.
What unchanged samples are you referring to? Did you replace all the development samples with empty strings or was it just a portion you replaced with empty strings hence retaining a few that you refer to as unchanged? If not, are the unchanged samples in the test set or?
Evaluation 3 and Logic trap 3.
You use a hard to follow example to illustrate the logic trap you define as  “the change in attribution scores is brought about by the model reasoning process rather than the attribution method unreliability”. Because you indicate that deep models are vulnerable to adversarial samples, which indeed is right and therefore you would expect attribution scores to be faithful to the shift caused by the attack.
The argument feels more like, the change in attribution scores is with respect to the change in samples which eventually will meet a different model reasoning process?
For the results you discuss and summarize, Is the claim that the original evaluation method correctly obtains a low similarity in the F1 scores of the adversarial sample subset and the original sample subset, whereas the attribution method says otherwise?
A rewrite of this section particularly the experiment and its results to add clarity or rather use of a different example would improve the work. 
Lines 088-092 Last, the over-belief in existing evaluation metrics encourages efforts to propose more accurate attribution methods, notwithstanding the evaluation system is unreliable.
The statement above looks more like you intended to say discourages rather than encourages. Please have a look.
Lines 276 and 328 AOPC rather than APOC Lines 528 With no overlap between the two subsets, there is no way we can hypothesis the adversarial samples share similar model reasoning to the original samples.
hypothesise rather than hypothesis in the above sentence.
You can probably do away with some repetitions of long sentences such as what is in the introduction as well as in the conclusion. “ Though strictly accurate evaluation metrics for attribution methods might be a “unicorn” which will likely never be found, we should not just ignore logic traps in existing evaluation methods and draw conclusions recklessly.” "
ARR_2022_319_review,"The work argues that input attribution method evaluation criteria (the means in which one attribution method is compared against another) posses several ignored flaws and have ""negatively affected the research field"". The paper exemplifies the flaws by highlighting how high-level goals of evaluation methods can be subverted or as the paper calls them, have ""logical traps"". These issues are shown exploitable to produce inconsistent preferences for an attribution method; i.e. the evaluation criteria are thus rendered pointless. The authors also make some points with regard to some views of what attributions should or should not be and argues otherwise. ","+ On its face the paper is right in its main point that evaluation criteria can be exploited to   produce inconsistent ranking of attribution methods, and in general the use of evaluation metrics   can be problematic to research in the area. ","1 The paper poses mostly subjective points which are themselves largely not novel or are flawed in   themselves. It starts with the emphasized sentence on page 1 attributed to Jacovi et al which I   will call ""Opinion A"":     ""As an explanation method, the evaluation criteria of attribution methods should be how      accurately it reflects the true reasoning process of the model (faithfulness), not how      convincing it is to humans (plausibility)""   This point is debatable. Whether attributions should be purely faithful to model behaviour or   offer human-interpretability is a decision to be made in the definition of an attribution method   or its motivation. Neither option is wrong as long as the method is applied to its intended use   case. Further, the noted choice of faithfulness has implications on some of the ""logic traps""   described subsequently. I elaborate on each in the comments section below.
  Significant positioning fixes need to be implemented in this work starting with Opinion A.   Either arguments for faithfulness need to be made or the paper needs to restrict itself to   attributions whose primary concern is faithfulness. The latter option would have to be written   from the perspective that faithfulness is an apriori goal.
  The novelty of arguments being made needs to be addressed as well. I find that the cores of the   ""logic traps"" are all known points and are often well considered in attribution, evaluation, and   robustness work. Presently the paper does not demonstrate that (logic traps) ""in existing   evaluation methods have been ignored for a long time"". If there is a gap in some segment of the   community, a survey or systemization paper where applicable would be more appropriate.
2 Arguments and experiments with regard to ""model reasoning process"" are vague in key definitions   and thus non-convincing. Specifically, no definition of ""model reasoning process"" is given. 
  Experiment outlined in Figure 7 argues that extractor's bits are equivalent to ""model reasoning   process"" but this is arguable. The extractor can derive the same bits using different means or   different bits using very similar means. While I do not disagree with the main points regarding   model reasoning process and robustness, I do not think the experiments demonstrate them. ","Detailed comments regarding Weakness 1):   * Logic Trap 1: The point being made is obvious:     ""The decision-making process of neural networks is not equal to the decision-making      process of humans.""
  The example Experiment 1 incorporates not a single attribution method. Is it presumed that all   attributions will be wrong as there is no human explanation possible? Can we also say that any   human explanation would be wrong for the same reason? If there is no ground truth, how can it be   wrong for an attribution method to say anything?
  Regardless, no-one *expects* models to fully replicate the reasoning of a human. Replicating   human behaviour may be useful to gauge human-interpretability of explanations but this is   precluded by Opinion A.   Finally, it is well understood that what is correct to a human may not be correct in a model as   per faithfulness vs. explainability discussion which is had alongside attribution evaluations in   literature and in Opinion A (some examples from cited works below).
  * Logic Trap 2: The second point made is a roundabout way of saying that ablation orders are     varied:     ""Using an attribution method as the ground truth to evaluate the target attribution method.""
  The authors are rightly pointing out that numerous forms of attribution evaluation based on   ablation or reconstruction inputs have been used to motivate attribution methods. Here the   opposite conclusion to Opinion A is helpful. Expecting humans to interpret an attribution one way   may lead to one ablation order whereas another form of interpretation may lead to another. There   is no single correct metric because there is no single interpretation. This point has been made   at least in [a].
  * Logic Trap 3: The third point is known:     ""The change in attribution scores maybe because the model reasoning process is really changed     rather than the attribution method is unreliable.""
  Ghorbani et al. (2019) note in their concluding discussion that fragility of attribution is   reflective of fragility of the model (and that their attacks have not ""broken"" an attribution). 
  Likewise Alvarez-Melis et al. (2018) point out that the non-robust artifacts in attributions they   discover are actually reasonable if faithfulness to model is the only goal of an attribution. 
  Thus Logic Trap 3 does not seem to add anything beyond Opinion A.   * 3.1 -- ""Attacking attribution methods by replacing the target model.""
  This section seems to be pointing out that attribution methods have a problem of domain   faithfulness in that they often internally apply a model to inputs that they have not been   trained on or don't expect to operate reliably on.
  * 3.2 -- ""Should We Use Attributions Methods in a Black-Box Way?""
  The paper argues that black-box is not a worthwhile goal. This is again highly subjective as   there are several reasons, despite presented arguments, to prefer black-box methods, like 1)   having explanations of the semantics of what a model is modeling instead of an explanation   tainted by how the model is implemented, 2) black-box means model agnostic, hence can apply to   any model structure, 3) some scenarios just do not have access to the model internals.
Other comments: - The term ""logic traps"" is not define or explained. Dictionaries and reference materials equate   them to logical fallacies which I'm unsure is the intended meaning here.
- The term ""erasure-based"" is used in the intro but later the term ""ablation"" is used.
- Typo/word choice near ""with none word overlap"".
- Typo/grammar near ""there are works (...) disprove"" - Suggestion of 3.3, point 2 is unclear. Adversarial examples can be highly confident and I suspect   means of incorporating confidence can themselves be subverted adversarially.
- Figure 6 is not useful. I presume the paths are supposed to be indicative of model behaviour but   as mentioned in earlier comments, defining it is a crucial problem in explanation work. The   Figure is suggestive of it being a triviality.
- Several points in the paper use phrases like ""A lot of works ..."" or ""most existing methods ..."". 
  I think it would be more appropriate to list the works or methods instead of noting the relative   abundance.
- There are some inconsistencies in the notation for AOPC and the k parameter with potential   related typos in its definition.
- Potential grammar issue near ""can be seen as an attribution"".
- Grammar issue near ""results are mainly depend"" - Grammar issue near ""achieve 86.4% accuracy"" - Where is footnote 1?
- In Figure 4, the resulting attributions for the target word ""good"" in LOO, Marg are not   presented or indicated by color.
References from comments: [a] - Wang et al. ""Interpreting Interpretations: Organizing Attribution Methods by Criteria"". ",1.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1 The paper poses mostly subjective points which are themselves largely not novel or are flawed in   themselves. It starts with the emphasized sentence on page 1 attributed to Jacovi et al which I   will call ""Opinion A"":     ""As an explanation method, the evaluation criteria of attribution methods should be how      accurately it reflects the true reasoning process of the model (faithfulness), not how      convincing it is to humans (plausibility)""   This point is debatable. Whether attributions should be purely faithful to model behaviour or   offer human-interpretability is a decision to be made in the definition of an attribution method   or its motivation. Neither option is wrong as long as the method is applied to its intended use   case. Further, the noted choice of faithfulness has implications on some of the ""logic traps""   described subsequently. I elaborate on each in the comments section below.
  Significant positioning fixes need to be implemented in this work starting with Opinion A.   Either arguments for faithfulness need to be made or the paper needs to restrict itself to   attributions whose primary concern is faithfulness. The latter option would have to be written   from the perspective that faithfulness is an apriori goal.
  The novelty of arguments being made needs to be addressed as well. I find that the cores of the   ""logic traps"" are all known points and are often well considered in attribution, evaluation, and   robustness work. Presently the paper does not demonstrate that (logic traps) ""in existing   evaluation methods have been ignored for a long time"". If there is a gap in some segment of the   community, a survey or systemization paper where applicable would be more appropriate.
2 Arguments and experiments with regard to ""model reasoning process"" are vague in key definitions   and thus non-convincing. Specifically, no definition of ""model reasoning process"" is given. 
  Experiment outlined in Figure 7 argues that extractor's bits are equivalent to ""model reasoning   process"" but this is arguable. The extractor can derive the same bits using different means or   different bits using very similar means. While I do not disagree with the main points regarding   model reasoning process and robustness, I do not think the experiments demonstrate them. 
Detailed comments regarding Weakness 1):   * Logic Trap 1: The point being made is obvious:     ""The decision-making process of neural networks is not equal to the decision-making      process of humans.""
  The example Experiment 1 incorporates not a single attribution method. Is it presumed that all   attributions will be wrong as there is no human explanation possible? Can we also say that any   human explanation would be wrong for the same reason? If there is no ground truth, how can it be   wrong for an attribution method to say anything?
  Regardless, no-one *expects* models to fully replicate the reasoning of a human. Replicating   human behaviour may be useful to gauge human-interpretability of explanations but this is   precluded by Opinion A.   Finally, it is well understood that what is correct to a human may not be correct in a model as   per faithfulness vs. explainability discussion which is had alongside attribution evaluations in   literature and in Opinion A (some examples from cited works below).
  * Logic Trap 2: The second point made is a roundabout way of saying that ablation orders are     varied:     ""Using an attribution method as the ground truth to evaluate the target attribution method.""
  The authors are rightly pointing out that numerous forms of attribution evaluation based on   ablation or reconstruction inputs have been used to motivate attribution methods. Here the   opposite conclusion to Opinion A is helpful. Expecting humans to interpret an attribution one way   may lead to one ablation order whereas another form of interpretation may lead to another. There   is no single correct metric because there is no single interpretation. This point has been made   at least in [a].
  * Logic Trap 3: The third point is known:     ""The change in attribution scores maybe because the model reasoning process is really changed     rather than the attribution method is unreliable.""
  Ghorbani et al. (2019) note in their concluding discussion that fragility of attribution is   reflective of fragility of the model (and that their attacks have not ""broken"" an attribution). 
  Likewise Alvarez-Melis et al. (2018) point out that the non-robust artifacts in attributions they   discover are actually reasonable if faithfulness to model is the only goal of an attribution. 
  Thus Logic Trap 3 does not seem to add anything beyond Opinion A.   * 3.1 -- ""Attacking attribution methods by replacing the target model.""
  This section seems to be pointing out that attribution methods have a problem of domain   faithfulness in that they often internally apply a model to inputs that they have not been   trained on or don't expect to operate reliably on.
  * 3.2 -- ""Should We Use Attributions Methods in a Black-Box Way?""
  The paper argues that black-box is not a worthwhile goal. This is again highly subjective as   there are several reasons, despite presented arguments, to prefer black-box methods, like 1)   having explanations of the semantics of what a model is modeling instead of an explanation   tainted by how the model is implemented, 2) black-box means model agnostic, hence can apply to   any model structure, 3) some scenarios just do not have access to the model internals.
Other comments: - The term ""logic traps"" is not define or explained. Dictionaries and reference materials equate   them to logical fallacies which I'm unsure is the intended meaning here.
- The term ""erasure-based"" is used in the intro but later the term ""ablation"" is used.
- Typo/word choice near ""with none word overlap"".
- Typo/grammar near ""there are works (...) disprove"" - Suggestion of 3.3, point 2 is unclear. Adversarial examples can be highly confident and I suspect   means of incorporating confidence can themselves be subverted adversarially.
- Figure 6 is not useful. I presume the paths are supposed to be indicative of model behaviour but   as mentioned in earlier comments, defining it is a crucial problem in explanation work. The   Figure is suggestive of it being a triviality.
- Several points in the paper use phrases like ""A lot of works ..."" or ""most existing methods ..."". 
  I think it would be more appropriate to list the works or methods instead of noting the relative   abundance.
- There are some inconsistencies in the notation for AOPC and the k parameter with potential   related typos in its definition.
- Potential grammar issue near ""can be seen as an attribution"".
- Grammar issue near ""results are mainly depend"" - Grammar issue near ""achieve 86.4% accuracy"" - Where is footnote 1?
- In Figure 4, the resulting attributions for the target word ""good"" in LOO, Marg are not   presented or indicated by color.
References from comments: [a] - Wang et al. ""Interpreting Interpretations: Organizing Attribution Methods by Criteria"". "
ARR_2022_277_review,"This paper discusses hype and overclaiming phenomena in NLP research. It first identifies four types of underclaiming in existing work when reporting and citing experiment results and models’ capabilities and limitations. It then describes how underclaiming can affect making progress. It further summarises the major concerns regarding advanced AI. It then follows a few suggestions on how to mitigate such issue, including making clear reports, and adopting better evaluation practices, analysis and forecasting. ","This paper addresses an important issue that hasn't been broadly discussed in NLP research community. As it points out, researchers should try to adopt appropriate evaluation and analysis, and make credible conclusion about the limitations and capabilities of systems used. In addition, hype can potentially impact early researchers' understanding of progress in NLP.  I think this is a good opinionated paper, especially at nowadays where it is almost impossible to track all relevant paper on arxiv. The paper is sound and convincing. A good practice is also provided to mitigate underclaiming in writing and citations for future studies. ","I find it difficult connecting section 5 to the issues of hype and overclaiming in general. Are they about the long-term effects of overclaiming or hype? In my opinion, it is more related to AI risk research and potential risks of misuse of AI. It would be better to add one or two sentences to explain how they are connected to overclaiming. ","An additional practice to avoid underclaiming is to define clearly, before conducting experiments, the desired behaviours of a model in order to properly evaluate if the model ""succeeds"" or ""fails"".
Note: This paper is an opinionated paper and the replicability score does not applied, although literature used to draw the conclusion is provided. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I find it difficult connecting section 5 to the issues of hype and overclaiming in general. Are they about the long-term effects of overclaiming or hype? In my opinion, it is more related to AI risk research and potential risks of misuse of AI. It would be better to add one or two sentences to explain how they are connected to overclaiming. 
An additional practice to avoid underclaiming is to define clearly, before conducting experiments, the desired behaviours of a model in order to properly evaluate if the model ""succeeds"" or ""fails"".
Note: This paper is an opinionated paper and the replicability score does not applied, although literature used to draw the conclusion is provided. "
ARR_2022_162_review,This paper proposes an ELECTRA-style tasks for the purpose of cross-lingual pretraining. The resulting pretrained models are competitive against previous  pretrained models. Extensive experiments were conducted to show the effectiveness and efficiency of the proposed approach. ,From the experiments we can see that the resulting pretrained models have some advantages. ,"1. The proposed approach to pretraining has limited novelty since it more or less just follows the strategies used in ELECTRA. 
2. It is not clear whether baselines participating in the comparison are built on the same datasets that are used to build XLM-E. ","1. From the results in Table 1, we can see that XLM-E lags behind baselines in ""Structured Prediction"" tasks while outperforms baselines in  other tasks. Any possible reason for such a phenomenon?
Some typos and grammatical errors. 
1. "" A detailed efficiency analysis in presented in Section 4.5"". Here ""in"" --> ""is"" 2. "" XLM-E substantially outperform XML on both tasks"". Here ""outperform"" --> ""outperforms"" 3. ""... using parallel corpus"" --> ""using parallel corpora"" ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The proposed approach to pretraining has limited novelty since it more or less just follows the strategies used in ELECTRA. 
2. It is not clear whether baselines participating in the comparison are built on the same datasets that are used to build XLM-E. 
1. From the results in Table 1, we can see that XLM-E lags behind baselines in ""Structured Prediction"" tasks while outperforms baselines in  other tasks. Any possible reason for such a phenomenon?
Some typos and grammatical errors. 
1. "" A detailed efficiency analysis in presented in Section 4.5"". Here ""in"" --> ""is"" 2. "" XLM-E substantially outperform XML on both tasks"". Here ""outperform"" --> ""outperforms"" 3. ""... using parallel corpus"" --> ""using parallel corpora"" "
ARR_2022_162_review,"In this paper, the author proposed XLM-E, a cross-lingual ELECTRA-style pretrained LM, which merges the benefits of the XLM and Electra model. Also, the authors introduce a gated relative position bias which the author claim is helpful to the pretrained model. In the experiments, they show their method achieves better results on most cross-lingual transferability tasks and show XLM-E requires less computation resource than previous methods. Moreover, they compare the cross-lingual alignment ability of XLM-E to several baseline models.
Although the paper is complete and well written, the proposed method is not that exciting. Although I gave him 3.5 points, if I could, I would give him a score between 3-3.5. ( maybe 3.25) ","1. The paper is well written and easy to follow. 
2. The experiments are complete but missing some baseline. 
3. The proposed method is much efficient than previous methods. ","1. It is well known that ELECTRA-style is efficient and the TLM-based model is helpful to the cross-lingual representation. It is no surprise that combining both methods would work. 
2. The TLM-based pre-trained method required translation pairs. I understand most of the baseline required translation pairs, too. However, instead of using translation pairs, I would like to see more research try to achieve better cross-lingual transferability without using translation pairs. 
3. There is no comparison between the usual relative position bias and gated relative position bias. ","1. missing baseline in Fig.3 and Fig.4: I would like to see the comparison including InfoXLM an XLM-Align. 
2. missing baseline in table 5, 6: I would like to see the comparison including InfoXLM and XLM-Align. 
3. I'm interested in the difference between the results of using usual relative position bias and gated relative position bias. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. It is well known that ELECTRA-style is efficient and the TLM-based model is helpful to the cross-lingual representation. It is no surprise that combining both methods would work. 
2. The TLM-based pre-trained method required translation pairs. I understand most of the baseline required translation pairs, too. However, instead of using translation pairs, I would like to see more research try to achieve better cross-lingual transferability without using translation pairs. 
3. There is no comparison between the usual relative position bias and gated relative position bias. 
1. missing baseline in Fig.3 and Fig.4: I would like to see the comparison including InfoXLM an XLM-Align. 
2. missing baseline in table 5, 6: I would like to see the comparison including InfoXLM and XLM-Align. 
3. I'm interested in the difference between the results of using usual relative position bias and gated relative position bias. "
ARR_2022_278_review,"This paper is about improving the prosody of neural text-to-speech (NTTS) systems using the surrounding context of a given input text. The study introduced an extension to a well known NTTS system i.e., FastSpeech-2. The extension is a phoneme level conditional VAE. As cited in the current paper both FastSpeech-2 and conditional VAE are already proposed in the literature. The main novelty of this paper is representation of surrounding utterances using a pre-trained  BERT model and generation of prosodically varied samples with the help of learned contextual information. Authors followed standard TTS evaluation protocols to evaluate their proposed architecture, and evaluation results are in favor of the proposed architecture. ","- This paper introduced a new component to FastSpeech-2, a well known non-autoregressive NTTS architecture, called as cross utterance conditional VAE (CUC-VAE).  - The CUC-VAE contains two main components 1) cross utterance (CU) embedding and 2) CU enhanced conditional VAE. ","- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.
- It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  - In general, there are quite a few things missing -- details provided in comments section. ","**Typos:** - Background section: ""...high fidelity thank to…"" -> ""...high fidelity thanks to…"" - Background section: "" … Fang et al., 2019).Many…"" -> "" … Fang et al., 2019). Many…"" - Figure-1: ""...which integrated to into…"" -> ""...which integrated into…"" **Comments:** - Author did not mention how the initial durations of phonemes are obtained.
- Are durations of phonemes predicted in frames or seconds?
- Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.
- There is no mentioning of train, valid and test set splits in the dataset section.
- In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?
- How do you represent the reference mel-spectrogram at phoneme level?
- Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.
- It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  - In general, there are quite a few things missing -- details provided in comments section. 
**Typos:** - Background section: ""...high fidelity thank to…"" -> ""...high fidelity thanks to…"" - Background section: "" … Fang et al., 2019).Many…"" -> "" … Fang et al., 2019). Many…"" - Figure-1: ""...which integrated to into…"" -> ""...which integrated into…"" **Comments:** - Author did not mention how the initial durations of phonemes are obtained.
- Are durations of phonemes predicted in frames or seconds?
- Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.
- There is no mentioning of train, valid and test set splits in the dataset section.
- In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?
- How do you represent the reference mel-spectrogram at phoneme level?
- Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? "
ARR_2022_278_review,"This paper aims to improve the expressiveness of prosody in TTS in an intelligible and natural-sounding way. It is applied for read speech in English.
The method uses a conditional variational autoencoder to generate prosodic attributes for input text. The specific innovation of this paper is the cross-utterance conditioning, so that in order to synthesise prosodic speech for an utterance, the VAE is conditioned on text of several preceding and following utterances as well as the current one. This means, for example, that the phrase 'Mary asked the time' is given different prosody in the contexts 'Who asked the time? ------.' vs. '------, and was told it was only five'. The paper is clear about how this methodological contribution builds on and differs from other recent efforts to improve the prosody of TTS.  In addition, a set of several complementary evaluations are applied to judge the TTS output of this method. The subjective evaluations include human perceptions of naturalness, and AB testing for preference of systems with/out the cross-utterance context. Ablation studies demonstrate the contribution of different components. Objective evaluations include word error rate (corresponding to intelligibility), reconstruction error, and expressive diversity (energy and pitch variability). The results show that using cross-utterance context leads to greater expression of appropriate prosodic variation. ","1. Extremely clear and informative writing, logical presentation of context/literature, what was done here, how, why, and what doing it that way means. 
2. Good motivation: using realistic linguistic context to determine output prosody, with the practical advantages of non-autoregressive TTS. 
3. Code is available, data and processing are fully described, demo page comparing various TTS outputs is nice too. 
4. The evaluations cover a lot of angles and taken all together they are convincing. 
5. Baselines seem good (though admittedly I'm not an expert here) and particularly appreciate the human judgements baseline for original (i.e. non-TTS, human-spoken) speech. ","1. The sample that was used for the diversity evaluations evidently was based on 'three phonemes in randomly selected 11 utterances', this seems a bit puzzling as written. Either my understanding of what the sample consisted of is wrong and it could be clarified, or this decision could be briefly justified (e.g. was it the same 3 phoneme types in each utterance or a random 3 phonemes in each, presumably there was some logistical or theoretical reason not to use the full length of the 11 synthesised utterances, and is it safe to assume this sample is representative enough of diversity in the whole?) ",1. Figure 1 perhaps has space to make some of the text a bit larger ,4.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. The sample that was used for the diversity evaluations evidently was based on 'three phonemes in randomly selected 11 utterances', this seems a bit puzzling as written. Either my understanding of what the sample consisted of is wrong and it could be clarified, or this decision could be briefly justified (e.g. was it the same 3 phoneme types in each utterance or a random 3 phonemes in each, presumably there was some logistical or theoretical reason not to use the full length of the 11 synthesised utterances, and is it safe to assume this sample is representative enough of diversity in the whole?) 
1. Figure 1 perhaps has space to make some of the text a bit larger "
ARR_2022_316_review,"This paper introduces an approach to improve code language models so that the code they produce is compilable. The approach involves two techniques: 1) it uses policy gradients with KL-control _alla_ Ziegler et al. (2018) by translating the compiler's output into a +1/-1 reward. 2) it uses the generated sequences to train a discriminator that learns to detect whether a sequence compiles or not. Interestingly, the generator backpropagates into the generators' representation, possibly allowing it to learn to encode features that inform a sequence's compilability and which are useful to the generator. ","1. While the RL approach is the same as in Ziegler et al. (2018), the joint training of the discriminator with the generator looks rather novel and even an exciting development. 
2. The evaluation takes care to evaluate not only compilability but also the generalization of the model and its general fluency. 
3. The results look strong (but lookout for some caveats below) ","1. There is no associated code that could guarantee the reproducibility of these results. 
2. The evaluation is done on (as far as I can see) no standard splits of the datasets. Without making these splits available, future work will not be able to compare to these results. 
3. The compared methods are not on equal grounds. The proposed approach does a beam search (with k=5) using the discriminator output to select compilable sequences. For the comparison to be fairer, the authors could consider extracting k=5 samples from the other models and filter for the ones that compile (+ use the model probabilities to break ties) or, almost equivalently, doing rejection sampling on the distribution P(c)*1[compiler(c)]. One can argue that this gives an advantage to the baselines because they have access to the compiler, but on the other hand, it would be possible to train an independent discriminator that predicts whether a sequence compiles or not, as it was done for the proposed model, and no access to the compiler would be needed. Alternatively, IR metrics such as MAP or Prec@k could be used. 
4. The claim that this is the first work to integrate the compiler feedback to improve compilability is not correct. Korbak et al. (2021) cited at the end of the related work section also learn from the compiler feedback signal. While this could be easily amended, it would be better if this paper would compare their approach to this earlier work for the task studied in this other study, namely, the code completion task. ","Other comments: 1. Levenshtein is a distance metric (lower is better), not a similarity (higher is better) one afaik. It would be good if the authors explained how the authors convert Levenshtein distance into similarity (I have checked the cited paper, and it does not provide any clue neither). 
2. L337: It's not clear why the authors chose to use 41k pairs and not the full dataset.
Typos/grammar/etc (not exhaustive): (in general) ""code"" is uncountable in English. I believe you cannot say ""codes"". 
L101: comprehensive experiment -> comprehensive experiments L231: This fine-tuning process incorporates a (..) discriminator will... -> which will L240: We hope the generator will have more perceiving to -> more perception power? 
L298: same as above Eq. 8: c in the summation is not used anywhere in the terms. Does it expand into t and s? Please, clarify. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"1. There is no associated code that could guarantee the reproducibility of these results. 
2. The evaluation is done on (as far as I can see) no standard splits of the datasets. Without making these splits available, future work will not be able to compare to these results. 
3. The compared methods are not on equal grounds. The proposed approach does a beam search (with k=5) using the discriminator output to select compilable sequences. For the comparison to be fairer, the authors could consider extracting k=5 samples from the other models and filter for the ones that compile (+ use the model probabilities to break ties) or, almost equivalently, doing rejection sampling on the distribution P(c)*1[compiler(c)]. One can argue that this gives an advantage to the baselines because they have access to the compiler, but on the other hand, it would be possible to train an independent discriminator that predicts whether a sequence compiles or not, as it was done for the proposed model, and no access to the compiler would be needed. Alternatively, IR metrics such as MAP or Prec@k could be used. 
4. The claim that this is the first work to integrate the compiler feedback to improve compilability is not correct. Korbak et al. (2021) cited at the end of the related work section also learn from the compiler feedback signal. While this could be easily amended, it would be better if this paper would compare their approach to this earlier work for the task studied in this other study, namely, the code completion task. 
Other comments: 1. Levenshtein is a distance metric (lower is better), not a similarity (higher is better) one afaik. It would be good if the authors explained how the authors convert Levenshtein distance into similarity (I have checked the cited paper, and it does not provide any clue neither). 
2. L337: It's not clear why the authors chose to use 41k pairs and not the full dataset.
Typos/grammar/etc (not exhaustive): (in general) ""code"" is uncountable in English. I believe you cannot say ""codes"". 
L101: comprehensive experiment -> comprehensive experiments L231: This fine-tuning process incorporates a (..) discriminator will... -> which will L240: We hope the generator will have more perceiving to -> more perception power? 
L298: same as above Eq. 8: c in the summation is not used anywhere in the terms. Does it expand into t and s? Please, clarify. "
ARR_2022_0_review,"This papers presents a new dataset for evaluating model performance on figurative language. The dataset is Wino-grad style: it contains figurative phrases with divergent meanings, allowing assessment of whether models can correctly identify the appropriate interpretation of figurative phrases. This dataset is composed via crowdsourcing, with extensive expert validation. An analysis of the dataset is presented, including some linguistic analysis of what types of figuration and syntactic structures are present, as well as analysis of what kinds of metaphors. They then evaluate two types of models on the proposed task of identifying the correct meaning. Additional prompting-based improvements are proposed, as well as some analysis of how the models can work for generating interpretations. Comprehensive error analysis is performed, with a particular eye for where model performance and human performance differ, offering some interesting insights into the difficulty of processing metaphoric language. ","The paper is clear and well-written, and presents a strong background knowledge of the linguistic aspects of metaphor. It is framed particularly well wrt. the background literature, and I personally found the authors attention to the linguistic aspects of the proposed dataset to be encouraging. The proposed dataset seems very well thought out, and the extensive expert checks indicate that the resulting dataset is of high quality. This kind of resource is currently valuable not just for the inspection of LMs as proposed, but also for paraphrasing and generation tasks, and I see this dataset being a very valuable resource. The analysis performed is good, and the difference between the auto-regressive and masked language models is an interesting finding. ","There is of course a fair amount of creativity in the pairs, which may yield some strange inferences. There's something about these paired sentences with the sarcastic reading (""bright as coal"", ""easy as kindergarten for a newborn"") that seem particuarly forced: I wouldn't expect these sentences to be very common in everyday language, and then perhaps LM performance is expected, as they wouldn't have seen these things before. I feel they're a strange kind of sentence with a somewhat difficult semantics even for humans. For the pilot pair, I'm not sure I would get the intended meanings for the difference between ""ballet dancer"" and ""modern dancer"" - I feel the ""dancer"" alone could yield either reading. So there may be quirks in the dataset that make it tricky to interpret exactly what's going on with model performance.
I'm curious about the strong performance of the RoBERTa model - could it be that this kind of classification task isn't particularly difficult, if we give the model enough training and data? I think the paper shows clear difficulties in the generation and inference aspects, but perhaps classification is getting better. ","It should be noted that there are also lots of recent work on novel, challenging NLI sets, which the authors seem aware of. There are some recent and contemporary works that also deal with figurative language (Chakrabarty 2021, ""Figurative Language in RTE"", Poliak 2018, https://aclanthology.org/D18-1007/) the authors should be aware of: in my mind, this work provides a novel, high-quality dataset, they could benefit from clarifying how their work is distinct from these.
A possible question: this work seems focused on starting with metaphors and getting literal interpretations. I think there are also significant applications the other way, particularly in metaphor generation, where the literal interpretations can be used as input and the metaphors as output. ",4.5 ,Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,All ethical issues are minor and addressed by the authors. ,"There is of course a fair amount of creativity in the pairs, which may yield some strange inferences. There's something about these paired sentences with the sarcastic reading (""bright as coal"", ""easy as kindergarten for a newborn"") that seem particuarly forced: I wouldn't expect these sentences to be very common in everyday language, and then perhaps LM performance is expected, as they wouldn't have seen these things before. I feel they're a strange kind of sentence with a somewhat difficult semantics even for humans. For the pilot pair, I'm not sure I would get the intended meanings for the difference between ""ballet dancer"" and ""modern dancer"" - I feel the ""dancer"" alone could yield either reading. So there may be quirks in the dataset that make it tricky to interpret exactly what's going on with model performance.
I'm curious about the strong performance of the RoBERTa model - could it be that this kind of classification task isn't particularly difficult, if we give the model enough training and data? I think the paper shows clear difficulties in the generation and inference aspects, but perhaps classification is getting better. 
It should be noted that there are also lots of recent work on novel, challenging NLI sets, which the authors seem aware of. There are some recent and contemporary works that also deal with figurative language (Chakrabarty 2021, ""Figurative Language in RTE"", Poliak 2018, https://aclanthology.org/D18-1007/) the authors should be aware of: in my mind, this work provides a novel, high-quality dataset, they could benefit from clarifying how their work is distinct from these.
A possible question: this work seems focused on starting with metaphors and getting literal interpretations. I think there are also significant applications the other way, particularly in metaphor generation, where the literal interpretations can be used as input and the metaphors as output. "
ARR_2022_0_review,"This paper introduced Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings.  The authors evaluated the performance of several language models with Fig-QA and found that the models achieve performance significantly over chance, but  largely fall beyond human performance. ","The authors opened a new perspective in the milieu of ACL with this figurative language.  They first organized its typology and presented quantitative results.  Then the authors crowdsourced 10256 metaphors and call it Fig-QA, a new benchmark task.  It can be used to evaluate the nonliteral reasoning abilities of language models, from multiple viewpoint of zero-shot, fine-tuned and prompting methods.  The overall proposal introduced a novel way to evaluate the language models, and showed how they indeed are far from human capability.
The discussion of the limitation of the present language models is rich and this work provides an important understanding about SOTA language models in the way different from other language model evaluation papers. ","I guess that there are theories about figurative languages. How can section 3.2 be grounded among such theory? If he authors could ground their results in relation to such theories, I think this paper will have even larger impact. ","Just a conjecture, but I am a non-native and I am certain that I am worse than GPT in these tasks. 
I guess that a non-native human cannot do well in these tasks, in general. 
 If you crowdsource the performance on Fig-QA among non-natives, it might reveal what could be  necessary to train LMs.
-It would be good if Fig-QA is  built for  other languages as well.
-Although I did not find any datasets nor software attached, I highly recommend Fig-QA to be publicised. ","5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"I guess that there are theories about figurative languages. How can section 3.2 be grounded among such theory? If he authors could ground their results in relation to such theories, I think this paper will have even larger impact. 
Just a conjecture, but I am a non-native and I am certain that I am worse than GPT in these tasks. 
I guess that a non-native human cannot do well in these tasks, in general. 
 If you crowdsource the performance on Fig-QA among non-natives, it might reveal what could be  necessary to train LMs.
-It would be good if Fig-QA is  built for  other languages as well.
-Although I did not find any datasets nor software attached, I highly recommend Fig-QA to be publicised. "
ARR_2022_0_review,"This paper tests the ability of large pretrained language models to assign correct meanings to creative metaphors in English, and also to generate plausible interpretations of such metaphors. Pretrained models are capable of doing this, but performance falls far short of human-level in both zero-shot and pre-trained settings. Models employ a variety of interpretive heuristics such as absolute probability of the answer. ","Tasks for which pre-trained LMs do a significantly worse job than humans even with fine-tuning are relatively hard to find. This task is hard even for fine-tuned GPT-3, which makes it a good potential benchmark for future research into figurative language. There is an extensive set of experiments and a relatively large novel dataset. Overall, I think this paper is quite strong. ","The paper ""A howling success or a working sea? Testing what BERT knows about metaphors"" by Pedinotti et al, Blackbox NLP 21, should be cited here if possible. ","I suspect the forced-choice setting makes a significant difference to human interpreters, as suggested in ft. 2 ""The opposing meanings help to avoid ambiguity in the correct answer, make the task intuitive for human annotators, and help prevent annotation artifacts that have plagued other NLI datasets."" Seeing both possible answers makes the pragmatic scale on which the metaphor is being interpreted very explicit (for metaphor interpretation as structural mapping of conceptual domains, see eg Gentner and Bowdle 2008). For example, ""He hustles like he's a billionaire's son"" (Table 1) might be interpreted as a description of how he gets things done (by using his contacts and social privilege) or of how hard he works (not hard because he doesn't need to). The scale ""not at all - hardcore"" selects the meaning. Similarly, ""She was as embarrassed as a kid who got an A"" might mean she is ashamed of being called a nerd, or proud of her academic success. Natural examples of creative metaphors with unexpected scalar mappings are not hard to find (for instance, the American F4 Phantom jet was nicknamed the ""Flying Brick""; while as a conventional metaphor, ""flies like a brick"" means ""cannot fly at all"", the intended meaning is that the Phantom was unaerodynamic but had a very powerful engine).
While the paper notes that humans do not have a problem processing metaphors (sec 7.3), I believe these results are for metaphors in context, and often for conventional ones, not creative ones. I am not sure of whether similar results apply to creative metaphors, especially out of context. ",4.5 ,Maybe,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The paper ""A howling success or a working sea? Testing what BERT knows about metaphors"" by Pedinotti et al, Blackbox NLP 21, should be cited here if possible. 
I suspect the forced-choice setting makes a significant difference to human interpreters, as suggested in ft. 2 ""The opposing meanings help to avoid ambiguity in the correct answer, make the task intuitive for human annotators, and help prevent annotation artifacts that have plagued other NLI datasets."" Seeing both possible answers makes the pragmatic scale on which the metaphor is being interpreted very explicit (for metaphor interpretation as structural mapping of conceptual domains, see eg Gentner and Bowdle 2008). For example, ""He hustles like he's a billionaire's son"" (Table 1) might be interpreted as a description of how he gets things done (by using his contacts and social privilege) or of how hard he works (not hard because he doesn't need to). The scale ""not at all - hardcore"" selects the meaning. Similarly, ""She was as embarrassed as a kid who got an A"" might mean she is ashamed of being called a nerd, or proud of her academic success. Natural examples of creative metaphors with unexpected scalar mappings are not hard to find (for instance, the American F4 Phantom jet was nicknamed the ""Flying Brick""; while as a conventional metaphor, ""flies like a brick"" means ""cannot fly at all"", the intended meaning is that the Phantom was unaerodynamic but had a very powerful engine).
While the paper notes that humans do not have a problem processing metaphors (sec 7.3), I believe these results are for metaphors in context, and often for conventional ones, not creative ones. I am not sure of whether similar results apply to creative metaphors, especially out of context. "
ARR_2022_235_review,"This paper presents an analysis of the representations obtained from the CLIP language encoder and compares them to representations from the GPT-2 language model. The paper makes the following main contributions:  1) It studies the isotropy of the language encoder of the CLIP model and compares it to that of the GPT-2 language model and finds that:     - CLIP language representations are more isotropic than GPT-2 ones. 
2) It evaluates contextualized word and sentence embeddings obtained from the CLIP language encoder to those obtained from GPT-2 and finds:      - CLIP contextualized word embeddings (CWEs) outperform GPT-2 CEWs on 6 world-level evaluation tasks. 
     - CLIP sentence embeddings outperform GPT-2 sentence embeddings on the STS-benchmark. ","- The paper is very well written and easy to follow.
- The experimental setup is straight-forward and closely follows that of related work.
- The results are highly interesting and of great relevance for the community. Mostly because:      - The high anisotropy of contextualized word representations derived from GPT-2 or BERT-like models is a puzzeling finding (1) and this paper provides evidence that high anisotropy might be a result of the training objective and not the architecture. This opens up interesting directions for future work. 
    - While the original CLIP paper shows strong results on the visual modality, the text modality is left mostly unexplored. This paper fills that gap and demonstrates the applicability and usefulness of CLIP language representations.  (1) Ethayarajh (2019) https://aclanthology.org/D19-1006/ ",- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings. ,"- I would recommend to clarify notation when it comes to GPT-2. There's the GPT-2 architecture and the GPT-2 pre-trained language model. Saying that CLIP uses GPT-2 as it's language model (line 84) might lead readers to assume it uses the pre-trained GPT-2. Which is not the case. It will be helpful if the authors are more explicit and consistent when referring to the CLIP language encoder.
- In lines 507-514 authors talk about how CWEs lose mutual information with the the input word and cite Voita et al (2019). Additionally, they argue how this might not be the case for the CLIP [EOS] token. I'm not sure if the finding of Voita et al (2019) applies to the CLIP representations as no language modeling objective is used during CLIP training. So mutual information with the input might be higher for all tokens, not just the [EOS] token. This is something that could be checked. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings. 
- I would recommend to clarify notation when it comes to GPT-2. There's the GPT-2 architecture and the GPT-2 pre-trained language model. Saying that CLIP uses GPT-2 as it's language model (line 84) might lead readers to assume it uses the pre-trained GPT-2. Which is not the case. It will be helpful if the authors are more explicit and consistent when referring to the CLIP language encoder.
- In lines 507-514 authors talk about how CWEs lose mutual information with the the input word and cite Voita et al (2019). Additionally, they argue how this might not be the case for the CLIP [EOS] token. I'm not sure if the finding of Voita et al (2019) applies to the CLIP representations as no language modeling objective is used during CLIP training. So mutual information with the input might be higher for all tokens, not just the [EOS] token. This is something that could be checked. "
ARR_2022_354_review,"This paper studies the routing fluctuation problem in the mixture of experts (MoE) Transformers: existing MoE methods assign the same input to different experts even when the training almost ends. This fluctuation hurts sample efficiency and leads to slower convergence.  It introduces a two-stage training routing strategy called StableMoE to stabilize the routing: first learns a lightweight and more balanced router via a balanced loss and the router distillation, then freezes the router for stable token-to-expert assignment for second-stage training that only has the task loss. The novelty is in the decoupled two-stage training strategy and the use of a stable routing strategy. The proposed balanced loss is not distinct from the one in Switch Transformer.
Experimental results on language model and multilingual machine translation tasks show StableMoE achieves better performance than other MoE methods and converges faster. The ablation study further confirms the two-stage training stabilizes the routing and verify the effectiveness of StableMoE. It remains unclear whether the total budget for training (i.e. total compute resources) is smaller compared to exiting MoE methods. It also misses the inference speedup results. ","- This paper identifies the routing fluctuation problem and provides a thorough analysis as indicated in the ablation study.  - The two-stage training strategies in StableMoE are simple and effective to reduce the routing fluctuation during training. The lightweight distilled router is novel and useful.
- The empirical results show strong task performance improvements and faster training convergence over existing MoE methods. ","- Experiments need further improvements. StableMoE converges faster than other MoE methods, but the total compute budgets are not revealed and remain unclear. The inference speed comparisons are also missing.
- The overall contribution of StableMoE is strong, but the contribution of the balance loss is marginal. Comparison (either empirical results or theoretical justifications) to existing load-balancing methods are missing. ","- Does the routing fluctuation problem exist in common MoE models or just the one compared (BASE Layer)? The experiment baselines include Hash Layer and Switch Transformer, but the paper only reports the token-to-expert assignment changes for the BASE Layer model.   - How is the balance loss different from the one in Switch Transformer? The two losses are both encouraging the model to assign the tokens to experts in a more balanced way.  - The main results compare the training FLOPS, how about the total compute budget/costs for convergence? And also the inference speed is unclear.
- For ""still uses original assignment scores as input, so the gate signal can also be learned in training stage 2"" (line 215 to 216), the routing strategy is fixed, so how the gate single could be also learned during stage 2? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Experiments need further improvements. StableMoE converges faster than other MoE methods, but the total compute budgets are not revealed and remain unclear. The inference speed comparisons are also missing.
- The overall contribution of StableMoE is strong, but the contribution of the balance loss is marginal. Comparison (either empirical results or theoretical justifications) to existing load-balancing methods are missing. 
- Does the routing fluctuation problem exist in common MoE models or just the one compared (BASE Layer)? The experiment baselines include Hash Layer and Switch Transformer, but the paper only reports the token-to-expert assignment changes for the BASE Layer model.   - How is the balance loss different from the one in Switch Transformer? The two losses are both encouraging the model to assign the tokens to experts in a more balanced way.  - The main results compare the training FLOPS, how about the total compute budget/costs for convergence? And also the inference speed is unclear.
- For ""still uses original assignment scores as input, so the gate signal can also be learned in training stage 2"" (line 215 to 216), the routing strategy is fixed, so how the gate single could be also learned during stage 2? "
ARR_2022_120_review,"This paper introduces LexGLUE, a new benchmark for language understanding for English legal texts. 
The authors claim it is important to have a benchmark for this, as NLU is more and more used for applications in legal domains, but it is not so clear what works and what not. 
With the easier access to datasets and tools for legal NLP, hope is also to introduce more interdisciplinary researchers to legal NLP and  speed up the adoption and transparent evaluation of new legal NLP methods. 
The authors collect 7 datasets for legal NLP (Multi-label classification, Multi class classification, multiple choice QA). 
They use a SVM baseline and different pretrained transformers on this task to get an initial performance picture ",- Authors chose only publicly available datasets - Datasets have a variety of sizes and sub domains - Baseline evaluation with many different models - Nice ethics statement ,"- Only English, it is understandable and explained in the paper why that is but still sad - Only 2.5 different tasks (Multi-label classification, Multi class classification, multiple choice QA) - Datasets are not new, but just aggregated from other people ","It would be nice to hedge a bit more the NLU claim, as the tasks in this benchmark are mostly classification. NLU for me is more sematnic tasks NLI/QA/semantic parsing, relation extraction, ... ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)","4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- Only English, it is understandable and explained in the paper why that is but still sad - Only 2.5 different tasks (Multi-label classification, Multi class classification, multiple choice QA) - Datasets are not new, but just aggregated from other people 
It would be nice to hedge a bit more the NLU claim, as the tasks in this benchmark are mostly classification. NLU for me is more sematnic tasks NLI/QA/semantic parsing, relation extraction, ... "
ARR_2022_120_review,"The authors present LexGLUE, a benchmark for legal language understanding. Their goal is that in releasing this benchmark, it will be easier to measure progress on legal language understanding, which currently has relatively fragmented resources and evaluations. They also release several baselines on the benchmark they release. ","This is a well written paper that's motivation is quite clear. 
The datasets/benchmarks gathered should be useful to researchers working on legal language understanding. ","None of the datasets presented are actually new, rather these are just new splits and a packaging of existing resources into a benchmark. 
The tasks require input texts of drastically different lengths, which a single model might not be very good at. 
The baselines are a little bit old and arbitrary (no encoder-decoder models and only one type of hierarchical modeling). ","LexGLUE feels like kind of an odd name for a legal focused dataset 2 Line 231-232: s/access/assess 3 It feels odd to try to make only have english a desirable property of the benchmark, instead of a limitation 3.2  what does it mean articles cannot be violated? 
are these cases normally split chronologically? While this ensures a lack of overlap, it seems possible that each court produces a slightly different distribution of decisions. Is this type of out of distribution evaluation intended?
4.2 Why are all of the models tested encoder-only? Why no encoder-decoder models like T5? 
BERT-like models etc. not that good at storing sentence encoding representations in the CLS token (see e.g., Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders (Liu et al 2021) or Condenser: a Pre-training Architecture for Dense Retrieval (Gao & Callan 2021), so difficult to conclude these baselines are particularly strong. In general, hierarchical modeling seems to be its own task area, where both different pretraining techniques might produce better sentence encodings and different methods of hierarchical modeling may produce better overall results. 
This baseline seems to combine tasks of various different lengths – do you expect the same models to be good at both? Is it even important for one model to be good at both?
5.2: feels a bit hard to say that your result clearly favors the legal models -- 13 total results, 6 of which the legal models do the best Why is there no “overall” score reported, like there is with GLUE? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","a bit unclear if legal == ethical, esp. as sometimes things at small scale can be ethical but at large scale become less so Models that are better at predicting the outcome of a court case, for instance, could be entrenching existing biases that the criminal justice system has ","None of the datasets presented are actually new, rather these are just new splits and a packaging of existing resources into a benchmark. 
The tasks require input texts of drastically different lengths, which a single model might not be very good at. 
The baselines are a little bit old and arbitrary (no encoder-decoder models and only one type of hierarchical modeling). 
LexGLUE feels like kind of an odd name for a legal focused dataset 2 Line 231-232: s/access/assess 3 It feels odd to try to make only have english a desirable property of the benchmark, instead of a limitation 3.2  what does it mean articles cannot be violated? 
are these cases normally split chronologically? While this ensures a lack of overlap, it seems possible that each court produces a slightly different distribution of decisions. Is this type of out of distribution evaluation intended?
4.2 Why are all of the models tested encoder-only? Why no encoder-decoder models like T5? 
BERT-like models etc. not that good at storing sentence encoding representations in the CLS token (see e.g., Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders (Liu et al 2021) or Condenser: a Pre-training Architecture for Dense Retrieval (Gao & Callan 2021), so difficult to conclude these baselines are particularly strong. In general, hierarchical modeling seems to be its own task area, where both different pretraining techniques might produce better sentence encodings and different methods of hierarchical modeling may produce better overall results. 
This baseline seems to combine tasks of various different lengths – do you expect the same models to be good at both? Is it even important for one model to be good at both?
5.2: feels a bit hard to say that your result clearly favors the legal models -- 13 total results, 6 of which the legal models do the best Why is there no “overall” score reported, like there is with GLUE? "
ARR_2022_199_review,"This paper proposes “Show, Don’t Tell” (SDT), which frames an example dialogue as a prompt for describing schema to the DST model. Unlike previous works using descriptions of given schema, The SDT uses an example dialogue that consists of context and prompt based on the schema. The experimental results show that the SDT-based models perform strongly on SGD and MultiWOZ cross-domain benchmarks. Moreover, the SDT-based models consistently outperform their baselines in various model sizes and show data efficiency and robustness. ","- Simple modifications but strong results - Especially, they propose optimal input representation of API schema compared to other description-based baselines - Various analyses ","- Creating/Selecting a dialogue containing all slots is not easy when the dialogue is especially in multi-domains. 
  - In multi-domain dialogue, the length of dialogue is often long and the information of schema is more complex. 
  - To encode whole information for the long and complex dialogue, there is another challenge regarding sequence length.
- Moreover, the long token length of the SDT prompt yields computational complexity.
- Limited analysis on the number of dialogue examples. ( I think it is hard to increase the number of example dialogues for the same reason.) ","- LLMs in Section 6: Did you mean the Large Language Model? I think it is not a normal expression.
- How do the SDT prompts work on in-context learning? ( one-shot inference without gradient update using GPT-3.) ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Creating/Selecting a dialogue containing all slots is not easy when the dialogue is especially in multi-domains. 
  - In multi-domain dialogue, the length of dialogue is often long and the information of schema is more complex. 
  - To encode whole information for the long and complex dialogue, there is another challenge regarding sequence length.
- Moreover, the long token length of the SDT prompt yields computational complexity.
- Limited analysis on the number of dialogue examples. ( I think it is hard to increase the number of example dialogues for the same reason.) 
- LLMs in Section 6: Did you mean the Large Language Model? I think it is not a normal expression.
- How do the SDT prompts work on in-context learning? ( one-shot inference without gradient update using GPT-3.) "
ARR_2022_199_review,"This paper proposed to use a single dialog example(for per service or slot) to form an example-based prompt for schema-guided dialog state tracking. Based on T5 seq2seq fine-tuning, they compared the example-based prompt with the previous description-based prompt. The evaluation is based on two schema-guided datasets: SGD and MultiWOZ-leave-one-out, and two SOTA models for schema-guide dialog: T5-ind and T5-seq. The results showed that although the example-based prompt will take more annotation time, using example-based prompt, the fine-tuned seq2seq model will lead to better generalization to new APIs, more data-efficient, and robust to schema variations. However, some details are missing and some ablation studies are not well-designed. ","On the specific schema-guided dialog state tracking task, compared to the description-based prompt, the proposed example-based prompt can generalize better to new APIs, more data-efficient and more robust to schema variation. 
The idea of using example-based prompt for seq2seq fine-tuning seems simply focused and promising, which can be used for other seq2seq dialog state tracking settings, and may also be extended to other dialog tasks. ","1. Some of the details and analysis are not clear, please see comments 1,2,3,4 2. Some of the ablation studies are unclear or problematic. See comments 5,6 ","1. line 130, It is confusing why SGD dataset requires manually created dialog examples for prompts; But for MultiWOZ, it just chooses the example from the training set. please explain why. 
2. line 175 seems lacking of experimental results to support it. 
3. line 121, the paper lacks details for how to design and use prompts for dialog with multi-domains/services. According to line 140,  I only can guess it split the dialog state with multi-domain/service into separate examples, correct? please explain more. If possible, please show some examples in the Appendix for easy understanding. 
4. Error analysis in 5.2 is crappy, more quantitive error analysis will help. 
5. Table 5, the models above the line are using slot descriptions, while the SDT-seq is using name only. It seems they are not directly comparable. Although we may guess that description is more robust than name or slot-IDs, as shown in previous works[1,2]. Please explain more, and also list all name-based results on SGD-X. 6. line 165, finetuning T5-seq with prompt examples is meaningless given there are only #services new examples in prompt. Instead, for the combination of using description + example, could you simply append those example-based prompt after description-based prompt.  This new experiment will help to show how the interesting combination will help.   [1]Cao, Jie, and Yi Zhang. "" A Comparative Study on Schema-Guided Dialogue State Tracking."" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. 
[2]Lee, H., Gupta, R., Rastogi, A., Cao, Y., Zhang, B. and Wu, Y., 2021. SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. arXiv preprint arXiv:2110.06800.
7. The model names T5-ind, T5-seq are misleading. Maybe using Desc-ind/seq, Eg-ind,Demo-ind/seq? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. Some of the details and analysis are not clear, please see comments 1,2,3,4 2. Some of the ablation studies are unclear or problematic. See comments 5,6 
1. line 130, It is confusing why SGD dataset requires manually created dialog examples for prompts; But for MultiWOZ, it just chooses the example from the training set. please explain why. 
2. line 175 seems lacking of experimental results to support it. 
3. line 121, the paper lacks details for how to design and use prompts for dialog with multi-domains/services. According to line 140,  I only can guess it split the dialog state with multi-domain/service into separate examples, correct? please explain more. If possible, please show some examples in the Appendix for easy understanding. 
4. Error analysis in 5.2 is crappy, more quantitive error analysis will help. 
5. Table 5, the models above the line are using slot descriptions, while the SDT-seq is using name only. It seems they are not directly comparable. Although we may guess that description is more robust than name or slot-IDs, as shown in previous works[1,2]. Please explain more, and also list all name-based results on SGD-X. 6. line 165, finetuning T5-seq with prompt examples is meaningless given there are only #services new examples in prompt. Instead, for the combination of using description + example, could you simply append those example-based prompt after description-based prompt.  This new experiment will help to show how the interesting combination will help.   [1]Cao, Jie, and Yi Zhang. "" A Comparative Study on Schema-Guided Dialogue State Tracking."" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. 
[2]Lee, H., Gupta, R., Rastogi, A., Cao, Y., Zhang, B. and Wu, Y., 2021. SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. arXiv preprint arXiv:2110.06800.
7. The model names T5-ind, T5-seq are misleading. Maybe using Desc-ind/seq, Eg-ind,Demo-ind/seq? "
ARR_2022_283_review,"The paper addresses the problem of privacy for document embeddings. The authors define a strong sentence-level privacy for documents and propose deepcandidate, an unsupervised embedding technique to generate sentence-private document embeddings. Experiments on sentiment analysis and text classification were conducted to show the usefulness and effectiveness of the proposed method. ","- The paper is theoretically sound. There are a lot of mathematical notations in the paper but the authors defined them clearly, explained in detail, also showed the some necessary derivations (in Appendix) to help the readers to understand.
- Experimental results show SentDP performs much better on stronger privacy guarantee, empirically proved the previous theoretical statements.
- The authors proposed a novel approach from a strong privacy definition for document embeddings, which is significant different from the existing works. ","- It would be nice to show more experimental results of the proposed method from more aspects such as an ablation study or parameter analysis.
- There is still a clear gap between the private embeddings and non-private ones. For some baseline approaches the embedding performances are basically random guessing. The paper failed to explain this tradeoff and potential solutions. ","- One question: x' is sampled within certain distance from x in the embedding space, does that mean they are semantically/syntactically similar? If so, how much high-level concepts from the documents can be hided? ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- It would be nice to show more experimental results of the proposed method from more aspects such as an ablation study or parameter analysis.
- There is still a clear gap between the private embeddings and non-private ones. For some baseline approaches the embedding performances are basically random guessing. The paper failed to explain this tradeoff and potential solutions. 
- One question: x' is sampled within certain distance from x in the embedding space, does that mean they are semantically/syntactically similar? If so, how much high-level concepts from the documents can be hided? "
ARR_2022_283_review,"This paper introduces an approach to produce privacy-preserving document embeddings with the property that every single sentence of the document could be replaced by some other random one while obtaining a similar embedding for the document. The paper is way out of my area of expertise, and thus this review must remain an educated guess. 
The proposed method replaces sentence embeddings from the private document for embeddings obtained from a set of public documents. The method uses embeddings that are close to the original ones by picking those with high ""Tukey Depth"" with respect to the set of candidate embeddings. Finally, the embeddings are spread apart by passing them through a network trained with an unsupervised clustering signal, and, if understand correctly, averaged together. ","1. The method seems to be well grounded in a theoretical framework. 
2. The paper is very clear, and even though I lack the relevant background I could follow it (with quite some effort, which is also quite natural) 3. The empirical results seem good (but I am not familiar with the state of the art in this area) ","1. As an outsider, I wonder whether some natural baselines could have been explored. For instance, given that the method uses sentence embeddings after a clustering transformation, I wonder what would happen if cluster centroids were directly used instead. Similarly, I wonder what's the need for computing the Tucker Depth, and not just using the closest vector in terms of cosine similarity. ","L404: It might be worth recalling the reader that k is the number of sentences at this point, given that this fact was introduced much earlier on and not used until then. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. As an outsider, I wonder whether some natural baselines could have been explored. For instance, given that the method uses sentence embeddings after a clustering transformation, I wonder what would happen if cluster centroids were directly used instead. Similarly, I wonder what's the need for computing the Tucker Depth, and not just using the closest vector in terms of cosine similarity. 
L404: It might be worth recalling the reader that k is the number of sentences at this point, given that this fact was introduced much earlier on and not used until then. "
ARR_2022_196_review,The authors of this paper propose an empirical study for the zero-shot capabilities of CLIP models and demonstrate that CLIP models have strong few-shot capabilities. The authors propose the TAP-C method for evaluating VQA and demonstrate zero-shot cross-modal transfer capabilities on the visual entailment. ,"1. The paper conducts rich experiments and presents several interesting insights, which have a certain value to the community. 
2. The paper is well-written and easy to follow. 
3. The proposed two-step prompt generation method is interesting for studying the zero-shot performance. ","1. There should be more VLU tasks to prove the argument of the paper, e.g., NLVR. ",1. It is best to use vector graphics in the paper. ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. There should be more VLU tasks to prove the argument of the paper, e.g., NLVR. 
1. It is best to use vector graphics in the paper. "
ARR_2022_80_review,"This paper presentes a multi task learning approach for automatic grading of English essays, by considering a holistic score as well as scores on individual essay traits. The authors proposed an LSTM based model and compared single task and multi task settings to show that Multi-task learning based system gives better performance, and is also much faster than the single task setup. They also present a comparison with a BERT model, and report a series of ablation tests to understand the relationship between traits and holistic scores. ","In Automatic Essay Scoring research, it is more common to develop models for a holistic scoring, although in general, there is an agreement that a score may have many dimensions (e.g., content, spelling/grammar, organization etc). This paper is among the few papers in the direction of modeling multiple dimensions of essay scoring.
They performed ablation tests in the multi task learning setup, to understand what traits are useful for each set of essays - I think this is an interesting experiment I did not see before in this task's context.
They use a popular dataset which is publicly available and uploaded their code along with the paper. ","-  The paper does not seem to have any comparison with previous work on this topic at all. They directly do different experiments using their own architecture. Simple baselines (e.g., document length) that are commonly used for this problem can be used as a comparison point, in a STL setup, where the predicted variable can be different (holistic score, individual trait scores), keeping the text representation constant.
- The paper misses a discussion on the limitations of the current approach. For example, the authors commented in the response pdf to one of the reviewers that performance difference across traits is due to topical variation. The modeling process does not have any specific component to account for such topical variations resulting in different scores. It could be a potential limitation. I am not saying a paper is bad because it has a limitation. I think acknowledging limitations gives a more holistic perspective for the reader about the approach. ","I reviewed the previous version of this paper, and most of the minor comments I mentioned have been addressed in this version.  One other comment:  - How are the trait  scores obtained for the prompts that did not have them in the original dataset? The authors claim they took from another source, but understanding how they are created is relevant for this paper.  - I think having a few points of comparison to your approach will give a better perspective for us as readers. Comparison between LSTM and BERT is good, but not enough, as  this topic still has a dominant approach of combining some linguistic features with neural models, for modeling overall score. For individual traits too, predicting the trait score instead of individual score, keeping rest of the set up same, may give you a quick comparison point, and make the paper more complete. ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)","3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"-  The paper does not seem to have any comparison with previous work on this topic at all. They directly do different experiments using their own architecture. Simple baselines (e.g., document length) that are commonly used for this problem can be used as a comparison point, in a STL setup, where the predicted variable can be different (holistic score, individual trait scores), keeping the text representation constant.
- The paper misses a discussion on the limitations of the current approach. For example, the authors commented in the response pdf to one of the reviewers that performance difference across traits is due to topical variation. The modeling process does not have any specific component to account for such topical variations resulting in different scores. It could be a potential limitation. I am not saying a paper is bad because it has a limitation. I think acknowledging limitations gives a more holistic perspective for the reader about the approach. 
I reviewed the previous version of this paper, and most of the minor comments I mentioned have been addressed in this version.  One other comment:  - How are the trait  scores obtained for the prompts that did not have them in the original dataset? The authors claim they took from another source, but understanding how they are created is relevant for this paper.  - I think having a few points of comparison to your approach will give a better perspective for us as readers. Comparison between LSTM and BERT is good, but not enough, as  this topic still has a dominant approach of combining some linguistic features with neural models, for modeling overall score. For individual traits too, predicting the trait score instead of individual score, keeping rest of the set up same, may give you a quick comparison point, and make the paper more complete. "
ARR_2022_336_review,"This paper pursues a line of research about 'prompting', a method that leverages the prediction capability of pre-trained language models to help perform few-shot classification.  The paper explores one of the research topics in this line of research: how to find good mappings from the original task labels (e.g., positive vs negative) to tokens that can be predicted by the language model (the `label words', e.g., ""great"", ""perfect"" vs ""terrible"", ""awful"").  More specifically, the authors propose a simpler method to use multiple labels words; and a selection method that makes sure that no label word is shared by multiple labels. 
Experiments are presented, with comparison to baselines and earlier methods (Shick et al., 2020; Gao et al., 2021), followed by analyses and a discussion. 
This is a second revised version. ","- The authors propose a simple method to use multiple label words per class by summing their language model probabilities, and does not need to perform fine-tuning during label-word search.  Gao et al. (2021) instead only keep the best label word for each class, and perform fine-tuning multiple times during label-word search.
- Tested on seven classification tasks in the BLUE benchmark, the method performs better in general than earlier prompting methods on few-shot experiments (n=16 examples in each class in train and in dev).
- Each experiment is performed five times, mean and standard deviation are provided.
- The Analysis and Discussion sections count three pages including tables and figures, and show example label words, examine the impact of deduplication, multiple labels, and label mapping, the impact of the number of training examples, and attempt to analyze why this methods works. ","- (minor) Studying the correlation between inter-class JS divergence and performance is a good point.  However, I found its results not entirely convincing because it depends on the presence of two of the six tasks: CoLA and SST-2.  CoLA is a quite different task, and the authors also report the correlation without this task, which becomes much smaller.  At that point the presence of a positive correlation entirely depends on the presence of SST-2.  Or to phrase it differently, no such correlation is observed in the subset of tasks {QQP, MRPC, QNLI, RTE}. ","- The authors have taken into account all my previous comments, and plan to look into the correlation point above. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- (minor) Studying the correlation between inter-class JS divergence and performance is a good point.  However, I found its results not entirely convincing because it depends on the presence of two of the six tasks: CoLA and SST-2.  CoLA is a quite different task, and the authors also report the correlation without this task, which becomes much smaller.  At that point the presence of a positive correlation entirely depends on the presence of SST-2.  Or to phrase it differently, no such correlation is observed in the subset of tasks {QQP, MRPC, QNLI, RTE}. 
- The authors have taken into account all my previous comments, and plan to look into the correlation point above. "
ARR_2022_258_review,"The paper provides a corpus of editorial edits for Wikipedia articles, scientific articles and news. Computational models (RoBERTa, FELIX, Pegasus) are tested against the edit intention labels and the human revisions. ",- defining an edit intention taxonomy and creating annotated data - testing the edit intention task with a baseline RoBERTa model - comparing human revisions and automated revisions based in 2 automated systems ,- do not provide a new system improving over the RoBERTa baseline system for the edit intention task ,"The paper provides a new data set for editorial edits with clear description on how the data set was assembled and how it can be used. 
Suggestions for improving the paper: - provide another SOTA approach for the edit-intention task going beyond the RoBERTa baseline - signal earlier that the automatic metrics do not correlate with the human metrics. The automatic evaluation configuration section is little bit hard to follow. Some of the explanations in the appendix could help to improve the readability of this section. I'm still not sure though, what Overall indicate in Table 7 - Is the FELIX system useful? First, it does not learn when the revisions are complete (having the system is Figure 2 is not very informative) and looking at the suggestions provided by the samples in the appendix, another concern may be raised. FELIX generates frequently hallucinations that potentially are contradictory to the initial text. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",none ,"- do not provide a new system improving over the RoBERTa baseline system for the edit intention task 
The paper provides a new data set for editorial edits with clear description on how the data set was assembled and how it can be used. 
Suggestions for improving the paper: - provide another SOTA approach for the edit-intention task going beyond the RoBERTa baseline - signal earlier that the automatic metrics do not correlate with the human metrics. The automatic evaluation configuration section is little bit hard to follow. Some of the explanations in the appendix could help to improve the readability of this section. I'm still not sure though, what Overall indicate in Table 7 - Is the FELIX system useful? First, it does not learn when the revisions are complete (having the system is Figure 2 is not very informative) and looking at the suggestions provided by the samples in the appendix, another concern may be raised. FELIX generates frequently hallucinations that potentially are contradictory to the initial text. "
ARR_2022_142_review,"The paper presents QuALITY, a new benchmark for question answering over long passages. All the questions are multiple-choice, composed by professional writers and validated by MTurk annotators to be answerable and unambiguous. A subset of especially challenging questions is also selected in a task where annotators answer the question under strict time constraints. The paper presents a detailed analysis of the dataset and a thorough evaluation of long-context and extractive QA models on the presented data, demonstrating that all the models are far behind human performance. ","- Long-passage QA datasets are harder to collect and relatively scarce, so the new dataset would be a valuable addition to the field.
- The data collection and annotation process is very well thought out and includes multiple validation steps. The data is further validated in qualitative and quantitative analysis.
- The experimental part is thorough: both long-context models and extractive models are evaluated, and there are additional experiments with supplementary training data and no-context baselines. The choice of the QA baselines seems reasonable to me (although my expertise in QA is limited).  - The paper is clearly written and easy to follow, and both the data collection and the experimental evaluation are documented in detail. ",My only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page. ,"- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as ""Description"" or ""Symbolism"". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?
- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer’s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!
- L275: this inclusion criteria -> these inclusion criteria - L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader. ",4.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"My only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page. 
- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as ""Description"" or ""Symbolism"". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?
- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer’s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!
- L275: this inclusion criteria -> these inclusion criteria - L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader. "
ARR_2022_257_review,"This work presents a series of research works and their debates around the question ""Is Attention Explanation?"". This work summarizes past research works in a progressively deeper way, from the ""what"" to the ""why"" to the ""how"". Finally, the authors then give a discussion and conclusion for follow-up researchers. ","1. The authors present the debate using a line of thinking like a human in this paper. This is certainly a good way to do it, but it is not an efficient way either. 
2. This work is believed to have taken a lot of effort and the motivation of the paper is to be valuable to the community. ","1. Unfortunately, the paper does not give enough in-depth analysis nor does it indicate some definite future research lines. 
2. And the discussion of the impact of different tasks in this paper is not sufficiently informative and does not fit well with other sections. ",A guide or summary table can be used to help readers better and more quickly understand past research works. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",5 = They could easily reproduce the results.,,"1. Unfortunately, the paper does not give enough in-depth analysis nor does it indicate some definite future research lines. 
2. And the discussion of the impact of different tasks in this paper is not sufficiently informative and does not fit well with other sections. 
A guide or summary table can be used to help readers better and more quickly understand past research works. "
ARR_2022_339_review,"This paper focuses on an approach of Non-Autoregressive Generation (NAR) for summarization tasks, it includes three parts. The first part is using search-based method to the training data, the second part is training an encoder-only model, the third part includes the training strategy and a length-control algorithm. ","1. This work combines an unsupervised approach and NAR generation on a summarization task. The experiment shows some gain over baselines.  2. The experiment settings, ROUGE score and speed are detailed. ","1. The proposed NAR model is almost an encoder part of a transformer model, papers [1,2] are used an encoder-only model for NAR. The novelty is not clear described.  [1] Non-Autoregressive Text Generation with Pre-trained Language Models [2] Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation 2. Lack of an ablation analysis to show the gain from each change. For example, with the special blank token (the difference between this model and [1]) vs without it. ","1. The three observations taking a lot of space in the introduction are very general, the reader may want to know more about the specific issue this paper is targeted, and the contributions made in this work. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The proposed NAR model is almost an encoder part of a transformer model, papers [1,2] are used an encoder-only model for NAR. The novelty is not clear described.  [1] Non-Autoregressive Text Generation with Pre-trained Language Models [2] Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation 2. Lack of an ablation analysis to show the gain from each change. For example, with the special blank token (the difference between this model and [1]) vs without it. 
1. The three observations taking a lot of space in the introduction are very general, the reader may want to know more about the specific issue this paper is targeted, and the contributions made in this work. "
ARR_2022_339_review,"The paper proposed an unsupervised summarization method. It first generates pseudo summaries with a search-based method. A non-autoregressive model is then trained to predict these pseudo summaries. CTC loss is used during the training. Finally, a length-control inference method is proposed to decode short summaries from the model. ","1. Experiment results show strong unsupervised parsing performance on Gigaword and DUC2004 datasets. 
2. The encoder-only architecture is intuitively coherent with the nature of summarization. Ablation studies also show that NAUS is better than an encoder-decoder baseline. 
3. The proposed method is more efficient than search or autoregressive baselines. ",1. The novelty of this method seems to be marginal. The only novelty component proposed in this paper is Length-Control inference. ,"1. It would be better to separate the description of the previous methods and the proposed method in section 2. 
2. Some examples could help the audience to better understand the importance of length-control inference. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The novelty of this method seems to be marginal. The only novelty component proposed in this paper is Length-Control inference. 
1. It would be better to separate the description of the previous methods and the proposed method in section 2. 
2. Some examples could help the audience to better understand the importance of length-control inference. "
ARR_2022_36_review,"This paper proposes a new task XBRL tagging, which aims to annotate financial documents into the XBRL format. The target entities are mainly numbers, and the model needs to assign different types to these numbers. The size of type set is large, 139. This paper also proposes their own models on this task SEC-BERT-SHAPE. What they do is fine-tuning BERT on financial documents, and replace the fragment of numbers into a special token. They report SOTA performance on this task. ","- A new task that could be useful in the financial domain.
- A dataset for the new task.
- A model with SOTA performance.
- Compared with different baseline models. The setting of the experiments is good.
- The replacing of number fragments is interesting and could be useful for other related tasks. ","- It is unclear why SEC-BERT is better than FIN-BERT. The only difference seems to be the financial documents that BERT is pre-trained on. The paper fails to analyze why the different choice of documents leads to a large difference in performance.
- The technical novelty of SEC-BERT is limited, but it is fine to me since the model is not the only contribution in this paper. ","Why FIN-BERT is much worse than SEC-BERT?
What about the other XBRL tags that are not in the 139 tags? If this task is for real applications, the other tags are also necessary, right? ",3.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- It is unclear why SEC-BERT is better than FIN-BERT. The only difference seems to be the financial documents that BERT is pre-trained on. The paper fails to analyze why the different choice of documents leads to a large difference in performance.
- The technical novelty of SEC-BERT is limited, but it is fine to me since the model is not the only contribution in this paper. 
Why FIN-BERT is much worse than SEC-BERT?
What about the other XBRL tags that are not in the 139 tags? If this task is for real applications, the other tags are also necessary, right? "
ARR_2022_39_review,"This paper introduces an auto-regressive blank infilling model based upon an encoder-decoder architecture.  The model is very similar to T5 except for (i) a generic [MASK] token is used in place of unique sentinel tokens, (ii) the [MASK] tokens can be shuffled on the decoder side of the model, and (iii) 2D positional encodings are used -- the position of the corrupted text and the tokens within the corrupted span.  This architecture is intended to address the varying performance of encoder and decoder models over natural language understanding, conditional generation and unconditional generation tasks. ","- The paper is very clearly written.
- The illustrations of easy to follow and enhance understanding of concepts.
- Paper provides a concise and excellent review of competing architectures.
- I believe that the model could be re-implemented from information provided in Section 2.
- Thorough presentation of results. ","- Result in Table 1 are substantially lower than SOTA results, casting doubt on usefulness of this approach.
- Approach appears to be very incremental to T5, leading to questions about novelty.
- A clearer ablation studying comparing the seven potential combinations of the three differences to T5 is missing.
- The array of GLM models evaluated (e.g., GLM_{Doc}, GLM_{Sent}, GLM_{410M}, etc.) were not well motivated and did not add much to the presentation. ","I generally liked the paper, but I am not sure that the results (as presented) make a compelling argument to add another model to the transformer-based “pantheon of models.”  It would be great to see how larger GLM models compare to SOTA results, given that hardware resources do not appear to be a constraint.
Typo: Should “BRET” in line 65 be “BERT”? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Result in Table 1 are substantially lower than SOTA results, casting doubt on usefulness of this approach.
- Approach appears to be very incremental to T5, leading to questions about novelty.
- A clearer ablation studying comparing the seven potential combinations of the three differences to T5 is missing.
- The array of GLM models evaluated (e.g., GLM_{Doc}, GLM_{Sent}, GLM_{410M}, etc.) were not well motivated and did not add much to the presentation. 
I generally liked the paper, but I am not sure that the results (as presented) make a compelling argument to add another model to the transformer-based “pantheon of models.”  It would be great to see how larger GLM models compare to SOTA results, given that hardware resources do not appear to be a constraint.
Typo: Should “BRET” in line 65 be “BERT”? "
ARR_2022_329_review,"This paper introduces a new benchmark for the under-researched Turkish language, involving seven different NLP tasks. For each task, authors provide one or two datasets some of which are created by them while some others are taken from existing literature. The authors also establish baseline results on each dataset using various models. ",The paper has a clear motivation and tackles an important problem since there is a clear need for more benchmarks that will promote the development of better models for many under-studied languages. The proposed benchmark can help facilitate the development of better-performing models for the Turkish language. ,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case. ","General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice.  2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others.  3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
 Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format)  - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps.  - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus?  - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case. 
General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice.  2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others.  3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
 Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format)  - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps.  - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus?  - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in "
ARR_2022_247_review,"This paper presents NLU++, a new dataset for NLU tasks including intent detection and slot labeling across two domains, banking and hotels. The authors emphasize a focus on multi-intent sentences, a combination of cross-domain and domain-specific intents, and high quality through expert annotation. ","- The annotated dataset appears to by high-quality and useful to the research community.
- The authors do an interesting and extensive evaluation of the dataset for intent detection and slot labeling tasks with different baselines. ","- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text.  - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. ","Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text.  - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. 
Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? "
ARR_2022_248_review,"The paper explored the approaches to use external data to improve the spoken NER performance of both pipeline and E2E-based systems. These approaches include using unlabeled and text data to perform self-training, knowledge distillation and transfer learning. The experimental results showed that all these approaches lead to better performance in both pipeline and E2E-based spoken NER systems. ","This work explores many possible approaches in exploring how to use external data for spoken NER task. It covers both the speech and text from data aspects. In terms of the training approaches, the authors tried the self-training, transfer learning and knowledge distillation. This work is meaningful to the community. ","It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. ","Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:           Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ...."" ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. 
Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:           Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ...."" "
ARR_2022_248_review,"This work focuses on spoken NER task and explores the use of external data in various form which is not annotated for the task such as plain speech audio, plain text, and speech with transcripts. The paper compares the pipeline approach and end-2-end approach to analyze how they benefit from external data. ","S1: Impressive improvement in both E2E and pipeline models, up to 16% and 6% respectively.  S2: Experimenting with both E2E and pipeline approaches including respective baseline shows to impact of external data regardless of the approach.  S3: Error analysis in different forms gives some insight about where each approach (E2E or pipeline) differs or fails. ","W1: Considering that E2E model benefits from mostly training over pseudo-labels (“distillation”), the resulting model actually represents another pipeline, which requires the text-NER. This hurts the main arguments favoring the E2E approach over the pipeline model.
W2: For the E2E model, for methods that do not use pseudo-labeling, improvements are rather limited. This point needs to be discussed more comprehensively, especially to understand what makes the difference when looking at the comparison for “Un-Sp” external data type ","- Different fonts for the captions make it harder to read. I suggest using a consistent font for the caption and the main text.  - A sentence that introduces the findings is missing before enumerating points in line 433 - Positions of the figures are not fitting with the explanations, which makes the paper harder to read. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"W1: Considering that E2E model benefits from mostly training over pseudo-labels (“distillation”), the resulting model actually represents another pipeline, which requires the text-NER. This hurts the main arguments favoring the E2E approach over the pipeline model.
W2: For the E2E model, for methods that do not use pseudo-labeling, improvements are rather limited. This point needs to be discussed more comprehensively, especially to understand what makes the difference when looking at the comparison for “Un-Sp” external data type 
- Different fonts for the captions make it harder to read. I suggest using a consistent font for the caption and the main text.  - A sentence that introduces the findings is missing before enumerating points in line 433 - Positions of the figures are not fitting with the explanations, which makes the paper harder to read. "
ARR_2022_326_review,This paper proposes an entity linking method that leverages coreferent mentions in a corpus. The model is trained to produce a directed minimum spanning tree where each tree is constrained to have one or more coreferent mentions and a single KG entity to which all coreferent mentions are mapped. The proposed model is evaluated on two benchmark datasets - MedMentions (a dataset for biomedical entity linking ) and ZeShEL (a dataset for zero-shot entity linking). Each of these datasets contains a substantial number of unseen entities in the test set. The proposed method is shown to have improved recall@k for candidate retrieval. A cross-encoder is used for re-ranking the retrieved candidates and it yields comparable or better results to the baseline ,"1. The affinity-based graph construction with mention-mention and mention-entity edges for entity linking was already explored in prior work. This paper extends this line of work and formulates the entity linking task as a coreference resolution task. 
2. An extensive evaluation is done using different retrieval and tree construction methods that demonstrate the advantages of the model. ","1. Missing related work: A closely related line of work is collective entity linking that considers multiple mentions in the same document. I think the proposed model is a special case of collective entity linking. However, the paper does not mention this line of work. 
2. No error analysis is performed. So it’s hard to tell in which cases the model does not perform well. An error analysis would help to identify potential limitations of this model and pave way for future model improvements. ","1. What is the target KB size used for MedMentions in the experiments? Is it the number of unique entities in the labeled partition (as shown in Table 2) or is it the entire UMLS? 
2. What is used as ‘descriptions’ of entities in MedMentions? To the best of my knowledge, every entity in UMLS does not have a description. 
4. How are the training and inference batches constructed? Do they contain mentions from the same document to leverage coreferent mentions as much as possible? Since a fixed batch size (B=128) is used, there is a chance that a document will be split into several batches, or a batch may contain mentions from very different documents. A clear description of how batches are constructed would be helpful. 
4. Does the model need to compute \psi(m_i, e) for all e in the target KB during inference? If that is the case, then how does it impact the efficiency of the model during inference? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"1. Missing related work: A closely related line of work is collective entity linking that considers multiple mentions in the same document. I think the proposed model is a special case of collective entity linking. However, the paper does not mention this line of work. 
2. No error analysis is performed. So it’s hard to tell in which cases the model does not perform well. An error analysis would help to identify potential limitations of this model and pave way for future model improvements. 
1. What is the target KB size used for MedMentions in the experiments? Is it the number of unique entities in the labeled partition (as shown in Table 2) or is it the entire UMLS? 
2. What is used as ‘descriptions’ of entities in MedMentions? To the best of my knowledge, every entity in UMLS does not have a description. 
4. How are the training and inference batches constructed? Do they contain mentions from the same document to leverage coreferent mentions as much as possible? Since a fixed batch size (B=128) is used, there is a chance that a document will be split into several batches, or a batch may contain mentions from very different documents. A clear description of how batches are constructed would be helpful. 
4. Does the model need to compute \psi(m_i, e) for all e in the target KB during inference? If that is the case, then how does it impact the efficiency of the model during inference? "
ARR_2022_152_review,"This paper focuses on an important task incomplete utterance rewriting, the goal of which is to rewrite a semantically incomplete sentence into a semantically self-contained utterance. Since several words in the rewritten utterance come from the incomplete utterance, a lot of previous work has modeled this task as some sort of editing task. Following the same line, this paper proposes to leverage a picker and a generator to jointly predict the final utterance. Experimental results on two benchmarks are used to validate the effectiveness of the method. ","The paper is well written and easy to follow. The idea of combining the picker and the generator is straightforward and can be integrated easily with generative language models. The experimental results on four datasets validate the effectiveness of the proposed method.
--- Regarding the concerns I raised in my previous review (Review `XYZ`), I think the resubmission addresses some of my concerns in an appropriate way (see below).
> 1. This paper claims that the previous method is over fine-tuned on specific datasets (Line 69-71). However, according to my knowledge, the baseline model RUN (Liu et al. 2020) was experimented on four different datasets, including two used in this paper. Why not report experimental results on another two datasets? I suspect that the experimental results in this paper may be also over fine-tuned for specific datasets.
This paper adds to the experiments on the proposed datasets, and the results look promising and make me feel more convincing.
> 2. There are some issues in the experimental setting. First, the comparison to baselines are unfair. If the method in this paper leverages T5 as the encoder, it should at least try to employ the same one to the most advanced baseline (e.g., SARG). Otherwise, the comparison is unfair. Second, according to my knowledge, the reported F3 performance in RUN (Liu et al. 2020) is 47.7 (higher than this paper's result). However, the reproduced result in this paper is 45.1 using the author's code. I am wondering if the paper tuned the baseline performance as in their method.
This paper addresses some issue raised by my previous review. For example, they double checked the baseline performance and reported the ones from the original paper. Although the paper did not do a comparison with SARG (plus T5), it also showed sufficient superiority under the limited data setting. ","Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.
Besides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point. ","There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.
- line 151: why the length of R is not equal to the one of U?
- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.
Besides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point. 
There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.
- line 151: why the length of R is not equal to the one of U?
- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one "
ARR_2022_90_review,This paper studies how the vision model impacts the performance of multimodal translation through multiple probing experiments. The authors experiment with the recent ViT models trained on image classification as well as DETR trained on object detection and CATR trained on image captioning. The authors find that stronger vision models can improve probing tasks performance while not helping much on standard MMT. ,"- The authors propose several probing tasks complementary to Caglayan et al. - The authors show that, while the benefit of a stronger vision model is not obvious under standard MMT task, the vision model can help probing tasks.
- The authors propose to use selective attention combined with gated fusion to fuse vision features and source text. Such fusion can improve the performance of the probing tasks.
- The authors provided a very detailed analysis of different components of their methods. ","- The selective attention is rather straightforward.
- The motivation of proposing three new probing tasks is not obvious to me. The authors can just simply use the tasks in Caglayan et al. And also the color-based probing is almost the same as the Color Deprivation in Caglayan et al. ","A general comment: For MMT task, it is already known that the vision model does not contribute much to the standard MMT task. It seems quite artificial to me to create these probing tasks to draw conclusions for MMT. Since we know a stronger vision model helps image captioning, so it is foreseeable that a stronger vision model is helpful when visual information is missing. Can the authors elaborate on how significant the conclusion is and why we need such deliberately designed probing tasks? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The selective attention is rather straightforward.
- The motivation of proposing three new probing tasks is not obvious to me. The authors can just simply use the tasks in Caglayan et al. And also the color-based probing is almost the same as the Color Deprivation in Caglayan et al. 
A general comment: For MMT task, it is already known that the vision model does not contribute much to the standard MMT task. It seems quite artificial to me to create these probing tasks to draw conclusions for MMT. Since we know a stronger vision model helps image captioning, so it is foreseeable that a stronger vision model is helpful when visual information is missing. Can the authors elaborate on how significant the conclusion is and why we need such deliberately designed probing tasks? "
ARR_2022_90_review,"The paper adopts stronger vision models, such as Vision Transformer to represent images, and it investigates whether stronger vision models lead to better performance in MMT tasks. A selective attention mechanism is proposed for Vision Transformer to correlate words with image patches. The paper also conducts multiple probing experiments to explore the effectiveness of stronger visual features and different learning objectives. ","Strength: 1.Popular and stronger models in CV are introduced in the paper, and a corresponding attention method is proposed to utilize the patch-level image representation. The paper discusses whether stronger visual features help improve the performance of MMT, and the answer is positive.
2.The experiments are conducted on not only the original task, but on multiple probing tasks to demonstrate the effectiveness of the selective attention method. Though improvements brought by ViT with the selective attention are not significant enough, the method still outperforms previous baselines when inputs have masked words. ","Weakness 1.The figure descriptions need to be more specific. For example, what do the labels, “mask-1”, “mask-2” and so on mean? I think “the limited textual context” should be clearer that it refers to “noun-based probing”. ",Does this selective attention only work on patch-based vision models? And why use a single-head attention? Do the heads of attention affect the model performance? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Weakness 1.The figure descriptions need to be more specific. For example, what do the labels, “mask-1”, “mask-2” and so on mean? I think “the limited textual context” should be clearer that it refers to “noun-based probing”. 
Does this selective attention only work on patch-based vision models? And why use a single-head attention? Do the heads of attention affect the model performance? "
ARR_2022_29_review,"This paper studies efficiently processes long input sequences for conditional language generation with Transformer. In addition to local attentions where each token attends to its neighbors within a limited distance, their proposed efficient efficient attention mechanism dynamically constructs global tokens by aggregating representations for input tokens that are divided into even blocks (Transient Global Attention). The global tokens can be attended by all input tokens and thus enable long-range interaction. They adopt the design of T5 for other components of the Transformer and pre-train the model using the gap sentence prediction objective by Pegasus. The resulting model, LongT5, outperforms existing models designed for long sequences on long document summarization with arXiv, PubMed, BigPatent, and MediaSum. On NaturalQuestion and TriviaQA, their largest model also outperforms existing models. Their additional experiments show that increasing the input length and model size usually lead to improved performance. ","- Their pre-trained models are helpful for the research community.
- Their proposed efficient Transformer model (LongT5) performs better than the model with the original attention mechanism (T5) across different input lengths. It also achieves better performance on different datasets than results reported by existing work. ","While I appreciate their effort in building pre-trained models for long sequences, I do have the following concerns: - The results by other models (Table 2, Table 4 bottom) are taken from the original papers whose experiments might not be conducted under the same computational limit (i.e., GPU/TPU memory). It is likely that the improved performance is due to the authors' better computational resource that allows longer inputs. Also, it is unclear if techniques that can reduce memory usage (e.g., gradient checkpointing) are used by the authors in this paper to reach longer inputs.
- In Table 4 and Figure 2, due to the lack of T5 models pre-trained with the Gap Sentence Prediction objective, It is unclear the better performance by LongT5 is due to the Transient Global Attention or the Gap Sentence Prediction pre-training objective.
- There lacks some ablation studies for the Transient Global Attention (e.g., is it necessary to use all global tokens?). ",- Would be better if the input lengths used by different comparisons are listed. ,"3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"While I appreciate their effort in building pre-trained models for long sequences, I do have the following concerns: - The results by other models (Table 2, Table 4 bottom) are taken from the original papers whose experiments might not be conducted under the same computational limit (i.e., GPU/TPU memory). It is likely that the improved performance is due to the authors' better computational resource that allows longer inputs. Also, it is unclear if techniques that can reduce memory usage (e.g., gradient checkpointing) are used by the authors in this paper to reach longer inputs.
- In Table 4 and Figure 2, due to the lack of T5 models pre-trained with the Gap Sentence Prediction objective, It is unclear the better performance by LongT5 is due to the Transient Global Attention or the Gap Sentence Prediction pre-training objective.
- There lacks some ablation studies for the Transient Global Attention (e.g., is it necessary to use all global tokens?). 
- Would be better if the input lengths used by different comparisons are listed. "
ARR_2022_26_review,"The authors introduce a new annotated event detection dataset that is focused on annotation suicidal events. 
The annotation focuses on the following event types: suicide-related actions, thoughts/ideation, risk factors related to life, relationship, health, and other, in addition to protective factors related to positive activities such as taking medication. 
The author apply state-of-the-art event detection models on their dataset, where the performance is poorer in comparison to more established event detection domains. As a result, the authors are calling the community to build models that can improve the performance. ","- The developed dataset is interesting and the task is very important to NLPers who work on mental health. I see its impact going beyond the event detection task to other tasks in the same domain that are related to understanding what kind of triggers affect suicidality. This, hopefully, would yield better performance of the suicide risk assessment and other mental-health related classifiers.  - The fact that the annotators are mental health domain experts makes of a more reliable data.  - I find the discussion of the challenges of annotations very valuable and it resonates with many previous research where cases of someone talking about a friend trying to committing suicide should be differentiated than the person who is posting attempting suicide.  - The baseline models that the authors use are strong and recent.  - Thorough reproducibility check list (e.g model parameters, annotation examples) ","- Although it is not uncommon to see the ""OTHER"" label in annotation schema as a candidate label when the annotators cannot find a better mapping with the rest of the labels, I think OTHER label makes the annotations weaker. The annotators would default to it in many cases and as a result the number of annotations for that label become much higher in comparison to the other labels (in your case 15343 for OTHER vs. 21635 for the rest of the labels). This would generate problems such as data imbalance, potential noisy labels where the model might over predict the OTHER label.  - The call to the community by the authors for better performance models is a valid one, but we cannot eliminate putting some burden of the poor performance on the dataset itself. This might be due to embedded annotations' issues that can be simply a result of the domain itself. Did the authors do a thorough analysis to confirm that hypothesis?
- I think it is very important to see the performance of the models on each label separately. For instance, a valuable analysis would be to compare the performance per label with the annotation challenges. ","- Please check the weaknesses section for suggestions on how to improve the work. For instance, I would emphasize again that the readers need to see the performance on each event type to better understand the challenges and how to improve the models. 
For RF-HEALTH event type, you mention that it is about mentions that directly affect the subject's health. This can be ambiguous, for instance you might have a transitive relationship between life events and health, thus the event types are not exclusive (as you mention this was part of your annotation design). Could you please elaborate more on that? 
Is your dataset intersected with CLPsych 2019 dataset (Zirikly et al. 2019)? It would be very interesting and valuable if we have the ED annotations on that dataset to further push the performance of the risk assessment models with the use of the triggers.   Typos: Preposition is missing after models on line 276 ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,"It is good that the authors mention IRB exemption and their efforts to further remove any user mentions. However, given the sensitivity of the topic, I think there should be data user agreement or something similar in place, even if Reddit data is publicly available. ","- Although it is not uncommon to see the ""OTHER"" label in annotation schema as a candidate label when the annotators cannot find a better mapping with the rest of the labels, I think OTHER label makes the annotations weaker. The annotators would default to it in many cases and as a result the number of annotations for that label become much higher in comparison to the other labels (in your case 15343 for OTHER vs. 21635 for the rest of the labels). This would generate problems such as data imbalance, potential noisy labels where the model might over predict the OTHER label.  - The call to the community by the authors for better performance models is a valid one, but we cannot eliminate putting some burden of the poor performance on the dataset itself. This might be due to embedded annotations' issues that can be simply a result of the domain itself. Did the authors do a thorough analysis to confirm that hypothesis?
- I think it is very important to see the performance of the models on each label separately. For instance, a valuable analysis would be to compare the performance per label with the annotation challenges. 
- Please check the weaknesses section for suggestions on how to improve the work. For instance, I would emphasize again that the readers need to see the performance on each event type to better understand the challenges and how to improve the models. 
For RF-HEALTH event type, you mention that it is about mentions that directly affect the subject's health. This can be ambiguous, for instance you might have a transitive relationship between life events and health, thus the event types are not exclusive (as you mention this was part of your annotation design). Could you please elaborate more on that? 
Is your dataset intersected with CLPsych 2019 dataset (Zirikly et al. 2019)? It would be very interesting and valuable if we have the ED annotations on that dataset to further push the performance of the risk assessment models with the use of the triggers.   Typos: Preposition is missing after models on line 276 "
ARR_2022_26_review,The work focuses on the Event Detection task to identify event trigger words of suicide-related events in public posts of discussion forums. ,"Authors introduce a new dataset for ED (SuicideED) that features seven suicidal event types to comprehensively capture suicide actions and ideation, and general risk and protective factors. ",Limited awareness of the literature. ,"The manuscript is centered on a very interesting and timely topic, which is also quite relevant to the themes of ARR. Organization of the paper is good and the proposed method is quite novel. The length of the manuscript is about right.
The manuscript, however, does not link well with recent literature on AI for mental health, e.g., see sentic computing for patient centered applications. Also, latest trends in mental disorder detection with attentive relation networks are missing. Finally, check Ji et al.’s recent review of suicidal ideation detection.
The manuscript presents some bad English constructions, grammar mistakes, and misuse of articles: a professional language editing service (e.g., the ones offered by IEEE, Elsevier, or Springer) is strongly recommended in order to sufficiently improve the paper's presentation quality for meeting the high standards of ARR.
Finally, double-check both definition and usage of acronyms: every acronym, e.g., CNN, should be defined only once (at the first occurrence) and always used afterwards (except for abstract and section titles). Also, it is not recommendable to generate acronyms for multiword expressions that are shorter than 3 words, e.g., ED (unless they are universally recognized, e.g., AI). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"
The manuscript is centered on a very interesting and timely topic, which is also quite relevant to the themes of ARR. Organization of the paper is good and the proposed method is quite novel. The length of the manuscript is about right.
The manuscript, however, does not link well with recent literature on AI for mental health, e.g., see sentic computing for patient centered applications. Also, latest trends in mental disorder detection with attentive relation networks are missing. Finally, check Ji et al.’s recent review of suicidal ideation detection.
The manuscript presents some bad English constructions, grammar mistakes, and misuse of articles: a professional language editing service (e.g., the ones offered by IEEE, Elsevier, or Springer) is strongly recommended in order to sufficiently improve the paper's presentation quality for meeting the high standards of ARR.
Finally, double-check both definition and usage of acronyms: every acronym, e.g., CNN, should be defined only once (at the first occurrence) and always used afterwards (except for abstract and section titles). Also, it is not recommendable to generate acronyms for multiword expressions that are shorter than 3 words, e.g., ED (unless they are universally recognized, e.g., AI). "
ARR_2022_26_review,The authors construct a corpus on suicide events. The token level annotations and event types are invaluable. This corpus enable quantification of the performance of the state-of-the-art modeling approaches. The performance scores show that we still need to work a lot on this task. ,"The inter-annotator agreement and the use of Reddit data, which is user-generated text, make this work valuable. The token level annotations and event types are invaluable. ","The paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? 
2- ""with more than 50 words are kept to increase"" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? 
3- ""Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019)."" - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & Hürriyetoğlu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... ","1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. 
2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. 
4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. 
5- Do you use sentences or whole posts as the unit to be annotated?
English language: ""pubic posts"" -> public posts ""a ED model must"" -> ""an ED model must"" ",2.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? 
2- ""with more than 50 words are kept to increase"" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? 
3- ""Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019)."" - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & Hürriyetoğlu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... 
1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. 
2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. 
4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. 
5- Do you use sentences or whole posts as the unit to be annotated?
English language: ""pubic posts"" -> public posts ""a ED model must"" -> ""an ED model must"" "
ARR_2022_344_review,"The paper explores zero-short cross-lingual transfer (through fine-tuning) for the task of part-of-speech tagging, as a case-study. The authors evaluate the effect of several factors such as the LDND distance between the source and target languages, matching language families, matching writing systems and the pre-training of the source and target languages. In addition, the paper proposes insights on the selection of the source language when transferring to a target language of little or no annotated data. Moreover, the authors promise the release of 65 fine-tuned language models. The paper shows that pre-training on the target language and the LDND distance between the source and target languages have the biggest impact on the cross-lingual performance. They also show that English is not necessarily the optimal source language as usually perceived by the community. ","- The paper investigates an interesting question, especially with the recent increasing interest  in multilingual NLP. While the paper focuses on part-of-speech tagging as a case-study, the conclusions might be generalizable to other cross-lingual tasks such as named-entity recognition and dependency parsing.
- The paper proposes an in-depth analysis, both quantitative and qualitative, that studies several factors that impact the cross-lingual performance. ","- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.
- There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.
- There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1. ","- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.
- It would be valuable to investigate the reason behind the high impact of the LDND distance.
- I would recommend removing singletons from Figure 4.
- Add statistical significance tests in Section 5.
- Cite Pires et al., 2019 in the 2nd paragraph in the introduction. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.
- There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.
- There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1. 
- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.
- It would be valuable to investigate the reason behind the high impact of the LDND distance.
- I would recommend removing singletons from Figure 4.
- Add statistical significance tests in Section 5.
- Cite Pires et al., 2019 in the 2nd paragraph in the introduction. "
ARR_2022_344_review,"This work provides a comprehensive empirical analysis on cross-lingual transfer for POS tagging over 100 languages (on UD v2.8). The pre-trained XLM-RoBERTa model is taken as the backbone, fine-tuned on one source language with a large enough training set (65 source languages) and then tested on target languages (105 languages). The results show that there can be various factors that influence the effectiveness of cross-lingual transfer, like pre-training inclusion, language families, etc. Further analyses and discussions over the influencing factors are also included. ","This work provides a comprehensive exploration for cross-lingual transfer for POS tagging, with abundant experimental results.
Certain influencing factors for transfer effectiveness are analyzed, with further qualitative discussions on best source language/language-pair, which may provide some useful information for future work and relevant applications. ","One of my concerns is that though it's nice to have results over such a large number of language pairs, only looking at the overall trend might not be enough to reveal certain interesting patterns. There can be too many influencing factors, some of which are explored in this work, while some others might also be important, such as training data size, quality of annotation, etc. Especially, when looking at the source languages, it seems that a large portion of them belong to Indo-European, which may make the overall results biased. Maybe it would be nice to explore a small subset of languages from different language families.
It seems that ""pretraining-inclusion"" is a major predictor, which may be not surprising. Nevertheless, there might be a hidden factor inside: if a certain language is not included in the pre-training, the vocabulary of the pre-training model might not even have items for the words in this language, which might be split into strange pieces by the subword segmenter. In addition, source-target vocabulary overlapping may be another important factor to look at. I think there should be more exploration and analysis in these aspects. ","There is a related work [1] performing similar experiments and analyses as in this work, but with more tasks and ranking features (though not using pre-trained models). It would be helpful to check it.
The prediction of POS tagger mostly depends on the lexicons themselves. It might be interesting to explore simpler models with static (cross-lingual) word embeddings, especially for the languages that are not included in the pre-training of XLM-R. [1] (Lin et al., 2019) Choosing Transfer Languages for Cross-Lingual Learning. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"One of my concerns is that though it's nice to have results over such a large number of language pairs, only looking at the overall trend might not be enough to reveal certain interesting patterns. There can be too many influencing factors, some of which are explored in this work, while some others might also be important, such as training data size, quality of annotation, etc. Especially, when looking at the source languages, it seems that a large portion of them belong to Indo-European, which may make the overall results biased. Maybe it would be nice to explore a small subset of languages from different language families.
It seems that ""pretraining-inclusion"" is a major predictor, which may be not surprising. Nevertheless, there might be a hidden factor inside: if a certain language is not included in the pre-training, the vocabulary of the pre-training model might not even have items for the words in this language, which might be split into strange pieces by the subword segmenter. In addition, source-target vocabulary overlapping may be another important factor to look at. I think there should be more exploration and analysis in these aspects. 
There is a related work [1] performing similar experiments and analyses as in this work, but with more tasks and ranking features (though not using pre-trained models). It would be helpful to check it.
The prediction of POS tagger mostly depends on the lexicons themselves. It might be interesting to explore simpler models with static (cross-lingual) word embeddings, especially for the languages that are not included in the pre-training of XLM-R. [1] (Lin et al., 2019) Choosing Transfer Languages for Cross-Lingual Learning. "
ARR_2022_130_review,"The paper proposes a new dataset for reading comprehension with multi-span answers, MultiSpanQA. 
In addition, a new model for MultiSpanQA is introduced.
The **dataset** is created using a re-annotation of questions with multiple answers from Natural Questions. 
This re-annotation process consists of two phases: - First, each instance is given one of four categories: (1) Good example, (2) Bad question, (3) Bad answer span(s), and (4) Bad QA pair.
- Second, instances are categorized to different ""semantic structures"" (for example, whether all answer spans are needed to correctly answer a question, or each span is an answer on its own). 
An expanded version of the dataset also includes single-span and unanswerable questions for better simulation of real-world scenarios, The **proposed model** is a sequence tagging module (BIO), augmented by three modules: - Semantic structure prediction - Span number prediction - Span adjustment module that combines the predicted number of spans with preliminary predicted spans.
The authors demonstrated that their proposed model improves over single-span models (generalized for multi-span QA) and over vanilla sequence tagging models. ","- The problem of multi-span QA is very important and helps deviating from the limiting single-span setting - The dataset is potentially useful for the QA community - To the best of my knowledge, the 5-way semantic annotation scheme proposed by the authors is novel (and I found it elegant and important to better understand the task) - The proposed model improves over all baselines ","1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. ","Comments & questions: - Abstract: The sentence in lines 12-17 (""After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version"") is cumbersome and can be made clearer.  - Do you perform re-annotation for the expanded dataset as well? The text now says ""..and applying the same preprocessing"" (line 355) - this point can be made more clear.
- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?
Typos: - Line 47: ""constitinga"" -> ""consisting"" - Line 216: ""classifies"" -> ""classify"" ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. 
Comments & questions: - Abstract: The sentence in lines 12-17 (""After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version"") is cumbersome and can be made clearer.  - Do you perform re-annotation for the expanded dataset as well? The text now says ""..and applying the same preprocessing"" (line 355) - this point can be made more clear.
- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?
Typos: - Line 47: ""constitinga"" -> ""consisting"" - Line 216: ""classifies"" -> ""classify"" "
ARR_2022_130_review,"This paper tackles question answering under a new setting, where models should extract multiple spans from textual context as an answer set. For this problem, the authors propose a new dataset, MultiSpanQA, consisting of 6.4k questions and 19k unanswerable questions. The context and questions of MultiSpanQA are extracted from Natural Questions and the answer sets are re-annotated by three trained annotators. 
The authors cast this problem as a sequence tagging problem and show its efficacy by comparison with a simple baseline. ","- This paper proposed a multiple-span answer setting for extractive question answering and a dataset, MultiSpanQA, consisting of 6.4k multi-span questions for this problem based on the re-annotation of the Natural Questions dataset.
- The proposed joint training with span number prediction and structure prediction, and span adjustment module can improve the QA scores on MultiSpanQA compared to a pure sequence tagging baseline. ","- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. 
  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. 
  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small. 
  * The sizes are Train: 6465, Val: 646, Test: 646. 
  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. 
  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. ","#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world ",2.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. 
  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. 
  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small. 
  * The sizes are Train: 6465, Val: 646, Test: 646. 
  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. 
  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. 
#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world "
ARR_2022_130_review,This paper focused on the task of multi-span question answering. The main contribution is to propose a dataset and its expansion for the focused task. The authors also proposed some baselines and show the basic performance of this task. ,"Multi-span question answering is a new problem in machine reading comprehension. Constructing such a dataset is interesting and beneficial for this research area.
The constructed dataset is large, and the passages and questions come from the real texts. ","I wonder whether some multi-span answer structures are really needed or not? For example, in Table 2, in Conjunction, Multi-part-disjunction (Redunant) and Shared Structure, multi answers are separated by ""and"". If we do not separate them, I think is also ok by regarding them as a single answer.  As mentioned in the abstract part, a new evaluation metric is proposed. What is it? ","1. In line 047, ""consistinga"" should be ""consisting"". 
2. In Table 5, the performance of ""DESCRIPTION"" is very low. What is that reason? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"I wonder whether some multi-span answer structures are really needed or not? For example, in Table 2, in Conjunction, Multi-part-disjunction (Redunant) and Shared Structure, multi answers are separated by ""and"". If we do not separate them, I think is also ok by regarding them as a single answer.  As mentioned in the abstract part, a new evaluation metric is proposed. What is it? 
1. In line 047, ""consistinga"" should be ""consisting"". 
2. In Table 5, the performance of ""DESCRIPTION"" is very low. What is that reason? "
ARR_2022_225_review,"The paper describes a method of conditional prompt generation, in order to fine-tune pre-trained language models to a new task while keeping most of the parameters frozen. 
The input sentences are first passed through the frozen LM, then mapped to a prompt shape which is then appended to the input of the same LM. 
The output is then used to make a task-specific prediction. 
Only the mapping and the output layers are updated during training, while the rest of the model is kept frozen. ","The underlying idea is quite interesting. Essentially it shows that it can be beneficial to pass the same input through the same frozen model twice. This is an interesting finding. 
The paper is clear and well written. It is mostly quite straightforward with its presentation, objectively showing both the positives and negatives of the proposed model without trying to oversell or overcomplicate it.
The revised version has strengthened the paper, adding some necessary experiments and analysis, therefore I have increased my score. ","When comparing the proposed method to the Compacter, it essentially has a similar number of parameters (only 10% lower) and achieves a similar performance across the datasets. It also likely has a considerably larger computation requirement, as the model needs to run twice for each sentence. 
Therefore, the paper lacks a clear reason for why the proposed method is in any way an improvement over the previously proposed Compactor.
There are quite a lot of grammatical errors, especially in the introduction. The paper should be proof-read by a more fluent English speaker.
The contributions in the introduction refer to 1.55M as the parameter count for adapter-based methods, but omits Compacter which is also adapter-based and has considerably fewer parameters.
Table 2 refers to 2 adapter-based methods: Adapter and Compacter. Table 3 refers to 1 adapter-based method: Adapter. But the text in 4.4 states that the adapter-based method in Table 3 is actually the Compacter. This is confusing and it is currently unclear whether the results in Table 3 are indeed for the Compacter model or not. ","On line 198 it is explained that the second layer has a task-specific prompt, while the first layer has an instance-specific prompt. But RoBERTa has more than 2 layers; it is currently unclear what happens in the other layers.
The introduction claims that the downside of existing prompt-tuning methods is that they are ""incompatible with the traditional LM objective"". It is unclear what that means exactly and why the LM objective would be relevant here. The LM objective is only used for pre-training these models and as far as I can see the paper does not try to evaluate this model as a LM. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"When comparing the proposed method to the Compacter, it essentially has a similar number of parameters (only 10% lower) and achieves a similar performance across the datasets. It also likely has a considerably larger computation requirement, as the model needs to run twice for each sentence. 
Therefore, the paper lacks a clear reason for why the proposed method is in any way an improvement over the previously proposed Compactor.
There are quite a lot of grammatical errors, especially in the introduction. The paper should be proof-read by a more fluent English speaker.
The contributions in the introduction refer to 1.55M as the parameter count for adapter-based methods, but omits Compacter which is also adapter-based and has considerably fewer parameters.
Table 2 refers to 2 adapter-based methods: Adapter and Compacter. Table 3 refers to 1 adapter-based method: Adapter. But the text in 4.4 states that the adapter-based method in Table 3 is actually the Compacter. This is confusing and it is currently unclear whether the results in Table 3 are indeed for the Compacter model or not. 
On line 198 it is explained that the second layer has a task-specific prompt, while the first layer has an instance-specific prompt. But RoBERTa has more than 2 layers; it is currently unclear what happens in the other layers.
The introduction claims that the downside of existing prompt-tuning methods is that they are ""incompatible with the traditional LM objective"". It is unclear what that means exactly and why the LM objective would be relevant here. The LM objective is only used for pre-training these models and as far as I can see the paper does not try to evaluate this model as a LM. "
ARR_2022_186_review,"The paper proposed an expert-guided heuristic linguistic adversarial augmentation that simulates the overlapping or ambiguous categories or label shift where a word or phrase can have different entity labels in different scenarios. They also proposed a challenging dataset consisting of 1000 high-quality examples to facilitate benchmarking the out-of-distribution performance of NER models.  With limited expert-guided adversarial augmented training data, the NER model’s performance was boosted on their challenge set and out-of-domain set. ",They proposed a novel and valuable adversarial augmentation method that simulates the common problem of NER models facing real cases. The augmentation method boosted the performance of models on the challenge and out-of-domain sets. They empirically showed that mixing the original and guided-adversarial examples can further enhance the generalization ability of NER models consistently. ,There are two main concerns from me . The first one is that how to generate the adversarial training set with code is not clearly discussed and  the detail of adversarial training set is not be provided. The second concern is that the reason of excluding 25% of the word phrases said in the last paragraph in 4.1 and the reason of using first 50 examples from the OntoNotes test set is not be fully discussed. ,"1.  For the Appendix H section, it should be reorganized which is difficult to follow. ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"There are two main concerns from me . The first one is that how to generate the adversarial training set with code is not clearly discussed and  the detail of adversarial training set is not be provided. The second concern is that the reason of excluding 25% of the word phrases said in the last paragraph in 4.1 and the reason of using first 50 examples from the OntoNotes test set is not be fully discussed. 
1.  For the Appendix H section, it should be reorganized which is difficult to follow. "
ARR_2022_186_review,"This paper introduces an expert-guided adversarial dataset for the NER problem. The paper discusses how to (manually) build such dataset in order to use it to augment the training of NER models, to make them more robust to out-of-domain shifts. 
The paper proposes a strategy based on some transition rules to create the expert-guided dataset. The experimental results seems to confirm the utility of such dataset when testing against a Challenge Set and a Out Of Domain dataset. ",- Interesting problem for the ACL community ,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient ","My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1?  - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?  - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient 
My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1?  - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?  - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence. "
ARR_2022_44_review,"This paper proposed ACTUNE, which first requires data annotation based on uncertainty and then uses active self-training to fine-tune a pre-trained language model. The setting of this work is limited labeled data and much more unlabelled data.
Contributions: 1. Design a framework that: (1) explore the high-certainty samples for data annotation; (2) generate pseudo-labels for certain samples. This framework can improve the label efficiency (requires less data annotation to achieve a similar performance) by sampling samples on both uncertainty and diversity. 
2. Explore the SSAL approaches for NLP, which have not been well-studied. 
3. Conduct exhaustive experiments to demonstrate the improvement of ACTUNE. ","1. The topic of achieving comparable performance with limited sources is interesting. Different to prompt with few-shot samples, this framework selects the most high-uncertainty data while keeping diversity can be better utilize the information from unlabeled data. 
2. This framework provided a solution for selecting the most informative training samples. 
3. This paper provided details on datasets, hyperparameters, and implementations and is easily followed. ","1. In my view, ACTUNE tries to search a small subset of the vanilla dataset with high coverage. It seems to explore a sub-optimal within ""small price"", which may have limitations that overdraft the potential with more data. 
2. The comparison between ACTUNE and baseline models seems not fair. According to Section 2.1, the number of labeled samples is 100 (initial labeled data) + 1000 (labeling budget b) = 1100. But the baseline models only have 1000 labeled data. 
3. The presentation of experimental results can be improved:  (1) The figures of experiment results are not well presented. To clearly recognize the details of Fig. 1, Fig. 3, Fig. 4, and Fig. 5, they need to be zoomed in to 300%.  (2) Insufficient color contrast in Fig. 3(c) makes distinguishing the last two lines difficult. ","Comments: 1. The definition of diversity is unclear. Eq. (5) introduces the inter-class diversity, but In Eq. (8), diversity means sampling from different clusters. 
2. The usage of the momentum-based memory bank is confusing. For example, as shown in Line 268, if $f(x; \theta^t)=[0.05,0.95]$, and $m_t$ in Eq. (10) is larger than $m_L$ (suppose to be 0.8), then the score of class 1 in $g(x;\theta^t)$ is larger than 0.8 times 0.95, which is larger than 0.5. In this case, how can predictions in the memory bank affect the current prediction? 
3. I think it would be better if the authors could provide more descriptions in Figure 6. For example, Line 549 says, ""Figure 6(e) and 6(f) show the results"". What can the reader learn if he/she is not familiar with this visualization? The same problem also appears in Line 1176.
Some typos do not affect understanding: 1. Line 122: we -> We 2. Line 188: result -> results 3. Line 475: Fig. 5(b) and Fig. 1(f) -> Fig. 1(b) and Fig. 1(f) ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. In my view, ACTUNE tries to search a small subset of the vanilla dataset with high coverage. It seems to explore a sub-optimal within ""small price"", which may have limitations that overdraft the potential with more data. 
2. The comparison between ACTUNE and baseline models seems not fair. According to Section 2.1, the number of labeled samples is 100 (initial labeled data) + 1000 (labeling budget b) = 1100. But the baseline models only have 1000 labeled data. 
3. The presentation of experimental results can be improved:  (1) The figures of experiment results are not well presented. To clearly recognize the details of Fig. 1, Fig. 3, Fig. 4, and Fig. 5, they need to be zoomed in to 300%.  (2) Insufficient color contrast in Fig. 3(c) makes distinguishing the last two lines difficult. 
Comments: 1. The definition of diversity is unclear. Eq. (5) introduces the inter-class diversity, but In Eq. (8), diversity means sampling from different clusters. 
2. The usage of the momentum-based memory bank is confusing. For example, as shown in Line 268, if $f(x; \theta^t)=[0.05,0.95]$, and $m_t$ in Eq. (10) is larger than $m_L$ (suppose to be 0.8), then the score of class 1 in $g(x;\theta^t)$ is larger than 0.8 times 0.95, which is larger than 0.5. In this case, how can predictions in the memory bank affect the current prediction? 
3. I think it would be better if the authors could provide more descriptions in Figure 6. For example, Line 549 says, ""Figure 6(e) and 6(f) show the results"". What can the reader learn if he/she is not familiar with this visualization? The same problem also appears in Line 1176.
Some typos do not affect understanding: 1. Line 122: we -> We 2. Line 188: result -> results 3. Line 475: Fig. 5(b) and Fig. 1(f) -> Fig. 1(b) and Fig. 1(f) "
ARR_2022_189_review,"The paper describes the realisation of text-to-speech models for low-resource endangered languages spoken in Canada. The work focuses on the quality of speech synthesis models when less than 5 hours or speech are available to train the model. It compares different architectures on two endangered languages (for one of them, the training set is built by the authors), showing that FastSpeech is less data-hungry than Tacotron, hence supporting its adoption in these cases. Finally, it validates that the resulting systems produce utterances of acceptable quality via human judgment. ","The paper has the merit of demonstrating that acceptable TTS can be built even with limited amount of data. The work is well motivated, but its main strength is the extensive manual evaluation carried out to assess the quality of the produced utterances, providing a human-centered validation of the system outputs. ","A few statements are debatable:  - ""the positive assessment of a few widely respected and community-engaged language speakers is practically sufficient to assess the pedagogical value [...]"": this sentence is not motivated and seems an assumption made by the authors at their convenience;  - ""for additional discussion of the social and environmental ramifications of this change [...]"": although it is an appreciable improvement, it is hard to believe that a ~30% speedup can have societal impact, as it is not changing abruptly the amount of resources needed to train/employ a system; All the experiments in the paper are limited to a single-speaker training set. It would be interesting to understand the effect of having multiple-speaker training sets. With this respect, I am not sure why ""it was deemed inappropriate to use the voices of speakers who passed away"". Although the right to be forgotten supports this approach, these speakers donated their voices to save endangered languages: as such, not exploiting these data seems to go against their own goal when they donated the utterances. However, it is a delicate topic, so the approach adopted by the paper is reasonable. Less motivated, instead, in my opinion, is the choice of removing ""utterances containing non Kanien'kéha characters"": proper names in different languages may also occur in real use cases.
Finally, the work has no mention that the produced data and code will be released. In addition, the outputs are evaluated only with manual analysis. As a result, this work will be hard to reproduce. ","Since the systems have been build for pedagogical purposes, why not trying to use it as a teacher for machines as well in self-supervised trainings? This would complete the work in my opinion.
Lines 212-215 are hard to read, I am still not 100% sure about the meaning of the sentence. At line 264 the citation style should be ""(Kumar, 2017)"". 
I have not been able to fully understand section 3.3, especially the second paragraph. Probably I am missing contextual knowledge, such as the meaning of PENÁĆ and who are the people named there. An introduction to these concepts may help readers with different background knowledge as I am.
Section 4.2.1 is titled ""Architecture choice"" but it seems more a literature review of the existing approaches than a description of the choices made in this work. Maybe the title can be improved.
Lines 491-492 cannot be understood before reading the appendix. Maybe they can be improved with a few more details.
MUSHRA may need a reference (line 504).
In the caption of figure 2 'Phone' is mentioned but it is not present in the picture.
The expression ""donor model"" sounds weird to me (552), I would advice replacing with something different (pre-trained model, ..).
Why the human evaluations for the two languages were different?
In line 956 there is a comma between the subject and the verb. 
In line 972 there is a repetition ""not need be required"". ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",1 = They would not be able to reproduce the results here no matter how hard they tried.,,"A few statements are debatable:  - ""the positive assessment of a few widely respected and community-engaged language speakers is practically sufficient to assess the pedagogical value [...]"": this sentence is not motivated and seems an assumption made by the authors at their convenience;  - ""for additional discussion of the social and environmental ramifications of this change [...]"": although it is an appreciable improvement, it is hard to believe that a ~30% speedup can have societal impact, as it is not changing abruptly the amount of resources needed to train/employ a system; All the experiments in the paper are limited to a single-speaker training set. It would be interesting to understand the effect of having multiple-speaker training sets. With this respect, I am not sure why ""it was deemed inappropriate to use the voices of speakers who passed away"". Although the right to be forgotten supports this approach, these speakers donated their voices to save endangered languages: as such, not exploiting these data seems to go against their own goal when they donated the utterances. However, it is a delicate topic, so the approach adopted by the paper is reasonable. Less motivated, instead, in my opinion, is the choice of removing ""utterances containing non Kanien'kéha characters"": proper names in different languages may also occur in real use cases.
Finally, the work has no mention that the produced data and code will be released. In addition, the outputs are evaluated only with manual analysis. As a result, this work will be hard to reproduce. 
Since the systems have been build for pedagogical purposes, why not trying to use it as a teacher for machines as well in self-supervised trainings? This would complete the work in my opinion.
Lines 212-215 are hard to read, I am still not 100% sure about the meaning of the sentence. At line 264 the citation style should be ""(Kumar, 2017)"". 
I have not been able to fully understand section 3.3, especially the second paragraph. Probably I am missing contextual knowledge, such as the meaning of PENÁĆ and who are the people named there. An introduction to these concepts may help readers with different background knowledge as I am.
Section 4.2.1 is titled ""Architecture choice"" but it seems more a literature review of the existing approaches than a description of the choices made in this work. Maybe the title can be improved.
Lines 491-492 cannot be understood before reading the appendix. Maybe they can be improved with a few more details.
MUSHRA may need a reference (line 504).
In the caption of figure 2 'Phone' is mentioned but it is not present in the picture.
The expression ""donor model"" sounds weird to me (552), I would advice replacing with something different (pre-trained model, ..).
Why the human evaluations for the two languages were different?
In line 956 there is a comma between the subject and the verb. 
In line 972 there is a repetition ""not need be required"". "
ARR_2022_293_review,"The paper presents a novel approach to understanding math problems from their textual formulations. The approach builds on those from related work, choosing syntactic representations. The key novelties are (1) an internal graph representation of the operators and (2) a novel pretraining setting. The model achieves vast improvements over prior art. ","The new model addresses several key problems of previous work and appears to contribute a very logically motivated extension, modeling the structure of the required mathematical operations. The model description is clear and the experimental setup and results are reasonably clear and allow for an easy comparison with related work. There is also an ablation study to analyze the contribution of the individual components of the model.  The paper is easy to read. ","The model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. ",I'd like to see my doubt about the pretraining cleared up. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. 
I'd like to see my doubt about the pretraining cleared up. "
ARR_2022_83_review,"The paper is targeted to NER, where named entities are very unbalanced for the task, presenting sentence-level resampling strategies to solve the imbalance problem. This paper is well-motivated, technically sound, and intuitively better than previous token-level methods. The resampling approach can be adapted to different NER model structures. Experimental results show its effectiveness on different models and different domain datasets, especially on small datasets. ","1. The proposed method is interesting, and shows improved performance on a wide range of datasets.     2. Experiments are solid, and analysises are comprehensive. ","1. The improvements are marginal, inparticular the ConLL.    2. The method can be regarded as data reweighting, where the important data point is with large weight.  Thus, the paper should compare with this line of work. ",1. It seems that the four proposed resampling methods behave differently in different settings.  The paper might be stronger if there exists one method better than the others.       2. Significance test might be needed.      3. See the weakness 2. ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The improvements are marginal, inparticular the ConLL.    2. The method can be regarded as data reweighting, where the important data point is with large weight.  Thus, the paper should compare with this line of work. 
1. It seems that the four proposed resampling methods behave differently in different settings.  The paper might be stronger if there exists one method better than the others.       2. Significance test might be needed.      3. See the weakness 2. "
ARR_2022_141_review,"The paper presents a sentence-level matching method for fine-grain document similarity of research papers (named ASPIRE). The method relies on co-citation sentences as learning signal to train a multi-vector model. The key difference to related work is the ability of determining paper’s similarity not only on a single aspect but multiple ones, whereby the set of aspects is not predefined.  The method uses a combination of a SciBERT encoder, contrastive learning, multi-instance learning, and optimal transport. In experiments with four datasets all baselines are outperformed. Besides the promising experimental results, one strength of presented method is its scalable inference that could easily enable future applications based on the method. ","- ASPIRE outperforms the strong baselines (e.g., SPECTER).
- A series of relevant ablations are presented. 
-Even though the paper presents a niche topic, the approach is important and could be also applied on other domains apart from scientific documents. 
-The multi vector approach makes aspect similarity feasible for production environments due to scalable inference (with ANN). ","- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).
- CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed. ","- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.
- Notation of BERT_\theta and BERT_\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer.  - How are negative sampled for BERT_\epsilon?
Additional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.
- Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.
Typos: -	Line 259: “cotation” -	Line 285: Missing “.” ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).
- CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed. 
- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.
- Notation of BERT_\theta and BERT_\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer.  - How are negative sampled for BERT_\epsilon?
Additional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.
- Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.
Typos: -	Line 259: “cotation” -	Line 285: Missing “.” "
ARR_2022_141_review,"This paper tries to use co-citations in full-text papers to learn an aspect matching model with fine-grained aspect representation of paper abstracts in sentence level for fine-grained retrieval, and it also achieves to compute document-level distance with a multi-vector document similarity model for abstract level retrieval. ","Advantages of this paper are as follows. 
• Co-citations used in this paper seems to be a new data source for fine-grained retrieval task. 
• The proposed ASPIRE model uses co-citing sentences as textual supervision signals to train fine-grained retrieval models. 
• This paper gets good performance across several datasets. ","Weaknesses of the paper: • The mix of different approaches and tasks leads to a confusion for readers. 
• The logic flow of the paper needs to be improved. 
• Not sure why the proposed model work for the experimental datasets, since those datasets do not have such textual supervision as co-citing sentences. 
• There are no comparison and contrast between proposed approaches and baselines. ",I strongly suggest to improve the narrative of the paper. ,2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Weaknesses of the paper: • The mix of different approaches and tasks leads to a confusion for readers. 
• The logic flow of the paper needs to be improved. 
• Not sure why the proposed model work for the experimental datasets, since those datasets do not have such textual supervision as co-citing sentences. 
• There are no comparison and contrast between proposed approaches and baselines. 
I strongly suggest to improve the narrative of the paper. "
ARR_2022_335_review,"The authors aim to improve interpretability for structure and style control in knowledge-grounded conversational models. They propose to use two sequential latent variables for structure and style respectively.  1) m - binary indicator for segment boundaries within a sentence 2) z - style controller attribute to switch between content, knowledge, and style decoders. They use a variational framework for training using an evidence lower bound of the likelihood. Overall their encoder-decoder model outperforms the baselines in automatic and human metrics on two knowledge grounded dialog datasets WizardsOfWikipedia and CMU_DoG. Their models is also more robust and generalizable as it consistently outperforms the baselines with 10% or lower amount of in-domain data. While adding adapters to their decoders they can adjust the style (sentiment) of their responses while maintaining decent performance in automatic evaluation. Their metrics pklg and lklg suggest that their models can easily adapt the latent variables' distribution for different datasets. ","- Interpretable model which predicts segment boundaries and is able to switch style decoders based on the context. Potentially useful for many applications.
- Robust and generalizable model which can easily adapt to new styles with limited training data. ","- The writing structure and flow can be improved. Many of the crucial details regarding automatic labeling, baselines, human evaluation results etc. are moved to appendix which disrupts the reading flow. ","- Their model shows improvement in low resource setting. But, it will be interesting to see what are the overall gains compared to the baselines with 100% of the in-domain data.
- missing section in line 492 ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- The writing structure and flow can be improved. Many of the crucial details regarding automatic labeling, baselines, human evaluation results etc. are moved to appendix which disrupts the reading flow. 
- Their model shows improvement in low resource setting. But, it will be interesting to see what are the overall gains compared to the baselines with 100% of the in-domain data.
- missing section in line 492 "
ARR_2022_335_review,"This paper explores the knowledge-grounded conversation generation task. Two latent variables are employed for the controlling of types and boundaries for segments during generation, after an auxiliary task to classify segments into knowledge-relevant or knowledge-irrelevant. Such a strategy is fused into a pretrained encoder-decoder architecture, and realizes superior performance than strong baselines on two benchmark datasets. 
Although this paper is well-motivated, I do not see many significant revisions for the previous comments. ","1. The motivation of this paper is solid, I believe the segmentation based on the relation of text pieces between knowledge is reasonable to solve knowledge-grounded dialogue generation tasks.
2. The proposed model is basically complete and shows its superiority to strong baselines. It is based on a pre-trained BART model, and the proposed Module Indicator/ Boundary Indicator can help the base model to better combine information from both the context and knowledge in generated responses. ","1. The writing still needs improvement. There are only some minor revisions compared to the last version, where they fix some typos, moving part of human evaluation results into the main part, and change few expressions to make them more clear. However, some major problems still exist. 
1) The descriptions of the model are difficult for readers to follow, including complex notations and some unclear definitions (e.g.,  the different styles mentioned). 
2) Although it adds more details about human evaluation, there is still no clear definition of metrics pklg and lklg. And I also have concerns about their justification.
2. An ablation study is still missing. Without such analyses, readers will have no idea about how each component contributes to the final performance. E.g., how the style adapter affects the generated responses, or how it performs if one kind of latent is removed. ","Please check the weakness section.
1. A case study with only one sample is insufficient for a competent paper in the dialogue area. Consider adding more samples and putting more highlights on the samples to show your superiority.
2. Still missing some references that utilize pretrained model in an encoder-decoder architecture for auxiliary-information-grounded dialogue generation.  Typo: L492: Sec ?? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The writing still needs improvement. There are only some minor revisions compared to the last version, where they fix some typos, moving part of human evaluation results into the main part, and change few expressions to make them more clear. However, some major problems still exist. 
1) The descriptions of the model are difficult for readers to follow, including complex notations and some unclear definitions (e.g.,  the different styles mentioned). 
2) Although it adds more details about human evaluation, there is still no clear definition of metrics pklg and lklg. And I also have concerns about their justification.
2. An ablation study is still missing. Without such analyses, readers will have no idea about how each component contributes to the final performance. E.g., how the style adapter affects the generated responses, or how it performs if one kind of latent is removed. 
Please check the weakness section.
1. A case study with only one sample is insufficient for a competent paper in the dialogue area. Consider adding more samples and putting more highlights on the samples to show your superiority.
2. Still missing some references that utilize pretrained model in an encoder-decoder architecture for auxiliary-information-grounded dialogue generation.  Typo: L492: Sec ?? "
ARR_2022_254_review,"Automated evaluation of coherence in a conversation is a longstanding challenge in conversational models. Recent models address the task using contrastive learning where the goal is to learn a classifier to distinguish between a coherent conversation and an incoherent one. However, the problem of existing models lies in heuristic manipulations that result in unrealistic and too easy negative examples.
The paper presents a method, namely DEAM, to overcome the problem of constructing more realistic negative examples. To this end, the proposed method (a) converts an utterance to an AMR graph, a semantic representation that abstracts away from syntax, (b) injects inconsistency to AMR, (c) translates the manipulated AMR back to text. The effectiveness of DEAM rests on determining ways to manipulate AMRs—i.e., step (b). This is done via four logical flaws that dialogue models mainly suffer from: contradiction, coreference inconsistency, irrelevancy, and engagement decline.
DEAM is compared against other seemingly trivial baselines that shuffle utterances, shuffle one speaker’s utterances, or insert random utterances. Under the same setting, DEAM outperforms all existing baselines in terms of correlation with human judgment. Also, via an ablation study, decreased engagement and irrelevancy are shown to be the most impactful issues. ","- Evaluating conversational models automatically is notoriously cumbersome. The proposed method in the paper aims at gauging coherence in conversations, which is important in building natural-sounding dialogue systems.
- The proposed perturbations in AMR seem to be easy for the community to use for their own datasets.  - The analysis of the proposed paper is convincing; especially the correlation with the human judgment that is shown to be consistently the highest among the three baselines.
Overall, I think the paper is a nice contribution and can be of interest to the community. ","- The four logical flaws in dialogue systems are derived from “the observation of interactions between advanced dialogue systems and humans” (L272-273). More information about these observations is helpful. For instance, in which dialogue models these observations were made? How often do they occur? Are there other flaws too? The last question can be important for future research.
- The paper can also benefit from an error analysis. What are the circumstances that DEAM fails? To what extent does failure in AMR parsing cause an issue in DEAM?
- The AMR abstraction from syntax may attribute to inadvertent inconsistencies. In Figure 4 top-right, I can see that the verb tense is changed from past to present. In this particular example, this may not be much of an issue, but for some cases, this can lead to unintended inconsistency. For example, if the conversation is about a deceased person, past tense is presumably used frequently and only a tense change to present can make the conversation flawed. ","- L85: FED dataset -> the FED dataset - L307: AMR-to-Text model -> the AMR-to-Text model - L346: “apparently” weakens the point. I suggest removing it - L350: The sentence about question-type utterances sounds detached from the paragraph.
- L407: PersonaChat dataset -> the PersonaChat dataset - L462: \citet should be used - In Figure 4 bottom-left, the first sentence should remain the same (i.e., it should start with “He was” rather than “They are”) ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The four logical flaws in dialogue systems are derived from “the observation of interactions between advanced dialogue systems and humans” (L272-273). More information about these observations is helpful. For instance, in which dialogue models these observations were made? How often do they occur? Are there other flaws too? The last question can be important for future research.
- The paper can also benefit from an error analysis. What are the circumstances that DEAM fails? To what extent does failure in AMR parsing cause an issue in DEAM?
- The AMR abstraction from syntax may attribute to inadvertent inconsistencies. In Figure 4 top-right, I can see that the verb tense is changed from past to present. In this particular example, this may not be much of an issue, but for some cases, this can lead to unintended inconsistency. For example, if the conversation is about a deceased person, past tense is presumably used frequently and only a tense change to present can make the conversation flawed. 
- L85: FED dataset -> the FED dataset - L307: AMR-to-Text model -> the AMR-to-Text model - L346: “apparently” weakens the point. I suggest removing it - L350: The sentence about question-type utterances sounds detached from the paragraph.
- L407: PersonaChat dataset -> the PersonaChat dataset - L462: \citet should be used - In Figure 4 bottom-left, the first sentence should remain the same (i.e., it should start with “He was” rather than “They are”) "
ARR_2022_35_review,"Authors first convert text and audio data to phonetic representation. Then they pre-train the language model based on phonetic data and then fine-tune it in the Swahili NER task. Various combinations for pre-learning are considered: only text, only audio, text + audio, text in a more resourced related (Kinyarwanda) language, audio in a more resourced related language.  It is shown that pre-training in a more resourced language gives a slight improvement compared to the model without pre-training in simple NER tasks, where you just need to determine whether or not there are named entities in the text, or to determine the presence and number of named entities in the text. On a more complex NER task, in which you need to determine the type and location of named entities, pre-training gives a significant performance gain in comparison with a model that was not pre-trained. ","- It's a sensible idea to use the maximum available resources, and not be limited to text only.
- The paper is well written. ","- There is no compelling advantage of adding audio data to pre-training: text-only pre-learning (SNER+ST1) is better than or the same as text-only pre-learning (SNER+SAT).
- The idea was assessed only on one task (NER), albeit with three varieties. A more extensive assessment and deeper analysis is expected from a long article. ","- Tables 1&2: please add standard deviations since you anyway have already run each model at least three times - Table 1, caption: consider removing ""calculated with the scikit-learn library. ( Pedregosa et al., 2011)"", because an average can be calculated anywhere (say in Excel), and you don't need the scikit-learn for that.
- Figure 4: if the y-axis is indeed in percents, then the ticks should be 0, 1, 2, etc. Otherwise, one can think that the improvements are in hundredths of a percent (very small).
- line 499: corpus (David, 2020) ~~corpus~~ ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No concerns ,"- There is no compelling advantage of adding audio data to pre-training: text-only pre-learning (SNER+ST1) is better than or the same as text-only pre-learning (SNER+SAT).
- The idea was assessed only on one task (NER), albeit with three varieties. A more extensive assessment and deeper analysis is expected from a long article. 
- Tables 1&2: please add standard deviations since you anyway have already run each model at least three times - Table 1, caption: consider removing ""calculated with the scikit-learn library. ( Pedregosa et al., 2011)"", because an average can be calculated anywhere (say in Excel), and you don't need the scikit-learn for that.
- Figure 4: if the y-axis is indeed in percents, then the ticks should be 0, 1, 2, etc. Otherwise, one can think that the improvements are in hundredths of a percent (very small).
- line 499: corpus (David, 2020) ~~corpus~~ "
ARR_2022_358_review,"The authors propose a framework for taxonomy expansion (TE). Particularly, they explore the training of two common operations in TE (attach and merge) together. Experiments are performed on three WordNet taxonomies and the results show improvements for attaching and competitive results for merging. ","- The task is quite interesting and it is important for updating resources in different languages.
- Unlike previous approaches, the proposed model is able to give answers to two operations. In addition, the performance in both tasks are improved or got encouraging results.
- Diverse experiments conducted along the work. ","- Some definitions and statements are not clear or well justified.
- Lack of clarity in the definition of the input/outputs for each subtask ","063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
092 Missing reference for wordnet definition 112-114 ""The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1."" - > It doesn't seem clear.
115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention ""mango"" and ""nutrient"" and do not seem to be concepts according to your definition.
188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?
Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"- Some definitions and statements are not clear or well justified.
- Lack of clarity in the definition of the input/outputs for each subtask 
063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
092 Missing reference for wordnet definition 112-114 ""The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1."" - > It doesn't seem clear.
115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention ""mango"" and ""nutrient"" and do not seem to be concepts according to your definition.
188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?
Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity. "
ARR_2022_236_review,"This paper works on multimodal misinformation detection, particularly in the context of ""miscaptioned"" images on Twitter. The authors collect a large multimodal data of 884k tweets on the topics of Climate Change, COVID-19, and Military Vehicles. Next, a misinformation detection method, based on CLIP embeddings is proposed. The effectiveness of this approach is demonstrated on (a) a simulated dataset mismatched image captions and (b) real-world data of misinformation. ","- The paper is very well written and structured.
- The collected dataset could be useful for multimodal research.
- The authors demonstrate strong gains over the baseline approach of detection using zero-shot CLIP.
- Experiments are mostly rigorous (for a short paper). ","- My main criticism is that the ""mismatched"" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.   - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise) ",- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant? ,"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- My main criticism is that the ""mismatched"" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.   - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise) 
- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant? "
ARR_2022_236_review,"The paper addresses a sub-topic in the realm of mis/disinformation, namely images posted on social media with a misleading caption. With a focus on the topics of climate change, covid-19 and military vehicles, a dataset is collected from twitter using an iterative process to come to the best set of query terms, and selecting posts that contain at least one image. Half of the data is then articifially made out-of-context, by both randomly connecting captions to images, and doing so in a more informed way by selecting on semantic similarity. Two existing datasets consisting of images and deceptive captions are then used for evaluation, and the zero-shot CLIP model is used as baseline on the task. The new dataset is used to finetune this baseline model, comparing different multi-modal fusion techniques. This approach was found to outperform the baseline model, and several insights were offered by inspecting the influence of text clustering and image OCR'ing. ",- Clear and proper data collection procedure - The use of multiple evaluation sets - The insight that multiplying the image- and textual CLIP embeddings is beneficial - Analysis of the percentage of hard and random negatives and their impact - Insights into the value of Optical Character Recognition on performance - Insights into the value of tweet text clustering ,"- Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?
- Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning ","- The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.
- Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?
- Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning 
- The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.
- Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated "
ARR_2022_236_review,"The paper deals with misinformation detection in Twitter, using multimodal data (images and text). 
Authors focus on three topics: COVID, climate change, and military vehicles. ","Authors perform experiements on multiple settings, using several quality metrics. 
The results show a superiority of the proposed method. 
Partial ablation study is also reported. ","The paper is not very clearly writtem. Too much information is moved to Appendix. 
Some important details of data collection and data generation are omitted. 
The main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. ","I did not notice many typos. However, the paper organization can be improved. 
For example, data generation and data collection can be described in the same section. 
Also, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. 
Below are some detailed comments/questions:  1. What ""in-the-wild"" term means? 
2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). 
3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. 
4. Conclusions from Table 4 are missing. 
5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. ","2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The paper is not very clearly writtem. Too much information is moved to Appendix. 
Some important details of data collection and data generation are omitted. 
The main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. 
I did not notice many typos. However, the paper organization can be improved. 
For example, data generation and data collection can be described in the same section. 
Also, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. 
Below are some detailed comments/questions:  1. What ""in-the-wild"" term means? 
2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). 
3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. 
4. Conclusions from Table 4 are missing. 
5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. "
ARR_2022_123_review,"This paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. 
However, there are some issues with the paper: 1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. 
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. 
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. ","This paper investigates the ability of the raw sequence-to-sequence model in document-level translation. The perspective is interesting. It proposes a multi-resolutional data processing step which iteratively split document corpus into k (1, 2, 4, 8,...) parts and feeds them to the sentence-to-sentence translation model. They also provide a new document-level dataset focusing on 3 phenomena: tense consistency, conjunction presence, pronoun translation. Empirically, they show that the multi-resolutional data processing can effectively address the learning problem with long sentences, and obtain improvements over some baselines. ","1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. 
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. 
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. ","1) It's better to adopt experiment settings consistent with previous work. 
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved. 
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. 
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. 
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. 
1) It's better to adopt experiment settings consistent with previous work. 
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved. 
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021. "
ARR_2022_239_review,"*(minor edits from previous review XYZ)* Text style transfer is the task of rewriting a sentence into a target style while approximately preserving its content. Modern style transfer research operates in an ""unsupervised"" setting, where no parallel training data (pairs of sentences differing in style) is available, but assume access to a large unpaired corpus in each style.
This paper argues that a large unpaired corpus to train style transfer systems might be hard to obtain in practice, especially in certain domains. To tackle this issue, the authors present a new meta-learning approach (DAML) which trains a style transfer system that can quickly adapt to unseen domains during inference (with a few unpaired examples). The authors build their style transfer system using a discriminative learning objective (via a style classifier) while fine-tuning T5, which they call ST5. The authors approach DAML-ST5 outperforms several baselines on sentiment transfer and Shakespeare author imitiation, and ablation studies confirm the design decisions. ","*(identical to my previous review GAJd, see ""Weaknesses"" for my response to the revised manuscript)* 1. This paper tackles a practically relevant problem. While current style transfer research does not leverage supervised data, it requires a large amount of unpaired data which may not be practical to obtain in low-resource languages or domains. Hence, building style transfer systems which can quickly adapt in low-resource settings is important, since it eliminates the expensive requirement of hand-curating unpaired datasets for each low-resource domain / language.
2. The paper presents an interesting method based on model-agnostic meta learning [1] (with modifications to make it suitable for domain adaptation) to learn a good initialization which works well across domains. During inference, the model can quickly adapt to a new domain, with decent performance with just 1% of the target domain data. Experimental results confirm the proposed approach outperforms several strong baselines. The paper also has ablation studies to justify the various design decisions used in the approach.
[1] - https://arxiv.org/abs/1703.03400 ","The authors presented an excellent response and addressed all the concerns in my previous review GAJd in their revised manuscript. In particular, the authors added experiments on the new Shakespeare dataset, used extra automatic metrics to evaluate their approach and found consistent trends, clarified some questions I had about the modeling, added comparisons to recent few-shot style transfer approaches.
I have increased my score to 4. It would be nice to move some of the new results into the main body of the paper with the extra 9th page, especially the experiments on the Shakespeare dataset. ","Several references are missing their venues / journals / arXiv identifiers, you can get the correct bib entries for papers from https://aclanthology.org, Google Scholar or arXiv. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The authors presented an excellent response and addressed all the concerns in my previous review GAJd in their revised manuscript. In particular, the authors added experiments on the new Shakespeare dataset, used extra automatic metrics to evaluate their approach and found consistent trends, clarified some questions I had about the modeling, added comparisons to recent few-shot style transfer approaches.
I have increased my score to 4. It would be nice to move some of the new results into the main body of the paper with the extra 9th page, especially the experiments on the Shakespeare dataset. 
Several references are missing their venues / journals / arXiv identifiers, you can get the correct bib entries for papers from https://aclanthology.org, Google Scholar or arXiv. "
ARR_2022_357_review,"This work tackles the automatic identification of human values in arguments. 
Human values can provide the underlying motives behind arguments and can explain why someone takes a position or what goals the person sees as worth striving for. 
Opposing values or different ways of prioritizing these values lead to conflicts and disagreements. If the underlying values can be made explicit, valuable insights can be gained in the analysis of different discourses. In other contexts arguments can be automatically generated for a target audience based on appropriate matching values and thus be more convincing. This motivates the authors to tackle the task and initially address the following problems: they systematically evaluate existing taxonomies from social and psychological science. Based on this they develop a framework that integrates the values from the various theories and that provides different levels of granularity, thus making an automatic classification of these values more feasible. The framework is used to create a resource with arguments from different geographical regions annotated with human values. The authors conduct some classification experiments with transformer-based models and discuss the results in an analysis. ","The authors provide the following contributions: - a systematic literature review of existing theories about the classification of human values in argumentation - a theoretically-informed framework to annotate human values in arguments. Different levels of granularity provide flexibility for the annotation of new data and is useful to do automatic classification of human values, especially when data is scarce and the classes are imbalanced - a resource annotated with their new framework that is made accessible for the research community. The annotation process is described in detail. The resource contains arguments from different cultural regions.  - experiments for the automatic classification of human values using the new data set. These can serve as a baseline for future research and they provide some initial insights about the difficulty of the task and the robustness ","Section 3 could benefit from a more clear structure and some insights from the background section that lead to the design of the proposed taxonomy. It took me some time to understand that the schemata introduced in section 2.1 are also considered when designing the new taxonomy (Table 1), making this more explicit (already in section 2.1) would help to make this more clear. To me it is not clear how the beginning of section 3 (claim 1: human values are implicit, claim 2: human values make arguments more persuasive, claim 3: the implicit connection has to me made explicit) provide justifications for the new design. To me the motivation for the proposed taxonomy is a) that it combines different theories therefore is more complete, missing values have been added and b) that is has different levels of granularity which is useful for both, quantitative analyses and machine learning. Although this was hinted at in the introduction, I am missing the clear motivation / advantages for the new taxonomy in this section. 
On top of that there is a missing reference that is relevant, as it tackles the automatic classification of human values using the Schwartz taxonomy (though in social media and using feature-based approaches, no Deep Learning): A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content (https://aclanthology.org/E17-1069.pdf) ","This paper has some weaknesses in section 2 / 3 but these can be resolved by some rewording / making things more clear / adding the missing reference. The theoretical review and development of the new taxonomy, the resource and the experiments are a strong contribution and will be valuable for the research community.  minor remarks: 212-218: unclear, what is meant by 'task'? The 'task' for the ML model? 
247-255: sounds more like future work or has to be rephrased to make clear that this is a motivation for using cross-cultural data 289: how was it translated? manually? 
Why only 50 arguments from Africa but 100 from India and China? 
typos: 155 (present*s*), 268 conclusion(s) ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Section 3 could benefit from a more clear structure and some insights from the background section that lead to the design of the proposed taxonomy. It took me some time to understand that the schemata introduced in section 2.1 are also considered when designing the new taxonomy (Table 1), making this more explicit (already in section 2.1) would help to make this more clear. To me it is not clear how the beginning of section 3 (claim 1: human values are implicit, claim 2: human values make arguments more persuasive, claim 3: the implicit connection has to me made explicit) provide justifications for the new design. To me the motivation for the proposed taxonomy is a) that it combines different theories therefore is more complete, missing values have been added and b) that is has different levels of granularity which is useful for both, quantitative analyses and machine learning. Although this was hinted at in the introduction, I am missing the clear motivation / advantages for the new taxonomy in this section. 
On top of that there is a missing reference that is relevant, as it tackles the automatic classification of human values using the Schwartz taxonomy (though in social media and using feature-based approaches, no Deep Learning): A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content (https://aclanthology.org/E17-1069.pdf) 
This paper has some weaknesses in section 2 / 3 but these can be resolved by some rewording / making things more clear / adding the missing reference. The theoretical review and development of the new taxonomy, the resource and the experiments are a strong contribution and will be valuable for the research community.  minor remarks: 212-218: unclear, what is meant by 'task'? The 'task' for the ML model? 
247-255: sounds more like future work or has to be rephrased to make clear that this is a motivation for using cross-cultural data 289: how was it translated? manually? 
Why only 50 arguments from Africa but 100 from India and China? 
typos: 155 (present*s*), 268 conclusion(s) "
ARR_2022_357_review,"This paper addresses the issue of automatically mining values behind arguments. Relying on a taxonomy of values based on literature and consistent with those used in social sciences, and collecting data across 4 cultural domain, the paper presents a robust study on the automatic detection of values with very encouraging results. ","The paper is exquisitely well-written and presented. The experimental design is sound, and the data collection follows good practice, leading to very encouraging results.  The background literature review is excellent.  Overall, I think this is an excellent paper which I would like to see at ACL. Addressing the weaknesses below would strengthen the paper even further. ","There is a slight imbalance between the background and the description and discussion of results in the paper. Given the very strong set up, the final part of the paper seems somewhat underwhelming. Further discussion on the results and their significance, with more space allocated to what they mean for further work and applications would have made for an even stronger paper.
At alpha=.49 inter-annotator agreement is low, which is not unexpected given the complexity of the task. However, further discussion on how this could be addressed (e.g. with better annotation manuals or training) is missing. ","Line 458: equally effected -> equally affected Overall, no other issues with the writing, which is excellent. I would advise the authors to dedicate a bit more space on discussing the results and their significance for further work and applications, perhaps shortening the background sections slighlty. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",The authors ensured explicitly that ethical concerns were addressed in the design of the experiment (e.g. in the payment structure for crowdsourced work) and in the domain. ,"There is a slight imbalance between the background and the description and discussion of results in the paper. Given the very strong set up, the final part of the paper seems somewhat underwhelming. Further discussion on the results and their significance, with more space allocated to what they mean for further work and applications would have made for an even stronger paper.
At alpha=.49 inter-annotator agreement is low, which is not unexpected given the complexity of the task. However, further discussion on how this could be addressed (e.g. with better annotation manuals or training) is missing. 
Line 458: equally effected -> equally affected Overall, no other issues with the writing, which is excellent. I would advise the authors to dedicate a bit more space on discussing the results and their significance for further work and applications, perhaps shortening the background sections slighlty. "
ARR_2022_357_review,"The paper introduces a dataset targeted at computational argumentation, more specifically at implicit assumptions which are often involved in premises supporting arguments. Such implicit assumptions, labelled as “human values” in the paper and in the social sciences, are shared beliefs regarding the positive assessment of some dimension, e.g., “having a comfortable life is good”, “being polite is good”, etc. The dataset released with the paper contains 5270 English arguments annotated with a 54-level taxonomy of human values, which is  organized hierarchically and covers four geographical cultures (USA, China, India, Africa). The release is complemented by modeling experiments which set the baseline for future work on the data.
The main strengths of the paper are in 1) the novelty of the dataset and the fine-grained taxonomy it adopts, 2) the interdisciplinary nature of the adopted taxonomy, and 3) the size and coverage (for the U.S. part).  The main weaknesses are 1) the cursory description of the annotation schema, 2) the need for a more detailed discussion of the modeling results (what works, what doesn’t), 3) the lack of ground for cross-cultural generalization which is claimed as a contribution.  For these reasons, I consider the paper good but, acceptance-wise, I am not fully convinced by in its current version. I recommend the authors address the points summarized above and discussed below, and I look forward to read the next version of the paper, either as a resubmission or in its final version if it is accepted. ","- As a strong supporter of the introduction of interdisciplinary work in NLP, I really liked the theoretical motivation behind the paper. Moreover, when it comes to computational argumentation, I believe such implicit assumptions like the ones at the core of the paper are of enormous relevance and still a bottleneck in the NLP/Argument Mining handling of real-world data.
- I also strongly appreciate the fine-grainedness of the adopted taxonomy and the annotation effort it required.  - The US subset, which covers the vast majority of the dataset, is indeed very large and commonly employed in Argument Mining, allowing for robust generalization. ","- Precisely because the annotation is the major contribution of the dataset, I would have expected a more structured discussion of its different layers. Figure 1, in the Introduction, is extremely complex and not discussed at all. Similarly, Table 1 is incredibly rich and the text of section 3 basically focusses on the comparison between the adopted annotation schema and previous work/theories that the NLP reader is not supposed to be familiar with.  - The modeling results, albeit meant as baseline experiments to accompany the dataset, should have been discussed more thoroughly. What is the effect of frequency of a specific annotated human value on the performance in modeling it? Besides excluding the extremely rare human value from the test set, was the sampling of training/development/test also conducted taking care of the general distribution of the categories?  - I believe that the cross-cultural claim put forward by the authors should be smoothened for two reasons: 1) not only the non US subsets are very small, but the sample of users who writes on English forums cannot be considered representative of the culture of the respective geographical locations; 2) even bypassing 1, the authors do not provide any discussion of the distribution of values in the 4 involved geographic locations — which is what I was very much looking forward to as a reader, since the onset of the paper. ","- Write-up: The contextualization provided in 2.2 would benefit from examples of the comparison terms (framing in the Entman sense; framing in the Argument Mining sense) and by more explicit connections between those argumentation schemes and the ones adopted in the datasets. E.g., care in moral foundation theory, Benevolence: caring in the dataset).  - Modeling: BERT is multi-label, SVM multi-class. Can the authors elaborate on how this (paired with the skewed frequency coming with the high-granular dataset) affects the modeling?  - Conceptually: Human values are a phenomenon that has its own standing independently from argumentation. They are expected to vary across social groups, geographical locations, etc. The authors should probably acknowledge this explicitly, and I think they could make their motivation/contribution even stronger by reflecting on the gain of both sides of this interdisciplinary study. Human values can of huge interest for computational argumentation (main motivation of the paper). But what is the specific perspective/contribution that the focus on argumentative text can give on the study of human values? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Precisely because the annotation is the major contribution of the dataset, I would have expected a more structured discussion of its different layers. Figure 1, in the Introduction, is extremely complex and not discussed at all. Similarly, Table 1 is incredibly rich and the text of section 3 basically focusses on the comparison between the adopted annotation schema and previous work/theories that the NLP reader is not supposed to be familiar with.  - The modeling results, albeit meant as baseline experiments to accompany the dataset, should have been discussed more thoroughly. What is the effect of frequency of a specific annotated human value on the performance in modeling it? Besides excluding the extremely rare human value from the test set, was the sampling of training/development/test also conducted taking care of the general distribution of the categories?  - I believe that the cross-cultural claim put forward by the authors should be smoothened for two reasons: 1) not only the non US subsets are very small, but the sample of users who writes on English forums cannot be considered representative of the culture of the respective geographical locations; 2) even bypassing 1, the authors do not provide any discussion of the distribution of values in the 4 involved geographic locations — which is what I was very much looking forward to as a reader, since the onset of the paper. 
- Write-up: The contextualization provided in 2.2 would benefit from examples of the comparison terms (framing in the Entman sense; framing in the Argument Mining sense) and by more explicit connections between those argumentation schemes and the ones adopted in the datasets. E.g., care in moral foundation theory, Benevolence: caring in the dataset).  - Modeling: BERT is multi-label, SVM multi-class. Can the authors elaborate on how this (paired with the skewed frequency coming with the high-granular dataset) affects the modeling?  - Conceptually: Human values are a phenomenon that has its own standing independently from argumentation. They are expected to vary across social groups, geographical locations, etc. The authors should probably acknowledge this explicitly, and I think they could make their motivation/contribution even stronger by reflecting on the gain of both sides of this interdisciplinary study. Human values can of huge interest for computational argumentation (main motivation of the paper). But what is the specific perspective/contribution that the focus on argumentative text can give on the study of human values? "
ARR_2022_357_review,"The paper presents a novel dataset of arguments annotated with human values from a taxonomy of 54 values. The dataset contains 5270 arguments annotated by 3 crowdworkers each. More precisely, the dataset contains arguments from different cultures, namely 50 arguments for Africa, 100 for China, 100 for India and 5020 for USA (from an existing dataset). In addition, the authors propose some baselines to automatically establish the link between the considered human values and the natural language arguments. ","- the paper tackle an interesting issue for the ACL community, and in particular, for the Argument Mining community. Human values may help to improve the research in the area about assessing the quality of natural language arguments.
- the paper is the first that aims to identify values behind arguments.
- an annotated dataset is build for this task, which has an indubitable value for the whole community.
- the paper is well written and the motivation and methodology is clearly detailed.
- results are sound. ","- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. ","- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion "
ARR_2022_58_review,"This paper addresses and focuses on disambiguation resolution on task-oriented dialog systems. The authors augment the seminal databases (MultiWOZ and SGD) with synthetic disambiguation turns, and use it as a new dataset to make their dialog system understand the user's answer and do follow up clarification questions. ","Overall this paper is complete and very well written. The ablation studies are convincing, and the results are promising. Furthermore the analysis presented in the paper is concise and complete as well. This paper has a potential to inspire other dialog researchers to work on enhancing universal dialog skills. ","It might be difficult for readers to understand the explanation in Sec 3.1. and 3.2., rephrasing the paragraph along with its pseudo-code can add clarity. ","It would be really helpful for the dialog community if authors open their experiment script, especially their script to do automatic augmentation (Sec. 3.2.) ",3.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"It might be difficult for readers to understand the explanation in Sec 3.1. and 3.2., rephrasing the paragraph along with its pseudo-code can add clarity. 
It would be really helpful for the dialog community if authors open their experiment script, especially their script to do automatic augmentation (Sec. 3.2.) "
ARR_2022_280_review,"This paper provides a general framework for cross lingual UMLS entity linking which can be adapted to other source languages with some prerequisites specifically four steps - unsupervised learning of a language specific UMLS dictionary, generation of candidate mentions, high level recall matching of candidate mentions to UMLs concepts  and leveraging pre trained neural (Transformer) language models for contextual relevance to determine which candidates are correct.  The method exploits a small annotated  corpus in the source language. 
The approach is evaluated on standard in domain datasets for both English and Hebrew and the results improve on the state of the art. 
In summary the work involves a novel contribution to the field of cross lingual entity linking within the biomedical domain. ","- The paper is in general well written - makes a nice contribution to state of the art of cross lingual NEL - any NEL related contribution targeted to language other than English is of value.
- appears to have good command of the state of the art. 
-methodology clearly explained and good evaluation.
- code appears to be available - not sure about documentation for reproducibility ","- much of the approach  is based on combining existing published work with respect to the dictionary construction and candidate generation so it makes it difficult to assess how much contribution has been made by the authors.
-  Some issues on readability - There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - no error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened. ","-  Some issues on readability -  There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - consider adding into future work an error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened.
- line 207 - equation - should one of the L* have the t subscript?  Or am I wrong? ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- much of the approach  is based on combining existing published work with respect to the dictionary construction and candidate generation so it makes it difficult to assess how much contribution has been made by the authors.
-  Some issues on readability - There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - no error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened. 
-  Some issues on readability -  There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - consider adding into future work an error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened.
- line 207 - equation - should one of the L* have the t subscript?  Or am I wrong? "
ARR_2022_195_review,"The paper extensively explores the ways to extend GECTOR model by using larger transformers of different kind, model distillation and ensembling. It allows to achieve new SOTA on BEA2019 test. The main results are: + new SOTA achieved by using an ensemble of large pretrained Transformer models + a single model trained on data annotated by a large ensemble model outperforms standard GECTOR training + Edit majority ensembling works better than probability ensembling ",+ new SOTA on BEA 2019 test + a better ensembling method proposed + a nice study on knowledge distillation + the paper is clearly written ,"- Judging the paper for a workshop, I find practically no weaknesses. However, I have minor concerns whether the contribution is significant enough for people outside GEC community - seems to be an extension of earlier published work https://s3.eu-central-1.amazonaws.com/ucu.edu.ua/wp-content/uploads/sites/8/2021/04/Improving-Sequence-Tagging-Approach-for-Grammatical-Error-Correction-Task-.pdf  The authors should properly highlight the contribution made on top of it. ",- Please explain better how knowledge distillation is performed. Do you mean that you find erroneous sentences in unannotated data and then correct them by a strong ensemble to obtain the target sentence? ,3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Judging the paper for a workshop, I find practically no weaknesses. However, I have minor concerns whether the contribution is significant enough for people outside GEC community - seems to be an extension of earlier published work https://s3.eu-central-1.amazonaws.com/ucu.edu.ua/wp-content/uploads/sites/8/2021/04/Improving-Sequence-Tagging-Approach-for-Grammatical-Error-Correction-Task-.pdf  The authors should properly highlight the contribution made on top of it. 
- Please explain better how knowledge distillation is performed. Do you mean that you find erroneous sentences in unannotated data and then correct them by a strong ensemble to obtain the target sentence? "
ARR_2022_195_review,"This paper looks at ensembles of Large Transformers tagger models for Grammatical Error Correction. The authors discovered an ensemble that achieves a strong new SOTA of 76.05 on BEA-2019 test without pretraining on synthetic data. The authors additionally report model distillation results on synthetic data, where a single model achieves near SOTA with 73.21 on the BEA-2019 test. The single model is additionally not SOTA only in comparison to a much larger T5 model.
The models utilized in this paper are based on the well known GECToR system. Performance improvements are obtained by: (1) upgrading model size to Large (GECToR used Base), (2) changing tokenizer (GECToR changed tokenizer from the one used during pretraining), (3) filtering of edit free sentences, (4) increasing vocabulary size (the vocabulary of possible edits), (5) ensembling by majority vote on edit spans instead of ensembling on average output tag probabilities, (6) distillation. Two new synthetic datasets are constructed for distillation: one from the One Billion Word Benchmark and one from the The Blog Authorship Corpus.
Training is performed in three stages: (1) optionally train on synthetic data, (2) finetune on the edits of Lang-8, NUCLE, FCE, W&I in this order, (3) finetune on W&I only. At inference the paper uses a tweak from GECToR, adjusting the keep probability by a tuned amount. Ensembling is performed over RoBERTa, DeBERTa and XLNet with different model sizes and edit vocabulary sizes. ","Strenghts: - Performance reported in the paper is very strong and likely to be of interest to many practitioners in GEC, especially because it is obtained without auto-regressive decoding.
- Comparison to existing work seems fair and thorough.
- Work is fully opensourced.
- The paper presents results for many ensembles and numerous ablations, providing valuable experimental results for future research. ","Weaknesses: - The paper combines known techniques and datasets in a novel way, but all the elements are from other work on GEC.
- It would be nice to see more speed comparison between the model in the paper and some of the previous models, particularly on GPU or TPU. ","Formatting of the paper could be improved, e.g. quotes in the title of section 5.1 are incorrect, several tables exceed the column width or are not centered.
Ironically there are also quite a few grammatical errors. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"Weaknesses: - The paper combines known techniques and datasets in a novel way, but all the elements are from other work on GEC.
- It would be nice to see more speed comparison between the model in the paper and some of the previous models, particularly on GPU or TPU. 
Formatting of the paper could be improved, e.g. quotes in the title of section 5.1 are incorrect, several tables exceed the column width or are not centered.
Ironically there are also quite a few grammatical errors. "
ARR_2022_133_review,"The paper introduces a pre-trained vision language model (FewVLM) for prompt-based few-shot vision language tasks such as image captioning and vision question answering. The model is pre-trained with a combined objective of masked language modeling and prefix language modeling. Compared to giant pre-trained vision language models, FewVLM is relatively smaller, but it achieves significantly better zero-shot and few-shot performances, as reported. The authors also conducted a fine-grained analysis understanding the effect of different prompts, data sizes, and pre-training objectives. Their findings include that 1) zero-shot tasks are more sensitive to prompt crafting than few-shot tasks. 2) low-quality prompt also learn fast when increasing data size 3) the masked language modeling objective helps vqa more while the prefix language modeling objective boosts captioning performance. ","- The idea is straightforward and the results presented are solid and strong. It shows that with the proper objective for pre-training, the pre-trained models could be more performant on zero-shot and few-shot tasks even when the model size is much smaller than those giant pre-trained vision language models - The analysis is comprehensive and interesting, and some of the conclusions align well with the findings in NLP tasks. For example, prompt crafting is essential for zero-shot prediction, which inspires better prompt searching. ","- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.  - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines. ",- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.  - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines. 
- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers? "
ARR_2022_133_review,"This paper proposes a moderate-sized vision-and-language model for low-resource learning of several vision-and-language tasks. Specifically, the proposed methods includes first pre-training a sequence to sequence transformer based model with prefix language modeling and masked language modeling. Then, the pre-trained model can be adapted to several vision-and-language tasks (e.g., VQAv2) in a zero-shot or few-shot manner with hand-craft prompts. Empirical results show that the proposed methods could achieve comparable results to PICa on VQA tasks. This paper also analyzes the effect of using different hand-craft prompts/noisy prompts for zero-shot/few-shot performance and the effect of two pre-training objectives on different tasks. ","1. The proposed method is effective regarding zero-shot and few-shot performance, achieving comparable performance to larger vision-and-language models (i.e., PICa) on VQA tasks. On most tasks, the proposed method gets large improvement compared with VL-T5, which has almost the same architecture (same parameter number). 
2. Analysis shows that different prompt design will affect the zero-shot performance a lot. 
3. Interesting analysis demonstrating the effect of different pre-training objectives on different tasks, suggesting that the zero-shot and few-shot performance on downstream tasks can get better if including similar pre-training objective. ","1. The few-shot performance is evaluated over 5 randomly sampled different training and dev splits, it's better to also report the performance variance across 5 splits. If the variance is high, it might be better to include an analysis of how to pick training/dev data for few-shot learning / what kind of data will be more helpful for the performance. 
2. The proposed method gets large improvement compared with VL-T5, which has almost the same architecture. I'm also interested in how much of the improvement comes from different pre-training objective and how much of the improvement comes from better hand-craft prompt design (if we consider VL-T5 as using dataset-specific prefixes as hand-craft prompt). ","Typos: Line 166: ""or significantly improves"", no ""or""?
Suggestions: Related paper: Learning to Prompt for Vision-Language Models. Zhou et al. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The few-shot performance is evaluated over 5 randomly sampled different training and dev splits, it's better to also report the performance variance across 5 splits. If the variance is high, it might be better to include an analysis of how to pick training/dev data for few-shot learning / what kind of data will be more helpful for the performance. 
2. The proposed method gets large improvement compared with VL-T5, which has almost the same architecture. I'm also interested in how much of the improvement comes from different pre-training objective and how much of the improvement comes from better hand-craft prompt design (if we consider VL-T5 as using dataset-specific prefixes as hand-craft prompt). 
Typos: Line 166: ""or significantly improves"", no ""or""?
Suggestions: Related paper: Learning to Prompt for Vision-Language Models. Zhou et al. "
ARR_2022_133_review,"In this paper, the author(s) propose a new method, FewVLM, for learning vision-language tasks using a few examples. FewVLM is pre-trained using a mixture of PrefixLM and MaskLM losses on the MDCOCO and Visual Genome datasets. When adapting to downstream tasks of VQA and Image Captioning, the author(s) study the role of prompts in improving performance in the zero-shot and few-shot settings. Their analyses reveal that zero-shot performance can vary significantly with different prompts, but the performance stabilizes with the availability of a few examples. In their main results, the author(s) show that FewVLM can outperform Frozen and PICa while having far fewer parameters. ","- The paper studies an interesting question of low-resource adaptation of vision language models to downstream tasks. This is perhaps one of the first works to explore the role of prompts in vision language models.
- FewVLM is a (relatively) small scale model that can be run in academic settings and still manages to significantly outperform Frozen that is about one order of magnitude larger. Further, it can achieve results almost on par with PICa which uses GPT-3 as a backbone.
- Extensive ablation analysis of different prompts and settings in addition to the role of the pre-training objectives. ","In section 6.3, the author(s) mention that Unified VLP underperforms FewVLM on the captioning task. However, the conjecture mentioned is not clear. Why does the architecture or the pre-training make the model underperform? Particularly when the objective seems well-aligned with the downstream task? ","line 47: model → model. 
line 275: to the text → to the text as in ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"In section 6.3, the author(s) mention that Unified VLP underperforms FewVLM on the captioning task. However, the conjecture mentioned is not clear. Why does the architecture or the pre-training make the model underperform? Particularly when the objective seems well-aligned with the downstream task? 
line 47: model → model. 
line 275: to the text → to the text as in "
ARR_2022_347_review,"This paper proposed a swift-super model pair that achieves significantly lower computation overhead by bypassing most of the simpler examples through the smaller swift model. The main novelty is that the paper proposed an energy-based interpretation of the swift model predictions, and results in estimating the density function of the examples suitable for the swift model. This is new to the community.  In addition, the paper also carefully constructs the probing position of the softmax layer, in order to enable the whole method applicable to seq2seq models that have a decoder. ","1. Estimating the density function of the swift model is very interesting, and the author clearly explained the difference between this proposed method and others, such as softmax and entropy-based methods. 
2. Another point is that different from other models that are aimed at fast inference, this model enables their application on a seq2seq setting. 
3. The paper is clearly written, and the experiments are convincing. ","1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the ""time"" dimension, it will be less favorable than ""FLOPs."" So I'd be happy if the authors could also provide these figures in the ""time"" dimension. 
2. Some of the names seem not consistently referred to in the text. See ""comments, suggestions and typos."" ","1. If I understand correctly, in Line 283, there seem to be typos on the operators in the equation -F(x;f) < t = log(...). 
2. There is an inconsistency in the definition of ""logits"" between Line 251 (where f_y(x) could negative, and are regarded as logits) and Line 391 (where logits is the normalized conditional probability of the i-th class.) ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the ""time"" dimension, it will be less favorable than ""FLOPs."" So I'd be happy if the authors could also provide these figures in the ""time"" dimension. 
2. Some of the names seem not consistently referred to in the text. See ""comments, suggestions and typos."" 
1. If I understand correctly, in Line 283, there seem to be typos on the operators in the equation -F(x;f) < t = log(...). 
2. There is an inconsistency in the definition of ""logits"" between Line 251 (where f_y(x) could negative, and are regarded as logits) and Line 391 (where logits is the normalized conditional probability of the i-th class.) "
ARR_2022_347_review,"The authors introduce a dynamic inference approach that combines large and small models to provide the quality of the large model with the inference speed of the small model. Changing the problem of inference selection into a routing problem were based on the initial output of the encoder, an energy-based model determines if the swift (authors naming for the small model) is likely able to produce effective results. If the output of the encoder is far out of the distribution then the large model takes over to ensure there is quality.  To motivate and ground their work, the authors compare their method to the used of random and entropy dynamic inference to show that their energy-based method performs the best.  They evaluate their approach on GLUE and SuperGLUE finding a 3.3x and 2.9 speedup with almost no variation inaccuracy. ","1. There is a strong framework for increasing inference without allowing degradation in quality which is seen in other models. 
2. The approach provides an easy-to-understand approach for combining the quality of models which is clearly not tied to any specific model and should scale with continued LM releases. 
3. The approach the authors introduce is able to be combined with distillation which further shows how robust their ideas are general and expandable. ","1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead. 
2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5. ","1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing. 
2. How would this approach scale to batch size > 1? 
3. What is the required memory overhead to keep  4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc. ",3.5 ,No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",None ,"1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead. 
2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5. 
1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing. 
2. How would this approach scale to batch size > 1? 
3. What is the required memory overhead to keep  4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc. "
ARR_2022_347_review,"This paper ""integrates Super and Swift language models for achieving efficient inference without sacrificing the accuracy"". I.e. it introduces a method to combine a faster but less accurate and a slower but more accurate model so that the final product is about two times faster than Super model but is as efficient. And it is also more memory efficient than many other BERTs as tested on GLUE devset. ","1. The paper proposes a what seems to be very efficient approach to decreasing computational costs. Hence, strength 1: efficiency of solution. 
2. There is a lot of evidence that the approach is efficient: diagrams comparing it to state-of-the-art models, the code, lots of fresh and influential sources cited. 
3. The language (wording, grammar, etc) is perfect (minus a couple of typos). 
4. The paper is well-structured, in other words, classically structured: all the usually recommended components are in the right place. 
5. I've checked formulas in the cited source: LeCun et al., 2006. Seems plausible, although the authors got rid of beta and took derivative (meaning 'derivative of a function') of y, and I am missing how this happened. But the limitations in the size of papers wouldn't allow for a longer explanation anyway. From what my brief examination allows to say, the formulas look plausible. ","1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work. 
2. I have a feeling that the authors also took two precautions against immediate derivatives from their approach, and that gave them a couple of pages to add to their paper: paragraphs 3.3.1 Softmax-Based Mechanism and  3.3.2 Entropy-Based Mechanism explain why they did not use the softmax score and the entropy score, and that gave them one more paragraph: 4.1.1 Ablation Studies. But I'm also fine with this strategy. ","Consequently, earlier exists require -> exits Our method is easily.. orthogonal to many other existing methods (and later: To investigate the orthogonality of E-LANG with others)- I don't really get the meaning of 'orthogonal' and  'orthogonality' in this context. Seems that it is in the same direction with the general trends. 
and are also applicable for encoder-decoder -> is also ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work. 
2. I have a feeling that the authors also took two precautions against immediate derivatives from their approach, and that gave them a couple of pages to add to their paper: paragraphs 3.3.1 Softmax-Based Mechanism and  3.3.2 Entropy-Based Mechanism explain why they did not use the softmax score and the entropy score, and that gave them one more paragraph: 4.1.1 Ablation Studies. But I'm also fine with this strategy. 
Consequently, earlier exists require -> exits Our method is easily.. orthogonal to many other existing methods (and later: To investigate the orthogonality of E-LANG with others)- I don't really get the meaning of 'orthogonal' and  'orthogonality' in this context. Seems that it is in the same direction with the general trends. 
and are also applicable for encoder-decoder -> is also "
ARR_2022_347_review,"This paper proposes a dynamic inference approach to speedup inference with large language models. 
In particular, an energy based router is adopted to determine whether the swift model or the super model should be used to run inference on the given input example.  Experiments on classification as well as translation tasks show that the proposed approach achieves substantial speedup while retain the accuracy of the super model. ","The paper provides solutions for both classification and sequence-to-sequence models. 
More importantly, the proposed approach is simple and can be easily applied to existing production enviroments. 
The experiments are solid, the proposed approach is tested against both BERT and T5 models on several benchmarks. 
Extensive ablation studies show that energy based approach is better than alternatives such as softmax-based approach and entropy-based approach. 
In addition, it is great to see that combining the E-Lang with Distillation further improves model accuracy. 
Releasing the code helps others to reproduce the results. ","This is no necessarily a weakness:  The approach proposed here reminds me a lot about coarse-to-fine inference [1, 2], where the coarse/weak model is used to drastically pruning the search space in order to speedup inference.  Although these approaches were developed for structured prediction tasks and were designed for non-neural models, it would still be nice to have discussion about the connection between E-lang and this line of work.
[1] Improved inference for unlexicalized parsing [2] Structured Prediction Cascades ",It would be interesting to show/compare some examples that are routed to the swift model by energy/softmax/entropy based approaches. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"This is no necessarily a weakness:  The approach proposed here reminds me a lot about coarse-to-fine inference [1, 2], where the coarse/weak model is used to drastically pruning the search space in order to speedup inference.  Although these approaches were developed for structured prediction tasks and were designed for non-neural models, it would still be nice to have discussion about the connection between E-lang and this line of work.
[1] Improved inference for unlexicalized parsing [2] Structured Prediction Cascades 
It would be interesting to show/compare some examples that are routed to the swift model by energy/softmax/entropy based approaches. "
ARR_2022_229_review,"This paper proposes three new tasks based on a corpus of historical texts. Moreover, it prepares and releases such a corpus of historical tests (Chronicling America), data that can be used as pre-training for language models. The data collection and preparation are described in the paper, as well as the data used for the proposed challenges/tasks. After the three tasks are introduced, the paper investigates the performance of baselines systems, mainly based on language models. Everything is to be released with the paper, providing a useful data resource for future work. ","-The paper provides a useful resource in the form of a very large corpus of historical texts, carefully cleaned and prepare. 
-The paper proposed a suite of challenging machine learning-based tasks that can be useful to evaluate models in the domain of historical text and time-aware tasks. ","-The paper lacks a clear structure and at points it is a bit hard to follow the thread (see below for more detailed comments and suggestions). This being said, the goals are clearly described. 
-The evaluation is a bit poor in which there is no error analysis or clear insights (what these results tell us, what is lacking, etc.) and the choice of baselines and experimental setting not fully described. 
-The proposed tasks lack, in my opinion, a strong motivation. There is a short paragraph describing the motivation of each task (which is good) but perhaps not entirely convincing why the tasks are necessary.
Note: many of these weaknesses can be addressed in a subsequent submission, and I would be happy to reassess them if a resubmission is made. ","In the following I propose a few suggestions that I would personally think that would improve the paper.
-Reduce list of contributions in the introduction -Add a table with relevant statistics (e.g. number of documents per time period, etc.) about the collected corpus. I think this would give a better overview of the (massive) nature of the resource. 
-Move Section 3 (essentially related work) before Section 2 or before conclusion not to break the thread of the paper. 
Sections 2 and 4 (and even 5) could be merged as they are all data-related - this would also give a bit of structure to the paper, as now it has a concatenation of highly-related sections. 
-Add examples from the dataset and especially from the datasets on which the three proposed tasks are based on. 
-In Section 7.4 the statistics can be presented in a table in a more summarised manner. 
-Section 8 can be renamed “Results” or something similar? Also, it could be integrated with the previous one. I find it a bit odd to name a section “Baselines” and show the results, but this may be a personal opinion. 
-I was a bit confused by Section 9. It indicates that some text has been filtered after all the results were presented. These texts were filtered for the whole corpus? If so, I believe it is better to move this section to the data preparation sections, as it is an integral part of the whole preparation. 
-I would add a bit more of a discussion on the nature of the data (fully open, consent, limitations for research/commercial purposes, etc.) and how it will be shared. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"-The paper lacks a clear structure and at points it is a bit hard to follow the thread (see below for more detailed comments and suggestions). This being said, the goals are clearly described. 
-The evaluation is a bit poor in which there is no error analysis or clear insights (what these results tell us, what is lacking, etc.) and the choice of baselines and experimental setting not fully described. 
-The proposed tasks lack, in my opinion, a strong motivation. There is a short paragraph describing the motivation of each task (which is good) but perhaps not entirely convincing why the tasks are necessary.
Note: many of these weaknesses can be addressed in a subsequent submission, and I would be happy to reassess them if a resubmission is made. 
In the following I propose a few suggestions that I would personally think that would improve the paper.
-Reduce list of contributions in the introduction -Add a table with relevant statistics (e.g. number of documents per time period, etc.) about the collected corpus. I think this would give a better overview of the (massive) nature of the resource. 
-Move Section 3 (essentially related work) before Section 2 or before conclusion not to break the thread of the paper. 
Sections 2 and 4 (and even 5) could be merged as they are all data-related - this would also give a bit of structure to the paper, as now it has a concatenation of highly-related sections. 
-Add examples from the dataset and especially from the datasets on which the three proposed tasks are based on. 
-In Section 7.4 the statistics can be presented in a table in a more summarised manner. 
-Section 8 can be renamed “Results” or something similar? Also, it could be integrated with the previous one. I find it a bit odd to name a section “Baselines” and show the results, but this may be a personal opinion. 
-I was a bit confused by Section 9. It indicates that some text has been filtered after all the results were presented. These texts were filtered for the whole corpus? If so, I believe it is better to move this section to the data preparation sections, as it is an integral part of the whole preparation. 
-I would add a bit more of a discussion on the nature of the data (fully open, consent, limitations for research/commercial purposes, etc.) and how it will be shared. "
ARR_2022_229_review,"This paper presents a challenge that covers three tasks on historical text, that suit neural language models well: temporal classification, place classification and prediction of missing words. 
To complement the previous, a benchmark is prepared with data for training, validation and testing, and a RoBERTa model is trained and used as a baseline. 
Besides describing the challenge, the data and the baseline, the paper has a short discussion about discriminatory language in the data and how an existing model can be used for filtering abusive texts. ","New challenge that might be useful for evaluating the ""pre-train and fine-tune"" scenario nowadays common in NLP, with additional interest for studying the evolution of language; tasks are well-enough motivated; a procedure to make the challenges future-proof; the release of a new public corpus and a benchmark. ","Despite the motivation and presented statistics, the paper could transmit a better overview of the proposed challenge. As it is, I find it hard to evaluate its merits. Among other improvements, examples of the data for each task and its annotation would help.
The reason for balancing the test sets regarding the year of publication, not the training, is not clear.
After the abstract and introduction, I was expecting more about the presence of hate speech in the data, but it does not go further than reporting the % of documents filtered for being tagged as abusive by an existing model. ","Although, at a high level, tasks are well-motivated, what makes the authors think that other researchers will adhere to a challenge in this format?
Why using RoBERTa as the baseline? What were the alternatives?
Not clear how the perplexity is applied in the evaluation of the RetroGap challenge. This measure should be further explained for self-containedness.
Could a model trained for the RetroGap task contribute to better OCR?
Typos: consists in -> consists of (several times until section 5) pdf2dvju -> pdf2djvu described in Section ) avoid contractions like didn't and can't ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Despite the motivation and presented statistics, the paper could transmit a better overview of the proposed challenge. As it is, I find it hard to evaluate its merits. Among other improvements, examples of the data for each task and its annotation would help.
The reason for balancing the test sets regarding the year of publication, not the training, is not clear.
After the abstract and introduction, I was expecting more about the presence of hate speech in the data, but it does not go further than reporting the % of documents filtered for being tagged as abusive by an existing model. 
Although, at a high level, tasks are well-motivated, what makes the authors think that other researchers will adhere to a challenge in this format?
Why using RoBERTa as the baseline? What were the alternatives?
Not clear how the perplexity is applied in the evaluation of the RetroGap challenge. This measure should be further explained for self-containedness.
Could a model trained for the RetroGap task contribute to better OCR?
Typos: consists in -> consists of (several times until section 5) pdf2dvju -> pdf2djvu described in Section ) avoid contractions like didn't and can't "
ARR_2022_226_review,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data. With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines. ","1. The proposed two-stage training is novel and highly effective. Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines. 
2. Their idea is simple and clearly conveyed. With the experimental success, it is well-suited for a short paper. ","1. The ""baseline"" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset). 
2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages. 
3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below) ","L76: incomplete sentence ""... and ."" 
I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The ""baseline"" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset). 
2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages. 
3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below) 
L76: incomplete sentence ""... and ."" 
I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing. "
ARR_2022_226_review,"This paper aims to improve the quality of arbitrary translation directions (arbitrary ""XY"" translation) for a multilingual NMT (MNMT) system. The proposed approach consists of two stages: 1) Pretrain an NMT model on all available parallel data between any languages; 2) Finetune the model on ""many-to-one"" data for all target languages (multilingual many-to-one finetuning). The paper also studied large-scale training with in-house settings, including En-centric vs. multilingual-centric pretraining, and light-weight decoder architecture. Experimental results on WMT21 demonstrated that the proposed approach outperformed direct bi-lingual systems and pivot translation systems. ","This paper addresses and important problem of improving arbitrary translation directions between any language pairs in an NMT system, which is not well solved yet. The proposed two stage training (pretraining + finetuning) is potentially powerful and provides a new direction in solving this problem. ","1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned. 
2. Baselines chosen by the paper for comparison appear to be weak. 
3. The paper could also have been better organized. ","Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning. 
2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data.
Regarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup.
Regarding paper organization: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity. 
2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec. 2. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned. 
2. Baselines chosen by the paper for comparison appear to be weak. 
3. The paper could also have been better organized. 
Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning. 
2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data.
Regarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup.
Regarding paper organization: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity. 
2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec. 2. "
ARR_2022_226_review,"This paper discusses some practical aspects of Multilingual MT.  1- Observing that many-to-many multilingual systems have issues with target language generation, they propose to fine-tune a N-to-N multilingual MT system into N   N-to-1 many-to-one MT system. This represents a middleground: N systems have to be managed (vs N^2 bilingual system or 1 many-to-many system), but a large pretraining with all language direction has to be done only once. They report good BLEU improvement on the WMT21 multilingual translation task.
2- They also propose to improve the inference lattency of the systems by using an ""unbalanced"" Transformer architecture with 9 encoding layers but only 2 decoding layers. This architecture can reduce decoding lattency by 50% over a ""balanced"" architecture.
3- They also make some experiments to establish the usefulness of mult-centric pretraining over english-centric pretraining. ","I found the paper quite interesting , even if at times a bit unclear. The idea of fine-tuning many-to-many MT system into several many-to-one systems is simple but is shown to work well over two wmt21 tasks comprising 30 translation directions each.  Reducing the decoder depth for improving lattency do not appear so innovative to me, but it is at least interesting to see some number for the BLEU/lattency compromises this implies in a real large-scale setting.
Overall, I think there is enough to learn from this, given it is a short paper. ","I regret that the authors do not make comparisons with training N  N-to-1 systems from scratch. Training would be more costly, but would BLEU be better? ( or worse, due to the models never seeing some language directions).
Generally speaking, there is nothing strikingly innovative in the paper.
The text has many typos; and the paper could be clearer (see my recommendations below). ","The way you present your results do not sell them very well. It is a bit hard to tell at first glance from figure 1 if your method is working or not. At the very least, you should present an average of the improvements over all the languages (or maybe a separate average for english-centric and non-english centric pairs). The caption of figure 1 mention some number but you do not explain what they are: are they the average improvement over all languages?
You say your training data is 3.7B, but is that 3.7 B words? 3.7MB? 3.7B sentence pairs?
You mention doing some preliminary experiments showing that ""complete many-to-many training is still as challenging as one-to-many"". I would have liked that you detail a bit more your findings.
It is not clear to me why results in Table 2 and Table 1 are different for the pivot-based baselines. Is it because they are actually distilled in table 2?
I found section 2.2 a bit confusing. You seem to be trying a lot of unrelated things and presenting them simultaneously (training strategies, decoder architecture,...). And then you suddenly mention distillation in the Result section. You should restructure this a bit (maybe with one or two additional sections) to make clearer what you are doing and why/. Many typos: l076: ""and .""
Figure 1 caption: baslines -> baselines l123: ""graduation accumulation"" -> gradient accumulation l127:  ""graduation accumulation"" -> gradient accumulation l 212:  ""the the issue"" l254: ""Table 1""  -> I think this should be Table 2 ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"I regret that the authors do not make comparisons with training N  N-to-1 systems from scratch. Training would be more costly, but would BLEU be better? ( or worse, due to the models never seeing some language directions).
Generally speaking, there is nothing strikingly innovative in the paper.
The text has many typos; and the paper could be clearer (see my recommendations below). 
The way you present your results do not sell them very well. It is a bit hard to tell at first glance from figure 1 if your method is working or not. At the very least, you should present an average of the improvements over all the languages (or maybe a separate average for english-centric and non-english centric pairs). The caption of figure 1 mention some number but you do not explain what they are: are they the average improvement over all languages?
You say your training data is 3.7B, but is that 3.7 B words? 3.7MB? 3.7B sentence pairs?
You mention doing some preliminary experiments showing that ""complete many-to-many training is still as challenging as one-to-many"". I would have liked that you detail a bit more your findings.
It is not clear to me why results in Table 2 and Table 1 are different for the pivot-based baselines. Is it because they are actually distilled in table 2?
I found section 2.2 a bit confusing. You seem to be trying a lot of unrelated things and presenting them simultaneously (training strategies, decoder architecture,...). And then you suddenly mention distillation in the Result section. You should restructure this a bit (maybe with one or two additional sections) to make clearer what you are doing and why/. Many typos: l076: ""and .""
Figure 1 caption: baslines -> baselines l123: ""graduation accumulation"" -> gradient accumulation l127:  ""graduation accumulation"" -> gradient accumulation l 212:  ""the the issue"" l254: ""Table 1""  -> I think this should be Table 2 "
ARR_2022_348_review,"In this paper authors propose to leverage uni-modal self-supervised learning to promote the multimodal audio visual speech recognition (AVSR). Audio and visual encoders are pre-trained with large-scale uni-modal dataset, with paired audio-visual data used next for the multimodal framework. Combined CTC and seq2seq losses are leveraged. ","The proposed methodology is technically sound, and this paper is clearly written overall. Well-designed experiments and ablation studies are presented to show the effectiveness of the proposed approach. ","I don't find particularly concerns with this paper, though I have questions about several experimental details (see section below for more information). ","1. In lines 366-367, it's said the relative weights for combining CTC and seq2seq losses are different during training and decoding. Why is it the case? 
2. For results shown in Sections 4.3 and 4.4, is it possible to add error bars? 
3. What is the size of the proposed model, compared to baselines? 
4. In table 3, for the proposed model, WER for the audio-visual setting vs. audio-only setting are close (2.6% vs. 2.7%). Do authors have more justifications on going with multi-modal for this task? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I don't find particularly concerns with this paper, though I have questions about several experimental details (see section below for more information). 
1. In lines 366-367, it's said the relative weights for combining CTC and seq2seq losses are different during training and decoding. Why is it the case? 
2. For results shown in Sections 4.3 and 4.4, is it possible to add error bars? 
3. What is the size of the proposed model, compared to baselines? 
4. In table 3, for the proposed model, WER for the audio-visual setting vs. audio-only setting are close (2.6% vs. 2.7%). Do authors have more justifications on going with multi-modal for this task? "
ARR_2022_348_review,"The paper deals with the problem of audio-visual speech recognition (AVSR). The authors note that although the effectiveness of self-supervised learning has been shown in both modalities separately, they largely remain under explored for AVSR. So, they train audio and visual encoders separately using unimodal data and then integrate them together in a multimodal framework for AVSR. They show the effectiveness of their approach through various experiments and improve performance. ","- A framework for integrating unimodal self-supervised learned model for multimodal AVSR.
- The authors show large performance gains over previous work. ","- The contributions of the paper are not very clear.
- The explanation of the methods is not well explained.
- The performance improvement from audio-only to audio-visual is very small (WER: 2.7 to 2.6) and there is significance analysis. This raises a big question on the necessity to add the visual modality in this way and also justification on the extra computation for such a small gain. ","Questions/Comments: - Why did the authors did not include experiments with external LM for their approach? This will make a better comparison with other models using external LM.
- Line 183: “we truncate the first convolutional layer in MoCo v2” -> why?
- In the Audio-only model, what are the differences from Watanabe et al., 2017? This is very important to note the contribution of this work.
- Line 250: “a canonical transformer decoder” -> what is it?
- Line 254: “is arguably a decoder” -> why arguably?
- Line 577: the authors discuss about limited ImageNet data for pretraining. For self-supervised pretraining, the authors are not limited to labeled images as in ImageNet. They can use any image for pretraining. So, I’m not sure why the authors discuss this as a problem.
Typos: - Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"- The contributions of the paper are not very clear.
- The explanation of the methods is not well explained.
- The performance improvement from audio-only to audio-visual is very small (WER: 2.7 to 2.6) and there is significance analysis. This raises a big question on the necessity to add the visual modality in this way and also justification on the extra computation for such a small gain. 
Questions/Comments: - Why did the authors did not include experiments with external LM for their approach? This will make a better comparison with other models using external LM.
- Line 183: “we truncate the first convolutional layer in MoCo v2” -> why?
- In the Audio-only model, what are the differences from Watanabe et al., 2017? This is very important to note the contribution of this work.
- Line 250: “a canonical transformer decoder” -> what is it?
- Line 254: “is arguably a decoder” -> why arguably?
- Line 577: the authors discuss about limited ImageNet data for pretraining. For self-supervised pretraining, the authors are not limited to labeled images as in ImageNet. They can use any image for pretraining. So, I’m not sure why the authors discuss this as a problem.
Typos: - Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping "
ARR_2022_348_review,"The paper leverages self-supervised audio and visual representations for audio-visual speech recognition (AVSR) on the LRS2 and LRW. As shown in Figure 1, the audio branch is initialized from a pre-trained wav2vec 2.0, the visual branch is initialized from a pre-trained MoCo v2, and the two branches are fused before seq2seq+CTC decoding. Results on LRS2 show decent improvement over the recent E2E conformer Lip reading work. Ablation studies, noise-robustness, and low-resource AVSR are also helpful. The paper presentation and writings are also clear. However, there is limited novelty as utilizing self-supervised representations for AVSR has been done before. ","1. Demonstrate the effectiveness of pre-trained self-supervised representations for AVSR.  2. The paper looks technically correct, and the approach should be easily reproducible. ","Limited novelty: utilizing self-supervised speech representations for LRS has been conducted in [1], in which PASE is used instead of wav2vec 2. Also, see concurrent work [2] on training a self-supervised audio-visual representation from scratch for AVSR. The seq2seq+CTC joint decoding is also quite standard for ASR.  Reference:  [1] LiRA: Learning Visual Speech Representations from Audio through Self-supervision https://arxiv.org/abs/2106.09171 [2] LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION https://openreview.net/pdf?id=Z1Qlm11uOM ",I wonder if the authors plan to extend the proposed approach to LRS3? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"Limited novelty: utilizing self-supervised speech representations for LRS has been conducted in [1], in which PASE is used instead of wav2vec 2. Also, see concurrent work [2] on training a self-supervised audio-visual representation from scratch for AVSR. The seq2seq+CTC joint decoding is also quite standard for ASR.  Reference:  [1] LiRA: Learning Visual Speech Representations from Audio through Self-supervision https://arxiv.org/abs/2106.09171 [2] LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION https://openreview.net/pdf?id=Z1Qlm11uOM 
I wonder if the authors plan to extend the proposed approach to LRS3? "
ARR_2022_185_review,"This paper argues that analyze contextual embeddings is particularly effective with paraphrases, because it allows experiments to control for meaning which is presumably the same in paraphrases. The authors draw several key conclusions about BERT via this paraphrase-based analysis. ","The idea of controlling for meaning with paraphrases is unique and promising. It is clever in that instead of focusing on the ways in which context should change the meaning of words, it focuses on the specific contexts which should cause the meanings to converge. This allows for more precise control over what we expect should happen. ","The meaningfulness of these conclusions strongly depend on PPDB being true paraphrases. If they aren't paraphrases in all dimensions and in the strictness sense, it's unclear that one is truly controlling for meaning. In many example paraphrases shown in the paper, it's clear that one wouldn't not expect representation to be the same for even an ideal representation. For example, ""are mad"" and ""'re out of your mind"" can and often do have different meanings. Even for the highest rated paraphrases such as ""where did they come from?"" vs ""where are they from"" can mean drastically different things, and it's unclear whether a representation that has an identical interpretation of those two phrases is what we really want. ","I would recommend revisiting strong assumptions being made in this paper. No two sentences mean exactly the same thing, even before discounting noise from PPDB. How these analyses are affected by differences in sentence or phrase meanings should be a big part of the discussion. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The meaningfulness of these conclusions strongly depend on PPDB being true paraphrases. If they aren't paraphrases in all dimensions and in the strictness sense, it's unclear that one is truly controlling for meaning. In many example paraphrases shown in the paper, it's clear that one wouldn't not expect representation to be the same for even an ideal representation. For example, ""are mad"" and ""'re out of your mind"" can and often do have different meanings. Even for the highest rated paraphrases such as ""where did they come from?"" vs ""where are they from"" can mean drastically different things, and it's unclear whether a representation that has an identical interpretation of those two phrases is what we really want. 
I would recommend revisiting strong assumptions being made in this paper. No two sentences mean exactly the same thing, even before discounting noise from PPDB. How these analyses are affected by differences in sentence or phrase meanings should be a big part of the discussion. "
ARR_2022_185_review,"This paper evaluates BERT representations of words in a paraphrase corpus (PPDB), mostly analyzing the extent to which representations of equivalent words are consistent between their respective sentences, demonstrating that word form carries more weight in BERT than the semantic neighborhood. ","The idea is nice and, as far as I am aware, novel, presenting insights that are both useful and straightforward. The paper is well-written and the methods are well-applied. ",N/A --- I reviewed this paper in its previous iteration and it appears the authors have quelled pretty much all of my concerns. Well done! ,"""BERT does consistently represent phrases that are paraphrases"" (l.41) has a different meaning from ""BERT does represent phrases that are paraphrases consistently"", which is probably what you meant. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"N/A --- I reviewed this paper in its previous iteration and it appears the authors have quelled pretty much all of my concerns. Well done! 
""BERT does consistently represent phrases that are paraphrases"" (l.41) has a different meaning from ""BERT does represent phrases that are paraphrases consistently"", which is probably what you meant. "
ARR_2022_47_review,"The paper presents an approach to supervised contrastive learning for NLU - classification task by anchoring CL losses using label embeddings. The overall CL loss comprises of three losses. First, the (Multihead) Instance-centered contrastive loss that aims to bring the instance embedding closer to the label embedding by anchoring on the instance and its label as positive example and instance-other labels as negative examples. Second, the label centered contrastive loss which uses labels to mine positive and negative labels from a batch. And finally, to promote better uniformity across learned embeddings - label embeddings are also regularized.  The authors conduct experiments on standard benchmark NLP tasks showing that their approach produces better/competitive results to baselines. They also show the effectiveness of their approach in Few Shot and Data imbalance settings.  Ablation study provides the importance of each of the component / architectural choices. ","1. Well written, well reasoned, easy to follow how model / architecture / experiments are setup up. 
2. Extensive experimentation / ablation with statistical significant testing. 
3. Effective use of label semantics in Supervised Contrastive Learning. 
4. The work seems well researched given their knowledge of CL field which is showcased not only in related work section but also the mention of alignment/uniformity, preventing degradation in CL models, regularizing label embeddings etc. From a CL modeling perspective - the authors have crossed all the ""t""s dotted all the ""i""s. 
5. Results show improvements / competitiveness across standard benchmarks and their technique is easy to implement / replicate. ","1. While the paper details a lot of ""How""s - the ""Why""s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.
2. The author mention uniformity / alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https://arxiv.org/pdf/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https://arxiv.org/pdf/1708.06320.pdf) ","1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read ""The input of LaCon contains two parts consisting of the text and all the labels for the task) ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,N/A ,"1. While the paper details a lot of ""How""s - the ""Why""s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.
2. The author mention uniformity / alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https://arxiv.org/pdf/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https://arxiv.org/pdf/1708.06320.pdf) 
1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read ""The input of LaCon contains two parts consisting of the text and all the labels for the task) "
ARR_2022_47_review,"This paper proposes a novel supervised contrastive learning method, LaCon, which makes sufficient use of label information by three defined contrastive loss: instance-centered contrastive loss, label-centered contrastive loss, and label embedding regularizer loss. Experiments on eight datasets show that this method is competitive against other baselines. Extensive experiments on few-shot and imbalanced data also demonstrate that this method has a good capability. Visualization of label and instance representation also shows the reason for different performances. ","- This paper proposed a good supervised contrastive learning method. Especially as described in Sec. 4.1-4.2, using it in few-shot learning and data imbalance setting can improve the performance than using traditional methods.
- Sufficient experiments are conducted. To validate the effectiveness of this new method on natural language understanding, it experiments on 8 datasets. Moreover, different settings including few-shot and imbalanced data are taken into account. Details about experiments are demonstrated in Appendix.
- Good presentation. Figure 1 shows the core information of this paper, three contrastive losses, which can help readers to understand the design. Figure 4 shows the different representations of CE and LaCon, and it can be used to explain the reason from a perspective of representation learning. ","- The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.
- Code is unavailable. It's hard to reproduce the results.
- Writing should be improved and double-checked. ","Please refer to the comments above.
Grammar errors:  - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?
- line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.
- Code is unavailable. It's hard to reproduce the results.
- Writing should be improved and double-checked. 
Please refer to the comments above.
Grammar errors:  - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?
- line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ? "
ARR_2022_47_review,"This paper proposes LaCon, a new supervised contrastive learning approach, which is based on learning label embeddings jointly. Specifically, three losses are proposed: 1) multi-head instance-centered contrastive loss, 2) label-centered contrastive loss, and 3) label embedding regularization loss. The authors shows that combining all three losses improves the fine-tuning results of BERT. ","- LaCon achieves strong results in 8 datasets and outperforms several prior methods.
- Table 3 demonstrates the gain of each component of the proposed method. The authors show that all three proposed losses help.
- It is good to see that the authors provide the mean and standard deviation out of 10 runs in their experiments and also bold the ones with p-value < 0.05.
- Overall, the writing is clear and easy to follow. I like how the authors visualize the method in Figure 1. ","- It is surprising to see that unlike conventional NCE or InfoNCE, the authors omit the positive term in the denominator in equations (3), (4), and (5). In NCE or InfoNCE, adding the positive term in the denominator is intuitive since it is optimizing the log probability of selecting the positive example. However, the authors do not explain the reason behind their counter-intuitive change nor provide an ablation study on this modification. I would like to see the justification of this change and experimental results that support it.
- In subsection 2.6, the authors introduce LaCon with label fusion which requires additional computation compared to LaCon-vanilla and CE baseline. It would be better if the authors can quantify the computation overhead by providing the inference time of the methods.
- The Instance-centered Contrastive Loss (ICL) is not very novel given that it is just a different view of the softmax classifier. By treating the weights of the last linear layer as the embedding vectors of each class, optimizing the cross-entropy is almost identical to ICL (except that the vectors should be L2 normalized which is basically applying weight normalization to the last linear layer and normalize the features right before the last layer). ","- In line 211, ""we modify the InfoNCE (Gutmann and Hyvärinen, 2010) to calculate the loss"", here the authors cite the wrong paper. Gutmann and Hyvärinen (2010) propose NCE, but InfoNCE is proposed by Oord et al. (2018). The proposed loss is similar to both NCE and InfoNCE, so changing either the citation or name of the method is fine. The same error also appears in line 94.
- The proposed method lies in the metric learning field; however, the authors do mention the related metric learning work in their section 5 (related work). For example, the contrastive loss was first proposed by Chopra et al. (2005) and triplet loss (Weinberger et al., 2005; Schroff et al., 2015) is also quite relevant to this paper.
- It would be great if we can see that the proposed method works for other pretrained models besides BERT as well.
References - Oord, Aäron van den et al. “Representation Learning with Contrastive Predictive Coding.” ArXiv abs/1807.03748 (2018): n. pag.
- Chopra, Sumit, Raia Hadsell, and Yann LeCun. "" Learning a similarity metric discriminatively, with application to face verification."" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE, 2005.
- Weinberger, Kilian Q., John Blitzer, and Lawrence Saul. "" Distance metric learning for large margin nearest neighbor classification."" Advances in neural information processing systems 18 (2005).
- Schroff, Florian, Dmitry Kalenichenko, and James Philbin. "" Facenet: A unified embedding for face recognition and clustering."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- It is surprising to see that unlike conventional NCE or InfoNCE, the authors omit the positive term in the denominator in equations (3), (4), and (5). In NCE or InfoNCE, adding the positive term in the denominator is intuitive since it is optimizing the log probability of selecting the positive example. However, the authors do not explain the reason behind their counter-intuitive change nor provide an ablation study on this modification. I would like to see the justification of this change and experimental results that support it.
- In subsection 2.6, the authors introduce LaCon with label fusion which requires additional computation compared to LaCon-vanilla and CE baseline. It would be better if the authors can quantify the computation overhead by providing the inference time of the methods.
- The Instance-centered Contrastive Loss (ICL) is not very novel given that it is just a different view of the softmax classifier. By treating the weights of the last linear layer as the embedding vectors of each class, optimizing the cross-entropy is almost identical to ICL (except that the vectors should be L2 normalized which is basically applying weight normalization to the last linear layer and normalize the features right before the last layer). 
- In line 211, ""we modify the InfoNCE (Gutmann and Hyvärinen, 2010) to calculate the loss"", here the authors cite the wrong paper. Gutmann and Hyvärinen (2010) propose NCE, but InfoNCE is proposed by Oord et al. (2018). The proposed loss is similar to both NCE and InfoNCE, so changing either the citation or name of the method is fine. The same error also appears in line 94.
- The proposed method lies in the metric learning field; however, the authors do mention the related metric learning work in their section 5 (related work). For example, the contrastive loss was first proposed by Chopra et al. (2005) and triplet loss (Weinberger et al., 2005; Schroff et al., 2015) is also quite relevant to this paper.
- It would be great if we can see that the proposed method works for other pretrained models besides BERT as well.
References - Oord, Aäron van den et al. “Representation Learning with Contrastive Predictive Coding.” ArXiv abs/1807.03748 (2018): n. pag.
- Chopra, Sumit, Raia Hadsell, and Yann LeCun. "" Learning a similarity metric discriminatively, with application to face verification."" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE, 2005.
- Weinberger, Kilian Q., John Blitzer, and Lawrence Saul. "" Distance metric learning for large margin nearest neighbor classification."" Advances in neural information processing systems 18 (2005).
- Schroff, Florian, Dmitry Kalenichenko, and James Philbin. "" Facenet: A unified embedding for face recognition and clustering."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. "
ARR_2022_290_review,"The paper proposes a neural OpenIe system that generates compact extractions. The idea is exciting, and the results look promising. On the downside, the methodology lacks some details which are difficult to understand in the current version: 1. If the decisions are based on pair interactions in the table, how are constituents with more than three words handled? 
2. How are Constraints in 3.1.2 effectively implemented? For instance, how do the symmetry loss looks like? 
3. Methodology described in 3.2 and Figure 3. looks similar to [1], but I do not see the paper cited nor any other using a similar technique. 
4. If the method is trained in 4. Wouldn't the system just learn to reproduce the underlying rules of the automatic benchmark creation? 
5. Maybe a driving example describing step by step the example in Figures 2 and 3 could help to clarify the methodology. 
6. It is not well defined what the authors mean with ""compact"" extractions, maybe a crispier definition can bring a bit of light on the methodology and design choices.
[1] Matching the Blanks: Distributional Similarity for Relation Learning, Soares et. al. ",1. Interesting idea 2. Promising results ,1. Lacks clarity in some important methodological details and problem definitions (see above) ,"Potentially important citation [1] Matching the Blanks: Distributional Similarity for Relation Learning, Soares et. al. ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Lacks clarity in some important methodological details and problem definitions (see above) 
Potentially important citation [1] Matching the Blanks: Distributional Similarity for Relation Learning, Soares et. al. "
ARR_2022_290_review,"This work deals with a purported flaw of modern neural OpenIE systems: their tendency to extract overly specific arguments which are not useful for upstream tasks. The authors present a neural OpenIE system which produces “compact” extractions. The concept of “compactness” isn’t explicitly defined, but examples are provided and various proxies are supplied (e.g. triplets should be as short as possible without affecting the semantics, triplets shouldn’t contain arguments which are in themselves triplets).    The suggested solution is innovative in two ways: 1. It is based on a new training and evaluation dataset. This dataset is derived from a standard one using a heuristic algorithm which identifies and extracts compact triplets while filtering out complex ones.  2. A novel OpenIE algorithm is employed: it is based on a slot filling architecture recently suggested for relation extraction, but in this work it’s employed as part of a pipeline system which the authors show further increases performance. ","1. The paper deals with an important practical problem of how to optimize the usefulness of Open IE triplets for upstream tasks.  2. The paper is clear and does a great job of reviewing the literature in this area and citing relevant sources.  3. The authors suggest a pipeline approach: a table filling stage for the identifying arguments and predicates, and a second stage model which links the two. I think combining these ideas is indeed novel, and the authors also show that the suggested solution performs better than an end-to-end solution based on table filling alone.  4. Both automatic and manual evaluation methods are used to show that the suggested solution performs better than existing approaches on IE datasets modified to contain compact extractions. ","1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of “compact” extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don’t think there’s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem. 
E.g. based on the criteria for informative extractions described in the paper “Annotating and predicting non-restrictive noun phrase modifications” (Stanovsky et. al 2016), extracting “Hercule Poirot” “is” “a Belgian detective” from the sentence  “Hercule Poirot is a Belgian detective, created by Agatha Christie” might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.
2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.
3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score). ","Despite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction).
I would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",1 = They would not be able to reproduce the results here no matter how hard they tried.,,"1. I think the paper goes a bit too quickly from the problem description of systems producing extractions which are too specific, to their specific flavor of “compact” extractions designed to solve this problem. The paper does cite different sources which explain the problem in more depth, but I don’t think there’s a consensus that the type of compact extractions the authors promote is the ideal solution for the problem. 
E.g. based on the criteria for informative extractions described in the paper “Annotating and predicting non-restrictive noun phrase modifications” (Stanovsky et. al 2016), extracting “Hercule Poirot” “is” “a Belgian detective” from the sentence  “Hercule Poirot is a Belgian detective, created by Agatha Christie” might not be specific enough (as the modification here seems restrictive). More discussion of this criteria and how it relates to compact extractions will be helpful.
2. Continuing the above point, the evaluation in this paper is intrinsic and uses datasets modified to include extractions of the type the authors are promoting, so comparison to existing systems is not 1:1. Some kind of extrinsic evaluation (e.g. on a KBC or KB alignment task) could be helpful in showing that the suggested openIE method does in fact work better for upstream tasks.
3. The authors give no indication of whether the modified datasets, code, system, or the data used in manual evaluation will be made available. Assuming these are not public, it is harder still to estimate whether the suggested approach is really a good fit for upstream tasks (this is also why I give this paper low reproducibility scores. If the authors were to share code/data, I would have assigned a higher score). 
Despite the stated weaknesses, I'm narrowly in favor of accepting this paper. It deals with an important and not frequently discussed practical problem (making OpenIE triplets more useful for upstream tasks), the approach is interesting and the evaluation shows that the system produces triplets of the type the authors advocate more accurately than other systems (I think more can be done to convince that this style of queries is indeed optimal for upstream tasks, but still, this work seems like a step in the right direction).
I would highly encourage the authors to make the code, datasets and sample extractions public, as it can really increase the impact of this work, allow others to consume and evaluate the outputs in real-world tasks, etc. "
ARR_2022_48_review,"To alleviate the problem of error propagation due to Automatic Speech Recognition (ASR) system for multi-modal sentiment analysis task the paper provides a refinement approach by detecting the positions of the sentiment words in the text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues such as low voice, sad face and textual context. Their approach outperforms the baselines. ",The approach does try to address the real-world problem of error propagation due to ASR. The  architecture is novel. Good benchmarking and ablation. ,It would be difficult to detect the positions of sentiment words if an overall sentence (more than one word) has become noisy due to ASR. The same would be true if there are inserts and deletes in the resulting noisy sentence. This approach may fail in such cases. What is the solution for the same?  Some qualitative analysis of the results and/or error analysis will be useful to understand these type of cases better. The case study section can be replaced with qualitative and /or error analysis. ,"The last sentence of the abstract “Furthermore, our approach can be adapted for other multimodal feature fusion models easily.” needs more explanation somewhere in the paper. This is a very vague sentence without any proper basis.  There is no mention of making the built datasets publicly available. Though the dataset can be built to reproduce the results it would be better if authors can share the same publicly. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"It would be difficult to detect the positions of sentiment words if an overall sentence (more than one word) has become noisy due to ASR. The same would be true if there are inserts and deletes in the resulting noisy sentence. This approach may fail in such cases. What is the solution for the same?  Some qualitative analysis of the results and/or error analysis will be useful to understand these type of cases better. The case study section can be replaced with qualitative and /or error analysis. 
The last sentence of the abstract “Furthermore, our approach can be adapted for other multimodal feature fusion models easily.” needs more explanation somewhere in the paper. This is a very vague sentence without any proper basis.  There is no mention of making the built datasets publicly available. Though the dataset can be built to reproduce the results it would be better if authors can share the same publicly. "
ARR_2022_48_review,"This paper points out that the performance of current state-of-the-art multimodal emotion analysis (MSA) models decreases sharply when the automatic speech recognition (ASR) models predict sentiment words wrong. Therefore, this paper proposes a sentiment word aware MSA model named SWRM, which detects and alleviates the harm caused by sentiment word prediction errors of the ASR models. Specifically, SWRM consists of a sentiment word position detection module based on BERT, a multimodal feature extraction module based on LSTM and BERT, an aggregation network and a multimodal gating network for fusing different features. ","1. This paper focuses on the problem that the sentiment word prediction errors of the ASR models corrupt the effects of current state-of-the-art MSA models, which is valuable in practice. 
2. The proposed model SWRM achieved improvements over other baselines, especially when the ASR model performed poorly. ","1. The contribution of this paper is slightly less: the improvement on prediction effect of SWRM seems to mainly come from the capability of BERT in detecting the position of sentiment words and predicting correct sentiment words. 
2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates. 
3. Insufficient datasets and explanation of experimental settings: this paper only used one dataset (CMU-MOSI) for evaluation, which cannot fully compare the performance and robustness between SWRM and baseline models. Particularly, CMU-MOSI is one of the datasets employed by Dumpala et al. (2018), and there is a larger dataset named CMU-MOSEI in (Dumpala et al., 2018). It is suggested to add the CMU-MOSEI dataset for evaluation. Moreover, the experimental part doesn’t mention the setting of hyperparameters. 
4. Lines 308-319 are quite confused to me. The definition of notations should be largely improved for clarity. ","1. Line 220: is absent -> are absent; Line 259: in the training and testing phrases -> in the training and testing phases; Lines 310-311: k_i is defined as the number of sentiment words, but k_i is not used elsewhere; Line 352: Subsequently, We -> Subsequently, we. 
2. As mentioned above, I think that supplementary datasets are important for better comparing SWRM and baselines. Moreover, the effect of sentiment word detection and correction of SWRM should better be shown. 
3. From my opinion, instead of correcting sentiment word errors from the ASR models, improving the performance of the ASR models is a thorough solution to guarantee the effect of the MSA models in practice. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The contribution of this paper is slightly less: the improvement on prediction effect of SWRM seems to mainly come from the capability of BERT in detecting the position of sentiment words and predicting correct sentiment words. 
2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates. 
3. Insufficient datasets and explanation of experimental settings: this paper only used one dataset (CMU-MOSI) for evaluation, which cannot fully compare the performance and robustness between SWRM and baseline models. Particularly, CMU-MOSI is one of the datasets employed by Dumpala et al. (2018), and there is a larger dataset named CMU-MOSEI in (Dumpala et al., 2018). It is suggested to add the CMU-MOSEI dataset for evaluation. Moreover, the experimental part doesn’t mention the setting of hyperparameters. 
4. Lines 308-319 are quite confused to me. The definition of notations should be largely improved for clarity. 
1. Line 220: is absent -> are absent; Line 259: in the training and testing phrases -> in the training and testing phases; Lines 310-311: k_i is defined as the number of sentiment words, but k_i is not used elsewhere; Line 352: Subsequently, We -> Subsequently, we. 
2. As mentioned above, I think that supplementary datasets are important for better comparing SWRM and baselines. Moreover, the effect of sentiment word detection and correction of SWRM should better be shown. 
3. From my opinion, instead of correcting sentiment word errors from the ASR models, improving the performance of the ASR models is a thorough solution to guarantee the effect of the MSA models in practice. "
ARR_2022_325_review,"This paper presents a method for inducing unsupervised tags for word sequences, later to be compared against POS tags. The method combines a CRF autoencoder (CRF-AE) for the first step of inducing pseudo-labels, with a rule-based approach for the later step of regenerating the word sequence from these pseudotags. The system hinges on contextualized character-based ELMo representations and some algebraic modifications that are empirically proven to be useful. The system outperforms previous approaches to this problem on both the PTB dataset and a small subset of UD treebanks. ",The system outperforms existing ones on the task of unsupervised POS tag induction. ,"This might be a bit unfair, but a serious concern of mine is the task itself. Barring any further information on the evaluation protocol, most notably on the *number of induced tags allowed for the system*, one can think of a trivial system that outperforms any of the ones presented by almost 10 points on PTB - just learn a tag for each vocabulary item. The authors offer no analysis of their system's actual outputs, of which ""many"" correlate with which ""one"" in the evaluation scheme, and although they use a second metric that contains a component that mitigates this problem, they leave this detail out and send the reader to the appendix to understand what the metric does (this is completely unacceptable. If you're using a metric, especially if you're arguing that it's better than the other one, at least explain the virtues of it along general lines in the main text).
The multilingual evaluation claims to be of ""the UD data"", a dataset of over 100 languages, yet the authors only select ten without explaining why. The availability of ELMo embeddings, in a resource tucked away in the appendix as well, is for about 45 languages and the intersection is significantly more than 10. ","Section 3 (the Methods) is very difficult to follow. Almost every subsection selectively gets rid of part of the notation without warning, and inconsistencies abound. For example, in Figure 3, are h_i vectors or lists of vectors (with different layers) and if the latter, at what part are they aggregated? What's the input and output of the minus operand (it has two outgoing arrows?); Is W^s the same as W^{hourglass}? Is it a new parameter? Why is the (unnumbered) equation before line 427 referring to equation (8) as replacing it but taking different inputs (forward and backward h's as opposed to h's from different indices, which by the way may or may not be indicating layers or words, since the variable ""i"" is reserved for neither). Using vague terminology like ""purified representations"" (l.253) doesn't help this overall messiness, making the system very hard to understand.
In equation (12), there's a dependence on the vocabulary. How do OOV words get generated under this formulation?
Do you fine-tune the ELMo parameters?
Other stuff: - The last sentence of the first paragraph is unfinished.
- l.64 an-->a - l.294 spareness-->sparseness - l.328 I'm pretty sure you meant ""supervised"" rather than ""unsupervised"", these are the standard splits for *all* WSJ tasks, not limited to dependency parsing either. ",2.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"This might be a bit unfair, but a serious concern of mine is the task itself. Barring any further information on the evaluation protocol, most notably on the *number of induced tags allowed for the system*, one can think of a trivial system that outperforms any of the ones presented by almost 10 points on PTB - just learn a tag for each vocabulary item. The authors offer no analysis of their system's actual outputs, of which ""many"" correlate with which ""one"" in the evaluation scheme, and although they use a second metric that contains a component that mitigates this problem, they leave this detail out and send the reader to the appendix to understand what the metric does (this is completely unacceptable. If you're using a metric, especially if you're arguing that it's better than the other one, at least explain the virtues of it along general lines in the main text).
The multilingual evaluation claims to be of ""the UD data"", a dataset of over 100 languages, yet the authors only select ten without explaining why. The availability of ELMo embeddings, in a resource tucked away in the appendix as well, is for about 45 languages and the intersection is significantly more than 10. 
Section 3 (the Methods) is very difficult to follow. Almost every subsection selectively gets rid of part of the notation without warning, and inconsistencies abound. For example, in Figure 3, are h_i vectors or lists of vectors (with different layers) and if the latter, at what part are they aggregated? What's the input and output of the minus operand (it has two outgoing arrows?); Is W^s the same as W^{hourglass}? Is it a new parameter? Why is the (unnumbered) equation before line 427 referring to equation (8) as replacing it but taking different inputs (forward and backward h's as opposed to h's from different indices, which by the way may or may not be indicating layers or words, since the variable ""i"" is reserved for neither). Using vague terminology like ""purified representations"" (l.253) doesn't help this overall messiness, making the system very hard to understand.
In equation (12), there's a dependence on the vocabulary. How do OOV words get generated under this formulation?
Do you fine-tune the ELMo parameters?
Other stuff: - The last sentence of the first paragraph is unfinished.
- l.64 an-->a - l.294 spareness-->sparseness - l.328 I'm pretty sure you meant ""supervised"" rather than ""unsupervised"", these are the standard splits for *all* WSJ tasks, not limited to dependency parsing either. "
ARR_2022_93_review,"The authors of this paper propose a new method that assesses dialogue models through the live evaluation process and overcomes the drawback of requiring pre-created reference dialogues. Further, the authors claim that their proposed method achieves highly reliable evaluation with a correlation value of 0.96 when experiments were replicated.  The proposed experiment design focuses on asking a crowd worker to carry out a live conversation with a randomly selected model and is asked to evaluate the model through a Likert scale and continuous scale on 7 evaluation measures. Further, as a part of the experiment, the authors also put quality checks in place to ensure that they were getting good quality data. To avoid traditional quality checks, the authors introduced a random degraded model and monitored how crowdsourced workers rated those models compared to other pretrained models obtained from the ParlAI framework. The degradation is introduced by random sampling from training dialogues and distortion of meanings.
Results from multiple runs of the human evaluation show that the proposed data collection methodology achieves high interrater consistency across multiple runs and the 7 metrics as demonstrated in Tables 3 & 4. ","1. The authors tackle an important problem of getting good quality ratings for human evaluation of dialogue systems and results obtained in the over three different runs suggest that the high consistency in ratings from the human evaluation is achievable. 
2. The authors conduct a large-scale evaluation to provide validity to their study across multiple runs. The data collected from these experiments would be valuable to the community to analyze and build on it and the authors plan to release it. 
3. Quality criteria designed by the authors for these experiments do a  good job of eliminating ratings from low-quality crowdsourced workers. ","1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings. 
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section. 
3. Lack of screenshots of the experimental interface ","Comments: 1. Please add screenshots of the interface that was designed. 
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2? 
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response? 
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked. 
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model? 
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020. 
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122. 
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13). 
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928. 
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings. 
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section. 
3. Lack of screenshots of the experimental interface 
Comments: 1. Please add screenshots of the interface that was designed. 
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2? 
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response? 
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked. 
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model? 
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020. 
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122. 
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13). 
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928. 
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087. "
ARR_2022_25_review,"This paper describes the development of a data set that can be used to develop a system that can generate automated feedback to learners' responses to short-answer questions.  The data set includes questions, their answers, and their feedback in the domain of computer networking, mostly in English but with a sizable German subset as well.  The paper describes the construction of the data set, including agreement and encountered challenges, as well as experimental results that can serve as a baseline for future work. ","Although the domain is niche, since the authors do an extremely thorough job of thoughtfully constructing their data set with expert annotators and guidance, agreement-measurement, and validity evidence, this paper should serve as a model to the community with respect to how to compile similar data sets.
While the authors mention that the data set is small -- 4,519 response/feedback pairs covering 22 English and 8 German questions -- it's actually quite large for something that is completely human-compiled and human-reviewed.
This paper is very clear, easy to follow, and well-organized. ","Unfortunately, the final data set contains imbalanced classes, something the authors aim to address in future versions of the data set.  I wouldn't use this as a reason to reject this paper, however.
Some in our community may find this work, and its domain, rather niche; this paper would be a great fit for the BEA workshop. ","Can the authors mention the dates during which the data was collected?  Since this was such a big manual effort, I wouldn't be surprised if the bulk of the work was done in 2021 on data collected in 2020, for instance. This is also important since the domain is computer networking which changes fairly rapidly.
On line 005, insert ""many"" between ""by"" and ""Automatic"".
On line 040, change ""interesting"" to ""useful"".
On line 054, ""in the last decades"" should read ""over the past decades"".
On line 154, ""detrimental for"" should be ""detrimental to"".
The last sentence of section 2.2, beginning with ""Lastly, structured collections..."" seems out-of-place here.  Should this be a separate paragraph?  Or can you do more to tie it in with the preceding sentences?
On line 395, ""refined for multiple years"" should be ""refined over multiple years"".
In this field, it's typical to refer to learners' responses to questions as ""responses"" rather than ""submissions"".  Just a minor thing you may want to consider :) ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Unfortunately, the final data set contains imbalanced classes, something the authors aim to address in future versions of the data set.  I wouldn't use this as a reason to reject this paper, however.
Some in our community may find this work, and its domain, rather niche; this paper would be a great fit for the BEA workshop. 
Can the authors mention the dates during which the data was collected?  Since this was such a big manual effort, I wouldn't be surprised if the bulk of the work was done in 2021 on data collected in 2020, for instance. This is also important since the domain is computer networking which changes fairly rapidly.
On line 005, insert ""many"" between ""by"" and ""Automatic"".
On line 040, change ""interesting"" to ""useful"".
On line 054, ""in the last decades"" should read ""over the past decades"".
On line 154, ""detrimental for"" should be ""detrimental to"".
The last sentence of section 2.2, beginning with ""Lastly, structured collections..."" seems out-of-place here.  Should this be a separate paragraph?  Or can you do more to tie it in with the preceding sentences?
On line 395, ""refined for multiple years"" should be ""refined over multiple years"".
In this field, it's typical to refer to learners' responses to questions as ""responses"" rather than ""submissions"".  Just a minor thing you may want to consider :) "
ARR_2022_68_review,"This work studies using human feedback as contextual bandit learning to improve QA models. The user interacts with a QA model trained initially with conventional supervised learning, providing binary feedback (correct v.s incorrect) to judge the correctness of returned answers. Notably, the author does not really deploy the model. Instead, they derive pseudo-feedbacks from gold labels, which causes a significant gap between their simulation and real-world interaction.  After collecting feedback through simulation, they fine-tune the deployed QA model by maximizing the likelihood of predicted answer with positive feedback and minimizing the likelihood of the one with negative feedback. They consider two learning settings, online and offline learning.
Within this framework, they conduct experiments on 6 MRC datasets. The result shows that a QA model initialized with a small number of labeled training data can be improved by a large margin of accuracy after being fine-tuned with derived feedback data. When trained with full training sets in source domains, the QA model can also be adapted to target domains, where only user feedback is available.
The author has discussed related work on using interactive feedback for other NLP tasks. Although the author claims that their work is the first for QA, there is significant overlap between this work and Campos et al. (2020), which also uses binary user feedback to improve a Conversational QA model. But it is not cited in the paper.
References: Campos, Jon Ander, et al. ""Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning."" Proceedings of the 28th International Conference on Computational Linguistics. 2020. ","1. This work is well motivated by using human feedback to improve QA models. 
2. Although the employed baseline methods are kind of weak, the improvement is significant. 
3. The paper is well written and easy to follow. ","1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data? 
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail. 
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback. 
Moreover,  Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers.  3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. ",Line 277: “The may be attributed…” -> “This may be attributed… ,"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data? 
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail. 
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback. 
Moreover,  Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers.  3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. 
Line 277: “The may be attributed…” -> “This may be attributed… "
ARR_2022_67_review,"This paper investigates social biases in sense embedding. 
First, by either manually (WEAT) or automatically (WAT) assigning sense ids, this paper extends the previous datasets for examining word embedding biases to sense embedding.   Second, this paper proposes a new dataset for testing sense embedding biases (SSSB), by creating templates and filling them with target/attribute words. 
Third, using these datasets, this paper performs a set of experiments to test the social biases of sense embeddings. The results show that different senses of the same word bear different levels of biases. Moreover, even when the word embedding shows low biases, the biases of sense embeddings can be large. 
Generally, this paper introduces an important but not well-studied research question (i.e. to what extent are sense embeddings biased?), and proposes a framework for testing this (extends existing datasets and creates a new one). However, some choices of the experimental setup can be improved and some of the claims made are not convincing from the existing results. ","1. This paper introduces an important yet under-studied research question --- to measure the social biases in sense embeddings. This is very important, because understanding the senses of words is a fundamental problem in NLU, and sense embedding plays a vital role in this direction. 
2. This paper extends the existing datasets (WEAT and WAT) for testing social biases of (static and contextualized) word embedding to sense embeddings, by assigning sense ids to words, either manually (WEAT) or automatically (WAT). The assigning procedure is solid and can thus benefit the following studies.   3. This paper creates a new dataset (SSSB) to examine the biases of sense embeddings. It is very beneficial (as in 2), and most cases of the dataset are of good quality. 
4. Using these datasets, this paper examines the biases of sense embeddings and compares them with word embedding biases. This paper finds that likewise word embeddings, sense embeddings contain social biases as well. Moreover, this paper observes that different senses of the same word are of different bias levels, and they are also different with word embedding biases. ","1. Some claims in the paper lack enough groundings. For instance, in lines 246-249, ""This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet."" This claim will be justified if the authors can provide the specific bias scores and numbers of examples of each bias type, but I didn't find the corresponding part for analyzing this. Also, this paper mentions several times the intuition ""occupations and not actions associated with those occupations are related to gender, hence can encode social biases"" (lines 595-597). However, I don't really agree. Take ""engineer"" as an instance, in the Merriam-webster dictionary, the first meaning of ""engineer"" as a verb (https://www.merriam-webster.com/dictionary/engineer#:~:text=engineered%3B%20engineering%3B%20engineers,craft%20engineer%20a%20business%20deal) is ""to lay out, construct, or manage as an engineer"". I think it is very much biased towards the male gender as well, according to social conventions. 
2. Some analyses can be more detailed. For example, in ""language/nationality"", the data includes Japanese, Chinese, English, Arabic,  German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them. 
3. The definition of ""bias"" is debatable. In SSSB, a language that is ""difficult to learn/understand/write"" is considered to be stereotypical, and ""easy to learn/understand/write"" is anti-stereotypical. In daily conversations, I think it is not widely considered as ""biased"". Also, I don't really understand the meaning of ""<xxx language is hash>"". Do you mean ""harsh"" by ""hash""? If so, I think the conclusions derived from this dataset are less trustworthy. 
4. Writing can be improved. For example, even though I read very carefully, I am not sure I fully follow the method in lines 203-210. It will be nice if you can provide some examples for s(_i) and a(_j) 5. A question: why would you use Equation 7 to derive word embeddings? From your results, I assume the sense embeddings are not normalized. This will bring an issue: the embedding will be dominated by the sense with a larger length. To make the experiments more rigorous, I think it would be nice to also use pre-trained static word embeddings (e.g. skip-gram) and normalize the embedding. ","1. I think it would be more clear if you can introduce sense embeddings a bit before introducing the bias measuring procedures. 
2. A number of typos exist. Mostly, it doesn't influence the reading. However, sometimes it affects my understanding of the paper (e.g., again ""occupations and not actions associated with those occupations are related to gender"" (lines 595-596, and -> but, if I understand it correctly)). Another round of proofreading may be needed. ",2.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,I think the ethical concerns are well-addressed in Section 9. ,"1. Some claims in the paper lack enough groundings. For instance, in lines 246-249, ""This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet."" This claim will be justified if the authors can provide the specific bias scores and numbers of examples of each bias type, but I didn't find the corresponding part for analyzing this. Also, this paper mentions several times the intuition ""occupations and not actions associated with those occupations are related to gender, hence can encode social biases"" (lines 595-597). However, I don't really agree. Take ""engineer"" as an instance, in the Merriam-webster dictionary, the first meaning of ""engineer"" as a verb (https://www.merriam-webster.com/dictionary/engineer#:~:text=engineered%3B%20engineering%3B%20engineers,craft%20engineer%20a%20business%20deal) is ""to lay out, construct, or manage as an engineer"". I think it is very much biased towards the male gender as well, according to social conventions. 
2. Some analyses can be more detailed. For example, in ""language/nationality"", the data includes Japanese, Chinese, English, Arabic,  German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them. 
3. The definition of ""bias"" is debatable. In SSSB, a language that is ""difficult to learn/understand/write"" is considered to be stereotypical, and ""easy to learn/understand/write"" is anti-stereotypical. In daily conversations, I think it is not widely considered as ""biased"". Also, I don't really understand the meaning of ""<xxx language is hash>"". Do you mean ""harsh"" by ""hash""? If so, I think the conclusions derived from this dataset are less trustworthy. 
4. Writing can be improved. For example, even though I read very carefully, I am not sure I fully follow the method in lines 203-210. It will be nice if you can provide some examples for s(_i) and a(_j) 5. A question: why would you use Equation 7 to derive word embeddings? From your results, I assume the sense embeddings are not normalized. This will bring an issue: the embedding will be dominated by the sense with a larger length. To make the experiments more rigorous, I think it would be nice to also use pre-trained static word embeddings (e.g. skip-gram) and normalize the embedding. 
1. I think it would be more clear if you can introduce sense embeddings a bit before introducing the bias measuring procedures. 
2. A number of typos exist. Mostly, it doesn't influence the reading. However, sometimes it affects my understanding of the paper (e.g., again ""occupations and not actions associated with those occupations are related to gender"" (lines 595-596, and -> but, if I understand it correctly)). Another round of proofreading may be needed. "
ARR_2022_67_review,"This paper tackles the problem of measuring social biases in sense embeddings. It presents the new Sense-Sensitive Social Bias (SSSB) dataset and extends existing bias evaluation datasets (WEAT and WAT) to be compatible with word sense-level evaluation. It presents new evaluation approaches for measuring social biases in static and contextualized sense embeddings, and reports on experimental analysis of state-of-the-art sense embedding models under these evaluation schemes. The findings demonstrate that sense embeddings do not conform to the behavior of word embeddings with respect to social biases, and that sense embeddings often show higher degrees of bias under the presented evaluations. ","This is a strong paper. The text is well written and clearly structured, with a logical flow that makes it easy to follow the arguments. Related work is presented in appropriate detail and the paper is clearly situated with respect to existing work in computational analysis of social biases within the ACL community. The experiments are thorough and well-motivated, and the methodology is clear and reproducible. he analysis is well described and appropriate, and includes a thoughtful discussion of differences in observed behavior between word and sense embeddings on the various datasets evaluated. The three categories included in SSSB are well-motivated, and the occupation/action distinction is an especially clever choice. The tables and figures are (largely) appropriate and informative. ","The main weakness of this paper is that it is not sufficiently grounded in the sociolinguistic side of bias measurement, and therefore makes some significant claims and design decisions that are not entirely appropriate.
The proposed task and the experimental design are predicated on the assumption that word senses as semantically disjoint with respect to social biases, but this is not really true (which is actually nodded to in the paper's discussion section). References to nationality and language can often be used metonymically to imply one another (particularly for voicing negative opinions), and colour and race are most certainly strongly correlated in visual design and storytelling (eg the use of dark colours, especially black, for evil in Western European storytelling, which is less common in African stories). Occupations and associated actions, while not used metonymically, may still be correlated: it is likely higher surprisal to refer to a woman ""engineering a solution"" than a man, for example. ( It is worth noting that the maximum pairwise similarity approach used in WAT calculation runs counter to this assumption of disjoint senses.)
There is also an important distinction between observability of a bias (e.g., through surveying/prompting native speakers) and measuring it using computational tools. Social biases may be observable without being measured by current approaches. These nuances are important to discuss throughout the paper: social bias assessment is not a computational task alone, but must incorporate human judgment and be framed through specific human viewpoints with acknowledged limitations.
There are several further issues with the precision of the language used in the paper that are important to address when discussing a sensitive topic such as social bias measurement.
- The term ""bias"" itself needs some context. Biases may be positive or neutral (eg preferring to complete ""they sat at the computer to write ___"" with ""code"" vs ""a book"" when trained on different data), as well as negative. As such, it is not clear what it means to have an ""unbiased"" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.
- The term ""stereotype"" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of ""stereotype/anti-stereotype"" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, ""Japanese people are stupid"" is not a commonly-held belief, so this is not a stereotype.
- The terms ""pleasant"" and ""unpleasant"" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. "" Positive"" and ""negative"" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.
- Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.)
- Referring to nationality groups as ""disadvantaged groups"" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.)
It is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings.
Other issues with the presentation: - Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.
- Figures 2 and 3 are unreadable.
- P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results. ","- The inclusion of the verb sense of ""carpenter"" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.
- There is growing consensus to capitalize racial identifiers (certainly ""Black"", increasingly ""White"").
- Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.
- It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.
- It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.
- The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).
- It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.
- It would be interesting to see what human behavior is for these prompts/comparisons... - Dev et al 2019 and Nadeem et al 2020 references are missing publication information.
Typos - Figure 1, graph titles ""embembeddings"" - Line 051: extra ""is"" - Line 122: missing ""are"" - Line 603: ""bises"" -> ""biases"" - Line 610: missing ""being"" - Spaces not needed between section symbol and number - Spaces after non-terminal periods (""vs."", ""cf."") should be escaped to avoid spacing issues ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weakness of this paper is that it is not sufficiently grounded in the sociolinguistic side of bias measurement, and therefore makes some significant claims and design decisions that are not entirely appropriate.
The proposed task and the experimental design are predicated on the assumption that word senses as semantically disjoint with respect to social biases, but this is not really true (which is actually nodded to in the paper's discussion section). References to nationality and language can often be used metonymically to imply one another (particularly for voicing negative opinions), and colour and race are most certainly strongly correlated in visual design and storytelling (eg the use of dark colours, especially black, for evil in Western European storytelling, which is less common in African stories). Occupations and associated actions, while not used metonymically, may still be correlated: it is likely higher surprisal to refer to a woman ""engineering a solution"" than a man, for example. ( It is worth noting that the maximum pairwise similarity approach used in WAT calculation runs counter to this assumption of disjoint senses.)
There is also an important distinction between observability of a bias (e.g., through surveying/prompting native speakers) and measuring it using computational tools. Social biases may be observable without being measured by current approaches. These nuances are important to discuss throughout the paper: social bias assessment is not a computational task alone, but must incorporate human judgment and be framed through specific human viewpoints with acknowledged limitations.
There are several further issues with the precision of the language used in the paper that are important to address when discussing a sensitive topic such as social bias measurement.
- The term ""bias"" itself needs some context. Biases may be positive or neutral (eg preferring to complete ""they sat at the computer to write ___"" with ""code"" vs ""a book"" when trained on different data), as well as negative. As such, it is not clear what it means to have an ""unbiased"" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.
- The term ""stereotype"" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of ""stereotype/anti-stereotype"" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, ""Japanese people are stupid"" is not a commonly-held belief, so this is not a stereotype.
- The terms ""pleasant"" and ""unpleasant"" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. "" Positive"" and ""negative"" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.
- Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.)
- Referring to nationality groups as ""disadvantaged groups"" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.)
It is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings.
Other issues with the presentation: - Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.
- Figures 2 and 3 are unreadable.
- P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results. 
- The inclusion of the verb sense of ""carpenter"" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.
- There is growing consensus to capitalize racial identifiers (certainly ""Black"", increasingly ""White"").
- Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.
- It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.
- It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.
- The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).
- It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.
- It would be interesting to see what human behavior is for these prompts/comparisons... - Dev et al 2019 and Nadeem et al 2020 references are missing publication information.
Typos - Figure 1, graph titles ""embembeddings"" - Line 051: extra ""is"" - Line 122: missing ""are"" - Line 603: ""bises"" -> ""biases"" - Line 610: missing ""being"" - Spaces not needed between section symbol and number - Spaces after non-terminal periods (""vs."", ""cf."") should be escaped to avoid spacing issues "
ARR_2022_206_review,"The paper proposes a method to incorporate hierarchical structure information into text summarization tasks. The method encodes hierarchical structure information, such as section titles, sentence positions, and section positions. The method achieves SOTA Rouge performance for three public popular summarization datasets. ",The performance of the proposed model achieves SOTA Rouge performance on three public popular summarization datasets. ,"I am concerned about the novelty of the methods. This method simply encodes additional structure information into the Transformer models in a standard way. In this paper, both hierarchical position embedding and section title embedding are borrowed from other papers, so I would say the contribution of this paper is not substantial.
The paper has some unclear parts: (1) In the line 232, I think DPE is never defined. 
(2) Segment Embeddings in Figure 1 is not discussed. I believe it is not same to the segment embedding in BERT. 
(3) Regarding section title embedding, I believe section titles are not single vector, how to add it to BOS? ","Since this is a text generation task and automatic metrics (such as rouge) are not very reliable, I was wondering if you had a human evaluation result? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I am concerned about the novelty of the methods. This method simply encodes additional structure information into the Transformer models in a standard way. In this paper, both hierarchical position embedding and section title embedding are borrowed from other papers, so I would say the contribution of this paper is not substantial.
The paper has some unclear parts: (1) In the line 232, I think DPE is never defined. 
(2) Segment Embeddings in Figure 1 is not discussed. I believe it is not same to the segment embedding in BERT. 
(3) Regarding section title embedding, I believe section titles are not single vector, how to add it to BOS? 
Since this is a text generation task and automatic metrics (such as rouge) are not very reliable, I was wondering if you had a human evaluation result? "
ARR_2022_206_review,"This paper proposes to incorporate the hierarchical structure of source sentences into the extractive summarization system models. This hierarchy would allow for modeling inter-sentence relationships, which should be captured to improve the final system as the authors claim. Specifically, the authors propose to utilize hierarchical position (HP) and Section title (ST) of sentences to refine the output embeddings based on the inter sentential and sectional information. They run their model on three datasets, namely CNN/DM, arXiv, and Pubmed, showing their system staying competitive (in some cases outperforms model-wise baselines). ","While the idea of hierarchical modeling the input documents has a long history (Cohan et al., 2018), this paper has approached the problem with a novel way of utilizing section titles, as well as hierarchical position (HP) information of the sentences. 
The paper is well-structured and well-written in most parts. 
Promising results are reported when comparing with the baseline without injection of HiStruct information (e.g., longformer-base/large vs. HiStruct+ longformer-base/large and etc.). ","-Why did the authors decide to experiment with their model on CNN/DM? Hierarchical modeling of documents make sense on long documents such as arXiv and Pubmed. Is there any reason to indicate that it also work on shorter input documents such as CNN/DM…?
-In the analysis section, BERTSUMEXT has been compared with HiStruct+ (Roberta-base) with the claim that HiStruct+ improves the SOTA. Is it a fair comparison as the BERTSUMEXT and HiStruct+ (Roberta-base) do not use the same LMS. In other words, it is still unclear where the improvements of HiStruct+ (Roberta-base) vs. BERTSUMEXT are coming from? Are they because of the hierarchical info injection, or simply because of Roberta-base LM?
-The paper does not include the human evaluation process, which is an integral part of the summarization system setup. This gets, especially, higher priority as the results of the HiStruct+ model vs. baselines are close to each other. 
  -While results are promising when comparing HiStruct+ with the reproduced baselines (Table 2), the proposed model still does nor outperform the abstractive SOTA. ","The methodology section needs re-working and ideas could be better presented. 
Some important information such as the number of extracted oracle sentences should come directly under the experimental setup, not in the Appendix. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"-Why did the authors decide to experiment with their model on CNN/DM? Hierarchical modeling of documents make sense on long documents such as arXiv and Pubmed. Is there any reason to indicate that it also work on shorter input documents such as CNN/DM…?
-In the analysis section, BERTSUMEXT has been compared with HiStruct+ (Roberta-base) with the claim that HiStruct+ improves the SOTA. Is it a fair comparison as the BERTSUMEXT and HiStruct+ (Roberta-base) do not use the same LMS. In other words, it is still unclear where the improvements of HiStruct+ (Roberta-base) vs. BERTSUMEXT are coming from? Are they because of the hierarchical info injection, or simply because of Roberta-base LM?
-The paper does not include the human evaluation process, which is an integral part of the summarization system setup. This gets, especially, higher priority as the results of the HiStruct+ model vs. baselines are close to each other. 
  -While results are promising when comparing HiStruct+ with the reproduced baselines (Table 2), the proposed model still does nor outperform the abstractive SOTA. 
The methodology section needs re-working and ideas could be better presented. 
Some important information such as the number of extracted oracle sentences should come directly under the experimental setup, not in the Appendix. "
ARR_2022_209_review,"The paper proposes a new approach named BEEP to combine information from clinical notes and relevant medical literature to enhance the prediction of patient outcomes (prolonged mechanical ventilation, in-hospital mortality and length of stay). The medical literature is retrieved from PubMed and then ranked per relevance. The embeddings of clinical notes and top relevant literature are then combined to predict the patient outcomes on MIMIC-III datasets. Experiments show improved accuracy on 2 out of 3 tasks. ","- Overall well-written narratives with clear descriptions of methodology and experiments, though a lot of information is in the appendix which makes it a bit difficult to switch between main content and appendix.
- The idea of retrieving medical literature to enhance and provide evidence to patient outcome prediction is attractive. The proposed methods of retrieval and reranking are reasonable.
- Experiments are well established and clear. The proposed method BEEP shows advantages on PMV and MOR tasks. ","- The proposed method heavily relies on BERT-based encoders and BERT has a word limit of 512 tokens. But most discharge summaries in MIMIC-III have much more than 512 tokens. This may mean a lot of information in discharge summaries is truncated and the model may not be able to build a comprehensive representation of patient condition.
- The reliability and interoperability of the proposed method are in doubt based on Figure 3 which shows a high percentage of unhelpful literature is retrieved especially for the LOS task. How will such unhelpful literature impact patient outcomes? How can this be improved?
- The performance on LOS is not convincing and the paper does not provide much insight on why.
- The experiments do not seem to consider structured features at all (e.g. 17 clinical features from [1] based on MIMIC-III) which however are critical for patient outcome prediction from both clinical and ML perspectives [2]. The experiments may need a baseline that leverages structured features to show the advantage of using clinical notes and interpret BEEP's performance.
[1] https://www.nature.com/articles/s41597-019-0103-9 [2] https://arxiv.org/abs/2107.11665 ","- In the abstract and experiment section, expressions like ""5 points"" are confusing. "" 5% increase"" or ""0.05 increase"" would be clearer.
- In the abstract, what is ""increasing F1 by up to t points and precision @Top-K by a large margin of over 25%"" based on? The paper may make the statement clearer by mentioning the specific setup that achieves the largest improvement margin.
- Based on Appendix B, the bi-encoder is trained on TREC 2016 with 30 EHRs. The paper may discuss how representative these 30 EHRs are to MIMIC-III EHRs. Also as 30 is rather small, the paper may discuss whether it is enough empirically.
- Line 559, the paper may discuss why the model does not perform well on LOS, why a high percentage of unhelpful literature are retrieved (even for correct predictions) and how such a high percentage of unhelpful literature impact the reliability of the model.
- The paper may discuss why use MeSH rather than other ontologies like UMLS, SNOMED, HPO etc.
- Are those 8 categories in Appendix I mutually exclusive? ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",A section of ethical consideration will be needed given the nature of this work on clinical data. ,"- The proposed method heavily relies on BERT-based encoders and BERT has a word limit of 512 tokens. But most discharge summaries in MIMIC-III have much more than 512 tokens. This may mean a lot of information in discharge summaries is truncated and the model may not be able to build a comprehensive representation of patient condition.
- The reliability and interoperability of the proposed method are in doubt based on Figure 3 which shows a high percentage of unhelpful literature is retrieved especially for the LOS task. How will such unhelpful literature impact patient outcomes? How can this be improved?
- The performance on LOS is not convincing and the paper does not provide much insight on why.
- The experiments do not seem to consider structured features at all (e.g. 17 clinical features from [1] based on MIMIC-III) which however are critical for patient outcome prediction from both clinical and ML perspectives [2]. The experiments may need a baseline that leverages structured features to show the advantage of using clinical notes and interpret BEEP's performance.
[1] https://www.nature.com/articles/s41597-019-0103-9 [2] https://arxiv.org/abs/2107.11665 
- In the abstract and experiment section, expressions like ""5 points"" are confusing. "" 5% increase"" or ""0.05 increase"" would be clearer.
- In the abstract, what is ""increasing F1 by up to t points and precision @Top-K by a large margin of over 25%"" based on? The paper may make the statement clearer by mentioning the specific setup that achieves the largest improvement margin.
- Based on Appendix B, the bi-encoder is trained on TREC 2016 with 30 EHRs. The paper may discuss how representative these 30 EHRs are to MIMIC-III EHRs. Also as 30 is rather small, the paper may discuss whether it is enough empirically.
- Line 559, the paper may discuss why the model does not perform well on LOS, why a high percentage of unhelpful literature are retrieved (even for correct predictions) and how such a high percentage of unhelpful literature impact the reliability of the model.
- The paper may discuss why use MeSH rather than other ontologies like UMLS, SNOMED, HPO etc.
- Are those 8 categories in Appendix I mutually exclusive? "
ARR_2022_209_review,This manuscript presents a system that combines both clinical notes and patient-specific medical literature for clinical outcome prediction. The problem is important and interesting. The idea of incorporating patient-specific medical literature is novel. The evaluation of the proposed system is convincing. ,"A well-written paper with a novel idea. 
The evaluation is convincing. ",The technique contribution is not very sufficient as most components of the system use existing methods or tools. ,"As BERT is incapable of processing very long texts (more than 512 tokens) like clinical records [1], other text representation methods should be considered.
[1] Ding, M., Zhou, C., Yang, H. and Tang, J., 2020. Cogltx: Applying bert to long texts. Advances in Neural Information Processing Systems, 33, pp.12792-12804. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The technique contribution is not very sufficient as most components of the system use existing methods or tools. 
As BERT is incapable of processing very long texts (more than 512 tokens) like clinical records [1], other text representation methods should be considered.
[1] Ding, M., Zhou, C., Yang, H. and Tang, J., 2020. Cogltx: Applying bert to long texts. Advances in Neural Information Processing Systems, 33, pp.12792-12804. "
ARR_2022_209_review,"In this work, the authors propose a method to improve the prediction of clinical outcomes from clinical notes text by finding relevant evidence from documents in a database and augmenting the input. They mainly experiment with three prediction tasks: ventilation prediction, mortality prediction, and length of stay prediction and propose different ways to combine and fuse the evidence information from documents. Results show some improvements over simple baseline models that use only clinical text.
The central architecture of the paper's proposed approaches consists of a sparse-dense retriever and reranker to select relevant documents, and an aggregation strategy to encode the selected documents along with the clinical notes. While the model components are all previously explored, it is interesting to see their application in a clinical outcome prediction setting with notes.
The paper is clear in terms of the problem motivation and goals. However, there are some missing details from the explanation of the task setup and the results are slightly unconvincing due to there not being one clear approach (out of four averaging and voting strategies to combine documents). Furthermore, the performance scores are not as high as some other approaches in literature (Rajkomar et al.) that use structured clinical data such as ICD codes in addition to clinical notes. So the much higher additional effort to obtain evidence instead of relying on other structured data in a patient's EHR is not justified. ","1. The paper is clear in terms of problem motivation and goals. 
2. The experimental setup in terms of the variety of tasks chosen and the analysis is good. 
3. I like the human evaluation of the evidence analysis. ","1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date? 
2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice. ","1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients? 
2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients? 
3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures. 
4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature. 
5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric. ",2.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date? 
2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice. 
1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients? 
2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients? 
3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures. 
4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature. 
5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric. "
ARR_2022_209_review,"An interesting and thorough paper combining clinical text with relevant-to-patient medical literature to improve patient outcome predictions. An excellent and meaningful application of language models to a very important task, attempting to bridge the gap between corpus-based clinical models and Evidence Based Medicine. The work involves training models for Pubmed abstract retrieval based on the patient's admission note (the query); combining the text of the clinical note and the retrieved and re-ranked abstracts for patient outcome predictions, namely predicting prolonged mechanical ventilation, mortality, and length of stay. ","A well written and thorough paper. 
Extensive experiments. 
Promising results. 
Reproducible work: the code, MIMIC III cohort selection, paper identifiers, and models are publicly available at the time of submission (in an anonymous manner). ","The novelty of the approach is limited. 
The attempt to train the document retrieval and outcome prediction jointly is unsuccessful. ","The related work section needs to be expanded as it omits important prior work on both Patient-Specific Literature Retrieval and Clinical Outcome Prediction.
Your attempt to train the retrieval and outcome prediction jointly seems unsuccessful, have you considered experimenting with different approaches, such as  Retrieval-Augmented Language Model pre-training and fine-tuning?
There are too many references to the appendix in the paper. For example, it will be helpful to include more detail in section ""Learning To Retrieve Using Outcomes"", as opposed to referring to the appendix.  It will be helpful to include ethical concerns, detailing the limitations of the approach regarding practical, clinical application. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","As suggested to the authors, it will be helpful to include a comment detailing the limitations of the approach regarding practical, clinical application. ","The novelty of the approach is limited. 
The attempt to train the document retrieval and outcome prediction jointly is unsuccessful. 
The related work section needs to be expanded as it omits important prior work on both Patient-Specific Literature Retrieval and Clinical Outcome Prediction.
Your attempt to train the retrieval and outcome prediction jointly seems unsuccessful, have you considered experimenting with different approaches, such as  Retrieval-Augmented Language Model pre-training and fine-tuning?
There are too many references to the appendix in the paper. For example, it will be helpful to include more detail in section ""Learning To Retrieve Using Outcomes"", as opposed to referring to the appendix.  It will be helpful to include ethical concerns, detailing the limitations of the approach regarding practical, clinical application. "
ARR_2022_113_review,"This paper focuses generally on the task of diverse decoding from conditional language generation models. Specifically, they introduce the idea of best-first search, which unlike traditional beam search involves exploring the output space using a depth-first path completion strategy. As this results in many candidate outputs, this work also utilizes path recombination to combine similar hypotheses. The recombination methods generalize previous work to account for merge a hypothesis with candidates from earlier timesteps, and then propagates any merges back through the original lattice; these changes allow for significantly more merges to occur. The authors evaluate the performance of these methods on document summarization and machine translation. ","This paper is very well-written, and clearly articulates the issues with not only beam search, but BFS approaches generally. The methods proposed are relatively novel, and show to explore significantly more of the search space than beam search, and explores in a more principal way than random sampling-based methods. The results indicate that using these methods results in significantly more diversity in the output candidates, which based on the higher oracle ROUGE scores, has the potential to improve performance given an additional re-ranking mechanism. This seems like it could be a non-trivial step forward for decoding strategies.
The figures are very compelling and help visualize the mechanisms that differentiate the proposed methods from previous work. Figures 11-13 in particular are fascinating visualizations of the different paths, I greatly appreciate the authors including them in the appendix. In addition, the included error analysis is helpful in identifying and motivating future work, potentially on when to reduce the aggressiveness of path merging. ","The paper makes several significant claims about best-first search and recombination being much more efficient than beam search, but the discussions on actual run-time are lacking. In the appendix the authors note that the best-first search code could be optimized relatively easily, but one question that doesn’t seem to be answered is: compared to the baseline approaches, how much longer does it take to generate outputs, at least as the method is implemented now?
While the potential for significant improvement on quality scores is there due to the significantly higher portion of the search space explored, the authors don’t actually attempt any kind of re-ranking, which leaves an open question: are we able to actually extract the best output sequence from all the generated candidates? Did the authors try any simple re-ranking mechanisms to do this?
For evaluating diversity, one drawback of using distinct unigrams and bigrams is that it weights all n-grams that appear the same, which does not take into account the fact that infrequent n-grams the same. Thus, it would be good to also include Ent-n, and entropy-based metric introduced by Zhang et al (2018) that does account for this. ","One previous decoding method used the combine similar outputs was to over-generate candidates, and then perform post-decoding clustering to group similar candidates together (Ippolito et al., 2019). While the methods proposed in this paper are distinct, these methods should be mentioned in the related work.
Missing Citations: - Generating informative and diverse conversational responses via adversarial information maximization (Zhang et al., 2018) - Comparison of Diverse Decoding Methods from Conditional Language Models (Ippolito et al., 2019) ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The paper makes several significant claims about best-first search and recombination being much more efficient than beam search, but the discussions on actual run-time are lacking. In the appendix the authors note that the best-first search code could be optimized relatively easily, but one question that doesn’t seem to be answered is: compared to the baseline approaches, how much longer does it take to generate outputs, at least as the method is implemented now?
While the potential for significant improvement on quality scores is there due to the significantly higher portion of the search space explored, the authors don’t actually attempt any kind of re-ranking, which leaves an open question: are we able to actually extract the best output sequence from all the generated candidates? Did the authors try any simple re-ranking mechanisms to do this?
For evaluating diversity, one drawback of using distinct unigrams and bigrams is that it weights all n-grams that appear the same, which does not take into account the fact that infrequent n-grams the same. Thus, it would be good to also include Ent-n, and entropy-based metric introduced by Zhang et al (2018) that does account for this. 
One previous decoding method used the combine similar outputs was to over-generate candidates, and then perform post-decoding clustering to group similar candidates together (Ippolito et al., 2019). While the methods proposed in this paper are distinct, these methods should be mentioned in the related work.
Missing Citations: - Generating informative and diverse conversational responses via adversarial information maximization (Zhang et al., 2018) - Comparison of Diverse Decoding Methods from Conditional Language Models (Ippolito et al., 2019) "
ARR_2022_113_review,"This paper proposes to use the Best-first search algorithm to improve the Beam search method for generating diverse sentences. It constructs a lattice graph along with the hypothesis recombination method to shrink the size of the graph. In experiments, the authors compare with several baselines, showing the advantage of the proposed method. ",the paper is clearly written and good to follow. the experiments and analysis are impressive. ,"The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases. ","- Figure 2, it is not clear about ""merge target"". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, ""including that it optimizes for the wrong objective"". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work.  - In figure 5, the y-axis label may use ""Exact Match ratio"" directly.
- Line 409, could you cite the ""R2"" metric?
- Appendix A, the authors state ""better model score cannot result in better hypothesis"". You'd better state clearly what idea hypothesis you want. "" a near-optimal model score"" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph? ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases. 
- Figure 2, it is not clear about ""merge target"". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, ""including that it optimizes for the wrong objective"". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work.  - In figure 5, the y-axis label may use ""Exact Match ratio"" directly.
- Line 409, could you cite the ""R2"" metric?
- Appendix A, the authors state ""better model score cannot result in better hypothesis"". You'd better state clearly what idea hypothesis you want. "" a near-optimal model score"" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph? "
ARR_2022_113_review,This paper proposes a novel method to explore the search space of neural text generation models. The proposed method includes two key components of a modified best-first search and a path recombination mechanism. The authors conduct experiments on text summarization and machine translation tasks. The experiment results show that the proposed method generates massive-scale candidate sentences and obtain comparable or even better metric scores. ,"- The description of the proposed approach is clear and easy to follow.
- The paper presents a well-rounded set of experiments on text summarization and machine translation.
- The authors provide a lot of details in the appendix, which helps the reproducibility. ","- Although BFS is briefly introduced in Section 3, it's still uneasy to understand for people who have not studied the problem. More explanation is preferable. ","- Algorithm 1, line 11: the function s(·) should accept a single argument according to line 198.
- Figure 6: the font size is a little bit small. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"- Although BFS is briefly introduced in Section 3, it's still uneasy to understand for people who have not studied the problem. More explanation is preferable. 
- Algorithm 1, line 11: the function s(·) should accept a single argument according to line 198.
- Figure 6: the font size is a little bit small. "
ARR_2022_305_review,"The author proposed a novel detection approach that separates factual from non-factual hallucinations of entities. Empirical results suggest that this method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks. Furthermore, the author used this method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness. ","1) One of the main contributions of this paper is to construct a dataset annotating entity- or token-level hallucination.
2) The author proposed a method using an entity’s prior and posterior probabilities to infer whether it is hallucinated and factual, and the experiment shows that the results are much better than the baselines. ","1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. 
2) Lacking strong baselines both on entity-level factuality evaluation and summarization. ","1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? 
2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? 
3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? 
4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. 
2) Lacking strong baselines both on entity-level factuality evaluation and summarization. 
1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? 
2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? 
3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? 
4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). "
ARR_2022_305_review,"This work presents a study of factual hallucinations, wherein summarization models hallucinate information (i.e. generate information not present in the source document) which is still factually correct. An example of this would be background knowledge about the summarization topic. The main contributions are: (1) an annotation of a subset of the XSUM summarization dataset at the entity level, including both hallucination, and factuality (2) A simple, yet effective classification method for the labels of 1, using a KNN classifier on conditional and unconditional entity probabilities as well as presence in the source document (3) Use of their classifier to guide offline RL training of a summarizer to improve factuality. Authors find the classifier is effective and correlates well with humans, and the RL summarizer achieves better faithfulness/factuality than baselines while not significantly sacrificing ROUGE. ","From my perspective, the strongest aspect of the paper is the annotated dataset. The value of entity-level annotations for hallucination and factuality for XSUM will be very useful for future work, both for training new methods and allowing comparison across a common, fine-grained benchmark for factuality/hallucination classification. 
The paper also focuses on the notion of factual hallucination, which is often treated the same as non-factual hallucination in works on factuality. Whether or not I personally agree with the current feasibility of factual hallucinations to be useful in summarization systems, this is a question that warrants more discussion such as this work.   The experiments presented are quite extensive, spanning multiple datasets and generally demonstrating that the proposed methods are effective. 
The notion of negatively weighting non-factual entities in the offline RL setting is interesting, and a natural way to incorporate the classifier into training. ","Generally, I found the evaluations could have been stronger. First, there are only two classification baselines and both of them are rule-based, given a fixed set of simple features. The paper does not discuss in detail why a method using simple probability/overlap based features should in general be better than simply training a classifier on the available data (e.g. a linear layer on top of RoBERTa), nor does it compare to a trained baseline. If the point of the proposed method is that the selected features are particularly strong, then the authors should compare to other features (i.e. neural). If the point of the method is that KNN is a good choice for these features, then the authors should compare to other algorithms on the same features. I particularly focus on the evaluation of the classifier because later sections of the paper (i.e. the RL-based model) use the classifier as a sub-routine, and so demonstrating its efficacy is particularly important. 
Some minor concerns: - The work suggests that hallucinations can be described by hallucinated entities, which may be factual or not. I found this a bit unnatural, as the entities themselves are unlikely to be factual or non-factual in isolation. Rather, facts involving these entities can be classified either way. This may be something to elaborate on in future versions of the work.
- While I found the dataset to be useful, it is generated using one summarization model (based on BART) which may not well describe the kinds of mistakes that other models make, especially as new architectures/pretrained models are released in the coming months/years. As such, the dataset may have limited usefulness in the further future. ","As mentioned above, I would suggest including further baselines for the classification task. If it is financially feasible, I would also suggest a small-scale human evaluation of even a subset of the summarization systems. It is difficult to draw strong conclusions based only on automatic notions of factuality and quality, particularly when we are comparing the two. My understanding is that the goal of this work is to show that methods such as the one presented here can improve factuality and factuality in hallucination without significantly damaging the quality of the summarization model. Without at least some human judgement, it is difficult to judge whether or not this holds. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Generally, I found the evaluations could have been stronger. First, there are only two classification baselines and both of them are rule-based, given a fixed set of simple features. The paper does not discuss in detail why a method using simple probability/overlap based features should in general be better than simply training a classifier on the available data (e.g. a linear layer on top of RoBERTa), nor does it compare to a trained baseline. If the point of the proposed method is that the selected features are particularly strong, then the authors should compare to other features (i.e. neural). If the point of the method is that KNN is a good choice for these features, then the authors should compare to other algorithms on the same features. I particularly focus on the evaluation of the classifier because later sections of the paper (i.e. the RL-based model) use the classifier as a sub-routine, and so demonstrating its efficacy is particularly important. 
Some minor concerns: - The work suggests that hallucinations can be described by hallucinated entities, which may be factual or not. I found this a bit unnatural, as the entities themselves are unlikely to be factual or non-factual in isolation. Rather, facts involving these entities can be classified either way. This may be something to elaborate on in future versions of the work.
- While I found the dataset to be useful, it is generated using one summarization model (based on BART) which may not well describe the kinds of mistakes that other models make, especially as new architectures/pretrained models are released in the coming months/years. As such, the dataset may have limited usefulness in the further future. 
As mentioned above, I would suggest including further baselines for the classification task. If it is financially feasible, I would also suggest a small-scale human evaluation of even a subset of the summarization systems. It is difficult to draw strong conclusions based only on automatic notions of factuality and quality, particularly when we are comparing the two. My understanding is that the goal of this work is to show that methods such as the one presented here can improve factuality and factuality in hallucination without significantly damaging the quality of the summarization model. Without at least some human judgement, it is difficult to judge whether or not this holds. "
ARR_2022_264_review,"The paper tackles the known issue of multi-label hierarchical extreme classification, the scenario in which each item can be assigned to multiple classes from a large set of possible classes (in the order of thousands) which are hierarchically structured. The paper surveys existing metrics very thoroughly and then proceeds to define the formal desiderata for metrics designed for the targeted problem and evaluates existing metrics with respect to these properties. A new metric is proposed, ICM, which is based on information content and satisfies all desiderata but one (which is already an improvement with respect to existing metrics. The proposed metric is then evaluated on synthetic data and in a case study from the automatic encoding of discharge reports.  I really like the paper and I think should be accepted. More detailed feedback is nevertheless provided below. ","- Very clear and useful survey, accompanied by a formal definition and assessment of the desiderata for a proper evaluation metric - Novel metric proposed - Evaluation on both synthetic data and a real-world case study ","The case study does not provide very strong evidence for the superiority of ICM with respect to the other measures. A paper like this one, whose  ambition is to make a strong methodological contribution, should have a better coverage of real-world evaluation.  It would be made stronger if the authors could test their metric in one or two further domains, and extend the pool of evaluated classifiers to architecture in which the distribution of labels in the development set also plays a role. ","While defining the formal desiderata, probably for reasons of space, the authors do not provide enough intuitive description of what the target of the different principles is. I believe that the paper would be made much stronger if the authors would keep the somewhat didactic attitude that characterizes the overview. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The case study does not provide very strong evidence for the superiority of ICM with respect to the other measures. A paper like this one, whose  ambition is to make a strong methodological contribution, should have a better coverage of real-world evaluation.  It would be made stronger if the authors could test their metric in one or two further domains, and extend the pool of evaluated classifiers to architecture in which the distribution of labels in the development set also plays a role. 
While defining the formal desiderata, probably for reasons of space, the authors do not provide enough intuitive description of what the target of the different principles is. I believe that the paper would be made much stronger if the authors would keep the somewhat didactic attitude that characterizes the overview. "
ARR_2022_15_review,"***This is from Reviewer XYZ of the previous version.*** 
I have read other reviews, author responses, and the new version. I would like to keep my original score. 
---- This paper presents a lightweight method to train a controllable text generation model by learning small prefix vectors on top of a frozen LM such as GPT-2. The parameter-efficient prefix vectors are of the same design as the prefix-tuning paper (Li and Liang 2021), while this paper proposes to focus on using prefixes to steer the LM generation process according to given attributes & aspects, such as sentiment (pos./neg.) and topics (sports/world). The author(s) argue(s) that the proposed method is also the first work to explore multi-aspect control.
The prefixes can be learned either without attribute labels (via label variables; similar to the VQ-VAE method) or with attribute labels (via a margin loss). The experiments are conducted over multiple datasets for both single and multiple aspect control for text generation. Perplexity and attribute relevance are used for automatic evaluation; Attribute relevance and linguistic quality are two human-evaluation metrics. The results show the effectiveness of the proposed method, although there are still some limitations (e.g., tending to shift to movie reviews regardless of prompts). ","- The proposed method is a natural extension of prefix-tuning Li and Liang (2021), which focuses on parameter-efficiently learning of conditional NLG tasks (e.g., summarization). This paper, instead, focuses on the controllable free-text generation with given attributes.
- Benefiting from the parameter efficiency of Li and Liang (2021)'s work, the proposed method in this paper can mix multiple aspects for controlling. Also, the inference speed is maintained --- comparable to the original base model GPT-2.
- The experiments show the effectiveness of the proposed method in most cases, outperforming several baseline methods in automatic evaluation metrics. ","- The modeling of the mixture of multiple aspects is not explored deeper enough, and thus the so-called ""first to explore"" (in the Introduction) sounds more like an overclaim to me.
- The qualitative results (Table 5) and human evaluation both show that there are still limitations of the proposed method and the improvement is not so obvious.
- Given the arguments about the downstream applications (Line 116-117), I would like to see more downstream performance of such controllable text generation models. Think about how you would connect the proposed models to real NLG applications, and show the advantage of your model. I would say style transfer could be a good start. ",N/A. The authors have addressed most of my previous comments and suggestions. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The modeling of the mixture of multiple aspects is not explored deeper enough, and thus the so-called ""first to explore"" (in the Introduction) sounds more like an overclaim to me.
- The qualitative results (Table 5) and human evaluation both show that there are still limitations of the proposed method and the improvement is not so obvious.
- Given the arguments about the downstream applications (Line 116-117), I would like to see more downstream performance of such controllable text generation models. Think about how you would connect the proposed models to real NLG applications, and show the advantage of your model. I would say style transfer could be a good start. 
N/A. The authors have addressed most of my previous comments and suggestions. "
ARR_2022_15_review,"This paper propose prefix-based models for controllable text generation. Similar to [1], prefixes are token embeddings of language models (e.g., GPT-2) used for learning attribute-specific information and steering the generation of the fixed language models. The authors further add a contrastive loss to enhance the models' controllability. In addition, an unsupervised learning method is introduced to handle scenarios where labels are not available. The authors evaluated the proposed models on multiple controllable text generation tasks, such as controlling sentiment and topics. The experimental results show that comparing to baselines like PPLM and GeDi, the proposed model can achieve a good balance between fluency and controllability.
[1] Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021 ","- The proposed lightweight model achieved strong performance in multiple controllable text generation tasks.
- The idea of controlling language models in unsupervised way is interesting and new. ","- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.
- For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.
[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 ",Please consider adding new human evaluation results and baselines as mentioned in weaknesses. ,2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",Missing the discussion of potential misuses of language model controlling techniques. ,"- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.
- For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.
[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 
Please consider adding new human evaluation results and baselines as mentioned in weaknesses. "
ARR_2022_274_review,"Masked language models suffer from 2 big problems:  Overthinking - too much processing may hinder results for some instances. 
Long inference times - due to high rate of queries and amount of computation needed. 
To address these issues, adaptive inference has been suggested. In adaptive inference, classification layers are added to intermediate layers of the model. During inference, the model may choose to perform early exit, and make a prediction on a specific sample  without utilizing all of it’s processing power (i.e. all transformer blocks). 
Whether or not the model makes an early prediction is based on hyperparameters and architecture specific details. 
Using this approach helps deal with both issues at once. It prevents some samples from being over-processed, and allows the model to make a prediction before overfitting, and also speeds up inference, as many samples do not reach the final layer for a prediction.
In this work, the authors suggest a new way of deciding whether to perform early exit. This is a combination of 2 previous approaches which exploit the classifier’s output (as a confidence measure) and the agreement between previous classifiers. Each method has its advantages and drawbacks: Exiting based on a confidence threshold is dependent on a single layer’s output, making it prone to noisy decisions. However it is easy to control the model’s speed-accuracy tradeoff  in real time  by adjusting the confidence threshold. Exiting based on classifier consensus is not as easily adjustable, but provides more stable results. By combining the two methods, the writers utilize the advantages of each method while avoiding their flaws. 
  They define some threshold which determines whether a classifier is confident of it’s prediction. Additionally, they define a patience counter which counts how many consecutive classification layers have been confident in their prediction (based on the previous definition). The proposed model will perform an early exit based on layer K’s prediction if layer K is confident, and the patient counter is above a predetermined limit. 
This allows for robustness while still providing an easy real time control of the speed-accuracy tradeoff, and achieves SOTA results on most of the GLUE benchmark tasks. ","This paper’s topic is practical and can benefit both research and industrial usages.  Progressing on previous works, utilizing their strengths and overcoming drawbacks.
Improvements to SOTA maintained through various tasks and benchmarks.  Interesting ablation studies, strengthening the main claim of the paper.  applicable to other DL tasks. ","The paper is missing some implementation details. How many tests were run? Are the results an average of a few runs? If so, what is the STD of these results? How many epochs of training?  A similar but not identical issue: There is no mention of the computational budget\cost in this article.  A natural question to ask is how PCEE-BERT compares to a model that requires both patience and confidence, for which the patience would also be defined as consistent prediction (Similar to PABEE). It makes me wonder why this was not tried.  It is claimed that the improvement in results (compared to PABEE) stems from the fact that PCEE-BERT requires some measurement of confidence in order to perform an early exit. To be convinced of this being the main explanation, I would have liked to see the two models compared in a “budgeted exiting” scheme and getting comparable results. If you can not perform such experiments for some reason, maybe address this in a “future work” section.
What is the cost of using the PCEE-BERT? How does it affect training time? how many more parameters are added to the model? ","Training the BERT base model with multiple intermediate classification layers may impede the performance of each separate layer (a Bert model with only 3 layers, will perform better than the 3rd layer on a full model trained with multiple classification layers). This is because the optimizer is focused on improving a single classifier’s loss (instead of many). For this reason, I would like to see the PCEE-BERT compared to smaller models (instead of a large model with budgeted exit).  Using only the speedup ratio for diagnosing the behavior of the model is lacking important information. Which layers are the ones actually being used? A histogram of the number of examples classified by each layer may reveal interesting evidence on the model’s performance. May also allow better comparison between models.  Some of the results in the paper suggest the “overthinking problem” may not exist. First, I fail to see how figure 1 demonstrates the problem at all. What I understand from this plot is that the MCC (which is a way of measuring the model's success) gets higher with the number of layers, and the entropy (which is an inverse measurement of how sure the model is of it's prediction) gets lower with the number of layers. So it seems to me that  the more layers the model has, the better it performs on both measurements, although it is possible that I am missing something. If the overthinking problem did exist, I would expect the mcc to have a maximum at an intermediate layer. Second, I would expect the performance of the model to slightly increase when the speedup ratio is low (but not 0). It seems from the results that the best performance is still achieved by the largest model. Hence, It seems that using the largest possible model would still yield the best results (as opposed to what is claimed in the overthinking problem).
I am not a vision expert, but it seems to me that the ratio between added parameters of a classification layer and convolutional layers is much less significant than that of a simple FC layer and a transformer. Maybe the problem is actually non-existent in vision tasks?  It seems that adjusting PABEE through the patience parameter still provides a useful range for the S/R tradeoff. Can this not be done in real time? Am I missing something?  I fail to see how the ablation study of cross layer ensemble is relevant to this paper. You need to further explain why you are comparing PCEE-BERT to an ensemble model, and what insight did you gain from the results. In addition, the exact exit strategy of the model you are comparing PCEE-BERT to is unclear. Is it similar to a budgeted exit where you simply take the ensemble instead of the desired intermediate layer? or is it a PCEE-BERT in which when a decision to exit has been made, the ensemble is used instead of the intermediate layer?  Why is the loss structured that way? Why is it normalized? and why is it weighted? Is training layer 12 more important than training layer 4?  l2 remove “the”  l6 more widely = wider? forbids = blocks, prevents, prohibits, impedes? 
l14 enough numbers of consecutive = enough consecutive? 
l24 make achieve l29 have become or became l33 remove “and” l39 remove comma, add “and” l58 have no doubt (need a comma or without have) l65 much often (add more)  l81 drop “the” l89 process = handle, deal with? 
l135 exit’s = exit layer’s ? 
l158 block instead of black l159 ** same as l14 **  l189 ablation l331 missing “not”?  l24 make achieve l58 have no doubt (need a comma or without have) l65 much often l158 black->block l159 number of, you already said it once, I understand what you mean, but it is not the numbers that you want enough of. Maybe just drop it or rephrase. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,N/A ,"The paper is missing some implementation details. How many tests were run? Are the results an average of a few runs? If so, what is the STD of these results? How many epochs of training?  A similar but not identical issue: There is no mention of the computational budget\cost in this article.  A natural question to ask is how PCEE-BERT compares to a model that requires both patience and confidence, for which the patience would also be defined as consistent prediction (Similar to PABEE). It makes me wonder why this was not tried.  It is claimed that the improvement in results (compared to PABEE) stems from the fact that PCEE-BERT requires some measurement of confidence in order to perform an early exit. To be convinced of this being the main explanation, I would have liked to see the two models compared in a “budgeted exiting” scheme and getting comparable results. If you can not perform such experiments for some reason, maybe address this in a “future work” section.
What is the cost of using the PCEE-BERT? How does it affect training time? how many more parameters are added to the model? 
Training the BERT base model with multiple intermediate classification layers may impede the performance of each separate layer (a Bert model with only 3 layers, will perform better than the 3rd layer on a full model trained with multiple classification layers). This is because the optimizer is focused on improving a single classifier’s loss (instead of many). For this reason, I would like to see the PCEE-BERT compared to smaller models (instead of a large model with budgeted exit).  Using only the speedup ratio for diagnosing the behavior of the model is lacking important information. Which layers are the ones actually being used? A histogram of the number of examples classified by each layer may reveal interesting evidence on the model’s performance. May also allow better comparison between models.  Some of the results in the paper suggest the “overthinking problem” may not exist. First, I fail to see how figure 1 demonstrates the problem at all. What I understand from this plot is that the MCC (which is a way of measuring the model's success) gets higher with the number of layers, and the entropy (which is an inverse measurement of how sure the model is of it's prediction) gets lower with the number of layers. So it seems to me that  the more layers the model has, the better it performs on both measurements, although it is possible that I am missing something. If the overthinking problem did exist, I would expect the mcc to have a maximum at an intermediate layer. Second, I would expect the performance of the model to slightly increase when the speedup ratio is low (but not 0). It seems from the results that the best performance is still achieved by the largest model. Hence, It seems that using the largest possible model would still yield the best results (as opposed to what is claimed in the overthinking problem).
I am not a vision expert, but it seems to me that the ratio between added parameters of a classification layer and convolutional layers is much less significant than that of a simple FC layer and a transformer. Maybe the problem is actually non-existent in vision tasks?  It seems that adjusting PABEE through the patience parameter still provides a useful range for the S/R tradeoff. Can this not be done in real time? Am I missing something?  I fail to see how the ablation study of cross layer ensemble is relevant to this paper. You need to further explain why you are comparing PCEE-BERT to an ensemble model, and what insight did you gain from the results. In addition, the exact exit strategy of the model you are comparing PCEE-BERT to is unclear. Is it similar to a budgeted exit where you simply take the ensemble instead of the desired intermediate layer? or is it a PCEE-BERT in which when a decision to exit has been made, the ensemble is used instead of the intermediate layer?  Why is the loss structured that way? Why is it normalized? and why is it weighted? Is training layer 12 more important than training layer 4?  l2 remove “the”  l6 more widely = wider? forbids = blocks, prevents, prohibits, impedes? 
l14 enough numbers of consecutive = enough consecutive? 
l24 make achieve l29 have become or became l33 remove “and” l39 remove comma, add “and” l58 have no doubt (need a comma or without have) l65 much often (add more)  l81 drop “the” l89 process = handle, deal with? 
l135 exit’s = exit layer’s ? 
l158 block instead of black l159 ** same as l14 **  l189 ablation l331 missing “not”?  l24 make achieve l58 have no doubt (need a comma or without have) l65 much often l158 black->block l159 number of, you already said it once, I understand what you mean, but it is not the numbers that you want enough of. Maybe just drop it or rephrase. "
ARR_2022_274_review,"The authors present a method of adaptively truncating a BERT forward pass (or any deep-net with an iteratively-modified hidden vector a-la self-attention) at inference time in a classification setting; this method in theory allows a practitioner to trade off accuracy for latency: early-stopping takes fewer FLOPs but does slightly worse on average.
The model is, like the baselines compared to, adaptive, in the sense of allowing different test examples to use more or less computation as needed. The way it works is by training a per-layer classifier during finetuning for each layer; each hidden layer can be used to directly predict the label. Once a sufficient number of layers yield sufficiently low-entropy predictions, computation can be terminated (with both of these ""sufficiencies"" in the previous sentence corresponding to a different inference-time hyperparameter).
The classification tasks on GLUE are analyzed under this setting; the setup does a bit better than the competing adaptive-computation methods in the regime of heavily-truncated computations (only going through about 25% of the BERT's layers at test-time), and does comparably or marginally better, in general, than the other adaptive methods at less aggressive speedups.
The authors also demonstrate that the method is applicable to a ResNet trained on CIFAR to do vision classification.
Generally, the performance gains are not unambiguous and the settings in which this method is strongly preferred to other comparable systems could be clarified. I believe the comparison would be greatly strengthened by comparing directly to the more obvious way of reducing inference-time computation (namely, simply using a smaller fixed-sized model, comparable in FLOPs or wall-clock throughput to the adaptive large model).  I suspect the body of researchers interested directly in studying the performance-computation tradeoff of problems may find the work of interest---this method seems to consistently outperform the other comparable systems across a few of the tasks probed. ","- Comparison of this method to a number of directly comparable published adaptive computation methods on a number of baselines (GLUE tasks and a basic CIFAR-10/100 vision setup).
- Method outperforms competing methods in very-restricted-computation regime (that is, when very aggressively early-stopping inference forward-passes).
- Method does comparably or better than a few comparable systems for a few tasks across different accuracy/performance tradeoffs.
- The method is intuitive and the presentation was generally clear (except a few smaller points of clarification I outline below in ""suggestions""). ","- Not totally clear exactly when you'd want this scheme over the various other related early-stopping-adaptive-inference BERT setups. It seems like this setup generally works about the same as (or marginally better than) the others on many of the tasks (Table 2), and better in the regime of very-truncated computations (only doing a forward pass through 25% of the network on average). Could be clarified a bit if there are other clear differences in behavior beyond doing better in the very-aggressive-early-stopping regime.
- The main comparison I'd really like to see is a comparison to smaller fixed-size models, ideally of different sizes. If the point of the early-exiting program is to save computation, then the most straightforward to reduce computation is to try a smaller model (simpler than a large model that does adaptive inference-time things which are not present during learning). It's possible that just using fixed-size smaller BERTs will do better in the performance-versus-latency tradeoff. I don't think this is a rejection-worthy shortcoming but I think it would really improve the strength of the claims in the paper.
- I'm concerned by the speedup metric (this is not a rejection-level concern---I'm just arguing that they're using a proxy measurement without providing proof that it stands as a linearly-related proxy). The central results are task-specific-performance versus relative speedup afforded by the early stopping model. However, the speedup used as the x axis of the graphs is essentially the average percentage of layers the model goes through---that is, for a 50-layer BERT, if examples stop on average at layer 25, this will have a speedup ratio of 0.5. The authors assert ""according to our experiments, it is proportional to actual wall-clock runtime"". I think it's very important to show this, particularly because in the world of GPU usage and batching, there's often a weak coupling and non-linear relationship between the number of FLOPs required total (which is what this is) and the actual runtime (which depends on GPU utilization, which depends on the shape of the calculation, which depends on batching), and in particular this can vary from task-to-task. So this is a proxy measurement of efficiency (the ""efficiency"" we would actually care about in any setting is throughput or wall-clock time, rather than the number of flops required). The authors claim they observed a proportionality between the two, but the extent to which one actually stands as a proxy for the other (in particular, as a linearly-related proxy) seems very important to explicitly quantify and present. ","- There's two task-specific hyperparams to set to parameterize early stopping, aren't there, the $\tau$ param which binarizes the ""is this a low-entropy-enough"" decision, and the number of layers after which stopping occurs if the entropy's sufficiently low, I believe. These assumedly interact and each of them contribute to the speedup. How was this two-parameter decision turned into a single-parameter speedup? That is, the paper/system is motivated by there being a single knob to turn to control accuracy/computation tradeoffs, but this is two knobs, I think. Could be clarified in the ""experiments"" writeup.
- Generally, I found the motivations somewhat contrived (the part about keeping up with high demand for flu queries via adaptively scaling back latency during flu season, for example, or sec 3.1 saying that the fact that there's a discontinuity in the PABEE latency-accuracy tradeoff curve, etc). It's not at all clear that ""the accuracy on this GLUE task must be exactly 60%"", as given in 3.1 as a desideratum limiting industrial applicability of other techniques, ever makes sense. I thing it really suffices to say ""we are investigating the accuracy/latency tradeoff, motivated by the fact that some applications will require lower latency"" and leave it at that? I would expect most readers would recognize this to be one of the central tradeoffs across the entire field and acknowledge that it's useful as an object of study.
- Just to be clear -- the different early-existing classifiers $f^{(i)}()$, for $i=1,\ldots,m$ each get the training label during fine-tuning right (that is to say, each of them is trained to predict the exact same label as the others, conditioned on a different intermediate vector)? This could be stated more explicitly in subsecs 2.2 and 2.2.1 (I may have just missed it).
- What are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?
- hard to read table 2 because of its size. maybe if there's room in next revision or appendix turn each of these cols into a small fig like fig 3? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Not totally clear exactly when you'd want this scheme over the various other related early-stopping-adaptive-inference BERT setups. It seems like this setup generally works about the same as (or marginally better than) the others on many of the tasks (Table 2), and better in the regime of very-truncated computations (only doing a forward pass through 25% of the network on average). Could be clarified a bit if there are other clear differences in behavior beyond doing better in the very-aggressive-early-stopping regime.
- The main comparison I'd really like to see is a comparison to smaller fixed-size models, ideally of different sizes. If the point of the early-exiting program is to save computation, then the most straightforward to reduce computation is to try a smaller model (simpler than a large model that does adaptive inference-time things which are not present during learning). It's possible that just using fixed-size smaller BERTs will do better in the performance-versus-latency tradeoff. I don't think this is a rejection-worthy shortcoming but I think it would really improve the strength of the claims in the paper.
- I'm concerned by the speedup metric (this is not a rejection-level concern---I'm just arguing that they're using a proxy measurement without providing proof that it stands as a linearly-related proxy). The central results are task-specific-performance versus relative speedup afforded by the early stopping model. However, the speedup used as the x axis of the graphs is essentially the average percentage of layers the model goes through---that is, for a 50-layer BERT, if examples stop on average at layer 25, this will have a speedup ratio of 0.5. The authors assert ""according to our experiments, it is proportional to actual wall-clock runtime"". I think it's very important to show this, particularly because in the world of GPU usage and batching, there's often a weak coupling and non-linear relationship between the number of FLOPs required total (which is what this is) and the actual runtime (which depends on GPU utilization, which depends on the shape of the calculation, which depends on batching), and in particular this can vary from task-to-task. So this is a proxy measurement of efficiency (the ""efficiency"" we would actually care about in any setting is throughput or wall-clock time, rather than the number of flops required). The authors claim they observed a proportionality between the two, but the extent to which one actually stands as a proxy for the other (in particular, as a linearly-related proxy) seems very important to explicitly quantify and present. 
- There's two task-specific hyperparams to set to parameterize early stopping, aren't there, the $\tau$ param which binarizes the ""is this a low-entropy-enough"" decision, and the number of layers after which stopping occurs if the entropy's sufficiently low, I believe. These assumedly interact and each of them contribute to the speedup. How was this two-parameter decision turned into a single-parameter speedup? That is, the paper/system is motivated by there being a single knob to turn to control accuracy/computation tradeoffs, but this is two knobs, I think. Could be clarified in the ""experiments"" writeup.
- Generally, I found the motivations somewhat contrived (the part about keeping up with high demand for flu queries via adaptively scaling back latency during flu season, for example, or sec 3.1 saying that the fact that there's a discontinuity in the PABEE latency-accuracy tradeoff curve, etc). It's not at all clear that ""the accuracy on this GLUE task must be exactly 60%"", as given in 3.1 as a desideratum limiting industrial applicability of other techniques, ever makes sense. I thing it really suffices to say ""we are investigating the accuracy/latency tradeoff, motivated by the fact that some applications will require lower latency"" and leave it at that? I would expect most readers would recognize this to be one of the central tradeoffs across the entire field and acknowledge that it's useful as an object of study.
- Just to be clear -- the different early-existing classifiers $f^{(i)}()$, for $i=1,\ldots,m$ each get the training label during fine-tuning right (that is to say, each of them is trained to predict the exact same label as the others, conditioned on a different intermediate vector)? This could be stated more explicitly in subsecs 2.2 and 2.2.1 (I may have just missed it).
- What are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?
- hard to read table 2 because of its size. maybe if there's room in next revision or appendix turn each of these cols into a small fig like fig 3? "
ARR_2022_315_review,"In this paper, the author proposes a deep-learning based inductive logic reasoning method. The method firstly extracts query-related (candidate-related) information. Then, the method conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. ","1. Combining deep learning with ILP is interesting. The authors introduce a smooth connection between deep representation learning with logic reasoning by associating distributed representations with discrete logic predicates and their probabilistic evaluations. 
2. The research described in the paper seems technically sound and correct. ","1. The experiments are not convincing. For example, the authors use the development dataset to evaluate the results. Test data is not publicly available. The reason is insufficient. In addition, four different query relations is small. It is hard to convince the effectiveness of DILR. 
2. This work is not clearly written. For example, the second paragraph of section 5.1 should be rewritten and divided into more than one paragraph. ","1. What is DNNs, in Section 1 Introduction ? 
2. The rule is ""if A is in B and B is part of country C, then A is in country C"". How to solve the example by DILR? Specifically, how to process the example by Hierarchical Attentive Reader and Multi-hop Reasoner ? 
3. BERT is a strong baseline model from Table 1. Do you explain why DILR-BERT is stronger than BERT in detail? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The experiments are not convincing. For example, the authors use the development dataset to evaluate the results. Test data is not publicly available. The reason is insufficient. In addition, four different query relations is small. It is hard to convince the effectiveness of DILR. 
2. This work is not clearly written. For example, the second paragraph of section 5.1 should be rewritten and divided into more than one paragraph. 
1. What is DNNs, in Section 1 Introduction ? 
2. The rule is ""if A is in B and B is part of country C, then A is in country C"". How to solve the example by DILR? Specifically, how to process the example by Hierarchical Attentive Reader and Multi-hop Reasoner ? 
3. BERT is a strong baseline model from Table 1. Do you explain why DILR-BERT is stronger than BERT in detail? "
ARR_2022_315_review,"The authors present a neuro-symbolic approach to answer multi-hop questions by combining neural representations with differentiable ILP reasoning. They show the effectiveness of their approach on multiple-choice structured queries (Wikihop and Medhop dataset). The improvements are noteworthy, albeit on a narrow task. The authors have done a good job responding to previous reviews and making appropriate changes where possible. However, I think the system description is still very involved and make it hard to judge the technical contributions of their system. ",- A novel approach that combines deep-learning with logic(ILP)  - An attention-based approach to produce representations that do not rely on NER - Improved scores on WikiHop and MedHop subsets compared to prior work - Ablations show clear benefit of each part of their system ,"- Complicated system with a narrow scope (but that seems to be the current issue with other neuro-ILP work too)   - Complex attention mechanisms that are hard to interpret - The Complexity vs Accuracy tradeoff may not be worth it (e.g. BERT is a much much simpler model with a marginal drop in accuracy). The interpretability of the soft rules could be a potential benefit of the DILR approach, but it is still limited (no interpretation on relations, mostly attention-based entity chains) ","- #54: Min et al 2019 is a decomposition-based approach not a Neural Module Network - #55: Doesn't your attention-based approach also implicitly encodes relevant contexts into unnamed relations?
- #277: What is i and j here refer to? It is only introduced later on the next page. It would help if every term is clearly defined. I would even be okay if the equations are only spelled out in the appendix with the main paper only using some meaningful function names. E.g. B_ij = subj_context_attn(S, C) - It would help a lot in terms of understanding if the indices are not shared. E.g if ""i"" refers to a token in the subject, don't use it to also refer to the index in the answer candidate later. This will make the reader's life much easier since []_{ij} would then always refer to the attention between the subject and context. We don't have to carefully look around each equation to understand the meaning of each term.
- In 4.2, please think about what are the key ideas that you want the reader to know vs what are the key details needed for reproducibility. Try to move the latter to an appendix.  - The description of the BERT model is unclear. You get the representations for the query-subject OR the candidate? Shouldn't you be getting a representation for query-subject-candidate-context for each candidate and context? Or do you get multiple vector representations and concatenate them? 
  - Are all the models re-trained on the same subset? ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Complicated system with a narrow scope (but that seems to be the current issue with other neuro-ILP work too)   - Complex attention mechanisms that are hard to interpret - The Complexity vs Accuracy tradeoff may not be worth it (e.g. BERT is a much much simpler model with a marginal drop in accuracy). The interpretability of the soft rules could be a potential benefit of the DILR approach, but it is still limited (no interpretation on relations, mostly attention-based entity chains) 
- #54: Min et al 2019 is a decomposition-based approach not a Neural Module Network - #55: Doesn't your attention-based approach also implicitly encodes relevant contexts into unnamed relations?
- #277: What is i and j here refer to? It is only introduced later on the next page. It would help if every term is clearly defined. I would even be okay if the equations are only spelled out in the appendix with the main paper only using some meaningful function names. E.g. B_ij = subj_context_attn(S, C) - It would help a lot in terms of understanding if the indices are not shared. E.g if ""i"" refers to a token in the subject, don't use it to also refer to the index in the answer candidate later. This will make the reader's life much easier since []_{ij} would then always refer to the attention between the subject and context. We don't have to carefully look around each equation to understand the meaning of each term.
- In 4.2, please think about what are the key ideas that you want the reader to know vs what are the key details needed for reproducibility. Try to move the latter to an appendix.  - The description of the BERT model is unclear. You get the representations for the query-subject OR the candidate? Shouldn't you be getting a representation for query-subject-candidate-context for each candidate and context? Or do you get multiple vector representations and concatenate them? 
  - Are all the models re-trained on the same subset? "
ARR_2022_161_review,"This paper provides evidence for the hypothesis that learned representations in BERT and RoBERTa encode abstract information about argument structure. Evidence comes from two experimental methods loosely inspired by psycholinguistics: In experiment 1, inspired by the sentence sorting task, they find that similarity scores are higher for representations of sentences (i.e. an average of token embeddings) with the same argument structure (but different verbs) than for sentences with the same verb (but different argument structures). In experiment 2, inspired by jabbewocky priming studies, they find that representations of verbs placed into a novel (and usually ungrammatical or incoherent) syntactic frame have representations that more closely resemble verbs that canonically appear in that frame.  The authors argue that these results provide evidence for abstract constructions corresponding to different argument structures, as proposed in Construction Grammar. There is also evidence that abstractness of argument structure in representations increases as a function of the amount of pretraining: LMs with more pretraining data show a stronger tendency to group sentences by argument structure than by verb identity. ","Overall, I think this is a nice paper, with a few clear opportunities for improvement.
- The representation of argument structure is an underexplored topic in linguistic analysis of LMs in my opinion (considering its importance in linguistics), so this paper is helping to fill that gap.  - The results provide converging evidence that argument structure is encoded in contextual embeddings (though other interpretations remain possible).
- The experimental paradigms are simple (both rely on comparisons of Euclidean distance in an embedding space), but novel in their choice of which quantities to compare.
- The paper studies models trained both on different sized datasets and on different languages. This inspires greater confidence in the replicability of the results, and is useful from a cognitive modeling perspective. ","My main concern is that the experiments do nothing to distinguish between the main claim that argument structure constructions (a la construction grammar) are encoded in models, from an alternative explanation that surface features correlated with (but certainly not) argument structure constructions are what these similarity metrics are picking up on. This is a pernicious problem with template data, but a serious one. For instance, only ditransitive sentences had two proper names (or two animate nouns), only resultatives had adjectives, and only caused-motion sentences had the word ""the"" twice (or two inanimate nouns). Several more such features can be thought of that would identify the template. If the confluence of these features has a greater impact on the sentence representation than the lexical identity of these features, we would still see that sentences with the same argument structure are ranked more similar, but it would not be evidence that an abstract construction is represented, thereby undermining the main claim. ","Suggestions To address the problem of surface features, there should be a set of control stimuli that preserve many of the relevant surface features while eliminating the possibility of an argument structure construction. This could come in a few forms: - Shuffling the words/phrases in the stimuli - Interleaving the words/phrases of the stimuli with a random word list If LMs still rank altered sentences generated from the same template closer than sentences with the same verb, then it is clearly the function words, number of phrases of a certain type, or order of phrases that determines similarity.
Ln 346: This method for producing sentence embeddings is not immediately problematic, but still just one of many ways that could affect results. What about repeating this at different layers (including the final layer), or studying [CLS] in BERT? Or S-BERT, which is pretrained to produce meaningful sentence embeddings?
Ln 489: Using just the first subword embedding is just one arbitrary choice of several. What about the last subword embedding, an average of all subword embeddings?
Comments - Ln 41: I don’t think these works assume a generative framework. They just acknowledge contrasts in acceptability (which are historically used as evidence in generative grammar…and in other frameworks).
- Ln 139: This is an uncharitable misrepresentation of lexicalist theories. Most would not argue for multiple lexical entries for “sneeze”, but rather posit some kind of functional head or type shifter for introducing arguments.
- L445: This experiment makes sense, but it has nothing to do with priming (except for the human study it’s inspired by).  - Ln 545: This is an excellent point, and if you choose to save this for future work, then you should acknowledge this limitation up front and hedge all claims about ASCs Style - Ln 78: “Neural reality” sounds vague and sensationalist (also confusing, as it sounds like it’s about the human brain) - Ln 221: Avoid factives (show that) when discussing experimental results - Ln 335: These models don’t really simulate nonnative English speakers. Nonnative English speakers are fluent in a non-English language, while the miniBERTas are not fully trained on their “L1”.
Questions - Ln 487: Why not also test BERT, miniBERTas, and non-English models?
- Ln 496: How often is the congruent prototype the closest prototype for a particular Jabberwocky verb?
Other related work - Constructions in MLMs: https://aclanthology.org/2020.conll-1.13/ - Argument structure in NNs: https://aclanthology.org/W19-0129/ ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"My main concern is that the experiments do nothing to distinguish between the main claim that argument structure constructions (a la construction grammar) are encoded in models, from an alternative explanation that surface features correlated with (but certainly not) argument structure constructions are what these similarity metrics are picking up on. This is a pernicious problem with template data, but a serious one. For instance, only ditransitive sentences had two proper names (or two animate nouns), only resultatives had adjectives, and only caused-motion sentences had the word ""the"" twice (or two inanimate nouns). Several more such features can be thought of that would identify the template. If the confluence of these features has a greater impact on the sentence representation than the lexical identity of these features, we would still see that sentences with the same argument structure are ranked more similar, but it would not be evidence that an abstract construction is represented, thereby undermining the main claim. 
Suggestions To address the problem of surface features, there should be a set of control stimuli that preserve many of the relevant surface features while eliminating the possibility of an argument structure construction. This could come in a few forms: - Shuffling the words/phrases in the stimuli - Interleaving the words/phrases of the stimuli with a random word list If LMs still rank altered sentences generated from the same template closer than sentences with the same verb, then it is clearly the function words, number of phrases of a certain type, or order of phrases that determines similarity.
Ln 346: This method for producing sentence embeddings is not immediately problematic, but still just one of many ways that could affect results. What about repeating this at different layers (including the final layer), or studying [CLS] in BERT? Or S-BERT, which is pretrained to produce meaningful sentence embeddings?
Ln 489: Using just the first subword embedding is just one arbitrary choice of several. What about the last subword embedding, an average of all subword embeddings?
Comments - Ln 41: I don’t think these works assume a generative framework. They just acknowledge contrasts in acceptability (which are historically used as evidence in generative grammar…and in other frameworks).
- Ln 139: This is an uncharitable misrepresentation of lexicalist theories. Most would not argue for multiple lexical entries for “sneeze”, but rather posit some kind of functional head or type shifter for introducing arguments.
- L445: This experiment makes sense, but it has nothing to do with priming (except for the human study it’s inspired by).  - Ln 545: This is an excellent point, and if you choose to save this for future work, then you should acknowledge this limitation up front and hedge all claims about ASCs Style - Ln 78: “Neural reality” sounds vague and sensationalist (also confusing, as it sounds like it’s about the human brain) - Ln 221: Avoid factives (show that) when discussing experimental results - Ln 335: These models don’t really simulate nonnative English speakers. Nonnative English speakers are fluent in a non-English language, while the miniBERTas are not fully trained on their “L1”.
Questions - Ln 487: Why not also test BERT, miniBERTas, and non-English models?
- Ln 496: How often is the congruent prototype the closest prototype for a particular Jabberwocky verb?
Other related work - Constructions in MLMs: https://aclanthology.org/2020.conll-1.13/ - Argument structure in NNs: https://aclanthology.org/W19-0129/ "
ARR_2022_161_review,"The paper focuses on the topic of *argument structure constructions* (ASCs), an important topic in the framework of construction grammars. In construction grammar theory it is argued that the argument structure of a verb is not governed by the lexical entry of the verb itself, but by more abstract argument structure constructions that are independent of the verb. A rich body of work has addressed this topic and it has been demonstrated that humans process argument structure in a way that is explained well by the theory of ASCs.  The authors borrow several experiments from this body of (psycho)linguistic research to assess to what extent current language models comply with construction grammar theory. After a comprehensive background section they introduce a setup consisting of two experiments.  The first experiment is based on the sentence sorting task of Bencini and Goldberg (2020). In the original task 16 sentences are created by combining four verbs with four constructions, after which a participant is asked to sort them based on sentence meaning. If the sorting is done based on the construction type, and not on verb form, this is taken as evidence as abstract argument structure playing a more central role for sentence meaning than verb lexicality does. This paper applies this setup to LMs by clustering the 4x4 sentences based on their (averaged) sentence representations. It is found that all models that were considered (both mono- and multilingual) yield clusters that are closer to construction sorting than verb sorting. This is taken as first evidence of ASCs playing a role in LMs as well.
The second experiment is based on that of Johnson and Goldberg (2013). The original experiment utilized a priming-based task to probe for the reality of ASCs. A participant was provided a nonsense sentence of a certain construction, after which they were asked to determine whether a string of characters was a real English word or a non-word. It was shown that the response time for words that are considered prototypical for a certain construction type (e.g. _give_ for ditransitives) was significantly lower for congruent construction-word pairs. In this paper this setup is adapted by comparing the contextualised representation of a semantically implausible verb in a certain ASC with the averaged representation of a prototypical verb. By comparing the similarities of congruent and incongruent construction-verb pairs it can be shown that the argument structure is embedded within the representation of the nonsensical verb, and the authors find significant evidence for this to be the case. ","The paper is structured clearly and well written, making it easy to follow the line of argumentation. This paper provides an interesting angle to the recent research direction of applying theories derived from (psycho)linguistics to neural LMs, and manages to find intriguing evidence as to what type of abstract linguistic information is encoded in the representations of a language model.  I expect this paper will open the door for exciting future work that will gain more elaborate insights into the mechanisms deployed by LMs to deal with structural, abstract features of language. The authors show that the rich body of work within psycholinguistics can be directly applied to the current generation of LMs, and I look forward to seeing this direction be explored in more detail by future work. ","The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments.  The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas). ","### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.
- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,
- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity.  - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.
- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?
--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).
- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.
- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’.  - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already.  - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.
--- ### Typos/Style: Very well written paper, no remarks here. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments.  The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas). 
### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.
- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,
- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity.  - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.
- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?
--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).
- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.
- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’.  - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already.  - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.
--- ### Typos/Style: Very well written paper, no remarks here. "
ARR_2022_161_review,"The authors are presenting a paper on two multilingual probing tasks, e.g. sentence sorting and modeling of priming of Jabberwocky sentences, in order to test  the reality of argument constructions in the sentence embedding representations of modern pre-trained language models.  Several models trained on different amounts of data have been tested, with the goal of simulating different levels of proficiency in human speakers.
In the first sentence sorting experment, inspired by the experiment by Bencini and Goldberg (2000), the authors generate sentences by using verbs that are compatible with 4 different constructions and create sentence embeddings by averaging the token vectors. The results of vector clustering show that in all the tested languages sentences that share the same constructions are more similar than sentences sharing the same main verb, with the only exception of the LM model trained with the  smallest amount of data. Such a finding is certainly interesting as a construction-oriented preference in the original task was observed only in more proficient speakers.
For the Jabberwocky experiment, as a difference from the Johnson and Goldberg (2013) setting, sentences for each construction are generated by randomly filling real words in the templates. The contextual embeddings of such sentences are then compared to prototype embeddings of the constructions, which are obtained by averaging the verb embeddings of a prototypical verb for each construction. The authors found that for congruent prototype-Jabberwocky pairings distance are significantly lower, even in the scenario where construction prototypes are built by using relatively low frequency verbs.  The paper is overall well-structured and the experiments are clear and well-presented. Personally, I am not aware of any other work that probes contextualized word representations for argument constructions, and I appreciate that the authors take their inspiration directly from the psycholinguistic literature on the psychological reality of argument constructions. 
Although I would still have a couple of questions (see the section below), I feel I can say this work deserves publication. ","The paper makes an original contribution to the literature on probing the contextualized representations of pre-trained language models. 
The experimental hypotheses are clear and the experiments well-designed. ","No important weaknesses, as far as I can see. ","QUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using  standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?
- Section 5.1: ""... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding"". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?
TYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"
QUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using  standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?
- Section 5.1: ""... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding"". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?
TYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. "
ARR_2022_3_review,"This paper probes language-only and language-and-vision models for their knowledge about the visual properties of concrete concepts (i.e., the common colors associated with 'dog', 'banana', etc). 
The main question is shaped around the issue of reporting bias: do vision & language models (trained on visually grounded text & images) suffer less from reporting bias than text-only models (trained on wikipedia etc)? 
The probing experiments conclude that visual grounding can alleviate some of the reporting bias found in pretrained text-only models. ","- Reporting bias is an important problem to try to quantify, given NLP is   assuming that wikipedia trained models are good representations of the world. 
  The experiments in this paper are a reasonable first step towards this goal.
- Well written, easy to follow for the most part. ( It is hard to structure this   many experiments and models well.)
- Thorough analyses and breakdowns across attribute types and distributions;    this made the results much more interesting and useful. ","1, The paper conflates two separate questions: a, Does the visual information in multimodal models guard against reporting bias, and b, Does more-visually grounded text suffer less from reporting bias than less-visually grounded text? 
By only comparing pretrained Bert vs Oscar, the experiments cannot separate questions, and is in danger of drawing unmerited conclustions (E.g. L504 'concludes that ... images+text outperform text-only models': but how well does text-only Oscar perform here?). 
Disentangling them would require training a BERT model on Oscar's text data (maybe the model alluded to in preliminary experiments, L354?). 
I think the 'distilled' model is close to this, but the discussion around this model is unclear.
2, Oscar (and distilled Bert) are trained on COCO, which part of Visual Genome. 
There is therefore a 'training on test' problem here, which makes it difficult to separate the effect of adding the visual modality vs matching the test distribution much better than wikipedia. In fairness, Section 5.3 and Table 5 compare VG-trained Oscar with (probably) VG-free CLIP and don't see any large differences between the two, indicating that it's not the VG-in-training that is driving the differences between Bert and the multimodal models (and so more likely to be genre/domain differences, or the actual visual inputs, see point  1).
3, The paper is missing a discussion of a lot of early related work that used e.g. McRae norms; e.g. Silberer et al (2013) and references therein.  It would also be useful to check whether the McRae norms agreed with the extracted VG properties. 
The McRae dataset is obviously much smaller than the extracted VG dataset in this paper, but perhaps models do better (or show different patterns) for the more salient concepts included in McRae. ","The term ""common sense"" is (IMO) currently overused and underdefined in NLP. The discussion on L113-120, about ""visual commonsense"" vs ""seteroytpic visual attributes"" etc is helpfully explicit about the context of the term in this paper; I would still encourage reconsidering whether ""visual commonsense"" is the best terminology for this paper.
Do all the models use the same tokenisation? The 'mask first token' methodology (Footnote 4) would lead to better MLM results for models that use finer-grained/shorter subwords, if tokenisation is not controlled for.
Why does soft prompt tuning have such a huge effect on Color (~20-30) points but much less for the other attribute types?
Figure 2/451: I think the Vokenization model has the second highest correlation gap between wikipedia and coda, after BERT - if this is true (I didn't do the math), does this imply vokenisation doesn't work?
In Table 3, Column Color, Bert_L has the same values for Tune=N/Y. Is this a typo or a coincidence?
A.2 Table 9 is with or without tuning? 
In general the figures in the appendix could use more explanation (e.g. what are the color indices in Figures 8,9; what are the subjects in Figures 6,7). It would be helpful to show high/low Corr subjs (Table 11) for Single/Multi/Any subjects separately.
Section 4.2 (Elictation methods) includes paragraphs on soft promt tuning and knowledge distilation, which is confusing. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1, The paper conflates two separate questions: a, Does the visual information in multimodal models guard against reporting bias, and b, Does more-visually grounded text suffer less from reporting bias than less-visually grounded text? 
By only comparing pretrained Bert vs Oscar, the experiments cannot separate questions, and is in danger of drawing unmerited conclustions (E.g. L504 'concludes that ... images+text outperform text-only models': but how well does text-only Oscar perform here?). 
Disentangling them would require training a BERT model on Oscar's text data (maybe the model alluded to in preliminary experiments, L354?). 
I think the 'distilled' model is close to this, but the discussion around this model is unclear.
2, Oscar (and distilled Bert) are trained on COCO, which part of Visual Genome. 
There is therefore a 'training on test' problem here, which makes it difficult to separate the effect of adding the visual modality vs matching the test distribution much better than wikipedia. In fairness, Section 5.3 and Table 5 compare VG-trained Oscar with (probably) VG-free CLIP and don't see any large differences between the two, indicating that it's not the VG-in-training that is driving the differences between Bert and the multimodal models (and so more likely to be genre/domain differences, or the actual visual inputs, see point  1).
3, The paper is missing a discussion of a lot of early related work that used e.g. McRae norms; e.g. Silberer et al (2013) and references therein.  It would also be useful to check whether the McRae norms agreed with the extracted VG properties. 
The McRae dataset is obviously much smaller than the extracted VG dataset in this paper, but perhaps models do better (or show different patterns) for the more salient concepts included in McRae. 
The term ""common sense"" is (IMO) currently overused and underdefined in NLP. The discussion on L113-120, about ""visual commonsense"" vs ""seteroytpic visual attributes"" etc is helpfully explicit about the context of the term in this paper; I would still encourage reconsidering whether ""visual commonsense"" is the best terminology for this paper.
Do all the models use the same tokenisation? The 'mask first token' methodology (Footnote 4) would lead to better MLM results for models that use finer-grained/shorter subwords, if tokenisation is not controlled for.
Why does soft prompt tuning have such a huge effect on Color (~20-30) points but much less for the other attribute types?
Figure 2/451: I think the Vokenization model has the second highest correlation gap between wikipedia and coda, after BERT - if this is true (I didn't do the math), does this imply vokenisation doesn't work?
In Table 3, Column Color, Bert_L has the same values for Tune=N/Y. Is this a typo or a coincidence?
A.2 Table 9 is with or without tuning? 
In general the figures in the appendix could use more explanation (e.g. what are the color indices in Figures 8,9; what are the subjects in Figures 6,7). It would be helpful to show high/low Corr subjs (Table 11) for Single/Multi/Any subjects separately.
Section 4.2 (Elictation methods) includes paragraphs on soft promt tuning and knowledge distilation, which is confusing. "
ARR_2022_3_review,"This paper centers on commonsense knowledge probing and the issue of reporting bias. Authors construct a new dataset to probe the commonsense embedded in unimodal and multimodal language models with prompt tuning methods and discuss the performance discrepancy between the two groups of models. Besides, authors attempt to evaluate the correlation between the commonsense in language models and pre-training data such as Wikipedia and VG. They observe that although multimodal models are better storing visual commonsense, they are still subject to reporting bias. Also, knowledge distillation is able to preserve the commonsense when the models are compressed. ",The experiments are pretty comprehensive and the topic of the paper (reporting bias) is an intriguing and challenging issue for commonsense reasoning capacity of language models. ,"The writing could be further improved, including the structure organization and detailed concept clarification. It's also unclear why V&L models are able to store visual commonsense effectively (Is it really from the images or just from the captions?). ","1. It's better to add more vision-and-language models including VisualBERT and ViLBERT, since they are also pre-trained with MLM objective. 
2. It seems not reasonable to me for the formula in Adjective Projection part. If we want to estimate whether the object is large or small, we need to first calculate the similarity between ""large"" and the object, then ""small"" and the object, respectively. Then we should calculate the difference between the two similarity instead of using the similarity between the object embedding and the difference between ""large"" and ""small"" embeddings. 
3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues. 
4. Paper title is a bit overclaimed and also does not cover one of the main topics reporting bias. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"The writing could be further improved, including the structure organization and detailed concept clarification. It's also unclear why V&L models are able to store visual commonsense effectively (Is it really from the images or just from the captions?). 
1. It's better to add more vision-and-language models including VisualBERT and ViLBERT, since they are also pre-trained with MLM objective. 
2. It seems not reasonable to me for the formula in Adjective Projection part. If we want to estimate whether the object is large or small, we need to first calculate the similarity between ""large"" and the object, then ""small"" and the object, respectively. Then we should calculate the difference between the two similarity instead of using the similarity between the object embedding and the difference between ""large"" and ""small"" embeddings. 
3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues. 
4. Paper title is a bit overclaimed and also does not cover one of the main topics reporting bias. "
ARR_2022_77_review,"This paper presents the residue detection approach, which is specific to NLP adversarial attacks. The method leverages a linear classifier operating on the residue of sentence embedding to detect the adversary.
In the empirical evaluation, they detect substitution attacks on four classification datasets, and a targeted universal concatenation attack on a regression dataset. Results show the proposed method can outperform state-of-the-art detection approaches, such as FGWS, on most datasets. ","The proposed residue detection method is novel, which leverages the residue of sentence embeddings and feeds the residue into a linear classifier to identify adversarial attacks.  The study proposes two hypotheses of the residue properties in NLP adversarial samples. It further provides the rationale analysis and empirical evaluations to validate these properties.
The evaluations cover two types of adversarial attacks on five datasets. Besides, they conducted experiments on both NLP and image tasks. ","In section 3, some contents are basically introducing previous work, such as PGD, which are not closely related to the proposed method. I would suggest putting these parts into related work section.  Figure2 provides insightful findings on the residue properties. However, they are evaluated only on one dataset. It would be great to apply it to more datasets. ",Please see the suggestions and comments in the summary of weaknesses. ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,I have no ethical concerns related to this paper. ,"In section 3, some contents are basically introducing previous work, such as PGD, which are not closely related to the proposed method. I would suggest putting these parts into related work section.  Figure2 provides insightful findings on the residue properties. However, they are evaluated only on one dataset. It would be great to apply it to more datasets. 
Please see the suggestions and comments in the summary of weaknesses. "
ARR_2022_78_review,This paper propose to use an additional loss to automatically learn model confidence for NMT models. Experiments show that the learned confidence can better reflect the quality the model output and using the learned confidence can result in better BLEU score. ,"1. The work is clearly motivated, the authors aim to learn a better calibrated confidence for NMT models, which is useful in many practical scenarios. 
2. Experiments indicate the effectiveness of the learned confidence. ","1. Previous studies find that the mis-calibration problem is closely related with model size (Guo et al., 2019; Wang et al., 2020). In this work, the authors did not conduct experiments using larger NMT models, making the results less convincing. 
2. Table 1 shows that the proposed method only brings marginal improvements in terms of Pearson’s correlation. 
3. Table 2 shows that the learned confidence score can result in better BLEU score. To my knowledge, model confidence can affect the BLEU score with difference beam sizes. I wonder wether the proposed method can improve BLEU in larger beam size (i.e., beam size = 100). Moreover, in Table 2, the authors did not compare the proposed method with graduated LS (Wang et al., 2020), which is a closely related baseline. ","In Table 4, I wonder whether ""the model probability"" is estimated using MC dropout. Since MC dropout is very useful in OOD detection. Please provide more details. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Previous studies find that the mis-calibration problem is closely related with model size (Guo et al., 2019; Wang et al., 2020). In this work, the authors did not conduct experiments using larger NMT models, making the results less convincing. 
2. Table 1 shows that the proposed method only brings marginal improvements in terms of Pearson’s correlation. 
3. Table 2 shows that the learned confidence score can result in better BLEU score. To my knowledge, model confidence can affect the BLEU score with difference beam sizes. I wonder wether the proposed method can improve BLEU in larger beam size (i.e., beam size = 100). Moreover, in Table 2, the authors did not compare the proposed method with graduated LS (Wang et al., 2020), which is a closely related baseline. 
In Table 4, I wonder whether ""the model probability"" is estimated using MC dropout. Since MC dropout is very useful in OOD detection. Please provide more details. "
ARR_2022_78_review,"The authors propose to add a confidence estimating network, acting as a gating unit that allow ground-truth probability to be merged in to the decoder probability. The confidence network will produce a loss by adding up the gating values, which is combined with cross-entropy loss in training with a hyperparameter lambda. ","- In Table 1, Conf outperforms TP (model probability) for all language pairs.
- proposed an lambda annealing schedule to avoid training the confidence network as an finetuning step ","- In Table 1, although Conf outperforms TP, but the advantage of D-Conf comparing to D-TP is modest.  Given these numbers are Pearson's correlation, practically if the correlation cannot be improved by around 10% in absolute term, it's unlikely people will adopt it for estimating confidence scores given the extra cost on model training, inference cost and hyperparameter optimization.
- The hyper-parameter is crucial for the proposed loss in this paper. If lambda is too strong, then no confidence will be learned, c_t is always 1. If lambda is too small, perhaps the model will always produce c_t = 0 to peak the ground truth. However, critically, I was not able to find information about how the authors found/optimized the hyper-parameter. ","Many mistakes, for example Eq.6 does not have the summation. And also grammar/spelling mistakes. ",2.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- In Table 1, although Conf outperforms TP, but the advantage of D-Conf comparing to D-TP is modest.  Given these numbers are Pearson's correlation, practically if the correlation cannot be improved by around 10% in absolute term, it's unlikely people will adopt it for estimating confidence scores given the extra cost on model training, inference cost and hyperparameter optimization.
- The hyper-parameter is crucial for the proposed loss in this paper. If lambda is too strong, then no confidence will be learned, c_t is always 1. If lambda is too small, perhaps the model will always produce c_t = 0 to peak the ground truth. However, critically, I was not able to find information about how the authors found/optimized the hyper-parameter. 
Many mistakes, for example Eq.6 does not have the summation. And also grammar/spelling mistakes. "
ARR_2022_219_review,"The paper works on creating sentence embeddings targeting the semantic similarity (STS) task. It identifies a core issue in two previous approaches. The discrete augmentation approaches (Wu et al., 2020; Meng et al., 2021) sometimes distorts semantic meaning of sentences, creating positive pairs that do not indeed have the same meaning. For this, the paper purposes a stronger baseline based on semantic role labeling (SRL). On the other hand, the more promising continuous augmentation approaches (Carlsson et al., 2021; Gao et al., 2021) only have positive training examples that are embeddings that come from identical sentences, which introduce cues about sentence length. The paper then verifies this issue by showing a performance drop of one such method SimCSE when such cue is manually removed.  The paper then proposes a remedy to said issue by mapping the representations of both positive and negative examples to a pseudo sentence with the same length and structure. The proposed approach proves effective by achieving new state-of-the-art on several STS benchmarks. Analyses also show that the pseudo tokens play an important role. ","- The paper identifies an intuitive issue of the current SOTA method (i.e., Gao et al., 2021) that uses embeddings from the same sentence, though with different drop-out masks, as positive examples. Such issue of relying on sentence length is exposed by manually removing the cue of length from the dataset. This strongly motivates the paper's subsequent modeling endeavors.  - The paper proposes a model that is seemingly sound to address the previous issue.
- The paper shows through evaluation on a suite of benchmarks that the proposed method is superior. There are also adequate analysis showing that the core idea of the method, pseudo tokens, does play an important role. ","- The paper hypothesizes that SimCSE suffers from the cue of sentence length and syntax. However, the experiments only targets sentence length but not syntax.  - The writing of this paper can benefit by some work (see more below). Specifically, I find Section 3 difficult to understand as someone who does not directly work on this task. Specifically, a good mount of terminologies are introduced without explanations. I suggest a good effort of rewriting this section to be easier understandable by general NLP researchers.
- Though a universal issue in related papers and should not be blamed on the authors, why only consider BERT-base? It is known that other models such as BERT-large, RoBERTa, DeBERTa, etc. could produce better embeddings, and that the observations in these works do not necessarily hold in those larger and better models.  - The introduction of the SRL-based discrete augmentation approach (line 434 onwards) is unclear and cannot be possibly understood by readers without any experience in SRL. I suggest at least discussing the following:   - Intuitively why relying on semantic roles is better than work like CLEAR   - What SRL model you use   - What the sequence ""[ARG0, PRED, ARGM − NEG, ARG1]"" mean, and what these PropBank labels mean   - What is your reason of using this sequence as opposed to alternatives ","- (Line 3-6, and similarly in the Intro) The claim is a finding of the paper, so best prefix the sentence with ""We find that"". Or, if it has been discussed elsewhere, provide citations.  - (7): semantics-aware or semantically-aware - (9-10): explore -> exploit - (42): works on -> works by - Figure 1 caption: embeddings o the : typo - Figure 1 caption: ""In a realistic scenario, negative examples have the same length and structure, while positive examples act in the opposite way."" I don't think this is true. Positive or negative examples should have similar distribution of length and structure, so that they don't become a cue during inference.  - (99): the first mention of ""momentum-encoder"" in the paper body should immediately come with citation or explanation.  - (136): abbreviations like ""MoCo"" should not appear in the section header, since a reader might not know what it means.  - (153): what is a ""key""?
- (180): same -> the same - (186-198): I feel that this is a better paragraph describing existing issue and motivation than (63-76). I would suggest moving it to the Intro, and just briefly re-mention the issue here in Related Work.  - (245-246): could be -> should be - (248): a -> another - (252-55): Isn't it obvious that ""positive examples in SimCSE have the same length"", since SimCSE enocdes the same sentence differently as positive examples? How does this need ""Through careful observation""?
- (288): ""textual similarity"" should be ""sentence length and structure""? Because the models are predicting textual similarity, after all.
- (300-301): I don't understand ""unchangeable syntax"" - (310-314): I don't understand ""query"", ""key and value"". What do they mean here? Same for ""core"", ""pseudo syntactic"".  - (376): It might worth mentioning SimCSE is the state-of-the-art method mention in the Abstract.  - (392): Remove ""In this subsection"" ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"- The paper hypothesizes that SimCSE suffers from the cue of sentence length and syntax. However, the experiments only targets sentence length but not syntax.  - The writing of this paper can benefit by some work (see more below). Specifically, I find Section 3 difficult to understand as someone who does not directly work on this task. Specifically, a good mount of terminologies are introduced without explanations. I suggest a good effort of rewriting this section to be easier understandable by general NLP researchers.
- Though a universal issue in related papers and should not be blamed on the authors, why only consider BERT-base? It is known that other models such as BERT-large, RoBERTa, DeBERTa, etc. could produce better embeddings, and that the observations in these works do not necessarily hold in those larger and better models.  - The introduction of the SRL-based discrete augmentation approach (line 434 onwards) is unclear and cannot be possibly understood by readers without any experience in SRL. I suggest at least discussing the following:   - Intuitively why relying on semantic roles is better than work like CLEAR   - What SRL model you use   - What the sequence ""[ARG0, PRED, ARGM − NEG, ARG1]"" mean, and what these PropBank labels mean   - What is your reason of using this sequence as opposed to alternatives 
- (Line 3-6, and similarly in the Intro) The claim is a finding of the paper, so best prefix the sentence with ""We find that"". Or, if it has been discussed elsewhere, provide citations.  - (7): semantics-aware or semantically-aware - (9-10): explore -> exploit - (42): works on -> works by - Figure 1 caption: embeddings o the : typo - Figure 1 caption: ""In a realistic scenario, negative examples have the same length and structure, while positive examples act in the opposite way."" I don't think this is true. Positive or negative examples should have similar distribution of length and structure, so that they don't become a cue during inference.  - (99): the first mention of ""momentum-encoder"" in the paper body should immediately come with citation or explanation.  - (136): abbreviations like ""MoCo"" should not appear in the section header, since a reader might not know what it means.  - (153): what is a ""key""?
- (180): same -> the same - (186-198): I feel that this is a better paragraph describing existing issue and motivation than (63-76). I would suggest moving it to the Intro, and just briefly re-mention the issue here in Related Work.  - (245-246): could be -> should be - (248): a -> another - (252-55): Isn't it obvious that ""positive examples in SimCSE have the same length"", since SimCSE enocdes the same sentence differently as positive examples? How does this need ""Through careful observation""?
- (288): ""textual similarity"" should be ""sentence length and structure""? Because the models are predicting textual similarity, after all.
- (300-301): I don't understand ""unchangeable syntax"" - (310-314): I don't understand ""query"", ""key and value"". What do they mean here? Same for ""core"", ""pseudo syntactic"".  - (376): It might worth mentioning SimCSE is the state-of-the-art method mention in the Abstract.  - (392): Remove ""In this subsection"" "
ARR_2022_219_review,"This paper proposes to learn sentence embeddings with psudo tokens, aiming at addressing the sentence length bias issue of existing approaches that learn sentence embeddings based on contrastive loss. ","The idea of mapping the sentence onto a fixed number of pseudo tokens and then maps it back is very interesting. 
Experiments on STS task shows the proposed approach achieves good improvement over existing sentence embedding models. ","Regarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that ""Through careful observation we find that all positive examples have the same length .."".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by ""syntactic structures"".  It would be great to have some examples or more descriptions on what does ""syntactic structures"" referring to.
Regarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.
Regarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. ","Is the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.
This paper created a subset by filtering based on length of the positive/negative example pairs. How big is this new subset? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Regarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that ""Through careful observation we find that all positive examples have the same length .."".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by ""syntactic structures"".  It would be great to have some examples or more descriptions on what does ""syntactic structures"" referring to.
Regarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.
Regarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. 
Is the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.
This paper created a subset by filtering based on length of the positive/negative example pairs. How big is this new subset? "
ARR_2022_219_review,"This work presents PT-BERT, an unsupervised method for training sentence embeddings. The main claim is that  current contrastive learning-based methods, which are based on either discrete augmentation or continuous augmentation, do not consider the impact of superficial features such as sentence length and syntax. Thus the authors propose a method, where they incorporate pseudo embedding and momentum updating to further help the contrastive learning.
Experiments over multiple downstream tasks, mostly over semantic similarity benchmarks, demonstrate the effectiveness of the method, mostly compared to the recent SimCSE method. ","The idea of spotting the syntax and length “overfitting” of sentence embedding models is correct and in place. The problems of existing models that the authors presented are really crucial for better text embedding. It is novel and interesting and seems to be effective and it is really highly appreciated.
The authors apply their method over multiple state-of-the-art sentence encoding methods and show nice gains over many downstream tasks. They include also additional beneficial analyses at the end of the paper, such as alignment and uniformity measurements. ","The paper is written in a way that only readers that are familiar with MoCo can follow this work. For example, the authors write about a “dictionary”, and a “memory bank” without providing further explanations about how they use them and how they are composed. The paper should be more self-contained and is pretty hard to follow in its current form.
It is not clear to me how the pseudo sentences are actually generated. The authors write “During training, we 397 create a pseudo sentence {0, 1, 2, ..., 127} for every 398 input and map the original sentence to this pseudo sentence by attention”, but how exactly is a different pseudo embedding is created per sentence? According to Fig 2, all the pseudo sentences are the same, and I can’t see what is the motivation for that (In Fig 1 we can correctly see different sentences per instance). If there is a different pseudo sentence per sentence, then the mapping strategy is missing from the paper.
One discrete data augmentation ablation missing from the paper is the synonym/paraphrase replacement method (e.g., [1]) which matches the (correct) intuition of the authors, presented in Fig 1.
A comment about the architecture: There is neither intuition nor motivation as to why a single same attention layer should be used for both stages of obtaining the sentence embedding (Eq. 1 and 3). Also, an ablation regarding the architecture is missing, e.g., a number of layers, using different sets of parameters in Eq 1 and 3, etc., which could have been at least added to the appendix in case there is no space left.
Regarding the evaluations, an interesting missing evaluation is single-sentence classification tasks, such as sentiment analysis, which was carried out in similar papers (SimCSE). I encourage the authors to add such important analysis to their revised version of the paper. Also, there is no code available and hyperparameter choices are not depicted at all.
Lastly, the paper lacks some interesting/cherry-picked examples for sentences that can provide some insights regarding the method. A PCA/TSNE plot of the new embeddings compared to the previous embeddings can be informative to show the changes after post-processing.  [1] Jiao, Xiaoqi, et al. ""Tinybert: Distilling BERT for natural language understanding."" arXiv preprint arXiv:1909.10351 (2019).‏ ","- L96: There is an incorrect reference for the attention mechanism.
- L300: The presented sentence here is a series of numbers and not tokens. Also, it has a length of m+1 (please indicate whether the first token is the CLS). Moreover, in Fig. 2 the sentence starts with 1 and not with 0.
- L304: “the parameter” -> “each parameter” - L305: “The size of this layer depends on the length of pseudo sentences”- This sentence is vague and not clear at all. How can a neural network depend on the length of the input?
- L352: “obtained” -> “obtains” - L360: “is” -> “are” - L444: The SRL-guided data construction is not clear. Please provide an example.
- L464-L465: These are the asymptotic formulas, which are obviously not used for creating Fig 3. Please change to the concrete formulas, or at least state that. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"The paper is written in a way that only readers that are familiar with MoCo can follow this work. For example, the authors write about a “dictionary”, and a “memory bank” without providing further explanations about how they use them and how they are composed. The paper should be more self-contained and is pretty hard to follow in its current form.
It is not clear to me how the pseudo sentences are actually generated. The authors write “During training, we 397 create a pseudo sentence {0, 1, 2, ..., 127} for every 398 input and map the original sentence to this pseudo sentence by attention”, but how exactly is a different pseudo embedding is created per sentence? According to Fig 2, all the pseudo sentences are the same, and I can’t see what is the motivation for that (In Fig 1 we can correctly see different sentences per instance). If there is a different pseudo sentence per sentence, then the mapping strategy is missing from the paper.
One discrete data augmentation ablation missing from the paper is the synonym/paraphrase replacement method (e.g., [1]) which matches the (correct) intuition of the authors, presented in Fig 1.
A comment about the architecture: There is neither intuition nor motivation as to why a single same attention layer should be used for both stages of obtaining the sentence embedding (Eq. 1 and 3). Also, an ablation regarding the architecture is missing, e.g., a number of layers, using different sets of parameters in Eq 1 and 3, etc., which could have been at least added to the appendix in case there is no space left.
Regarding the evaluations, an interesting missing evaluation is single-sentence classification tasks, such as sentiment analysis, which was carried out in similar papers (SimCSE). I encourage the authors to add such important analysis to their revised version of the paper. Also, there is no code available and hyperparameter choices are not depicted at all.
Lastly, the paper lacks some interesting/cherry-picked examples for sentences that can provide some insights regarding the method. A PCA/TSNE plot of the new embeddings compared to the previous embeddings can be informative to show the changes after post-processing.  [1] Jiao, Xiaoqi, et al. ""Tinybert: Distilling BERT for natural language understanding."" arXiv preprint arXiv:1909.10351 (2019).‏ 
- L96: There is an incorrect reference for the attention mechanism.
- L300: The presented sentence here is a series of numbers and not tokens. Also, it has a length of m+1 (please indicate whether the first token is the CLS). Moreover, in Fig. 2 the sentence starts with 1 and not with 0.
- L304: “the parameter” -> “each parameter” - L305: “The size of this layer depends on the length of pseudo sentences”- This sentence is vague and not clear at all. How can a neural network depend on the length of the input?
- L352: “obtained” -> “obtains” - L360: “is” -> “are” - L444: The SRL-guided data construction is not clear. Please provide an example.
- L464-L465: These are the asymptotic formulas, which are obviously not used for creating Fig 3. Please change to the concrete formulas, or at least state that. "
ARR_2022_103_review,"My understanding of this paper is that they adapt a framework of “metamorphic relations” for use in probing an LM. This is a relation between inputs, follow up inputs based on the internal structure of those inputs, and outputs. The distinction between this and previous work is that they consider relations between inputs, rather than looking at single inputs.  The first experiment they run is applying a particular transformation, in the form of extra words, to a pair of texts. They then run a sentiment classifier and see whether that classifier maintains are ranking between the pair of sequences after their transformation. If it maintains a ranking, this indicates that the model is systematic. The next experiment also involves transformations in NLI inputs, which lead to specific changes in inference labels. Finally, they test if transitive relations of hypernymy and synonymity, when applied to pairs (x1,x2) and (x2,x3), also apply to (x1,x3). ",Overall I really like these complex rule based methods. I think that this is more principled and clearly considered than probing methods which use learned peripheral models. ,"I’m skeptical that systematicity requires that the same transformation always makes the same adjustment geometrically on the output. Consider the following examples:  “I just bought this new software.” 
“I just bought this new mascara.”
Now we append the same transformation to the text: “It doesn’t run!” We shouldn’t expect that this transformation is going to have the same effect on both reviews, and yet you would consider this to not be a systematic combination. A transformation that has different effects based on context is not necessarily a systematicity violation.
“we can conclude that pairwise-systematicity testing reveals a different issue in the model f than classic non-metamorphic testing.” Where is the classic testing that you would contrast this with to check whether this issue shows up?
In general, it seems like one advantage of this method *should* be avoiding the expense of gold standard labels that would be used for another kind of probe, but if instead you need to have a bunch of information about hypernyms and hyponyms, that doesn’t seem like a significant gain. ","This paper needs more examples of the kind of relation they are looking at, earlier in the paper as well as throughout the paper. Without examples, it’s hard to evaluate whether the transforming the source inputs with T is more expensive than using ground truth data.  Should the section on the geometry of hypernyms possibly have a citation to http://proceedings.mlr.press/v97/allen19a/allen19a.pdf ? I’m not necessarily pushing it but maybe you should check.
If you don’t have any further geometric analysis, it’s not clear to me that section 4.2 adds any information, as it seems to just be a set of definitions that are not later used.
I’m not clear on why Yanaka et al. wouldn’t be considered a metamorphic test as well? ( line 520).
I found several points where I was extremely confused. I don’t understand how you are avoiding having any kind of ground truth labels for things like hypernymy? If you don’t have hypernym information for a particular word set, how will you be able to know whether they are maintaining the output property at all? ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I’m skeptical that systematicity requires that the same transformation always makes the same adjustment geometrically on the output. Consider the following examples:  “I just bought this new software.” 
“I just bought this new mascara.”
Now we append the same transformation to the text: “It doesn’t run!” We shouldn’t expect that this transformation is going to have the same effect on both reviews, and yet you would consider this to not be a systematic combination. A transformation that has different effects based on context is not necessarily a systematicity violation.
“we can conclude that pairwise-systematicity testing reveals a different issue in the model f than classic non-metamorphic testing.” Where is the classic testing that you would contrast this with to check whether this issue shows up?
In general, it seems like one advantage of this method *should* be avoiding the expense of gold standard labels that would be used for another kind of probe, but if instead you need to have a bunch of information about hypernyms and hyponyms, that doesn’t seem like a significant gain. 
This paper needs more examples of the kind of relation they are looking at, earlier in the paper as well as throughout the paper. Without examples, it’s hard to evaluate whether the transforming the source inputs with T is more expensive than using ground truth data.  Should the section on the geometry of hypernyms possibly have a citation to http://proceedings.mlr.press/v97/allen19a/allen19a.pdf ? I’m not necessarily pushing it but maybe you should check.
If you don’t have any further geometric analysis, it’s not clear to me that section 4.2 adds any information, as it seems to just be a set of definitions that are not later used.
I’m not clear on why Yanaka et al. wouldn’t be considered a metamorphic test as well? ( line 520).
I found several points where I was extremely confused. I don’t understand how you are avoiding having any kind of ground truth labels for things like hypernymy? If you don’t have hypernym information for a particular word set, how will you be able to know whether they are maintaining the output property at all? "
ARR_2022_103_review,"This paper proposes using metamorphic testing to verify the behavior of NLP models. One appeal of this approach is that it can be used in some cases without collecting any training data, verifying that the outputs obey some desired relation between inputs, rather than being “correct” in the sense that they match the ground truth. The authors outline a general metamorphic testing framework (with a useful graphical model formalism to describe different variants of it), and show how most past work falls narrowly into the category of robustness tests. A robustness test is where a transformation is made to the input, and then the test verifies that some property (e.g., label invariance) holds between the original and modified inputs. In contrast, they propose three different extensions of this approach beyond robustness for systematicity, compositionality, and transitivity. Systematicity testing essentially generalizes robustness testing to take a model’s performance on related instances as a ground truth. Compositionality testing as formulated here is testing some level of faithfulness: does the representation of some semantic property in the model encode whether or not the model outputs reflect that property? Finally, transitivity testing tests whether relations between inputs that should be transitive obey transitivity. Overall, I find the general framework and specific results to be valuable contributions. ","1. The paper is well written and organized. The framework seems like a useful high-level contribution for testing NLP models, and the graphical model formalism seems like a useful formalism for visualizing and comparing different testing approaches. 
2. The systematicity testing is an interesting novel evaluation paradigm. I did not expect out-of-the-box NLP models to perform so well on this (+90%). 
3. The compositionality test also seems useful, and it is probably easier to interpret the numbers coming out of it in absolute terms compared to probing, since we should expect a strong model to achieve 100% performance. 
4. It is interesting that the transitivity test – which formalizes whether the model predictions satisfy transitivity when they should (independent of whether they are correct) suggests that models are highly non-transitive. This result is a valuable contribution, especially given that more primitive fully supervised transitivity tests have reached the opposite conclusion. ","1. The compositionality and transitivity tests require labeled data to train the probes, which defeats some of the utility of the method. 
2. Compositionality typically means something like “the meaning of a full expression can be computed recursively as a function of its parts, where the structure of the recursion is modulated by the syntactic structure.” I don’t think that’s exactly what your “compositionality” test is formalizing, though it is somewhat related. I would suggest changing the name to something more precise; perhaps “faithfulness”, since what you’re really testing is the correspondence between the relations the model encodes and the relations that exist in the output? Or maybe I’m missing something, and you can elaborate on how this test reflects a more general definition of compositionality. 
3. There is an inconsistency in the presentation of the systematicity test. In Equation 4, the test checks whether Psrc => Pflw. Later on, at line 340: this is condition is written as being bidirectional: Psrc ⇔ Pflw. Which one of these is right? 
4. It's not obvious how humans would do under this kind of linguistic evaluation. It would be interesting to do human evaluations of this method. In other words, use human annotators in the role of the model, and see how well they do compared to neural models as a baseline. ","- What is the training data for your probes in the compositionality and transitivity experiments?
- For the transitivity experiments, does 50% constitute a random baseline? If so, do you have any potential explanation why models are so substantially undershooting random guessing?
- The notation of x1 and x2 vs. x = (xa, xb) in the compositionality section is a bit confusing, especially when x1 and x2 are already themselves pairs. I would suggest changing this. Maybe you could have x and x’, or rewrite xa as x_{11} and xb as x_{12}. It was unclear whether x_1 was a pair or single sentence on my first read.
- Nit: In Def 2.2, is v a constant across all R, or can we pick a different v for each R? This is only a technical concern when the number of examples is countably infinite.
- Line 405: This last sentence seems a bit circular. You’ve defined systematicity such that the geometric property is satisfied if and only if f is systematic. If this is all you mean, I think the sentence could be clarified, but currently, it seems like it may be making a stronger claim than this.  - Line 530: Use \citep not \citet ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1. The compositionality and transitivity tests require labeled data to train the probes, which defeats some of the utility of the method. 
2. Compositionality typically means something like “the meaning of a full expression can be computed recursively as a function of its parts, where the structure of the recursion is modulated by the syntactic structure.” I don’t think that’s exactly what your “compositionality” test is formalizing, though it is somewhat related. I would suggest changing the name to something more precise; perhaps “faithfulness”, since what you’re really testing is the correspondence between the relations the model encodes and the relations that exist in the output? Or maybe I’m missing something, and you can elaborate on how this test reflects a more general definition of compositionality. 
3. There is an inconsistency in the presentation of the systematicity test. In Equation 4, the test checks whether Psrc => Pflw. Later on, at line 340: this is condition is written as being bidirectional: Psrc ⇔ Pflw. Which one of these is right? 
4. It's not obvious how humans would do under this kind of linguistic evaluation. It would be interesting to do human evaluations of this method. In other words, use human annotators in the role of the model, and see how well they do compared to neural models as a baseline. 
- What is the training data for your probes in the compositionality and transitivity experiments?
- For the transitivity experiments, does 50% constitute a random baseline? If so, do you have any potential explanation why models are so substantially undershooting random guessing?
- The notation of x1 and x2 vs. x = (xa, xb) in the compositionality section is a bit confusing, especially when x1 and x2 are already themselves pairs. I would suggest changing this. Maybe you could have x and x’, or rewrite xa as x_{11} and xb as x_{12}. It was unclear whether x_1 was a pair or single sentence on my first read.
- Nit: In Def 2.2, is v a constant across all R, or can we pick a different v for each R? This is only a technical concern when the number of examples is countably infinite.
- Line 405: This last sentence seems a bit circular. You’ve defined systematicity such that the geometric property is satisfied if and only if f is systematic. If this is all you mean, I think the sentence could be clarified, but currently, it seems like it may be making a stronger claim than this.  - Line 530: Use \citep not \citet "
ARR_2022_103_review,"This paper uses metamorphic testing to check the safety of neural NLP models and proposes three new classes of metamorphic relations to test the NLP model's systematicity, compositionally, and transitivity. The method can increase the number of test cases by a polynomial factor. The paper also proposes to use a graphical notation to summarize the inner structure of metamorphic relations. ",The proposed method is flexible and can yield a much larger number of test cases than existing methods. ,"The biggest concern regarding this paper is the lack of human verification. That is treating humans (or a group of annotators) as a black-box (same as neural network model) and checking whether the proposed transformation indeed produces valid comparison. The transformation is all based on intuitions and without human validation,  it's hard to determine whether it will be valid and therefore we do not know whether the satisfy value is indeed meaningful. ","In the introduction, the paper says ""reliance on ground truth data limits the quantity and quality of test cases we can produce"". In my opinion, ground truth data will increase the quality of the test cases. I was wondering why the author believes that annotation limits the quality of the test cases. ",2.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"The biggest concern regarding this paper is the lack of human verification. That is treating humans (or a group of annotators) as a black-box (same as neural network model) and checking whether the proposed transformation indeed produces valid comparison. The transformation is all based on intuitions and without human validation,  it's hard to determine whether it will be valid and therefore we do not know whether the satisfy value is indeed meaningful. 
In the introduction, the paper says ""reliance on ground truth data limits the quantity and quality of test cases we can produce"". In my opinion, ground truth data will increase the quality of the test cases. I was wondering why the author believes that annotation limits the quality of the test cases. "
ARR_2022_224_review,"This paper concerns compounds and elaborate expressions (EEs) in Hmong, Lahu, and Chinese. An elaborate expression is an expression ABAC (A B1 A B2 in the paper but i prefer ABAC to indicate the identity of the words so swill use that) like “people pile people lump” fo ra throng of people. It starts with the prior observation that there is a phonological hierarchy that dictates the ordering of the B and C words (or for compounds the A and B words). It then fits SVMs/decision trees/etc to see if you can predict the ordering based on phonology, which is successful as a classifier.  It then looks to see if you can predict (out of sample) based on a neural embedding model. This classifier also works very well. ","It is nice to see a paper investigating a specific linguistic phenomenon of theoretical interest in a non-English, non-Indo-European language. There seems to be a rich prior literature on EEs in Hmong, Lahu, and Chinese and this paper contributes to that literature. ","The paper is framed as pitting, to some extent, distributional information against the phonological hierarchy. But these kinds of phonological constraints tend to be soft and violable. So I was not convinced by the takeaway of the first set of experiments using the phonological classifiers. What does this add on top of just examining the distributional patterns of phonological features in the compounds and EEs?  The motivation for the neural experiment was also a bit confusing to me. Essentially, it shows that you can predict the ordering of B and C in ABAC based on the training data, because B and C occur in that same ordering elsewhere in the corpus. This doesn’t seem particularly informative as a result.
As the paper points out, there seem to be two routes to learning this information: (soft?) phonological constraints and the distributions in texts. But I’m not fully convinced that this work breaks ground theoretically or methodologically. ","Might want to check out work by Emily Morgan and Roger Levy on binomial pairs, predicting the ordering of items in a phrase like “peanut butter and jelly”. ","2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,NA ,"The paper is framed as pitting, to some extent, distributional information against the phonological hierarchy. But these kinds of phonological constraints tend to be soft and violable. So I was not convinced by the takeaway of the first set of experiments using the phonological classifiers. What does this add on top of just examining the distributional patterns of phonological features in the compounds and EEs?  The motivation for the neural experiment was also a bit confusing to me. Essentially, it shows that you can predict the ordering of B and C in ABAC based on the training data, because B and C occur in that same ordering elsewhere in the corpus. This doesn’t seem particularly informative as a result.
As the paper points out, there seem to be two routes to learning this information: (soft?) phonological constraints and the distributions in texts. But I’m not fully convinced that this work breaks ground theoretically or methodologically. 
Might want to check out work by Emily Morgan and Roger Levy on binomial pairs, predicting the ordering of items in a phrase like “peanut butter and jelly”. "
ARR_2022_345_review,"This paper proposes a novel unsupervised sentence embedding framework named DiffCSE. DiffCSE learns sentence embeddings that are aware of, but not necessarily invariant to, MLM-based word replacement. Technically, DiffCSE combines the standard contrastive objective from SimCSE with a difference-prediction objective conditioned on the sentence embedding. Experiments on STS datasets and transfer tasks show the effectiveness of DiffCSE. ","This paper is excellent in terms of both writing and technical novelty.
- Writing: This paper provides sufficient background and includes a comprehensive survey of existing contrastive learning-based sentence embedding frameworks. I was able to read this paper very smoothly.
- Novelty: The proposed framework is very simple but not naive. It is based on the idea of equivariant contrastive learning, which is first brought to NLP by the authors in the present paper, to my knowledge. The simple yet effective framework should inspire many readers. Besides, the authors confirmed the necessity of the components of the proposed method through thorough ablation studies. The thoroughness is almost unparalleled as a conference paper. ",I was not able to find any major concerns. Some minor comments are written in the next section. ,"- I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?
- It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed. ","5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.",Maybe,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"I was not able to find any major concerns. Some minor comments are written in the next section. 
- I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?
- It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed. "
ARR_2022_292_review,"This paper describes a contrastive learning approach to automatically solving math word problems (MWP) and investigates multilingual approaches to the problem. Additionally, it provides further evidence that the top layers of BERT will learn task-specific patterns, as shown in prior works.  This paper treats MWP solving as a text-to-equation-tree translation problem using an encoder-decoder architecture. To motivate the use of contrastive learning, the paper opens with an analysis of the effect of training epoch and encoder layer on the clustering of MWPs by prototype equation. t-SNE plots show expected clustering effects as layer/epoch increases. Analysis of raw high dimensional representations show that problems with similar lexical semantics or topic are given different representations when the prototype equation differs, especially in layer 12, while problems with the same prototype equation are embedded closer together. Moreover, it is shown that MPWs that are represented closer to the center of the cluster of problems with the same prototype equation are more likely to be correctly solved.  The contrastive learning approach proposed here involves finding difficult negative examples, which is done by choosing structurally similar equation trees with different operations in the intermediate nodes. Additional positive examples come from either trees or subtrees which consist of the same structure and operations as the target equation. For the multilingual approach, mBERT is substituted as the encoder. Results show that the contrastive learning method improves MWP solving in both the monolingual and multilingual settings compared to recent baselines. Ablations show the value of choosing difficult negative examples and other design decisions. Analysis shows that the contrastive learning objective results in well defined clusters. Accuracy is especially improved for examples farther from the cluster center. ",The contrastive learning for MWP solving seems to improve performance ,"Technique is limited to problems that can be modeled by equation trees.  A lot of paper real estate is given to an analysis that basically shows:  - undertrained models don’t work - only using part of the encoding function (the bottom N layers) doesn’t work I don’t think this analysis will be of much use to the ACL community.  It seems like the cosine similarity of lower layers in figure 3 are relatively high, while the t-SNE visualizations in Figure 2 are more mixed. Do you think t-SNE is accurately representing the latent space? ",The paper would benefit from connections to prior work on BERTology. An intro to this line of research can be found at https://huggingface.co/docs/transformers/bertology ,"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Technique is limited to problems that can be modeled by equation trees.  A lot of paper real estate is given to an analysis that basically shows:  - undertrained models don’t work - only using part of the encoding function (the bottom N layers) doesn’t work I don’t think this analysis will be of much use to the ACL community.  It seems like the cosine similarity of lower layers in figure 3 are relatively high, while the t-SNE visualizations in Figure 2 are more mixed. Do you think t-SNE is accurately representing the latent space? 
The paper would benefit from connections to prior work on BERTology. An intro to this line of research can be found at https://huggingface.co/docs/transformers/bertology "
ARR_2022_292_review,"This paper employs contrastive learning to improve performance on Math Word    Problems (MWP) tasks. The use of contrastive learning is motivated by the    observation that ``semantically similar'' problems appear closer together in    the embedding space, despite belonging to different problem classes. After    demonstrating this phenomenon via several angles, the authors train a seq2seq    MWP model (BERT encoder + tree decoder) supplemented with a contrastive loss    --- mono- and multilingually. They show that, in learning from both positive    and negative examples, their model outperforms many baselines, as well as the    non-contrastive model. Furthermore, they demonstrate that employing    contrastive learning a multilingual scenario improves performance w.r.t. the    multilingual non-contrastive case. These results are supplemented by thorough    analyses. ","This paper is generally thorough. After introducing the    problem (albeit very opaquely), the authors proceed to provide a simple    solution and show strong empirical evidence in its support. They a offer    unique application of contrastive learning to an under-explored problem and    highlight how its usage can be generalized. The analysis before and after    training is strong and insightful. ","The main weakness of this paper is the motivation, which is extremely    imprecise. The pedagogical literature cited in lines 36-46 is opaquely    summarized and does not lend sufficient grounding for introducing the    problem: ""mathematics is about patterns and not merely about numbers"";    ""...students explore patterns, not just memorize procedures"". The authors    distill these insights into their hypothesis, where they posit that    ""semantics affects problem-solving"". The word ""semantics"" is used    extremely liberally here, and it is was unclear to me what the authors    actually meant here (vs. prototype equations). The analysis and discussion    offered by Section 3 is difficult to follow because of this, and it is not    clear how instances ""similar semantics"" are tracked or computed (see, e.g.    Figure 3). Upon several re-readings, it became apparent that what the authors    actually refer to is lexical overlap (at least as evidenced by Probs C and D    in Figure 1). Due to this, the representation clusters for each prototype    equation tend to be expanded by the problems with high lexical overlap across    classes. This is more or less straightforward, but more care needs to be    devoted to making the language precise and less lofty. ","Comments:    * When investigating cosine similarity, authors should consider the work that    investigates MLM embedding spaces, namely    https://aclanthology.org/D19-1006.pdf and    https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the    anisotropy phenomenon.     * It would be interesting to see how the embedding spaces look like for BERT      out of the box.
   * It is important to make distinction between `procedures' and 'patterns'    Questions:    * What layer are the plots in the top row in Figure 2 from? Conversely, what      epoch are the plots in the bottom from?
   * Relevant to above point: how are ""similar semantics"" calculated? What is      meant by `semantics'? This sort of language not useful. ( lines 216-20)    * Lines 250-52: what does this mean?     Style/typos:    * Line 46 - hypothesize instead of `think'    * Line 55 - citation for encoder/decoder    * Line 155 - representations ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weakness of this paper is the motivation, which is extremely    imprecise. The pedagogical literature cited in lines 36-46 is opaquely    summarized and does not lend sufficient grounding for introducing the    problem: ""mathematics is about patterns and not merely about numbers"";    ""...students explore patterns, not just memorize procedures"". The authors    distill these insights into their hypothesis, where they posit that    ""semantics affects problem-solving"". The word ""semantics"" is used    extremely liberally here, and it is was unclear to me what the authors    actually meant here (vs. prototype equations). The analysis and discussion    offered by Section 3 is difficult to follow because of this, and it is not    clear how instances ""similar semantics"" are tracked or computed (see, e.g.    Figure 3). Upon several re-readings, it became apparent that what the authors    actually refer to is lexical overlap (at least as evidenced by Probs C and D    in Figure 1). Due to this, the representation clusters for each prototype    equation tend to be expanded by the problems with high lexical overlap across    classes. This is more or less straightforward, but more care needs to be    devoted to making the language precise and less lofty. 
Comments:    * When investigating cosine similarity, authors should consider the work that    investigates MLM embedding spaces, namely    https://aclanthology.org/D19-1006.pdf and    https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the    anisotropy phenomenon.     * It would be interesting to see how the embedding spaces look like for BERT      out of the box.
   * It is important to make distinction between `procedures' and 'patterns'    Questions:    * What layer are the plots in the top row in Figure 2 from? Conversely, what      epoch are the plots in the bottom from?
   * Relevant to above point: how are ""similar semantics"" calculated? What is      meant by `semantics'? This sort of language not useful. ( lines 216-20)    * Lines 250-52: what does this mean?     Style/typos:    * Line 46 - hypothesize instead of `think'    * Line 55 - citation for encoder/decoder    * Line 155 - representations "
ARR_2022_292_review,"The paper describes an approach for improving  Math Word Problem (MWP) solving. 
Due to the non-distinction of MWP patterns, current methods, which are based on neural networks, perform poorly. The authors  propose a contrastive learning method to assist  the model to correctly separate confused patterns. The proposed methods outperform  two baselines in both monolingual and multilingual settings. ","1) The paper is written well and is a pleasure to read (although some details are missing) 2) The paper presents detailed results, which outperform the two baselines in both monolingual and multilingual settings. 
3)  The coupling of both semantic encoding (Bert encoder) and contrastive learning is  a key contribution and strength of the paper, in my opinion. ","1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. ",The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. "
ARR_2022_188_review,"The paper describes the method to trace historical sound change by measuring the distance between pairs of character distributions over time. Using spelling as a necessary proxy to phonetics, the authors model phonological change through the use of diachronic character embeddings. 
They show the viability of the proposed approach on synthetic datasets and  on real sound changes in Danish geographical names (in particular, the authors focus on lenition). ","I really like how the work combines linguistic problems with data-driven solutions. Modeling sound change computationally is a rarely seen task in the NLP venues, which is sad, since it is extremely important for general historical linguistics. 
The paper under review proposes a smart method to trace gradual phonological replacements like lenition (t -> d, k -> g, etc). It will be of great use both to linguists and to NLP practitioners interested in change detection. In fact, the paper extends the computational change detection field from only semantics to phonology. ","1) Although the proposed method is indeed interesting, the  authors do not make any attempt to compare it to any other prior methods. They limit themselves to showing that the method produces statistically significant linear regressions. But this is, to my mind, insufficient as empirical evaluation. Why not implement a simple baseline (for example, character frequencies, etc) and compare to it? This would make the paper more persuasive.
2) The strange results for ""p --> b"" sound change deserves more explanations beyond just noting that this is a rarer phoneme than the others. If the method is not able to trace sound change in 1/3 cases, it must be shown that it is still better than anything else (baselines).
3) The method is entirely based on written signal (spelling). It is understandable that we lack proper phonological datasets, but I would like to see more discussion on how this spelling proxy might influence the performance of the proposed method. ","l. 066: ""phonology: Inspired by"" --> ""phonology. Inspired by"" May be, reposition Figures 1 and 2 to use the page space more efficiently?
Did you try cosine distance instead of Euclidean? It might perform better in the end.
The authors several times mention ""neural methods that make use of dense character representations"" (which they don't use). I believe that the word ""neural"" is redundant here. Dense representations can be produced without using any neural networks (for example, by applying SVD to PMI vectors).
These papers, although not 100% on-topic, might still be mentioned in the related work: https://aclanthology.org/W19-4713/ https://aclanthology.org/W19-4732/ ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1) Although the proposed method is indeed interesting, the  authors do not make any attempt to compare it to any other prior methods. They limit themselves to showing that the method produces statistically significant linear regressions. But this is, to my mind, insufficient as empirical evaluation. Why not implement a simple baseline (for example, character frequencies, etc) and compare to it? This would make the paper more persuasive.
2) The strange results for ""p --> b"" sound change deserves more explanations beyond just noting that this is a rarer phoneme than the others. If the method is not able to trace sound change in 1/3 cases, it must be shown that it is still better than anything else (baselines).
3) The method is entirely based on written signal (spelling). It is understandable that we lack proper phonological datasets, but I would like to see more discussion on how this spelling proxy might influence the performance of the proposed method. 
l. 066: ""phonology: Inspired by"" --> ""phonology. Inspired by"" May be, reposition Figures 1 and 2 to use the page space more efficiently?
Did you try cosine distance instead of Euclidean? It might perform better in the end.
The authors several times mention ""neural methods that make use of dense character representations"" (which they don't use). I believe that the word ""neural"" is redundant here. Dense representations can be produced without using any neural networks (for example, by applying SVD to PMI vectors).
These papers, although not 100% on-topic, might still be mentioned in the related work: https://aclanthology.org/W19-4713/ https://aclanthology.org/W19-4732/ "
ARR_2022_188_review,"This paper use methods relying on spelling, using character embeddings, to detect sound change across time. They rely on three datasets to test the effectiveness of their methods: a synthetic one in a language having a simplified phonotactic system, one in Danish with synthetically generated sound change, and a real one in Danish, with known sound change. They show that they can successfully detect the selected sound changes in the datasets, and identify precisely the context of the change. ","The paper is globally nice to read and introduces an interesting idea. The related works section is particularly well-written, with the right level of detail, even though some references are missing. 
The experimental setup is sound and clear; in particular, the use of a control dataset is appreciated. ","1) The main issue in this paper is that you only show that you can successfully detect known sound changes, and that you don’t spuriously detect them when they’re absent, thanks to the control corpus. But you might detect a lot of changes in characters embeddings using your methods, that are not related to sound change. Using your system in an exploratory fashion, to detect all possible changes and categorize them, would greatly strengthen the paper. As it is, to my understanding, there is no way to be sure that the character embeddings do not lead to detecting a lot of changes independent from phonology, and this is a major issue. Moreover, the link between spelling and sound change is not straightforward to me and should be more clearly justified.
2) Semantic and phonological change across time should indeed take a logistic shape, as you describe in your description; your experiments also seem to show that the change in the Geo corpus is not linear. Why did you stick to a linear shape? Shoemark et al (2019) that you cite also tried a logarithmic shape, you could try it too.
3) There is a large amount of work on semantic change using contextualized embeddings and pre-trained language models, that you do not evoke in your related works (from Mario Giulianelli, Matej Martinc…).  4) The formalism you use, a --> b / c, should be introduced more clearly from the beginning. Especially since you use it as early as in the summary, where it can’t be comprehended without prior knowledge of this formalism. The explanation at line 190 might benefit from a scheme. Similarly, some words like “plosive” might benefit from a short definition (even in a footnote) for readers not familiar with the domain. ","Nor clear how the Bin:Control effect is computed. Globally, all metrics described in Section 5 would benefit from equations.
Overall, the localization of Tables and Figures in the paper would be more optimal to ease the reading. 
Tables 2 to 4 look strange to me without lines.
L.32: Choose only one between “since” and “however” L59-62: Add references here.
L. 362 and 364: add “the” distance, “the” main effect.
L. 390: in both corpora (no “the”) L. 480: repetition of “language”’. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1) The main issue in this paper is that you only show that you can successfully detect known sound changes, and that you don’t spuriously detect them when they’re absent, thanks to the control corpus. But you might detect a lot of changes in characters embeddings using your methods, that are not related to sound change. Using your system in an exploratory fashion, to detect all possible changes and categorize them, would greatly strengthen the paper. As it is, to my understanding, there is no way to be sure that the character embeddings do not lead to detecting a lot of changes independent from phonology, and this is a major issue. Moreover, the link between spelling and sound change is not straightforward to me and should be more clearly justified.
2) Semantic and phonological change across time should indeed take a logistic shape, as you describe in your description; your experiments also seem to show that the change in the Geo corpus is not linear. Why did you stick to a linear shape? Shoemark et al (2019) that you cite also tried a logarithmic shape, you could try it too.
3) There is a large amount of work on semantic change using contextualized embeddings and pre-trained language models, that you do not evoke in your related works (from Mario Giulianelli, Matej Martinc…).  4) The formalism you use, a --> b / c, should be introduced more clearly from the beginning. Especially since you use it as early as in the summary, where it can’t be comprehended without prior knowledge of this formalism. The explanation at line 190 might benefit from a scheme. Similarly, some words like “plosive” might benefit from a short definition (even in a footnote) for readers not familiar with the domain. 
Nor clear how the Bin:Control effect is computed. Globally, all metrics described in Section 5 would benefit from equations.
Overall, the localization of Tables and Figures in the paper would be more optimal to ease the reading. 
Tables 2 to 4 look strange to me without lines.
L.32: Choose only one between “since” and “however” L59-62: Add references here.
L. 362 and 364: add “the” distance, “the” main effect.
L. 390: in both corpora (no “the”) L. 480: repetition of “language”’. "
ARR_2022_45_review,"This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task. Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability. In addition, the word embedding and the output representation are used together to compute generation probability. The experiment results show the proposal method outperforms baseline model on two benchmark datasets. ","1. The paper is well organized, and it explains the proposed solution clearly. ","1. The idea is incremental. Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers.
    [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019).
    [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""
2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version. ",1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage? ,"2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The idea is incremental. Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers.
    [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019).
    [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""
2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version. 
1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage? "
ARR_2022_187_review,"This paper proposes a neural pairwise ranking model to evaluate readability assessment. The model outperforms existing methods on two tasks - cross-corpus ranking and zero shot, cross-lingual ranking - based on ranking metrics. The paper also publish a new bilingual (English-French) readability dataset for cross-lingual evaluation. ","1. The pairwise ranking method shows better robustness on various monolingual dataset and out-domain cross-lingual dataset than previous methods. 
2. The paper creates a new cross-lingual dataset for readability assessment. ","1. The paper does not clearly introduce the dataset. For example, what is average length of each text in NewsEla? If the text is long, the paper should also mention how the text is encoded by BERT when it exceeds the maximum length. 
2. As the paper already pointed out, the nature of pairwise model restricts the inference for downstream applications, so that the contribution of this paper is not very strong. ","- L400: citation for ""for evaluating ranking or information-retrieval tasks in literature""?
- L502: Is NPRM (0.999, 0.995, 0.990, 0.948) better than regBERT (0.999, 0.997, 0.994, 0.977)?
- L613: ""while"" -> """" ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The paper does not clearly introduce the dataset. For example, what is average length of each text in NewsEla? If the text is long, the paper should also mention how the text is encoded by BERT when it exceeds the maximum length. 
2. As the paper already pointed out, the nature of pairwise model restricts the inference for downstream applications, so that the contribution of this paper is not very strong. 
- L400: citation for ""for evaluating ranking or information-retrieval tasks in literature""?
- L502: Is NPRM (0.999, 0.995, 0.990, 0.948) better than regBERT (0.999, 0.997, 0.994, 0.977)?
- L613: ""while"" -> """" "
ARR_2022_187_review,"This paper explores the topic of automatic readability assessment, specifically reframing the task as one of pairwise ranking. The authors experiment with neural and other baseline models. ","- A number of data-sets are used, some for testing only - Cross-corpus and cross-lingual experiments are done to show generalizability - A number of baseline systems are used, such as traditional classification- and regression-based systems (where possible) for comparison - A data-set is being released along with this work, which may lead to more development opportunities in the future ","- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.
- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? ","- Are ""slugs"" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.
- ""In this background"" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. ""With this background"", ""Given this background"", etc.
- 2 Related Work - ""result in"" --> ""results in"" - 4.4 Evaluation - ""w"" --> ""we""? It's unclear what is meant.
- Limitations of NPRM - ""while"" --> ""why"" (or just get rid of ""while"") ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.
- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? 
- Are ""slugs"" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.
- ""In this background"" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. ""With this background"", ""Given this background"", etc.
- 2 Related Work - ""result in"" --> ""results in"" - 4.4 Evaluation - ""w"" --> ""we""? It's unclear what is meant.
- Limitations of NPRM - ""while"" --> ""why"" (or just get rid of ""while"") "
ARR_2022_187_review,"This paper proposes to examine the problem of Automatic Readability Assessment (ARA) as a neural pairwise ranking task, with the goal of addressing multiple transferability issues with classification-based ARA.  The paper compares a pairwise neural ranking model with existing non-ranking ARA methods and explores the monolingual and cross-lingual transferability of their ARA ranking model.  From a technical contributions standpoint, the paper mostly provides benchmarking results for ranking vs. other ARA paradigms, rather than a new model. ","The paper is thorough in what ARA experiments are run across different paradigms (classification, regression, ranking) and transferability settings (monolingual/cross-lingual transfer).  The paper also provides a well-motivated argument for the benefits of using a ranking-based paradigm for readability assessment, especially for settings involving transfer. ","The paper lacks significance tests in their comparisons between models, which may draw some of the conclusions in the paper into question.  The results themselves could be presented more clearly with a little more reflection on why certain methods seem to work better than others.  Most of my concerns are covered in more detail in Comments, Suggestions, and Typos. ","Lines 207-208: Isn't this just mathematically equivalent to taking Binary Cross Entropy loss over the sigmoid?
Lines 225-232: One limitation of the current ranking based approach is that while it's easier to compare across different datasets with potentially different label scales, running comparisons over multiple instances may be less efficient.  For example, if we want to get an idea of the absolute readability for a new document compared to a set of S other documents that have ""already been scored"", we would have to re-run the NPRM over the S documents in your comparison basis.  Have you thought of methods of aggregating or scaling the NPRM score to get around this drawback?  Or are there particular applications you can see where this comparison problem as not being as much of an efficiency issue.
Section 5.3: One suggestion I had to make things a little easier to follow in this section is to add a table comparing the performance of the best models for each paradigm (classification, regression, pairwise-ranking) on the ranking metrics.  Currently, it's a little hard to follow the written results here because the reader has to check between multiple tables to make the comparison.
Tables 1-6: I would recommend bolding the best performance numbers in these tables.
Results: Overall, did you run any significance tests when making comparisons between two different models?  Some of the numbers, such as the pairwise ranking evaluations and the comparisons between classification, regression, and ranking on the ranking metrics are quite close.  I would recommend adding a note in the text whenever you make a comparison between two models as to whether the difference in results is significant.
Line 559-561: Do you think the drop in performance for Vikidia-Fr is because of the domain difference between Newsela and Vikidia? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The paper lacks significance tests in their comparisons between models, which may draw some of the conclusions in the paper into question.  The results themselves could be presented more clearly with a little more reflection on why certain methods seem to work better than others.  Most of my concerns are covered in more detail in Comments, Suggestions, and Typos. 
Lines 207-208: Isn't this just mathematically equivalent to taking Binary Cross Entropy loss over the sigmoid?
Lines 225-232: One limitation of the current ranking based approach is that while it's easier to compare across different datasets with potentially different label scales, running comparisons over multiple instances may be less efficient.  For example, if we want to get an idea of the absolute readability for a new document compared to a set of S other documents that have ""already been scored"", we would have to re-run the NPRM over the S documents in your comparison basis.  Have you thought of methods of aggregating or scaling the NPRM score to get around this drawback?  Or are there particular applications you can see where this comparison problem as not being as much of an efficiency issue.
Section 5.3: One suggestion I had to make things a little easier to follow in this section is to add a table comparing the performance of the best models for each paradigm (classification, regression, pairwise-ranking) on the ranking metrics.  Currently, it's a little hard to follow the written results here because the reader has to check between multiple tables to make the comparison.
Tables 1-6: I would recommend bolding the best performance numbers in these tables.
Results: Overall, did you run any significance tests when making comparisons between two different models?  Some of the numbers, such as the pairwise ranking evaluations and the comparisons between classification, regression, and ranking on the ranking metrics are quite close.  I would recommend adding a note in the text whenever you make a comparison between two models as to whether the difference in results is significant.
Line 559-561: Do you think the drop in performance for Vikidia-Fr is because of the domain difference between Newsela and Vikidia? "
ARR_2022_187_review,"The paper proposes a new pairwise ranking approach for the task of automatic readability assessment (ARA). The proposed approach is compared against other modeling paradigms used in the ARA literature, namely classification and regression across 3 languages (English, French and Spanish). The experiments show that the approach works competitively in monolingual settings and gets strong results in cross-corpus and zero shot cross-lingual settings. The paper also releases a new parallel bilingual ARA dataset for future research. ","1. Clear motivation for need of such a technique with cross-corpus compatibility being a realistic desired property for ARA approaches.
2. Well defined research questions, adequately answered through experiments.  3. Thorough experimentation, spanning 3 languages and different modeling approaches.
4. New dataset (and the first of its kind) for a relatively understudied task of ARA. ","1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty.  2. The writing is difficult to follow in many places and can be simplified. ","1. Line 360-367 are occupying too much space than needed.  2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :)   3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text.  4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant.  5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself?  6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better.  7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful.  8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results.  10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable? ",3.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty.  2. The writing is difficult to follow in many places and can be simplified. 
1. Line 360-367 are occupying too much space than needed.  2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :)   3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text.  4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant.  5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself?  6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better.  7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful.  8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results.  10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable? "
ARR_2022_91_review,"This paper tackles the problem of retrieval-augmented generation. 
The authors suggest two ways to enhance such models (e.g. RAG): 1. Training a reranker to better approximate the contribution of each retrieved document to the generation process. The reranker also enables enables retrieved documents from different retrievers (e.g. DPR & BM25) to be combined seamlessly. 
2. Distilling the knowledge from the reranker to the retriever ",- The paper reads well and is easy to follow - Strong results across numerous tasks - Nice ablations ,"- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation).  - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.
[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering ","- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)? ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation).  - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.
[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering 
- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)? "
ARR_2022_91_review,"The paper proposes a novel model called R^2G which combines neural initial retrieval and reranking into a BART-based sequence- to-sequence generation model.
Furthermore they ensemble different first stage retrieval methods, combining neural retrieval and lexical retrieval with BM25. 
They propose a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output.
They compare their novel model on the KILT leaderboard and demonstrate effectiveness gains compared to relevant related work. ","The paper proposes an extension of the RAG architecture by including another lexical index.
I like the discussion of the different solutions for end-to-end training of the system and also the explanations why certain approaches do not work and the other ones do.
I also think that the extension of the RAG system with a re-ranking approach is interesting for the task. ","I am wondering how novel the application of the knowledge distillation from the reranker as teacher model to provide labels to the DPR model is, see Hofstätter et al.""Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation"" (https://arxiv.org/abs/2010.02666).
Furthermore I am missing an ablation of training the reranker with the combined scores of the positive passages (line 277) in comparison to the standard way the reranker is trained in Nogueira et al.. I would also appreciate an ablation study of combining DPR and BM25 results with reranking in comparison to established solutions like combination with Reciprocal Rank Fusion (see Chen et al. ""Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models"") or combining the scores with a linear combination of the BM25 and DPR score (see Karpuhkin et al. ""Dense passage retrieval for open-domain question answering"") I am also wondering why only 5 of 11 existing datasets of the KILT leaderboard are selected, I would be interested why you exactly choose these datasets. ","I think the overall writing is clear and understandable however I thought it could be better organized in some sections: Line 207-2018 describes dense retrieval in the section about re-ranking, I think dense retrieval should be described beforehand and not in the section ""Reranking"" The formulas in Line 310 are not described and embedded in the text, I think describing them in the neighbouring text would make clear why the formulas are there. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"I am wondering how novel the application of the knowledge distillation from the reranker as teacher model to provide labels to the DPR model is, see Hofstätter et al.""Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation"" (https://arxiv.org/abs/2010.02666).
Furthermore I am missing an ablation of training the reranker with the combined scores of the positive passages (line 277) in comparison to the standard way the reranker is trained in Nogueira et al.. I would also appreciate an ablation study of combining DPR and BM25 results with reranking in comparison to established solutions like combination with Reciprocal Rank Fusion (see Chen et al. ""Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models"") or combining the scores with a linear combination of the BM25 and DPR score (see Karpuhkin et al. ""Dense passage retrieval for open-domain question answering"") I am also wondering why only 5 of 11 existing datasets of the KILT leaderboard are selected, I would be interested why you exactly choose these datasets. 
I think the overall writing is clear and understandable however I thought it could be better organized in some sections: Line 207-2018 describes dense retrieval in the section about re-ranking, I think dense retrieval should be described beforehand and not in the section ""Reranking"" The formulas in Line 310 are not described and embedded in the text, I think describing them in the neighbouring text would make clear why the formulas are there. "
ARR_2022_153_review,"This paper studies the Machine Translation of the endangered language Livonian. The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations. Then different ""base"" machine translations are trained and finetuned on the LIV->EN data. Backtranslations and pivot translation are also explored. ","NMT for endangered languages is a critical and interesting research direction. The proposed parallel corpus is valuable and make great contributions to Livonian NMT. Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation. ",Some evaluations results are mixing. Please see the detailed comments below. ,"Question: 1. In line 257, it's strange that including the collected LIV-EN parallel data for finetuning actually makes the NMT system perform worse. Could you provide more discussions/explanations on this?
2. Could you provide some insight why the multilingual model is ""noticeably weaker"" (line 236) on the ET→En and LV→EN evaluations?
3. It's interesting that the LV→EN model performs better than the ET→EN model, especially in section 2 authors mentioned that Livonian and Latvian languages are similar in many aspects.  minor: 1. Maybe the information that ""the translation is done by hired experts (section 3)"" can be added to the footnote 2 on page 2 (section 1)? I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.
2. Line 194, maybe I'm not familiar with the context, could you explain what does ""implement the support of ..."" mean?
3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.
4. Line 210, on what ET, EN, LV data is the Sentencepiece tokenizer obtained on? Those in the 4-language parallel corpus?
5. Line 255: typo, ""perform performed"" ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Some evaluations results are mixing. Please see the detailed comments below. 
Question: 1. In line 257, it's strange that including the collected LIV-EN parallel data for finetuning actually makes the NMT system perform worse. Could you provide more discussions/explanations on this?
2. Could you provide some insight why the multilingual model is ""noticeably weaker"" (line 236) on the ET→En and LV→EN evaluations?
3. It's interesting that the LV→EN model performs better than the ET→EN model, especially in section 2 authors mentioned that Livonian and Latvian languages are similar in many aspects.  minor: 1. Maybe the information that ""the translation is done by hired experts (section 3)"" can be added to the footnote 2 on page 2 (section 1)? I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.
2. Line 194, maybe I'm not familiar with the context, could you explain what does ""implement the support of ..."" mean?
3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.
4. Line 210, on what ET, EN, LV data is the Sentencepiece tokenizer obtained on? Those in the 4-language parallel corpus?
5. Line 255: typo, ""perform performed"" "
ARR_2022_153_review,"This paper presents explores Livonian-English translation; Livonian is a very low-resource Uralic language with ~20 native speakers. 
The work combines creation of a resource for Livonian (10k parallel sentences), which previously had only <100 parallel sentences available on OPUS, including a manually translated multi-way parallel evaluation set. 
These resources are experimentally validated, and further, this work does a preliminary exploration of existing transfer techniques (fine-tuning, pivot translation, backtranslation) using monolingual and parallel resources to find which language(s) are best able to assist for transfer learning in this very low-resource setting where there are multiple languages which may be appropriate (contact languages (Latvian) vs typologically related (Estonian). 
The resources are to be publicly released (already done anonymously on OPUS, will be further shared once able to be deanonymized). ","- Collation of existing data into one resource and creation of a standardized multi-way evaluation set for Livonian will certainly assist further work on this language, and is appropriate for the ACL theme track - Experimentation on the resource provides a basis for comparison, and preliminarily looks into the contrast between contact languages and typological relatedness ",- The experiments in the paper do not necessarily directly answer the research questions as posed in the introduction ,"With an additional page, it would be nice to focus on some nice-to-have analysis to make it clearer what to take away from the experiments for those interested in this language pair, or others: *why* might there be a slight preference for pivoting through Estonian? Is this simply because the Estonian model performs slightly better than the Latvian before pivoting, and have nothing to do with the appropriateness of contact language vs typologically related language? Are there any reasons you can find for why finetuning is worse than pivoting? Are there any ways (besides downstream BLEU) to how use of a contact language or same language family affects the model?  - Table 2 layout is current a bit difficult to read; consider adding additional row and column headers saying source/target, pivot vs multilingual vs finetuning, to make it easier to match results to experiments - Would not call 8.92 a ""very respectable BLEU score""; perhaps write that it is ""improved"" instead - The way the multilingual model's vocabulary is chosen (25k units taken from imbalanced multilingual sources, size-wise) is likely to hurt performance on Livonian-English; it might make more sense to check to see what vocabulary granularity is being used for Livonian (if all or mostly characters, for example, that suggests the multilingual vocab is not appropriate, and perhaps a set of smaller BPE vocabs for each language should be created and then joined after) ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The experiments in the paper do not necessarily directly answer the research questions as posed in the introduction 
With an additional page, it would be nice to focus on some nice-to-have analysis to make it clearer what to take away from the experiments for those interested in this language pair, or others: *why* might there be a slight preference for pivoting through Estonian? Is this simply because the Estonian model performs slightly better than the Latvian before pivoting, and have nothing to do with the appropriateness of contact language vs typologically related language? Are there any reasons you can find for why finetuning is worse than pivoting? Are there any ways (besides downstream BLEU) to how use of a contact language or same language family affects the model?  - Table 2 layout is current a bit difficult to read; consider adding additional row and column headers saying source/target, pivot vs multilingual vs finetuning, to make it easier to match results to experiments - Would not call 8.92 a ""very respectable BLEU score""; perhaps write that it is ""improved"" instead - The way the multilingual model's vocabulary is chosen (25k units taken from imbalanced multilingual sources, size-wise) is likely to hurt performance on Livonian-English; it might make more sense to check to see what vocabulary granularity is being used for Livonian (if all or mostly characters, for example, that suggests the multilingual vocab is not appropriate, and perhaps a set of smaller BPE vocabs for each language should be created and then joined after) "
ARR_2022_249_review,This paper presents a study on leveraging crosslingual pretrained language models for zero-shot event-argument extraction (EAE). The main idea proposed in this paper is to devise event-argument templates which are language agnostic and act as a bridge across across languages. Event argument extraction is then reduced to a slot filling task where the difference between languages lies in the different arguments. The model uses cross-lingual embeddings and treats slot filling as a sequence generation task. ,"- Good empirical results (modulo experimental caveat mentioned in my comments below) - Simple model, works for many languages.
- Template-based approach could be useful for other tasks (e.g., IE, or even cross-lingual generation) - the paper is well-written and includes several experiments, across languages, and experimental conditions and also abalation studies and interesting discussion about the results ",The authors have addressed all my comments on the previous version of their paper! No obvious weaknesses. ,One suggestion is to mention in a footnote why it is difficult to present back-translation baselines (like you explain in your letter). ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The authors have addressed all my comments on the previous version of their paper! No obvious weaknesses. 
One suggestion is to mention in a footnote why it is difficult to present back-translation baselines (like you explain in your letter). "
ARR_2022_249_review,This paper proposed to format the event argument extraction task as a generation task and designed language-agnostic templates to fine-tune multilingual seq2seq language models for zero-shot cross lingual transfer. The experimental results show its superiority over traditional classification models. Overall the idea is clear and simple with strong motivations. The analysis introduced its advantages as well as its current limitations. This work is a well-done paper. ,"1. The problem formulation for zero-shot cross-lingual event extraction is new and clear. 
2. The results show the strong potential of generative models for information extraction. 
3. The analysis is detailed and informative for future work. ",1.The novelty is a bit limited for the small focused task. ,"In the information extraction area, recently a lot of work has been explored to replace the traditional classification methods with the generative models. It is not novel enough to directly adapt to another task. For the major contribution of this work,  the special design of inputs and outputs could well capture the characteristics of the task. Fine-tuning multilingual LMs enables good generalization for zero-shot cross-lingual transfer. If the language-agnostic templates could be applied to other structure prediction tasks with good improvements, the contribution could be larger. Currently this paper is okay but not novel enough. It would be better to discuss the potential of language-agnostic templates (html tag). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"1.The novelty is a bit limited for the small focused task. 
In the information extraction area, recently a lot of work has been explored to replace the traditional classification methods with the generative models. It is not novel enough to directly adapt to another task. For the major contribution of this work,  the special design of inputs and outputs could well capture the characteristics of the task. Fine-tuning multilingual LMs enables good generalization for zero-shot cross-lingual transfer. If the language-agnostic templates could be applied to other structure prediction tasks with good improvements, the contribution could be larger. Currently this paper is okay but not novel enough. It would be better to discuss the potential of language-agnostic templates (html tag). "
ARR_2022_249_review,"*Note: I reviewed this paper in an earlier ARR cycle. There are no changes in the updated version that warrant a change in my score or the review. I’ve updated a summary of weaknesses to reflect the updates, and have listed a few suggestions on grammar.*
This work presents a method (X-GEAR) for zero-shot, cross-lingual event argument extraction. X-GEAR takes as input i) a passage, ii) a trigger word (a predicate, e.g., ""killed""), and iii) a template indicating the desired roles (e.g., <Victim>NONE</Victim>). The output is the template filled with event arguments extracted from the passage (e.g., NONE might be replaced with civilian). X-GEAR is built using the standard Seq2seq framework with a copy mechanism, where the input is composed of the triplet (passage, template, trigger word) flattened as a sequence, and the output is the template filled with desired roles.
The method relies on recent advances in large, multilingual pre-trained language models (PTLM) such as MT5, which have been shown to perform robust cross-lingual reasoning. The key insight of the method is to use language-agnostic special tokens (e.g., <Victim>) for the template. Fine-tuning on the source language helps learn meaningful representations for templates, which allows their approach to work across target languages supported by the PTLM. ","- The paper presents a simple but intuitive method for solving an important problem. The simplicity of the proposed method is a significant strength of this work. As the authors note, existing systems that perform structured extraction often rely on a pipeline of sub-modules. X-GEAR replaces that with a simple Seq2seq framework which is considerably easier to use and maintain.
- The proposed method is clearly defined, the experiments are thorough and show considerable gains over the baselines.
- The analysis provides several insights into the strengths and weaknesses of the proposed approach. ","The authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. ","- L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557). ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. 
- L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557). "
ARR_2022_327_review,"This paper works on multi-parallel word alignment (MPWA) through graph neural network (GNN), in particular the graph attention network (GAT). Instead of limiting the word alignment model training to parallel text (bitext), the authors follow the line of work which exploits multi-parallel corpora, which enables recovering missing alignments between a source and a target language. For this, the authors leveraged the community detection (CD) algorithm for graph, which makes the sub-graph within a community complete and while disconnecting inter-community edges, for training data labels.
The GNN proposed in the paper takes as input the node features which take into account the graph structure, community membership, and content embeddings which include word, position, and language. To optimize the resulting alignment edges, the authors propose two strategies to post-process the alignments based on the grow-diag-final-and employed in the statistical MT alignment, Thresholding GDFA (TGDFA) and TGDFA+orig ","The approach of multi-parallel word alignment (MPWA) is interesting not only because word alignment has largely been not worked on since the advent of neural machine translation (NMT) and only recently resurfaced, but also because of the growing interest of interpretable neural network models, which the current NMT does not really have. In addition, the SMT word alignment also benefits low-resource languages for which parallel or even monolingual corpora are not adequate.
The experimental result shows that the proposed graph neural network (GNN) approach outperforms the strong statistical word alignment baseline and prior graph-based and matrix-based approaches in terms of the overall alignment error rate and F1. ","Although the paper is interesting and strongly supported by experimental results, understanding the details is tough. For one, the difference between TGDFA+orig and TGDFA is not very clear in what is meant by the original bilingual GDFA. An illustration (page limit permitting) may be helpful to understand the difference.
It is unclear if HELFI training set uses 3 languages or 84 as listed in the appendix. ","Apart from the concerns above, it is better to have some insights on how more other language pairs not in the test set correlates with increased performance. This can be achieved by experimental setups which include or exclude other languages not in the test set. For example, in HELFI, there can be one setup which includes only Finnish, Greek, and Hebrew and another that includes all the 84 languages in the Appendix A. The Appendix A is awkward as the section heading is not in one page with the entire content, which can be achieved by the typesetting. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",(nothing to concern) ,"Although the paper is interesting and strongly supported by experimental results, understanding the details is tough. For one, the difference between TGDFA+orig and TGDFA is not very clear in what is meant by the original bilingual GDFA. An illustration (page limit permitting) may be helpful to understand the difference.
It is unclear if HELFI training set uses 3 languages or 84 as listed in the appendix. 
Apart from the concerns above, it is better to have some insights on how more other language pairs not in the test set correlates with increased performance. This can be achieved by experimental setups which include or exclude other languages not in the test set. For example, in HELFI, there can be one setup which includes only Finnish, Greek, and Hebrew and another that includes all the 84 languages in the Appendix A. The Appendix A is awkward as the section heading is not in one page with the entire content, which can be achieved by the typesetting. "
ARR_2022_328_review,"This paper releases a dataset of punctuated transcripts of spontaneous speech. The speech is automatically generated from instructional livestreams but the punctuation is added by manual annotation.  The main novelty of this work is the dataset itself, which includes punctuation on spontaneous speech because models trained on prepared speech do not perform as well.  Finally, the authors demonstrate that performance on downstream tasks improves with better punctuation annotations. ","- New punctuation dataset on spontaneous speech is useful for cascade systems where the ASR system does not output punctuation, if the original audio doesn't exist or where computing resources are limited and transcripts exist.
- ","1. Punctuation only includes four marks (.,!?), but others are more common (specifically "":). Why not include these? ( non-scientific source: http://www.viviancook.uk/Punctuation/PunctFigs.htm).
2. Table 4, if I understand the experiment correctly, seems unfair. You're comparing sentence segmentation from models trained on mostly punctuated text to your model, which expects unpunctuated text. A fairer comparison would be to run the original text through a pretrained punctuation restoration model. ",What is the point of the taxonomy when you have four punctuation marks? ~L130 The annotator disagreement might be interesting for the different punctuation marks. I expect high agreement for question marks but much lower for commas. ,"4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,None ,"1. Punctuation only includes four marks (.,!?), but others are more common (specifically "":). Why not include these? ( non-scientific source: http://www.viviancook.uk/Punctuation/PunctFigs.htm).
2. Table 4, if I understand the experiment correctly, seems unfair. You're comparing sentence segmentation from models trained on mostly punctuated text to your model, which expects unpunctuated text. A fairer comparison would be to run the original text through a pretrained punctuation restoration model. 
What is the point of the taxonomy when you have four punctuation marks? ~L130 The annotator disagreement might be interesting for the different punctuation marks. I expect high agreement for question marks but much lower for commas. "
ARR_2022_328_review,"The main contribution of the paper is a human-annotated dataset (BehancePR) for punctuation restoration in livestreaming video transcripts. Punctuation restoration is the process of restoring text structures such as sentences, phrases etc by inserting four types of punctuation marks (period, comma, question mark and exclamation mark) into non-punctuated text. The paper also illustrates that several standard NLP toolkits underperform on the sentence-boundary detection task on non-punctuated transcripts. ","The paper highlights some interesting points why existing models might be unsuitable for task of punctuation restoration on video transcripts. 
Although it is unclear how effective the presented dataset would be, it has a potential for being useful in further research in this area. 
The trained model seems to use one additional punctuation mark than the norm in the area. ","It is unclear if the additional punctuation mark of exclamation mark adds any significant theoretical difficulty to the problem apart from the lack of trained models. In fact, it would play a role in poor performance of models that have not been trained on dataset without that punctuation mark, which doesn't truly highlight the flaw in these models. The drawn conclusion that this is due to the ""different nature of data"" thus does not hold much value. 
Further, in the field of PR, it seems like there is plenty of scope for creation of synthetic datasets by simply removing punctuation marks from arbitray English texts, movie transcripts and so on. Comparative study of such synthetic datasets, and an analysis of why human-annotated sets might be of more value, if at all, should be performed. ","Creation of synthetic datasets on large scale, by removing punctuation marks seems possible. This option should be explored. 
Models trained on such synthetic datasets can also be compared on the human-annotated datasets. These might serve as a better baseline than models trained on data without the same set of punctuation marks. ","2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,Ethical considerations have been satisfactorily discussed and there are no glaring ethical issues. ,"It is unclear if the additional punctuation mark of exclamation mark adds any significant theoretical difficulty to the problem apart from the lack of trained models. In fact, it would play a role in poor performance of models that have not been trained on dataset without that punctuation mark, which doesn't truly highlight the flaw in these models. The drawn conclusion that this is due to the ""different nature of data"" thus does not hold much value. 
Further, in the field of PR, it seems like there is plenty of scope for creation of synthetic datasets by simply removing punctuation marks from arbitray English texts, movie transcripts and so on. Comparative study of such synthetic datasets, and an analysis of why human-annotated sets might be of more value, if at all, should be performed. 
Creation of synthetic datasets on large scale, by removing punctuation marks seems possible. This option should be explored. 
Models trained on such synthetic datasets can also be compared on the human-annotated datasets. These might serve as a better baseline than models trained on data without the same set of punctuation marks. "
ARR_2022_328_review,"The paper presents a dedicated annotated corpus for the punctuation restoration task. The dataset annotated 4 types of punctuations, i.e. comma, period, question, and exclamation for live streaming video transcripts. The use of live streaming video transcripts poses new and interesting challenges compared to existing datasets in which  punctuation restoration is included as a sub-task of automatic speech recognition. Some of the challenges include the presence of more speakers, less formal and more spontaneous speech, and presence of frequent emotional sentences that need to be marked by exclamation marks. The dataset is annotated by experienced annotators. The authors have evaluated the state-of-the-art punctuation restoration models (neural based BiLSTM and graphical based CRF) and found that the performance of the PR models on their data set is far below compared to existing datasets revealing its challenging nature. Adding data augmentation brought slight changes in the performance. When these models were tested for domain adaptation i.e. training on existing datasets and testing on proposed dataset it was found that performance of all the models degrade significantly. They have also evaluated the performance of existing NLP toolkits for the task sentence splitting on this dataset and found that they perform very poorly suggesting the importance of this kind of training data. ","The paper presents a punctuation restoration dataset from the domain of livestreaming videos. Livestreaming videos are one of the main modern mediums of communication and an increasing number of livestreaming videos are becoming a potential knowledge base.  The paper describes many distinctive characteristics of livestreaming video transcripts that are essential to study for the task of punctuation restoration as well as for the field of automatic speech recognition.
The paper shows how the state-of-art model performs poorly on this dataset raising the need to develop models which can deal with the practical challenges posed by the dataset.
The paper also demonstrates that the current models performance degrades when the domain is changed from formal to informal domains even when they are almost monologual. Further study of this problem is very important.
This paper also reveals how existing NLP toolkits fail in the simple task of sentence boundary identification without the availability of proper punctuation. This suggests the need for the inclusion of punctuation restoration models, which can handle the type of challenges described in the paper, in these toolkits.
The dataset will be made public which will help the progress of the field. ",None ,"The authors may include some discussion on inter-annotator disagreements.
8 annotators were used but only the roles of 6 annotators are described. Was there any hierarchical annotation role?
Livestreaming videos tend to have more word eros. What is the impact of word errors?
In domain adaptation, if the models are trained on the proposed dataset and tested on the existing dataset, does it perform better than the other way around? ",4.5 ,Maybe,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"
The authors may include some discussion on inter-annotator disagreements.
8 annotators were used but only the roles of 6 annotators are described. Was there any hierarchical annotation role?
Livestreaming videos tend to have more word eros. What is the impact of word errors?
In domain adaptation, if the models are trained on the proposed dataset and tested on the existing dataset, does it perform better than the other way around? "
ARR_2022_246_review,"The paper proposes a minor improvement to the existing word sense disambiguation approaches. Its aim is to overcome the problem of training data imbalance (not all words have the same number of senses, and this is related to their frequencies). 
Essentially, the authors assign different weights to words during training, depending on their frequencies. Empirically, this results in improvements in WSD performance on several English datasets. ","The proposed re-weighting approach is simple and focused, and the improvements are persuasive. I really like that it is linguistically motivated (based on Zipf's law) . ","- Although the proposed approach does bring WSD improvements, it is rather incremental. This is probably not a ""weakness"" per se, just that the paper is not an eye-opener.  - The evaluation is done on English data only, which leaves some doubts about how this would work with other languages. ","L. 135: "" linguistic law exists in many corpora"" --> probably  ""linguistic law is manifested in many corpora""? The whole statement is a bit strange: Zipf's law is well known to be manifested not ""in many corpora"", but in _all_ human languages, and thus, in any imaginable corpus of natural language texts. ",3.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Although the proposed approach does bring WSD improvements, it is rather incremental. This is probably not a ""weakness"" per se, just that the paper is not an eye-opener.  - The evaluation is done on English data only, which leaves some doubts about how this would work with other languages. 
L. 135: "" linguistic law exists in many corpora"" --> probably  ""linguistic law is manifested in many corpora""? The whole statement is a bit strange: Zipf's law is well known to be manifested not ""in many corpora"", but in _all_ human languages, and thus, in any imaginable corpus of natural language texts. "
ARR_2022_27_review,"This paper studies the problem of lifelong learning in the context of language model pretraining. The goal is to continuously learn new pretraining data domains for a language model such that we do not need to retrain a new one from scratch. They propose a method called ELLE, which aims to expand the current model's width and depth upon a new data source is available. A few related techniques are also applied to further boost performance, e.g. function recovering finetuning, domain specific tokens. Overall, the method is valid with plenty of experimental results, yet its practicality remains unclear. ","1. This paper studies lifelong learning for model pretraining. This setup is less studied in lifelong learning literature. 
2. The proposed method works well on all experiments considered, with an astonishing amount of analysis (in the appendix). 
3. The paper is easy to follow and well written. ","A critical limitation is how practical the method is given the nature of model expansion. This method is studied under a still relatively synthetic setup. There are a lot of problems that I can imagine for real-world large scale models such as GPT3. To mention a few: (1) How should we decide when to expand the model? In reality, these models are trained on O(100B) data, covering a significant portion of the whole internet. So when do we know a new data source emerges? How to detect this? ( 2) Large scale models are trained using sharding systems of thousands of accelerators, it is a challenging technical question to expand them in a way described in this paper. ( 3) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization.
[1] Finetuned Language Models Are Zero-Shot Learners. Wei et al., 2021. 
[2] Program Synthesis with Large Language Models. Austin et al., 2021. ",How is the prompt token computed at inference time? Is it included using nearest neighbor search or not included at all? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"A critical limitation is how practical the method is given the nature of model expansion. This method is studied under a still relatively synthetic setup. There are a lot of problems that I can imagine for real-world large scale models such as GPT3. To mention a few: (1) How should we decide when to expand the model? In reality, these models are trained on O(100B) data, covering a significant portion of the whole internet. So when do we know a new data source emerges? How to detect this? ( 2) Large scale models are trained using sharding systems of thousands of accelerators, it is a challenging technical question to expand them in a way described in this paper. ( 3) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization.
[1] Finetuned Language Models Are Zero-Shot Learners. Wei et al., 2021. 
[2] Program Synthesis with Large Language Models. Austin et al., 2021. 
How is the prompt token computed at inference time? Is it included using nearest neighbor search or not included at all? "
ARR_2022_28_review,"This paper discusses how to transfer text from one domain to another by preserving some attributes (constraints). The paper proposes an extension to the ARAE architecture. In particular, the authors introduce cooperative losses to better model the constraints. The proposed method seems reasonable and in general using the cooperative losses seem nice. The extensive experimental evaluation seems to confirm the validity of the model. ","The presented method is reasonable. Moreover, the presented problem is relevant to the CL community. The experimental evaluation is extensive and it provides many insights. ","The main concerns with this paper is that it doesn't fully explain some choices in the model (see comments/questions section). Moreover, some parts of the paper are actually not fully clear. Finally, some details are missing, making the paper incomplete. ","- Algorithm 1 is not really explained. For example, at each step (1, 2, 2a, 3, 3a) are you sampling a different batch from S and T? Is the notation L(X) meaning that you optimize only the parameters X of the architecture?
- Line 232: When you say you ""mine"", what do you exactly mean? Does this mean you sample P sentences from the set of sentences of S and T with similar constraints?
- Lines 237-238 and Line 262: Why would you want to use the representation from the critic last layer?  - Line 239: ""Ci are a set of constraints for a sentence"" should be moved before.
- Table 1: It seems that the results for DRG and ARAE are not averaged over 5 runs (they're exactly the same of the previous paper version) - Table 1: How did you choose the p=0.6?
- Table 1: row=ARAE, column=POLITICAL-FL It seems this value should be the one in bold.
- Lines 349-353: It seems you're comparing results for ARAE + CONTRA, ARAE + CLF and ARAE + CONTRA + CL with respect to simple ARAE, while in the text you mention only ARAE + CONTRA and ARAE + CLF.
- Line 361: and SIM to -> and SIM with respect to - Figure 3: Please, rephrase the caption of the errors bars (or explain it in the text). It is not clear what do you mean.
- Line 389: You mention here you used different p values as in Table 1. This table doesn't report results with different values for p. - Lines 422-423: why using nucleous sampling when the best results were with greedy decoding? Where does 0.9 come from?
- In general, in the experiments, what are the source and target domains?
- Line 426-Table4: What do you want to demonstrate here? Could you add an explanation? What constraints/attributes are preserved? What is the source domain? What is the target domain?
- Lines 559-560: This is not entirely true. In Cycle Consistency loss you can iterate between two phases of the reconstructions (A-B-A and B-A-B) with two separate standard backpropagation processes.
- Line 573: works focuses -> works focus ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The main concerns with this paper is that it doesn't fully explain some choices in the model (see comments/questions section). Moreover, some parts of the paper are actually not fully clear. Finally, some details are missing, making the paper incomplete. 
- Algorithm 1 is not really explained. For example, at each step (1, 2, 2a, 3, 3a) are you sampling a different batch from S and T? Is the notation L(X) meaning that you optimize only the parameters X of the architecture?
- Line 232: When you say you ""mine"", what do you exactly mean? Does this mean you sample P sentences from the set of sentences of S and T with similar constraints?
- Lines 237-238 and Line 262: Why would you want to use the representation from the critic last layer?  - Line 239: ""Ci are a set of constraints for a sentence"" should be moved before.
- Table 1: It seems that the results for DRG and ARAE are not averaged over 5 runs (they're exactly the same of the previous paper version) - Table 1: How did you choose the p=0.6?
- Table 1: row=ARAE, column=POLITICAL-FL It seems this value should be the one in bold.
- Lines 349-353: It seems you're comparing results for ARAE + CONTRA, ARAE + CLF and ARAE + CONTRA + CL with respect to simple ARAE, while in the text you mention only ARAE + CONTRA and ARAE + CLF.
- Line 361: and SIM to -> and SIM with respect to - Figure 3: Please, rephrase the caption of the errors bars (or explain it in the text). It is not clear what do you mean.
- Line 389: You mention here you used different p values as in Table 1. This table doesn't report results with different values for p. - Lines 422-423: why using nucleous sampling when the best results were with greedy decoding? Where does 0.9 come from?
- In general, in the experiments, what are the source and target domains?
- Line 426-Table4: What do you want to demonstrate here? Could you add an explanation? What constraints/attributes are preserved? What is the source domain? What is the target domain?
- Lines 559-560: This is not entirely true. In Cycle Consistency loss you can iterate between two phases of the reconstructions (A-B-A and B-A-B) with two separate standard backpropagation processes.
- Line 573: works focuses -> works focus "
ARR_2022_28_review,"This paper presents a new method to constrain certain lexical, syntactic and domain-specific aspects of sentence during style transfer. The method is based on the introduction of two complementary losses, a self-supervised contrastive loss that controls the distance between samples and a classification loss that aims to satisfy the constraints. The method is evaluated on multiple datasets, using a range of automatic metrics as well as a human evaluation. The authors highlight the importance of enforcing such constraints of identity (e.g., text length and descriptiveness) during style transfer, which can be useful for data augmentation and domain adaptation applications. ","This work proposes a useful method transfer from vision to NLP. The proposed method can be useful to various applications in NLP, it might improve model robustness in data augmentation and domain adaptation scenarios. The paper accurately summarizes related work and reflects upon open issues. The presented problem is addressed with an appropriate method and is evaluated extensively. ","I have reviewed this paper in the previous round and was happy to see that the authors have addressed the previously found weaknesses: - Method: The method section has been re-structured and as a result became much easier to read.
- Human evaluation: The appendix now provides more extensive information, including details on annotator consent.
- The applications of this work are described more clearly - One thing that is still missing is a short discussion of possible limitations. ","- Some crucial information is only presented in the appendix (most importantly the details on the evaluation metrics, which cannot be understood without reading the appendix). This should be included in the main paper if accepted. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I have reviewed this paper in the previous round and was happy to see that the authors have addressed the previously found weaknesses: - Method: The method section has been re-structured and as a result became much easier to read.
- Human evaluation: The appendix now provides more extensive information, including details on annotator consent.
- The applications of this work are described more clearly - One thing that is still missing is a short discussion of possible limitations. 
- Some crucial information is only presented in the appendix (most importantly the details on the evaluation metrics, which cannot be understood without reading the appendix). This should be included in the main paper if accepted. "
ARR_2022_256_review,This paper proposes an unsupervised knowledge distillation framework with a multiple-task multiple-teacher model for cross-language named entity recognition. An entity similarity evaluator is proposed to help NER as an auxiliary task. The results show that the approach outperforms baselines on seven languages. ,"1. 	This is the first work that using multi-task learning in cross-lingual NER. 
2. 	The similarity metric model boosts the performance as an auxiliary task part. 
3. 	The experiments show that the proposed approach performs better than the baselines. ","1. 	Innovation is weak, only the modification of existing methods. Although existing cross-lingual NER methods don’t consider multi-task learning, it has been widely used in transfer learning/unsupervised learning. 
2. 	The experiment is not sufficient. A key contribution of this work is the teacher-student framework. But little experiments are carried out to prove its effectiveness. It seems that MTST in ablation study aims to prove it but the description “multiple-teacher to single-teacher” and “teacher and student have the same neural network structure” is confusing. ","There are some grammatical mistakes, such as a)	in line 115, it should be ""that introduces an entity similarity evaluator""; b)	etc. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. 	Innovation is weak, only the modification of existing methods. Although existing cross-lingual NER methods don’t consider multi-task learning, it has been widely used in transfer learning/unsupervised learning. 
2. 	The experiment is not sufficient. A key contribution of this work is the teacher-student framework. But little experiments are carried out to prove its effectiveness. It seems that MTST in ablation study aims to prove it but the description “multiple-teacher to single-teacher” and “teacher and student have the same neural network structure” is confusing. 
There are some grammatical mistakes, such as a)	in line 115, it should be ""that introduces an entity similarity evaluator""; b)	etc. "
ARR_2022_256_review,"In this paper, the authors propose an unsupervised multiple-task and multiple-teacher model for cross-lingual NER. The student model learns two source language patterns of entity recognition and entity similarity evaluation. They propose a weighting strategy to take consideration of the reliability of the teachers. Experimental results show that the proposed model yields significant improvements on six target language datasets and outperforms the existing state-of-the-art approaches. ","1. The authors propose an unsupervised knowledge distillation framework for cross-language named entity recognition and develop a teaching and learning procedure under this framework. 
2. The authors present a novel multiple-task and multiple-teacher model that introduces a entity similarity evaluator to boost the performance of student recognizer on target languages. 
3. Extensive experiments on seven languages compared with SOTA baselines and the results confirm the effectiveness of MTMT model. ","1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. 
2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.
[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J]. arXiv preprint arXiv:2110.08151, 2021. 
[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020. ","This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.
Please try more multilingual feature extractors and test MTMT model on some low-resourced languages. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. 
2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.
[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J]. arXiv preprint arXiv:2110.08151, 2021. 
[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020. 
This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.
Please try more multilingual feature extractors and test MTMT model on some low-resourced languages. "
ARR_2022_256_review,"The paper proposes a multi-task knowledge distillation framework for unsupervised cross-lingual NER applications. This framework consists of two teacher training models as two tasks, i.e., Entity recognizer teacher and similarity evaluator teacher for entity recognition and token type identification, respectively in the source language. Similarity evaluator teacher model is based on siamese neural network. It is trained as a similarity prediction task with two uniformly sampled tokens from two input sentences. The input sentences are randomly selected from the input dataset. Then a Teacher Student Distillation model is simultaneously trained with both teachers’ soft labels to improve the representation for the target language. For student model training, the loss is constructed as weighted supervisory singles from different teacher models.  The experiments are conducted on three datasets and six low-resource languages. The model performance is compared with several existing baselines. To understand the insights of the model, an ablation study, case study, and other analyses were conducted. ","1. The paper is well written and easy to follow. 
2. Cross-lingual NER is a challenging problem, and the proposed approach can be easily scaled to more low-resource languages. 
3. Ablation study and multiple analysis experiments are the strength of the paper. 
4. The idea of using the additional similarity evaluator teacher model in the multi-tasking distillation framework is novel and interesting. ","1. I am concerned about the intuition/motivation of the Similarity evaluator teacher model. Paper reported a concrete example in lines 70-85. There can be a possibility of negative transfer. Let’s consider a counter-example: suppose for a low-resource language LR with sentences S1 = {w1, w2, w3, w4}, S2 = {w1, w5, w3, w4} and S3 = {w1, w6, w3, w4}. Where (w2, w5) and (w2, w6) are considered as token pairs by Similarity teachers. The English NER model predicts correct labels Lc for w2 and incorrect label Lic for w5 and w6. While training with the student model, it is possible that the similarity evaluator teacher may provide incorrect supervision for w2 as the other two tokens have incorrect labels. 
2. It is unclear how many tokens are uniformly sampled from each sentence while Entity Similarity Pairs Construction and how the token pairing has been done? There should be a label distribution analysis of the source language training dataset used for the Similarity evaluator teacher model. ","1. Some sentences are confusing. For example (1) lines 7-11 sentence. In particular, what is the meaning of the phrase “identical single task across both domain.” 
2. In line 23, it should be: …on the three datasets…. 
3. In lines 523-524, it is unclear what embedding distribution is considered from the student model; add more detail. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1. I am concerned about the intuition/motivation of the Similarity evaluator teacher model. Paper reported a concrete example in lines 70-85. There can be a possibility of negative transfer. Let’s consider a counter-example: suppose for a low-resource language LR with sentences S1 = {w1, w2, w3, w4}, S2 = {w1, w5, w3, w4} and S3 = {w1, w6, w3, w4}. Where (w2, w5) and (w2, w6) are considered as token pairs by Similarity teachers. The English NER model predicts correct labels Lc for w2 and incorrect label Lic for w5 and w6. While training with the student model, it is possible that the similarity evaluator teacher may provide incorrect supervision for w2 as the other two tokens have incorrect labels. 
2. It is unclear how many tokens are uniformly sampled from each sentence while Entity Similarity Pairs Construction and how the token pairing has been done? There should be a label distribution analysis of the source language training dataset used for the Similarity evaluator teacher model. 
1. Some sentences are confusing. For example (1) lines 7-11 sentence. In particular, what is the meaning of the phrase “identical single task across both domain.” 
2. In line 23, it should be: …on the three datasets…. 
3. In lines 523-524, it is unclear what embedding distribution is considered from the student model; add more detail. "
ARR_2022_256_review,"The paper studied how to use both knowledge distillation and multitask learning to improve the performance of the cross-lingual NER task. Two student models are supervised by two teacher models. One of them handles the entity recognition task, the other is responsible for entity similarity. Experiments demonstrate the effectiveness of the model. ","1. 	The paper proposes to combine multitask learning and knowledge distillation to discover more knowledge in the cross-lingual NER task. 
2. 	Experiments results are good. ","1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. ","1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' ",2.5 ,No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",5 = They could easily reproduce the results.,,"1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' "
ARR_2022_338_review,This work proposes to explicitly model sentence-level representations of both the source and target side of unsupervised machine translation. The authors utilize normalizing flows to model the sentence representations in a flexible space as transformed from a (shared between languages) simple base distribution. At translation time the invertibility of normalizing flows can be used to map between sentence representations in different languages. In experiments the authors test the methods' viability on many language pairs and show competitive performance across the board. ,"- The proposed method seems sound and novel.
- The authors run extensive experiments on unsupervised machine translation and show moderate improvements across the board. Applying the method on top of XLM seems to result in good improvements over existing techniques, except for MASS.
- The paper is mostly well-written except for one crucial point mentioned below in the weaknesses. ","- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.
- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.
- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper. ","Some typos: - 001: ""The latent variables"" -> ""Latent variables"" - 154: ""efficiently to compute"" -> ""efficient to compute"" - 299: ""We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder"" - unclear - 403: ""langauge"" -> ""language"" ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.
- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.
- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper. 
Some typos: - 001: ""The latent variables"" -> ""Latent variables"" - 154: ""efficiently to compute"" -> ""efficient to compute"" - 299: ""We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder"" - unclear - 403: ""langauge"" -> ""language"" "
ARR_2022_143_review,This paper proposes a cluster based compact network and a cluster-based pruning strategy for k-nearest-neighbor machine translation for domain adaptation. The proposed methods are able to achieve low latency and keep the BLEU scores of translation accuracies. Experiments on a couple of datasets show the good performance of the methods described in this paper. ,Strong: 1. 	Novel cluster-based compact network and pruning strategies for non-parameter domain adaptation in machine translation; 2. 	Better results of balancing decoding latency and accuracy. ,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. ","Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). "
ARR_2022_143_review,"This paper propose a more efficient KNN-based MT approach to non-parametric models for machine translation. Instead of learning extra no. of parameters to fine-tune / adapt a model, KNN-based MT learns a smaller meta network without unfreezing the pre-trained base model. The proposed method uses clustering to reduce the features that has to be retrieved through NCE. Further pruning on the reduced dimension of the datastore before the KNN retrieval. ","1. It's really great to see the authors put effort into component analysis, e.g. notable mentions: 1a: Table 3, comparing the NCE compression to SVD and PCA (it's an arduous effort to run SVD and PCA on features on MT-tasks size datasets). 
1b. Section 4.4.2. comparing different pruning methods 2. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design ","1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
2. Replicability of method is not clear, there's no indication that the code will be released. ","- I might have missed the specific in the paper; hopefully, I get CKMT = ""Compact-network K-nearest-neighbor MT"" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = ""Pruned CKMT"", hope I get that right too. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
2. Replicability of method is not clear, there's no indication that the code will be released. 
- I might have missed the specific in the paper; hopefully, I get CKMT = ""Compact-network K-nearest-neighbor MT"" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = ""Pruned CKMT"", hope I get that right too. "
ARR_2022_337_review,This paper mainly investigates prompt-based finetuning in few-shot learning. It demonstrates two strategies that can reduce the requirement for prompts design and memory cost: null prompts tuning and that it can be made memory efficient by finetuning only the bias terms. ,"1. It demonstrates that prompt-based finetuning can be achieved via null prompts, that alleviates the effort of designing prompt patterns across different tasks/datasets, which is more generalizable, and questions the requirement of well designed prompt patterns. 
2. It demonstrates that by only finetuning the bias term, the finetuning process can be highly memory efficient, with only updating 0.1% of the parameters. ","1. From the results in Table 2, it shows that in ALBERT, with null prompt, the performance largely decreases (from 8 #Wins to 1), which raises the question of the necessity and robustness of null prompting. Moreover, it shows that prompt-based finetuning with BitFit can achieve comparable or even better performance, with memory efficiency introduced. It suggests that only using BitFit is a more wise choice. ","1. It may be better to conduct more experiments across different models. 
2. It may be better to analyze the principle of the success of null prompts instead of only experimental evaluations. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. From the results in Table 2, it shows that in ALBERT, with null prompt, the performance largely decreases (from 8 #Wins to 1), which raises the question of the necessity and robustness of null prompting. Moreover, it shows that prompt-based finetuning with BitFit can achieve comparable or even better performance, with memory efficiency introduced. It suggests that only using BitFit is a more wise choice. 
1. It may be better to conduct more experiments across different models. 
2. It may be better to analyze the principle of the success of null prompts instead of only experimental evaluations. "
ARR_2022_259_review,"The paper presents a method for representing the relevance of a linguistic dataset to the corresponding language and its speakers. As a proxy for speakers of a certain language the authors use geographical entities, particularly countries. The representation they aim to build relies on entity linking, so the authors explore this problem on several multilingual datasets, and draw conclusions regarding the cross-lingual consistency of NER and EL systems. ","The paper addresses an important problem, that gives a new way of assessing the representativeness of a dataset for a specific language. Since such text collections are at the basis of every other language task, and provide language models on which much of the higher level processing is based, it is important to have collections that are representative for the language (and speakers) that are targeted. ","While the main idea of the paper is valuable and interesting, and thoroughly explored, it is based on some assumptions whose soundness is debatable. Details are in the comments section.
- there is a disconnect between the visualizations and the rest of the processing.
- the preprocessing of the datasets (many for low-resource languages) needs resources that are themselves scarce, incomplete, or borrowed from other languages (that may use other scripts, and hence there is a transliteration problem on top of others). This makes the kind of processing presented here a bit unrealistic, in the sense that it could not be deployed on any collected dataset, and give an objective view of the representativeness of that dataset for the corresponding language (this is linked to the first point, and explanations are below) - some information in the data is discarded (topical adjectives, historical entities), and it is not clear what impact using it would have on the final geographical mapping. ","With regards to the disconnect between the visualizations and the rest of the processing: the visualizations are based on geographical statistics for entities in a text, but these entities are already marked. It would have been useful to see how an end-to-end process performs: apply NER on the NER and QA datasets, and build the same visualizations as in section 3. How do the visualization using imperfect NER/EL resources and processing tools compare to the visualizations obtained on the annotated data? Are they very far apart, or the underlying ""character"" of the dataset is still retrievable even in such imperfect conditions? This links to the second potential weakness, regarding the applicability of this method to newly collected datasets (which is the aim, right?).  The geographical mapping presented is left to the subjective inspection of a human judge. Which is not necessarily bad in itself, but as the more detailed maps in the appendix show, the characteristics of some datasets are very very similar (e.g. for European countries for example, or other geographically close countries). It may be useful to have a more rigorous evaluation of the geographical mapping, by showing that from the geographical distribution of entities, one can predict the country corresponding to the dataset's language. This could be done in an unsupervised manner, or using a linear regression model, or something similarly simple -- maybe by deducting an ""average"" entity geographical distribution model, such that local characteristics become more prominent, or by computing (in an unsupervised manner) some weights that would downplay the contribution of entities from countries that are always represented (like a ""country tfidf"" maybe?).  Some geographical indicators are disregarded, and that may have an impact on the visualizations. Annotating topical adjectives that indicate countries seems doable, based on the anchor texts of links pointing to countries, which are easy to obtain (for some languages). The same for some of the historical entities that no longer exist, but some of which have corresponding GPS coordinates that could be used. The point is that both the resources and the process used to build the geographical maps of the datasets are incomplete. Some are by necessity (because the available resources are incomplete), some by choice (the adjectives and historical figures). We need to know the impact of such processing constraints.
It is interesting to analyze the correlation between socio-economic factors, but how does that impact the construction or characteristics of the datasets? Some of these factors -- e.g. the GDP, -- could be (in this experiment) a proxy for the level of web presence of the population, and the level of information digitization of that particular population. Maybe some parameters that measure these issues more explicitly -- which seem more closely relevant to the process of textual collection building -- would provide better insights into data characteristics.
Using a country as a proxy for language is useful, but it may skew the data representation, as the authors themselves recognize. What happens with languages that occupy the same country-level geographical space, but are distinct, as happens with multi-lingual countries? The same with languages that cross many borders. A bit more insight into how these are reflected in dataset characteristics and how they impact the usefulness of the dataset would be very useful.
Why does the cross-language consistency matter here? Each dataset (for the geographical mapping) is analyzed separately, so while cross-lingual consistency is indeed a problem, it is not clear how it is related to the problem of dataset mapping. Is the cross-lingual consistency a signal of something other than the general performance of NER/EL systems?  Some little typos: were => where (almost everywhere ""were"" appears) then => than (line 319) ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"While the main idea of the paper is valuable and interesting, and thoroughly explored, it is based on some assumptions whose soundness is debatable. Details are in the comments section.
- there is a disconnect between the visualizations and the rest of the processing.
- the preprocessing of the datasets (many for low-resource languages) needs resources that are themselves scarce, incomplete, or borrowed from other languages (that may use other scripts, and hence there is a transliteration problem on top of others). This makes the kind of processing presented here a bit unrealistic, in the sense that it could not be deployed on any collected dataset, and give an objective view of the representativeness of that dataset for the corresponding language (this is linked to the first point, and explanations are below) - some information in the data is discarded (topical adjectives, historical entities), and it is not clear what impact using it would have on the final geographical mapping. 
With regards to the disconnect between the visualizations and the rest of the processing: the visualizations are based on geographical statistics for entities in a text, but these entities are already marked. It would have been useful to see how an end-to-end process performs: apply NER on the NER and QA datasets, and build the same visualizations as in section 3. How do the visualization using imperfect NER/EL resources and processing tools compare to the visualizations obtained on the annotated data? Are they very far apart, or the underlying ""character"" of the dataset is still retrievable even in such imperfect conditions? This links to the second potential weakness, regarding the applicability of this method to newly collected datasets (which is the aim, right?).  The geographical mapping presented is left to the subjective inspection of a human judge. Which is not necessarily bad in itself, but as the more detailed maps in the appendix show, the characteristics of some datasets are very very similar (e.g. for European countries for example, or other geographically close countries). It may be useful to have a more rigorous evaluation of the geographical mapping, by showing that from the geographical distribution of entities, one can predict the country corresponding to the dataset's language. This could be done in an unsupervised manner, or using a linear regression model, or something similarly simple -- maybe by deducting an ""average"" entity geographical distribution model, such that local characteristics become more prominent, or by computing (in an unsupervised manner) some weights that would downplay the contribution of entities from countries that are always represented (like a ""country tfidf"" maybe?).  Some geographical indicators are disregarded, and that may have an impact on the visualizations. Annotating topical adjectives that indicate countries seems doable, based on the anchor texts of links pointing to countries, which are easy to obtain (for some languages). The same for some of the historical entities that no longer exist, but some of which have corresponding GPS coordinates that could be used. The point is that both the resources and the process used to build the geographical maps of the datasets are incomplete. Some are by necessity (because the available resources are incomplete), some by choice (the adjectives and historical figures). We need to know the impact of such processing constraints.
It is interesting to analyze the correlation between socio-economic factors, but how does that impact the construction or characteristics of the datasets? Some of these factors -- e.g. the GDP, -- could be (in this experiment) a proxy for the level of web presence of the population, and the level of information digitization of that particular population. Maybe some parameters that measure these issues more explicitly -- which seem more closely relevant to the process of textual collection building -- would provide better insights into data characteristics.
Using a country as a proxy for language is useful, but it may skew the data representation, as the authors themselves recognize. What happens with languages that occupy the same country-level geographical space, but are distinct, as happens with multi-lingual countries? The same with languages that cross many borders. A bit more insight into how these are reflected in dataset characteristics and how they impact the usefulness of the dataset would be very useful.
Why does the cross-language consistency matter here? Each dataset (for the geographical mapping) is analyzed separately, so while cross-lingual consistency is indeed a problem, it is not clear how it is related to the problem of dataset mapping. Is the cross-lingual consistency a signal of something other than the general performance of NER/EL systems?  Some little typos: were => where (almost everywhere ""were"" appears) then => than (line 319) "
ARR_2022_81_review,"This paper introduces a new multimodal dialogue state tracking dataset, with simulated dialogues of moving objects that have varying shapes, colors, and sizes. They also design a novel Video Dialogue Transformer Network as a strong baseline for the task. ",The dataset and model designed in this work appear to be novel and would be broadly useful to the dialogue systems and multimodal learning communities. ,"As the authors mention, the dataset is limited to simulated dialogues of simple objects, rather than human objects, but this is a preliminary study. In addition, it is not specified whether the data and code will be publicly released, which is the primary contribution. Some of the baselines used were weak. Other stronger baselines should be considered, e.g., SUMBT, DSDST, GCDST, SOM-DST, CSFN-DST, MinTL, and Pegasus. ","- line 249: ""tuned"" -> ""tune"" - line 325: is a word missing after ""linear""?
- line 343: ""recovers"" -> ""recover"" - please clarify what is meant in line 375 by ""harmful"" and annotation bias in line 373 - line 494: ""Asides"" -> ""Aside"" - lines 528-529: should ""only improves the results significantly"" be ""insignificantly""?
- line 541: ""We observed that"" is repeated twice - line 584: please elaborate on what is meant by filtering the videos and training a new model, as well as how to interpret Table 5 ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"As the authors mention, the dataset is limited to simulated dialogues of simple objects, rather than human objects, but this is a preliminary study. In addition, it is not specified whether the data and code will be publicly released, which is the primary contribution. Some of the baselines used were weak. Other stronger baselines should be considered, e.g., SUMBT, DSDST, GCDST, SOM-DST, CSFN-DST, MinTL, and Pegasus. 
- line 249: ""tuned"" -> ""tune"" - line 325: is a word missing after ""linear""?
- line 343: ""recovers"" -> ""recover"" - please clarify what is meant in line 375 by ""harmful"" and annotation bias in line 373 - line 494: ""Asides"" -> ""Aside"" - lines 528-529: should ""only improves the results significantly"" be ""insignificantly""?
- line 541: ""We observed that"" is repeated twice - line 584: please elaborate on what is meant by filtering the videos and training a new model, as well as how to interpret Table 5 "
ARR_2022_81_review,"This paper introduced an extended definition which extends unimodal DST to multimodal DST. To demonstrate this new schema of DST, the authors generated dialogues based on an extended CATER video split, with additional ground-truth labeling for bounding boxes and visual objects across frames.  The dialogues mainly focus on visually-grounded object reasoning, where a question is raised to query the states of objects in the given video clip.
The proposed DST definition comprises of dialogue states from the traditional DST setting and dialogue states that represent the states of visible objects. Two temporal-based slots are also added such that an object can be located by each turn.
The model architecture is based on an autoregressive approach, where the dialogue states of last turn along with turn utterances (t-1 and t) are given to generate the frame start/end and object states. The backbone is Transformer block, and Mask Vision Modeling (similar to Mask Language Modeling) is used to self-supervise the learning of visual representations.
In experiments, the authors presented several baseline models to evaluate different aspects of the proposed DVD-DST dataset. The model outperformed these baselines, and analyses on model components were provided. ","1. The paper is generally well-written. Most of the content does not cause confusion. 
2. The introduction of Multimodal DST (MM-DST) is interesting. Though there are some concurrent works in introducing MM-DST, the paper defines a more complete set of dialogue states: single object to multiple, and maintaining object states also becomes a DST task. This new definition is in general convincing and promising, and can be extended to more complex MM-DST problems. 
3. The creation of the dataset refers to prior work and took into consideration the distribution bias of datasets. Potential biasedness was further discussed with some simple baseline models proposed in Sec. 4. It can be seen that the dataset is generally in high quality. 
4. The video self-supervised training improved the overall performance, showing that with this technique, the Transformer blocks learn to distinguish visual cues and learn meaningful representations. 
5. The experiments and ablation studies are rich and cover most of potential research questions. ","1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements.  This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines? 
2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509. 
3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning. 
4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required. ","1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}: 4 should be x1 x2 y1 y2, and this should be stated. Table 3:  the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough. 
2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work. 
3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since ""DVD"" seems to distract readers from its real meaning. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,No ethical concerns are outstanding. ,"1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements.  This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines? 
2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509. 
3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning. 
4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required. 
1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}: 4 should be x1 x2 y1 y2, and this should be stated. Table 3:  the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough. 
2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work. 
3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since ""DVD"" seems to distract readers from its real meaning. "
ARR_2022_38_review,"This paper describes a framework to inculcate temporal order into question answering over knowledge bases. They add a time estimation module to the temporal KGQA. They also add an auxiliary contrastive loss to improve time sensitivity of the encoder, particularly for answer prediction and time estimation for questions that differ only by a time relation word (e.g., before/after). They also add one to improve time-order classification between timestamp embedding pairs. Their framework also has a narrower search space for faster training and inference. This model achieves state-of-the-art performance on CronQuestions dataset. ","The paper proposes a framework that pushes the state-of-the-art on the CronQuestions dataset significantly. The authors clearly outline the shortcomings of previous work and propose solutions to each of them. Apart from performance improvements, they also improve training and inference speeds by finding a way to narrow the search space. ","1. All the baselines that have been compared against look like incremental work built on top of each other. It is possible that other models have not been built on CronQuestions, so it would be better if authors had repurposed other work for this dataset. ",Please update the citation for CronQuestions dataset to include the ACL 2021 paper link. The arXiv link misled me initially. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. All the baselines that have been compared against look like incremental work built on top of each other. It is possible that other models have not been built on CronQuestions, so it would be better if authors had repurposed other work for this dataset. 
Please update the citation for CronQuestions dataset to include the ACL 2021 paper link. The arXiv link misled me initially. "
ARR_2022_37_review,"This work presents a novel NER learning framework to address the poor performances issue on out of vocabulary (OOV) entity recognition; and, more in general, to help NER systems to generalize better and avoid ""mention"" memorization. To do so, the authors propose an approach based on two mutual information objectives: 1) generalizing the information maximization and 2) superfluous information minimization. ","1. The approach seems to work very well on the datasets used by the authors and to achieve a new state of the art in the OOV entity recognition task. 
2. Both the ablation and the interpretability analysis seem to back the authors' claims and give a complete view of the proposed framework. 
3. I liked the very comprehensive and well-documented choice of the comparison systems. 
4. Even if I am not an expert in information-theoretic approaches, their approach seemed intuitive and explained very well with meaningful examples. ","1. The authors provide the performance improvement of the proposed approach on the OOV dataset but do not show how the performances on the NER datasets change (if they). 
2. I am not an expert in the field, so I don't know other span-based approaches other than the ones the authors cite, but it seems very expensive in terms of speed classifying all the possible spans in possibly long input sentences. ","line 128: BN has never been defined. 
line 216: subdividing -> subdivide? 
line 420: the the -> the line 430: MOreover -> Moreover line 436: 5Our -> 5 Our There are other typos that I did not mention, like parentheses starting right after the word without any space, more in general, the paper needs a good session of typos polishing. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The authors provide the performance improvement of the proposed approach on the OOV dataset but do not show how the performances on the NER datasets change (if they). 
2. I am not an expert in the field, so I don't know other span-based approaches other than the ones the authors cite, but it seems very expensive in terms of speed classifying all the possible spans in possibly long input sentences. 
line 128: BN has never been defined. 
line 216: subdividing -> subdivide? 
line 420: the the -> the line 430: MOreover -> Moreover line 436: 5Our -> 5 Our There are other typos that I did not mention, like parentheses starting right after the word without any space, more in general, the paper needs a good session of typos polishing. "
ARR_2022_37_review,"The paper proposes an approach with an information-theoretic solution for the out-of-vocabulary (OOV) entity recognition problem. Specifically, the proposed approach contains two mutual information based training objectives: i) Information maximization between context and entity surface forms. 2) Superfluous information minimization, which entails minimizing task-non-specific information. Both these objectives are modeled after the information bottleneck principle in information theory science.
The paper leverages several experimental datasets and baselines and in the end demonstrates the robustness and effectiveness of the proposed solution. ","1) The additional of the information theoretic approach within the current state-of-the-art for NER extraction i.e. SpanNER architecture is not just creative but an elegant engineering solution. It seems to extend the state-of-the-art incrementally, while showing a nice understanding of addressing precisely some problems or information leakages that can help boost NER for OOV NER.
2) The solution proposed tackles a relevant advancement of the NER problem in the NLP community specifically OOV NER.
3) From this paper, one can gain new insights into the OOV NER problem in terms of both the problem itself and the solution.  4) One can also gain a surveyistic birds-eye-view on the state-of-the-art systems and available datasets for the task.
5) The experiments while they could have been better structured/organized (see my comments in “Summary of Weaknesses”) are still informative, and offer sound and meaningful observations. ","1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important.  2) The paper has the following phrase explaining the problem that it wishes to address: “entail preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” However, majority of the paper and indeed the method itself seems to then emphasis the following specific problem, i.e. “eliminating task-specific nuisances.” I think maybe the former line should then either be removed or somewhere it state whether it is a hypothesis of the authors that “eliminating task-specific nuisances” indeed means “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” Perhaps the authors would need another whole paper to analyze what “eliminating task-specific nuisances” w.r.t. the proposed approach precisely means. So I would not say this as a weakness of this paper. However, is the phrase “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information” is used, it would help the reader if a connection is made to the actual solution which is mostly referred to as “eliminating task-specific nuisances.”
3) I also wonder if the authors would be willing to rephrase “nuisances” in “eliminating task-specific nuisances?” Maybe as “noise?”
4) Table 4. Curious why the ablation results are not shown on the CONLL 2003 dataset even though the method seems to have a maximum impact on it? Further, the CONLL 2003 – OOV data in particular seems quite relevant in this experimental setting. Can the authors provide these results? Otherwise the empirical analysis would seem incomplete to me by missing a seeming very relevant detail.  5) Not a weakness of this work as such. But maybe as a suggestion for future work. Lin et al [1] described four categories of artificially simulating OOV scenarios in NER. An empirical analysis with the method proposed in this work over their dataset or at least a dataset using their OOV synthesis methodology would be very interesting to have particularly on this problem of OOV NER.
6) The authors did not mention MINER code will be publicly released if the paper were accepted.
---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. ","Table 1 in this paper corresponds exactly to Table 3 Vanilla Baseline in Lin et al. [1]. However, Lin et al. mention using the ACE 2005 dataset (paragraph 1 in section 2.1 in Lin et al. [1]) and this paper says it uses the CONLL 2003 dataset. Two concerns here: 1) the dataset used in this paper must be clarified – how do the authors get the exact same results?; 2) if indeed the experimental datasets are the same, I am not sure if it is okay to replicate the exact same table across papers.
-------------------------------- Comments on Structure -------------------------------- Line 194: Should it be section 3.4? The subsection organization seemed confusing.
--------- Typos --------- The paper has several typos and grammar errors more toward the latter half of the paper. I list a few of the typos here. But this paper if accepted needs better proofreading.
Line 83: “cus via eliminating” -> “cues via eliminating” Line 123: “shotcut learning problem” -> “shortcut learning problem” Line 217: “we can subdividing” -> “we can subdivide” Line 334: “static results” -> statistic?
Line 420: “the the span classification” ---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important.  2) The paper has the following phrase explaining the problem that it wishes to address: “entail preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” However, majority of the paper and indeed the method itself seems to then emphasis the following specific problem, i.e. “eliminating task-specific nuisances.” I think maybe the former line should then either be removed or somewhere it state whether it is a hypothesis of the authors that “eliminating task-specific nuisances” indeed means “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” Perhaps the authors would need another whole paper to analyze what “eliminating task-specific nuisances” w.r.t. the proposed approach precisely means. So I would not say this as a weakness of this paper. However, is the phrase “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information” is used, it would help the reader if a connection is made to the actual solution which is mostly referred to as “eliminating task-specific nuisances.”
3) I also wonder if the authors would be willing to rephrase “nuisances” in “eliminating task-specific nuisances?” Maybe as “noise?”
4) Table 4. Curious why the ablation results are not shown on the CONLL 2003 dataset even though the method seems to have a maximum impact on it? Further, the CONLL 2003 – OOV data in particular seems quite relevant in this experimental setting. Can the authors provide these results? Otherwise the empirical analysis would seem incomplete to me by missing a seeming very relevant detail.  5) Not a weakness of this work as such. But maybe as a suggestion for future work. Lin et al [1] described four categories of artificially simulating OOV scenarios in NER. An empirical analysis with the method proposed in this work over their dataset or at least a dataset using their OOV synthesis methodology would be very interesting to have particularly on this problem of OOV NER.
6) The authors did not mention MINER code will be publicly released if the paper were accepted.
---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. 
Table 1 in this paper corresponds exactly to Table 3 Vanilla Baseline in Lin et al. [1]. However, Lin et al. mention using the ACE 2005 dataset (paragraph 1 in section 2.1 in Lin et al. [1]) and this paper says it uses the CONLL 2003 dataset. Two concerns here: 1) the dataset used in this paper must be clarified – how do the authors get the exact same results?; 2) if indeed the experimental datasets are the same, I am not sure if it is okay to replicate the exact same table across papers.
-------------------------------- Comments on Structure -------------------------------- Line 194: Should it be section 3.4? The subsection organization seemed confusing.
--------- Typos --------- The paper has several typos and grammar errors more toward the latter half of the paper. I list a few of the typos here. But this paper if accepted needs better proofreading.
Line 83: “cus via eliminating” -> “cues via eliminating” Line 123: “shotcut learning problem” -> “shortcut learning problem” Line 217: “we can subdividing” -> “we can subdivide” Line 334: “static results” -> statistic?
Line 420: “the the span classification” ---------------- References ---------------- [1] Lin, Hongyu, et al. ""A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. "
ARR_2022_121_review,"This paper introduces a novel model based on pre-trained Los (BERT family) for the task of Automated Essay Scoring (AES). The key feature of this model is that it computes representations at multiple scales (token, segment and document level). Another contribution is use of multiple loss functions (margin ranking and similarity) and transfer learning to improve over their baseline model. ","1. Their approach uses hierarchical representations which is intuitive. 
2. The use of multiple loss functions also makes sense (but not completely novel). 
3. They utilise transfer learning on out of domain data which is useful addition. ","1. The use of BERT architecture for this task has already been explored in R^2 BERT [Yang et al. 2020]. Existing work [1] explores use of contextualised representations of multiple sentences for document classification, so the idea of multi-scale representation may not be completely novel.
2. For document-level representation, the model uses only first 510 tokens. Ironically, the authors claim to get most improvement on prompt 1,2,8 for ASAP dataset (all of which have >510 tokens) 3. Some statements in the discussion are misleading. in [518-521], the authors mention ""We further re-implement BERT^2 proposed by (Yang et al., 2020), and the performance is not so strong as the published state-of-the art like models 9, 10 and 12."". The BERT^2 model is same as model 10. If the authors are trying to claim that their own implementation of BERT^2 is not as well-performing as the published result, there could be some implementation issues not taken care of resulting in sub-par performance.
4. For the loss functions, the authors should either give references to existing work which proposes these losses (for any task) or explicitly indicate that it is their contribution.  [1] Lu, Jinghui, et al. ""A Sentence-Level Hierarchical BERT Model for Document Classification with Limited Labelled Data."" International Conference on Discovery Science. Springer, Cham, 2021. ","1. The citation of R^2 BERT in table 2 is incorrect. 
2. The work R-DROP has been mentioned in introduction without giving any context (it is a dropout strategy) 3. The equations for attention pool are incomplete (terms under the summation missing). It makes them hard to understand. 
4. The grammar in the paper writing could be improved. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The use of BERT architecture for this task has already been explored in R^2 BERT [Yang et al. 2020]. Existing work [1] explores use of contextualised representations of multiple sentences for document classification, so the idea of multi-scale representation may not be completely novel.
2. For document-level representation, the model uses only first 510 tokens. Ironically, the authors claim to get most improvement on prompt 1,2,8 for ASAP dataset (all of which have >510 tokens) 3. Some statements in the discussion are misleading. in [518-521], the authors mention ""We further re-implement BERT^2 proposed by (Yang et al., 2020), and the performance is not so strong as the published state-of-the art like models 9, 10 and 12."". The BERT^2 model is same as model 10. If the authors are trying to claim that their own implementation of BERT^2 is not as well-performing as the published result, there could be some implementation issues not taken care of resulting in sub-par performance.
4. For the loss functions, the authors should either give references to existing work which proposes these losses (for any task) or explicitly indicate that it is their contribution.  [1] Lu, Jinghui, et al. ""A Sentence-Level Hierarchical BERT Model for Document Classification with Limited Labelled Data."" International Conference on Discovery Science. Springer, Cham, 2021. 
1. The citation of R^2 BERT in table 2 is incorrect. 
2. The work R-DROP has been mentioned in introduction without giving any context (it is a dropout strategy) 3. The equations for attention pool are incomplete (terms under the summation missing). It makes them hard to understand. 
4. The grammar in the paper writing could be improved. "
ARR_2022_121_review,The paper describes an approach to automatically grade essays using pre-trained language models such as BERT and Longformer. The paper also describes an approach to grade out of domain essays (i.e. essays written in response to another prompt). ,"1. Using multiple pre-trained language models (BERT and Longformer) as well as more than 1 dataset (ASAP Automatic Essay Grading dataset and Common Literacy Prize dataset).
2. Analysis is quite clear and complete. ","1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). ","Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays.  Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,None ,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). 
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays.  Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference. "
ARR_2022_355_review,"This work reports on the results of a cross-document misinformation detection system using event graph reasoning. Recognizing the need for automatic detection of real versus fake news, they train a system for event detection at two levels – document level and event level. To do this, they created their own fake data, apply cross-document coreference, group data into three sets based on event “clusters” (topically related events), and report event detection results. Their approach is as follows:  1.) 	Creation of a within-document knowledge graph based on IE (“IE” is never defined. I assume it stands for information extraction)  2.) 	Creation of a cross-document knowledge graph, connecting each cross-document event cluster to “events in the cluster”  3.) 	Connection of document nodes with all events and entities in the document.
4.) 	Encoding of cross-document knowledge graphs via graph neural networks and use of event and document level events to detect misinformation at both levels.  Their results outperform HDSF and GROVER at the document level and random guessing and logistic regression heuristics at the event level. ","1. 	Overall, work seems valid and logical. It improves upon current misinformation detection. 
2. 	Figures are extremely helpful ","1. 	Missing some details to lend full credibility. This pipeline seems to be fully automated. Were humans involved at any level and were automatic annotations, such as coreference resolution, subjected to human sanity checks?  This paper states that, “We employ a cross-document event coreference resolution system (Lai et al., 2021; Wen et al., 2021) to identify clusters of events from multiple documents that refer to the same real-world events.” Was any coreference spot-checked? What is the reliability of the system employed? It next states that, “For each event cluster, we add a node to represent the overall information of the real-world complex event corresponding to the cluster.” How was this added? Automatically or by hand? If automatically, what was the reliability of the nodes themselves? If by humans, what was the agreement? These details may impact your final results. This is followed by, “Then, an edge is added between each event node and corresponding cluster node to allow reasoning among cross-document coreferential events.” How is the edge added? Finally, how were your event structures pre-defined? I assume they differ by event, how did you determine the trigger, arg1, and arg2? Were there ever more than two arguments?
2. 	Justification of the need to create fake data is not fully motivated. You state that collection of annotation is time consuming and expensive but that these datasets are of higher quality, both of which are very true. You further state that you follow previous work that created datasets by manipulating knowledge graphs. While references are provided, no mention of validation of such manipulation is provided to satisfy a reader as to the of created fake news nor is mention made as to the heterogeneity or homogeneity of created fake news. How generalizable is this to real fake news? ","1)	Many acronyms not defined, such as “IE” and “RNN” 2)	Missing references for some claims that are presented as if they are common knowledge. For example, it is claimed that “conspiracy theories tend to be closely related to each other and convey highly similar information because they share the same biases or aim to manipulate readers in the same way.” However, your rational for creating your own fake news is the lack of sufficient existent data. Considering prior work on fake information detection, it is not always the case that conspiracy theories and/or fake news in general is similar between authors. The claim may very well be true for specific type of data. However, work validating the claim must be included. 
3)	You state that you improve upon current state of the art work by 7 points. Did the state of the art work use real or fake training data? What are your results on real data? 
4)	Heuristics used as baseline for event-level detection seem valid. However, multiple publications from 2021 report similar work and could potentially be used as a baseline (ex. COVID-19 misinformation detection via identification claims and within-document events and entities.) Many such resources are not based on coreference resolution so may not be applicable. However, comparison to a different method would seem to bolster your findings. 
5)	Grammatical error in “the two real news compliment each other”, “in the first news”, and “the two fake news contradict the real news with different details” ",3.5 ,Maybe,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"1. 	Missing some details to lend full credibility. This pipeline seems to be fully automated. Were humans involved at any level and were automatic annotations, such as coreference resolution, subjected to human sanity checks?  This paper states that, “We employ a cross-document event coreference resolution system (Lai et al., 2021; Wen et al., 2021) to identify clusters of events from multiple documents that refer to the same real-world events.” Was any coreference spot-checked? What is the reliability of the system employed? It next states that, “For each event cluster, we add a node to represent the overall information of the real-world complex event corresponding to the cluster.” How was this added? Automatically or by hand? If automatically, what was the reliability of the nodes themselves? If by humans, what was the agreement? These details may impact your final results. This is followed by, “Then, an edge is added between each event node and corresponding cluster node to allow reasoning among cross-document coreferential events.” How is the edge added? Finally, how were your event structures pre-defined? I assume they differ by event, how did you determine the trigger, arg1, and arg2? Were there ever more than two arguments?
2. 	Justification of the need to create fake data is not fully motivated. You state that collection of annotation is time consuming and expensive but that these datasets are of higher quality, both of which are very true. You further state that you follow previous work that created datasets by manipulating knowledge graphs. While references are provided, no mention of validation of such manipulation is provided to satisfy a reader as to the of created fake news nor is mention made as to the heterogeneity or homogeneity of created fake news. How generalizable is this to real fake news? 
1)	Many acronyms not defined, such as “IE” and “RNN” 2)	Missing references for some claims that are presented as if they are common knowledge. For example, it is claimed that “conspiracy theories tend to be closely related to each other and convey highly similar information because they share the same biases or aim to manipulate readers in the same way.” However, your rational for creating your own fake news is the lack of sufficient existent data. Considering prior work on fake information detection, it is not always the case that conspiracy theories and/or fake news in general is similar between authors. The claim may very well be true for specific type of data. However, work validating the claim must be included. 
3)	You state that you improve upon current state of the art work by 7 points. Did the state of the art work use real or fake training data? What are your results on real data? 
4)	Heuristics used as baseline for event-level detection seem valid. However, multiple publications from 2021 report similar work and could potentially be used as a baseline (ex. COVID-19 misinformation detection via identification claims and within-document events and entities.) Many such resources are not based on coreference resolution so may not be applicable. However, comparison to a different method would seem to bolster your findings. 
5)	Grammatical error in “the two real news compliment each other”, “in the first news”, and “the two fake news contradict the real news with different details” "
ARR_2022_355_review,The authors propose a cross-document misinformation detection task. The misinformation is detected based on the difference of the event information graph created using a cluster of documents based on the inconsistency of the event information provided in a document. The approach is tested on three event datasets by splitting the events into clusters smaller than the clusters in their original datasets. Misinformation is created by changing correct information in the documents. ,"1- The task, approach, and dataset are worth noting. 
2- The ablation studies that investigate the contribution of event detection and cross-document event coreference shed light on the performance. 
3- Some weaknesses of the approach and the experimental setting are discussed in detail. For instance, i) fake news creatin is creative, ii) the dataset used may not be reflecting real world. ","1- It is not clear how this task and the approach will perform for new information (updates) about an event (even if it is contradictory to what is known about an event) 2- The approach operates on clusters. New events may not have clusters.  3- Measurement of the performance maybe highly affected by the way the evaluation dataset is created.
4- Comparing random and logistic regression based baselines to something that is empowered by BERT is not fair. Even if it is a baseline.
5- I appreciate the effort spent for this paper. It is a lot of work and some parts are described very well. However, many steps are either missing or could be much clear. ","1- ""because events are more important to storytelling"": Do you have any investigation or reference for this? Sentiments may be playing significant role as well.
2- The concepts ""topic"" and ""event"" are confusing as they are used in the paper. The approach is based on events. The datasets utilized are based on topics. I see the details of the datasets and I understand it. But this is an issue in the whole article.
3- How is the entire doc used to create the knowledge graph in the subsection ""KG representation""?
4- resoltuion -> resolution ","2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"1- It is not clear how this task and the approach will perform for new information (updates) about an event (even if it is contradictory to what is known about an event) 2- The approach operates on clusters. New events may not have clusters.  3- Measurement of the performance maybe highly affected by the way the evaluation dataset is created.
4- Comparing random and logistic regression based baselines to something that is empowered by BERT is not fair. Even if it is a baseline.
5- I appreciate the effort spent for this paper. It is a lot of work and some parts are described very well. However, many steps are either missing or could be much clear. 
1- ""because events are more important to storytelling"": Do you have any investigation or reference for this? Sentiments may be playing significant role as well.
2- The concepts ""topic"" and ""event"" are confusing as they are used in the paper. The approach is based on events. The datasets utilized are based on topics. I see the details of the datasets and I understand it. But this is an issue in the whole article.
3- How is the entire doc used to create the knowledge graph in the subsection ""KG representation""?
4- resoltuion -> resolution "
ARR_2022_234_review,"The paper investigates uncertainty estimation (UE) in two NLP task: named entity recognition and text classification. Uncertainty estimation is an important research area in many machine learning fields, however, as the authors point out, there has been little attention given to UE in natural language processing. The authors explain in detail UE methods and conduct extensive experiments comparing the different methods for a Transformer model trained for the selected tasks. The authors also introduce two new modifications to the existing approaches and show that they are able to improve the existing approaches. ","The topic of uncertainty estimation is an interesting and important research area which has not received the attention it deserves in NLP. This paper provides a good overview of the different approaches and shows their relative strengths and weaknesses through extensive experiments.
The paper introduces some interesting modifications to the existing approaches (Mahalanobis distance and DPP Monte Carlo dropout) and the authors are able to show that the proposed modifications lead to state-of-the-art performance in most of the selected tasks. The proposed approaches are computationally cheap.
The authors connect their research well to previous work and, to the best of my knowledge, the related work section cites the relevant work sufficiently.
The paper is well written. The different sections are well structured and the explanation of technical details is clear and concise.
All-in-all this is a good paper. ",The experiments are run using only one model (ELECTRA). It would be interesting to see some comparison between different models. ,"Typo/gramma: Sentence starting on line 614: ""The proposed in this work method based on the Mahalanobis distance and spectral normalization of a weight matrix..."" ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The experiments are run using only one model (ELECTRA). It would be interesting to see some comparison between different models. 
Typo/gramma: Sentence starting on line 614: ""The proposed in this work method based on the Mahalanobis distance and spectral normalization of a weight matrix..."" "
ARR_2022_197_review,"This paper proposes a model to rank pronunciations of a word given a sequence of graphemes. A limited set of potential pronunciations are generated based on orthographic rules. The model takes a sequence of graphemes and a sequence of phones, and produces a score. The approach enumerated the exponentially many sequences generated by the orthographic rules, and find the sequence of phones that achieves the highest score. Empirically, the set of potential pronunciations are not prohibitively large, and is feasible to enumerate. The performance of the model is on par with many other approaches. ","I was one of the reviewers in the previous round. I advocated acceptance and I still do. The greatest strength is the simplicity of the approach. The approach does not work for all languages, but it is still valuable for many, such as Thai. ","The paper is easy to follow, but the presentation can be improved. Below is a list of minor points, and they can be fixed rather easily.
- The paper does not defend itself well enough. Instead of claiming ""both training and predicting processes are language independent,"" it should openly acknowledge that the approach won't be applicable to certain language orthography that is not very syllabic. This is addressed in the conclusion, but should appear early.
- The training loss is not explicit. The description in section 3 is not detailed enough.
- The paragraph and the table right above section 4.1 are good to have, but they feel cherry-picked. The paper should consider discussing other benefits, such as how easy it is to add new words to the model.
- Section 4.1 is weak, if not hurting the paper. There should be a deterministic mapping (or something very close to deterministic) for Japanese Hiragana, and the results definitely do not help the claim that the approach is language independent. The approach would not be effective if we apply it to Japanese Kanji or Mandarin Chinese. ","Though there is a accompanying repository, I still think it's worth talking about the model architecture, training hyperparameters, and the training loss.
The model is an energy-based model. Given the limited space, there probably isn't much to be said, but it might be worth mentioning so that the readers are aware of this. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The paper is easy to follow, but the presentation can be improved. Below is a list of minor points, and they can be fixed rather easily.
- The paper does not defend itself well enough. Instead of claiming ""both training and predicting processes are language independent,"" it should openly acknowledge that the approach won't be applicable to certain language orthography that is not very syllabic. This is addressed in the conclusion, but should appear early.
- The training loss is not explicit. The description in section 3 is not detailed enough.
- The paragraph and the table right above section 4.1 are good to have, but they feel cherry-picked. The paper should consider discussing other benefits, such as how easy it is to add new words to the model.
- Section 4.1 is weak, if not hurting the paper. There should be a deterministic mapping (or something very close to deterministic) for Japanese Hiragana, and the results definitely do not help the claim that the approach is language independent. The approach would not be effective if we apply it to Japanese Kanji or Mandarin Chinese. 
Though there is a accompanying repository, I still think it's worth talking about the model architecture, training hyperparameters, and the training loss.
The model is an energy-based model. Given the limited space, there probably isn't much to be said, but it might be worth mentioning so that the readers are aware of this. "
ARR_2022_55_review,"(This is a resubmission that is re-assigned to me, so I'm pasting my previous comments here and append updates at the end for each section.)
This paper builds upon the existing round-trip-based definition of NMT adversarial examples (RTT, Zhang et al. 2021) and points out a problem with the approach -- that is, the semantic destruction that happened during round-trip translation might be solely introduced by the back-translation process. To alleviate this problem, the paper proposes an alternative definition -- Doubly Round-trip Translation (DRTT), that adds another round-trip translation process starting from the target-side, to ensure that the semantic destruction is not solely introduced by back-translation.
Since this definition does not enforce source-side semantic agreement, the authors choose to build bilingual adversarial pairs (such that the target-side sentence will still have the same semantic as the perturbed source). To build such adversarial examples, the paper proposes a generation procedure that involves finding phrase pairs between parallel sentence pairs and conduct paired phrasal replacement with MLM and TLM.
Experiments show that training NMT models on such adversarial examples as augmentation data (?) can improve the model's robustness on both artificially and naturally noisy test sets compared to several other existing adversarial example generation methods.
=== UPDATE === The change introduced by the authors are as follows as far as I can tell: 1. moved around the notation explanation closer to where they are used so it's easier for the readers to follow. 
2. added a paragraph right before Section 4 to explain how the generated adversarial data is used. 
3. added an extra dataset created with CharSwap method (Michel et al. 2019) to test under robustness under meaning-preserving perturbations 4. case study moved to the appendix. ","+ The re-formulation of existing adversarial example generation is very insightful and will make a good reference for the community. It also made the proposal very well-motivated. 
+ Both the DRTT definition and the phrasal replacement algorithm are very clearly presented. ( However, the experiment section is much less so, see my first and second point for weakness.) 
+ The empirical improvements seem significant for both artificially and naturally noisy test sets. The baseline used for comparison seems strong and valid. ","+ My biggest concern with this paper is that there is a very significant transition gap between Section 3 and 4. Specifically, Section 3 is talking about a data generation method, while Section 4 starts with ""We evaluate our model under... "". It took a second pass through the earlier sections and some educated guess to conclude that the ""model"" is trained using adversarial data. 
+ Related to the above point, I don't think an adequate amount of experimental details are given for future work to reproduce the results in this paper at all. For example, if I understood correctly and the models are indeed trained on the adversarial data, what are the resulting training data size for the baseline and different augmentation methods? Are the original (non-adversarial) training data still used? Also, for results in Table 2, what is the system that is used to conduct back-translation? Is it the same system across different methods or not? 
+ The phrasal replacement procedure implicitly assume that the TLM to generate correct translation for the phrase so and still makes a valid translation pair. I hope to see some analysis on how strong is that assumption.
=== UPDATE === The first and the second point has been properly addressed in the author's reply and the revision. As for the third point, the authors offered the case study and the improvement in robustness as a reply, which does prove part of it, but what I have in mind is something more specific like comparing the human evaluation score (as the translation of $x_{\delta}$) of (1) $y_{\delta}$; (2) $y_{\delta}'$; (3) human translation of $x_\delta$. Ideally, (1) and (3) should be very close while (2) should be much worse. This is obviously too much to ask for resubmission on such a short cycle, though.
I do concur with the reviewer ag2n's concern that the paper ""is very parsimonious when it comes to citing previous work"". I don't think this is really improved in this version though. ",All my suggestions in the previous version have been addressed. The only minor comment I'll make here is that the baseline in Table 3 has been updated from 35.11 to 35.02 in this version. I'm not sure what has led to the change. ,"3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"+ My biggest concern with this paper is that there is a very significant transition gap between Section 3 and 4. Specifically, Section 3 is talking about a data generation method, while Section 4 starts with ""We evaluate our model under... "". It took a second pass through the earlier sections and some educated guess to conclude that the ""model"" is trained using adversarial data. 
+ Related to the above point, I don't think an adequate amount of experimental details are given for future work to reproduce the results in this paper at all. For example, if I understood correctly and the models are indeed trained on the adversarial data, what are the resulting training data size for the baseline and different augmentation methods? Are the original (non-adversarial) training data still used? Also, for results in Table 2, what is the system that is used to conduct back-translation? Is it the same system across different methods or not? 
+ The phrasal replacement procedure implicitly assume that the TLM to generate correct translation for the phrase so and still makes a valid translation pair. I hope to see some analysis on how strong is that assumption.
=== UPDATE === The first and the second point has been properly addressed in the author's reply and the revision. As for the third point, the authors offered the case study and the improvement in robustness as a reply, which does prove part of it, but what I have in mind is something more specific like comparing the human evaluation score (as the translation of $x_{\delta}$) of (1) $y_{\delta}$; (2) $y_{\delta}'$; (3) human translation of $x_\delta$. Ideally, (1) and (3) should be very close while (2) should be much worse. This is obviously too much to ask for resubmission on such a short cycle, though.
I do concur with the reviewer ag2n's concern that the paper ""is very parsimonious when it comes to citing previous work"". I don't think this is really improved in this version though. 
All my suggestions in the previous version have been addressed. The only minor comment I'll make here is that the baseline in Table 3 has been updated from 35.11 to 35.02 in this version. I'm not sure what has led to the change. "
ARR_2022_282_review,"## Paper Summary Dialogue state trackers (DST) extract important information from multi-turn dialogue situations and structures the belief state that appears during the conversation in the form of domain-slot-value. Performance of DSTs are measured using two metrics - joint goal accuracy and slot accuracy. Joint goal accuracy verifies if the predicted belief states perfectly matches the gold labels. This metric scores the performance of the later turns as zero even if the model mis-predicts a particular turn and performs accurately in the later turns. So, in this sense this metric this metric underestimates the model performance. On the other hand slot accuracy is a function of total number of predefined slots of all domains and by definition it is prone to overestimating the model performance. To solve these problems the authors propose a new metric called Relative Slot Accuracy that does not depend on the predefined number of unique slots and also scores the models in a way that matches human intuition. ","## Summary of Strengths The strengths of the paper are as follows: 	1. The scoring metric defined in the paper is not a function of number of predefined slots. 
	2. The scoring metric is simple to implement and compute. ","## Summary of Weaknesses  The weaknesses of the paper are as follows: 	1. It is not clear how Relative slot accuracy correlates to the precision and recall of the model. 
	2. The paper lacks theoretical analysis and justification, which is required while defining a new metric. ",## Comments Suggestions and Typos Theoretical analysis and evidences should be added in the paper in addition to experimental evidences. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"## Summary of Weaknesses  The weaknesses of the paper are as follows: 	1. It is not clear how Relative slot accuracy correlates to the precision and recall of the model. 
	2. The paper lacks theoretical analysis and justification, which is required while defining a new metric. 
## Comments Suggestions and Typos Theoretical analysis and evidences should be added in the paper in addition to experimental evidences. "
ARR_2022_282_review,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation. Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn. The metric is very intuitive and makes perfect sense. ","-  Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake. Thus, JGA underestimates the performance of system, while another commonly used slot accuracy metric overestimates the performance. The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems. ","- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric. Please refer to the question. It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric. ","In Eq. 3, does T* include all the slots-values in gold and predicted belief states or just the predicted and gold slots-values in current turn?
How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)? If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA. I think you should include evaluation results with the AGA metric. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric. Please refer to the question. It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric. 
In Eq. 3, does T* include all the slots-values in gold and predicted belief states or just the predicted and gold slots-values in current turn?
How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)? If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA. I think you should include evaluation results with the AGA metric. "
ARR_2022_198_review,"The paper presents the first results on time expression grounding, which aims at define a specific period of the day for each time expression like ""morning"", ""night"" etc. The authors experiment with 28 languages and apply language agnostic methods. The results are promising and might be potentially useful for natural language understanding tasks. ",Strengths of the paper: - interesting problem definition - clear methodology - acceptable results - thorough analysis of data - experiments on multiple languages - potentially useful task for higher-level applications (e.g. dialogue systems) ,Weaknesses of the paper: - results could have been discussed in more detail - some factors like time zones could have also been considered when selecting data ,"1. Do you think that the location of a country within the time zone affects the results? If a country is located at the westernmost part of a specific time zone, there might be more than one hour difference between the sunrise/sunset with regard to the easternmost part. 
2. Have you thought of visualizing the data on a map? This might be helpful for discovering some patterns in the data. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Weaknesses of the paper: - results could have been discussed in more detail - some factors like time zones could have also been considered when selecting data 
1. Do you think that the location of a country within the time zone affects the results? If a country is located at the westernmost part of a specific time zone, there might be more than one hour difference between the sunrise/sunset with regard to the easternmost part. 
2. Have you thought of visualizing the data on a map? This might be helpful for discovering some patterns in the data. "
ARR_2022_198_review,"This paper concerns the task of temporal grounding, which is framed as the mapping of temporal expressions (morning, noon, afternoon, evening, night) to their start and end time hours in the day in different languages (and cultures). To achieve such a mapping,  different approaches are proposed: a) a corpus-based approach where start and end time are (indirectly) inferred from the distribution of time references in Wikipedia data; b) two language-model (LM) methods where start and time can be either i) directly predicted by multilingual BERT, or ii) indirectly estimated from the predictions of multilingual BERT. Such methods are evaluated against newly created gold standard temporal annotations for 4 languages (Italian, Hindi, English and Portuguese). Additionally, the most successful extractive corpus-based method is tested on other  (although unlabeled) 24 languages. The paper also includes several discussions on cultural/temporal differences across languages and additional analyses on the proposed methods. ","The paper contributes to a niche of research by proposing an interesting take on the study of time expressions in NLP.  Specifically, the cross-lingual and cross-cultural focus of this paper provides several insights on the challenges of temporal commonsense that can inform research on the topic by highlighting -- often neglected in NLP -- language variability and speakers (time) experiences. This, in particular, emerges in sections 2.3, 5.2, 5.3 but also in 7.   Besides being reported as a quite informative process (section 2), the creation and consequent release of temporal gold standard annotations for 4 different languages can be a useful resource for the community.  The paper is reasonably well-written and pleasant to read. ","(1) It is claimed that this paper “propose a task”, “reframe a research question”. It thus seems to suggest that this work addresses temporal grounding as a *new* task. However, information about its utility and scenarios of applicability is not really elaborated (just a generic sentence at l.60) and are rather left to the reader to imagine. Also, the related work is presented as more of a description of previous studies rather than a critical “mapping”. As a consequence, it is hard to situate the proposed methods and insights and how they differ from/ and contribute to the topic. I would suggest being more specific and explicit about its contributions and how it relates to previous work.  (2) I see some issues with the proposed methods and evaluation: 2a) Figure 2 shows how different annotators (for the same language) referred to different time ranges for the same part of the day. In fact, Hindi annotations suggest that night can span till 9 am, thus overlapping with the time range for the morning, too. Then, at lines 195-99, the extractive approach is defined as to infer *non-overlapping* time ranges. Finally, in Figure 3, the gold standard annotations are represented as non-overlapping time spans among parts of the day. It is however unmentioned how a clear-cut distinction across parts of the day with definite time spans was defined to create such a final benchmark.  2b) Section 3.1: the extractive corpus-based approach relies on *English* Wikipedia data. No information about the number of time expressions needed is however provided. Also, I wonder whether by relying on English time expressions -- which are then automatically translated into other languages -- a strong western bias is induced as well as translation errors: was there someone with the needed language competence to verify the soundness of such translations? This part should be better documented, for instance in the supplementary material. 
  2c) The translation error issue presents itself also in section 3.2. Footnote number 5 mentions that BERT queries are based on the template “The <morning> starts at <9.00>"", and that such sentences are automatically translated into other languages. In a language with grammatical gender such as Italian, however, the translation of “the” would vary depending on the mentioned part of the day (e.g. the afternoon: IL pomeriggio; the morning: LA mattina). Is this something that has been adapted? If not could it have skewed BERT predictions?
2d) In section 4.2. the proposed approaches are compared against a baseline based on the “greeting method” by Vilares and Gόmez-Rόdriguez (2018). Their work however does not really seem to serve well as a baseline since it does not entail a *method*. Rather, they created a corpus to conduct an analysis on the semantics of greetings. Thus their goal is different from the one pursued in this paper, and their corpus here is simply employed to test the extractive method (3.1) on a different source of data.  (3) Although the paper is reasonably well written and pleasant to read, some turns are taken for granted and not really justified. For instance, the analysis in section 5 seems to concern the 24 unlabeled languages, this is however left unstated. Also, for instance, in 5.1 the analysis is carried out for Italian only, but the choice for such a decision is not given. ","As a general comment, I find that this paper seems to be a bit scattered, or better, it alternatively shifts from a culturally-situated focus to the strive for “language-agnostic” approaches. Besides the fact that I am having some doubt as to whether such approaches grant the definition of language-independent method (it does not seem to be very effective on all languages, it largely relies on *English* data and concepts, e.g. noon), I think that such shifts bring the paper out of focus at times and may affect its narrative. Note that this is more  more of an opinionated point of view (which could be discarded and I am thus not putting in the weaknesses section), but I think the paper should enhance the former, linguistically and culturally grounded perspective, which in my opinion is also the main strength of this work.  Few minor suggestions: - footnote1. move it at line 58 - footnote2: specify HIT acronym - the related work would be maybe put to better use after the introduction Finally, as per https://2021.aclweb.org/ethics/Ethics-review-questions/, information about the compensation of annotators via Amazon Mechanical Turk has to be provided. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"(1) It is claimed that this paper “propose a task”, “reframe a research question”. It thus seems to suggest that this work addresses temporal grounding as a *new* task. However, information about its utility and scenarios of applicability is not really elaborated (just a generic sentence at l.60) and are rather left to the reader to imagine. Also, the related work is presented as more of a description of previous studies rather than a critical “mapping”. As a consequence, it is hard to situate the proposed methods and insights and how they differ from/ and contribute to the topic. I would suggest being more specific and explicit about its contributions and how it relates to previous work.  (2) I see some issues with the proposed methods and evaluation: 2a) Figure 2 shows how different annotators (for the same language) referred to different time ranges for the same part of the day. In fact, Hindi annotations suggest that night can span till 9 am, thus overlapping with the time range for the morning, too. Then, at lines 195-99, the extractive approach is defined as to infer *non-overlapping* time ranges. Finally, in Figure 3, the gold standard annotations are represented as non-overlapping time spans among parts of the day. It is however unmentioned how a clear-cut distinction across parts of the day with definite time spans was defined to create such a final benchmark.  2b) Section 3.1: the extractive corpus-based approach relies on *English* Wikipedia data. No information about the number of time expressions needed is however provided. Also, I wonder whether by relying on English time expressions -- which are then automatically translated into other languages -- a strong western bias is induced as well as translation errors: was there someone with the needed language competence to verify the soundness of such translations? This part should be better documented, for instance in the supplementary material. 
  2c) The translation error issue presents itself also in section 3.2. Footnote number 5 mentions that BERT queries are based on the template “The <morning> starts at <9.00>"", and that such sentences are automatically translated into other languages. In a language with grammatical gender such as Italian, however, the translation of “the” would vary depending on the mentioned part of the day (e.g. the afternoon: IL pomeriggio; the morning: LA mattina). Is this something that has been adapted? If not could it have skewed BERT predictions?
2d) In section 4.2. the proposed approaches are compared against a baseline based on the “greeting method” by Vilares and Gόmez-Rόdriguez (2018). Their work however does not really seem to serve well as a baseline since it does not entail a *method*. Rather, they created a corpus to conduct an analysis on the semantics of greetings. Thus their goal is different from the one pursued in this paper, and their corpus here is simply employed to test the extractive method (3.1) on a different source of data.  (3) Although the paper is reasonably well written and pleasant to read, some turns are taken for granted and not really justified. For instance, the analysis in section 5 seems to concern the 24 unlabeled languages, this is however left unstated. Also, for instance, in 5.1 the analysis is carried out for Italian only, but the choice for such a decision is not given. 
As a general comment, I find that this paper seems to be a bit scattered, or better, it alternatively shifts from a culturally-situated focus to the strive for “language-agnostic” approaches. Besides the fact that I am having some doubt as to whether such approaches grant the definition of language-independent method (it does not seem to be very effective on all languages, it largely relies on *English* data and concepts, e.g. noon), I think that such shifts bring the paper out of focus at times and may affect its narrative. Note that this is more  more of an opinionated point of view (which could be discarded and I am thus not putting in the weaknesses section), but I think the paper should enhance the former, linguistically and culturally grounded perspective, which in my opinion is also the main strength of this work.  Few minor suggestions: - footnote1. move it at line 58 - footnote2: specify HIT acronym - the related work would be maybe put to better use after the introduction Finally, as per https://2021.aclweb.org/ethics/Ethics-review-questions/, information about the compensation of annotators via Amazon Mechanical Turk has to be provided. "
ARR_2022_17_review,"This paper is an extension of the SUPERB benchmark for assessing pre-trained upstream speech models acquired through Self-supervised learning. The authors argue that SUPERB is limited to 10 tasks and that 5 new tasks, here AST, Out-of-Domain ASR, Voice Conversion, Speech separation and Speech enhancement would permit to better evaluate the semantic and generative capability of such SSL models. The authors compare the same SSL models used in the SUPERB paper on the 5 tasks. All SSL models were used as-is without fine-tuning on the task (frozen). Overall results show some superiority of Wav2vec and Hubert models on AST and OOD ASR tasks, while VC SE and SS tasks lead to less obvious superiority of models. The authors then performed experiments with varying data training for some SSL models or downstream tasks and showed that the ranking of ssl models stay consistent which should prove the robustness of these new evaluation tasks. ","- Setting up benchmarks is highly important to assess SSL speech models - The proposed tasks provide a complementary view to the 10 already existing task of SUPERB  - The authors have shown that the proposed tasks leads to consistent results according to the upstream and downstream training data - Although the authors did not trained the SSL models, this stay an impressive and useful amount of work, that according to what I understood from the text, should be made available to the community (when?) ","- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant.  - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021)  rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.   - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models.  - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) ","Most of my comments are given above.  Details : some references have no venue -> please correct this.     misc{hsu2021robust,       title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},        author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},       year={2021},       eprint={2104.01027},       archivePrefix={arXiv},       primaryClass={cs. SD} } ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant.  - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021)  rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.   - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models.  - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) 
Most of my comments are given above.  Details : some references have no venue -> please correct this.     misc{hsu2021robust,       title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},        author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},       year={2021},       eprint={2104.01027},       archivePrefix={arXiv},       primaryClass={cs. SD} } "
ARR_2022_18_review,"This paper studies style transfer for languages where no style-labelled corpora are available and furthers related prior work that either assumes access to large style-labelled corpora and the more recent few-shot transfer using only 3-10 sentences at inference for extracting the target style.
The authors first study the current few-shot style transfer method and find that is is unsuited for Indic languages. To address it's shortcomings, they (i) use an inference-time trick---back-translation of first generating a translation to English and then generating a style-specific exemplar in the target language, (ii) further enhance the style transfer process requiring only a single generation step to develop a new few-shot style transfer training objective that uses paraphrases and style vector differences, and (iii) consider an interpolated model combining the objectives of (i) and (ii), which they term multi-task learning.  They empirically evaluate the performance of the models using available datasets and also create another dataset for the Indic languages studied for validating the automatic metrics. ","Strengths: 1) The study extends style transfer from few-shot to zero-shot in low-resource scenarios.
2) A new dataset of 1000 sentence pairs in each of the four Indic languages (Hindi, Bengali, Kannada and Telugu) with formality and semantic similarity annotations is created.
3) A multi-task learning-based model and an approach that relies on style vector difference for controlling the training/inference mismatch in style.
4) Extensive comparison of the different models on 4 languages to demonstrate empirical validity of the models studied. ","1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as  the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7.  The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1,  and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly  the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG  = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? ","See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda  = 1.5  nA -> na (short vowel) ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as  the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7.  The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1,  and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly  the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG  = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? 
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda  = 1.5  nA -> na (short vowel) "
ARR_2022_279_review,This short paper presents an empirical study of a direct-parsing approach for sentiment graphs.  The methods come from previous work but this paper adapts them to the sentiment task and shows that they perform quite well across several data sets. ,"+ The paper presents a method that performs well on an important task + The fact that a direct-parsing approach would provide benefits seems sensible + Because of the strong performance, the released software may be valuable to the community ","- The paper does not appear to have methodological novelty (the paper is somewhat unclear on this score, but does not distinguish what if anything from its approach differs from the original PERIN approach) - The paper is hard to understand. It is not self-contained -- the methods are only really explained elsewhere, and I was left with several questions (some listed below) - The one design decision evaluated in the experiments is which graph encoding to use (node-centric, labeled-edge, and opinion-tuple).  The paper spends a large amount of its limited space discussing these three encodings and presenting results with each, but it is never explained why this is an important dimension to vary or why it makes sense that we see certain performance differences.
I have a hard time evaluating this paper because it seems like the primary empirical finding, that this approach works well across several data sets, is a helpful data point for the community.  However, aside from that one piece of information, I didn't learn much from this paper, due to the issues mentioned above. ","Line 21 -- what are ""the relations""?
Line 52-57, how this lossy conversion causes problems seems crucial to the paper's central thesis (it is avoiding this intermediate step that is hypothesized to improve performance in this paper).  Cutting all of the material on different graph encodings in order to more clearly explain this issue in the body text (I found it hard to understand even after reading App A) would improve the paper.
Line 70 - ""permutation-invariant,"" why is this important and permutations of what? ( must be the output structure) Line 201-205 -- This is interesting, making this statement quantitatively (probably at the example level rather than the dataset level, so for examples in the top quartile in terms of amount of nesting, we see certain larger performance gains) would be helpful.
Line 386 (of appendix) - ""it is impossible"" -- I don't understand why.  Especially given this is in the appendix, it would help to explain exactly why this is impossible.  In general all of appendix A (like the rest of the paper) is terse to a point of being quite hard to comprehend. ",2.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The paper does not appear to have methodological novelty (the paper is somewhat unclear on this score, but does not distinguish what if anything from its approach differs from the original PERIN approach) - The paper is hard to understand. It is not self-contained -- the methods are only really explained elsewhere, and I was left with several questions (some listed below) - The one design decision evaluated in the experiments is which graph encoding to use (node-centric, labeled-edge, and opinion-tuple).  The paper spends a large amount of its limited space discussing these three encodings and presenting results with each, but it is never explained why this is an important dimension to vary or why it makes sense that we see certain performance differences.
I have a hard time evaluating this paper because it seems like the primary empirical finding, that this approach works well across several data sets, is a helpful data point for the community.  However, aside from that one piece of information, I didn't learn much from this paper, due to the issues mentioned above. 
Line 21 -- what are ""the relations""?
Line 52-57, how this lossy conversion causes problems seems crucial to the paper's central thesis (it is avoiding this intermediate step that is hypothesized to improve performance in this paper).  Cutting all of the material on different graph encodings in order to more clearly explain this issue in the body text (I found it hard to understand even after reading App A) would improve the paper.
Line 70 - ""permutation-invariant,"" why is this important and permutations of what? ( must be the output structure) Line 201-205 -- This is interesting, making this statement quantitatively (probably at the example level rather than the dataset level, so for examples in the top quartile in terms of amount of nesting, we see certain larger performance gains) would be helpful.
Line 386 (of appendix) - ""it is impossible"" -- I don't understand why.  Especially given this is in the appendix, it would help to explain exactly why this is impossible.  In general all of appendix A (like the rest of the paper) is terse to a point of being quite hard to comprehend. "
ARR_2022_279_review,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity. Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph. The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets. ","1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping. 
2) The paper is clearly written and well structured. Given the page limitation of a short paper, this paper still provides abundant information. 
3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings. ","1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement. 
2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA. ","1) In Figure 2, please indicate STEP 1,2,3,4 in the corresponding places in the diagram for better illustration. 
2) It would be better to present a few case studies for readers. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement. 
2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA. 
1) In Figure 2, please indicate STEP 1,2,3,4 in the corresponding places in the diagram for better illustration. 
2) It would be better to present a few case studies for readers. "
ARR_2022_317_review,"This paper is about evaluating video retrieval methods or video tagging methods by perturbing text inputs, kind of like how adversarial attacks work. The idea is to take the text input and replace words which is connected to person entities and verbs and make changes so the text doesn't describe the video anymore. Person entity replacements is done using rule based methods very specific to the dataset considered, for instance changing character names in text describing movie clips and flipping gender terms in other videos. To change the verb (or the action) in the text, a masked language model is used to find good replacements for verb phrases. Results shows that existing models fail on such adversarial attacks to an extent. ","- Probably the first paper on adversarial attacks on Video-text models.
- Text perturbations are indeed confusing the models.  - It is well written paper. Easy to understand even for a person who is not really working in this particular sub-area. ","- Lack of novelty:    - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models. 
  - Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER. ","- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"- Lack of novelty:    - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models. 
  - Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER. 
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives. "
ARR_2022_163_review,"This paper proposes a solution for ""Contrastive Conflicts"". What exactly are “Contrastive Conflicts”? They occur when multiple questions are derived from a passage, each with different semantics. The questions are going to be close to the passage in representation space and by transitivity they are going to be close among themselves even though they are semantically different (Transitivity Conflict). In addition to this, if multiple questions derived from the same passage are in the same training batch, then the questions will see that passage as both positive and negative (In-Batch Conflict).
The solution proposed by the paper is to use smaller granularity units, i.e. contextualized sentences. Per sentence representations are computed by using per sentence special indicator tokens, then a similar approach to DPR is used to finetune sentence representations. Because different questions have answers in different sentences the contrastive conflict is generally resolved.
Improvements are reported on NQ, TriviaQA and SQuAD, especially on SQuAD where conflicts are reported to be severe (i.e. often multiple different questions are extracted from the same passage). Extensive experiments show that the method does well even in transfer learning. ","Strengths: - The paper obtains small but convincing improvements on NQ and TriviaQA, and large but a bit puzzling results on SQuAD (considering that one of the baselines does not match the DPR paper and that SQuAD can benefit dramatically from combining DPR with BM25, but it is not done in this paper).
- The paper presents many interesting ablations and transfer learning experiments that help further convince the reader of the efficacy of the method. ","Weaknesses: - Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn’t help as much as BM25.
- The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don’t mention that it’s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).
- The big improvement reported in Table 2 for SQuAD “Single” is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD “Multi” matches the DPR paper. ","Suggestions: - Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"Weaknesses: - Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn’t help as much as BM25.
- The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don’t mention that it’s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).
- The big improvement reported in Table 2 for SQuAD “Single” is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD “Multi” matches the DPR paper. 
Suggestions: - Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. "
ARR_2022_1_review,"The paper proposes a pre-training strategy that leverages on the multilingual knowledge base to learn representations for entities and relations among them. The pre-trained language models can thus serve knowledge base construction tasks such as link prediction (LP), (cross-lingual) entity linking (XEL), bilingual lexicon induction (BLI), and prompt-based knowledge probing (LM-KP).  For inference, the authors propose two types of inferences; one as autoregressive inference that makes the LM generate objects to complete KB tuplets using constrained beam search, and the other as similarity-based inference that generates similarity-comparable embeddings using a contrastive learning method (Mirror-BERT).  They evaluated the proposed approach on the aforementioned downstream tasks. For the LP task, they evaluated on the DBpedia and achieved the best performance on Hits@1 across all languages compared to baseline models (TransE, ComplEx, and RotatE). For the XEL task, they evaluated on the UMLS and outperformed baselines  (XLM-R and mBERT, both with contrastive learning) in most languages. For the BIL task, they evaluated on the VecMap and outperformed baselines (XLM-R + Mirror). For the LM-KP task, they evaluated on the mLAMA benchmark.  Based on the experimental results, they conclude that the proposed approach can leverage pre-training on multilingual knowledge bases to learn better representations for downstream knowledge base construction tasks.
Other comments are addressed below respectively. ","Based on the experiments, the paper demonstrates that using the surface form of knowledge tuples with special control tokens to pre-train language models can enable the LMs to learn better representations for entities and their relations. Several analyses in the paper might benefit the community for tackling pre-training scheme designs, such as how representations would differ w.r.t. different stages during the training (sec 3.6). ","- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. ","- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. "
ARR_2022_318_review,"This paper proposes a new pre-trained variational encoder-decoder dialog model which has continuous latent variables to deal with the one-to-many mapping problem in dialogue response generation. 
This paper conducts empirical experiments on 3 datasets to show their proposed model has better performances both on relevance and diversity than previous state-of-the-art dialog systems. 
They also conducted additional analysis to show the impact of latent variable sizes, different decoding strategies and position embeddings for their proposed model. 
However, in the model evaluation part, the paper does not show how their model can generate diverse responses given the same context. ","1. This paper proposes a new variational encoder-decoder dialog model for open-domain dialogue generation tasks, which shows better performance in terms of relevancy and diversity than previous state-of-the-art models. The new model leverages several techniques to better alleviate the one-to-many mapping problem in dialogue response generation, including conditional variational autoencoder, n-stream self-attention, memory scheme, masked language modeling, free bits and bag-of-words loss for reducing KL-vanishing, and position embeddings for Transformers. 
2. This paper conducts empirical experiments on three benchmark datasets to validate the effectiveness of the proposed model. The proposed model achieves better performances than previous baselines (e.g. PLATO) in both automatic metrics (e.g. BLEU-1/2, Distinct-1/2) and human evaluation (fluency, coherence, informativeness, overall). 
3. This paper analyzes the effect of latent variable sizes, sizes of K in top-k sampling and different combinations of position embeddings, which are helpful empirical observations for training Transformer-based variational encoder-decoders. ","1. This paper lacks a discussion on its novelty, e.g., why continuous latent variables are better than discrete latent variables for solving the one-to-many mapping problem. Though empirically the proposed DialogVED performs better than PLATO, DialogVED incorporates more other techniques (e.g. n-stream self-attention, memory scheme, masked language modeling, free bits) than PLATO. It is not very clear to me why we should choose those techniques to train a continuous variational encoder-decoder. Maybe some ablation studies on the training objective and model architecture can help explain it. 
2. The model evaluation does not show the model's superiority in solving the one-to-many mapping problem in open-domain dialogue response generation. Distinct-1/2 are corpus-level metrics and are not showing the diversity of generated responses given the same context. To verify their claim, the authors may consider using self-bleu and the ratio of unique generated sentences to better evaluate the diversity. ","1. It is unclear to me what's the input to the prior network. Fig 1 suggests the [CLS] token is at the end of the context, but line234 says the [CLS] token is at the beginning of the context. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. This paper lacks a discussion on its novelty, e.g., why continuous latent variables are better than discrete latent variables for solving the one-to-many mapping problem. Though empirically the proposed DialogVED performs better than PLATO, DialogVED incorporates more other techniques (e.g. n-stream self-attention, memory scheme, masked language modeling, free bits) than PLATO. It is not very clear to me why we should choose those techniques to train a continuous variational encoder-decoder. Maybe some ablation studies on the training objective and model architecture can help explain it. 
2. The model evaluation does not show the model's superiority in solving the one-to-many mapping problem in open-domain dialogue response generation. Distinct-1/2 are corpus-level metrics and are not showing the diversity of generated responses given the same context. To verify their claim, the authors may consider using self-bleu and the ratio of unique generated sentences to better evaluate the diversity. 
1. It is unclear to me what's the input to the prior network. Fig 1 suggests the [CLS] token is at the end of the context, but line234 says the [CLS] token is at the beginning of the context. "
ARR_2022_276_review,"This paper presents two extensions for improving non-autoregressive machine translation (NAT) with lexical constraints, with a focus on low frequency words. The authors take as base model an approach very similar to that of Xu & Carpuat 2021 and expand it with a) ""constrained training"", where the system already sees lexicon constraints at training time and b) ""alignment prompting"", where alignment information is included into the model. The motivation for this last method is to inform the model as to what source word(s) correspond to the constraint, so that the context can be better generated. Experimental results shows that the method is able to improve the generation quality (measured in BLEU), specially for out-of-domain tasks, while retaining the efficiency advantages of NAT systems. ","- The paper is clearly written and well-motivated. Section 3.3 gives a clear indication of shortcomings of current models, and Section 6.1 revisits the issue and analyzes how the model addresses it.
- The approach is general enough that it can be applied to different models.
- Table 5 shows good improvements for out-of-domain test sets.
- The approach is (mostly) clearly described and the authors plan to release the code. ","- The evaluation of the paper could be made stronger by using some of the standard datasets for terminology translation (e.g. wmt21 shared task) and evaluation metrics (Alam et al. 2021).
- The description of the alignment embedding seems a bit under-specified. Am I understanding it correctly that the constraints in the target sentence get an additional index that gets embedded (similar in concept to positional embeddings)? Are unaligned words in the constraints marked in a special manner? Are these embeddings recomputed in every refinement step of the LevT? ","- Line 232: The are surely sentences where not every bucket is represented, right? Would it be then more correct to say that you have **approximately** 6x the data? Or am I misunderstanding something?
- Line 248: This explanation, while plausible, could be relatively easy to check just by looking at the words themselves. Have you done that? Have you tried filtering those words out (e.g. using stopword lists or similar) as they are unlikely to appear as constraints in real life situations?
- Line 263: It would be better to use l_i.
- Subsubsection starting at 324: Would it make sense to use the lexicon information directly if available (which it is for some test conditions) and resort to automatic tools only if necessary? Related to this, have you measured how sensitive the method is against alignment errors? This is also specially relevant for out-of-domain settings, where the alignment model is also operating in out-of-domain conditions.
- Table 4: Please also include bold numbers for the baselines of previous work. Specifically for WMT17-WIKT the best result in terms of BLEU is actually in the baselines. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"- The evaluation of the paper could be made stronger by using some of the standard datasets for terminology translation (e.g. wmt21 shared task) and evaluation metrics (Alam et al. 2021).
- The description of the alignment embedding seems a bit under-specified. Am I understanding it correctly that the constraints in the target sentence get an additional index that gets embedded (similar in concept to positional embeddings)? Are unaligned words in the constraints marked in a special manner? Are these embeddings recomputed in every refinement step of the LevT? 
- Line 232: The are surely sentences where not every bucket is represented, right? Would it be then more correct to say that you have **approximately** 6x the data? Or am I misunderstanding something?
- Line 248: This explanation, while plausible, could be relatively easy to check just by looking at the words themselves. Have you done that? Have you tried filtering those words out (e.g. using stopword lists or similar) as they are unlikely to appear as constraints in real life situations?
- Line 263: It would be better to use l_i.
- Subsubsection starting at 324: Would it make sense to use the lexicon information directly if available (which it is for some test conditions) and resort to automatic tools only if necessary? Related to this, have you measured how sensitive the method is against alignment errors? This is also specially relevant for out-of-domain settings, where the alignment model is also operating in out-of-domain conditions.
- Table 4: Please also include bold numbers for the baselines of previous work. Specifically for WMT17-WIKT the best result in terms of BLEU is actually in the baselines. "
ARR_2022_276_review,This paper proposed improvements to the Levenshtein transformer for lexically constraint translation task. The proposed improvements through alignment constraints during training and or inference ,"- Clear writing and captivating content; the motivating study section is a nice approach to architecture and experimental design and the analysis section is a great read - The experiments/analyses are well-taught out to validate stated hypothesis and comparative studies with different ablations are applied to support analyses - Based on Ding et al. 2021 analysis of low-freq words, the hunch of injecting alignments knowledge outside of the AT / NAT model is a good one ","- Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.
- [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought. ","- I might have missed it but the results section stated ""When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.""; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder?  - Would the code and experiments be open sourced like Susanto et al?  - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?
- Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline.  - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one ","5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.",Maybe,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.
- [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought. 
- I might have missed it but the results section stated ""When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.""; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder?  - Would the code and experiments be open sourced like Susanto et al?  - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?
- Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline.  - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one "
ARR_2022_276_review,"This work proposes to improve lexically constrained non-autoregressive translation by incorporating constraints during training and utilizing the source-side alignment during training and inference. The main motivation is that even for rare constraints, the context is not necessarily rare, and aligning source-side context can be beneficial for the model's performance. The results on En-De in-domain and out-of-domain data, as well as a comparison with other constrained models show that the proposed model a) improves the quality b) does not increase latency ","- Paper contains an extensive comparison with previous works on lexically constrained NAT, as well as with AR Transformer model - ACT shows improvements on domain-specific datasets and on a constrained portion of in-domain data - Incorporated constraints do not result in latency increase ","- No comparison across languages (only De target language) - While testing on the full En-De test set, the improvements are minor ","*Typos:* - Table 1: htten --> hatten - line 155: An --> a - line 166: For NATs --> seems redundant, an NAT --> a - Figure 1: I am confused by frequency buckets. Does the x-axis represent the **inverse** frequency of constraint? Otherwise, I am genuinely confused. And while sampling self-constrains, do you sample from the word buckets based on the sentence itself or do you consider ALL possible words (from all sentences) in the bucket?  *Additional questions:* - During training you sample 0-3 reference tokens as constraints, since the BPE is used, does it mean that it might be the case that only part of the word is considered as a constraint? Or do you handle such a situation in some way?
Thanks! ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"- No comparison across languages (only De target language) - While testing on the full En-De test set, the improvements are minor 
*Typos:* - Table 1: htten --> hatten - line 155: An --> a - line 166: For NATs --> seems redundant, an NAT --> a - Figure 1: I am confused by frequency buckets. Does the x-axis represent the **inverse** frequency of constraint? Otherwise, I am genuinely confused. And while sampling self-constrains, do you sample from the word buckets based on the sentence itself or do you consider ALL possible words (from all sentences) in the bucket?  *Additional questions:* - During training you sample 0-3 reference tokens as constraints, since the BPE is used, does it mean that it might be the case that only part of the word is considered as a constraint? Or do you handle such a situation in some way?
Thanks! "
ARR_2022_75_review,"The paper proposed a semantic and syntactic disentanglement approach based on Siamese Semantic Distangelement Model ($S^2DM$). This semantic dissociation from syntax is learned in representation obtained from the pre-trained models (mBERT and XLM-100). This decoupling model reduces the ‘violation of syntactic constraint’ and advances the answer-span boundary detection performance (for Question Answering (QA) tasks) in zero-shot cross-lingual transfer for low-resource languages. The proposed approach uses two-step training (with labeled parallel and task-specific datasets) and six linguistically inspired training losses in two combinations (models).  The Experiments are conducted on one task (QA), three datasets (MLQA, XQuAD, TyDiQA), and across nine languages. Extensive experiments and an ablation study are conducted to support the paper's claim. ","1. The paper is well written and easy to follow. 
2. The proposed work is linguistically inspired and intuitive. The work or its variants (unsupervised version) may be extended to other cross-lingual/multilingual applications. 
3. The proposed approach's strength is mathematical analysis in section 3.3 and empirical analysis. The systematic study has been conducted and whenever required, the suitable argument and experiment are conducted, respectively. 
4. The paper's visual presentations (Figure-3, Figure-5, and Figure-6) are informative and thoughtful. ","1. One problem I can notice is the scalability issue of the proposed method.  As mentioned in section 4.3, step-01 of training required labeled parallel sentences, limiting the proposed approaches to the few high-resource languages. 
2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only). 
3. Many of the reported scores (particularly for mBERT) in Table-2,3 & 4 are close to baseline; the statistical significance test is recommended for reliability ","1. It is hard to read numbers in Figure-3 and Figure-5 with a printed copy. 
2. In Line 492, the referred table number is incorrect. 
3. Author(s) can plan to investigate the unsupervised approach as a future extension of this work which will be more promising. ",3.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. One problem I can notice is the scalability issue of the proposed method.  As mentioned in section 4.3, step-01 of training required labeled parallel sentences, limiting the proposed approaches to the few high-resource languages. 
2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only). 
3. Many of the reported scores (particularly for mBERT) in Table-2,3 & 4 are close to baseline; the statistical significance test is recommended for reliability 
1. It is hard to read numbers in Figure-3 and Figure-5 with a printed copy. 
2. In Line 492, the referred table number is incorrect. 
3. Author(s) can plan to investigate the unsupervised approach as a future extension of this work which will be more promising. "
ARR_2022_75_review,This paper proposes methods to enforce models to first disentangle semantic from syntax information in pre-trained models and then transfer only semantic information across languages in machine reading comprehension tasks. The authors evaluated their proposed framework on several data sets and showed very promising results. ,"- This paper proposes a novel and interesting idea.  - The observation and motivation from this paper could be potentially applicable to other tasks as well.  - The authors showed consistent improvement over strong baselines in several datasets.
- The authors provide interesting qualitative analyses that give insights about the reasons why their proposed method is helpful. ",- The authors do not promise to release their code. ,"- I would like to see more in-depth analyses about the improvement, e.g. where the improvement really comes from. Does it come from data points where syntactic constraints are violated? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The authors do not promise to release their code. 
- I would like to see more in-depth analyses about the improvement, e.g. where the improvement really comes from. Does it come from data points where syntactic constraints are violated? "
ARR_2022_214_review,"This paper proposes a simple and new random intermediate layer knowledge distillation approach (RAIL-KD). The main point is to select the intermediate layers from the teacher model randomly which are distilled to the corresponding student layers. Here all layers are taken into the training without any additional parameters. So it could avoid skip and search problem and reduce the computational cost of intermediate layer distillation. The whole idea is interesting and straightforward. It is great to have many analyses in the experiments. However, I have some concerns about the performance. ","1.This paper presents a new intermediate layer knowledge distillation approach with performance improvement and low computational cost.  2.This paper adds some comparisons of the current ILD methods about the efficiency and performance. The proposed RAIL-KD has a less computational overhead.  3.In the experiments, authors compare different distillation methods under many challenge datasets. ","1.Adding the random selection is not a new idea. The randomness of the layer selection could cause the unstable of performance. It is better to show more results such as the convergence analysis. Random selection might not be the best option here.
2.The performance is not convincing. The improvements under some settings are not significant.  3.The new approach could not find the best subset (subset mapping) since the proposed model just selects m layers randomly. But it is not clear if we should ignore it or solve it later. From my opinion, the attention-based approaches with best mapping search may achieve better performance. ","The whole approach lacks the evidences to prove the usefulness. Comparing to ALP-KD, the improvement is not significant. ",2.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1.Adding the random selection is not a new idea. The randomness of the layer selection could cause the unstable of performance. It is better to show more results such as the convergence analysis. Random selection might not be the best option here.
2.The performance is not convincing. The improvements under some settings are not significant.  3.The new approach could not find the best subset (subset mapping) since the proposed model just selects m layers randomly. But it is not clear if we should ignore it or solve it later. From my opinion, the attention-based approaches with best mapping search may achieve better performance. 
The whole approach lacks the evidences to prove the usefulness. Comparing to ALP-KD, the improvement is not significant. "
ARR_2022_101_review,"This paper borrows a idea from the optimal transport problem to propose a framework for evaluating and interpreting semantic textual similarity (STS). The authors use token-wise similarity from pre-trained language models, which differs their method from the previous sentence-wise comparisons between representations, like SimCSE. The method is simple but performs well as it defeats SimCSE on most STS and all iSTS datasets. The authors then analyze their method is both efficient and interpretable for STS. ","1. The method used in this paper is simple and interesting and it’s natural to utilize word-level similarity to textual similarity.  2. The main results and analyses verify the efficiency, interpretability, and strong performance of the method.  3. The paper has been structured well to answer my questions during the reading. The explanation for the main method has been well organized to help readers understand the algorithm with ease.
4. There is a wide range of analyses that support the claims of the authors, making this work fairly solid. ","1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.
2. Figure 2 does not show the time complexity of SimCSE_{CLS} method.
3. I am confused about the definition of ""\vec \mathbf{1}"" in Equation(1). ","Missing citation: BERTScore: Evaluating Text Generation with BERT (Zhang et al. 2020) For other suggestions, please refer to the weakness section. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.
2. Figure 2 does not show the time complexity of SimCSE_{CLS} method.
3. I am confused about the definition of ""\vec \mathbf{1}"" in Equation(1). 
Missing citation: BERTScore: Evaluating Text Generation with BERT (Zhang et al. 2020) For other suggestions, please refer to the weakness section. "
ARR_2022_65_review,"This paper introduces a large human-annotated Chinese predicate-argument dataset MuPAD that covers six domains. Different from the previous Chinese SRL dataset, MuPAD annotates hidden arguments (subject and object), which is a common phenomenon in Chinese and is useful for understanding semantics. The corpus construction contains a detailed annotation guideline and the annotation process includes double annotations and additional revision from the third expert annotator, which ensures the annotation quality. Additionally, this paper applies a frameless annotation strategy. To investigate the domain adaptation performance, the paper conducts a biaffine baseline and a multi-task learning model on top of the baseline model, which are the first scores on this new benchmark. ","1. This paper provides a clear, high-quality annotation process for building up a predicate-argument dataset and it firstly introduces non-canonical texts, e.g., ZX, PB, PC, into the SRL task. 
2. The paper presents the model performance on this new cross-domain benchmark and provides a reasonable analysis of the results. ","1. Though experts have higher annotator agreement, they are not guaranteed to perform 100% accuracy. What is the consistency score between expert annotators? 
2. The consistency and accuracy are confusing. What is the difference between arg-wise consistency and accuracy? The intuition is that given the low pred-wise consistency, the label annotation agreement should be even lower because all labels related to inconsistent predicates are different. 
3. Are the consistency and accuracy scores calculated before or after experts' revision? ","1. L70-71, researches ... focus ... make -> research ... focuses ... makes 2. L389, augment -> argument ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"1. Though experts have higher annotator agreement, they are not guaranteed to perform 100% accuracy. What is the consistency score between expert annotators? 
2. The consistency and accuracy are confusing. What is the difference between arg-wise consistency and accuracy? The intuition is that given the low pred-wise consistency, the label annotation agreement should be even lower because all labels related to inconsistent predicates are different. 
3. Are the consistency and accuracy scores calculated before or after experts' revision? 
1. L70-71, researches ... focus ... make -> research ... focuses ... makes 2. L389, augment -> argument "
ARR_2022_65_review,"The paper introduces a new Chinese semantic role labeling (SRL) dataset, MuPAD, that covers 6 domains (product comment, web fiction, law, medical etc.). The dataset is motivated by the fact that most Chinese SRL research has been on the news domain. A frame-free annotation scheme was adopted to avoid difficulty in determining frames of novel predicates. A total of 24 semantic role labels, including core and non-core ones, are used in the annotation. In addition to annotation procedure and quality control, the paper provides analysis on the annotations, including consistency on predicates/arguments, accuracy of annotators, and distribution of labels. Finally, the paper shows cross-domain transfer learning experiment results. The authors use a multi-task learning (MLT) approach, adding an auxiliary task of parsing a previous dataset with a different scheme but sharing the embeddings and some model parameters. On average across the domains, this approach outperforms the baseline by a large margin. They also showed that using contextualized word representation from BERT can further boost the performance, but the gap between MLT and baseline become smaller. ","1. The dataset is in a reasonably large scale. It is constructed under solid methodology and the process is documented in detail. Moreover, performance drop in domain shift is a prominent issue for learning-based approaches to NLP, so this multi-domain dataset has great potential to drive future research towards this important direction. 
2. The authors conducted systematic experiments on cross-domain transfer learning. The results illustrate the performance drop in domain shift and demonstrate how much we can get by applying several standard techniques (MTL, contextualized embeddings). These are informative for understanding the status of the task (Chinese SRL) so are worth sharing with the research community. 
3. In addition to methodology and experiments, the paper is also comprehensive in reviewing related datasets and approaches. The whole paper is organized well and is easy to read. It also does a good job explaining some Chinese-specific linguistic phenomena which may have brought challenges to the annotators. ","1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more “canonical” (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally. 
2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels? ","- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.
- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.
- More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?
- Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.
- In the SRL formulation (Sec. 5), I am not quite sure what is “the concerned word”. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?
- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).
- How was the train/dev/test split determined? This should be noted (even if it is simply done randomly). ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,"An average salary of 28 RMB/hour (line 265) sounds a bit low to me. In the final version, it would be useful to include some reference information such as the minimum wage of the area where the university is. ( I understand that mentioning this during submission can be an issue regarding anonymity, so this concern does not affect my evaluation above.) ","1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more “canonical” (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally. 
2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels? 
- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.
- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.
- More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?
- Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.
- In the SRL formulation (Sec. 5), I am not quite sure what is “the concerned word”. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?
- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).
- How was the train/dev/test split determined? This should be noted (even if it is simply done randomly). "
ARR_2022_65_review,"### Overview of the paper The paper presents **MuPAD**, a Chinese dataset for evaluating systems on Semantic Role Labeling (SRL), with a particular focus on out-of-domain performance. The authors argue that, over the recent years, research in SRL has predominantly focused on in-domain settings where a model is trained and evaluated on data coming from the same distribution/domain (CoNLL-2005 and CoNLL-2009 are two exceptions as they provide an out-of-domain test set, but it is extremely small only 400 sentences).
In order to address this issue, the authors collected sentences from various domains, more precisely, 1 source domain on which systems are usually trained and 5 target domains (out-of-distribution with respect to source domain). Then, they annotated the collected sentences with semantic role labels using a frame-free methodology (which eliminates the need to instruct the annotators about an ontology, e.g., PropBank or FrameNet), trained a system on the source domain and evaluated the resulting system on the out-of-domain test sets.
The main takeaway is that, indeed, even though the system achieves very good results on the test set of the source domain, there is a significant drop in performance on the target domains. These results motivate the need for MuPAD as a benchmark to evaluate current and future SRL systems on out-of-domain test sets too.
### Overall considerations In general, the paper is quite good but there are also some important drawbacks. As a researcher in SRL, I am usually very excited to see new resources for the task but, in this case, it seems like MuPAD will not be released openly to the research community. Without an open resource, the paper is still good (findings?) as it discusses the limitations of current SRL systems on out-of-domain instances. ","### Main strengths - **Resource:** A large dataset with manual predicate-argument annotations. The dataset comprises over 30K sentences in **Chinese** that were annotated by at least two annotators (and checked by a third annotator in case of disagreement).
- **Annotation methodology:** The authors spend a significant part of the paper describing their annotation methodology, its motivation, and the results. The methodology consisted in assigning each annotation item (sentence + predicate) to two (paid) undergraduate students who were trained beforehand. In case of disagreement, an expert annotator was involved to solve the disagreement. The results of the annotation process are described showing ""consistency ratio"" between the two annotators (which motivates the employment of an expert annotator to solve the disagreements).
### Minor strengths - **Empirical results:** The authors show how an SRL system performs on the task (in-domain and out-of-domain results). They use a system based on that of Cai et al. (2018) which is a bit old now (surpassed by many other systems in other benchmarks), but it is still good baseline. Moreover, the authors report the results of their system when trained jointly on MuPAD and the Chinese PropBank 2.0. In this setting, the results of the system are better but there is still a very noticeable drop in performance between source and target domains.
- **Hidden arguments:** The authors made the annotators indicate hidden arguments in the sentences, i.e., arguments that are implicit in the sentence. ","### Major weaknesses - **Dataset availability:** My major concern is about the availability of this dataset. Nowhere in the paper is stated that MuPAD will be openly released, which suggests that it will be released behind a paywall. In my opinion, this is the most disheartening weakness of this work in general (beyond the paper itself). The reviewers, meta-reviewers, action editors, and everybody else involved in the organization of a conference (e.g., NAACL) are being working on this paper for free, but the dataset will be a paid product. Of course, the creation of a dataset has its costs, but so does every other paper, and there are many datasets that are being released for free nowadays. A paywall is also a roadblock for research in the field. Of course, disregard this point if the dataset will be freely released.
- **Licensing:** Part of the dataset comes from the Chinese split of CoNLL-2009 which is released with a very restricting license (CoNLL-2009 is available for purchase through the Linguistic Data Consortium and it cannot be redistributed). I imagine that MuPAD will be released with a similar license. ","### Suggested additional references - **Lines 31-32:** one common reference for ""who did what to whom [...]"" is Marquez et al. (2008). "" Semantic Role Labeling: An Introduction to the Special Issue"".
- **Lines 55-56:** references for the effectiveness of pretrained language models has been demonstrated by Shi and Lin (2019), Conia and Navigli (2020) and Paolini et al. (2021), among others.
- In the ""SRL corpora"" paragraph (Related Work) it would be great to cite the CoNLL 2005, 2008, 2009 and 2012 tasks explicitly. For example, the CoNLL-2012 shared task (Pradhan et al., 2012) included SRL annotations for English, Chinese and Arabic, if I am not mistaken.
- **Lines 448-451:** For completeness, using multitask learning to leverage knowledge from heterogeneous resources has been already studied in SRL (Conia et al., 2021).
### Suggestions - Please, add hyperlinks to the papers in the bibliography.
### Other comments Even though the authors submitted a sample (120 sentences) of their dataset for reviewing purposes, I am choosing ""No usable datasets submitted."" so as to stress that the authors seem not to plan an open release of their resource.
--- - Marquez et al., 2008. Semantic Role Labeling: an introduction to the special issue. Computational Linguistics.
- Shi and Lin, 2019. Simple BERT models for relation extraction and semantic role labeling. Arxiv.
- Conia and Navigli, 2020. Bridging the gap in multilingual semantic role labeling: a language-agnostic approach. COLING.
- Conia et al., 2021. Unifying cross-lingual semantic role labeling with heterogeneous linguistic inventories. NAACL.
- Paolini et al., 2021. Structured prediction as translation between augmented natural languages. ICLR. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"### Major weaknesses - **Dataset availability:** My major concern is about the availability of this dataset. Nowhere in the paper is stated that MuPAD will be openly released, which suggests that it will be released behind a paywall. In my opinion, this is the most disheartening weakness of this work in general (beyond the paper itself). The reviewers, meta-reviewers, action editors, and everybody else involved in the organization of a conference (e.g., NAACL) are being working on this paper for free, but the dataset will be a paid product. Of course, the creation of a dataset has its costs, but so does every other paper, and there are many datasets that are being released for free nowadays. A paywall is also a roadblock for research in the field. Of course, disregard this point if the dataset will be freely released.
- **Licensing:** Part of the dataset comes from the Chinese split of CoNLL-2009 which is released with a very restricting license (CoNLL-2009 is available for purchase through the Linguistic Data Consortium and it cannot be redistributed). I imagine that MuPAD will be released with a similar license. 
### Suggested additional references - **Lines 31-32:** one common reference for ""who did what to whom [...]"" is Marquez et al. (2008). "" Semantic Role Labeling: An Introduction to the Special Issue"".
- **Lines 55-56:** references for the effectiveness of pretrained language models has been demonstrated by Shi and Lin (2019), Conia and Navigli (2020) and Paolini et al. (2021), among others.
- In the ""SRL corpora"" paragraph (Related Work) it would be great to cite the CoNLL 2005, 2008, 2009 and 2012 tasks explicitly. For example, the CoNLL-2012 shared task (Pradhan et al., 2012) included SRL annotations for English, Chinese and Arabic, if I am not mistaken.
- **Lines 448-451:** For completeness, using multitask learning to leverage knowledge from heterogeneous resources has been already studied in SRL (Conia et al., 2021).
### Suggestions - Please, add hyperlinks to the papers in the bibliography.
### Other comments Even though the authors submitted a sample (120 sentences) of their dataset for reviewing purposes, I am choosing ""No usable datasets submitted."" so as to stress that the authors seem not to plan an open release of their resource.
--- - Marquez et al., 2008. Semantic Role Labeling: an introduction to the special issue. Computational Linguistics.
- Shi and Lin, 2019. Simple BERT models for relation extraction and semantic role labeling. Arxiv.
- Conia and Navigli, 2020. Bridging the gap in multilingual semantic role labeling: a language-agnostic approach. COLING.
- Conia et al., 2021. Unifying cross-lingual semantic role labeling with heterogeneous linguistic inventories. NAACL.
- Paolini et al., 2021. Structured prediction as translation between augmented natural languages. ICLR. "
ARR_2022_111_review,"This paper presents a method for generating word-level adversarial attacks for structured prediction tasks. Such attacks are modified examples that differ by individual words compared to the original examples, yet receive a substantially different prediction when used as input to a trained predictor.
The main contribution of this paper is SHARP, a method that relies on search-based optimization to modify examples while preserving a sentence's fluency and meaning. In a series of extensive experimentation, the authors evaluate the efficacy of this approach and demonstrate its usefulness and efficiency in generating such attacks and in guarding against them.
Within the context of adversarial attacks this is an interesting paper that clearly demonstrates a method for generating such attacks. However, I remain unconvinced of the usefulness of such attacks for truly understanding the vulnerability of trained predictors. Adversarial attacks perturb examples such that models fail on the modified examples, but understanding what causes models to fail is a causal problem that remains unanswered in this paper. ","In the context of generating adversarial attacks, this paper successfully addresses the problem by adapting several useful search-based methods for its needs.  The authors provide an extremely detailed evaluation and experimental analysis, and clearly demonstrate the efficacy of their approach. ","While I do see the benefits of generating adversarial attacks and the advantages of building tools that guard against them, I find this line of work, and this paper as part of it, problematic.  In NLP, there is a growing interest in stress-tests that measure model performance in counterfactual scenarios (see CheckList, Counterfactual Invariance to stress tests, etc.). As opposed to adversarial attacks, such papers try to design counterfactual examples that we know we want to be robust to, and carefully measure model sensitivity to them. As this is a causal problem (estimating the effect of a counterfactual on an observed outcome), such methods examine performance with respect to modifications that we can reason about (i.e. flipping gender/race etc.). Reading this paper, I’m still not convinced that we should put such emphasis on examples that are modified based on word-level perturbations generated by a correlation-based search method. As such, I believe that training models to be robust to such attacks is somewhat tangential. Generally speaking, do we have a good reason to believe that these adversarial attacks are realistic? What do we learn from them on the model and its robustness and reliability?
If I put aside the power of adversarial attacks in uncovering model vulnerability, I do find this paper interesting and useful within this context. I suggest authors better address the causal nature of generating counterfactual examples, and try to analyze the sensitivity of their method to the type of attacks generated by SHARP. In doing that, this paper can certainly contribute to the growing literature on model vulnerability and robustness. ","The paper is generally well-written, but may benefit from an additional round of editing.  Some typos for example:  Line 588 - “investigate the sensitivity“ Line 590 -  “as an search problem” Line 595 - “guarantees a better“ ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"While I do see the benefits of generating adversarial attacks and the advantages of building tools that guard against them, I find this line of work, and this paper as part of it, problematic.  In NLP, there is a growing interest in stress-tests that measure model performance in counterfactual scenarios (see CheckList, Counterfactual Invariance to stress tests, etc.). As opposed to adversarial attacks, such papers try to design counterfactual examples that we know we want to be robust to, and carefully measure model sensitivity to them. As this is a causal problem (estimating the effect of a counterfactual on an observed outcome), such methods examine performance with respect to modifications that we can reason about (i.e. flipping gender/race etc.). Reading this paper, I’m still not convinced that we should put such emphasis on examples that are modified based on word-level perturbations generated by a correlation-based search method. As such, I believe that training models to be robust to such attacks is somewhat tangential. Generally speaking, do we have a good reason to believe that these adversarial attacks are realistic? What do we learn from them on the model and its robustness and reliability?
If I put aside the power of adversarial attacks in uncovering model vulnerability, I do find this paper interesting and useful within this context. I suggest authors better address the causal nature of generating counterfactual examples, and try to analyze the sensitivity of their method to the type of attacks generated by SHARP. In doing that, this paper can certainly contribute to the growing literature on model vulnerability and robustness. 
The paper is generally well-written, but may benefit from an additional round of editing.  Some typos for example:  Line 588 - “investigate the sensitivity“ Line 590 -  “as an search problem” Line 595 - “guarantees a better“ "
ARR_2022_204_review,"This paper proposes a new task formulation to solve complex tasks. In this new formulation, there are multiple agents, each of which is capable of solving some specific types of tasks. For example, there can be a QA agent that answers natural language (NL) questions and an instruction following agent that could execute actions to accomplish an NL intent. Given a complex task described in NL, the model is asked to communicate with each agent for their task-specific knowledge and use the returned answers to proceed with the task.
In this work, they instantiate the complex task as multi-hop QA and the agents as a TextQA agent that is able to reason over large text corpus, TableQA agents that could answer questions given structured data like tables, and MathQA agent that could perform numerical reasoning. Each agent also has their own auxiliary data like (NL, answer) supervision and their independent KBs. They design a model that is able to decompose a multi-hop question to simple questions that could be answered by one agent. They compare this model with other black-box models that do not perform communication with agents and show significant improvements on a synthetic dataset they create. ","- The proposed new task formulation is novel and interesting. Intuitively, it is a promising way to resolve the complex tasks people encounter daily. The paper also provides a detailed and clear definition of this new task. ","- The instantiation of the task could not fully justify the benefit of the new task formulation. In this new proposed setting, an ideal criterion for designing individual agents is that each has mutually exclusive functionalities, and it is challenging to develop a unified model. For example, the google search agent and the Alexa shopping agent described in the introduction make such a case. However, this work design a synthetic dataset, and the agents are separated by the different forms of knowledge (text vs table) and the different proportions of knowledge in the KB. This separation is OK as long as it could reveal the true distribution in reality -- there is some knowledge that is more accessible through text than structured data and vice versa. However, the data construction process did not consider this and did a random split. A more realistic setting will bring up some interesting questions like ""how does the decomposer know which agent is more suitable to answer the current question?"", "" how can we curate such annotations?"" etc, which are not explicitly touched by the current work. To me, my main takeaway is that question decomposition is helpful, which has been studied in previous works like BREAK (Wolfson el at + 2020). Related to this concern, I also have a question regarding training the question decomposition component. According to F3, the NL questions to the text agent and the table agent look pretty similar (e.g. [table] What movies has #1 written? vs. [text] #1 produces which materials?), what are the supervision signals that hint the model to predict one agent over another?
- Some descriptions of the experiment setting are somewhat vague, and therefore it is not super clear whether the comparisons are fair. My main question is how factual knowledge is provided to each model? 
     * In *Models with Access to Agent Knowledge*, how do you construct the context? Do you randomly sample some context from the *possible world* of the question? 
     * Do you somehow distinguish the source (e.g., knowledge of TextQA, knowledge of TableQA)? 
     * After decomposing the question through `NextGen`, how do you provide the context when querying an individual agent? Do you provide the ground truth context without distractors? Or do you train some retriever (like in *Models with Fact Supervision*) to retrieve the context? ",- Some case study and more systematic error analysis can probably help the readers to understand in which cases the proposed method works and how. ,2.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The instantiation of the task could not fully justify the benefit of the new task formulation. In this new proposed setting, an ideal criterion for designing individual agents is that each has mutually exclusive functionalities, and it is challenging to develop a unified model. For example, the google search agent and the Alexa shopping agent described in the introduction make such a case. However, this work design a synthetic dataset, and the agents are separated by the different forms of knowledge (text vs table) and the different proportions of knowledge in the KB. This separation is OK as long as it could reveal the true distribution in reality -- there is some knowledge that is more accessible through text than structured data and vice versa. However, the data construction process did not consider this and did a random split. A more realistic setting will bring up some interesting questions like ""how does the decomposer know which agent is more suitable to answer the current question?"", "" how can we curate such annotations?"" etc, which are not explicitly touched by the current work. To me, my main takeaway is that question decomposition is helpful, which has been studied in previous works like BREAK (Wolfson el at + 2020). Related to this concern, I also have a question regarding training the question decomposition component. According to F3, the NL questions to the text agent and the table agent look pretty similar (e.g. [table] What movies has #1 written? vs. [text] #1 produces which materials?), what are the supervision signals that hint the model to predict one agent over another?
- Some descriptions of the experiment setting are somewhat vague, and therefore it is not super clear whether the comparisons are fair. My main question is how factual knowledge is provided to each model? 
     * In *Models with Access to Agent Knowledge*, how do you construct the context? Do you randomly sample some context from the *possible world* of the question? 
     * Do you somehow distinguish the source (e.g., knowledge of TextQA, knowledge of TableQA)? 
     * After decomposing the question through `NextGen`, how do you provide the context when querying an individual agent? Do you provide the ground truth context without distractors? Or do you train some retriever (like in *Models with Fact Supervision*) to retrieve the context? 
- Some case study and more systematic error analysis can probably help the readers to understand in which cases the proposed method works and how. "
ARR_2022_266_review,"This work proposes to frame the task of controlled text generation as sampling from a combination of black-box models that are responsible for various components of interest such as fluency, the controlled attribute, and relevance to the prompt. Pre-trained black-box experts provide scores for each of these desired components that are linearly combined to form an ""energy-based"" model, from which samples can be drawn without needing any task-specific training. Experiments are conducted on several tasks such as controllable debiasing, sentiment and formality transfer and prompted generation, and results show that this method outperforms task-specific baselines. ","1. Paper is well-written and easy to follow. 
2. The idea of having a modular controlled text generation model that uses blackbox components is interesting and novel. The approach can be adapted to any desired components or metrics by using the appropriate pre-trained expert models, and does not need any additional task-specific training. 
3. Several experiments are conducted to show that their model outperforms task-specific baselines on multiple tasks. Ablation results are also shown which help assess the contributions of each component. 
4. Human evaluations are conducted to demonstrate the superior quality of the generated text. Some limitations of the model are also briefly discussed. ","1. One of the main drawbacks of this approach is that presumably the different component black-box experts of the controlled text generation have to be manually selected and the weighted linear combination has to be fine-tuned for each task. It is also not discussed if the inference time is significantly affected by this approach. 
2. For the sentiment transfer task, the model with the higher Hamming distance coefficient is considered to be the best model based on the BertScore with respect to the source, which essentially measures how much deviation has been introduced. It appears however that the model with the higher Discriminator coefficient is better, in terms of perplexity and the internal/external classifiers. Given that the Hamming distance in the reference is much higher, it may not be necessary to absolutely reduce the number of changes made, if it serves the overall purpose of the text generation to make more changes. This is somewhat true for the formality transfer task as well. 
3. In Table 3, for the formality transfer task, the method sees a decline in performance for the ->Informal task. While the improvement in the ->Formal task is probably a decent tradeoff, this issue is not addressed at all. 
4. Percentage preference through majority voting is reported for the human evaluation. More robust correlation/agreement metrics such as Cohen's Kappa should be reported for reliability. ","- BertScore and BLEURT are inconsistently typeset through the paper (alternatively as Bertscore or Bleurt). It would be better to maintain consistency.
- Line 244 in Section 2.3 refers to $E_{gen}$ and $E_{rev}$ which have not been previously introduced. It is not easy to deduce what they mean since they are not explained until the next section. Some re-writing for clarity might help here.  - Line 182: discirminate: discriminate - Line 203: This penalization token -> This penalizes token - Line 254: describe -> described - Line 376: Dathathri et al. (2020) -> (Dathathri et al, 2020) - Line 434: Ma et al citation missing year - Line 449: describedd -> described - Line 449: in the text -> in a text - Line 520: prodduct -> product - Table 3 BertScore(sc) -> BertScore (src) - Line 573: which use for -> which are used for  - Line 631: similar, approaches -> similar approaches ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. One of the main drawbacks of this approach is that presumably the different component black-box experts of the controlled text generation have to be manually selected and the weighted linear combination has to be fine-tuned for each task. It is also not discussed if the inference time is significantly affected by this approach. 
2. For the sentiment transfer task, the model with the higher Hamming distance coefficient is considered to be the best model based on the BertScore with respect to the source, which essentially measures how much deviation has been introduced. It appears however that the model with the higher Discriminator coefficient is better, in terms of perplexity and the internal/external classifiers. Given that the Hamming distance in the reference is much higher, it may not be necessary to absolutely reduce the number of changes made, if it serves the overall purpose of the text generation to make more changes. This is somewhat true for the formality transfer task as well. 
3. In Table 3, for the formality transfer task, the method sees a decline in performance for the ->Informal task. While the improvement in the ->Formal task is probably a decent tradeoff, this issue is not addressed at all. 
4. Percentage preference through majority voting is reported for the human evaluation. More robust correlation/agreement metrics such as Cohen's Kappa should be reported for reliability. 
- BertScore and BLEURT are inconsistently typeset through the paper (alternatively as Bertscore or Bleurt). It would be better to maintain consistency.
- Line 244 in Section 2.3 refers to $E_{gen}$ and $E_{rev}$ which have not been previously introduced. It is not easy to deduce what they mean since they are not explained until the next section. Some re-writing for clarity might help here.  - Line 182: discirminate: discriminate - Line 203: This penalization token -> This penalizes token - Line 254: describe -> described - Line 376: Dathathri et al. (2020) -> (Dathathri et al, 2020) - Line 434: Ma et al citation missing year - Line 449: describedd -> described - Line 449: in the text -> in a text - Line 520: prodduct -> product - Table 3 BertScore(sc) -> BertScore (src) - Line 573: which use for -> which are used for  - Line 631: similar, approaches -> similar approaches "
ARR_2022_266_review,"The authors of this work propose Mix and Match LM. Their method uses global-scores for controllable text generation by drawing samples from an energy-based model. Here, the values of the energies come from blackbox models that correspond to attributes of generation (fluency, faithfulness to conditioning, etc.). They show empirical gains relative to some other baseline methods. ","- The paper is written well and much of the method is quite accessible. I feel like readers would have a decent grasp of the method and approach after a single read.
- The method is simple.
- Demonstrate performance improvements against methods that are more computationally prohibitive or necessitate fine-tuning.
- Quite modular and hypothetically flexible to add in other aspects - more experimentation would need to be done to see how truly flexible to add stuff in. ","For the same problem and tasks, there's a lot of other prior work that are not energy-based modeling (two are) that could be compared to. After reading, I'm unsure how well this method would compare to those and what situations this approach would be better for. Many of those combine some blackbox methods in different ways. A discussion of these works and comparisons would really strengthen the paper greatly. Some of the work I'm talking about are: DExperts: Decoding-time controlled text generation with experts and anti-experts (Liu et al. 2021) Plug and Play Autoencoders for Conditional Text Generation (Mai et al 2020) Sentence Bottleneck Autoencoders from Transformer Language Models (Montero et al. 2021) A distributional approach to controlled text generation (Khalifa et al. 2020) GeDi (Kruase et al. 2020) DAPT (Gururangan et al. 2020) CTRL (Keskar et al. 2019) FUDGE (Yang and Klein 2021) You do cite a few of these, but comparisons to them would really strengthen the work.
These papers just came out, around or after the ARR deadline, but could be interesting too:  - Controlling Conditional Language Models with Distributional Policy Gradients (Korbak et al. 2021) - Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation (Mai and Henderson 2021) Many of these methods compare favorably against PPLM, so they could be stronger baselines against which your work could be compared to.
Another aspect that could be improved is the experimentation around the trade-offs between accuracy and BLEU score. Table 6 and 7 start this. It'd be informative to see some of the baselines varied slightly on a plot similar to Mai et al. 2020 or Montero et al 2021 for this task where self-BLEU/ref-BLEU is on one axis and accuracy via the classifier is on the other and things are plotted (top right being better).
A task such as toxicity via RealToxicityPrompts could be quite interesting and offer a test-bed for comparison against some of the other prior work.
I'd be interested in seeing how this method extends to other models, i.e. an auto-regressive model such as GPT-2. Is it simply extendible? How well would it do?
This is a paper on controllable text generation and a method for this. A discussion of the potential broader impact is missing. It has potentially wide-ranging downstream impact, so including one to highlight potential harms and effects is important. ","Misc comments: - line 35: cite GPT3 - line 46: cite GPT2 - Figure 1: The caption seems oddly placed being on the right-hand-side (presumably this was to fit the figure easily for review, but may not strictly be okay so something to check, I'm not sure).
- line 331: what discriminator did you use? Did you use the same one for the PPLM setup for evaluation?
Another flavor of related work to help motivate the approach (no fine-tuning, no optimization needed), could be around prompting, steering LMs without fine-tuning, etc.
It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Shick and Schutze 2020) Few-Shot Text Generation with Pattern-Exploiting Training (Shick and Schutze 2020) Eliciting Knowledge from Language Models Using Automatically Generated Prompts (Shin et al. 2020) Can Unconditional Language Models Recover Arbitrary Sentences? ( Subramani et al. 2019) ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"For the same problem and tasks, there's a lot of other prior work that are not energy-based modeling (two are) that could be compared to. After reading, I'm unsure how well this method would compare to those and what situations this approach would be better for. Many of those combine some blackbox methods in different ways. A discussion of these works and comparisons would really strengthen the paper greatly. Some of the work I'm talking about are: DExperts: Decoding-time controlled text generation with experts and anti-experts (Liu et al. 2021) Plug and Play Autoencoders for Conditional Text Generation (Mai et al 2020) Sentence Bottleneck Autoencoders from Transformer Language Models (Montero et al. 2021) A distributional approach to controlled text generation (Khalifa et al. 2020) GeDi (Kruase et al. 2020) DAPT (Gururangan et al. 2020) CTRL (Keskar et al. 2019) FUDGE (Yang and Klein 2021) You do cite a few of these, but comparisons to them would really strengthen the work.
These papers just came out, around or after the ARR deadline, but could be interesting too:  - Controlling Conditional Language Models with Distributional Policy Gradients (Korbak et al. 2021) - Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation (Mai and Henderson 2021) Many of these methods compare favorably against PPLM, so they could be stronger baselines against which your work could be compared to.
Another aspect that could be improved is the experimentation around the trade-offs between accuracy and BLEU score. Table 6 and 7 start this. It'd be informative to see some of the baselines varied slightly on a plot similar to Mai et al. 2020 or Montero et al 2021 for this task where self-BLEU/ref-BLEU is on one axis and accuracy via the classifier is on the other and things are plotted (top right being better).
A task such as toxicity via RealToxicityPrompts could be quite interesting and offer a test-bed for comparison against some of the other prior work.
I'd be interested in seeing how this method extends to other models, i.e. an auto-regressive model such as GPT-2. Is it simply extendible? How well would it do?
This is a paper on controllable text generation and a method for this. A discussion of the potential broader impact is missing. It has potentially wide-ranging downstream impact, so including one to highlight potential harms and effects is important. 
Misc comments: - line 35: cite GPT3 - line 46: cite GPT2 - Figure 1: The caption seems oddly placed being on the right-hand-side (presumably this was to fit the figure easily for review, but may not strictly be okay so something to check, I'm not sure).
- line 331: what discriminator did you use? Did you use the same one for the PPLM setup for evaluation?
Another flavor of related work to help motivate the approach (no fine-tuning, no optimization needed), could be around prompting, steering LMs without fine-tuning, etc.
It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Shick and Schutze 2020) Few-Shot Text Generation with Pattern-Exploiting Training (Shick and Schutze 2020) Eliciting Knowledge from Language Models Using Automatically Generated Prompts (Shin et al. 2020) Can Unconditional Language Models Recover Arbitrary Sentences? ( Subramani et al. 2019) "
ARR_2022_308_review,"This paper categorizes and analyzes different responses from benchmarks used to train conversational models and from conversational models. The authors find that benchmarks contain more than 60% hallucinated responses, and that models can amplify the amount of hallucinated responses.
Note: I've previously reviewed this paper. After reading through the authors' response and the new draft, I've updated my review. ","- Novel analysis detailing the frequency of hallucinations and the different response types associated with hallucinations - Includes many details for reproducibility and is nicely organized - Authors addressed much of previous feedback and improved the flow of some sections (e.g., Sec 2) as well ","This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets.  At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this ""data quality audit"" work, because this work sets a precedent/""standard"" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. ","- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - ""Limitations"" section in the Appendix could be moved to the ""Impact Statement & Ethics"" section. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets.  At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this ""data quality audit"" work, because this work sets a precedent/""standard"" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. 
- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - ""Limitations"" section in the Appendix could be moved to the ""Impact Statement & Ethics"" section. "
ARR_2022_307_review,"The authors address a problem that underlies the understanding of procedures: They introduce a task of sequencing unordered subtasks/steps of multimodal instructions, and describe the curation of two datasets (RecipeQA and WikiHow) for this task. 
They propose a sequence-aware pre-training method for improving current models' ability of reasoning over multimodal information to solve the task (MLM, (patch-based) ""image-swapping"" prediction ((P)ISP), sequential MRM). 
The models used for the experiments are Faster-RCNN and CLIP (vision), RoBERTa, and the multimodal single-stream models VisualBERT and CLIP-ViL.  Experimental results on the two new datasets show that (i) multimodality is beneficial, where humans seem to more effectively benefit from the visual information than the models, that (ii) models fall short against human performance irrespective of the modality, that (iii) the sequence-aware pre-training method the authors propose slightly improves effectiveness. An analysis on specific categories of WikiHow additionally shows that the multimodal models do not effectively use both modalities. ","- Release of two curated datasets for sequential procedure learning - Well written, clear motivation, good experimental methodology (ablation study, comparison of multiple state-of-the-models, comparison against human performance).
- Clear experimental findings: visual information relevant for sequential procedures, but improved models that more effectively use multimodal information is required, such as to come closer to the human upper bound ","- Training is performed on each dataset separately. This may result in the models being quite domain-specific, i.e., limiting their generalisation ability.  - Method is limited to 2 sequences (minor point) ","- PISP objective: I can see how patch-based swapping may help for learning a better intra-modal association in the case where the patches depict some objects the text refers to. But what is the assumption regarding the benefit of general patch-based (as in CLIP-ViL) swapping for the sequencing task (general intra-modal understanding is already the motivation for SMRM, isn't it?)?
- Sect 4.2.3: Maybe an illustration would help to explain this part, I found it hard to understand.
- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?
- Section 5: Why did you not also compare against a pre-trained ResNet version, as you did for CLIP (image-only)?
- Section 5: How would you explain the gap of the effectiveness in the Image-Only setting on WikiHow vs. RecipeQA? For the other settings/models/modalities, the difference in effectiveness is comparably marginal.  **Typos, Grammar, Style, and Presentation Improvements** - l135: ""consists"" -> ""which consists"" or ""consisting"" - Section 3.2: The amount of footnotes could be reduced.  - l318: select *the* same - l.466: *the* text-only ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- Training is performed on each dataset separately. This may result in the models being quite domain-specific, i.e., limiting their generalisation ability.  - Method is limited to 2 sequences (minor point) 
- PISP objective: I can see how patch-based swapping may help for learning a better intra-modal association in the case where the patches depict some objects the text refers to. But what is the assumption regarding the benefit of general patch-based (as in CLIP-ViL) swapping for the sequencing task (general intra-modal understanding is already the motivation for SMRM, isn't it?)?
- Sect 4.2.3: Maybe an illustration would help to explain this part, I found it hard to understand.
- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?
- Section 5: Why did you not also compare against a pre-trained ResNet version, as you did for CLIP (image-only)?
- Section 5: How would you explain the gap of the effectiveness in the Image-Only setting on WikiHow vs. RecipeQA? For the other settings/models/modalities, the difference in effectiveness is comparably marginal.  **Typos, Grammar, Style, and Presentation Improvements** - l135: ""consists"" -> ""which consists"" or ""consisting"" - Section 3.2: The amount of footnotes could be reduced.  - l318: select *the* same - l.466: *the* text-only "
ARR_2022_269_review,"This paper introduces a new method called CrossAligner which is used to transfer knowledge from English (or a high-resource language) to some other language without the presence of data using a zero-shot approach. Several methods that used CrossAligner  such as training techniques for machine translation, dialog classification, and more along with a detailed error analysis are introduced as intrinsic evaluations of the method. ","The paper presents statistical rigor on the technique it presents as novel, CrossAligner. 
The mathematics are sound and the approach makes sense. 
The qualitative findings are well presented and a deep research analysis covering the architecture, gradient losses, and intent and entity classification is performed. 
All metrics and results are gotten from previous work and others are introduced in a sound manner, based on investigative findings. ","In some cases, this paper goes overboard with the analysis in what could seem as an attempt to salvage good results. 
There are several findings presented on several, almost non-related, tasks and the related work section, while interesting seems like overkill. 
Some of the more important works on translation multi-lingual transfer are omitted. 
The findings are not outperforming the current state of the art in several tasks and in those tasks where the CrossAligner does outperform the state of the art, it is not bot much (1 point on F-score more or less). 
While the statistical rigor is fun to read, I am not sure that the overall findings are novel enough for an ACL paper. 
Despite the statistical rigor and attention paid on the details of error especially from Figure 2 where weighted losses are combined, the error analysis makes a lot of generic claims that do not seem to be based on the statistical proof. ","Line 048 - There is an assumption that it ""can"" be done based on non-relevant results. 
Line 058 - The intro here talks about alignment but then there seems to be a reference to Question Answering (QA), I understand that the case is not that but the writing here seems to be a little convoluted. 
Lines 068 - 080 - Are these lines referring back to the QA idea or is this now a translation problem, how did translation become part of a QA problem? Is it due to the desire to show evidence that the aligner works? Please explain better. 
Line 081 - The related work is a little long, you could mention the two types of transfer and cite some work in a few lines, not this many pages. It is not bad what you have done but not as important I think. Also, you may want to mention the latest low-resource XLM transfer findings from AmericasNLP (Mager et. al) for example to help strengthen this some. 
Line 185 - You are repeating some here. 
Line 173 - borne --> born Line 171-175 - Is there a percentage amount that was observed previously? 
Line 178 - Please explain what you mean by ""losing"" their positional information during translation. 
Line 188 - Algorithm 1, line 14, while it may be obvious, ""y"" is never introduced or explained, is it the labels? 
Line 188 - Algorithm 1, line 25, gradients are explained and shown as a method call, but the main technique (CA) is not really in the algorithm, is the novel idea the logloss gradient transfer? If so, you may want to separate it and show it better. 
Line 238 - ""forms a positive pair"", how is this pair formed by the -log on cosine similarity, doubtful but please explain more. 
Line 286 - Here the datasets point back to QA, but translation is covered, what is this paper about, QA? Intent Classification? Entity Classification? Or, translation? Or all? The line is now unclear. 
Line 310 - A ""minimalist"" setup, you may want to remove some of the related work and expand this, how can your work be reproduced off of these settings? 
Lines 320 - 330 - Now, it looks like we are combining tasks again, the other sections here, however, are good. 
Lines 381 - 410 - The gains here do not represent a major novelty, in several cases there are losses, which is fine but discouraging. 
Lines 430 - 432 - Has this type of loss been proposed in the past for interpretability? If so, please cite. 
Lines 441 - 453 - I am not convinced that doing this really helps show the performance, it almost seems as if you are mining to find a difference. Could you not show easily an f-score as you have already done? I understand that it probably isn't the case but the deep dive here is not warranted really in my opinion. 
Section 6.1 - Error Analysis summary, several issues, ""don't carry important sentence level semantics"" - this is not clear, please explain; ""culture and vernacular"" - how did you get that out of your findings?; "" The limits of machine translation"" - please see the AmericasNLP paper mentioned before and cite it as it is important; ""Finally, there were no substantial qualitative..."" - this paper does not show that really. 
Section 7 - The conclusion is weak as far as the results are concerned at a high level. I would suggest that the technique focused on more high level ways of measuring for gains. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"In some cases, this paper goes overboard with the analysis in what could seem as an attempt to salvage good results. 
There are several findings presented on several, almost non-related, tasks and the related work section, while interesting seems like overkill. 
Some of the more important works on translation multi-lingual transfer are omitted. 
The findings are not outperforming the current state of the art in several tasks and in those tasks where the CrossAligner does outperform the state of the art, it is not bot much (1 point on F-score more or less). 
While the statistical rigor is fun to read, I am not sure that the overall findings are novel enough for an ACL paper. 
Despite the statistical rigor and attention paid on the details of error especially from Figure 2 where weighted losses are combined, the error analysis makes a lot of generic claims that do not seem to be based on the statistical proof. 
Line 048 - There is an assumption that it ""can"" be done based on non-relevant results. 
Line 058 - The intro here talks about alignment but then there seems to be a reference to Question Answering (QA), I understand that the case is not that but the writing here seems to be a little convoluted. 
Lines 068 - 080 - Are these lines referring back to the QA idea or is this now a translation problem, how did translation become part of a QA problem? Is it due to the desire to show evidence that the aligner works? Please explain better. 
Line 081 - The related work is a little long, you could mention the two types of transfer and cite some work in a few lines, not this many pages. It is not bad what you have done but not as important I think. Also, you may want to mention the latest low-resource XLM transfer findings from AmericasNLP (Mager et. al) for example to help strengthen this some. 
Line 185 - You are repeating some here. 
Line 173 - borne --> born Line 171-175 - Is there a percentage amount that was observed previously? 
Line 178 - Please explain what you mean by ""losing"" their positional information during translation. 
Line 188 - Algorithm 1, line 14, while it may be obvious, ""y"" is never introduced or explained, is it the labels? 
Line 188 - Algorithm 1, line 25, gradients are explained and shown as a method call, but the main technique (CA) is not really in the algorithm, is the novel idea the logloss gradient transfer? If so, you may want to separate it and show it better. 
Line 238 - ""forms a positive pair"", how is this pair formed by the -log on cosine similarity, doubtful but please explain more. 
Line 286 - Here the datasets point back to QA, but translation is covered, what is this paper about, QA? Intent Classification? Entity Classification? Or, translation? Or all? The line is now unclear. 
Line 310 - A ""minimalist"" setup, you may want to remove some of the related work and expand this, how can your work be reproduced off of these settings? 
Lines 320 - 330 - Now, it looks like we are combining tasks again, the other sections here, however, are good. 
Lines 381 - 410 - The gains here do not represent a major novelty, in several cases there are losses, which is fine but discouraging. 
Lines 430 - 432 - Has this type of loss been proposed in the past for interpretability? If so, please cite. 
Lines 441 - 453 - I am not convinced that doing this really helps show the performance, it almost seems as if you are mining to find a difference. Could you not show easily an f-score as you have already done? I understand that it probably isn't the case but the deep dive here is not warranted really in my opinion. 
Section 6.1 - Error Analysis summary, several issues, ""don't carry important sentence level semantics"" - this is not clear, please explain; ""culture and vernacular"" - how did you get that out of your findings?; "" The limits of machine translation"" - please see the AmericasNLP paper mentioned before and cite it as it is important; ""Finally, there were no substantial qualitative..."" - this paper does not show that really. 
Section 7 - The conclusion is weak as far as the results are concerned at a high level. I would suggest that the technique focused on more high level ways of measuring for gains. "
ARR_2022_269_review,This paper proposes a framework for zero shot cross-lingual natural language understanding that makes use of translation systems and bases on learning the alignment from unlabeled parallel data. The authors evaluated their methods on a bunch of datasets about intent recognition and entity recognition tasks and showed some promising results. They also included an interesting qualitative analysis for more insights. ,"- The authors evaluated carefully their framework on different datasets and their results showed that their proposed framework is promising.
- The authors provided an error analysis that shows interesting insights for readers. ","- It is not clear for me about the novelty of the proposed methods.  - The proposed method relies on the quality of translation systems.  - I'm not sure whether the differences of some results are significant (see Table 1).  - The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective. ",- Did the authors run their experiments several times with different random initializations? ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- It is not clear for me about the novelty of the proposed methods.  - The proposed method relies on the quality of translation systems.  - I'm not sure whether the differences of some results are significant (see Table 1).  - The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective. 
- Did the authors run their experiments several times with different random initializations? "
ARR_2022_173_review,"This paper presents a technique for adapting the learning process based on CBMI (Conditional Bilingual Mutual Information), which is defined to be the ratio of the translation probability over the language model probability for each token.  The motivation of this method is that neural models tend to already excel at producing fluent text, but sometimes fail to produce accurate translations.  Experimentally, the approach shows a small but measurable difference (according to BLEU) over the baseline with no adaptive training, and performs similarly (perhaps better) than other similar methods.  One of the models is also compared via human evaluation to the baseline, scoring with adequacy and fluency, and the technique increases adequacy more than fluency (though both are already high). ","- Reasonable motivation for the technique from a theoretical view - Technique is practical, involves training a small LM model during training but no test-time overhead.
- Many similar techniques are reviewed, briefly explained, and compared ","- Some important details are omitted, e.g., pNMT is ""the probability output by the NMT model"" and pLM ""the probability output by an additional target-side language model"" without any real implementation details.  Are these computed using the full transformer models (embedding, encoder, decoder, softmax)?  Is the BPE vocabulary of the LM fixed to the same vocab as the NMT model?  Is the NMT model used in CMBI the one being trained, or the fixed one after 100k steps?  Learning rates and decay are also not discussed.  I have a good idea of the answer to some of these questions, but the details could be important.
- Is the baseline transformer model only trained for 100k steps?  If so, how do we know that training for another 100-200k steps alone might not account for the score improvement.
- Some explanations of techniques are too brief, e.g., Focal and Anti-focal loss are described exactly the same (same formula and even hyperparameters) yet give different results, Self-paced learning receives a too-brief single sentence description.  Also, section 5.4 and appendix C are not detailed enough to understand the experiment. ","- I'd like to see some discussion/visualization of learning curves, since the kind of differences in scores reported can be due to converged vs. non-converged models - Are the Self-paced learning results all from another paper?  In Table 1 only the base results are flagged as such, but in table 6, it looks like both results are.
- Using p(x|y_<j) -- the source probability conditioned on the target prefix -- seems a strange choice.  I would think the unconditional source probability would be better.  Yet, based on Eq 7, the source probability factor is cancelled out anyway, so it doesn't really matter.
- Eq 15 and 16 seem identical, but they are called ""focal loss"" and ""anti-focal loss"".  Shouldn't they be opposite?  Otherwise how are they different in practice… even the hyperparams from the appendix are largely the same.  Please highlight the difference in the description in Section 4.4.
- Typo: +0.55 -> 0.54 BLEU (line 473) - Section 5.4 - What is really happening here?  How are these priors calculated and used in training?  Is this related to equation 18 (from Appendix C)?  Also, the LM prior seems to give equivalent results to the ""prior selection"" and even to the full CBMI adaptive training method, why is this LM prior not compared to the LM prior in table 1?
- Are the CMBI values used in figure 4 and prior selection the non-normalized values - Appendix C:  hard to understand how Figure 5 is produced, since CBMI values are calculated per token, not per sentence.  Also how is equation 18 used? 
Figure 4 shows the LM prior dominating < 0, the TM prior dominating > 8, and CMBI prior dominating between them.  Why dos the ""prior selection"" method then have the TM prior between 0 and 6, and the CMBI prior above 6?  This doesn't match figure 4 (at least as labeled). ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Some important details are omitted, e.g., pNMT is ""the probability output by the NMT model"" and pLM ""the probability output by an additional target-side language model"" without any real implementation details.  Are these computed using the full transformer models (embedding, encoder, decoder, softmax)?  Is the BPE vocabulary of the LM fixed to the same vocab as the NMT model?  Is the NMT model used in CMBI the one being trained, or the fixed one after 100k steps?  Learning rates and decay are also not discussed.  I have a good idea of the answer to some of these questions, but the details could be important.
- Is the baseline transformer model only trained for 100k steps?  If so, how do we know that training for another 100-200k steps alone might not account for the score improvement.
- Some explanations of techniques are too brief, e.g., Focal and Anti-focal loss are described exactly the same (same formula and even hyperparameters) yet give different results, Self-paced learning receives a too-brief single sentence description.  Also, section 5.4 and appendix C are not detailed enough to understand the experiment. 
- I'd like to see some discussion/visualization of learning curves, since the kind of differences in scores reported can be due to converged vs. non-converged models - Are the Self-paced learning results all from another paper?  In Table 1 only the base results are flagged as such, but in table 6, it looks like both results are.
- Using p(x|y_<j) -- the source probability conditioned on the target prefix -- seems a strange choice.  I would think the unconditional source probability would be better.  Yet, based on Eq 7, the source probability factor is cancelled out anyway, so it doesn't really matter.
- Eq 15 and 16 seem identical, but they are called ""focal loss"" and ""anti-focal loss"".  Shouldn't they be opposite?  Otherwise how are they different in practice… even the hyperparams from the appendix are largely the same.  Please highlight the difference in the description in Section 4.4.
- Typo: +0.55 -> 0.54 BLEU (line 473) - Section 5.4 - What is really happening here?  How are these priors calculated and used in training?  Is this related to equation 18 (from Appendix C)?  Also, the LM prior seems to give equivalent results to the ""prior selection"" and even to the full CBMI adaptive training method, why is this LM prior not compared to the LM prior in table 1?
- Are the CMBI values used in figure 4 and prior selection the non-normalized values - Appendix C:  hard to understand how Figure 5 is produced, since CBMI values are calculated per token, not per sentence.  Also how is equation 18 used? 
Figure 4 shows the LM prior dominating < 0, the TM prior dominating > 8, and CMBI prior dominating between them.  Why dos the ""prior selection"" method then have the TM prior between 0 and 6, and the CMBI prior above 6?  This doesn't match figure 4 (at least as labeled). "
ARR_2022_363_review,"This paper suggests there is a vanishing-gradient issue when using a global learning rate to train additive late-fusion multimodal models and presents a solution named Modality-Specific Learning Rate (MSLR). The authors explored 3 different variants of MSLR, showing that when optimal unimodal learning rates are close, “smooth” variant performs the best, and when optimal unimodal LRs are far apart, “keep” and “dynamic” variants are better. The authors also performed layer conductance analysis to validate the hypothesis about vanishing gradients, showing that MSLR leads to models with larger layer conductance values. ","1) This paper introduces a conceptually simple and intuitive idea of Modality-Specific Learning Rates, which empirically works well for late-fusion additive models; 2) This paper presents layer conductance analysis and shows that layer conductance disparity between modalities are mitigated when MSLR is applied; Overall, this paper proposes a simple yet empirically beneficial method to apply modality-specific learning rates and this improves late-fusion performance. ","1) It is unclear how one should do the smoothing of LR in the “Smooth” variant. The authors proposed some values in the reasonable range but without any discussion on how they are decided. It would be great if there is a methodical approach or a description of how this is done. 
2) Layer conductance is not a good way to measure vanishing gradients. It is a measure of neuron importance for a given input. While the result of conductance analysis is in the form of “gradients”, it is a different concept from the gradients the model receives during training. Hence, I would argue that the layer conductance analysis is actually not measuring what the authors aim to measure and the conclusions should be revised. 
3) MSLR requires knowing the unimodal optimal learning rates in advance - which means we have to spend resources to tune unimodal models and discard them in the end. It would be interesting to see how this compares to directly tuning a late-fusion model with separate LR for each modality, especially with a similar or less compute budget. ","1) Relating to weakness 2), the authors need to properly measure gradients during training and validate their hypothesis about vanishing gradients, which would be easy to measure with norm of gradients. If the authors do not have the resources to conduct this analysis in time, I suggest the authors to revise their claims on vanishing gradients as layer conductance does not measure that. 
2) I suggest the authors to add a brief discussion about how to pick learning rates in the “Smooth” variant; 3) Typo - line 440, “...from the all the joint…” -> “...from all the joint…” 4) Typo - line 561, “while MSLR while achieves…” -> “while MSLR achieves…” ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1) It is unclear how one should do the smoothing of LR in the “Smooth” variant. The authors proposed some values in the reasonable range but without any discussion on how they are decided. It would be great if there is a methodical approach or a description of how this is done. 
2) Layer conductance is not a good way to measure vanishing gradients. It is a measure of neuron importance for a given input. While the result of conductance analysis is in the form of “gradients”, it is a different concept from the gradients the model receives during training. Hence, I would argue that the layer conductance analysis is actually not measuring what the authors aim to measure and the conclusions should be revised. 
3) MSLR requires knowing the unimodal optimal learning rates in advance - which means we have to spend resources to tune unimodal models and discard them in the end. It would be interesting to see how this compares to directly tuning a late-fusion model with separate LR for each modality, especially with a similar or less compute budget. 
1) Relating to weakness 2), the authors need to properly measure gradients during training and validate their hypothesis about vanishing gradients, which would be easy to measure with norm of gradients. If the authors do not have the resources to conduct this analysis in time, I suggest the authors to revise their claims on vanishing gradients as layer conductance does not measure that. 
2) I suggest the authors to add a brief discussion about how to pick learning rates in the “Smooth” variant; 3) Typo - line 440, “...from the all the joint…” -> “...from all the joint…” 4) Typo - line 561, “while MSLR while achieves…” -> “while MSLR achieves…” "
ARR_2022_117_review,"This paper introduces two grammatical error correction probing tasks for     Chinese masked language models: one for replacing erroneous characters or     words, and another for inserting the correct character or word in order to     make a sentence grammatical. The authors employ these tasks to investigate     models trained via different masking strategies: character-based masking,     word-based masking or both. They find that, for spans of length 1, their     character-masked models perform best for both task, while for spans of     length 2-4, the combined character and word model generally fares better. 
    The authors also experiment with fine-tuning existing pretrained RoBERTA     models, and demonstrate that these models fare worse on both tasks. ","This paper articulates an interesting distinction between character and     word-level masking for Chinese masked language modeling, and investigates     the implications of such modeling decisions in a practical way. Generally,     it is concise and easy to follow. The illustrations are very informative and     provide a good overview of the tasks. The examples are likewise very useful for non-Chinese speakers. 
    Naturally, the dataset and models the authors create can be useful for     future research. ","A study focusing on grammatical error correction should include an error     analysis that attempts to characterize the types of a mistakes the model     makes. The authors likely had to go forego this step due to space     constraints, but without it the insights produced here are generally     limited.  Furthermore, the authors do not include any sort of context for what kind of     performance the models are expected to achieve for these tasks in favorable     circumstances, e.g. in a supervised learning regime. Without this     information, it is difficult to ascertain if the out-of-the-box models'     probing performance is at all good, or if it pales in comparison to simple     supervised baselines. I imagine this would be easy to achieve by fine-tuning     a model on some splits of the dataset, or training another architecture for     the task, e.g. a BiLSTM.  Lastly, it would be useful to see how their models fare on downstream tasks,     how they compare with other Chinese-language MLMs like RoBERTa, and what     implications this might hold for Chinese MLM design. Indeed, the authors     provide this information in the appendix, but nothing is shown in the main     body of the paper, and the discussion is limited to a sentence. ","- How is P@10 calculated for spans of length 2+? This should be mentioned.
- Is the [MASK] token applied to the omitted characters in the insertion task?
Line 249: Michelle, not Micheal ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"A study focusing on grammatical error correction should include an error     analysis that attempts to characterize the types of a mistakes the model     makes. The authors likely had to go forego this step due to space     constraints, but without it the insights produced here are generally     limited.  Furthermore, the authors do not include any sort of context for what kind of     performance the models are expected to achieve for these tasks in favorable     circumstances, e.g. in a supervised learning regime. Without this     information, it is difficult to ascertain if the out-of-the-box models'     probing performance is at all good, or if it pales in comparison to simple     supervised baselines. I imagine this would be easy to achieve by fine-tuning     a model on some splits of the dataset, or training another architecture for     the task, e.g. a BiLSTM.  Lastly, it would be useful to see how their models fare on downstream tasks,     how they compare with other Chinese-language MLMs like RoBERTa, and what     implications this might hold for Chinese MLM design. Indeed, the authors     provide this information in the appendix, but nothing is shown in the main     body of the paper, and the discussion is limited to a sentence. 
- How is P@10 calculated for spans of length 2+? This should be mentioned.
- Is the [MASK] token applied to the omitted characters in the insertion task?
Line 249: Michelle, not Micheal "
ARR_2022_202_review,"In this paper, the authors propose SWCC to improve event representation learning. The method mainly modifies the objective function, which can be broken down into a modified InfoNCE objective, a prototype-based clustering objective, and a masked language model objective. The experiment includes two intrinsic evaluations, based on event embedding similarity and one extrinsic evaluation, MCNC. The result shows that the proposed method outperforms baselines on all the evaluations. The ablation and visualization studies also identify the importance of each component (objective function). ","1. The experiment results show strong performance of the proposed method. 
2. To the best of my knowledge, this is the first work that leverages prototype-based clustering in event representation learning. ","1. The write-up has many typos and some formulas/explanations are confusing. 
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. 
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. 
4. More strong baselines should be included/discussed in the experiments. ","Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. 
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. 
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? 
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.” 
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? 
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? 
12. Line 450: L_{pc} or L_{cp}| in Eq. 7? 
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”? 
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? 
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",None ,"1. The write-up has many typos and some formulas/explanations are confusing. 
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. 
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. 
4. More strong baselines should be included/discussed in the experiments. 
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. 
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. 
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? 
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.” 
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? 
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? 
12. Line 450: L_{pc} or L_{cp}| in Eq. 7? 
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”? 
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? 
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? "
ARR_2022_118_review,Second-time review and my previous reviewer id is Reviewer XYZ. ,See the previous review. ,See the previous review. ,"Here attach the comments about addressing my previous review.  A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?
A2: ""We believe any functions that first rise then decrease can also obtain similar results"" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.
A3:  Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic.
Overall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"
Here attach the comments about addressing my previous review.  A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?
A2: ""We believe any functions that first rise then decrease can also obtain similar results"" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.
A3:  Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic.
Overall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3. "
ARR_2022_63_review,"In this work, the authors proposed a unified model of task-oriented dialogue understanding and response generation. The two major enhancements are adopting task-oriented dialogue pre-training on a data collection, and introducing the prompt-based learning for the multi-task capability via one model. From the experimental results, the pre-training strategy proved useful to improve the performance on the benchmark MultiWOZ. ","While the idea of task-specific pre-training is not new, it is still interesting, and the proposed method proved effective in leveraging the language backbone T5, and can be potentially applied to other models and tasks. ","1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. 
2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. ","Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? 
2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? 
3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. 
2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. 
Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? 
2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? 
3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? "
ARR_2022_260_review,"The paper looks at distilling models for abstractive summarization. The paper makes the claim that the most obvious way to do knowledge distillation with seq2seq models (just using the generated output from the teacher model as a target output for the student; also called pseudo-labeling), is problematic. The problems with pseudo-labeling relate to known but important to restate problems with current abstractive summarization models in general: they generally copy from the target document, and they generally only copy the leading line of the target document. The paper attributes the causes for both of these problems being at least in part due to the attention distribution for the teacher models being too sharp, often focusing most of its weight on the next available word. In order to counteract these effects, the paper proposes to raise the temperature for the attention soft max in order to smooth the attention distribution of the teacher model. The temperature scaling modifications are evaluated on three standard datasets (CNN/DM, XSum, and NYT), with slight improvements in distillation performance found across all three datasets. ","- The paper is easy to read and follow - A lot of detail has been provided in the paper, making it quite easy for someone to be able to reproduce.  - Evaluations are done on reasonable datasets, against reasonable baselines, and shows promising results. Human evaluations are also provided - Abalations and additional comparisons with obvious baselines (like just using sampling in the teach model for generating pseudo labels) are done. ","- There is unfortunately not a whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered a hyperparameter. While I have no problem with what is currently in the paper, I am just not sure that this is enough to form a long paper proposing a new method. I think if this paper couched itself as more of a 'analysis/experimental' type of paper, expanding its analysis even further (and maybe expanding its scope to other tasks besides just abstractive summarization), it could be solid contribution. Unfortunately, the paper is presented more as a 'methods' paper, with the new method being proposed simply being a change in hyperparameters. ","- What hypothesis test are you using for computing the significance in table 2 and 3?
- Are there other tasks where temperature smoothing could be beneficial? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",5 = They could easily reproduce the results.,None ,"- There is unfortunately not a whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered a hyperparameter. While I have no problem with what is currently in the paper, I am just not sure that this is enough to form a long paper proposing a new method. I think if this paper couched itself as more of a 'analysis/experimental' type of paper, expanding its analysis even further (and maybe expanding its scope to other tasks besides just abstractive summarization), it could be solid contribution. Unfortunately, the paper is presented more as a 'methods' paper, with the new method being proposed simply being a change in hyperparameters. 
- What hypothesis test are you using for computing the significance in table 2 and 3?
- Are there other tasks where temperature smoothing could be beneficial? "
ARR_2022_260_review,"This paper proposes an extension of the pseudo-labeling method named PLATE for summarization distillation, extensive experiments on three datasets suggest the effectiveness of the proposed method versus vanilla sequence distillation. Ablation studies and empirical analysis reveal the performance gain may come from the diverse cross-attention distribution of the teacher model and concise and abstractive summaries generated from the teacher model. ","The paper is well-motivated and clearly presented;  The authors extensively study the effect of Pseudo-labeling with Larger Attention TEmperature on various summarization tasks. Substantial ablation studies and empirical analyses are also provided for revealing the performance gain over baseline methods, which may help future studies understand how to conduct effective summarization distillation. 
The findings of the diverse cross attention pattern, the conciseness, and abstractiveness of produced summarization pseudo-labels as well as the attention focus of the teacher model help future studies develop better summarization models. ","From Table 6, the difference of attention pattern results in a great difference in novel-n-grams ratios and length in the generated teacher summarization, but this does not necessarily translate to a better student summarization system in Table 2, further explanation or better ablation studies might be needed to understand how the length, novel-n-grams ratio, the different dataset will affect the distillation effects.  Given that this paper focuses on sequence distillation, wondering how the beam size, length penalty of the teacher model will affect the sequence distillation results? ","It may be good to know if a better teacher model (BART-large, PEGASUS-large) produces better pseudo-labels for summarization distillation. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"From Table 6, the difference of attention pattern results in a great difference in novel-n-grams ratios and length in the generated teacher summarization, but this does not necessarily translate to a better student summarization system in Table 2, further explanation or better ablation studies might be needed to understand how the length, novel-n-grams ratio, the different dataset will affect the distillation effects.  Given that this paper focuses on sequence distillation, wondering how the beam size, length penalty of the teacher model will affect the sequence distillation results? 
It may be good to know if a better teacher model (BART-large, PEGASUS-large) produces better pseudo-labels for summarization distillation. "
ARR_2022_301_review,"This paper uses extensive empirical experiments to study the impact of ""temporal misalignment"" on pretrained language models, across different tasks, domains, and specific time periods. ",1. Research questions are well defined and have been consistent throughout the paper 2. A large number of experiments are conducted to answer research questions ,"1. Usefulness. we know very well at this point that PTLMs' performances degrade when data distribution shifts; we also know that the magnitude of degradation depends on tasks, domains, or even different models themselves. Time, as an important way to change data distribution, certainly will have that impact as many previous studies show (cited). I think this type of study is valuable, but what could elevate the paper is to propose new methods to mitigate them. The paper tries to slice data and run experiments on top of that. But I think that's likely the initial steps most researchers would do and hardly provide new directions on how to address the problem more effectively. I recommend the authors to look at some earlier works on the drift of meaning in word embedding such as ""Dynamic Embeddings for Language Evolution"" and ""Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop,"" and see if there are possibilities to incorporate or even invent new methods to address the issue. 
2. Novelty. 
    - methods/models. The authors can look into more novel ways to address the issue as opposed to ""re-implementing"" similar studies on more data (which I appreciate, but doesn't quite contribute to novelty). 
    - metrics. F1, ROUGE, perplexity, and uni-gram overlaps are reasonable, but there are more creative and helpful metrics to measure the impact of time. Again, please look at the suggested papers above. 
3. Experimental setup. 
    - it doesn't quite make sense to me to train on future data and test on the past. In practice, we cannot go back in time. I suggest using a typical time-series forecasting set-up, that is, using a sliding window of historical data for training and test on later ones. 
    - try more PTLMs rather than just GPT2. ","Please see my comments above, additionally, 1. Line 070: Section 3.1 and then Section 3.2? 
2. Section 2.4: can move data descriptions to the appendix or shorten them. Doesn't really offer much insight. 
3. Line 190 - 191: define t1 < t2 as I suggested above and simplify the equation. 
4. Line 352: move label drift studies to the main text as they are helpful information. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Usefulness. we know very well at this point that PTLMs' performances degrade when data distribution shifts; we also know that the magnitude of degradation depends on tasks, domains, or even different models themselves. Time, as an important way to change data distribution, certainly will have that impact as many previous studies show (cited). I think this type of study is valuable, but what could elevate the paper is to propose new methods to mitigate them. The paper tries to slice data and run experiments on top of that. But I think that's likely the initial steps most researchers would do and hardly provide new directions on how to address the problem more effectively. I recommend the authors to look at some earlier works on the drift of meaning in word embedding such as ""Dynamic Embeddings for Language Evolution"" and ""Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop,"" and see if there are possibilities to incorporate or even invent new methods to address the issue. 
2. Novelty. 
    - methods/models. The authors can look into more novel ways to address the issue as opposed to ""re-implementing"" similar studies on more data (which I appreciate, but doesn't quite contribute to novelty). 
    - metrics. F1, ROUGE, perplexity, and uni-gram overlaps are reasonable, but there are more creative and helpful metrics to measure the impact of time. Again, please look at the suggested papers above. 
3. Experimental setup. 
    - it doesn't quite make sense to me to train on future data and test on the past. In practice, we cannot go back in time. I suggest using a typical time-series forecasting set-up, that is, using a sliding window of historical data for training and test on later ones. 
    - try more PTLMs rather than just GPT2. 
Please see my comments above, additionally, 1. Line 070: Section 3.1 and then Section 3.2? 
2. Section 2.4: can move data descriptions to the appendix or shorten them. Doesn't really offer much insight. 
3. Line 190 - 191: define t1 < t2 as I suggested above and simplify the equation. 
4. Line 352: move label drift studies to the main text as they are helpful information. "
ARR_2022_301_review,"This paper deals with the temporal misalignment problem, which occurs when an NLP model is trained on a dataset created from data of a certain time period and tested/used for data of another time period. 
This paper discusses the temporal misalignment problem through experiments on a number of NLP tasks with temporal misalignment. For that, TD, a metric for temporal degradation (of the task performance), is defined and used. 
The paper presents some findings about the temporal misalignment problem, such as answers to ""how does sensitivity to temporal misalignment vary with text domain and task?"" 
The paper brings NLP community's attention to the temporal misalignment problem. ","Firstly, the paper is beneficial to the NLP community, since it will bring NLP community's attention to the temporal misalignment problem. This is particularly important because currently people in this community extensively use the pretraining-DAPT-finetuning paradigm. 
The arguments in the paper are supported by comprehensive experiments with a number of tasks. 
The paper is well-written and well-organized. ","The problem caused by difference in training data and test data has been studied, although they might not focus on ""temporal"" aspect. 
There are a number of papers, where terms like ""covariate shift"", labeling adaptation, and instance adaptation (e.g., Jiang and Zhai (2007) and Shimodaira (2000). See below). It would be interesting to see the connection between the arguments of the current paper and such papers.
- Jiang and Zhai. Instance Weighting for Domain Adaptation in NLP. ACL. 2007.
- Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.
TD score is defined in Section 2.3, but there is no intuition or justification about why it is defined like this, at least in the same section. Descriptions about intuition can be found in the later sections, so this is simply a matter of organization of the paper.  It is not clear whether TD scores for different performance metrics can be compared with each other, across different tasks. There should be a discussion on the justification (or maybe limitation).
Some specific examples of temporal misalignment would help readers understand the paper, if they (the examples) are presented in Introduction. ","D in the text body should be in italic, consistently. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The problem caused by difference in training data and test data has been studied, although they might not focus on ""temporal"" aspect. 
There are a number of papers, where terms like ""covariate shift"", labeling adaptation, and instance adaptation (e.g., Jiang and Zhai (2007) and Shimodaira (2000). See below). It would be interesting to see the connection between the arguments of the current paper and such papers.
- Jiang and Zhai. Instance Weighting for Domain Adaptation in NLP. ACL. 2007.
- Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.
TD score is defined in Section 2.3, but there is no intuition or justification about why it is defined like this, at least in the same section. Descriptions about intuition can be found in the later sections, so this is simply a matter of organization of the paper.  It is not clear whether TD scores for different performance metrics can be compared with each other, across different tasks. There should be a discussion on the justification (or maybe limitation).
Some specific examples of temporal misalignment would help readers understand the paper, if they (the examples) are presented in Introduction. 
D in the text body should be in italic, consistently. "
ARR_2022_175_review,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them. It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence. It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information. The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model. The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap. Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma). However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly. ","- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).
- It investigates a potential application of this answer.
- The paper is very clearly written, and easy to follow. ","- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. 
If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. ","- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. 
If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. 
- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? "
ARR_2022_175_review,"This paper addresses the question of whether the spelling of words has been retained / encoded by large language models. First, it introduces a probe to discover the spelling of a word based on the embeddings in the model input. This probe, spelling bee, is essentially a character level language model which is conditioned on the word embedding. They find that a significant amount of information about spelling is retained by the embedding, and that explicitly providing information about spelling during training by using spelling bee does not provide additional advantage to the model. The authors conclude that this indicates language models “can quickly acquire all the character level information they need without directly observing the composition of each token”. ","I liked the fact that the training and test data splits considered the possibility that words in the test set might benefit too much from training  spelling bee on a train set with similarly spelled words.
I think that the question of whether the model has implicitly learned character composition is valuable, and there should be some significant interest in this paper as a result. ","I want to see a baseline that tests spelling bee on representations specifically optimized for morphology or spelling, so that we can see what the performance of this probe would be on complete information about spelling. As is, it's not clear what a true upper-bound performance would be for such a probe.
 I felt that the conclusions drawn from the attempt to train with additional spelling information were not well justified. After all, if training with the additional information actually damaged performance, the authors would not have concluded that the model doesn’t use the information or that the information was somehow harmful or misleading. Instead, they would rightly assume that the particular procedure they were using to add that information was not providing it in a structure that the model could easily use. However, because it doesn’t change  performance at all, the authors conclude that the model is able to acquire everything it needs without direct observation of the spelling. It’s not clear to me that these results contradict the idea that some information about character composition might be able to help a model.
There needs to be more detail on the implementation of spelling bee. ","I felt that this paper could do with more citations to the existing literature on morphological/character composition in neural models (e.g., https://aclanthology.org/P17-1184/) ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"I want to see a baseline that tests spelling bee on representations specifically optimized for morphology or spelling, so that we can see what the performance of this probe would be on complete information about spelling. As is, it's not clear what a true upper-bound performance would be for such a probe.
 I felt that the conclusions drawn from the attempt to train with additional spelling information were not well justified. After all, if training with the additional information actually damaged performance, the authors would not have concluded that the model doesn’t use the information or that the information was somehow harmful or misleading. Instead, they would rightly assume that the particular procedure they were using to add that information was not providing it in a structure that the model could easily use. However, because it doesn’t change  performance at all, the authors conclude that the model is able to acquire everything it needs without direct observation of the spelling. It’s not clear to me that these results contradict the idea that some information about character composition might be able to help a model.
There needs to be more detail on the implementation of spelling bee. 
I felt that this paper could do with more citations to the existing literature on morphological/character composition in neural models (e.g., https://aclanthology.org/P17-1184/) "
ARR_2022_311_review,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT. The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques. ","- The paper obtains good results with a straightforward approach.
- The paper is written fairly clearly. ",- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses. ,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs. Lines 236-239 make me think they're used in addition. This is fine, but should be emphasized more explicitly.
- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not. That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses. 
- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs. Lines 236-239 make me think they're used in addition. This is fine, but should be emphasized more explicitly.
- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not. That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose? "
ARR_2022_311_review,"The paper proposes a data augmentation method using a controllable smoothed representation, which is obtained by combining the one-hot representation and the smooth representation through masked language modeling. The authors showed the effectiveness of the proposed method on low-resourced sentence classification task. They also found that the smooth representation can be used on other data augmentation method to achieve better results. ","- The paper is well-structured. The authors explained the motivation and the methodology clearly. Figure 1 and 2 are informative and help the readers understand better understand the method.
- It is nice that the text smoothing can be combined with other data augmentation approaches to achieve better performances. ","- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector. ","- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector. 
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data? "
ARR_2022_311_review,The paper introduces a text smoothing approach and uses it in different downstream tasks. Different types of sentence classification tasks are improved by using such text smoothing method. ,The paper demonstrate improvement from using proposed text smoothing method. ,"This paper does not make a significant contribution. Smoothed Representation and Mixup Strategy are not proposed by the authors. Moreover, the strategy is too simple to combine the one-hot representation and smoothed representation with a weight parameter lambda.
Three low-resource sentence classification tasks are used in the experiments. Even more lower-resource task classifications are missed, such as MRPC in GLUE.  It lacks the configuration of lambda, I believe this can greatly affect performance.
I am still unsure about the motivation, the high probability of ""average"" is only due to MLM. Nevertheless, the semantic meaning of ""average"" is learned by MLM and a downstream task like sentiment analysis would only care about its semantic meaning. ","One of the biggest problems is that motivation is not convincing. I think it would be better to have examples of different predictions using your text smoothing approach. In Figure 2, how does the ""average"" likely affect the sentiment analysis label?
Though some of the tasks aren't very useful, I believe more classification tasks should be conducted. In my experience of GLUE, SST and MNLI are two of the most resourceful sentence classification tasks compared to other tasks in GLUE. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"This paper does not make a significant contribution. Smoothed Representation and Mixup Strategy are not proposed by the authors. Moreover, the strategy is too simple to combine the one-hot representation and smoothed representation with a weight parameter lambda.
Three low-resource sentence classification tasks are used in the experiments. Even more lower-resource task classifications are missed, such as MRPC in GLUE.  It lacks the configuration of lambda, I believe this can greatly affect performance.
I am still unsure about the motivation, the high probability of ""average"" is only due to MLM. Nevertheless, the semantic meaning of ""average"" is learned by MLM and a downstream task like sentiment analysis would only care about its semantic meaning. 
One of the biggest problems is that motivation is not convincing. I think it would be better to have examples of different predictions using your text smoothing approach. In Figure 2, how does the ""average"" likely affect the sentiment analysis label?
Though some of the tasks aren't very useful, I believe more classification tasks should be conducted. In my experience of GLUE, SST and MNLI are two of the most resourceful sentence classification tasks compared to other tasks in GLUE. "
ARR_2022_311_review,"This paper proposes a data augmentation technique named Text Smoothing, which converts sentences from their one-hot representations to controllable smoothed representations. Specifically, the authors multiply the output of a pre-trained BERT with the word embedding matrix to get the smoothed representation of an input token. Then the smoothed representation is combined with the one-hot representation by mixup to get the mixed representation. Using such mixed representation instead of the one-hot representation as the token input can be regarded as a kind of data augmentation and can significantly improve the model performance in the low-resource regime. ","__1. The paper is well organized and easy to follow.__
__2. The proposed method is novel and interesting:__ The idea of mixing the one-hot representation and the LM (smoothed) representation for an input token is very different from other works of data augmentation. It uses the knowledge of pre-trained BERT to integrate and compress information of multiple related words, providing richer semantics that a single token in the one-hot form cannot provide.  __3. The experimental results are impressive:__ The improvement brought by text smoothing is very significant and surpasses many strong baselines. Besides, the text smoothing can be also well combined with various data augmentation methods, showing great practicability and universality. ","__1. Lack of significance test:__      I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results. 
     __2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)? 
     __4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result? 
     __4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample? ","1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"__1. Lack of significance test:__      I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results. 
     __2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)? 
     __4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result? 
     __4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample? 
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation "
ARR_2022_165_review,"This paper proposes a “plug and play” method for controlled text generation using language models, i.e., it does not require finetuning of the language model used for generation. Their algorithm uses MCTS, where the policy balances sequence likelihood with the score from a discriminator that determines the presence of certain textual attributes. This balance encourages generation to meet certain constraints while maintaining quality. They provide empirical evidence of the effectiveness of the algorithm. ","The paper offers an intuitive extension of MCTS for controlled generation that doesn’t require fine-tuning of a language model. The method has the potential to be widely applicable. The experiments are very comprehensive, exploring multiple baselines, datasets, and parameter settings. ","- The MCTS algorithm itself in the context of text generation is not explained very well, which makes it hard to understand the proposed algorithm - The need for a discriminator for any possible constraint can be quite limiting to the applicability of the algorithm - There lacks a formal runtime analysis. Speed during decoding is often quite important and it's unclear what the extent of the computational drawbacks of this algorithm are - There have been a number of recent works also using MCTS for language generation. I am not very familiar with these works so it is hard for me to tell how novel this paper is in context ","- I don’t quite see where p(x|c) fits into equation 1. Is it just s_i? This could be made more clear - In terms of plug and play controlled generation methods that do not require fine-tuning of an LM, a citation is missing for Pascual et. al. 2021. Schick et. al 2021 also use a discriminator at each time step to control generation for certain attributes - Line 467: the standard definition of perplexity used in language modeling divides by the number of tokens, and so is not dependent on length - Line 228: token -> tokens ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- The MCTS algorithm itself in the context of text generation is not explained very well, which makes it hard to understand the proposed algorithm - The need for a discriminator for any possible constraint can be quite limiting to the applicability of the algorithm - There lacks a formal runtime analysis. Speed during decoding is often quite important and it's unclear what the extent of the computational drawbacks of this algorithm are - There have been a number of recent works also using MCTS for language generation. I am not very familiar with these works so it is hard for me to tell how novel this paper is in context 
- I don’t quite see where p(x|c) fits into equation 1. Is it just s_i? This could be made more clear - In terms of plug and play controlled generation methods that do not require fine-tuning of an LM, a citation is missing for Pascual et. al. 2021. Schick et. al 2021 also use a discriminator at each time step to control generation for certain attributes - Line 467: the standard definition of perplexity used in language modeling divides by the number of tokens, and so is not dependent on length - Line 228: token -> tokens "
ARR_2022_165_review,The authors propose to use Monte Carlo Tree Search to generate sentences that satisfy the constraints. The authors conduct rigorous experiment analysis to show the advantage of the method ,The paper is well written and easy to follow. The figures present the idea clearly. There are various experiments to support the proposed method. ,"What specific problem that previous approaches cannot solve but this paper can resolve? Or in terms of what aspect, discriminator-guided decoding can do better (not just score)? For example, every constraint is modeled as one discriminator, will it mean the model is better modularized? ","- could you give a detailed example of what ""constraint"" is used in this paper? It is not clear how multiple discriminators can enforce constraints simultaneously.
- MCTS is mainly used in RL since the state transition and reward function are probability distributions. It is unclear why you need MCTS to do a search, why not just depth-first search, best-first search, A* search, or any heuristic search? Could you highlight the necessary steps that can achieve discriminator-guided decoding?
- Do you assume you can naturally have every constraint as well as the corresponding discriminator?
- you can put all the HTTPS links as footnotes.
- Line 513 ""Go of RAM"" ->  ""GB of RAM"" ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"What specific problem that previous approaches cannot solve but this paper can resolve? Or in terms of what aspect, discriminator-guided decoding can do better (not just score)? For example, every constraint is modeled as one discriminator, will it mean the model is better modularized? 
- could you give a detailed example of what ""constraint"" is used in this paper? It is not clear how multiple discriminators can enforce constraints simultaneously.
- MCTS is mainly used in RL since the state transition and reward function are probability distributions. It is unclear why you need MCTS to do a search, why not just depth-first search, best-first search, A* search, or any heuristic search? Could you highlight the necessary steps that can achieve discriminator-guided decoding?
- Do you assume you can naturally have every constraint as well as the corresponding discriminator?
- you can put all the HTTPS links as footnotes.
- Line 513 ""Go of RAM"" ->  ""GB of RAM"" "
ARR_2022_7_review,"This paper aims to train a fully unsupervised pretrained retriever for zero-shot text retrieval. Specifi- cally, this paper proposes a novel iterative contrastive learning to pretrain dual-tower dense retriever, then use lexical matching method to further enhance the pretrained dense retrieval model. Results on BEIR benchmark show that the proposed method achieves SOTA performance compared with supervised dense retrieval model. ","1. The proposed unsupervised dense retrieval model achieves remarkable performance com- pared with supervised dense retrieval models on BEIR benchmark. 
2. Theproposedmodelisinitializedfrom6-layerDistilBERTandtheparametersofdualencoders are tied, so the proposed retrieval model is quite efficient. ","1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. 
2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. 
5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.
Reference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 ","the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. 
Line 300-304: q and d are confusing ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,N/A ,"1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. 
2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. 
5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.
Reference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 
the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. 
Line 300-304: q and d are confusing "
ARR_2022_7_review,"A method to train dense retrieval without the need of any supervised data using a combination of ICT and SimCSE. Further, they propose Iterative Contrastive Learning (ICoL) as a new way for caching negative embeddings. ","- The paper is well written and easy to understand.
- The paper conducts many experiments and presents many interesting insights - The results are convincing on the BEIR benchmark - ICoL appears to be an interesting approach showing improvements over MoCo - Overall a strong paper but with some weaknesses in evaluating the individual components / individual changes. ","**What is contributing to the performance improvement?**
- Because the paper presents so many novelties, it is a bit hard to grasp what led to the improvement on BEIR, i.e. what are the main factors that contribute to the improvement?
- It appears (Table 3) the strongest improvement come from Lexicon-Enhanced Dense Retrieval, i.e. combining the dense sim score with a BM25 score. This hybrid approach has been shown effective in several previous works, e.g. https://arxiv.org/abs/2005.00181 or https://arxiv.org/pdf/2004.13969.pdf (and many more) - It would be interesting to get the results for other dense + BM25 combinations. The dense retriever appears to be not the strongest (cf. Table 3, col. w/o LEDR), i.e. it is weaker than TAS-B (0.396 vs 0.415). So what would be the results of TAS-B+LEDR?
**Pre-training approach** - A large contribution of the author is the proposal of a new pre-training approach that combines ICT + SimCSE with a large negative cache (ICoL) - Table 3 shows an improvement for the sole dense retrieval model if pre-training is used (e.g. w/o LEDR vs. w/O LEDR & PT) - However, we know that BERT is undertrained (see RoBERTa paper) and performing more pre-training steps yields an improvemed BERT model - A comparison against other pre-training approaches would have been interesting. What happens when I train e.g. with MLM for the same amount of compute on C4 and then do fine-tuning? What about other pre-training methods for dense retrievers (for an overview see https://arxiv.org/abs/2112.07577)? Is the proposed pre-training method actually better than other pre-training strategies?
I think the 3 additions (pre-training, ICoL, hybrid retrieval with BM25) could be better separated / evaluated more individually. It would help to see what are the contributing factors for the improved performance. So far it appears that switching to a hybrid approach with BM25 made the large difference in performance.
Having a more clear separation in their evaluation would help to assess what is really relevant for future methods. ","- Line 231: DaPI was concurrently also proposed by Liu et al (which they call Mirror-BERT https://arxiv.org/abs/2104.08027, published April 16; SimCSE was published April 18) - Line 430: You say that the batch size for each GPU is 4,096, so 16*4k = 64k in total for your server. But how do you get a batch of 4,096 on a single 32GB GPU? A batch larger than 100 examples results usually in an out-of-memory error when using huggingface transformers with a distilbert model. Did you use some special off-loading?
- Maybe I might missed it in the paper: Did you specify your max sequence length for pre-training / fine-tuning? I.e. do you use 512 word pieces or do you pre-train on shorter paragraphs?  - Table 4: The heading ""w/o ICT (SimCSE 2021c)"" looks confusing, you would think that ICT was proposed by SimCSE 2021c) - Table 4: Adding SimCSE to pre-training appears to bring little effect, performance improves from 43.4 -> 43.8. Could this improvement just be due to randomness? How does the performance change for the fine-tuning setting when pre-training was without SimCSE (i.e. just ICT pre-training followed by supervised MS MARCO training)?  - Table 5: The three settings, I assume these are the hybrid approaches with BM25. How is the performance only for the dense retrievers without BM25? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"**What is contributing to the performance improvement?**
- Because the paper presents so many novelties, it is a bit hard to grasp what led to the improvement on BEIR, i.e. what are the main factors that contribute to the improvement?
- It appears (Table 3) the strongest improvement come from Lexicon-Enhanced Dense Retrieval, i.e. combining the dense sim score with a BM25 score. This hybrid approach has been shown effective in several previous works, e.g. https://arxiv.org/abs/2005.00181 or https://arxiv.org/pdf/2004.13969.pdf (and many more) - It would be interesting to get the results for other dense + BM25 combinations. The dense retriever appears to be not the strongest (cf. Table 3, col. w/o LEDR), i.e. it is weaker than TAS-B (0.396 vs 0.415). So what would be the results of TAS-B+LEDR?
**Pre-training approach** - A large contribution of the author is the proposal of a new pre-training approach that combines ICT + SimCSE with a large negative cache (ICoL) - Table 3 shows an improvement for the sole dense retrieval model if pre-training is used (e.g. w/o LEDR vs. w/O LEDR & PT) - However, we know that BERT is undertrained (see RoBERTa paper) and performing more pre-training steps yields an improvemed BERT model - A comparison against other pre-training approaches would have been interesting. What happens when I train e.g. with MLM for the same amount of compute on C4 and then do fine-tuning? What about other pre-training methods for dense retrievers (for an overview see https://arxiv.org/abs/2112.07577)? Is the proposed pre-training method actually better than other pre-training strategies?
I think the 3 additions (pre-training, ICoL, hybrid retrieval with BM25) could be better separated / evaluated more individually. It would help to see what are the contributing factors for the improved performance. So far it appears that switching to a hybrid approach with BM25 made the large difference in performance.
Having a more clear separation in their evaluation would help to assess what is really relevant for future methods. 
- Line 231: DaPI was concurrently also proposed by Liu et al (which they call Mirror-BERT https://arxiv.org/abs/2104.08027, published April 16; SimCSE was published April 18) - Line 430: You say that the batch size for each GPU is 4,096, so 16*4k = 64k in total for your server. But how do you get a batch of 4,096 on a single 32GB GPU? A batch larger than 100 examples results usually in an out-of-memory error when using huggingface transformers with a distilbert model. Did you use some special off-loading?
- Maybe I might missed it in the paper: Did you specify your max sequence length for pre-training / fine-tuning? I.e. do you use 512 word pieces or do you pre-train on shorter paragraphs?  - Table 4: The heading ""w/o ICT (SimCSE 2021c)"" looks confusing, you would think that ICT was proposed by SimCSE 2021c) - Table 4: Adding SimCSE to pre-training appears to bring little effect, performance improves from 43.4 -> 43.8. Could this improvement just be due to randomness? How does the performance change for the fine-tuning setting when pre-training was without SimCSE (i.e. just ICT pre-training followed by supervised MS MARCO training)?  - Table 5: The three settings, I assume these are the hybrid approaches with BM25. How is the performance only for the dense retrievers without BM25? "
ARR_2022_7_review,"In this paper the authors propose a pre-trained retriever that does not require any supervised date for training, called LaPraDoR. Specifically, they present 3 contributions in their paper :  1) LaPraDoR : an all-around unsupervised pretrained dense retriever, that achieves state-of-the-art performance on the BEIR benchmark. 
2) ICoL (Iterative Contrastive Learning) that iteratively trains the query and document encoders with a cache mechanism to mitigate the insufficient memory on a single GPU and allow more negative instances for better performance. 
3) LEDR (Lexicon-ENhanced Dense Retrieval) that combine BM25 with a dense retriever to consider both lexical and semantic matching. 
The authors evaluate these contributions on the recently proposed BEIR benchmark and show the effectiveness of their system. ","The paper is well written and clear, therefore the contributions presented are easy to understand. 
The authors propose 3 strong contributions that present an interest for the information retrieval community. ","The authors do not give any explanation or definition of the evaluation metric used. 
The results are presented really briefly, without discussion on the differences observed for the 18 english zero-shot evaluation datasets (while it is one of the interest of the BEIR benchmark). ","The authors should spend some time on analyzing the results obtained for the different datasets. 
For example, why the results for LaPraDor (unsupervised and FT) are lower than Late interaction for some datasets (FEVER or CQADupStack) while they are better for others ? Why the results are almost the same for Re-ranking, LaPraDoR unsupervised and FT while they are really differents for FEVER for example ? 
On the contrary, I found it really difficult to draw any conclusion from a case study with two examples. 
In the ""Carbon footprint"" section, the authors mention ""All emitted carbon dioxide has already been offset"". What do that mean exactly? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The authors do not give any explanation or definition of the evaluation metric used. 
The results are presented really briefly, without discussion on the differences observed for the 18 english zero-shot evaluation datasets (while it is one of the interest of the BEIR benchmark). 
The authors should spend some time on analyzing the results obtained for the different datasets. 
For example, why the results for LaPraDor (unsupervised and FT) are lower than Late interaction for some datasets (FEVER or CQADupStack) while they are better for others ? Why the results are almost the same for Re-ranking, LaPraDoR unsupervised and FT while they are really differents for FEVER for example ? 
On the contrary, I found it really difficult to draw any conclusion from a case study with two examples. 
In the ""Carbon footprint"" section, the authors mention ""All emitted carbon dioxide has already been offset"". What do that mean exactly? "
ARR_2022_7_review,"This paper proposes an unsupervised pre-training method for dense retriever. They propose ICoL to conduct effective pre-training borrowing the ideas from contrastive learning and ICT. The experimental results show its effectiveness on BEIR benchmark including 18 datasets. There are some weaknesses in this article, mainly the lack of novelty and the lack of connection between the two main strategies proposed, which weakens the overall contribution.
Overall, I think this paper is not ready for ACL* venue yet and might still be a good pick for workshops. ","1. The topic is the current trend in dense retrieval area which has value to explore. 
2. Good results on zero-shot evaluation with BEIR benchmark. 
3. The promise of open-source gives assurance of reproducibility. ","1. The main contribution of the proposed approach lacks novelty. Since BM25 has strong zero-shot capability, it seems that the combination of the BM25 serves mainly to enhance the zero-shot capability of the model. The proposed ICoL is mainly an integration of some existing methods. 
2. The authors do not illustrate why it is better to keep representations in the same hidden space, it also has not been experimentally verified that they are in the same hidden space. 
3. The link between the ICoL and BM25 weighting is not fully worked out. ","1. In line 300-301 ""{ d i,1−, . . . , d i,n−} is a set of randomly sampled query negatives"", query->document? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The main contribution of the proposed approach lacks novelty. Since BM25 has strong zero-shot capability, it seems that the combination of the BM25 serves mainly to enhance the zero-shot capability of the model. The proposed ICoL is mainly an integration of some existing methods. 
2. The authors do not illustrate why it is better to keep representations in the same hidden space, it also has not been experimentally verified that they are in the same hidden space. 
3. The link between the ICoL and BM25 weighting is not fully worked out. 
1. In line 300-301 ""{ d i,1−, . . . , d i,n−} is a set of randomly sampled query negatives"", query->document? "
ARR_2022_270_review,"This paper focuses on the privacy concerns of large language models specifically the identification of speaker attributes from the hidden states of GPT-2. This work attempts to demonstrate that privacy concerns exist by a simple attacker network that identifies persona attributes and proposes defense objectives that reduce the exploitation of the model's hidden state. With learning objectives that minimize mutual information and reduce over-learning, this work demonstrates that defense objectives satisfactorily defend against the proposed attacker network.
Problem Large language models are ubiquitous in application and particularly useful when constructing chatbots. Figure 1 shows that the learned representations of utterance can manifest sensitive information about the persona. The authors also show that with no access to the model, just the final sequence hidden state, simple attacker architectures (MLP) can infer personas from utterances with 37.59% accuracy over 4332 personas. In order to address this, the authors propose two additional learning objectives with an adversarial attacker network. From the perspective of social chatbots, no existing chatbot dataset for private persona information exists. The authors contribute a derivative dataset from PersonaChat that aligns private attributes to the respective personas.
Their contributions are as follows - Analyzing persona identification and understanding as a privacy risk - Defensive training loss - Experiments that validate the defense and shows no informativity loss i.e downstream capabilities.
Note, the defense is across the utterance embeddings which are inherently richer than output token sequences. A defense on sequence embeddings, i.e contextual embeddings in GPT, should constitute effective defense at the token sequence level.
The authors use a pre-trained version of GPT-2, DialogGPT, as the initial set of model weights and fine-tune DialogGPT with the PersonaChat dataset. The authors fine-tune their model with three combined losses: Language modeling CE loss, KL loss over the persona distribution predicted by the attacker, and a MI adversarial loss that learns to maximize the attackers classification accuracy which in turn forces the model to incur a defense loss. The KL loss seems to flatten the over-learning problem in CE fine-tuning and the MI loss makes the network private by leveraging a MLP to remove statistical co-occurences in the embeddings related to personas. The overall loss is computed as a weighted sum.
Experiments The authors gather a persona aligned utterance dataset from Dialogue NLI and PersonaChat. They train a 2 layer MLP attacker model using the sequence embedding as input. The metrics used to evaluate their model is Bayesian Privacy, Bert-score, Top-k, BLEU, PPL, and Distinct.
Tables Table 2 illustrates that traditional LM has a glaring over-learning issue that captures the personas of speakers. This reaffirms the authors motivations. Best guess and random are anticipated to be quite low. The KL portion of the loss in LM+KL variants show that the accuracy is decreased by the regularization effect of the KL loss which is quantified by the max-ratio score (the ratio of most predicted to all predictions). We see that MI adversarial loss forces the model to change output so that the MLP cannot guess persona attributes (reduces accuracy), but does so by making the optimal performance similar to the best guess in that the max ratio jumps significantly and predominately predicts a few persona attributes (my favorite color is blue). The LM+MI variants are likely sufficiently private from my understanding, but reveals attributes/personas that are not relevant; this is of concern in smaller persona datasets. The authors point this out which is very important i.e can fixing privacy open other threat surfaces for other persona applications. This problem emphasizes why KL loss is particularly useful. The authors show that the best model is one that achieves low accuracy on the person prediction task and achieves, secondly, the lowest max-ratio score.
Table 3 Shows that the scores across a variety of metrics are relatively unchanged for the defensed models to the original LM. We see an increase in PPL indicating that the CE encoding requires more bits but with no change to performance metrics. The PPL increase was explained by the authors as possibly stemming from the noisy nature of the KL loss with respect to the persona predictors task; the attacker model does not want a uniform model. The authors additionally use BERTScore to evaluate the similarity in embedding space, BLEU to check similarity of unigram and bigrams, and Dist to check the distinct unigram and bigrams.  Table 4 shows that the performance across unseen persona labels. This is an important evaluation because the training defense mechanism is significantly more useful if it does not require all personas present for training. We see that performance is pretty similar even when the defender is not allowed to see some of the personas. The subset of personas that are unseen are still significantly below the original language model. From my intuition, this means that the defense objective robustly defends against the attacker model regardless of persona. This invites a conversation about increasingly capable attackers.
Finally the authors conclude with a qualitative probe of persona predictions and visually demonstrate the output we see in high max-ratio scores; the repetition of the a particular persona/attribute ""my favorite color is blue"". This is intriguing because it shows that the attacking model has focused on some features that lead to an (sub)-optimal result, namely, ""my favorite color is blue"" persona/attribute. This result shows that the attacking model is no longer capable of probing the embeddings; however a more complex model might still be able to represent the GPT-2 parameterized defensed embeddings.
In summary it appears that the defense strategies work with little affect to the evaluated metrics. Various downstream applications of the defensed embeddings with compatible performance to traditional LM embeddings would concretely prove that there is little utility degradation, thus expanding on the evaluated metrics; this is largely out of the scope of the paper but would solidify its results. Additionally, expanding the attacker model to more complex neural networks is a worthwhile practical application for the proposed loss functions. ",- Privacy oriented dataset with personas - Interesting application of KL loss and MI adversarial losses - Experiments that demonstrate success of loss objectives The paper demonstrates a useful methodology for defending against NN-based persona attacks. The empirical results demonstrates that the proposed approach works for that particular neural network and specifically was not dependent on personas existing in training data. ,"This paper does not have any glaring concerns in my opinion.
Weaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?
- The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. ","Missing reference for ""First there is no existing data that can be used to quantify how much private information is revealed by a LM"". Maybe rephrase as, ""no standalone dataset to evaluate privacy of training data in a LM""? 
Carlini, Nicholas, et al. ""Extracting training data from large language models."" 30th USENIX Security Symposium (USENIX Security 21). 2021.
Explain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words.
Is this an error? 
""Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7."" 
Should this be ""the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3.""?
KL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",This paper does not have any ethical concerns but on the contrary addresses unethical privacy considerations of large language models. ,"This paper does not have any glaring concerns in my opinion.
Weaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?
- The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. 
Missing reference for ""First there is no existing data that can be used to quantify how much private information is revealed by a LM"". Maybe rephrase as, ""no standalone dataset to evaluate privacy of training data in a LM""? 
Carlini, Nicholas, et al. ""Extracting training data from large language models."" 30th USENIX Security Symposium (USENIX Security 21). 2021.
Explain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words.
Is this an error? 
""Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7."" 
Should this be ""the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3.""?
KL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N. "
ARR_2022_270_review,"This paper focuses on language-model based chatbots and how to reveal the owners, or personas, of each utterance and effectively defend against attacks that do this. It claims that for 35.9% of the cases in the experiment it is able to properly defend per a specific language model and Kullback-Leibler divergence. It does this by using preventing GPT-2 overlearning and in its results use metrics that are more common to the NLP industry. ","The paper is well written and time is taken to explain all of the topics well. Also, the foundation of attacks and then their defenses is formulated using mathematical conclusions that are sound. Details about each attack tried and the metrics that are applied are statistically proven and show that the authors have spent a lot of time in preparing it. ","The paper makes a few claims about accuracy and best guesses which are understandably not so common. One of the main metrics it leans on in its abstract is the achievement of 37.59% accuracy for defense. While this is done in a scientific way, the authors do not consider other avenues for testing that make more sense such as random experiments based on other work. ","034 - ""Unfortunately, large 035 language models tend to memorize training data"", cite or explain more... 042 - ""LM-based..."", how do you know this?
045 - This may be better attached to the previous sentence.
045 - Figure 1 does not seem to show that the attacks were successfully able to pick the persona consistently. It shows 5 out of 14, but the truth is not given in the figure.
083 - this paragraph is really good, the explanation helps understand what is done later in the experiments 089 - ""KL loss"", please cite == Problem Formulation = 158 - ""Casual"", describe what casual means with language instead of math please.  160 - The LM shown is the negative log of the probability, while that could be a typical training phase, how do you assume it is the ""casual"" one?
173 - ""The goal"", why did you choose this goal, is there evidence that by correctly choosing a persona you are able to get more personal information?
196 - ""Machine Learning as a Service"" --> machine learning as a service (MLaaS) 196 - ""can be directly.."", are you sure? Please cite the evidence where this has happened.
== Defense Learning Strategies == 204 - ""simple LM training"", what is this, cite?
206 -LM = LMs?
207 - ""to avoid..."", sentence no longer needed 213 - 216 - The statement makes sense but do you have something to show that?
220 - ""intuition..."", this is good == KL Loss == - The KL divergence is introduced here but mentioned before. Also, please cite.
== MI Loss == 281 - 299 The paper takes on the idea of a game gotten from other work. This is okay but it probably should have been mentioned as the main premise for the work instead of a finding.
== Experiment == --> Experiments?
319 - experimental setting --> experimental settings == Experimental Setting == --> Experimental Settings?
- Table 1 --> Good 363 - PPL, this is not defined anywhere.
371 - Table 2, from random 0 to 37.59 is a stretch. Also, the fact that there is ""no"" knowledge is probably not a fair comparison. Not sure whether to leave it or not but ""best guesses"" makes more sense.
384 - 52x --> 52 times? This number does not seem to be a valid measurement. The method of collection and distribution assumptions are questionable.
== Ablation Study == - Good comparisons ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The paper makes a few claims about accuracy and best guesses which are understandably not so common. One of the main metrics it leans on in its abstract is the achievement of 37.59% accuracy for defense. While this is done in a scientific way, the authors do not consider other avenues for testing that make more sense such as random experiments based on other work. 
034 - ""Unfortunately, large 035 language models tend to memorize training data"", cite or explain more... 042 - ""LM-based..."", how do you know this?
045 - This may be better attached to the previous sentence.
045 - Figure 1 does not seem to show that the attacks were successfully able to pick the persona consistently. It shows 5 out of 14, but the truth is not given in the figure.
083 - this paragraph is really good, the explanation helps understand what is done later in the experiments 089 - ""KL loss"", please cite == Problem Formulation = 158 - ""Casual"", describe what casual means with language instead of math please.  160 - The LM shown is the negative log of the probability, while that could be a typical training phase, how do you assume it is the ""casual"" one?
173 - ""The goal"", why did you choose this goal, is there evidence that by correctly choosing a persona you are able to get more personal information?
196 - ""Machine Learning as a Service"" --> machine learning as a service (MLaaS) 196 - ""can be directly.."", are you sure? Please cite the evidence where this has happened.
== Defense Learning Strategies == 204 - ""simple LM training"", what is this, cite?
206 -LM = LMs?
207 - ""to avoid..."", sentence no longer needed 213 - 216 - The statement makes sense but do you have something to show that?
220 - ""intuition..."", this is good == KL Loss == - The KL divergence is introduced here but mentioned before. Also, please cite.
== MI Loss == 281 - 299 The paper takes on the idea of a game gotten from other work. This is okay but it probably should have been mentioned as the main premise for the work instead of a finding.
== Experiment == --> Experiments?
319 - experimental setting --> experimental settings == Experimental Setting == --> Experimental Settings?
- Table 1 --> Good 363 - PPL, this is not defined anywhere.
371 - Table 2, from random 0 to 37.59 is a stretch. Also, the fact that there is ""no"" knowledge is probably not a fair comparison. Not sure whether to leave it or not but ""best guesses"" makes more sense.
384 - 52x --> 52 times? This number does not seem to be a valid measurement. The method of collection and distribution assumptions are questionable.
== Ablation Study == - Good comparisons "
ARR_2022_270_review,"This paper tackles the privacy concerns of modern social chatbots by investigating the privacy leakage of language model-based chatbots. The authors further show that the speakers' personas can be inferred with a simple neural network, finally proposing a practical defense objective to protect 'persona leakage' from hidden states.
In detail, the authors formulate the attacking causal language models as an adversary which owns an external annotated dialog dataset inferring speakers' personas from utterances' embeddings. The persona inference attack is viewed as a supervised classification task, and the defense learning strategies to prevent it include loss functions such as Kullback-Leibler divergence and mutual information. Loss functions utilized in the training phase are aggregated with weight factors to make up the overall loss.
In experiment GPT2-based DialoGPT pretrained upon Reddit comment chains, PersonaChat is used in fine-tuning, about 10K dialogs and 98K personas. The authors state evaluation metrics for both privacy and perplexity, showing that the proposed training strategy can fool the attacker while preserving the naturalness of the answer. ","- This paper attacks an important and up-to-date issue of privacy leakage and suggests a new objective for defending network, using publicly available dialog-based PLM and PersonaChat dataset.
- The paper adequately formulates the problem, which is a bit obscure if stated in natural language, and suggests the evaluation scheme to check the effect of the proposed training objective. ",- The structure of the whole paper is not grasped at a glance due to the presentation being less clear. ,"Suggestions - This reviewer thinks that the indicator for adversary should be 'it' rather than he (line 374, 448).
- In the usage of cases 'LM+KL+MI', 'LM+MI', and 'LM+KL', LM is commonly placed in, which means the authors can abbreviate it by using only KL and MI. It would make the whole passage more readable.
- In Section 4, it would be better if some paragraphs are split, and the meaning of results would be better presented if organized into a single table with the interpretation.
Typos - Please correct citation format to inline, such as in line 262.
- There are many usages of 'casual' language modeling. Think this is a typo. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",No ethical concerns so far. ,"- The structure of the whole paper is not grasped at a glance due to the presentation being less clear. 
Suggestions - This reviewer thinks that the indicator for adversary should be 'it' rather than he (line 374, 448).
- In the usage of cases 'LM+KL+MI', 'LM+MI', and 'LM+KL', LM is commonly placed in, which means the authors can abbreviate it by using only KL and MI. It would make the whole passage more readable.
- In Section 4, it would be better if some paragraphs are split, and the meaning of results would be better presented if organized into a single table with the interpretation.
Typos - Please correct citation format to inline, such as in line 262.
- There are many usages of 'casual' language modeling. Think this is a typo. "
ARR_2022_8_review,"In this paper authors provided a statistical definition of phonemes, proposed a neural network based approach (information quantizer (IQ)) to improve semantic-driven phoneme discovery (SPD), and showed its theoretical guarantee. ","This paper proposed a new model architecture to improve SPD, with its theoretical guarantee. Overall, this paper is well structured and written, with carefully designed experiments and analysis to show its effectiveness. ",I think a few more details are needed to clarify experimental design/results (see details in the section below). ,"1. What's the size/computation of proposed IQ, compared to baselines? Does model size/computation play a role in the performance difference shown in Tables 1, 3, 5? 
2. In Table (5b), there is a big improvement of PER for IQ. How strong are baselines for this case? 
3. There are a couple of cases that it's written as ""Figure"", but meant for ""Table"", e.g. ""Figure 3"", ""Figure 5"". ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I think a few more details are needed to clarify experimental design/results (see details in the section below). 
1. What's the size/computation of proposed IQ, compared to baselines? Does model size/computation play a role in the performance difference shown in Tables 1, 3, 5? 
2. In Table (5b), there is a big improvement of PER for IQ. How strong are baselines for this case? 
3. There are a couple of cases that it's written as ""Figure"", but meant for ""Table"", e.g. ""Figure 3"", ""Figure 5"". "
ARR_2022_8_review,"This paper focuses on the task of phoneme discovery. It proposes a statistical definition of phonemes (through their conditional distributions over word labels) and introduces a novel neural network, information quantizer, which makes use of this statistics to create a phoneme inventory from a set of words. The proposed model is reported to outperform the existing state-of-the-art speech recognition models. ","The paper is well-written, the approach (specifically, defining phonemes as distributions over word labels) is rather elegant, the numerical results seem to be rather strong. ","The amount of material (in particular, mathematical proofs) presented goes beoynd an 8-page *ACL paper, in my opinion.
The method is defined as ""semanti-driven"", but I think this is very misleading: the approach doesn't directly deal with semantics, but rather with word labels. If the approach was truly semantic, the model wouldn't be able to distinguish between complete synonyms.
I do not have enough expertise to evaluate the novelty of the actual model presented. ","- Line 100: why is this a class of neural networks, and not a neural network?
- The conclusion is extremely short, the paper could benefit from some discussion.
- Line 476: typo, compare our models _to_ - Line 510: Figure 8 is mentioned, but the text doesn't say this is actually in the Appendix.
- Table 3 is labelled as Figure 3. As a result, Figure 4 should be Figure 3, etc.
- Line 431: I couldn't understand which n-grams were excluded: unigrams, bigrams, but also bigrams+trigrams? What exactly is the latter type? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The amount of material (in particular, mathematical proofs) presented goes beoynd an 8-page *ACL paper, in my opinion.
The method is defined as ""semanti-driven"", but I think this is very misleading: the approach doesn't directly deal with semantics, but rather with word labels. If the approach was truly semantic, the model wouldn't be able to distinguish between complete synonyms.
I do not have enough expertise to evaluate the novelty of the actual model presented. 
- Line 100: why is this a class of neural networks, and not a neural network?
- The conclusion is extremely short, the paper could benefit from some discussion.
- Line 476: typo, compare our models _to_ - Line 510: Figure 8 is mentioned, but the text doesn't say this is actually in the Appendix.
- Table 3 is labelled as Figure 3. As a result, Figure 4 should be Figure 3, etc.
- Line 431: I couldn't understand which n-grams were excluded: unigrams, bigrams, but also bigrams+trigrams? What exactly is the latter type? "
ARR_2022_8_review,"The paper provides a statistical definition of phonemes that is almost equivalent to the linguistic definition of phonemes and proposes a NN-based method to learn a phoneme model based on the definition. More precisely speaking, the paper defines a phoneme set as a minimum set of speech segment grouping such that a replacement of phonemes results in a change in the distributions of semantic words. The mapping from speech segments to word distributions is realized by a neural network and quantization. ",Significant advantages of the proposed method over several existing methods are shown in some experimental results. The theoretical validity of the proposed method is argued in detail with a lot of detailed information in the appendices. ,Please take a look at some questions on the equations and confusing phonemes below. ,"\mathbb{Q}_K is defined at L894 as a class of functions, while Q_K is the K-th code distribution in the codebook. \mathbb{Q}_K^NN is defined at L899. \mathbb{Q}_K^NN must be a subclass of \mathbb{Q}_K, because the definition of \mathbb{Q}_K does not have any requirement on the functions q. (Question 1) Should \hat{\theta} and \hat{q} of the left hand of Eq. (14) at L14 be \hat{\theta}_n and \hat{q}_n? Should L905 be corrected accordingly?
(Question 2) In L908, Q_K and Q_K^NN are mentioned. But, I suppose they should be \mathbb{Q}_K and \mathbb{Q}_K^NN instead. Should L910 be corrected accordingly?
(Question 3) What is q_k in L911? I suppose it should be Q_k instead. But, even in that case, I could not follow the transformation from L910 to L911. Please clarify the transformation.
I also have questions about confusing phonemes.  (Question 4)  What is the precise definition of “error probability” of (10a)? While the confusion between nasals /n/ and /m/ is mentioned in the main part of the paper, there seem other pairs with even higher error probabilities. Among them, the pair of /ch/ and /ah/ looks very different from each other. Why do they have such high error probabilities? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",Maybe,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Please take a look at some questions on the equations and confusing phonemes below. 
\mathbb{Q}_K is defined at L894 as a class of functions, while Q_K is the K-th code distribution in the codebook. \mathbb{Q}_K^NN is defined at L899. \mathbb{Q}_K^NN must be a subclass of \mathbb{Q}_K, because the definition of \mathbb{Q}_K does not have any requirement on the functions q. (Question 1) Should \hat{\theta} and \hat{q} of the left hand of Eq. (14) at L14 be \hat{\theta}_n and \hat{q}_n? Should L905 be corrected accordingly?
(Question 2) In L908, Q_K and Q_K^NN are mentioned. But, I suppose they should be \mathbb{Q}_K and \mathbb{Q}_K^NN instead. Should L910 be corrected accordingly?
(Question 3) What is q_k in L911? I suppose it should be Q_k instead. But, even in that case, I could not follow the transformation from L910 to L911. Please clarify the transformation.
I also have questions about confusing phonemes.  (Question 4)  What is the precise definition of “error probability” of (10a)? While the confusion between nasals /n/ and /m/ is mentioned in the main part of the paper, there seem other pairs with even higher error probabilities. Among them, the pair of /ch/ and /ah/ looks very different from each other. Why do they have such high error probabilities? "
ARR_2022_8_review,"This paper presents a self-supervised framework for phoneme discovery for zero resource speech recognition. The authors present a rigorous, well thought-out algorithm. They present results on various datasets with both ground truth segmentation and a segmenter model, presenting the efficacy of their approach. ","1) I enjoyed the formalism in the paper. It helps understand the reasoning behind their model. 
2) The improvements over the previous approaches are considerable with interesting analysis.  Overall good paper, I hope the authors can factor in my suggestions in their future version of the paper. ","1) If I understand correctly there is a need to know the word and phoneme segment boundaries for this task. This is a pretty strong assumption and can be unreliable for many languages. The experimentation done by the authors use both ground truth and provided segmentation which I think is good to show that the technique works even with a segmental model. But the authors should rephrase term ""mild assumption"". 
2) Details about the model training and dataset is missing. Which will make this work accessible to a smaller set of research community. It would be great if the authors can provide code or additional details about the model. ","1) Regarding the related works -- ""there is a long line of work that use supervised, multilingual systems"" -- it would be good to acknowledge some of the older works too. 
2) Following up on that, there are works that recognize articulatory features, or directly predict phones -- mentioning some of those works would also be useful. 
3) For the results in 5b, it would be good to add some models from the above work for comparison. As different communities would be interested in different aspects of this paper. 
4) There is a recent work on unsupervised speech recognition at Neurips 2021 (https://arxiv.org/pdf/2105.11084.pdf) which does something similar but without the need for segmental acoustic models. It would be good to make a contrast or have a discussion about that for the readers to have a better understanding. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"1) If I understand correctly there is a need to know the word and phoneme segment boundaries for this task. This is a pretty strong assumption and can be unreliable for many languages. The experimentation done by the authors use both ground truth and provided segmentation which I think is good to show that the technique works even with a segmental model. But the authors should rephrase term ""mild assumption"". 
2) Details about the model training and dataset is missing. Which will make this work accessible to a smaller set of research community. It would be great if the authors can provide code or additional details about the model. 
1) Regarding the related works -- ""there is a long line of work that use supervised, multilingual systems"" -- it would be good to acknowledge some of the older works too. 
2) Following up on that, there are works that recognize articulatory features, or directly predict phones -- mentioning some of those works would also be useful. 
3) For the results in 5b, it would be good to add some models from the above work for comparison. As different communities would be interested in different aspects of this paper. 
4) There is a recent work on unsupervised speech recognition at Neurips 2021 (https://arxiv.org/pdf/2105.11084.pdf) which does something similar but without the need for segmental acoustic models. It would be good to make a contrast or have a discussion about that for the readers to have a better understanding. "
ARR_2022_11_review,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process. Their approach focuses on a context in which there is no need to explicitly identify a region corresponding to the answer for a generated question (so the task is “answer-unaware”). ","The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful. It also contains well organized related literature. Overall, the paper is well written, and easy to understand for a large audience, with good use of examples. ","The methodology section could be strengthened by further justification of some of the design decisions; for example, what models were considered in addition to T5 (line 148-152).  Were there other tasks that were considered beyond the three examined (line 153-154).
The evaluation section could benefit from further motivation for the three experiments shown; were other considered? Were there other datasets that were considered? ","How did you determine what questions you were going to ask the annotators? How is it related to what has been done in related research?
What was the process for identifying annotators? Why only three annotators?  Glad to see that the ""full annotation data"" will be available, and I'm assuming that any other required data will also be available to other researchers as well. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",No ethical concerns. ,"The methodology section could be strengthened by further justification of some of the design decisions; for example, what models were considered in addition to T5 (line 148-152).  Were there other tasks that were considered beyond the three examined (line 153-154).
The evaluation section could benefit from further motivation for the three experiments shown; were other considered? Were there other datasets that were considered? 
How did you determine what questions you were going to ask the annotators? How is it related to what has been done in related research?
What was the process for identifying annotators? Why only three annotators?  Glad to see that the ""full annotation data"" will be available, and I'm assuming that any other required data will also be available to other researchers as well. "
ARR_2022_108_review,"This paper studies whether and how contextual modeling in DocNMT is transferable via multilingual modeling. The authors explore the effect of 3 factors on the transfer with simple concatenation-based DocNMT: the number of teacher languages with document level data, the balance between document and sentence level data at training, and the data type of document level data. The experiments are conducted on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT, particularly on document specific metrics. ","A simple approach uses different proportions of sentence-level and document-level data to train and study its impact on translation performance. In addition, a large number of experiments were done to analyze the causes and human evaluation of its generation. The motivation of this paper is worthy of recognition. ","1. First of all, compared with other excellent papers, this paper is slightly less innovative. 
2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. 
3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. 
4. It lacks case study to show which document-level translation errors are improved by the proposed method. ","1. It is suggested to add a structure diagram to illustrate your proposed method. 
2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. 
3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. First of all, compared with other excellent papers, this paper is slightly less innovative. 
2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. 
3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. 
4. It lacks case study to show which document-level translation errors are improved by the proposed method. 
1. It is suggested to add a structure diagram to illustrate your proposed method. 
2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. 
3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. "
ARR_2022_108_review,"The paper addresses the problem of document-aligned data scarcity by attempting to transfer contextual knowledge from language pairs with more document-aligned data to ones with less or even without any via training multilingual MT models using the concatenation approach. The reported results show that while the sentence-level metric BLEU decreases in many cases when compared to the baseline, document-level specific metrics reflect improvement, which is also backed up by human evaluation. The authors experiment with varying the number of teacher and student languages (from which to which transfer is made), adjusting the data schedule (amount of document-level data used for training), and also using back-translated documents for additional training data. They find that the best results are achieved with several teacher models and back-translated documents help just as much as genuine documents. ","The paper explores an area not yet much studied in the field of MT - using multilingual models for improving document-level MT, and presents interesting, promising results. Models are trained on a very diverse set of languages using well known data sets and easy to follow training workflows, making the whole process seemingly fully reproducible. Results are evaluated by the classic automatic MT evaluation metric BLEU, as well as document-level MT specific challenge test sets, and even support the automatic metrics by performing human evaluation. ",Nothing particularly wrong here. ,"- The authors mention using `the latest WMT evaluation sets`, which currently would be from WMT2021, but if/when the paper is published the `latest` would already be different, so it would be good to specify the exact year.
- The paper mentions using both BPE and Sentencepiece - are they used on top of one another or different for each training data set? If so, then why? This part is unclear... - Is the motivation for using 64K token vocabulary for Europarl the larger size of the training data? The IWSLT data set has a far more diverse variety of languages/writing systems/characters, so it would seemingly make more sense use a larger vocabulary for that instead. ",4.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"
- The authors mention using `the latest WMT evaluation sets`, which currently would be from WMT2021, but if/when the paper is published the `latest` would already be different, so it would be good to specify the exact year.
- The paper mentions using both BPE and Sentencepiece - are they used on top of one another or different for each training data set? If so, then why? This part is unclear... - Is the motivation for using 64K token vocabulary for Europarl the larger size of the training data? The IWSLT data set has a far more diverse variety of languages/writing systems/characters, so it would seemingly make more sense use a larger vocabulary for that instead. "
ARR_2022_108_review,This paper studies and analyses a novel document-level NMT framework using multilingual transfer learning to improve the document-level translation quality for some student languages without parallel document corpus by applying the teacher models trained on parallel document corpus. ,The research question about zero-shot generalization for document-level NMT is clearly stated. And the number of experiments is impressive. And the results are conniving by applying automatic and human evaluations. ,This key motivation of this work is just applying the idea of zero-shot multilingual sentence-level NMT to the multilingual document-level NMT task. It appears that innovation of this paper is less. ,1. The reference of a related work (https://aclanthology.org/2020.wmt-1.53) is missing in this paper. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"This key motivation of this work is just applying the idea of zero-shot multilingual sentence-level NMT to the multilingual document-level NMT task. It appears that innovation of this paper is less. 
1. The reference of a related work (https://aclanthology.org/2020.wmt-1.53) is missing in this paper. "
ARR_2022_107_review,"This paper proposes a latent-variable model based on a variational probabilistic modeling framework for conditional compute in multi-domain and multilingual machine translation. They introduce latent variables which are learned during model training. These latent variables decide which sub-network of the model is selected for each task / language pair. 
The results on multi-domain machine translation show promise compared to vanilla Transformer and Adapter baselines. But most of the improvements can be captured by a heuristic multi-task group dropout baseline, and there is a small improvement (0.5 bleu) over this baseline with the variational approach proposed. 
The paper includes some interesting analysis of the models as well. On the other hand, some one important ablations on whether group dropout helps vs a baseline with group size=1 is missing and the improvements on multilingual translation are not significant enough. Overall, this leaves me unsure whether the added complexity of this approach is worth it over the simpler baselines (vanilla Transformer or heuristic multi-task group dropout). ","1. This work proposes a technique to learn which subnetworks to use for which task / language pair based on learning from the data. 
2. The proposed probabilistic framework is mathematically sound and explained clearly and completely. 
3. There are experiments compared with decent baselines and multiple benchmarks and there has been a good attempt to evaluate the new technique well. 
4. The analysis showing that accuracy of Adapter baselines declines on a domain when that domain is divided into two pseudo-domains, where as the accuracy on this proposed framework remains the same motivates the reason of using the proposed technique very well 5. A nice advantage of this approach is that it doesn’t need additional parameters to the added to the baseline Transformer model. There are some additional hyper-parameters to tune correctly per setup though. 
6. There is also analysis showing whether the learned subnetworks for related domains/languages overlap significantly. 
7. [ Updated] A nice baseline - heuristic multi-task group dropout has been added in the revised version of the paper that helps disentangle the impact of the variational approach. ","1. While the authors mention leaving this to future work, the study of how many groups to divide each layer into is missing and how sensitive is model performance to the choice of number of groups is missing → how does the proposed group dropout compare to just a regular dropout with group size = 1; I think this is important because the paper proposes “Latent Group Dropout” and whether group dropout is needed or not is a key question that is not experimentally verified. 
2. “ heuristic multi-task group dropout” (HMGD) captures most of the improvements between Transformer vs proposed LaMGD with a 0.5 BLEU difference between HMGD and LaMGD - It is worth making a case that the simpler approach HMGD is also a win 3. It is unclear if there was an attempt to tune the sizes and dropout for the adapter modules in the adapters baseline 4. Table 5 - very few pairs (7 out of 32) show significant gains over the Transformer baseline - it is actually unclear how much language specific model capacity helps on this TED benchmark at all. Even the Adapters baseline is very similar to the Transformer baseline. ","1. Regarding “Finally, we set the weight of the entropy term to 0.0001 in the training loss in every experiments.” - ideally, you should refer back to the exact term in the exact equation described in the previous sections 2. I don’t understand what “0^nd” means in section 3.1.4  3. Boldface in Table 3 doesn’t always refer to the best BLEU per column ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. While the authors mention leaving this to future work, the study of how many groups to divide each layer into is missing and how sensitive is model performance to the choice of number of groups is missing → how does the proposed group dropout compare to just a regular dropout with group size = 1; I think this is important because the paper proposes “Latent Group Dropout” and whether group dropout is needed or not is a key question that is not experimentally verified. 
2. “ heuristic multi-task group dropout” (HMGD) captures most of the improvements between Transformer vs proposed LaMGD with a 0.5 BLEU difference between HMGD and LaMGD - It is worth making a case that the simpler approach HMGD is also a win 3. It is unclear if there was an attempt to tune the sizes and dropout for the adapter modules in the adapters baseline 4. Table 5 - very few pairs (7 out of 32) show significant gains over the Transformer baseline - it is actually unclear how much language specific model capacity helps on this TED benchmark at all. Even the Adapters baseline is very similar to the Transformer baseline. 
1. Regarding “Finally, we set the weight of the entropy term to 0.0001 in the training loss in every experiments.” - ideally, you should refer back to the exact term in the exact equation described in the previous sections 2. I don’t understand what “0^nd” means in section 3.1.4  3. Boldface in Table 3 doesn’t always refer to the best BLEU per column "
ARR_2022_107_review,"This paper proposes a routing strategy for multitask machine translation. For a given language or domain, only a fixed amount of nodes is activated at each layer and the model learns which nodes should be activated. They experiment on both multilingual and multidomain settings, and the experimental results demonstrate that they can achieve comparable and sometimes marginally better performance than adapter-based methods with fewer parameters. ","1. The general idea is intuitive and makes sense, and the paper proposes a technically sound way of implementing it. 
2. The empirical results are good as they can achieve comparable performance with an adapter-based method with fewer parameters. 
3. The paper is well-organized. ","Previous comments:  1. There is little analysis of their methods. Sensitivity analysis of their hyper-parameters ($n_d$, $k$, $\tau$), some qualitative analysis are required. 
2. Requiring partitioning nodes into groups and a fixed number of nodes to be activated at each layer makes their method less flexible. 
3. More baselines should be included (e.g. Li et al., (2020), Gong et al., (2021a, b) as mentioned in the paper).
After reading the response: The added sensitivity analysis - Thanks for adding the sensitivity analysis on $k$! However, more analyses are still required as in my previous comment (e.g. the effect of $n_d$, the use of the annealing strategy for $\tau$, analysis of the translation outputs, ...).
The flexibility of their approach - The method requires a fixed number of nodes to be activated at each layer for all the tasks. This is inflexible because intuitively, harder tasks require more nodes to process while simpler tasks may only need a few nodes. However, their method cannot take this into account, and thus the method is not very flexible.
The added baseline - It'd be better to compare with published baselines (e.g. [1, 2]). The baselines share similar claims as the author(s) mentioned in the rebuttal. I agree that this paper doesn't need to obtain state-of-the-art performance, however, the comparisons are necessary so that other researchers can know which methods to use in practice. Therefore, comparisons are required even if the results are not favorable to their method.
[1] Lin et al., Learning Language-Specific Sub-network for Multilingual Machine Translation, ACL 2021. 
[2] Li et al., Deep transformers with latent depth, NeurIPS 2020. ","The added significance tests - The added significance tests are questionable. First, many of the results (e.g. most of the results in Table 5) are not significantly better than the Transformer baseline. Second, it is better to compare with the best baseline model (e.g. the HMGD Transformer in Table 3) rather than the vanilla Transformer baseline. Third, in Table 3 the author(s) claim that the average BLEU score is significantly better than the baseline using compare-mt, but more details are required as compare-mt does not support comparing average numbers. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Previous comments:  1. There is little analysis of their methods. Sensitivity analysis of their hyper-parameters ($n_d$, $k$, $\tau$), some qualitative analysis are required. 
2. Requiring partitioning nodes into groups and a fixed number of nodes to be activated at each layer makes their method less flexible. 
3. More baselines should be included (e.g. Li et al., (2020), Gong et al., (2021a, b) as mentioned in the paper).
After reading the response: The added sensitivity analysis - Thanks for adding the sensitivity analysis on $k$! However, more analyses are still required as in my previous comment (e.g. the effect of $n_d$, the use of the annealing strategy for $\tau$, analysis of the translation outputs, ...).
The flexibility of their approach - The method requires a fixed number of nodes to be activated at each layer for all the tasks. This is inflexible because intuitively, harder tasks require more nodes to process while simpler tasks may only need a few nodes. However, their method cannot take this into account, and thus the method is not very flexible.
The added baseline - It'd be better to compare with published baselines (e.g. [1, 2]). The baselines share similar claims as the author(s) mentioned in the rebuttal. I agree that this paper doesn't need to obtain state-of-the-art performance, however, the comparisons are necessary so that other researchers can know which methods to use in practice. Therefore, comparisons are required even if the results are not favorable to their method.
[1] Lin et al., Learning Language-Specific Sub-network for Multilingual Machine Translation, ACL 2021. 
[2] Li et al., Deep transformers with latent depth, NeurIPS 2020. 
The added significance tests - The added significance tests are questionable. First, many of the results (e.g. most of the results in Table 5) are not significantly better than the Transformer baseline. Second, it is better to compare with the best baseline model (e.g. the HMGD Transformer in Table 3) rather than the vanilla Transformer baseline. Third, in Table 3 the author(s) claim that the average BLEU score is significantly better than the baseline using compare-mt, but more details are required as compare-mt does not support comparing average numbers. "
ARR_2022_73_review,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.  It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets. ",The paper is well written and clear. The work is well motivated and enough related work is presented.  The error analysis is clear and helpful. This is a good short paper with a negative result. ,The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words. I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be). ,"- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.
- the discussion of Arabic use of diacritization is not accurate. Arabic ""dots"" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are.  Check out  https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf - I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger).  Any thoughts on that? ","2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,None ,"The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words. I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be). 
- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.
- the discussion of Arabic use of diacritization is not accurate. Arabic ""dots"" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are.  Check out  https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf - I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger).  Any thoughts on that? "
ARR_2022_31_review,"The paper analyzes the importance of negation in some natural language understanding tasks. It is highlighted that many of the corpora prepared for these tasks inherently contain fewer instances of negations than general English texts, moreover, a high percentage of these negations seem to be unimportant for NLU. ","The main strengths of the paper are the following: It provides a meticulous and linguistically apt analysis of the role of negation in natural language understanding tasks. 
The results of the analysis are somewhat counterintuitive, which underlines the significance of the research questions (i.e. how negation may affect the efficacy of NLU). 
The results may have a wide effect in the NLU field (it might influence the architecture of NLU systems). ","The main weaknesses of the paper are the following: More emphasis should be put on in which cases is negation important.
****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** ",****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weaknesses of the paper are the following: More emphasis should be put on in which cases is negation important.
****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** 
****The authors have adequately answered most of the weaknesses mentioned in the previous review.***** "
ARR_2022_88_review,"The paper presents a transformer based model for generating paraphrases for an input sentence with an added constraint of following a specific syntactic template. The proposed architecture consists of a normal transformer encoder to encode the source sentence, a tree-based transformer encoder to encode the syntax of the template, and finally a decoder which utilizes both sentence and syntax embeddings to generate the paraphrase. An additional attention regularization term is added to the cross entropy loss to explicitly align the decoder-syntax encoder cross-attention map to the paraphrase-template alignment known at training time. A template retriever is also proposed which can retrieve the appropriate templates for the input sentence, as for a given sentence not all syntactic templates would be compatible. Through the experiments, authors show that their method can outperform the existing baselines by a good margin, in terms of semantic and semantic similarity with the reference, as well as on the ability to generate diverse paraphrases. ","- Selecting appropriate syntactic templates for generating paraphrases is an important but overlooked aspect in controlled text generation, but it is addressed in the paper with the use of Syntactic Template Retriver.
- The proposed method obtains impressive improvements over the baselines for the most part.
- The evaluation setup looks comprehensive, with the methods compared over a wide variety of automatic metrics as well as with human evaluation. To the authors' credit they have added comparisons with other methods and experiments on QQPos dataset in the both syntactic and diverse paraphrase generation experiments as suggested by the reviewers. ","- While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold.  - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors.
Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline. ","1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right? 
2. It would have been useful if in Table 5, examples of the templates would have also been given. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold.  - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors.
Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline. 
1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right? 
2. It would have been useful if in Table 5, examples of the templates would have also been given. "
ARR_2022_88_review,"In this paper, the authors proposed Structural Information-augmented Syntax Controlled Paraphrasing (Si_SCP) which is a syntax-controlled paraphrase generation technique. They tackled two problems of such generation (a) encoding the structural information -- by using a tree transformer to capture parent-child and sibling relationships and (b) retrieving syntactic structure to guide the generation - by introducing a synthetic template retriever. ","(a) The paper is well written and easy to follow. 
(b) Various experiments and ablation studies are done to establish the effectiveness of the proposed mechanism. 
(c) The authors worked on the previous weaknesses thoroughly. ","Though the paper is detailed, I recommend the following areas to be addressed (a) Retrieval of the similar templates: The authors should clarify on why the query text and the whole query parse is important to retrieve templates? Is  it always the case that given a query, we can retrieve templates other than the own query template? Are all top K retrieved templates meaningful concerning the query or some thresholding on similarity is needed? 
(b) Human evaluation needs some more elaboration. The author should report the inter-annotator agreement along with the results.   (c) Syntactic evaluation: At the time of evaluating TED, did the authors use the top most retrieved template? They should elaborate on the process (d) Table 1: The performance of (Si_SCP) is slightly better than guiG in ParaNMT-small where as for QQP-OS the gains are higher. Is there any specific reasons or observation regarding the same? 
(e) Though the paper is focused on syntax guided paraphrase generation, it would be nice if the authors can discuss Si_SCP’s gain in comparison to unsupervised paraphrase generations [Krishna, Kalpesh, John Wieting, and Mohit Iyyer. "" Reformulating Unsupervised Style Transfer as Paraphrase Generation."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.] ","While I can see a detailed work in the paper, I would recommend the authors to address the points mentioned in the weaknesses. ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"Though the paper is detailed, I recommend the following areas to be addressed (a) Retrieval of the similar templates: The authors should clarify on why the query text and the whole query parse is important to retrieve templates? Is  it always the case that given a query, we can retrieve templates other than the own query template? Are all top K retrieved templates meaningful concerning the query or some thresholding on similarity is needed? 
(b) Human evaluation needs some more elaboration. The author should report the inter-annotator agreement along with the results.   (c) Syntactic evaluation: At the time of evaluating TED, did the authors use the top most retrieved template? They should elaborate on the process (d) Table 1: The performance of (Si_SCP) is slightly better than guiG in ParaNMT-small where as for QQP-OS the gains are higher. Is there any specific reasons or observation regarding the same? 
(e) Though the paper is focused on syntax guided paraphrase generation, it would be nice if the authors can discuss Si_SCP’s gain in comparison to unsupervised paraphrase generations [Krishna, Kalpesh, John Wieting, and Mohit Iyyer. "" Reformulating Unsupervised Style Transfer as Paraphrase Generation."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.] 
While I can see a detailed work in the paper, I would recommend the authors to address the points mentioned in the weaknesses. "
ARR_2022_250_review,"This paper tackles probing for Predicate Argument Structures (PASs) in Pretrained Language Models (PLMs).  Using standard probing techniques, the authors claim to probe PLMs for PASs diffrently compared to previous work. They employ probes (linear and non-linear classifiers) to quantify the strength of the learned representation with respect to a linguistic property.  The authors also provide a thorough analysis of how PLMs encode  predicate argument structure information and showcased  how verbal and nominal predicates and their PASs are represented differently. Finally, they empirically demonstrated how the effective integration of  of predicate-argument structure knowledge in SRL systems can boost their accuracy. ","1) The paper is written well and is a pleasure to read (although some details are missing) 2) The paper presents detailed results and the findings are interesting. 
3)  Enlightening idea: verbal and nominal predicates and their PASs are represented differently 4) the integration of predicate-argument structure knowledge into a state-of-the-art end-to-end architecture for SRL. ","Despite the good performance of the probing classifiers, it is difficult to determine whether PASs generalizations are learned, or whether a kind of separate rote memorization of each predicate take place. 
to ascertain that model learn significant generalizations, researchers tend to employ control tasks. Unfortunately, in this paper, the authors reduced the  procedure to initializing the weights of the language model at random. I do not think this is a principled way to ascertain that the intermediate representation actually encodes the property at hand. The authors should  use selectivity instead. It is computed by subtracting the performance of the probe with re-assigned labels from the reported proxy task performance. 
the authors are asked to refer to Hewitt & Liang, (2019): ""Designing and Interpreting Probes with Control Tasks"" for more details. ",The paper can be further improved if the authors uses selectivity in their experiments. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Despite the good performance of the probing classifiers, it is difficult to determine whether PASs generalizations are learned, or whether a kind of separate rote memorization of each predicate take place. 
to ascertain that model learn significant generalizations, researchers tend to employ control tasks. Unfortunately, in this paper, the authors reduced the  procedure to initializing the weights of the language model at random. I do not think this is a principled way to ascertain that the intermediate representation actually encodes the property at hand. The authors should  use selectivity instead. It is computed by subtracting the performance of the probe with re-assigned labels from the reported proxy task performance. 
the authors are asked to refer to Hewitt & Liang, (2019): ""Designing and Interpreting Probes with Control Tasks"" for more details. 
The paper can be further improved if the authors uses selectivity in their experiments. "
ARR_2022_145_review,"The paper proposed a promising indicator, gradient accordance, to alleviate Flooding method from hyper-parameter search. The performance of the algorithm is tested on five datasets, i.e., IMDB, AG News, SST-2, QNLI and MRPC, and achieved state-of-the-art robust accuracy. ","The paper works on an important task and the proposed metric is novel. Though Flooding method works very well if we search the flood level carefully, it is a very tedious job and could take lots of time if the size of the dataset is huge. Based on the agreement of different batches in the same epoch, the agreement between them could be used as an indicator of overfitting, and further applied to decide the flood level.  This paper tests the performance of the proposed indicator on a variety of tasks and achieved good results on some of them. Also, the analysis in the section 5 is also very helpful to the general audience as it includes detailed results and reasoning about why and how the proposed method works. ","I will list the detailed weakness in the next section.  In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable. ","1. "" a smoother loss"" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting?  There is a text overlap between 0.00 and 0.03 around the origin.
2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?
3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is ""our method is also the **best** among all the baseline methods"", which is a contradiction to me. Later the paper suggests that ""outperforming the **baselines** of our implementation."", what is the difference between ""baseline"" and ""your re-implemented results"".  4. Some introduction to the datasets might also be helpful, such as the size of each dataset.  5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?
6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?
7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here.  8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting?  9. Line 22, ""Bert"" --> ""BERT"" ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"I will list the detailed weakness in the next section.  In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable. 
1. "" a smoother loss"" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting?  There is a text overlap between 0.00 and 0.03 around the origin.
2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?
3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is ""our method is also the **best** among all the baseline methods"", which is a contradiction to me. Later the paper suggests that ""outperforming the **baselines** of our implementation."", what is the difference between ""baseline"" and ""your re-implemented results"".  4. Some introduction to the datasets might also be helpful, such as the size of each dataset.  5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?
6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?
7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here.  8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting?  9. Line 22, ""Bert"" --> ""BERT"" "
ARR_2022_145_review,"This paper applies a method known as *flooding* [1] when fine-tuning BERT-base on various downstream tasks. Flooding is a regularizer that prevents the model from converging to zero training loss by choosing an appropriate *flooding level*. Moreover, according to the authors, flooding encourages the model to converge to a flat region of the loss landscape.  The authors find that fine-tuning BERT-base with flooding significantly improves its robustness, and improves over all baselines in most settings, against three adversarial attacks (TextFooler, TextBugger, and BERTAttack) from the TextAttack framework.
Beyond the experimental contribution, the authors also propose an efficient way to find the *flooding level*, a crucial hyperparameter of the flooding method, based on *gradient accordance*. Moreover, they convincingly analyse and discuss their contributions.
[1] https://arxiv.org/abs/2002.08709 ","- The paper shows that adversarial robustness of BERT-base against common attacks can be significantly improved by training with a regularizer instead of leveraging adversarial training, which is computationally more demanding.
- The authors contribute an interesting method which makes the flooding method usable for fine-tuning and relate it to the notion of overfitting. This might be an interesting method for other NLP settings beyond robust training.  - The experimental results are strong and the authors provide additional analysis investigating their proposed methods. ","- The PGD adversarial training baseline might be a weak baseline as the authors restrict the number of gradient steps to 5. I would recommend to run PGD with more steps, at least for one of the downstream tasks, to provide a stronger baseline. ","- I don't understand what Figure 1 shows. Also, It's not clear to me how the paragraph starting from line 183 connects to the previous discussion. Maybe you can clarify this?
- Which attack was used to compute accuracy under attack in Figure 2? Could you add additional lines showing accuracy under attack against more than one method? This would illustrate if the optimal flood level generalizes across attacks.
- Figure 3 is a bit hard to read. Maybe move MRPC to a separate figure and put it in the appendix?
The authors might be interesting in this concurrent work (https://arxiv.org/abs/2104.04448) from the computer vision community  that studies the relationship between adversarial robustness and flatness. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- The PGD adversarial training baseline might be a weak baseline as the authors restrict the number of gradient steps to 5. I would recommend to run PGD with more steps, at least for one of the downstream tasks, to provide a stronger baseline. 
- I don't understand what Figure 1 shows. Also, It's not clear to me how the paragraph starting from line 183 connects to the previous discussion. Maybe you can clarify this?
- Which attack was used to compute accuracy under attack in Figure 2? Could you add additional lines showing accuracy under attack against more than one method? This would illustrate if the optimal flood level generalizes across attacks.
- Figure 3 is a bit hard to read. Maybe move MRPC to a separate figure and put it in the appendix?
The authors might be interesting in this concurrent work (https://arxiv.org/abs/2104.04448) from the computer vision community  that studies the relationship between adversarial robustness and flatness. "
ARR_2022_331_review,"This paper compiles and annotates a multi-reference Chinese Grammatical Error Correction (MuCGEC) evaluation dataset from three different Chinese Second Language sources. Multiple annotators and reviewers are involved in rewriting ungrammatical sentences and creating the most multi-referenced GEC dataset for Chinese. 
This work ensembles SOTA Seq2Edit and Seq2Seq models with Structured BERT and beats previous SOTAs on an earlier benchmark dataset NLPCC18. Moreover, char-based evaluation metrics are suggested, and ablation experiments are implemented on the number of references, error types, text sources, etc. ","The strict and (hopefully) accessible annotation guidelines and multiple annotators and reviewers ensure annotation quality. 
Their ensemble model compensates for the drawbacks of Seq2Edit and Seq2Seq models on Recall. 
Since the previous SOTA models do not provide code access, this paper's data and release contribute largely to GEC for Chinese. ","Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?
2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.   3) Briefly discussing what kinds of data augmentation techniques  ( ♥ )  other works use in Table 4 would be great (and maybe why not used in this paper). ","Line 17: add a space ""datasets. We"" --> ""datasets. We"" ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?
2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.   3) Briefly discussing what kinds of data augmentation techniques  ( ♥ )  other works use in Table 4 would be great (and maybe why not used in this paper). 
Line 17: add a space ""datasets. We"" --> ""datasets. We"" "
ARR_2022_331_review,"This paper introduces a new corpus of error-annotated sentences for grammatical error correction for Chinese. The dataset is composed of a subset of sentences from three different sources which were corrected by humans. The dataset contains multiple correction references for each erroneous sentence and is split into development and test in an attempt to standardise evaluation. The authors also make a case that character-based evaluation is better suited than word-based evaluation for Chinese GEC and carry out experiments with different systems on their new dataset using this approach.  This is the second revision of the paper that I have been asked to review. I was pleasantly surprised to see that the authors took my previous review into account and addressed as many of my points as possible in this new version. The paper has been thoroughly revised and is now much clearer and comprehensive, adding further clarifications and new experiments which include ensemble models. The paper is now more solid and worthy of publication. ","- Contribution of a human-annotated dataset for grammatical error correction for Chinese.
- Multi-reference and multi-source (multi-domain) dataset.
- Insights about better evaluation practices.
- A very good set of benchmark results on the new dataset.
- Comparison with previous work.
- Some error analysis. ","- While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker. ","1) Line 29: ""To support the GEC study..."". Your study or GEC in general? Maybe you mean ""To support GEC research/development/solutions""?
2) Line 53: ""Because, obviously, there are usually multiple acceptable references with close meanings for an incorrect sentence, as illustrated by the example in Table 1."" This is not a well-formed sentence. Rewrite or attach to the previous one.
3) Line 59: Choose either ""the model will be unfairly *penalised*"" or ""*performance* will be unfairly underestimated"".
4) Line 83: ""... for detailed illustration"". ???
5) Line 189: ""To facilitate illustration, our guidelines adopt a two-tier hierarchical error taxonomy..."" You said earlier that you adopted the ""direct re-rewriting"" approach so why does your annotation guidelines provide a taxonomy or errors? Is it just to make sure that all ocurrences of the same types of errors are handled equally by all annotators? Weren't they free to correct the sentences in any way they wanted as you stated in lines 178-180?
6) Line 264: ""We attribute this to our strict control of the over-correction phenomenon."" What do you mean exactly? The original annotation considered some sentences to be erroneous while your guidelines did not?
7) Line 310: ""Since it is usually ..."" This is not a well-formed sentence. Rewrite or attach to the previous one.
8) Line 399: ""... and only use the erroneous part for training"" Do you mean you discard correct sentences? As it stands, it sounds as if you only kept the incorrect sentences without their corrections. You might want to make this clearer.
9) ""... which does not need error-coded annotation"". This is not exactly so. ERRANT computes P, R and F from M2 files containing span-level annotations. For English, it is able to automatically generate these annotations from parallel text using an alignment and edit extraction algorithm. In the case of Chinese, you did this yourself. So while it is not necessary to manually annotate the error spans, you do need to extract them somehow before ERRANT can compute the measures.
10) Table 5: ""For calculating the human performance, each submitted result is considered as a sample if an annotator submits multiple results."" I am afraid this does not clearly explain how human performance was computed. Each annotator against the rest? Averaged across all of them? How are multiple corrections from a single annotator handled?  If you compared each annotation to the rest but the systems were compared to all the annotations, then I believe human evaluation is an underestimation. This is still not clear.
11) Line 514: ""The word-order errors can be identified by heuristic rules following Hinson et al. (2020)."" Did you classify the errors in the M2 files before feeding them into ERRANT?
12) Line 544: ""... we remove all extra references if a sentence has more than 2 gold-standard references"". Do you remove them randomly or sequentially?
13) TYPOS/ERRORS: ""them"" -> ""this approach""? ( line 72), ""both formal/informal"" -> ""both formal and informal"" (line 81), ""supplement"" -> ""supply""? ( lines 89, 867), ""from total"" -> ""from *the* total"" (line 127), ""Finally, we have obtained 7,137 sentences"" -> ""In the end, we obtained 7,137 sentences"" (line 138), ""suffers from"" -> ""poses"" (line 155), ""illustration"" -> ""annotation""? ( line 189), ""Golden"" -> ""Gold"" (lines 212, 220, 221, 317), ""sentence numbers"" -> ""number of sentences"" (Table 3 caption), ""numbers (proportion)"" -> ""number (proportion)"" (Table 3 caption), ""averaged character numbers"" -> ""average number of characters"" (Table 3 caption), ""averaged edit numbers"" -> ""average number of edits"" (Table 3 caption), ""averaged reference numbers"" -> ""average number of references"" (Table 3 caption), ""in the parenthesis of the..."" -> ""in parentheses in the..."" (Table 3 caption), ""previous"" -> ""original""? ( line 262), ""use"" (delete, line 270), ""in *the* re-annotated"" (line 271), ""twice of that"" -> ""twice that"" (line 273), ""edit number"" -> ""number of edits"" (line 281), ""the sentence length"" -> ""sentence length"" (line 282), ""numbers"" -> ""number"" (lines 283, 297), ""numbers"" -> ""the number"" (line 295), ""Same"" -> ""Identical"" (line 298), ""calculated"" -> ""counted"" (line 299), ""the different"" -> ""different"" (Figure 1 caption), ""reference number"" -> ""number of references"" (line 305), ""for"" -> ""to"" (line 307), ""the descending"" -> ""descending"" (line 326), ""sentence numbers"" -> ""number of sentences"" (line 327), ""It"" -> ""This"" (line 331), ""annotate"" -> ""annotated"" (Figure 2 caption), ""limitation"" -> ""limitations"" (line 343), ""SOTA"" -> ""state-of-the-art (SOTA)"" (line 353), ""these"" -> ""this"" (line 369), ""where"" -> ""on which"" (line 393), ""hugging face"" -> ""Hugging Face"" (431), ""these"" -> ""this"" (line 464), ""The"" -> ""A"" (line 466), ""reference number"" -> ""the number of references"" (Figure 3 caption), ""start"" -> ""have started"" (line 571), ""will be"" -> ""are"" (line 863), ""false"" -> ""incorrect""? ( line 865). ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"- While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker. 
1) Line 29: ""To support the GEC study..."". Your study or GEC in general? Maybe you mean ""To support GEC research/development/solutions""?
2) Line 53: ""Because, obviously, there are usually multiple acceptable references with close meanings for an incorrect sentence, as illustrated by the example in Table 1."" This is not a well-formed sentence. Rewrite or attach to the previous one.
3) Line 59: Choose either ""the model will be unfairly *penalised*"" or ""*performance* will be unfairly underestimated"".
4) Line 83: ""... for detailed illustration"". ???
5) Line 189: ""To facilitate illustration, our guidelines adopt a two-tier hierarchical error taxonomy..."" You said earlier that you adopted the ""direct re-rewriting"" approach so why does your annotation guidelines provide a taxonomy or errors? Is it just to make sure that all ocurrences of the same types of errors are handled equally by all annotators? Weren't they free to correct the sentences in any way they wanted as you stated in lines 178-180?
6) Line 264: ""We attribute this to our strict control of the over-correction phenomenon."" What do you mean exactly? The original annotation considered some sentences to be erroneous while your guidelines did not?
7) Line 310: ""Since it is usually ..."" This is not a well-formed sentence. Rewrite or attach to the previous one.
8) Line 399: ""... and only use the erroneous part for training"" Do you mean you discard correct sentences? As it stands, it sounds as if you only kept the incorrect sentences without their corrections. You might want to make this clearer.
9) ""... which does not need error-coded annotation"". This is not exactly so. ERRANT computes P, R and F from M2 files containing span-level annotations. For English, it is able to automatically generate these annotations from parallel text using an alignment and edit extraction algorithm. In the case of Chinese, you did this yourself. So while it is not necessary to manually annotate the error spans, you do need to extract them somehow before ERRANT can compute the measures.
10) Table 5: ""For calculating the human performance, each submitted result is considered as a sample if an annotator submits multiple results."" I am afraid this does not clearly explain how human performance was computed. Each annotator against the rest? Averaged across all of them? How are multiple corrections from a single annotator handled?  If you compared each annotation to the rest but the systems were compared to all the annotations, then I believe human evaluation is an underestimation. This is still not clear.
11) Line 514: ""The word-order errors can be identified by heuristic rules following Hinson et al. (2020)."" Did you classify the errors in the M2 files before feeding them into ERRANT?
12) Line 544: ""... we remove all extra references if a sentence has more than 2 gold-standard references"". Do you remove them randomly or sequentially?
13) TYPOS/ERRORS: ""them"" -> ""this approach""? ( line 72), ""both formal/informal"" -> ""both formal and informal"" (line 81), ""supplement"" -> ""supply""? ( lines 89, 867), ""from total"" -> ""from *the* total"" (line 127), ""Finally, we have obtained 7,137 sentences"" -> ""In the end, we obtained 7,137 sentences"" (line 138), ""suffers from"" -> ""poses"" (line 155), ""illustration"" -> ""annotation""? ( line 189), ""Golden"" -> ""Gold"" (lines 212, 220, 221, 317), ""sentence numbers"" -> ""number of sentences"" (Table 3 caption), ""numbers (proportion)"" -> ""number (proportion)"" (Table 3 caption), ""averaged character numbers"" -> ""average number of characters"" (Table 3 caption), ""averaged edit numbers"" -> ""average number of edits"" (Table 3 caption), ""averaged reference numbers"" -> ""average number of references"" (Table 3 caption), ""in the parenthesis of the..."" -> ""in parentheses in the..."" (Table 3 caption), ""previous"" -> ""original""? ( line 262), ""use"" (delete, line 270), ""in *the* re-annotated"" (line 271), ""twice of that"" -> ""twice that"" (line 273), ""edit number"" -> ""number of edits"" (line 281), ""the sentence length"" -> ""sentence length"" (line 282), ""numbers"" -> ""number"" (lines 283, 297), ""numbers"" -> ""the number"" (line 295), ""Same"" -> ""Identical"" (line 298), ""calculated"" -> ""counted"" (line 299), ""the different"" -> ""different"" (Figure 1 caption), ""reference number"" -> ""number of references"" (line 305), ""for"" -> ""to"" (line 307), ""the descending"" -> ""descending"" (line 326), ""sentence numbers"" -> ""number of sentences"" (line 327), ""It"" -> ""This"" (line 331), ""annotate"" -> ""annotated"" (Figure 2 caption), ""limitation"" -> ""limitations"" (line 343), ""SOTA"" -> ""state-of-the-art (SOTA)"" (line 353), ""these"" -> ""this"" (line 369), ""where"" -> ""on which"" (line 393), ""hugging face"" -> ""Hugging Face"" (431), ""these"" -> ""this"" (line 464), ""The"" -> ""A"" (line 466), ""reference number"" -> ""the number of references"" (Figure 3 caption), ""start"" -> ""have started"" (line 571), ""will be"" -> ""are"" (line 863), ""false"" -> ""incorrect""? ( line 865). "
ARR_2022_331_review,The paper releases a new dataset for Chinese Grammatical Error Correction which differs from the previous ones by its multi-source and multi-reference nature. The authors describe the annotation process and test several models of different type (Seq2Seq and Seq2Edits) on their data. ,"-- a new dataset is released -- it is of larger size, higher quality and has more references than the previous ones -- the authors convincingly describe why the direct rewriting annotation paradigm that they follow is better than the error-coded one used for the previous Chinese GEC dataset collection. ",-- the contribution is probably not significant outside GEC community ,"l.17 -- no space after period l.66 ""it will be taxing"" -> ""it will be difficult"" l.260-265 ""most of the sentences are considered to contain grammatical errors in the previous annotation, but a considerable part of them are not corrected in our annotation"" You can make such comparison only on the same data, thus I compare the number of erroneous sentences for NLPCC18(Orig) and MuCGEC(NLPCC18), but the number of correct sentences has increased only by about 80. May be it is more convincing to provide the average number of errors per sentence?
l.266-267 -> NLPCC18 has the shortest sentences, whereas CGED sentences are much longer.
l.267-270 This is possibly because, since HSK is an official Chinese proficiency test, candidates tend to use long sentences to show their ability in Chinese use -> ...because of the fact that HSK is ..., so candidates l.471 -472 I am sure whether I understood the terminology: do you call redundant errors the cases when the text contains more characters than its correction. May be, it is better to call them insertion and deletion errors? ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"-- the contribution is probably not significant outside GEC community 
l.17 -- no space after period l.66 ""it will be taxing"" -> ""it will be difficult"" l.260-265 ""most of the sentences are considered to contain grammatical errors in the previous annotation, but a considerable part of them are not corrected in our annotation"" You can make such comparison only on the same data, thus I compare the number of erroneous sentences for NLPCC18(Orig) and MuCGEC(NLPCC18), but the number of correct sentences has increased only by about 80. May be it is more convincing to provide the average number of errors per sentence?
l.266-267 -> NLPCC18 has the shortest sentences, whereas CGED sentences are much longer.
l.267-270 This is possibly because, since HSK is an official Chinese proficiency test, candidates tend to use long sentences to show their ability in Chinese use -> ...because of the fact that HSK is ..., so candidates l.471 -472 I am sure whether I understood the terminology: do you call redundant errors the cases when the text contains more characters than its correction. May be, it is better to call them insertion and deletion errors? "
ARR_2022_191_review,"The paper tackles the challenge of long documents/dialogs summarizations. 
It presents a simple but efficient approach ""split-then-summarize"" using a greedy algorithm for choosing the good partition.
The experiments in the second part of the paper show a real improvement using this generic algorithm (agnostic to the used model)  over the naive baseline. ","- The challenge that this paper tends to solve is a painful problem and the proposed solution is simple to implement and yields to good results.
- The paper is well written and easy to follow and understand.
- The set of experiments proving the efficiency of the method is good. ( not optimal) ","- In the related work, you cite several approaches - It would be fair to compare the SummN results with the implementation of the reported approaches. 
Comparing SummN to the naive Baseline (Bart) is obviously better, but what is the benefit of SummN over the previous solutions ? ","In the Target segmentation, it is not clear, what can we do when the number of sentences is lower than K ? 
In section 5.6, could you please provide the Kappa ? ",3.5 ,No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"- In the related work, you cite several approaches - It would be fair to compare the SummN results with the implementation of the reported approaches. 
Comparing SummN to the naive Baseline (Bart) is obviously better, but what is the benefit of SummN over the previous solutions ? 
In the Target segmentation, it is not clear, what can we do when the number of sentences is lower than K ? 
In section 5.6, could you please provide the Kappa ? "
ARR_2022_191_review,"The paper presents a framework for abstractive summarization of long documents that repeatedly segments text, summarizes each segment and then feeds the concatenated summaries as an input to the next iteration. When the input is below a predefined number of tokens, the final summary is generated. This design has the advantages of being able to use powerful pretrained models (e.g. BART) that are designed for shorter text, and avoiding expensive attention computation over long spans. ","- The approach is simple, powerful and flexible - a good idea for applying powerful pretrained models to longer text without truncation.
- A very comprehensive evaluation is provided with a good selection of datasets and both automatic and human evaluation which show the model’s clear advantage over baselines.
- The paper is well-written and easy to understand. ","- **Inefficient design of target segmentation**: If I understand correctly, all input segments are assigned a target segment. This probably causes unnecessary summarization of irrelevant input segments - if the text is massive and the gold summary is very short, why not ignore most of the input segments and only select sufficient input segments to cover the targets? Also, this allows targets to be duplicated over different input segments.  - **Missing analysis of model behavior**: It would be useful and interesting to know what exactly is happening at each stage, e.g. how much text is produced at each stage, and how noisy or detailed intermediate summaries look like.
- **Missing analysis of empirical running time** ","**Suggestions** - Line 249: “Huge time costs” sounds very informal, maybe change to “considerable running time” or similar - Please indicate if f-score, precision or recall is used when ROUGE is mentioned - Follow-up on weakness 1): ignoring input segments could be implemented by assigning “empty” targets, e.g. using a single special token that when predicted would trigger a segment to be discarded in the next step.
**Questions** - Table 9 in Appendix: what does “keyword emerged in gold summary” mean?
- “Separate model for each stage” is mentioned, but I am still not sure whether this means 2 (fine and coarse) or N models?
- Does the number of input segments at each coarse stage always stay the same as it was in stage 1, before reaching the fine-grained stage? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- **Inefficient design of target segmentation**: If I understand correctly, all input segments are assigned a target segment. This probably causes unnecessary summarization of irrelevant input segments - if the text is massive and the gold summary is very short, why not ignore most of the input segments and only select sufficient input segments to cover the targets? Also, this allows targets to be duplicated over different input segments.  - **Missing analysis of model behavior**: It would be useful and interesting to know what exactly is happening at each stage, e.g. how much text is produced at each stage, and how noisy or detailed intermediate summaries look like.
- **Missing analysis of empirical running time** 
**Suggestions** - Line 249: “Huge time costs” sounds very informal, maybe change to “considerable running time” or similar - Please indicate if f-score, precision or recall is used when ROUGE is mentioned - Follow-up on weakness 1): ignoring input segments could be implemented by assigning “empty” targets, e.g. using a single special token that when predicted would trigger a segment to be discarded in the next step.
**Questions** - Table 9 in Appendix: what does “keyword emerged in gold summary” mean?
- “Separate model for each stage” is mentioned, but I am still not sure whether this means 2 (fine and coarse) or N models?
- Does the number of input segments at each coarse stage always stay the same as it was in stage 1, before reaching the fine-grained stage? "
ARR_2022_191_review,"This work approaches the task of long text summarization with a multi-stage framework, dealing with the input length limitation of modern Transformer-based models. In this work, the summarization is performed in N coarse-grained stages and one fine-grained final stage. The coarse stage aims to not only generate the coarse summary but also segment the input text. The coarse stage performs multiple times to gradually reduce the input length specific to the input length limitation of the backbone model. The training instances for the coarse stage are prepared with a greedy algorithm for aligning the source segments and the target segments. BART is adopted as the backbone summarization model. The proposed method was evaluated on both dialogue and written data. A total of five datasets are involved in the experiments. Experimental results show the proposed methodology outperforms baseline models in most cases. ","- The work addresses a very practical and important issue in abstractive summarization. The length limitation of modern Transformer-based models forms a major barrier to long text summarization, while longer the document/dialogue, the higher the demand for summarization.  - The proposed method is model-agnostic where a variety of models can be chosen as the backbone summarizer. Table 6 shows all the three backbone models benefit from the multi-stage strategy. ",- The computation cost can be an issue for the multi-stage generation. A measurement of the runtime can be supplied for comprehension. ,"- I am curious about the number of total predictions to be made for a single input. As shown in Table 1, the number of N + 1 is 3 for the GovReport dataset. The total number of summarization to be performed is N + 1 or N^2 + 1? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The computation cost can be an issue for the multi-stage generation. A measurement of the runtime can be supplied for comprehension. 
- I am curious about the number of total predictions to be made for a single input. As shown in Table 1, the number of N + 1 is 3 for the GovReport dataset. The total number of summarization to be performed is N + 1 or N^2 + 1? "
ARR_2022_284_review,"The paper presents a new dataset for image retrieval in Language and Vision research, and some first results computed with state-of-the-art models in L&V (CLIP and Vilbert). The idea of the data collection is to present human annotators with a set of images, out which one is the target that needs to be described. The image sets are sampled from existing resources of static images, and videos. The data analysis in the paper shows that the datasets contain interesting and challenging phenomena for language-and-vision reasoning. The experiments with CLIP and Vilbert show that existing models are clearly not up to human performance on this task. ","This paper is an interesting, relevant, well-executed, and well-written resource paper. The dataset is well-motivated, the collection is described clearly and the tested models and baselines are perfectly reasonable. The data set will be of great use to the community and foster further research in challenging language phenomena in the area of L&V. ","The only thing I can complain about is the rather general framing of the dataset in the introduction. To me, it seems that ""contextual descriptions"" is a bit too vague and generic as a term for describing the contribution. The contexts explored in this work are very specific distractor-like types of contexts where an image appears in combination with extremely similar images. This is close to the classical referring expression setting that investigates descriptions of objects in the context of similar distractor objects. The resulting expressions may be called discriminative descriptions/unambiguous references or something along these lines. ","My suggestion would be to mention the connection to referring expressions somewhere in the paper and to think about refining the intro/notion of context a bit.
Yu, Licheng, et al. ""Modeling context in referring expressions."" European Conference on Computer Vision. Springer, Cham, 2016. 
Mao, Junhua, et al. ""Generation and comprehension of unambiguous object descriptions."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"The only thing I can complain about is the rather general framing of the dataset in the introduction. To me, it seems that ""contextual descriptions"" is a bit too vague and generic as a term for describing the contribution. The contexts explored in this work are very specific distractor-like types of contexts where an image appears in combination with extremely similar images. This is close to the classical referring expression setting that investigates descriptions of objects in the context of similar distractor objects. The resulting expressions may be called discriminative descriptions/unambiguous references or something along these lines. 
My suggestion would be to mention the connection to referring expressions somewhere in the paper and to think about refining the intro/notion of context a bit.
Yu, Licheng, et al. ""Modeling context in referring expressions."" European Conference on Computer Vision. Springer, Cham, 2016. 
Mao, Junhua, et al. ""Generation and comprehension of unambiguous object descriptions."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. "
ARR_2022_284_review,"The authors propose a multiple choice task posed over 10 images. These images are quite similar (either frames from the same video, or static images selected to be similar). Given a fine-grained visual description that applies to only one of the images, the task is to predict the correct one. Compared to human accuracy which hovers at ~90% for humans, the CLIP and ViLBERT get only roughly 30% accuracy. ","The task is well-posed and well-motivated: it is essentially a harder version of NLVR2. While NLVR2 is not saturated (SoTA = high 80s, human = high 90s), there's no reason why both of these tasks can't be considered in parallel. The paper is well-written and easy to follow. The experimental sections clearly illustrate the importance of selecting negative examples that are semantically similar during training. The filtration scheme that the authors use to construct their corpus ensures that only pairs humans (generally) agree upon make it into the final train/val/test sets. The error analyses show that questions with more similar images are harder, and shorter sentences are easier. ","In my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.
I had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results   directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these   subsets. ","Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"In my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.
I had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results   directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these   subsets. 
Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. "
ARR_2022_353_review,"The paper considers the problem of structural bias in NLI, such as predicting the entailment from hypothesis only. 
The problem is addressed by reformulating the NLI task as a generative task where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input.
With this formulation, they achieve a bias-free model, with the downside of decreased performance on the actual task. Then they further analyze the cause for this poor performance and propose ways to mitigate it and partially close the gap with the performance of the discriminative model by discriminative fine-tuning.
The contributions of the paper are addressing the structural bias in NLI, building a generative model that leads to unbiased models, and improving its performance with discriminative fine-tuning (allowing some bias into the model). ","The examined problem is interesting and well-motivated.
The paper is well-written and overall easy to follow.
The paper provides a way to remove structural bias for the NLI task, examines the cause for its poor performance and proposes ways to improve it by balancing the bias-performance trade-off.
The proposed formulation and analysis can lead to interesting extensions in future work. ","The proposed bias-free model, although eliminating the bias, suffers from poor performance. One thing that the paper would benefit from would be stressing the practical implication of the approach, having in mind that it is not useful in itself for solving the task. ",255-257: Can you please explain why this is the case? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",5 = They could easily reproduce the results.,,"The proposed bias-free model, although eliminating the bias, suffers from poor performance. One thing that the paper would benefit from would be stressing the practical implication of the approach, having in mind that it is not useful in itself for solving the task. 
255-257: Can you please explain why this is the case? "
ARR_2022_353_review,"This paper considers structural bias to be cases where the label can be predicted based on a specific feature (the authors motivate this with the example of hypothesis-only bias in NLI). They quantify this as a delta measure: the difference between accuracy on the standard test set and a ""hard"" test set. The latter is composed of all the wrongly predicted examples by a biased model. It then proposes to frame classification problems as generation problems and claims that a generative model with a uniform prior can overcome this bias. However, empirical results show that this is achieved at the expense of classification accuracy. The authors demonstrate  by finetuning the generative model as a classifier, however this reintroduces bias according to their delta metric. ","1. Interesting idea with theoretical grounding. 
2. Clear and well-written. 
3. Strong empirical results: 5-10 point reduction in the metric used to measure bias. Is there a difference between in-domain and out-of-domain performance for MNLI? ","1. The authors claim that this is a novel formulation of the NLI task but cite a prior paper that formulates this task the same way. The novelty is in the generative implementation and the scope of the claim should be reduced. 
2. I might have missed this, but the actual implementation of how p(y|B) is set to a uniform distribution during inference is not described. I would be happy to retract this point if the other reviewers saw this described somewhere. Additionally, how is training with a uniform prior actually implemented (Table 3)? 
3. One experiment I would like to see (and is missing at the moment) is a performance comparison of all models on challenge sets such as ANLI since they were constructed specifically to weed out models that perform well on the original test set using spurious features. 
4. Is there a reason for using only BERT over RoBERTa since the latter is generally a stronger baseline? Could a model pretrained on a larger and more diverse dataset be less prone to structural bias? 
5. The authors do not state if the code will be released. ","L50: ""Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems."": Such as?
L291: Is BERT trained with forward masks when using the autoregressive loss? ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"2 = From social media/a talk/other informal communication, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The authors claim that this is a novel formulation of the NLI task but cite a prior paper that formulates this task the same way. The novelty is in the generative implementation and the scope of the claim should be reduced. 
2. I might have missed this, but the actual implementation of how p(y|B) is set to a uniform distribution during inference is not described. I would be happy to retract this point if the other reviewers saw this described somewhere. Additionally, how is training with a uniform prior actually implemented (Table 3)? 
3. One experiment I would like to see (and is missing at the moment) is a performance comparison of all models on challenge sets such as ANLI since they were constructed specifically to weed out models that perform well on the original test set using spurious features. 
4. Is there a reason for using only BERT over RoBERTa since the latter is generally a stronger baseline? Could a model pretrained on a larger and more diverse dataset be less prone to structural bias? 
5. The authors do not state if the code will be released. 
L50: ""Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems."": Such as?
L291: Is BERT trained with forward masks when using the autoregressive loss? "
ARR_2022_128_review,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing. The key idea is to control the sampling to sample more from languages with higher losses. Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks. ","- Zero-shot dependency parsing is a difficult but valuable task in both academia and industry.
- The proposed method is intuitive.
- The empirical analyses were conducted on a large set of languages, which makes the results more convincible. ","- Based on the empirical analyses (Table 2), it is not clear if pure worst-case-aware sampling ($\phi=0$) is consistently better than pure loss-proportional sampling ($\phi=1$). It seems the results on mBERT and XLM-R give different conclusions. The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.
- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental. ","- Line 183: it would be good to show what languages are used in the PLM pretraining and if there is a difference of the proposed method between the seen and unseen languages.
- Table 2: there are only three values of $\phi$ listed, and there are no difference between $\phi=0.5$ and $\phi=1$. It would be good to show more details on how $\phi$ affects the final performance.
- Table 3: which PLM is used in this experiment? What is the $\phi$? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Based on the empirical analyses (Table 2), it is not clear if pure worst-case-aware sampling ($\phi=0$) is consistently better than pure loss-proportional sampling ($\phi=1$). It seems the results on mBERT and XLM-R give different conclusions. The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.
- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental. 
- Line 183: it would be good to show what languages are used in the PLM pretraining and if there is a difference of the proposed method between the seen and unseen languages.
- Table 2: there are only three values of $\phi$ listed, and there are no difference between $\phi=0.5$ and $\phi=1$. It would be good to show more details on how $\phi$ affects the final performance.
- Table 3: which PLM is used in this experiment? What is the $\phi$? "
ARR_2022_128_review,"The paper applies the worst-case aware curriculum learning algorithm to zero-shot cross-lingual transfer for dependency parsing. The main contribution is the empirical study on cross-lingual dependency parsing, which shows that this method improves accuracy in the zero-shot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks. ","1. The method description is very clear 2. I appreciate the analysis with varying homogeneity of training samples. Although the results are conflicting, it's nice to check how the selection of training languages impacts the effectiveness of this method. ","1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper. 
2. The results in the analysis with varying homogeneity of training samples (Section 4) are conflicting: they don't support the hypothesis that this method improves more when some language types are underrepresented in the training samples. Therefore, it's unclear why this method improves over baseline methods, i.e. does it help the model learn from skewed training samples or is it something else? 
3. I'd expect a discussion of how this method relates to and differs from existing methods on zero-shot cross-lingua transfer. For example, meta-learning methods optimize the initial parameters [1] or optimization algorithms [2] during fine-tuning toward better performance on outlier/unseen languages.
[1] Zero-Shot Cross-Lingual Transfer with Meta Learning. Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein. 2020.
[2] Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. Weijia Xu, Batool Haider, Jason Krone, Saab Mansour. 2021. ","For the analysis with varying homogeneity of training samples, would it help to look at performance on the romance and the outlier languages separately? Also, it might also be helpful to consider the typological distance between different language genera. ",2.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper. 
2. The results in the analysis with varying homogeneity of training samples (Section 4) are conflicting: they don't support the hypothesis that this method improves more when some language types are underrepresented in the training samples. Therefore, it's unclear why this method improves over baseline methods, i.e. does it help the model learn from skewed training samples or is it something else? 
3. I'd expect a discussion of how this method relates to and differs from existing methods on zero-shot cross-lingua transfer. For example, meta-learning methods optimize the initial parameters [1] or optimization algorithms [2] during fine-tuning toward better performance on outlier/unseen languages.
[1] Zero-Shot Cross-Lingual Transfer with Meta Learning. Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein. 2020.
[2] Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. Weijia Xu, Batool Haider, Jason Krone, Saab Mansour. 2021. 
For the analysis with varying homogeneity of training samples, would it help to look at performance on the romance and the outlier languages separately? Also, it might also be helpful to consider the typological distance between different language genera. "
ARR_2022_128_review,"This paper adopts a multi-task learning method from (Zhang et al 2020) in the zero-shot multilingual dependency parsing scenario. The method aims to use curriculum learning to optimize the worst-case-aware loss rather than averaged loss in common multi-task learning settings.  The experimental results show that this method outperforms general sampling method baselines in zero-shot learning and achieves state-of-the-art when using mBERT and XLM-R. They also explore the effect of varying the homogeneity of training languages by comparing models trained on homogeneous languages (e.g. GERMANIC, ROMANCE, SLAVIC) and skewed samples (e.g. ROMANCE+EU/AR/TR/ZH). Their results performs consistently better than baseline on all training samples but there is no clear pattern to show in which case this method works best and why. ","This paper shows that the worst-case-aware loss optimization is helpful for zero-shot multilingual dependency parsing. Although the method is model-agnostic and simple, it achieves the state-of-the-art without any external resource, which is not always available for low-resource languages. It would be interesting to also explore the effect of this method in other multilingual tasks.  The paper empirically investigates how the method performs with different training languages settings and gives an interesting analysis. ","Although the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR). ","1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.
2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.  3. The paper does not refer to the table 3 when discussing the results in section 4. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Although the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR). 
1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.
2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.  3. The paper does not refer to the table 3 when discussing the results in section 4. "
ARR_2022_232_review,"The paper is a resubmission of previous work that looked at neutralising framing bias in news stories. The authors here propose a new task called neutral summary generation where the aim is to create an unbiased summary of potentially biased stories covering one event. To measure the bias of the generated summaries, the authors use lexical estimates of polarity which is shown to correlate with relative framing bias. Finally, a new model is proposed that uses titles in addition to article summaries as a way of focusing on the framing used by each side. The results show that the suggested approach is promising but a lot of challenges remain. ","- The paper introduces a new task, namely neutral summary generation from multiple biased articles. The new task is more sensible compared to the originally proposed one. Overall the revised paper (and the authors' response) has addressed a lot of the issues in the original submission.
- The dataset is a useful resource for studying framing bias and multi-view summarisation.
- The use of polarity as a proxy for framing bias is an interesting research direction. ","- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed. ","### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements.  In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset?  Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed. 
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements.  In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset?  Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption. "
ARR_2022_294_review,"This paper presents and empirically confirms the ""prompt waywardness hypothesis"" in the context of continuous prompt tuning. The hypothesis is that for any arbitrary text there exists a continuous prompt whose discrete projection is said arbitrary text *and* whose resulting performance is almost as good as the best possible continuous prompt. Therefore, the paper suggests, it is difficult to evaluate continuous prompts in a discrete space because there always exist good continuous prompts that map to random or misleading discrete text.
A little more formally, the hypothesis is that for any downstream task and arbitrary text $p_d$ of length L, there exists some continuous prompt $\tilde{p}_c$ of length L such that $\tilde{p}_c$ results in a test loss close to that of the best continuous prompt $p^*_c$ of length L, and yet $\tilde{p}_c$ projects to $p_d$ (when projected into discrete space).
The authors empirically confirm this hypothesis by showing that for five downstream classification tasks they are able to find continuous prompts which project to certain arbitrary sentences (at least very closely, as evaluated by F1) but whose resulting accuracy is almost as good as that of the optimal prompt (as evaluated by the difference in test accuracy). These experiments use the GPT-2-large model (except when model size is ablated), and projection is defined as follows: continuous prompts are projected to discrete space by finding the token with the nearest-neighbor embedding (where token embeddings are taken to be GPT-2's embedding matrix). Finally, the paper discusses implications of the prompt waywardness hypothesis, the main implication being that continuous prompts may never be interpretable by being projected to discrete language. ","1. The paper is clearly written and easy to follow. 
2. The experimental results are thorough and convincing; the paper provides a strong analysis section evaluating the effects of hyperparameters, model size, and prompt length. 
3. The paper discusses implications of the hypothesis: the difficulty of interpreting discrete prompts and the untrustworthiness of discrete projections of continuous prompts. 
4. The conclusion that encouraging continuous prompts to project to semantically accurate instructions (Appendix B) provides interesting insight into the function of continuous prompts. ","The implications discussed in the paper apply *if* one were to try to project continuous prompts to discrete space using GPT-2's (or any other) embedding matrix, but, as far as I am aware, no one has attempted to interpret continuous prompts in this way. The paper could be strengthened by more explicitly discussing *why* we should be concerned with this particular method of prompt interpretation: 1. Do the authors think we should be especially concerned about nearest-neighbor, embedding matrix projections because that's how language model outputs and word2vec are calculated? This seems to be the case, but could be made more explicit. It's not clear why the use of an embedding matrix at the output layer of language models implies this would be a popular way of discretizing continuous prompts. 
2. Is it because the particular discrete projection studied is the simplest that the authors believe it might be used in the future? 
3. Do the authors think their hypothesis generalizes to other methods of discretely interpreting prompts? If so, that could also be made more explicit. ","1. Space permitting, it may be beneficial to move the finding of Appendix B to the main paper as I found this interesting. 
2. Line 246 is difficult to parse. Possibly split this into two sentences. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The implications discussed in the paper apply *if* one were to try to project continuous prompts to discrete space using GPT-2's (or any other) embedding matrix, but, as far as I am aware, no one has attempted to interpret continuous prompts in this way. The paper could be strengthened by more explicitly discussing *why* we should be concerned with this particular method of prompt interpretation: 1. Do the authors think we should be especially concerned about nearest-neighbor, embedding matrix projections because that's how language model outputs and word2vec are calculated? This seems to be the case, but could be made more explicit. It's not clear why the use of an embedding matrix at the output layer of language models implies this would be a popular way of discretizing continuous prompts. 
2. Is it because the particular discrete projection studied is the simplest that the authors believe it might be used in the future? 
3. Do the authors think their hypothesis generalizes to other methods of discretely interpreting prompts? If so, that could also be made more explicit. 
1. Space permitting, it may be beneficial to move the finding of Appendix B to the main paper as I found this interesting. 
2. Line 246 is difficult to parse. Possibly split this into two sentences. "
ARR_2022_294_review,"This paper proposed a Waywardness Hypothesis for the connection between continuous prompt and discrete prompt and use experiments to justify this connection. The author concluded that a soft prompt could complete a desired task while projecting to any given task-related/unrelated target text, at a small cost of accuracy.  Five classification tasks have been considered to verify the authors’ hypothesis. Experiments with large models and long prompts are also conducted to analyze the “waywardness” of the prompt method. The authors also discussed several social and research implications of waywardness. ","1. The idea to establish a connection between the continuous prompt and the discrete prompt is interesting. The hypothesis is justified by a few experiments and the proposed methods are well motivated. The writing is clear and the description of the waywardness is easy to follow. 
2. The proof method is simple and the design of the experiment and the objective are clear. All the experiments are easy to re-implement. 
3. Comprehensive extensive analysis: The paper offers the further analysis of different aspects, including model size, prompt length etc.. ","1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
  2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. ","1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
  2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
  2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. 
1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
  2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. "
ARR_2022_181_review,"The paper proposed boundary smoothing as a regularization technique for span-based neural NER models. The boundary smoothing can mitigate the over-confidence issue by reassigning the entity probabilities from annotated spans to the surrounding ones. The approach is simple, and mainly extends the Yu et al (2020) by adding the boundary smoothing. The experiments show the proposed approach can improve the performance of Yu et al (2020). ","The idea of the boundary smoothing is simple, and can improve the performance of the existing NER approach.
The in-depth analysis for the over-confidence issue is attractive.
The paper is well written. ","The idea of using boundary information is not completely innovative. For example, the idea of effectively using boundary information has been proposed in Shen et al.(2021).
The experiments don’t prove the claims. Firstly, the paper lacks adequate verification for the proposed idea, and evaluated it with only one baseline. Secondly, the paper used RoBERTa while the previous methods used BERT. Thirdly, Yu et al. (2020) conducted the experiments on GENIA, but the paper doesn’t. ","The paper lacks adequate verification for the idea. The paper claims that the boundary smoothing can be easily integrated into any span-based neural NER systems, but the paper only integrated it into the Yu et al. (2020). The paper should further evaluate the boundary smoothing idea in several SOTA approaches.
In addition, the idea of effectively using boundary information has been proposed in Shen et al.(2021), which constructs soft examples for partially matched spans, and trains a boundary regressor with boundary-level Smooth L1 loss.
In the experiments, I have some concerns: (1)	The paper uses RoBERTa for the experiments, while the baselines used BERT. It may be unfair. 
(2)	Why the Baseline outperforms Yu et al. (2020) on ACE 04 and ACE 05 greatly, but has comparable performance on OntoNotes 5 and CoNLL 2003? Yu et al. (2020) also conducted experiments on the GENIA dataset, but the paper doesn’t report the results on this dataset. 
(3)	According to the Table 4, the improvement of BS against LS is marginal.  Some statements are not very accurate. For example, Line 313, the submission says “this work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.”, however, Shen et al. 2021 proposed a span-based approach previously, and conducted the NER experiment on Chinese Weibo dataset. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"The idea of using boundary information is not completely innovative. For example, the idea of effectively using boundary information has been proposed in Shen et al.(2021).
The experiments don’t prove the claims. Firstly, the paper lacks adequate verification for the proposed idea, and evaluated it with only one baseline. Secondly, the paper used RoBERTa while the previous methods used BERT. Thirdly, Yu et al. (2020) conducted the experiments on GENIA, but the paper doesn’t. 
The paper lacks adequate verification for the idea. The paper claims that the boundary smoothing can be easily integrated into any span-based neural NER systems, but the paper only integrated it into the Yu et al. (2020). The paper should further evaluate the boundary smoothing idea in several SOTA approaches.
In addition, the idea of effectively using boundary information has been proposed in Shen et al.(2021), which constructs soft examples for partially matched spans, and trains a boundary regressor with boundary-level Smooth L1 loss.
In the experiments, I have some concerns: (1)	The paper uses RoBERTa for the experiments, while the baselines used BERT. It may be unfair. 
(2)	Why the Baseline outperforms Yu et al. (2020) on ACE 04 and ACE 05 greatly, but has comparable performance on OntoNotes 5 and CoNLL 2003? Yu et al. (2020) also conducted experiments on the GENIA dataset, but the paper doesn’t report the results on this dataset. 
(3)	According to the Table 4, the improvement of BS against LS is marginal.  Some statements are not very accurate. For example, Line 313, the submission says “this work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.”, however, Shen et al. 2021 proposed a span-based approach previously, and conducted the NER experiment on Chinese Weibo dataset. "
ARR_2022_181_review,"This paper proposes a smoothing regularization technique for NER. The technique proposed targets boundary smoothing where the authors claim that inconsistent boundaries are often seen as a problem in annotated NLP NER datasets. The results after applying the method shows less over-confidence, better model calibration, flatter neural minima and more smoothed loss landscapes which plausibly explain performance improvement rather than directly/only addressing the inconsistent boundary span labeling problem in NER. The result attributes are empirically verified. ","1. After 95% performance ranges, bringing improvements to any problem systematically is certainly a challenging dilemma. In this regard, the work proposed in this paper is already notable.
2. The problem of inconsistent span annotations identified in this paper indeed can also be intuited as an NLP annotation problem hence is a valid one that needs to be addressed. The solution to regularize this problem with smoothing technique for span-based NER method is sound and well justified throughout the paper. The reader would benefit from reading the theory and methods in this paper.
3. The code is publicly released.
4. The empirical analysis is well performed. I especially appreciate section 5.2 which directly tries to shed light on the question that formed the problem premise of this work and I quote ""How does boundary smoothing improve the model performance?"" It offers a slight indication of a negative result as in not being able to quantitatively verify it owing to irregularities in distribution between synthesized boundary noise and actual noise in the datasets. Nevertheless, it shows another promising result with smoothed loss landscapes indicating sound machine learning settings.
5. The fact that boundary smoothing addresses the over-confidence issue of target span predictions is quite meaningful, hence again a plus for this paper. ","Not quite a weakness of this work, just a question I am wondering about. Considering that the boundary smoothing regularization distributes itself around the entity span, is it that the actual problem of whether such a function addresses the boundary irregularity problem in the NLP datasets can never be verified? It is indeed understood from the method explanation in the paper that it can to some extent. ",Table 5. Note the caption. There are no results for LS in the table. ,"5 = Top-Notch: This paper has great merit, and easily warrants acceptance in a *ACL top-tier venue.",Yes,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"Not quite a weakness of this work, just a question I am wondering about. Considering that the boundary smoothing regularization distributes itself around the entity span, is it that the actual problem of whether such a function addresses the boundary irregularity problem in the NLP datasets can never be verified? It is indeed understood from the method explanation in the paper that it can to some extent. 
Table 5. Note the caption. There are no results for LS in the table. "
ARR_2022_43_review,"This paper attempts to address potential catastrophic forgetting, data imbalance and rare word issues, for intent detection in the medical domain. These three concerns are addressed with appropriate losses (L_pl, L_fl, L_co; Sections 3.2/3.3). Evaluations were performed against prior models, for various numbers of classes. ","Strengths of the paper include its addressing of several longstanding issues in incremental learning, including the rare word problem particularly relevant to the medical domain. ","Some concerns remain over the empirical evaluation, as detailed in the comments. ","1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to ""randomly stor[e] a few examples of old classes"" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.
Basically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?
In summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.
3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that ""The performance drops with removing Lco (""w/o CO""). It demonstrates that it is helpful to handle medical rare words"" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.
Minor grammatical/spelling concerns: (Line 97) ""Medica intent"" (Line 124) ""Contrastive Reply Networks"" (reply or replay?) 
(Line 161) ""...top n example(s)"" etc. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",N/A ,"Some concerns remain over the empirical evaluation, as detailed in the comments. 
1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to ""randomly stor[e] a few examples of old classes"" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.
Basically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?
In summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.
3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that ""The performance drops with removing Lco (""w/o CO""). It demonstrates that it is helpful to handle medical rare words"" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.
Minor grammatical/spelling concerns: (Line 97) ""Medica intent"" (Line 124) ""Contrastive Reply Networks"" (reply or replay?) 
(Line 161) ""...top n example(s)"" etc. "
ARR_2022_138_review,"This paper proposes a new problem of information extraction, text-to-table. Given a text, the model is required to produce the corresponding tables, including both schema and cell values. Two baselines have been developed for this task: a traditional information extraction baseline (relation extraction/name entity recognition); vanilla seq2seq model finetuned from pretrained language models. Apart from that, the authors propose two techniques, table constraint and relation embeddings for the seq2seq model to further boost the performance. The datasets are adapted from four existing table-to-text datasets, i.e., Rotowire, E2E, WikiTableText and WikiBio. The evaluation metric is calculated based on the number of correct non-header cells in the tables. Extensive experiments have been conducted, where the proposed method can beat the two baselines on average. ","1. This paper introduces a novel task, text-to-table. It is a reverse task of the original table-to-text task. It could benefit some downstream tasks like text mining. It feels like a good alternative to OpenIE. 
2. The authors propose two simple techniques, i.e., table constraint and relation embeddings, on top of seq2seq models for this task. Specifically, table constraint forces the rows to contain the same number of cells as the header. Relation embeddings are injected into the self-attention mechanism of the transformer to guide the model to specifically attend to the corresponding header when generating a cell value. The proposed methods can help beat the vanilla seq2seq baseline in experiments. 
3. The paper is clearly written except for several typos. ","1. The problem formulation is flawed. The authors argue that one major motivation of this paper is to learn schema in a data-driven way other than laborious manual schema engineering. However, on the proposed four datasets, I feel that the schemas are somehow easy to learn. For example, on E2E, WikiTableText and WikiBio, only two columns are involved. According to my understanding, there is no discrepancy between training and testing w.r.t. the table schema. Things on the fourth dataset, Rotowire, feel more complicated. There are two tables, containing around 5 and 9 columns respectively. It's not clear whether a difference exists between training and testing. And the evaluation metric only considers the cell values while ignoring the schema. That's another reason I feel the schema is fairly easy to learn in the proposed setting. Furthermore, under the current setting, this problem sounds more like another type of machine reading that the model is asked to fill in the table as the schema is easy to predict. So I highly suggest considering schema generalization where the schemas differ between training and testing and additionally evaluating schema prediction. 
2. It's not surprising that the major performance contribution comes from the pretrained language model (BART). But compared to that, the gain obtained from the proposed method is marginal (Table 5). ","1. In Section 3 (line 247-252), I am wondering tables are divided into three types. For me, one type (the column header) should work. 
2. Several typos exist in the paper. 
3. Like I suggest in the weaknesses, more exploration of schema learning and generalization will make this paper much stronger and more interesting. Another workaround is to rephrase the motivation to make this paper focus more on the non-header part. ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",n/a ,"1. The problem formulation is flawed. The authors argue that one major motivation of this paper is to learn schema in a data-driven way other than laborious manual schema engineering. However, on the proposed four datasets, I feel that the schemas are somehow easy to learn. For example, on E2E, WikiTableText and WikiBio, only two columns are involved. According to my understanding, there is no discrepancy between training and testing w.r.t. the table schema. Things on the fourth dataset, Rotowire, feel more complicated. There are two tables, containing around 5 and 9 columns respectively. It's not clear whether a difference exists between training and testing. And the evaluation metric only considers the cell values while ignoring the schema. That's another reason I feel the schema is fairly easy to learn in the proposed setting. Furthermore, under the current setting, this problem sounds more like another type of machine reading that the model is asked to fill in the table as the schema is easy to predict. So I highly suggest considering schema generalization where the schemas differ between training and testing and additionally evaluating schema prediction. 
2. It's not surprising that the major performance contribution comes from the pretrained language model (BART). But compared to that, the gain obtained from the proposed method is marginal (Table 5). 
1. In Section 3 (line 247-252), I am wondering tables are divided into three types. For me, one type (the column header) should work. 
2. Several typos exist in the paper. 
3. Like I suggest in the weaknesses, more exploration of schema learning and generalization will make this paper much stronger and more interesting. Another workaround is to rephrase the motivation to make this paper focus more on the non-header part. "
ARR_2022_138_review,"This paper poses a problem setting named text-to-table, as a new way of information extraction. They study it as an inverse problem of table-to-text (a well-studied problem). To realize this goal,  the author formalizes this task as a seq2seq problem via equipping BART with two extensions: table constraint and table relation embeddings. Experiment on four popular table-to-text dataset reveals the effectiveness of proposed methods (a little marginal). ","1. It proposes an interesting task named text-to-table to realize effective information extraction. 
2. The challenging issues discussed in Sec 5.6 and Appendix. F is very clear. ","1. The paper need further polish to make readers easy to follow.
2. In Table 6, the improvement of method is marginal and unstable.
3. The motivation of this new task is not strong enough to convince the reader.  Is it a necessary intermediate task for document summarization and text mining (as stated in L261)?
4. It directly reverse the table-to-text settings then conducts the experiments on four existing table-to-text datasets. More analysis of the involved datasets is required, such as the number of output tables, the size/schema of the output tables. ","Questions: 1. L68-L70, Is there any further explanation of the statement ""the schemas for extraction are implicitly included in the training data""?
2. How to generate the table content that not shown in the text?
3. Why not merge Table 1 and Table 2? They are both about the statistics of datasets used in experiments.
4. What’s the relation between text-to-table task and vanilla summarization task?
5. How to determine the number of output table(s)? Appendex. C don't provide an answer about this.
6. What’s the version of BART in Table3 and Table 4?
Suggestions: 1. The font size of Figure 2 and Figure 3 is too small.
Typos: 1. L237: Text-to-table -> text-to-table 2. L432: ""No baseline can be applies to all four datasets"" is confusing.
3. Table 3: lOur method -> Our method ",2.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The paper need further polish to make readers easy to follow.
2. In Table 6, the improvement of method is marginal and unstable.
3. The motivation of this new task is not strong enough to convince the reader.  Is it a necessary intermediate task for document summarization and text mining (as stated in L261)?
4. It directly reverse the table-to-text settings then conducts the experiments on four existing table-to-text datasets. More analysis of the involved datasets is required, such as the number of output tables, the size/schema of the output tables. 
Questions: 1. L68-L70, Is there any further explanation of the statement ""the schemas for extraction are implicitly included in the training data""?
2. How to generate the table content that not shown in the text?
3. Why not merge Table 1 and Table 2? They are both about the statistics of datasets used in experiments.
4. What’s the relation between text-to-table task and vanilla summarization task?
5. How to determine the number of output table(s)? Appendex. C don't provide an answer about this.
6. What’s the version of BART in Table3 and Table 4?
Suggestions: 1. The font size of Figure 2 and Figure 3 is too small.
Typos: 1. L237: Text-to-table -> text-to-table 2. L432: ""No baseline can be applies to all four datasets"" is confusing.
3. Table 3: lOur method -> Our method "
ARR_2022_222_review,"This paper presents a data augmentation with ranking-based learning for open relation extraction. In particular, the authors argued the following contributions: 1. three types of data augmentations, including back translation as positive, entity replacing as hard negative, entity swap for semi-hard negative 2. ranking-based loss function with both euclidean distance in the feature space and KL divergence in the label space. 
The proposed model shows the state of the art results on T-REx SPO and T-REx DS ","1. Results over two datasets show significant improvements over recent state-of-the-art models. 
2. Paper contains extensive results for a short paper and is easy to read. ","1. This paper proposed three different types of data augmentation methods. Both hard negative and semi-hard negatives focus on replacing the entity mentions. However, it is not well-explained why this can help to cluster the relational expression. Does this lead to a model which focuses on entity mention detection rather than relation patterns? From my understanding, replacing entity mentions of the same context should not largely change the relation semantics.
2. I think back translation plays a key role in this paper. I think it is necessary to add an ablation study to exclude the hard and semi-hard negatives. In particular, how does the model perform when both formula 1 and 2 only include t and t^p and use in-batch negatives (similar to contrastive learning). ","1. It is not clear to me why hard negatives and semi-hard negatives are generated from the positive sentence rather than the original sentence. Wouldn't there exist error propagation if the back translation is noisy? 
2. It seems that T-REx is a distantly supervised dataset with no human labels. am I correct? If so, is it possible to evaluate over annotated dataset? ( For example TACRED or FewRel?) ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. This paper proposed three different types of data augmentation methods. Both hard negative and semi-hard negatives focus on replacing the entity mentions. However, it is not well-explained why this can help to cluster the relational expression. Does this lead to a model which focuses on entity mention detection rather than relation patterns? From my understanding, replacing entity mentions of the same context should not largely change the relation semantics.
2. I think back translation plays a key role in this paper. I think it is necessary to add an ablation study to exclude the hard and semi-hard negatives. In particular, how does the model perform when both formula 1 and 2 only include t and t^p and use in-batch negatives (similar to contrastive learning). 
1. It is not clear to me why hard negatives and semi-hard negatives are generated from the positive sentence rather than the original sentence. Wouldn't there exist error propagation if the back translation is noisy? 
2. It seems that T-REx is a distantly supervised dataset with no human labels. am I correct? If so, is it possible to evaluate over annotated dataset? ( For example TACRED or FewRel?) "
ARR_2022_343_review,"The paper discusses the use of character-level machine translation: although recent work on this direction shows advantages over subword-level models, character-level models are rarely used in practice in the WMT shared tasks, and the authors attribute it to higher higher computational costs. The paper presents and extensive literature survey and an empirical evaluation of three character-level architectures, two of which have not been used in MT before. Experimental findings include: character-level models are less susceptible to the beam search curse; character-level models are more robust to source noise but generally underperform the subword models; sampling-based decoding methods produce poor quality outputs with character-level models. The paper also presents a two-step decoding method which is comparable to subword-level decoders in terms of efficiency, but reports poor translation quality. ","The paper is very well-written and accessible to the NLP audience beyond the MT community. It raises an important question of the practical advantages and disadvantages of character-level approaches in MT, and presents a thorough literature survey, analysis of recent shared task submissions, and extensive empirical evaluation. My expertise in MT is limited, but the methodology looks well-thought-out to me and the experimental setup seems robust. ","1. I would have liked to see more discussion and empirical analysis of how typological differences between languages affect the relative performance of subword- and character-level models. The claims of the paper are quite broad, but there is not enough evidence to convince me that the conclusions hold generally across languages. More specifically:    * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.
    * The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.
2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section). ","- I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].
- I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?
- I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?
- Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.
- In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training.
[1] Pourdamghani, Nima, and Kevin Knight. "" Deciphering related languages."" EMNLP 2017.
[2] He, Junxian, et al. ""A Probabilistic Formulation of Unsupervised Text Style Transfer."" ICLR 2019.
[3] Ryskina, Maria, et al. ""Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction."" SIGMORPHON 2021. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",5 = They could easily reproduce the results.,,"1. I would have liked to see more discussion and empirical analysis of how typological differences between languages affect the relative performance of subword- and character-level models. The claims of the paper are quite broad, but there is not enough evidence to convince me that the conclusions hold generally across languages. More specifically:    * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.
    * The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.
2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section). 
- I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].
- I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?
- I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?
- Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.
- In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training.
[1] Pourdamghani, Nima, and Kevin Knight. "" Deciphering related languages."" EMNLP 2017.
[2] He, Junxian, et al. ""A Probabilistic Formulation of Unsupervised Text Style Transfer."" ICLR 2019.
[3] Ryskina, Maria, et al. ""Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction."" SIGMORPHON 2021. "
ARR_2022_343_review,This paper is about character-level machine translation. The first part is a survey of 1) existing methods to enhance character-level translaiton and 2) methods used for WMT submissions in the past. The second part are experiments on character-level MT that compare different ways of encoding and decoding sequences of characters. ,"1) Comprehensive literature review: - I believe that the survey of existing literature is very comprehensive and covers most important works in this area. This survey in itself is valuable for the MT research community.
2) Experiments have reasonable methodology: - In my opinion, the techniques that are compared (e.g. Lee-style, Canine, etc.) are adequately chosen.
- The evaluation procedures (choice of metrics, several training runs to report variance, confidence intervals) are also adequate.
- I think that the large systems trained on WMT data for EN-CS and EN-DE are representative of the capabilities of current systems.
3) The findings are novel and actionable: - The paper does indeed provide novel evidence for why WMT submissions rarely have character-level systems.
- While previous works have claimed that character-level MT has higher domain robustness, this paper shows that this is not the case, and the findings are backed up by more reliable data.
- A further general conclusion is that most works on char-level MT focused on enhancing encoding, while decoding is under-explored. I think that is a reasonable and straightforward suggestion for future work. ","1) Questionable usefulness of experiments on small datasets - As the paper itself states in the beginning, a possible weakness of earlier works is that their experiments were conducted on small datasets. In such cases it is unclear whether conclusions also apply to current MT systems trained on large datasets.
- This criticism also applies to this paper under review, since many experiments are conducted using IWSLT data. I would like the paper to at least acknowledge this weakness.
- It is questionable whether the outcome of the IWSLT experiments can be used to sub-select experimental conditions to try on the larger WMT data sets.
2) The paper is too dismissive of MBR decoding, without much evidence - The paper implies that MBR is always worse than other decoding methods and that ""beam search with length normalization is the best decoding algorithm"".
- I would change the wording to be more careful about describing the MBR results. Sampling-based MBR decoding is very new in MT and has a lot of potential to be more optimized in the future. It is simply not well optimized yet. For instance, very recent published works such as https://arxiv.org/abs/2111.09388 show that MBR improves considerably if a learned metric like COMET is used as the utility function for MBR.
- I also take issue with this sentence: ""Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding that leads to much worse results than beam search."" I believe that the quality of sampling is not necessarily indicative of the ability of MBR to pick a good translation from a pool of samples. ","1) Suggestion for the general literature review: - The only kind of work I would add are proposals to learn an ideal segmentation (instead of fixing the segmentation before starting the training). One paper I think would make sense is: https://www.cl.uni-heidelberg.de/~sokolov/pubs/kreutzer18learning.pdf (also has character-level MT in the title).
2) Questions: - How many samples are the ""sample"" results in Table 2 based on? I believe this could be a point estimate of just 1 random sample. It is expected that the quality of one sample will be low, but the variance will also be very high. It would be better to show the mean and standard deviation of many samples, or at least clarify how exactly this result is produced.
3) Figures and presentation: - I believe that the color scheme in tables is confusing at times. In Table 1, I think it is confusing that a deeper shade of red means better results. In Table 2 it is non-intuitive that the first row is CHRF and the second row is COMET - and the table is quite hard to read.
4) Typos: - ""and thus relatively frequent occurrence of out-of-vocabulary tokens."" - a word is missing here - ""The model shrinks character sequences into less hidden states"" -> ""fewer hidden states"" - ""does not apply non-linearity"" -> ""does not apply a non-linearity"" ",4.5 ,No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"1) Questionable usefulness of experiments on small datasets - As the paper itself states in the beginning, a possible weakness of earlier works is that their experiments were conducted on small datasets. In such cases it is unclear whether conclusions also apply to current MT systems trained on large datasets.
- This criticism also applies to this paper under review, since many experiments are conducted using IWSLT data. I would like the paper to at least acknowledge this weakness.
- It is questionable whether the outcome of the IWSLT experiments can be used to sub-select experimental conditions to try on the larger WMT data sets.
2) The paper is too dismissive of MBR decoding, without much evidence - The paper implies that MBR is always worse than other decoding methods and that ""beam search with length normalization is the best decoding algorithm"".
- I would change the wording to be more careful about describing the MBR results. Sampling-based MBR decoding is very new in MT and has a lot of potential to be more optimized in the future. It is simply not well optimized yet. For instance, very recent published works such as https://arxiv.org/abs/2111.09388 show that MBR improves considerably if a learned metric like COMET is used as the utility function for MBR.
- I also take issue with this sentence: ""Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding that leads to much worse results than beam search."" I believe that the quality of sampling is not necessarily indicative of the ability of MBR to pick a good translation from a pool of samples. 
1) Suggestion for the general literature review: - The only kind of work I would add are proposals to learn an ideal segmentation (instead of fixing the segmentation before starting the training). One paper I think would make sense is: https://www.cl.uni-heidelberg.de/~sokolov/pubs/kreutzer18learning.pdf (also has character-level MT in the title).
2) Questions: - How many samples are the ""sample"" results in Table 2 based on? I believe this could be a point estimate of just 1 random sample. It is expected that the quality of one sample will be low, but the variance will also be very high. It would be better to show the mean and standard deviation of many samples, or at least clarify how exactly this result is produced.
3) Figures and presentation: - I believe that the color scheme in tables is confusing at times. In Table 1, I think it is confusing that a deeper shade of red means better results. In Table 2 it is non-intuitive that the first row is CHRF and the second row is COMET - and the table is quite hard to read.
4) Typos: - ""and thus relatively frequent occurrence of out-of-vocabulary tokens."" - a word is missing here - ""The model shrinks character sequences into less hidden states"" -> ""fewer hidden states"" - ""does not apply non-linearity"" -> ""does not apply a non-linearity"" "
ARR_2022_21_review,"The authors present an approach for the task of discourse dependency parsing (DDP), i.e. the task of identifying the structure and relationship between EDUs in a document. They explore different contextualized representations for DDP in a Sentence-First parsing framework, where a complete discourse tree is built up sentence by sentence. In addition, the authors propose a novel method for relation identification that exploits the writing patterns that people typically use to organize discourses. Experiments show that the proposed approaches outperform the state of the art on an English and a Chinese dataset.
CONTRIBUTIONS: (1) The authors formulate the task of relation identification in a novel sequence labeling paradigm to take advantage of the inherent structural information in the discourse. 
(2) The authors develop an approach for contextualized EDU representations to dynamically capture the information needed for the DDP task at different text granularity levels. 
(3) The authors show empirical success of their approach. ","- Overall, the paper is clear in its objectives and methodology followed. The work is well structured, easy to read and follow.
- The approach is well motivated and addresses a problem that is relevant to the community.
- The proposed approach is tested with reasonable models and appropriate experiments. The experimental results are promising, demonstrating the effectiveness of the proposed method. ","- Lack of illustrative examples regarding the model outputs.
- In the evaluation, I'm missing a detailed error analysis. It would be very enlightening to add a manual error analysis to figure out the most prevalent mistakes of the different approaches. ","- l. 25/26: ""... can benefit many downstream examples"" List/Cite some examples.
- l. 40: ""... previous studies have shown the benefit..."" Citation?
- l. 419: stmodel udy -> model study - l. 567: We -> we ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Lack of illustrative examples regarding the model outputs.
- In the evaluation, I'm missing a detailed error analysis. It would be very enlightening to add a manual error analysis to figure out the most prevalent mistakes of the different approaches. 
- l. 25/26: ""... can benefit many downstream examples"" List/Cite some examples.
- l. 40: ""... previous studies have shown the benefit..."" Citation?
- l. 419: stmodel udy -> model study - l. 567: We -> we "
ARR_2022_155_review,"This paper analyzes roles of image and text in the problem of claim detection, which is often the first stage of fake news detection. It introduces a novel dataset MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86 000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. It also evaluates strong unimodal and multimodal baselines, and analyzes the potential and drawbacks of current models. ","It introduces more fine-grained labels, the tertiary claim classes and visual claim classes, in multimodal claim detection, which is the first step of fake news detection.  Also, they add 2 more topics compared to previous work. ","1. The paper doesn't show how tertiary claim classes and visual claim classes can help in fake news detection.  So it leads to the question, why do we care the proposed label classes in claim detection? 
2. The annotated dataset size is limited for the use of largely used heavy multi-modal models. And while claims are related to 3 topics, they are still limited. 
3. Experiments include fine-tuning on ALBEF but not fine-tuning on CLIP. 
4. The dataset has conflicts in annotated data. Why not adding more annotators? ","1. Try to show the value of the dataset -- to show the use of tertiary claim classes and visual claim classes can actually help in fake news detection performance. Also, it will be interesting to see whether using the classification results on visual claim classes can help the classification performance on tertiary claim classes. 
2. Increase the number of annotators each data to avoid conflicts. If possible, annotate more data. 
3. In experiment, it can also do fine-tuning on CLIP. ",2.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1. The paper doesn't show how tertiary claim classes and visual claim classes can help in fake news detection.  So it leads to the question, why do we care the proposed label classes in claim detection? 
2. The annotated dataset size is limited for the use of largely used heavy multi-modal models. And while claims are related to 3 topics, they are still limited. 
3. Experiments include fine-tuning on ALBEF but not fine-tuning on CLIP. 
4. The dataset has conflicts in annotated data. Why not adding more annotators? 
1. Try to show the value of the dataset -- to show the use of tertiary claim classes and visual claim classes can actually help in fake news detection performance. Also, it will be interesting to see whether using the classification results on visual claim classes can help the classification performance on tertiary claim classes. 
2. Increase the number of annotators each data to avoid conflicts. If possible, annotate more data. 
3. In experiment, it can also do fine-tuning on CLIP. "
ARR_2022_155_review,The authors investigated the roles of image and text at the first stage of the fake news detection pipeline. ,objective and Data analysis parts are stronger. ,"From the title of this manuscript, it seems that the authors mainly contributed to the dataset.  The authors mostly applied conventional approaches like SVM. ",I think it will be better if the authors highlight their methodology as well. It will be better if the authors work on advanced AI algorithms such as deep learning. ,"3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,The authors should take proper permission of using any images. The permission details should be mentioned in the manuscript. ,"From the title of this manuscript, it seems that the authors mainly contributed to the dataset.  The authors mostly applied conventional approaches like SVM. 
I think it will be better if the authors highlight their methodology as well. It will be better if the authors work on advanced AI algorithms such as deep learning. "
ARR_2022_321_review,"This paper proposes a bidimensional leaderboard, or Billboard, that measures progress both on generation tasks and automatic evaluation of NLG simultaneously. The authors observe that while much progress has been made on automatic evaluation metrics, NLG modeling papers often still report very old metrics like BLEU and ROUGE. The Billboard is an attempt to bring the best practices of the metrics and modeling communities into one place. Generation models submitted to the Billboard would automatically get evaluated on new metrics as they are submitted, and a global ensemble metric (chosen to maximize correlation with human judgments) would also update as metrics are submitted, thus updating the ranking of generation systems. The paper presents Billboards for translation, summarization, and image captioning, as well as initial results on which metrics contribute most to the best ensemble metric.
This is an excellent idea and I think could be very impactful for the community. The paper is well-written and thorough, for example conducting a meta-analysis of which human annotations to use when learning the global ensemble metric. I see no significant risks in accepting this paper. ","- Great idea to organize generation and metrics into a single joint leaderboard - Has potential to improve community practices, in particular making it easier to use better evaluation metrics when writing generation papers - Insights into which existing metrics today perform best, in terms of correlation with human judgments - Paper is well-written and clear ","- Less of a weakness, but I do slightly disagree with the authors' focus on instance-level rather than system-level correlation (see below) ","In Section 2.4, the authors argue that metrics should be ranked in terms of instance-level judgments. While I acknowledge the clear advantages of this, particularly related to sample size, I would encourage the authors to also include system-level scores in the leaderboard. Ultimately, in the other half of the leaderboard, metrics will be used to make system-level comparisons. Even if there is high instance-level correlation, there could be different patterns at the system-level, for example if a metric has a small but consistent bias for or against a particular system's outputs. I wonder if perhaps there is some sort of compromise way to rank metrics that incorporates both instance-level and system-level scores. ",4.5 ,Yes,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Less of a weakness, but I do slightly disagree with the authors' focus on instance-level rather than system-level correlation (see below) 
In Section 2.4, the authors argue that metrics should be ranked in terms of instance-level judgments. While I acknowledge the clear advantages of this, particularly related to sample size, I would encourage the authors to also include system-level scores in the leaderboard. Ultimately, in the other half of the leaderboard, metrics will be used to make system-level comparisons. Even if there is high instance-level correlation, there could be different patterns at the system-level, for example if a metric has a small but consistent bias for or against a particular system's outputs. I wonder if perhaps there is some sort of compromise way to rank metrics that incorporates both instance-level and system-level scores. "
ARR_2022_321_review,"This paper proposes a leaderboard to join the progress reporting of the two important factors in automatic text generation: the output of generation models and the metrics used to evaluate the output. The authors propose a leaderboard were both model outputs and metrics can be submitted. The strength of submitted metrics is analyzed by their complementarity to human-based metrics. The current version of the leaderboard includes three downstream tasks, including machine translation, summarization and image captioning. Such a multidimensional leaderboard is a good idea especially for tasks such as language generation, where more than one criterion is important to be evaluated. ","- Design choices are based on an extensive case study that emphasises the gap between model developers and the use of recently proposed evaluation metrics.
- New submitted metrics are aligned to human judgements rather than to ROUGE.
- Reproducibility to previous versions is ensured.
- The paper takes in account the differences between experts and crowd-workers.
- The paper is well structured and written clearly. ","- Number of instances in a test set: Did the authors analyze if this affects the rankings?
- Section 3: There are no baselines presented for summarisation and image captioning.
- It does not become clear why automatic metrics tend to overrate machine-generated text? ",Line 473: missing word - “depending on the downstream task of interest” ,4.5 ,Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"- Number of instances in a test set: Did the authors analyze if this affects the rankings?
- Section 3: There are no baselines presented for summarisation and image captioning.
- It does not become clear why automatic metrics tend to overrate machine-generated text? 
Line 473: missing word - “depending on the downstream task of interest” "
ARR_2022_97_review,"This paper decomposes the concept-to-text natural language generation task into three stages, (i) lexicalization, (ii) recurrent aggregation and (iii) post-editing.  The intent is to enable large pre-trained models to better leverage transfer learning.  The approach is evaluated on several datasets including Schema-Guided Dialogue, MultiWoZ and WebNLG. ","- Decomposing text generation from meaning representations into distinct stages is a constructive, and potentially impactful approach.
- Results are presented on four datasets with varying degrees of training examples, and ablations were performed.
- Approach appears to consistently perform better on the Missing Slot Error metric. ","- Several important aspects of the approach are underspecified: How are the meaning representations segmented into individual facts (Section 2.1)?  What are the inputs/outputs to each stage of the system (Sections 2.2 to 2.4)?  How are the lexicalization, aggregation and post-editing tasks performed (e.g., is this just a T5_small model)?
- The development and exact form of the loss function (Eq 1) is unclear.
- The label inference process seems spurious (Section 2.6).  Some examples and statistics would be helpful. ","The work seems to be presented as an engineering task and provides limited insight into the fundaments of how decomposing the task help language models to consume meaning representations to generate text.  In particular, exploring the potential relationship between lexicalization and missing slot error would have made for a much stronger paper.  Also the level of specificity in Section 2 obscured the exact nature of the approach.
The terms s_x and v_x are not clearly defined.  Are these slots and value bindings, respectively?  If so, how are is each derived? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"- Several important aspects of the approach are underspecified: How are the meaning representations segmented into individual facts (Section 2.1)?  What are the inputs/outputs to each stage of the system (Sections 2.2 to 2.4)?  How are the lexicalization, aggregation and post-editing tasks performed (e.g., is this just a T5_small model)?
- The development and exact form of the loss function (Eq 1) is unclear.
- The label inference process seems spurious (Section 2.6).  Some examples and statistics would be helpful. 
The work seems to be presented as an engineering task and provides limited insight into the fundaments of how decomposing the task help language models to consume meaning representations to generate text.  In particular, exploring the potential relationship between lexicalization and missing slot error would have made for a much stronger paper.  Also the level of specificity in Section 2 obscured the exact nature of the approach.
The terms s_x and v_x are not clearly defined.  Are these slots and value bindings, respectively?  If so, how are is each derived? "
ARR_2022_97_review,"This paper proposed a hierarchical recurrent aggregation framework to reduce the few-shot NLG task. The proposed framework consists of three modules, lexicalization, aggregation, and post-editing, which are composed of PLMs and do not share parameters. The experimental results show that the proposed method is slightly better than the baseline under low-resource settings. ","The method performs better than the compared baseline under low-resource settings, especially for MER metrics. ","1. ** Poor efficiency**. The proposed method contains three different modules, each containing an unshared PLM so that the amount of parameters is three times that of the End2End method. As T5 is good at multi-task learning, I think the author should consider how to improve the performance of the model in the case of parameter sharing, rather than simply stacking multiple models. 
2. ** Time-consuming**. The proposed method requires recurrent generation so that the speed of response generation will be significantly reduced, especially when the amount of attribute-value pairs is relatively large. It should be noted that the NLG model is only a part of the dialogue system of the pipeline structure, and time efficiency also needs to be taken into consideration. 
3. ** Lack of strong baselines**. The baseline for comparison is only E2E-T5, which is a naive E2E-NLG method and may not be strong enough. Therefore, the improvement of the proposed method over a single T5 model(E2E-T5) may not be a very valuable conclusion.  I think the author should investigate and compare more few-shot NLG methods to enhance the persuasiveness of the experimental results. 
4. Lack of some additional insight into the effectiveness analysis for three modules. 
5. The innovations in this paper are more like tricks for improving metrics. ","**Suggestions**: See above.
**Questions**: In table 1, The MER indicators of +aggregation and +post-edit are both higher than 1.1, while the MER of +selection is very low. From Line.402-Line.405, the selection is selecting the one with lower MER from aggregation and post-edit. Why the best MER result is reduced to 0.14 after selection ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. ** Poor efficiency**. The proposed method contains three different modules, each containing an unshared PLM so that the amount of parameters is three times that of the End2End method. As T5 is good at multi-task learning, I think the author should consider how to improve the performance of the model in the case of parameter sharing, rather than simply stacking multiple models. 
2. ** Time-consuming**. The proposed method requires recurrent generation so that the speed of response generation will be significantly reduced, especially when the amount of attribute-value pairs is relatively large. It should be noted that the NLG model is only a part of the dialogue system of the pipeline structure, and time efficiency also needs to be taken into consideration. 
3. ** Lack of strong baselines**. The baseline for comparison is only E2E-T5, which is a naive E2E-NLG method and may not be strong enough. Therefore, the improvement of the proposed method over a single T5 model(E2E-T5) may not be a very valuable conclusion.  I think the author should investigate and compare more few-shot NLG methods to enhance the persuasiveness of the experimental results. 
4. Lack of some additional insight into the effectiveness analysis for three modules. 
5. The innovations in this paper are more like tricks for improving metrics. 
**Suggestions**: See above.
**Questions**: In table 1, The MER indicators of +aggregation and +post-edit are both higher than 1.1, while the MER of +selection is very low. From Line.402-Line.405, the selection is selecting the one with lower MER from aggregation and post-edit. Why the best MER result is reduced to 0.14 after selection "
ARR_2022_240_review,"This paper proposed a new PLM-pruning method based on knowledge distillation, aiming to resolve the overfitting problem under the pretrain-and-finetune paradigm. In the beginning, the authors present a hypothesis that model pruning increases the risk of overfitting at the fine-tuning phase as well as some empirical results that demonstrate this hypothesis (Fig. 2). Then they proposed a knowledge distillation method to resolve such problems for PLM-pruning. The theoretical analysis shows that such a method can make the difference between the predictions of teacher and student so small with high probability (PAC).  The empirical results on the GLUE benchmark also show the effectiveness of the proposed method and achieve the best results compared with a range of baselines.
----------Overall comments-----------   Overall, the method of PLM-pruning via knowledge distillation by sparse progressive is novel and interesting. Both theoretical and empirical results show the effectiveness of the proposed. The main concern to me are the significance and correctness of the theoretical analysis. ","1.  This paper is well-written and well organized. In particular, the motivation is clear and the conclusion makes sense.
2. The method of sparse progressive distillation for PLM pruning is novel and technical sound.
3. The theoretical analysis can demonstrate the sparse progressive distillation can be well optimized for PLM pruning to some extend. This can make the work much solid.
4. The experimental results are strong on a well-studied benchmark, GLUE, and the analysis are also extensive and thorough. ","The main concern to me is about the theoretical analysis, which seems too brief to reveal the intrinsic mechanism of the proposed methods, details are listed as follows.
- The corollary in line 245 assumes that $\mathbf{w}^*_{i}$ $(1 \leq i \leq n)$ are drawn i.i.d. from a distribution on $[-1, 1]$. Is this. i.i.d. assumption holds for the proposed networks? Besides, I not that the range of $\mathbf{W}$, $(-0.5, 0.5)$ is not equal to that of $\mathbf{w}^*$'s. Could this difference effect that derivations?
-  In the proposed method, the authors claim that $N$ sparse student modules have probabilities of $p_1, p_2, p_3, ..., p_N$ to substitute the corresponding teacher layers separately. But these probabilities have not been considered in the theoretical derivations. I guess these can affect the results.
- The nonlinear transformations in the neural works, such as $exp$ and $tanh$ have not been considered in the theoretical derivations. To my understanding, these are non-trivial results and should not be omitted to obtain the Eq. (10) directly.
Another concern is why the reported results are all on the dev set but not on the test set, which is uncommon or maybe I have missed some thing. ","---------Suggestions---------   1. The detailed derivations of corollary and the derivations of Eq. (10) are important, which should be displayed in detail in the appendix or on the main pages.  2.  The Figures and Tables can be moved on the top for better readability.  3. Significance test can be conducted for the results of Tab. 1-3.
----------Typos----------   - line 249: $\mathbf{W}$ --> $\mathbf{w}$. - Eq.(8) $c$ has not been defined.
- line 269: missing the right $|$. - The title of Tab. 3 is the same as that of Tab. 1. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",N/A ,"The main concern to me is about the theoretical analysis, which seems too brief to reveal the intrinsic mechanism of the proposed methods, details are listed as follows.
- The corollary in line 245 assumes that $\mathbf{w}^*_{i}$ $(1 \leq i \leq n)$ are drawn i.i.d. from a distribution on $[-1, 1]$. Is this. i.i.d. assumption holds for the proposed networks? Besides, I not that the range of $\mathbf{W}$, $(-0.5, 0.5)$ is not equal to that of $\mathbf{w}^*$'s. Could this difference effect that derivations?
-  In the proposed method, the authors claim that $N$ sparse student modules have probabilities of $p_1, p_2, p_3, ..., p_N$ to substitute the corresponding teacher layers separately. But these probabilities have not been considered in the theoretical derivations. I guess these can affect the results.
- The nonlinear transformations in the neural works, such as $exp$ and $tanh$ have not been considered in the theoretical derivations. To my understanding, these are non-trivial results and should not be omitted to obtain the Eq. (10) directly.
Another concern is why the reported results are all on the dev set but not on the test set, which is uncommon or maybe I have missed some thing. 
---------Suggestions---------   1. The detailed derivations of corollary and the derivations of Eq. (10) are important, which should be displayed in detail in the appendix or on the main pages.  2.  The Figures and Tables can be moved on the top for better readability.  3. Significance test can be conducted for the results of Tab. 1-3.
----------Typos----------   - line 249: $\mathbf{W}$ --> $\mathbf{w}$. - Eq.(8) $c$ has not been defined.
- line 269: missing the right $|$. - The title of Tab. 3 is the same as that of Tab. 1. "
ARR_2022_98_review,"The paper is about an unsupervised approach to sentence simplification that integrates paraphrasing into an iterative text simplification system. The authors start with an iterative framework in which an input sentence is revised using two edit operations: delete and paraphrase. For deletion, the system uses the constituency tree of the input sentence to obtain all constituents from different depths of the parse tree, and new candidate sentences are created by removing each of these phrases from the input sentence. For paraphrasing, the system uses lexically-constrained decoding, by identifying ""complex components"" in the input - which, in turn, is done by training a text complexity classifier whose attention head matrices in the second layer are used to perform the complex word identification. Candidate sentences generated by these edit operations are then filtered using a score function on the basis of grammaticality, simplicity, and semantic similarity. ","1. The approach is interesting, simple, and easy to understand - combining edit-operation-based and paraphrasing-based techniques into an iterative refinement framework. It is a hybrid between seq2seq generative modeling and explicit edit-based modeling and tries to leverage the strengths of these different groups of modeling techniques. 
2. Unsupervised system - different components (models and datasets) can be replaced, which makes the system less dependent on pairwise-annotated datasets. 
3. Interpretability and controllability - these are quite impactful characteristics of the system. ","1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets. 
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system. 
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges. 
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content. ","1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.   2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)? 
3. It will be nice to see some examples of the system on actual texts (vs. other components & models). 
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets. 
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system. 
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges. 
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content. 
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.   2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)? 
3. It will be nice to see some examples of the system on actual texts (vs. other components & models). 
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well. "
ARR_2022_272_review,The purpose of this paper is to build a system to detect negation cues and negation focus by gathering additional data and utilizing negation masking strategies. Results indicate some improvement as well as some transferability of negation across domains. ,"In order to build AugNB, the authors collected a large number of negation sentences. This large corpus might be useful for future research.   Using their proposed models (e.g., AugNB and CueNB), the authors conduct extensive experiments on a variety of datasets.   Based on the reported results, detecting negation cues and detecting focus of negation appear promising for most of the datasets. ","While the results are promising, the innovation of this paper is somewhat limited as the approaches are quite well known.   The paper indicates that only gold cue information is used for scope resolution (I. 219). Now, the question is what happens if you use predicted cues? Is it the case that a few wrong cues can give devastating results on focus detection?  More details should be there for the approach ""negation-focused data collection"" approach. It seems additional cues are collected only from a biomedical domain. Therefore, the model AugNB can be biased to perform better on the biomedical datasets only. 
Further, there should be more information on CueNB model as well. How is the masking objective utilized on top of AugNB? ",Numbers in Tables 2 and 3 look more crowded. I would round to one decimal point in these tables. ,2.5 ,No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)","2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"While the results are promising, the innovation of this paper is somewhat limited as the approaches are quite well known.   The paper indicates that only gold cue information is used for scope resolution (I. 219). Now, the question is what happens if you use predicted cues? Is it the case that a few wrong cues can give devastating results on focus detection?  More details should be there for the approach ""negation-focused data collection"" approach. It seems additional cues are collected only from a biomedical domain. Therefore, the model AugNB can be biased to perform better on the biomedical datasets only. 
Further, there should be more information on CueNB model as well. How is the masking objective utilized on top of AugNB? 
Numbers in Tables 2 and 3 look more crowded. I would round to one decimal point in these tables. "
ARR_2022_168_review,"The paper discusses the introduction into direct speech translation models of contrastive learning methods that aim at reducing the difference between the encoded representations of speech and text with the same content. The main goal is to increase the transfer learning benefits of cross-modal systems, trained both with textual and audio data, and hence obtaining better performance especially in ST, leveraging large parallel textual corpora, which are easily available on the contrary of ST data. The effectiveness of the proposed approach is validated in different data conditions, comparing it with several previous works and with a thorough analysis targeted to validate the claim that similarity of the representations of the same sentences with different modalities is actually increased. ","The approach proposed in the paper is interesting and the results reported are promising. The main strength of the paper in my opinion is the analysis section on the modality, which demonstrates that the proposed method actually brings the benefits it was introduced for. Moreover, it motivates a possible future adoption of the proposed architecture in application scenarios where the similarity between the representations of textual and audio versions of the same content is needed. ","The main problem of the paper in my opinion is related to the data Augmentation: I have not been able to understand exactly which are the data augmentation methods used, and their impact on the scores (line 461-470 were quite obscure to me) and how this affects the comparison with other methods. In other words, the higher scores obtained with respect to other works cited in Table 1 are due to the contrastive learning method or to the different data augmentation methods? The comparison with some of the reported works is already not fair due to the different data condition (the usage of external speech data). It would have been better to compare with the other architectures/works repeating their experiments with the same data conditions (including data augmentation methods).
The reproducibility of this work would not be easy, since the code is not open sources and some details of the experimental settings are not very clear:  - line 184-186: the 2 convolutions are said to have stride 4, and that in total they shrink the input by a factor of 4. The two claims are contrasting. I guess the 2 convolutions actually have stride 2. 4 is the kernel size maybe? 
 - line 236-251: the same letters (u and v) are used for different things and v is sometimes treated as a variable, sometimes as a function. This section could be more clear with a simpler notation. 
 - line 342-344: Why using 10k as vocabulary size? Most of the works in literature (and also cited in table 1) use 8k (eg. Wang et al. 2020a), as also suggested in https://aclanthology.org/2020.amta-research.13/. In addition, it is not clear to me on which data the vocabulary was built: on MuST-C or on the WMT corpus? ","Why did you introduce a parallelism between neural networks and the human brain in the introduction? This parallelism is questionable, as there are contrasting feelings and opinions about it. I am not sure it is adding anything to the motivation of the work, but it may rise concerns.
In the related works, among the many multi-task frameworks the addition of the CTC loss is not cited.
In lines 370-374, the paper claims that MT performance is not affected, but there is no evidence of it in the paper: the MT results are missing.
In the caption of table 2, what does strong mean? There are stronger cascade systems (e.g. you cited Bentivogli et al., 2020, whose cascade scores 28.8).
In line 152 the citation format is not correct. 
In line 162, ""in"" -> ""into"". 
In line 172, ""fout"" -> ""four"" In line 264 I guess a part of the sentence is missing: how is the contrastive loss computed? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"The main problem of the paper in my opinion is related to the data Augmentation: I have not been able to understand exactly which are the data augmentation methods used, and their impact on the scores (line 461-470 were quite obscure to me) and how this affects the comparison with other methods. In other words, the higher scores obtained with respect to other works cited in Table 1 are due to the contrastive learning method or to the different data augmentation methods? The comparison with some of the reported works is already not fair due to the different data condition (the usage of external speech data). It would have been better to compare with the other architectures/works repeating their experiments with the same data conditions (including data augmentation methods).
The reproducibility of this work would not be easy, since the code is not open sources and some details of the experimental settings are not very clear:  - line 184-186: the 2 convolutions are said to have stride 4, and that in total they shrink the input by a factor of 4. The two claims are contrasting. I guess the 2 convolutions actually have stride 2. 4 is the kernel size maybe? 
 - line 236-251: the same letters (u and v) are used for different things and v is sometimes treated as a variable, sometimes as a function. This section could be more clear with a simpler notation. 
 - line 342-344: Why using 10k as vocabulary size? Most of the works in literature (and also cited in table 1) use 8k (eg. Wang et al. 2020a), as also suggested in https://aclanthology.org/2020.amta-research.13/. In addition, it is not clear to me on which data the vocabulary was built: on MuST-C or on the WMT corpus? 
Why did you introduce a parallelism between neural networks and the human brain in the introduction? This parallelism is questionable, as there are contrasting feelings and opinions about it. I am not sure it is adding anything to the motivation of the work, but it may rise concerns.
In the related works, among the many multi-task frameworks the addition of the CTC loss is not cited.
In lines 370-374, the paper claims that MT performance is not affected, but there is no evidence of it in the paper: the MT results are missing.
In the caption of table 2, what does strong mean? There are stronger cascade systems (e.g. you cited Bentivogli et al., 2020, whose cascade scores 28.8).
In line 152 the citation format is not correct. 
In line 162, ""in"" -> ""into"". 
In line 172, ""fout"" -> ""four"" In line 264 I guess a part of the sentence is missing: how is the contrastive loss computed? "
ARR_2022_313_review,"This paper proposes a prompt-based data augmentation pipeline, PromDA, targeting to improve model performance for low-resource NLU with high quality synthetic data. PromDA  adds soft prompt to public pretrained language models for data generation and further apply consistency filtering to formulate a iterative model improvement pipeline. From the experiment results, PromDA shows great performance boosting to the baseline model and competitive performance compared to other few-shot NLU methods.  Overall, this paper is well-written and contains comprehensive experiment results and analysis. However, it lacks some details/discussions on the pipeline settings: 1. PromDA uses T5-Large for data augmentation and BERT-base for NLU model. It would be interesting to include the comparison of using different pretrained langauge model for data augmentation to understand the benefits of model size in this pipeline. Especially, except LAMBADA and BACK T, all other previous methods seem not using model larger than BERT-base in their pipelines. If PromDA could consistently outperform the baseline and other models with smaller data augmentation model (e.g. BERT-base), it will further justify the effectiveness of proposed pipeline.
2. The iterative consistency filtering might propagate errors in the synthetic data especially with high number of iterations. Is there any mechanism used to reduce such error propagation? And more analysis and insights to the optimal number of iterations would be helpful. ","1. Interesting intuition of applying prompt-based language model for data augmentation. 
2. Comprehensive experiment results and analysis. ",1. Lack of some key comparisons for the pipeline settings. ,My major suggestion is to have some analysis on the effect of model size for data augmentation. ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Lack of some key comparisons for the pipeline settings. 
My major suggestion is to have some analysis on the effect of model size for data augmentation. "
ARR_2022_167_review,"This paper proposes an LM pre-training method for bi-encoder models. The formal definition of their pre-training task includes: 1. the pre-training task is the same as the task definition used in extractive QA models 2. the model architecture is the bi-encoder architecture. First, one encoder computes contextualized vector representation of tokens in the given passage. Next, the other encoder computes vector representation of the given question. Then the model predicts the start and end indices by computing the dot-product similarity between the question vector and the token vectors. 
3. The pre-training dataset is generated jointly using question generation and a student-teacher framework. The generated question-answer pairs are then used for the pre-training.
This paper evaluates their pre-trained encoder on five downstream tasks: extractive QA, zero-shot paraphrase ranking, few-shot paragraph classification, few-shot named entity recognition, zero-shot sentiment analysis. The results show that the proposed pre-training method is sufficient to be used by bi-encoders in zero/few-shot settings. ","1. The importance of bi-encoder in QA and other NLP fields is increasing. This paper proposes a novel pre-training approach for bi-encoder models. 
2. The experimental results of this paper show the efficacy of their pre-training method. 
3. This paper shows an interesting finding on their proposed pre-training method; in the zero-shot setting, pre-training bi-encoders with the proposed pre-training method is better than using cross-encoder     1. Evidence: Table2 (QUIP + cross-encoder vs QUIP)     2. Evidence: Table 4 (QUIP + Cross-student vs QUIP)     3. Evidence: Table 5 4. This paper provides prompt methods for using their pre-trained encoder to downstream tasks. ","The motivation described in this paper is not well aligned with some experimental designs of this paper. For example, it is unclear why some downstream tasks are in zero/few-shot settings. ",It would be great if the authors provided experimental analysis on the reason bi-encoders outperform cross-encoders when the encoders are pre-trained on their method. ,3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The motivation described in this paper is not well aligned with some experimental designs of this paper. For example, it is unclear why some downstream tasks are in zero/few-shot settings. 
It would be great if the authors provided experimental analysis on the reason bi-encoders outperform cross-encoders when the encoders are pre-trained on their method. "
ARR_2022_167_review,"This paper propose a new pre-training objective for train Bi-encoder based QA model. The main claim is that the proposed pre-training objectives helps in improving the model's performance on non-QA downstream tasks like paraphrase detection, sentiment analysis etc. without extensive finetuing.  The main hypothesis is that the proposed way of pre-training helps the model in learning better token level representation needed for zero shot and few shot tasks. ","Synthetically generated questions have been nicely used for pre-training QA model to improve performance on tasks like Sentiment analysis, paraphrase detection in a zero shot, few shot setup. A good analysis of using QA as the pre-training objective rather than MLM. ","The paper has incremental or limited novelty as the question generation from passages using BART is already a known, Bi-encoder for QA already exists. 
Contributions of the paper are quite trivial. 
Bi-encoder+Unsupervised QA didn't perfrom great. So whether choosing QA as the pre-training is helping QA task or not is not very clear. 
what is the motivation behind independently encoding questions and passage is not very clear. isn't that a shared common encoder would help the model learning better question tokens to passage token matching? 
For QA task, It seems that the cross-encoder + MRQA is doing better than proposed QUIP. ","Highlight the best numbers in table 7.  - Have a strategy to filter out best quality question from the set of synthetically generated questions.
- May be design a curriculum learning paradigm for pre-training so that the batch sampling is done in a manner that helps the pre-training learn from easy to complex questions. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The paper has incremental or limited novelty as the question generation from passages using BART is already a known, Bi-encoder for QA already exists. 
Contributions of the paper are quite trivial. 
Bi-encoder+Unsupervised QA didn't perfrom great. So whether choosing QA as the pre-training is helping QA task or not is not very clear. 
what is the motivation behind independently encoding questions and passage is not very clear. isn't that a shared common encoder would help the model learning better question tokens to passage token matching? 
For QA task, It seems that the cross-encoder + MRQA is doing better than proposed QUIP. 
Highlight the best numbers in table 7.  - Have a strategy to filter out best quality question from the set of synthetically generated questions.
- May be design a curriculum learning paradigm for pre-training so that the batch sampling is done in a manner that helps the pre-training learn from easy to complex questions. "
ARR_2022_167_review,"This paper proposes a pre-training objective based on QA, aiming at learning general-purpose contextual representations. They use BART model to generate QA pairs with context passages and use a cross-encoder RoBERTa to relabel the examples. After that, bi-encoder model is trained to match the cross-encoder predictions on the generated questions. And this paper conducts many experiments on multiple datasets of four downstream tasks. ","1. The experimental effect is considerable under four different downstream tasks. 
2. Experiments require a lot of computing resources and time. ","1. The article is very trivial and didn't write a paragraph to summarize the contribution of the paper, which makes it difficult to focus on the core ideas. 
2. It is suggested to combine pictures and other forms to assist the description, which will be clearer to understand. 
3. Will the code and dataset of this paper be published? ",Please see my comments and questions in the core review. ,"3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. The article is very trivial and didn't write a paragraph to summarize the contribution of the paper, which makes it difficult to focus on the core ideas. 
2. It is suggested to combine pictures and other forms to assist the description, which will be clearer to understand. 
3. Will the code and dataset of this paper be published? 
Please see my comments and questions in the core review. "
ARR_2022_13_review,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics. The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems. ",-- thorough experimentation -- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation -- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system ,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)  -- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes ",-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words -- but I realize that that would make the paper yet longer again ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)  -- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes 
-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words -- but I realize that that would make the paper yet longer again "
ARR_2022_13_review,"This paper presents DiBiMT, a new test suite for semantic biases in machine translation. It is modelled on previous test suites such as ContraWSD and MuCoW, but combines manually annotated and curated data with decent multilingual coverage. The dataset creation is explained in detail, and it is then applied to evaluate five machine translation tools, with an extensive analysis of the results. ","- The paper introduces a useful dataset for MT evaluation.
- The paper provides an extensive analysis of existing MT systems on the basis of the dataset. ","- Some design choices could have been justified in more detail and explained with more examples - The formalization is hard to read at times, with multiple Greek letters and subscripts for somewhat easy-to-grasp concepts - Multilingual coverage could of course be better, but the current limitation is understandable and acceptable given the large amount of manual work involved ","- L100: Such > This - L182-188: an example would be welcome - §3.2.1: I would have liked to see a discussion on the properties of these examples. Are they shorter/longer than the sentences seen in the average MT training corpora? Are they similar in style/genre/domain? Are these invented examples or are they taken from e.g. news sources? How big is the risk of finding these sentences in the training corpora?
- L232: why not 2? I don't think that lemmas with only 2 senses are too easy... - L265-268: I would move this paragraph to §3.3.2. The current order suggests that §3.3.1 is done manually as well.
- L293: Do I understand correctly that as soon as a sentence fails in one of the 5 languages it is discarded? Doesn't this place too harsh of a restriction on sentence selection? It probably doesn't hurt if the number of sentences is not exactly identical for all language pairs, or does it?
- L349-353: What is the connection of the Isabelle et al. paper with DeepL? How certain is it that the Johnson et al. paper reliably depicts Google's current production system?
- Figure 2: I would find it more intuitive to report the MISS percentages in a table, analogously to Table 3. Do the MISS percentages correlate with the accuracy figures? Or does it happen that systems with low MISS percentages are penalized for it by low accuracy figures?
- Figure 3: If the red bars are parts of the grey bars, you could stack them instead of having two separate bars.
- Table 4 and 5 would be more readable if they were split into two tables each, to have one table per measure. E.g. first put the 8 SFII columns and then the 8 SPDI columns rather than alternating between them.
- §4.5: I didn't understand this section. An example could help.
- §4.7: I don't understand why and how you need manual annotation here. The original dataset contains source sentences and good+bad target words. So you should be able to retrieve a translation from your five systems, change the target word, and have a quick manual check for agreement etc. Why do you need to translate the entire sentence manually?
- Table 7: no need for decimal places if you only have 100 examples - Do you have an idea/conjecture why DeepL is so much better? Is this purely a data curation issue, or could this be due to some architectural changes? It's of course quite frustrating to see that somebody has basically solved the problem but doesn't say how :) - References: Please make sure that abbreviations and language names are capitalized.
- Appendix: What does ""Back to Model-specific Analyses."" mean?
- Appendix: Figure 5 has a column ""%mistakes"", but the other figures have ""Accuracy"". Does this mean that the mean accuracy of DeepL is actually 40% instead of 60%? Please check.
- Appendix, Table 2: What does the slash in Russian and Chinese mean? Is there no second lexicalization? Why not just leave that line empty? ","5 = Top-Notch: This paper has great merit, and easily warrants acceptance in a *ACL top-tier venue.",Maybe,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Some design choices could have been justified in more detail and explained with more examples - The formalization is hard to read at times, with multiple Greek letters and subscripts for somewhat easy-to-grasp concepts - Multilingual coverage could of course be better, but the current limitation is understandable and acceptable given the large amount of manual work involved 
- L100: Such > This - L182-188: an example would be welcome - §3.2.1: I would have liked to see a discussion on the properties of these examples. Are they shorter/longer than the sentences seen in the average MT training corpora? Are they similar in style/genre/domain? Are these invented examples or are they taken from e.g. news sources? How big is the risk of finding these sentences in the training corpora?
- L232: why not 2? I don't think that lemmas with only 2 senses are too easy... - L265-268: I would move this paragraph to §3.3.2. The current order suggests that §3.3.1 is done manually as well.
- L293: Do I understand correctly that as soon as a sentence fails in one of the 5 languages it is discarded? Doesn't this place too harsh of a restriction on sentence selection? It probably doesn't hurt if the number of sentences is not exactly identical for all language pairs, or does it?
- L349-353: What is the connection of the Isabelle et al. paper with DeepL? How certain is it that the Johnson et al. paper reliably depicts Google's current production system?
- Figure 2: I would find it more intuitive to report the MISS percentages in a table, analogously to Table 3. Do the MISS percentages correlate with the accuracy figures? Or does it happen that systems with low MISS percentages are penalized for it by low accuracy figures?
- Figure 3: If the red bars are parts of the grey bars, you could stack them instead of having two separate bars.
- Table 4 and 5 would be more readable if they were split into two tables each, to have one table per measure. E.g. first put the 8 SFII columns and then the 8 SPDI columns rather than alternating between them.
- §4.5: I didn't understand this section. An example could help.
- §4.7: I don't understand why and how you need manual annotation here. The original dataset contains source sentences and good+bad target words. So you should be able to retrieve a translation from your five systems, change the target word, and have a quick manual check for agreement etc. Why do you need to translate the entire sentence manually?
- Table 7: no need for decimal places if you only have 100 examples - Do you have an idea/conjecture why DeepL is so much better? Is this purely a data curation issue, or could this be due to some architectural changes? It's of course quite frustrating to see that somebody has basically solved the problem but doesn't say how :) - References: Please make sure that abbreviations and language names are capitalized.
- Appendix: What does ""Back to Model-specific Analyses."" mean?
- Appendix: Figure 5 has a column ""%mistakes"", but the other figures have ""Accuracy"". Does this mean that the mean accuracy of DeepL is actually 40% instead of 60%? Please check.
- Appendix, Table 2: What does the slash in Russian and Chinese mean? Is there no second lexicalization? Why not just leave that line empty? "
ARR_2022_13_review,"This paper presents a completely manually curated evaluation benchmark for semantic biases in Machine Translation for 5 language pairs. 7 SOTA MT commercial and non-commercial systems are tested on the testbed and detailed analysis of the results is given. The paper also presents 4 novel metrics for the task. The analysis includes details on how challenging the dataset is, and the impact of the encoder and decoder. The benchmark will be released. ","- the benchmarking testbed for several major language pairs is a useful contribution for the research community - the paper has presented several well-motivated experiments - testing with DeepL Translator and Google Translate provides results, useful for the translation community.
- the better results of DeepL Translator are interesting for the translation community. ","1. Although the translators may be highly professional, different translators usually have different choices, so producing annotations by using 1 translator per language does not seem enough. 
2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used. 
3. It would have been good to have the number of items per language pair when providing the statistics of the annotated dataset. ","The example on the first page sounds like figurative use of the word ""to march"". This is an important aspect of mismatched translations. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",no issues ,"1. Although the translators may be highly professional, different translators usually have different choices, so producing annotations by using 1 translator per language does not seem enough. 
2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used. 
3. It would have been good to have the number of items per language pair when providing the statistics of the annotated dataset. 
The example on the first page sounds like figurative use of the word ""to march"". This is an important aspect of mismatched translations. "
ARR_2022_105_review,"This paper proposed a unified framework to handle reference-only, source-only and source-reference-combined evaluation tasks. They propose monotonic regional attention and unified pretraining to better adapt multi-task training. The experiments show the superiority of their methods across tasks. ","This paper proposes a unified framework for translation evaluation, which can be adopted into reference-only, source-only and source-reference-combined tasks. With Monotonic Regional Attention, they unified the inputs in these tasks. Then, they synthesis the labels of data in a rank-based manner. Experimental results and details in ablation studies show the superiority of their methods. ","1)	The data labeling methods transform the prediction score in a ranked way. However, different evaluation approaches may produce different order. In this paper, only one evaluation approaches are used to generate the prediction score. 
2)	Writing should be improved as the paper is not easy to follow. ","1)	As an abbreviation, PLM in line 67 should be defined first 2)	The authors should give some intuitive case about hypothesis, source and reference in experiments 3)	The introduction of Hard MRA need improvement. I thought the monotonic means the sequence h->s->r. Because h->s is not allowed, the hard MRA forbids the h->s, s->r, and h->r 4)	Line081, learning-xbased should be learning-based ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1)	The data labeling methods transform the prediction score in a ranked way. However, different evaluation approaches may produce different order. In this paper, only one evaluation approaches are used to generate the prediction score. 
2)	Writing should be improved as the paper is not easy to follow. 
1)	As an abbreviation, PLM in line 67 should be defined first 2)	The authors should give some intuitive case about hypothesis, source and reference in experiments 3)	The introduction of Hard MRA need improvement. I thought the monotonic means the sequence h->s->r. Because h->s is not allowed, the hard MRA forbids the h->s, s->r, and h->r 4)	Line081, learning-xbased should be learning-based "
ARR_2022_210_review,"This seems to be a primarily theoretical ML paper that describes the _softmax bottleneck_ problem in neural LMs, which results from the fact that a single hidden state embedding cannot be close to the embeddings of all possible candidate next words under certain conditions. The paper then proposes an improvement for the traditional softmax use in neural LMs, namely the _multi-facet softmax_ (MFS). This is compared to an existing MoS approach of Yang et al., as well as some other baselines. The modeling experiments show that the described problem indeed occurs in the artificially designed data sets, and that the proposed MFS solution solved this problem. In addition, MFS yields better results than the baselines in terms of language modeling perplexity and in the task of answering ambiguous questions. ","The paper describes what seems to me like an important issue that may affect most/many of the existing LMs, and the results seem to be consistent across the presented tasks. ","The paper is very dense, with a lot of theoretical material, may be more suitable for a ML conference or a journal rather than *ACL conference. Having said that, I lack expertise in ML and may not have a good feel for where this kind of studies belong. ","- Line 036 - ""probabilities of becoming the next word"" reads awkward to me - Line 046 - should be ""man"" instead of ""king""?
- Line 166 - typo, ""thoery"" - Line 214 - theorem 2 could benefit from providing an intuitive explanation - Line 280 - typo, ""there existS"" ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The paper is very dense, with a lot of theoretical material, may be more suitable for a ML conference or a journal rather than *ACL conference. Having said that, I lack expertise in ML and may not have a good feel for where this kind of studies belong. 
- Line 036 - ""probabilities of becoming the next word"" reads awkward to me - Line 046 - should be ""man"" instead of ""king""?
- Line 166 - typo, ""thoery"" - Line 214 - theorem 2 could benefit from providing an intuitive explanation - Line 280 - typo, ""there existS"" "
ARR_2022_210_review,"This paper discusses a known problem regarding output distributions over words. 
The problem originates from using a single hidden state vector and global word vectors for dot product and softmax calculations. 
This leads to the model not being able to generate arbitrary distribution over words using inference.
Understanding the problem from a geometrically motivated perspective, the authors propose ""multi-facet softmax (MFS)"". 
Compared to the previous methods, MFS essentially introduces two improvements: multiple input hidden states and multiple partitions on the vocabulary.
The authors then move on to explain several experiments using GPT2 and BERT (found in the appendix) models, focusing on language modeling tasks. 
The comparison across different softmax enhancement methods shows that MFS brings noticeable perplexity improvements at a cost of slightly more parameters and inference time similar to that of the other methods, when compared to the softmax baseline. ","The paper explains the limitation of expressiveness of the output generation modules of our neural language models. 
Although the problem is known before, and sometimes referred to as ""softmax bottleneck"", the authors were able to provide their insights on the issue.
The proposed method MFS makes use of multiple input hidden states and multiple partitions on the vocabulary, and can be viewed as a softmax enhancement, which is backed by improved perplexity numbers.
The virtue of the paper, from my point of view, has to come from the unified view of all the considered softmax enhancement methods, i.e. the configuration section in Table 1. 
I think this is a very nice addition to the current literature in unifying the methods and providing a systematic view of what people have tried to overcome the aforementioned problem. ","While debatable, my biggest concern with the paper, or this line of work in general, is if the direction is really worth it.
Let me explain myself.
The paper begins with the famous ""king, man, queen, women"" example and motivates the work by saying the proper next word following the ""After debating whether to bow to the woman or the king first, the jester decided on the"" context should be ""woman"" and ""king"", instead of ""queen"" and ""king"". 
This ""golden answer"" is already questionable. 
The model learned from other contexts that ""queen"" could be a candidate here and in some sense, ""queen"" is not such a bad guess for who the ""woman"" is. 
In other words, the geometrical structure of words is probably what we want the model to learn in the first place.
Jumping out from the example and we could discuss the issue in a broader sense. 
The ""softmax bottleneck"" originates from the inner dimension of the dot product, i.e. the hidden dimension, being much smaller than the vocabulary size. 
If we really want to model arbitrary output distributions, then why not just define a fixed set of orthogonal word vectors and let the hidden states do the magic? 
The limitation of the memory size restricts us to using a hidden dimension size smaller than the vocabulary size, and this results in the low rank of the log probability matrix, and this results in a series of complicated methods that try to overcome the bottleneck. 
If one looks at the numbers, is 15.64 vs 15.89 PPL (GPT-2 Medium in Table 1) really worth the effort?
I know this point is quite subjective and it is definitely not personal against the authors. 
I simply want to raise a bit of concern with the direction we are going in. 
The authors did a good job in providing a unified view of the softmax enhancement methods (including theirs of course), and it is the direction in which the methods move that I have a doubt about. ","Maybe expand on the hyperparameter choices a bit more. 
How should one decide on the number of softmaxes, input hidden states and partitions? 
How did you do it?
L166, theory.
L280, there exists... L351, why 0.4 epochs? 
Which 40% of the training data is used? 
How is it decided?
L425, the reference to Tab.2 in the main text happens after the reference to Tab. 3.
The following papers could be nice additions to the reference: https://arxiv.org/abs/1412.6623 https://arxiv.org/abs/1704.08424 https://arxiv.org/abs/1705.08039 https://arxiv.org/abs/1806.04313 https://arxiv.org/abs/1910.12554 ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"While debatable, my biggest concern with the paper, or this line of work in general, is if the direction is really worth it.
Let me explain myself.
The paper begins with the famous ""king, man, queen, women"" example and motivates the work by saying the proper next word following the ""After debating whether to bow to the woman or the king first, the jester decided on the"" context should be ""woman"" and ""king"", instead of ""queen"" and ""king"". 
This ""golden answer"" is already questionable. 
The model learned from other contexts that ""queen"" could be a candidate here and in some sense, ""queen"" is not such a bad guess for who the ""woman"" is. 
In other words, the geometrical structure of words is probably what we want the model to learn in the first place.
Jumping out from the example and we could discuss the issue in a broader sense. 
The ""softmax bottleneck"" originates from the inner dimension of the dot product, i.e. the hidden dimension, being much smaller than the vocabulary size. 
If we really want to model arbitrary output distributions, then why not just define a fixed set of orthogonal word vectors and let the hidden states do the magic? 
The limitation of the memory size restricts us to using a hidden dimension size smaller than the vocabulary size, and this results in the low rank of the log probability matrix, and this results in a series of complicated methods that try to overcome the bottleneck. 
If one looks at the numbers, is 15.64 vs 15.89 PPL (GPT-2 Medium in Table 1) really worth the effort?
I know this point is quite subjective and it is definitely not personal against the authors. 
I simply want to raise a bit of concern with the direction we are going in. 
The authors did a good job in providing a unified view of the softmax enhancement methods (including theirs of course), and it is the direction in which the methods move that I have a doubt about. 
Maybe expand on the hyperparameter choices a bit more. 
How should one decide on the number of softmaxes, input hidden states and partitions? 
How did you do it?
L166, theory.
L280, there exists... L351, why 0.4 epochs? 
Which 40% of the training data is used? 
How is it decided?
L425, the reference to Tab.2 in the main text happens after the reference to Tab. 3.
The following papers could be nice additions to the reference: https://arxiv.org/abs/1412.6623 https://arxiv.org/abs/1704.08424 https://arxiv.org/abs/1705.08039 https://arxiv.org/abs/1806.04313 https://arxiv.org/abs/1910.12554 "
ARR_2022_210_review,"This paper highlights a limitation of conventional softmax classifiers that use a single low rank fixed weight matrix in the softmax layer to compute logits via dot products from a single contextual embedding. This issue is also known as ""the softmax bottleneck"" The limitation is that not all rankings of vocabulary items by probability can be represented and the authors highlight a consequence of this limitation with the following example. 
They show that there are slot filling templates where two proposed alternatives are clearly expressed in the text. 
While we would expect the language model to assign the two largest probabilities to the two alternatives, they show that this is not always possible and sometimes the model must resort to ranking other words not explicitly stated between the two alternatives. 
After providing a theoretical proof of the above limitation, the authors propose improving a model (Mixture of Softmax MoS) previously proposed to alleviate representational limitations for low rank softmax layers in two directions:   a) they create logits from multiple representation of the inputs (facets)   b) they partition the vocabulary to enable more expressive distributions over subsets of the vocabulary. 
They provide empirical evidence that their approach can provide improvements in perplexity in Language Models and top-k accuracy for Question Answering. ","The paper is well written.
The main idea is clearly expressed and the paper highlights theoretical insights and how they apply to real world examples.
The related work section is adequate, albeit incomplete (though I agree the missing references are very difficult to find).
Modest performance improvement over existing methods.
The paper brings awareness about how big language models may not necessarily distribute their probabilities in a way humans might anticipate. ","1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. ","There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption:  middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,"2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)",1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. 
There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption:  middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" "
ARR_2022_210_review,"The paper tackles a fundamental problem of linear layer followed by a softmax. Due to the particular parameterization, there is a problem termed softmax bottleneck, where the degree of freedom for the output word distribution is limited by the dimension of the hidden vector. Building on top of this observation, the paper formally shows that when some word embeddings are linearly dependent, it becomes impossible to produce certain ranking of words. To overcome this problem, special output layers or parameterization needs to be designed. One potential solution is to use a mixture of softmax (MoS), but the paper further argues that MoS is still not expressive enough, and attribute the limitation to the reuse of a single hidden vector. The paper then proposes to use multiple hidden vectors to compute mixture of softmax, and term the approach multi-facet softmax due to the use of multiple projections. Results show that multi-facet softmax can improve over the regular mixture of softmax. ",The paper is easy to follow. The approach is very well motivated. The figures are helpful for understanding the different approaches and the proposed approach. ,"The paper really struggles to find a case where the proposed approach is better than the regular MoS. The improvement on natural language seems to be small, and the paper needs to work on a synthetic language to show the gap between MoS and the proposed approach. The paper then looks into a QA task where the uncertainty is high to showcase the benefits of the proposed approach, but the gain seems to be small. Either natural language does not exhibit the variability that fits the theory, or the problem of modeling is not a lack of uncertainty in the output distribution but requires something else.
The use of the term bimodal suddenly appears in section 5, and the use of the terms hints that it is the multimodality of the distribution that causes the problem. Indeed, this has been pointed out in Chris Bishop's seminal paper, Mixture Density Networks. However, the connection between multimodality problem and the theory in the paper is missing. It would be much better to draw the connection in section 2, and signpost a little in section 5.
There are minor presentation issues. For example, it is not until section 2.2 did I start to understand why the problem stated in the introduction exists. it would be much better to explain the problem in words in the introduction. As another example, the figure contains too many colors, and the colors don't particularly mean anything and are a bit distracting. ","I would talk about theorem 1 informally in the introduction. I would also remove unnecessary colors in the figures.
Theorem 2 reads a bit abstract, so it would be much better to relate the theorem with the running, queen-king-woman-man example.
I also think that the argument about the limitation of MoS, in the second paragraph of section 3.2, is a little dense. It might be better to add more redundancy to that paragraph, using different ways to explain the same concept. ",4.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The paper really struggles to find a case where the proposed approach is better than the regular MoS. The improvement on natural language seems to be small, and the paper needs to work on a synthetic language to show the gap between MoS and the proposed approach. The paper then looks into a QA task where the uncertainty is high to showcase the benefits of the proposed approach, but the gain seems to be small. Either natural language does not exhibit the variability that fits the theory, or the problem of modeling is not a lack of uncertainty in the output distribution but requires something else.
The use of the term bimodal suddenly appears in section 5, and the use of the terms hints that it is the multimodality of the distribution that causes the problem. Indeed, this has been pointed out in Chris Bishop's seminal paper, Mixture Density Networks. However, the connection between multimodality problem and the theory in the paper is missing. It would be much better to draw the connection in section 2, and signpost a little in section 5.
There are minor presentation issues. For example, it is not until section 2.2 did I start to understand why the problem stated in the introduction exists. it would be much better to explain the problem in words in the introduction. As another example, the figure contains too many colors, and the colors don't particularly mean anything and are a bit distracting. 
I would talk about theorem 1 informally in the introduction. I would also remove unnecessary colors in the figures.
Theorem 2 reads a bit abstract, so it would be much better to relate the theorem with the running, queen-king-woman-man example.
I also think that the argument about the limitation of MoS, in the second paragraph of section 3.2, is a little dense. It might be better to add more redundancy to that paragraph, using different ways to explain the same concept. "
ARR_2022_71_review,"This paper presents two approaches for reducing the storage costs in late interaction ranking models. First, the term representations are auto-encoded using ""side information"" (representing the static embedding for the term). A quantization process further reduces the size of the representations on disk. The approach is evaluated on MS MARCO, TREC DL, and CAR and shows that it can effectively reduce the storage burden while having a minimal impact on ranking effectiveness. ","- The auto-encoding strategy of using the static embedding as ""side information"" to reduce the size of the representations is clever, novel, and practical. I could easily see this strategy becoming common practice when using these models. 
 - The approach can dramatically reduce the storage requirements of late interaction models while having minimal impact on ranking effectivness. 
 - The approach is parameterized, allowing it to easily scale to various operating points (trading off storage for performance). ","- Generally, the evaluation is rather ""narrow"" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large). 
 - The evaluation on CAR appears to have been conducted using ""automatic"" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided. 
 - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims. 
 - It would be nice to have seen this work on other base models-- particularly a T5-based model. ","- This work may feel more ""at home"" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well. 
 - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Generally, the evaluation is rather ""narrow"" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large). 
 - The evaluation on CAR appears to have been conducted using ""automatic"" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided. 
 - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims. 
 - It would be nice to have seen this work on other base models-- particularly a T5-based model. 
- This work may feel more ""at home"" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well. 
 - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer? "
ARR_2022_200_review,"This paper proposes an explicit future information utilization framework to improve monotonic attention mechanisms in simultaneous machine translation. Specifically, the authors try several approaches to integrate the future information from language modeling into the multi-head monotonic attention. Instead of using future information from language modeling directly in the output layer of the attention model, they transform the monotonic attention mechanism to explicitly use the future information. Several experiments show some improvement over the SoTA multi-head monotonic attention models. ",This paper proposes an approach to incorporate information from language modeling models into simultaneous neural machine translation. The ideas are novel and the experiments show some improvement over the baseline. ,We can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. ,"If you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"We can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. 
If you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. "
ARR_2022_361_review,The paper introduces a large pretrained Hebrew language model based on BERT objectives and architecture. The model has the largest vocabulary  trained on the largest Hebrew corpus to date. The authors also propose a pipeline evaluation for morphologically rich languages and show state-of-the-art performance of their model. ,"The authors have put effort in creating a large training dataset, as well as bringing several evaluation tasks together to measure progress for Hebrew NLP. The strong Hebrew language model and the evaluation pipeline would be useful for the NLP community. ","- It lacks novelty that I would expect from a long research paper. It is read more like a report, rather than a research paper.  - The paper could be restructured and some parts could be better written. Please see comments and suggestions for more details.  - Some contributions are not clear, e.g., which datasets were available, which are released (what kind of postprocessing is done)? hard to digest this from text. Some techniques that could be a contribution remain somehow hidden in lines 425-462, and some are not clear.     - Most of the evaluation tasks are already well-known, but had been executed separately. Morpheme-based tasks are more relevant to multilingual probing literature (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) which are not mentioned throughout the paper. ","- 091: I don't understand how this is relevant to the paper. It is mentioned a couple of times however I haven't seen such an effort, maybe I'm missing something?  - 096: typo: MLR -> MRL - 121: Not clear if you propose these tasks or only combine them into a pipeline? You could write your contributions as bullet points.  - 156: ""objective objectives"" sounds weird.
- Section 2 can be condensed quite a lot. 133-156 is already known, so too long. I'd be more interested to see a review on multilingual benchmarks and probing tasks, especially for morphologically rich languages. I'd rather see more details of the other Hebrew models.  - 174-176: Again sounds like the authors take GLUE (or similar benchmark) and translate the tasks into Hebrew.  - 178:181: But there is a rich literature on morphological and syntactic probing tasks (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) - Throughout the paper, the authors use the term NLU, however I'm not sure whether some of the lower level tasks count as NLU or syntactic tasks.  - 254: Oscar is introduced here but there is no description until line 276.
- 313 - footnote 2: What's the postprocessing effort here, not clear.  - 320-346: This could be a table.
- 330: Introduce SPMR, what does it stand for?
- 332: which UD version?
- Table 3: The english transcriptions are needed - footnote 4,5: what's the added value (if any)? why is 5 anonymous, didn't understand?
- 428-448: I didn't understand how you feed the subword or token representation to a char-LSTM? Doesn't your model need the ""char"" representation?
- 636-638: Language-agnostic is over claiming here.  - Why Aleph? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- It lacks novelty that I would expect from a long research paper. It is read more like a report, rather than a research paper.  - The paper could be restructured and some parts could be better written. Please see comments and suggestions for more details.  - Some contributions are not clear, e.g., which datasets were available, which are released (what kind of postprocessing is done)? hard to digest this from text. Some techniques that could be a contribution remain somehow hidden in lines 425-462, and some are not clear.     - Most of the evaluation tasks are already well-known, but had been executed separately. Morpheme-based tasks are more relevant to multilingual probing literature (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) which are not mentioned throughout the paper. 
- 091: I don't understand how this is relevant to the paper. It is mentioned a couple of times however I haven't seen such an effort, maybe I'm missing something?  - 096: typo: MLR -> MRL - 121: Not clear if you propose these tasks or only combine them into a pipeline? You could write your contributions as bullet points.  - 156: ""objective objectives"" sounds weird.
- Section 2 can be condensed quite a lot. 133-156 is already known, so too long. I'd be more interested to see a review on multilingual benchmarks and probing tasks, especially for morphologically rich languages. I'd rather see more details of the other Hebrew models.  - 174-176: Again sounds like the authors take GLUE (or similar benchmark) and translate the tasks into Hebrew.  - 178:181: But there is a rich literature on morphological and syntactic probing tasks (e.g., LINSPECTOR: Multilingual Probing Tasks for Word Representations) - Throughout the paper, the authors use the term NLU, however I'm not sure whether some of the lower level tasks count as NLU or syntactic tasks.  - 254: Oscar is introduced here but there is no description until line 276.
- 313 - footnote 2: What's the postprocessing effort here, not clear.  - 320-346: This could be a table.
- 330: Introduce SPMR, what does it stand for?
- 332: which UD version?
- Table 3: The english transcriptions are needed - footnote 4,5: what's the added value (if any)? why is 5 anonymous, didn't understand?
- 428-448: I didn't understand how you feed the subword or token representation to a char-LSTM? Doesn't your model need the ""char"" representation?
- 636-638: Language-agnostic is over claiming here.  - Why Aleph? "
ARR_2022_361_review,"**Note**: *This is only a slight revision of my previous review for a previous version of this paper. I did not re-check all the details of the paper carefully, I mostly focused on checking the parts where I had reservations towards the previous version; I simply hope that the parts which I already found good in the previous version stayed good or were improved in this version. But I found already the previous version of the paper to be very good.*
The paper describes a model called AlephBERT, which is a BERT language model for Hebrew that surpasses previous such models thanks to being trained on larger data and with better handling of the morphological richness of Hebrew. The paper also compiles together an evaluation toolkit for evaluating Hebrew language models, based on pre-existing tasks and datasets. The model and all code is planned to be released with the camera-ready version of the paper.
The paper is definitely mostly a resource paper: most of the stuff is laborious but mostly straightforward, gathering data from available sources, training a model using existing approaches, compiling a benchmarking toolkit from existing tasks and datasets, and evaluating the trained model with this toolkit. The only part which is more research-heavy is handling the rich morphology of Hebrew, where the authors experiment with introducing a morphological segmentation component into the neural setup (a task which is highly non-trivial for Hebrew).
The authors evaluate all of their contributions and prove that each of them brings improvements over the previous state of the art. ","The resources created by the authors seem to be extremely useful for nearly anyone dealing with Hebrew in NLP, as large pretrained language models are the core of most current approaches.
The approach used for handling complex Hebrew morphology is novel and potentially inspirative for other morphologically complex languages.
While I have a feeling that ACL does not prefer publishing pure resource papers, I believe that in case where the created resource is very useful, these papers should have their place at ACL. Besides, there is also a research component to the paper (although the research component itself would not suffice for a long paper).
The paper is very well written and very nice to read and easy to understand. ","I found several minor problems and uncertainties in the previous version of the paper, but the authors managed to address practically all of these in their revised version. My only remaining reservation thus is towards the claimed but not demonstrated language-agnosticity of the presented approach, which I find to be too strong a claim (or maybe I have a different understanding of what ""language agnostic"" means). ","In their response to the previous reviews, the authors list the following improvement: ""We describe the Twitter data acquisition and cleanup process."", but I have not found this improvement in the current version (but I admit I might have simply overlooked it; all I am saying is I did not find it at the places where I would expect it). ",4.5 ,No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"I found several minor problems and uncertainties in the previous version of the paper, but the authors managed to address practically all of these in their revised version. My only remaining reservation thus is towards the claimed but not demonstrated language-agnosticity of the presented approach, which I find to be too strong a claim (or maybe I have a different understanding of what ""language agnostic"" means). 
In their response to the previous reviews, the authors list the following improvement: ""We describe the Twitter data acquisition and cleanup process."", but I have not found this improvement in the current version (but I admit I might have simply overlooked it; all I am saying is I did not find it at the places where I would expect it). "
ARR_2022_61_review,"This paper proposes a method for Paraphrase Generation with an explicit control for the desired quality of the generated text. The quality of a paraphrase is defined as a 3 dimensional vector consisting of its semantic similarity with the source sentence, and lexical and syntactic diversity. The quality vector is provided as an input to the model along with the source sentence, and the model is trained to generate paraphrases of the input that express the specified quality. Since, for a given sentence, generating a paraphrase of an arbitrary quality might not be possible, the authors also propose a Quality Predictor model to estimate the typical paraphrase quality of a given sentence by training a regression model on the paraphrase dataset. The predicted quality values are added with different offset values, which can then be used to generate multiple paraphrases, and a method to obtain optimum offset values for a dataset using the dev-set is also proposed. This optimum offset value is then used to specify quality constraints for the test set examples and  accordingly the paraphrases are generated. Through their experiments, the authors show that the quality of the generated paraphrases monotonically increases with the offset specified in the input until a certain point, after which the responsiveness to the input might decrease as it becomes increasingly difficult to generate high quality paraphrases when the requested control values deviates significantly from the typical value. When compared to the baseline, their method outperforms on the three defined quality metrics (semantic similarity, lexical and syntactic diversity), i-BLEU score as well as the human judgements for semantic similarity. Their method also performs competitively with the gold standard defined using the actual paraphrases in the test set, with a better semantic similarity than the actual paraphrases for the most cases and the diversity metrics being often comparable and at times better than the ones in the ground truth test set. ","1. The paper addresses two major concerns about the existing work on controlled paraphrase generation. i) The controlled generation approaches typically require specific information like the syntax of the desired target sentence defined in terms of parse trees or POS tags, which might limit their usage for downstream tasks. In comparison, in the proposed method controlling the output quality is much more straightforward and allows the model with flexibility to generate paraphrases of the specified quality. ii) Often, in controlled generation approaches any arbitrary control specification might not be compatible with the source sentence and often in the previous work like in the case of syntactic paraphrasing, there hasn’t been much focus on obtaining these suitable control values for the inputs. The proposed method allows the selection of appropriate quality values for input sentences which help to avoid the specification of incompatible values. Both of these are important contributions to the field of paraphrase generation as well as controlled text generation. 
2. The problem is well formulated and the proposed methods are simple and intuitive. 
3. The experimental setup is thorough with respect to the datasets considered and metrics for evaluating the proposed method with baseline and the gold standard, as well as in terms of analyzing the behavior of their model under different control specifications. I also liked that the evaluation was performed by comparing the generated paraphrases with the input sentence, rather than the reference paraphrase in the test set. To me not only does this make more sense from a paraphrase evaluation perspective, but it also helps us view the reference paraphrases as a gold standard which helps to better interpret the performance of their model as well as the baseline. 
4. The paper is well written and easy to follow. ","1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. ","1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. "
ARR_2022_303_review,"This paper presents a new citation-based related work dataset, CORWA, as well as a baseline model that tags against the CORWA dataset, that is built over the S2ORC dataset, specific to NLP. The authors take the now-common, end-to-end neural network perspective in training a model to tag references but specifically contributing a model for citation-driven citation fragments. The authors decompose the task as three joint tasks of discourse tagging, citation span detection and citation type recognition. The authors also discuss a human in the loop (HITL) environment for accomplishing such work, based on their incremental, correction-based annotation of a trained transformer model in their annotation loop.
(This is a v2 submission and since the paper hasn't substantially changed, I've left the summary the same as in v1) ","-  Well written and edited. I did not find much issues with the English use and the paper reads fluently.
- Propose a new taxonomy of related work sentences (S3.1.1).
-  Surprisingly high interannotator agreement, albeit with just two annotators. However, it is not clear whether the annotators are part of the authoring team and this should be disclosed on first mention.   - Finding that reference vs dominant citation types differ in their citation span lengths and strategies to generate, and contributing the longformer-encoder-decoder model towards appropriate modeling of this goal. ","- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper.  The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses.  However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text.  Table 1's revised caption mentions 2.9K paragraphs as the size. ","This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR. 
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column.  I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work.  S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,None. ,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper.  The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses.  However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text.  Table 1's revised caption mentions 2.9K paragraphs as the size. 
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR. 
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column.  I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work.  S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base "
ARR_2022_303_review,"This paper introduces the Citation Oriented Related Work Annotation (CORWA) dataset for related work generation. The dataset contains three annotation tasks: discourse tagging, citation span detection and citation type recognition. Another main contribution is a baseline model which could tag the CORWA labels from unlabeled texts. Besides, this paper also proposes a novel framework for related work generation. ","The task of related work generation is very important in the academic research domain; The baseline model that does the joint-related work tagger is interesting as unlabeled data is used, compared with the previous version, the description and novelty are more clear. 
The CORWA dataset is a large contribution in this field which fills the gap, and the analysis and statistics are comprehensive. ","The paper organization could be improved. I understand that there should be details about the creation, evaluation and analysis of CORWA. With the space limit, some evaluation tables are moved to supplementary material. Table 6 might be more important but it is not in the main body. It is possible to make Figure 2,6 smaller to save some space. ","What is the goal for section 6? This section solves the task of full related work generation, but hard to understand how this is related to the main work.
Table 1: the width of the table borderline seems different in each row. And this table has a different style/format with other tables. ",3.5 ,No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.",,,"The paper organization could be improved. I understand that there should be details about the creation, evaluation and analysis of CORWA. With the space limit, some evaluation tables are moved to supplementary material. Table 6 might be more important but it is not in the main body. It is possible to make Figure 2,6 smaller to save some space. 
What is the goal for section 6? This section solves the task of full related work generation, but hard to understand how this is related to the main work.
Table 1: the width of the table borderline seems different in each row. And this table has a different style/format with other tables. "
ARR_2022_177_review,"This paper presents a semi-supervised method for word-level quality estimation by using sentence level QE models which are mapped to word-level QE using feature attribution (rationale extraction) methods. The core hypothesis is that sentence level quality stems from word level translation errors, thus sentence level models should store information about which individual tokens are likely to be correct/incorrect, and this information can be surfaced by feature attribution methods. This work follows recent research directions in model probing and “bertology”, applying feature attribution methods to QE.  The authors test several methods of feature attribution – empirical results show that feature attribution models trained on sentence-level data outperform a random baseline, but are significantly worse than supervised models trained on word-level QE training data, which is generated by automatically aligning tokens in MT hypotheses with their corresponding human post-edits. ","The idea of probing sentence-level models for word-level information is compelling -- the word- and sentence-level QE tasks are well-suited to testing feature attribution methods because standard training and evaluation datasets from the many iterations of the WMT QE shared tasks should allow for straightforward comparison with and leveraging of existing approaches, and the tasks are already divided into different levels of granularity. This is a useful insight from this work that certainly merits future exploration. ","The empirical evaluations in this work are difficult to interpret, and I am not sure what the key takeaways are. The metrics used in this work (AUC score, recall at K) are non-standard for the tasks under consideration, so it is difficult to interpret the results in the context of word-level QE evaluation. Although it is interesting that sentence-level QE models contain some information about probable word-level translation errors, it is unsurprising that sentence level models would learn to focus on target tokens that are likely to be wrong and I think the paper at least needs more analysis or insight into the details of what is going on to be useful for other researchers in the context of QE (see comments for some ideas).
As presented, the work cannot be framed as a new approach to word-level QE because it does not use the same evaluation metrics, and the approach cannot label tokens in a MT hypothesis as Good/Bad, so any comparison with supervised word-level QE approaches is not informative. ","Possible Analyses / experiments: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning?  - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an ""attribution budget"" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The empirical evaluations in this work are difficult to interpret, and I am not sure what the key takeaways are. The metrics used in this work (AUC score, recall at K) are non-standard for the tasks under consideration, so it is difficult to interpret the results in the context of word-level QE evaluation. Although it is interesting that sentence-level QE models contain some information about probable word-level translation errors, it is unsurprising that sentence level models would learn to focus on target tokens that are likely to be wrong and I think the paper at least needs more analysis or insight into the details of what is going on to be useful for other researchers in the context of QE (see comments for some ideas).
As presented, the work cannot be framed as a new approach to word-level QE because it does not use the same evaluation metrics, and the approach cannot label tokens in a MT hypothesis as Good/Bad, so any comparison with supervised word-level QE approaches is not informative. 
Possible Analyses / experiments: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning?  - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an ""attribution budget"" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task? "
ARR_2022_177_review,This paper describes a semi-supervised approach to machine translation (MT) quality estimation (QE) at word-level. The approach hinges on the assumption that successful QE models rely on translation errors to predict overall sentence quality. It uses the sentence-level features as a weak label for getting predictions for the words in the sentence by exploring a set of feature attribution methods. These methods assign relevance scores that explain model predictions. Overall the paper shows that explanations extracted from sentence-level QE models can be used to detect translation errors. ,"The paper introduces a novel way to approach word-level QE without having to resort to human annotation. it works losely as a semi-supervised word-level QE method using the sentence-level scores as weak labels for the word-level. This is the major contribution of the paper and its biggest strength. The main reason is because it enables practictioners and researchers of the field to have a word-level prediction without having to incur in the costs of annotating word-level data for training.
Experimental settings look sound and results indicate that rationale extraction methods are comparable to using unsupervised glass-box methods to word-level QE but inferior to supervised black-box methods (that use human-produced or human-derived labels). ","It would be great if the code is indeed released if the paper is accepted. This would help a lot with the reproducibility of the method proposed in the paper. Other than that, it is unclear how the approach described here compares with the ones proposed for the Explainable Quality Estimation shared task which propose similar approaches. If the paper is accepted it would be good to position this in relation to these approaches. ",- Any idea about the difference in performance between using XLMR-base and XLMR-large? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"It would be great if the code is indeed released if the paper is accepted. This would help a lot with the reproducibility of the method proposed in the paper. Other than that, it is unclear how the approach described here compares with the ones proposed for the Explainable Quality Estimation shared task which propose similar approaches. If the paper is accepted it would be good to position this in relation to these approaches. 
- Any idea about the difference in performance between using XLMR-base and XLMR-large? "
ARR_2022_177_review,"This paper proposes to extract word-level quality information for machine translated text from a state-of-the-art sentence-level quality estimation (QE) model, based on pre-trained contextual embeddings, fine-tuned in a semi-supervised way, i.e. using only sentence-level scores for fine-tuning. The main research question the authors focus on is, do QE models infer sentence-level translation quality scores based on word-level errors, which would provide insights on the model decision making (rationales) and alleviate the need of costly error-annotated word-level data. 
  To answer this question, an empirical work is conducted by comparing several interpretability related approaches, so-called feature attribution methods, as well as a supervised and an unsupervised approach, along with a random classification baseline. Experiments include three translation directions towards English and are performed on a publicly available dataset. Results show the superiority of two approaches, namely integrated gradients and attention weights, based on several metrics calculated on a test corpus where sentences containing only or no errors were ignored.
The analysis of these results is rather succinct, but proposes nonetheless a few interesting observation nuggets. First, more useful error-related word-level information is encoded at deeper layers of the model, second, both source and target sentences appear to have an impact on feature attribution, third, frequency of words in the machine translation training corpus and sentence-level QE seem to be in a relation of some sort. ","Overall, this paper is very well written, with a clear hypothesis and straightforward experimental setup.  The semi-supervised word-level translation errors extraction paradigm is, to the best of my knowledge, relatively novel when using contextual embedding models. The approach proposed by the authors is thus interesting for the research community and could pave the way towards more cost effective translation QE, fine-grained methods for explaining QE models outputs, etc.
While the experimental results do not necessarily emphasize which approach would allow for the best word-level QE results, which is not the scope of the study, they offer a comparison point for future work and validate the use of semi-supervised methods for word-level QE. The analysis provides a few insights on where the information is encoded in the model, as well as the importance of source and target sentences for QE modelling.
The use of publicly available resources, including the dataset and the various implementations, ensure a reasonable level of reproducibility. ","The main weaknesses of this paper are the empirical validation of the method proposed by the authors and the lack of deep analysis. The main findings are limited to applying to QE a set of existing interpretability oriented methods already applied generally to NLP tasks.  For the empirical validation of the method, the evaluation does not reflect a general scenario in machine translation where both perfect and fully erroneous translations exist. Additionally, as shown later in the analysis, there could be a relation between word frequency distributions in the machine translation training data and the ability of the QE model to rely on this information to estimate the sentence-level translation quality. This phenomenon should be investigated further, for instance by replacing the random baseline by a frequency based classifier, as a starting point. The various approaches used as feature attribution methods, along with the current experimental setup, do not offer a clear understanding of what the QE model does unfortunately.
For the analysis section, extracting a useful QE signal at deeper layers of the pre-trained model is somewhat expected, but is still interesting to find. However, we need more insights on what kind of information is encoded at each layer, by the use of probes for instance, at a sentence-level basis. As Figures 2 and 3 indicate, variability in feature attribution between layers show that some words, in some context (sentences in this case), receive different scores given an approach. What are the characteristics of these words and sentences? Do feature attribution methods agree or diverge given words in context? ","It would be interesting to see how relevant to word-level QE is the information encoded at different layers of the model without fine-tuning for sentence-level QE, by using simple classifiers as probes for instance. 
  The feature attribution method based on attention could easily be the focus of the study, whether by using the method proposed by (Kobayashi et al., 2020, “Attention is Not Only a Weight:Analyzing Transformers with Vector Norms”) as a comparison point, by analysing attention heads separately the same way it is conducted per layer, etc. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.",5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"The main weaknesses of this paper are the empirical validation of the method proposed by the authors and the lack of deep analysis. The main findings are limited to applying to QE a set of existing interpretability oriented methods already applied generally to NLP tasks.  For the empirical validation of the method, the evaluation does not reflect a general scenario in machine translation where both perfect and fully erroneous translations exist. Additionally, as shown later in the analysis, there could be a relation between word frequency distributions in the machine translation training data and the ability of the QE model to rely on this information to estimate the sentence-level translation quality. This phenomenon should be investigated further, for instance by replacing the random baseline by a frequency based classifier, as a starting point. The various approaches used as feature attribution methods, along with the current experimental setup, do not offer a clear understanding of what the QE model does unfortunately.
For the analysis section, extracting a useful QE signal at deeper layers of the pre-trained model is somewhat expected, but is still interesting to find. However, we need more insights on what kind of information is encoded at each layer, by the use of probes for instance, at a sentence-level basis. As Figures 2 and 3 indicate, variability in feature attribution between layers show that some words, in some context (sentences in this case), receive different scores given an approach. What are the characteristics of these words and sentences? Do feature attribution methods agree or diverge given words in context? 
It would be interesting to see how relevant to word-level QE is the information encoded at different layers of the model without fine-tuning for sentence-level QE, by using simple classifiers as probes for instance. 
  The feature attribution method based on attention could easily be the focus of the study, whether by using the method proposed by (Kobayashi et al., 2020, “Attention is Not Only a Weight:Analyzing Transformers with Vector Norms”) as a comparison point, by analysing attention heads separately the same way it is conducted per layer, etc. "
ARR_2022_178_review,"The paper describes an early exiting strategy based on hashing tokens to particular layers. It explores a number of possible hash strategies, and compares with previous work. They justify their work by performing experiments on inferring instance difficulty. ","Well described hashing methods and intuition. Well described experimental setup and publishing a dataset.
Very good speed evaluation, taking batching and real time into account, as FLOPs do not give the complete picture. ","The paper is difficult to understand for those not familiar with early exiting in deep learning. A better introduction would be helpful.
Also the paper looks like 2 short papers stapled together into one. The learning difficulty section and the hash section could easily be two separate papers. ",The most clear description of early exiting appears in line 338-345. It would be very helpful to the reader for those to appear much ealier in the paper. ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The paper is difficult to understand for those not familiar with early exiting in deep learning. A better introduction would be helpful.
Also the paper looks like 2 short papers stapled together into one. The learning difficulty section and the hash section could easily be two separate papers. 
The most clear description of early exiting appears in line 338-345. It would be very helpful to the reader for those to appear much ealier in the paper. "
ARR_2022_178_review,"This paper proposes HASHEE, a very simple Hash-based Early Exiting approach. Traditional early exiting methods focus on predicting instance difficulty by heuristic metrics or learnable modules so that easier instances can be assigned to lower layers to exit while more difficult instances can go through more layers. However, the preliminary experiments in this paper show that both human-defined and model-defined instance difficulty is hard to learn. The more important issue in early exiting is to keep the consistency between training and inference, i.e., let similar training instances and inference instances exit from the same layer. To this end, the authors propose to replace the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. This method requires no internal classifiers and can perform token-level early exiting without supervision. The method achieves higher performance with fewer FLOPs compared with strong baselines on classification, regression, and generation tasks, showing its superiority. ","__1. The paper is well organized and easy to follow. The proposed method is well illustrated.__
__2. The proposed method is novel and interesting__ The proposed hash-based early exiting approach removes the learning process and only sticks to the consistency between training and inference, which is novel and interesting.  The authors introduce four kinds of hash functions based on different intuition and find that the frequency hash outperforms other has functions as well as other baselines by a large margin, which is surprising.  __3. The experimental results are impressive__ The improvement brought by HASHEE is very significant and surpasses many strong baselines. Besides, HASHEE can be used in various kinds of tasks including classification, regression, and generation tasks, showing great practicability and universality. ","__1. The relation between instance difficulty and training-inference consistency remains vague:__      This paper seems to try to decouple the concept of instance difficulty and training-inference consistency in current early exiting works. However, I don't think these two things are orthogonal and can be directly decoupled.  Intuitively, according to the accuracy of the prediction, there are two main situations for training-inference inconsistency: the inconsistent exit makes the prediction during inference better than that during training, and vice versa. The first case is unlikely to occur. For the second case, considering instance difficulty reflects the decline in the prediction accuracy of the instances during training and inference, it may be regarded as the second case of training-inference consistency. Accordingly, I am still a little bit confused about the relation between instance difficulty and training-inference consistency after reading the paper.  I would suggest that the authors calculate the token-level difficulty of the model before and after using the hash function, and perform more analysis on this basis. In fact, if the hash function is instance-level, the sentence-level difficulty of all baselines (including static and dynamic models) can be calculated, which will provide a more comprehensive and fair comparison. 
     __2. Lack of the analysis of the relation between instance-level consistency and token-level consistency:__ The core idea derived from the preliminary experiments is to enhance the instance-level consistency between training and inference, i.e., mapping semantically similar instances into the same exiting layer. However, the practical method introduces the consistency constrain at the token level. The paper doesn’t show that whether the token-level method can truely address or mitigate the inconsistency problem at the instance level.  I would suggest the authors define metrics to reflect the instance-level and token-level consistency, and conduct an experiment to verify that whether they are correlated. ","1. I didn’t follow the description of the sentence-level hash function from Line 324 to Line 328: If we use the sequence encoder (e.g., Sentence-BERT) as a hash function to directly map the instances to the exiting layer, why do we still need an internal classifier at that layer? And considering all the instances can be hashed by the pre-trained sequence encoder in advance before training (and early exiting), the appearance of label imbalance should not cause any actual harm? Why does it become a problem?
2. The paper addresses many times (Line 95-97, Line 308-310) that the consistency between training and inference can be easily satisfied due to the smoothness of neural models. I would suggest giving more explanations on this. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"__1. The relation between instance difficulty and training-inference consistency remains vague:__      This paper seems to try to decouple the concept of instance difficulty and training-inference consistency in current early exiting works. However, I don't think these two things are orthogonal and can be directly decoupled.  Intuitively, according to the accuracy of the prediction, there are two main situations for training-inference inconsistency: the inconsistent exit makes the prediction during inference better than that during training, and vice versa. The first case is unlikely to occur. For the second case, considering instance difficulty reflects the decline in the prediction accuracy of the instances during training and inference, it may be regarded as the second case of training-inference consistency. Accordingly, I am still a little bit confused about the relation between instance difficulty and training-inference consistency after reading the paper.  I would suggest that the authors calculate the token-level difficulty of the model before and after using the hash function, and perform more analysis on this basis. In fact, if the hash function is instance-level, the sentence-level difficulty of all baselines (including static and dynamic models) can be calculated, which will provide a more comprehensive and fair comparison. 
     __2. Lack of the analysis of the relation between instance-level consistency and token-level consistency:__ The core idea derived from the preliminary experiments is to enhance the instance-level consistency between training and inference, i.e., mapping semantically similar instances into the same exiting layer. However, the practical method introduces the consistency constrain at the token level. The paper doesn’t show that whether the token-level method can truely address or mitigate the inconsistency problem at the instance level.  I would suggest the authors define metrics to reflect the instance-level and token-level consistency, and conduct an experiment to verify that whether they are correlated. 
1. I didn’t follow the description of the sentence-level hash function from Line 324 to Line 328: If we use the sequence encoder (e.g., Sentence-BERT) as a hash function to directly map the instances to the exiting layer, why do we still need an internal classifier at that layer? And considering all the instances can be hashed by the pre-trained sequence encoder in advance before training (and early exiting), the appearance of label imbalance should not cause any actual harm? Why does it become a problem?
2. The paper addresses many times (Line 95-97, Line 308-310) that the consistency between training and inference can be easily satisfied due to the smoothness of neural models. I would suggest giving more explanations on this. "
ARR_2022_41_review,"This paper explores the effects of using a generative model that provides question prompts to human annotators in either the standard or adversarial data collection for training a QA model. The proposed approach combines the benefits of dynamic adversarial data collection, i.e. human annotators generating data points for the model and generative models that are traditionally used to augment datasets with adversarial data points. 
This paper provides a thorough analysis of using Generative Annotation Assistants (GAAs) during data annotation process, examining the effects on annotation speedup, effectiveness in fooling the model, and performance improvement on downstream tasks. This paper collects data points from several experimental settings to compare the effects of sampling strategies, using model-in-the-loop (standard vs adversarial data collection), datasets used to train the GAAs, and whether answer prompts are provided or not. 
This paper shows that the use of GAA without adversarial model in the loop is can achieve near-adversarial accuracy but at much higher speed of generating the adversarial dataset. Thus this paper addresses limitations of the adversarial data collection process, which is the large amount of time needed to collect the dataset. 
This paper shows that use of GAA in both the standard and adversarial data collection helps improve model performance on downstream tasks. Additionally, in the standard data collection setting, GAA provides a measurable speedup in collecting the dataset in general, but also the effective examples that fool the model by a significant margin. 
Despite the exhaustive experimentation, all the experimental settings are done using one model-in-the-loop, one question prompt generator and 2 datasets of similar domains. This limitation has been called out in the paper. ","- The major takeaway is that the use of GAAs in the standard data collection process results in annotation efficiency, as measured by vMER, i.e. model error rate on the collected data, and time per model fooling example. Additionally, this results in significant improvement in model performance on downstream QA task, which is almost as performant as adversarial data collection without use of GAAs.
- Authors demonstrate effectiveness of using GAAs by exploring effects on the adversarial data collection process as well. Speedup isn't observed here as much as the increase in downstream task performance.
- Authors even experiment with providing answer prompts in addition to question prompts and observe a performance improvement on most downstream tasks for most of the experimental settings. So overall, providing answer prompts in addition to question prompts helps.
- This is the first work to investigate the use of generative assistants for crowdworkers doing the data annotation.
- This paper uses several experimental settings, albeit only few data points per setting to conduct the study.
- The paper has clear language, explaining in detail all the components used in the experimental setup and evaluation strategy. This paper also provides the motivation behind the experimental settings which makes the objective of the exercise much clear. ","- This approach is compute-intensive as it requires pretraining of the GAA model on the same dataset as the QA model needs to be trained on. It assumes that the resultant GAA model is performant enough to provide meaningful prompts to annotators. If this assumption doesn't hold true, then it can result in larger annotation times, thus bringing into question the speedup claim of the proposed approach.
- From table 3, it looks like using GAA with adversarial data collection hurts out-of-domain generalization, as seen by the dip in performance on MRQA dataset.
- This approach has been tested only on the SQuAD dataset for all experimental settings, and just additionally MRQA for evaluation. While it is understandable that all these experimental settings require lot of time and human effort to conduct, the fact that this has been tested only on one dataset raises concerns about generalizability of this framework. Especially when use of GAAs seems to hurt performance on the out of domain dataset MRQA for adversarial data collection. So in situations where adversarial data collection cost doesn't matter as much as quality of resultant QA model, using GAAs won't work.
- While some mild correlations emerge between sampling strategy and downstream performance, overall it seems like the best experimental setting in terms of a trade-off between annotation speed, efficiency and downstream performance depends highly on the downstream task at hand. Practically, its not feasible to try out all these experimental settings for data annotation for QA datasets for industrial/real-world applications. ","- In the setting where annotators were provided with both the question and answer prompts, doesn't this limit the example diversity? This is referring to the drawbacks of generative models mentioned in the introduction section. I think maybe more data is needed to answer this. If we have some metrics showing how many times users accepted the suggestions by GAA, and if we see the most of the GAA suggestions were accepted, then it means that it is the GAA that is generating data points for the most part, with minimal human intervention.
- In table 4, there is a spike in model performance in terms of annotation efficiency for likelihood sampling strategy in the Adversarial QA dataset. Such a trend was not seen for question prompts only setting, for both standard and adversarial data collection processes. Can you please provide a reason/explanation for this observation? ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,"4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.","3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"- This approach is compute-intensive as it requires pretraining of the GAA model on the same dataset as the QA model needs to be trained on. It assumes that the resultant GAA model is performant enough to provide meaningful prompts to annotators. If this assumption doesn't hold true, then it can result in larger annotation times, thus bringing into question the speedup claim of the proposed approach.
- From table 3, it looks like using GAA with adversarial data collection hurts out-of-domain generalization, as seen by the dip in performance on MRQA dataset.
- This approach has been tested only on the SQuAD dataset for all experimental settings, and just additionally MRQA for evaluation. While it is understandable that all these experimental settings require lot of time and human effort to conduct, the fact that this has been tested only on one dataset raises concerns about generalizability of this framework. Especially when use of GAAs seems to hurt performance on the out of domain dataset MRQA for adversarial data collection. So in situations where adversarial data collection cost doesn't matter as much as quality of resultant QA model, using GAAs won't work.
- While some mild correlations emerge between sampling strategy and downstream performance, overall it seems like the best experimental setting in terms of a trade-off between annotation speed, efficiency and downstream performance depends highly on the downstream task at hand. Practically, its not feasible to try out all these experimental settings for data annotation for QA datasets for industrial/real-world applications. 
- In the setting where annotators were provided with both the question and answer prompts, doesn't this limit the example diversity? This is referring to the drawbacks of generative models mentioned in the introduction section. I think maybe more data is needed to answer this. If we have some metrics showing how many times users accepted the suggestions by GAA, and if we see the most of the GAA suggestions were accepted, then it means that it is the GAA that is generating data points for the most part, with minimal human intervention.
- In table 4, there is a spike in model performance in terms of annotation efficiency for likelihood sampling strategy in the Adversarial QA dataset. Such a trend was not seen for question prompts only setting, for both standard and adversarial data collection processes. Can you please provide a reason/explanation for this observation? "
ARR_2022_299_review,"This paper focuses on revising errors made by a generation model while it generates additional tokens continuously. When generating the i-th token, the proposed approach will predict the token at position i-1 again for a potential revising. Compared to the first prediction for position i-1, the second prediction adds the predicted i-1 token as extra context in the self-attention module. So, it may make a different decision. A new hyperparameter is introduced to control the number of tokens can be refined. ","1. The proposed method can be integrated into a language model which using attention mechanism easily, which could benefits its adoption if this method shows the effectiveness.
2. Extensive experiment results show there are some gains for this method compared to some baselines on four generation tasks. ","1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction.  2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement. ","1. In Table 2, the ablation experiment has a setting only add ""local Constraint"" but without ""Refinement Mask"". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction.  2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement. 
1. In Table 2, the ablation experiment has a setting only add ""local Constraint"" but without ""Refinement Mask"". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position. "
ARR_2022_299_review,"This paper proposes a novel refinement method to synchronously refine the previously generated words and generate the next word for language generation models. The authors accomplish this goal with an interesting implementation without introducing additional parameters. Specifically, the authors reuses the context vectors at previous decoding steps (i.e., c_1, c_2, ..., c_{i-2}) to calculate the refined probabilities in a similar way to the standard generation probabilities (the only difference is that using c_{0<n<i-1} instead of c_{i-1}). A refinement operation will be conducted at a previous position, where the refinement probability is greater than the generation probability.
To reduce the computational cost and potential risk of ""over-refinement"", the authors design a local constraint that narrow the refinement span to the N nearest tokens. In model training, the authors randomly select future target words not greater than N to cover a variety of different future contexts as bleu parts. ","1. A novel approach to accomplish the modeling of future context. 
2. Comprehensive experiments to validate the effectiveness of the proposed approach across different tasks (e.g., standard and simultaneous machine translation, storytelling, and text summarization). 
3. Detailed analyses to show how each component (e.g., the hyper parameter N, local constraints and refinement mask) works. ","The main concern is the measure of the inference speed. The authors claimed that ""the search complexity of decoding with refinement as consistent as that of the original decoding with beam search"" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).
Even with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens/sec for Transformer-base model)?
How do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.
The score will be increased if the authors can address the concern.
[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. ","1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.
2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?
3. Is PPL a commonly-used metric for storytelling? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,5 = They could easily reproduce the results.,,"The main concern is the measure of the inference speed. The authors claimed that ""the search complexity of decoding with refinement as consistent as that of the original decoding with beam search"" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).
Even with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens/sec for Transformer-base model)?
How do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.
The score will be increased if the authors can address the concern.
[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. 
1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.
2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?
3. Is PPL a commonly-used metric for storytelling? "
ARR_2022_299_review,"This paper propose a novel encoder-decoder model for language generation. It revises the potential errors by considering the representation of  the target future context and generates the next target token synchronously. Despite a slight reduction in decoding speed compared to the Transformer baselines in three machine translation tasks, the proposed model is superior to competitive comparison systems in translation quality and efficiency using the same number of parameters as the Transformer. And the improvement is also demonstrated in other three language generation task, including summarization, storytelling and simultaneous machine translation. ","I really enjoyed reading this paper. The research question is clearly stated, and the experimental results are fascinating to go through. ",It is not clear when deletion operation is used in the refinement period. ,"1. How to ensure that the consecutive refined BEP tokens can be valid whole words after de-BPE operation? 
2. I suggest show the the effect of the proposed model with beam search during the refinement stage for the translation quality, although with inferior inference speed. 
3. Although achieving an improvement, what are the main advantages of this proposed over some simple and effective methods without additional parameters and speed reduction, such as back-translation and R-drop? 
4. The ratio between the generation and the refinement probabilities in the whole training objective is 1 in the equation 12. Is it the optimal setting? ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"It is not clear when deletion operation is used in the refinement period. 
1. How to ensure that the consecutive refined BEP tokens can be valid whole words after de-BPE operation? 
2. I suggest show the the effect of the proposed model with beam search during the refinement stage for the translation quality, although with inferior inference speed. 
3. Although achieving an improvement, what are the main advantages of this proposed over some simple and effective methods without additional parameters and speed reduction, such as back-translation and R-drop? 
4. The ratio between the generation and the refinement probabilities in the whole training objective is 1 in the equation 12. Is it the optimal setting? "
ARR_2022_299_review,"This paper presents a method to improve text generation by refining the target output while decoding. The idea is that at each decoding step t, the model predicts not only the current word but re-processes and predicts previous ones (in a window of k-words), so some generation errors can be corrected. ","-   The idea of simultaneously refining and decoding is sound. It has the potential to improve text generation with low overhead of resources.
-   The results on three tasks show that their model works well in comparison to a standard model, however, comparisons with similar methods are missing. ","- Comparison of results with equivalent methods described on related work are missing. For example, post-editing or multiple pass decoders.
- The general idea is well described but several details are not clear in the paper (refer to comments for details). This information is important for understanding and reproducibility, and it is necessary for properly reviewing the work. ","1 . Line 050: A common way to address the issue is using beam search.
2. Lines 076-079 Question: Do you replace a word independent of the context? For example, if you replace w_i, what happens with w_{i-1} and w_{i+1}. This could create incoherent text. I will assume that it is better to replace the whole phrase instead of independent words. 
3. Lines 197-198 Question: How do you calculate the joint probability P(y_1, y_2, ..| y_{<i}, X) if this part is not autoregressive?  I suggest adding the equation to the paper for clarity.
4. Line 197 Question: What it means the “0”? 
5. Lines 185-191 Question: Same question as in (2).  It is not clear if you replace the whole phrase or each word? 
6. Lines 199-207. The decoding algorithm mixing greedy and beam search should be added as pseudocode for clarity. 
7. Equation 12. I assume that P_g and P_r shared the same parameters, so their calculation varies only for the use of different masks. Does this imply making two passes of the decoder? Please clarify these points in the paper. 
8. As far as I understand, the refining mask is used only for training. Is this correct? ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"- Comparison of results with equivalent methods described on related work are missing. For example, post-editing or multiple pass decoders.
- The general idea is well described but several details are not clear in the paper (refer to comments for details). This information is important for understanding and reproducibility, and it is necessary for properly reviewing the work. 
1 . Line 050: A common way to address the issue is using beam search.
2. Lines 076-079 Question: Do you replace a word independent of the context? For example, if you replace w_i, what happens with w_{i-1} and w_{i+1}. This could create incoherent text. I will assume that it is better to replace the whole phrase instead of independent words. 
3. Lines 197-198 Question: How do you calculate the joint probability P(y_1, y_2, ..| y_{<i}, X) if this part is not autoregressive?  I suggest adding the equation to the paper for clarity.
4. Line 197 Question: What it means the “0”? 
5. Lines 185-191 Question: Same question as in (2).  It is not clear if you replace the whole phrase or each word? 
6. Lines 199-207. The decoding algorithm mixing greedy and beam search should be added as pseudocode for clarity. 
7. Equation 12. I assume that P_g and P_r shared the same parameters, so their calculation varies only for the use of different masks. Does this imply making two passes of the decoder? Please clarify these points in the paper. 
8. As far as I understand, the refining mask is used only for training. Is this correct? "
ARR_2022_183_review,"This paper expands on previous work, that has found that MT QE is considerably biased towards the fluency of the translation, often disregarding the source sentence (adequacy). The authors analyze a known dataset for the existence of this particular bias for 7 language directions and show that there is bias for two of them (en-de and en-zh). Consequently, they proposed 4 modifications of the successful monoTransQuest model, in which the QE model is jointly trained with another task: a bilingual QE, a QE augmented with binary classification of correct vs. artificial false translations, an adversarial task of not being based on target fluency and the debiased focal loss. Finally, they perform experiments of all these modifications against the baseline, and they show that the system augmented with the binary classifier gives the better tradeoff of bias reduction and increasing QE performance. ",- authors show good analysis and understanding of the problem - the strategy of training QE with complementary tasks is novel  - intuitive choice of complementary tasks and building of the NN architecture - positive results ,"1. the authors show that the bias appears only in two out of seven language directions, which pretty much depends on the quality of the MT systems that produced the MT outputs on which the QE systems were trained. It should be clear that for other language pairs or even datasets, other or no de-biasing may be needed. 
2. the ru-en shows biasing towards the source sentence. Although this language direction is included in the results, there are no conclusions related to it or efforts for debiasing of this case. 
3. In section 3.1 there is a lot of space used for a dataset that is already known, unless this refers to newly released data together with this paper (confused a bit). 
4. In section 3.3: what was the process of the manual annotation? How many annotators worked on that, what was their expertise, language fluency. Were there any inter- or intra-annotator statistics? ",Will the authors make the software available in a repository? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,"2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)",5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. the authors show that the bias appears only in two out of seven language directions, which pretty much depends on the quality of the MT systems that produced the MT outputs on which the QE systems were trained. It should be clear that for other language pairs or even datasets, other or no de-biasing may be needed. 
2. the ru-en shows biasing towards the source sentence. Although this language direction is included in the results, there are no conclusions related to it or efforts for debiasing of this case. 
3. In section 3.1 there is a lot of space used for a dataset that is already known, unless this refers to newly released data together with this paper (confused a bit). 
4. In section 3.3: what was the process of the manual annotation? How many annotators worked on that, what was their expertise, language fluency. Were there any inter- or intra-annotator statistics? 
Will the authors make the software available in a repository? "
ARR_2022_183_review,"This paper focus on mitigating the partial input bias of neural QE models, which means that some strong QE models tend to over-rely on the translation while ignoring the source sentence to some extent. To solve this problem, the authors propose to use auxiliary tasks to debias the QE model, including related tasks and adversarial tasks. Moreover, the authors also attempted to use focal loss to further reduce the partial input bias in the QE model. Experiments indicate that the involved approaches all can effectively help the QE model pay more attention to the source sentence. ","1. The research problem is clearly illustrated in this work. 
2. The proposed approaches is straightforward and easy to understand. ","1. Given that this work aims to investigate the partial input bias of QE models, more other representative QE models are required to demonstrate that the partial input bias problem is important and the proposed methods are effective. The authors only conduct experiments using one QE model, making the results less convincing. 
2. Figure 1 shows that the partial input bias problem is not severe in some other language pairs, such as RO-EN and ET-EN. The authors did not provide any detailed explanation of this phenomenon, which weakens the motivation to address the partial input bias for QE models. ","1. I suggest to conduct more experiments using more QE models on a wider range of language pairs. 
2. Some examples may make it better to understand why partial input bias exists. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"1. Given that this work aims to investigate the partial input bias of QE models, more other representative QE models are required to demonstrate that the partial input bias problem is important and the proposed methods are effective. The authors only conduct experiments using one QE model, making the results less convincing. 
2. Figure 1 shows that the partial input bias problem is not severe in some other language pairs, such as RO-EN and ET-EN. The authors did not provide any detailed explanation of this phenomenon, which weakens the motivation to address the partial input bias for QE models. 
1. I suggest to conduct more experiments using more QE models on a wider range of language pairs. 
2. Some examples may make it better to understand why partial input bias exists. "
ARR_2022_183_review,"The paper highlights an important issue of an existing bias in the SoTA methods of Quality Estimation. Due to this bias, the methods tend to only consider the features associated with the generated translations (and not the source sentences) for assessing the overall quality. The authors attribute the cause of this bias to both the model and the human annotators as they tend of provide a high score to fluent but inadequate translations.
The main contribution of this paper is to analyze the partial input bias problem and explore different approaches using auxiliary tasks for mitigating this bias. The proposed methods are designed by keeping in mind that they should aid in reducing the bias, be comparable or superior to the original performance and not lead to extensive computational overhead. The authors experiment with multitask and adversarial techniques and find that the former yields better results. 
The approaches are evaluated on two high resource language pairs i.e. En-De and En-Zh. ","1. The paper is well written and structed and the motivation is clearly laid out i.e. exploration of bias reduction methods for Quality Estimation. 
2. The suggested methods would aid in reducing the problem of input bias without the need to collect additional annotated data and expensive modification to the original model. 
3. Extensive evaluation of fluent but inadequate translations for En-De revealed that the bias is not only an artifact of the modelling technique but also the annotation mechanism. 
4. For the two languages, the proposed method not only reduces the partial input bias significantly but also retains the overall quality based on SoTA baseline. ","1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. ","Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ? ",3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. 
Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ? "
ARR_2022_296_review,"This paper proposes a data-efficient end-to-end event extraction method by designing event-specific generation templates. 
These event-specific templates describe the event types in a natural language way. 
The proposed method can use pre-trained language models and exploit label semantics from event type definitions. 
Experimental results show that DEGREE has better performance on low resource event extraction and can be compared with SOTA under the full supervised setting. ","This paper designs an event-specific template for generative event extraction. 
Compared with the previous methods, the proposed method is more consistent with natural language generation and achieves better performance in the low-resource setting. ","- The proposed framework is based on the previous template-based information extraction method. 
The template-based method's main drawback is the significantly increased training/inference cost (corresponding to the number of event types). The increase in the category of events (such as MAVEN with 100+ types) will exacerbate the problem.
- For extreme low-resource settings (1%), different data sampling may heavily affect the experimental results. 
It is better to conduct experiments multi-times on different subset samples. ","Some detailed questions about model inference: - Multi-events: Can DEGREE and DEGREE(ED) deal with multiple events of the same type in the same sentence? Although we can predict the same input sentence for different types one-by-one if a sentence contains multiple events of the same type, the same input (template and sentence) will correspond to various outputs. For example, some sentences have two death events. This is not a problem for other generation methods because they are trigger-driven (TANL, BART-GEN), or generate all extracted events in one step (Text2Event).
- Converting generated results to events: Are there some generated results that cannot be parsed into events? Generation methods are more uncontrollable than span extraction and span classification methods for event extraction. For example, the generated span maybe not appear in the input sentence.
There are many template methods for information extraction. 
It is suggested that authors should cite these articles: - Template-Based Named Entity Recognition Using BART. Findings of ACL_IJCNLP 2021 - Reading the Manual: Event Extraction as Definition Comprehension. spnlp@EMNLP 2020 ",3.5 ,No,1 = No usable datasets submitted.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- The proposed framework is based on the previous template-based information extraction method. 
The template-based method's main drawback is the significantly increased training/inference cost (corresponding to the number of event types). The increase in the category of events (such as MAVEN with 100+ types) will exacerbate the problem.
- For extreme low-resource settings (1%), different data sampling may heavily affect the experimental results. 
It is better to conduct experiments multi-times on different subset samples. 
Some detailed questions about model inference: - Multi-events: Can DEGREE and DEGREE(ED) deal with multiple events of the same type in the same sentence? Although we can predict the same input sentence for different types one-by-one if a sentence contains multiple events of the same type, the same input (template and sentence) will correspond to various outputs. For example, some sentences have two death events. This is not a problem for other generation methods because they are trigger-driven (TANL, BART-GEN), or generate all extracted events in one step (Text2Event).
- Converting generated results to events: Are there some generated results that cannot be parsed into events? Generation methods are more uncontrollable than span extraction and span classification methods for event extraction. For example, the generated span maybe not appear in the input sentence.
There are many template methods for information extraction. 
It is suggested that authors should cite these articles: - Template-Based Named Entity Recognition Using BART. Findings of ACL_IJCNLP 2021 - Reading the Manual: Event Extraction as Definition Comprehension. spnlp@EMNLP 2020 "
ARR_2022_296_review,"This paper presents a method for identifying event types and their arguments in a low-resource scenario, i.e., with a limited amount of training examples. The method uses distant supervision signals given in the form of templates that define event types and their arguments that were manually curated based on the definition of events in the ACE dataset. 
The authors demonstrate the impressive ability of their model to outperform the current sota and baselines in the low-resource scenario and comparable ability in the high-resource scenario. They also perform ablation studies to emphasize the contribution of every suggested component.  Although this is the paper summary section and criticism should not appear here - I feel obligated to mention here that the authors did not report any statistical analysis of their result! This makes the entire result section meaningless and I implore the authors to add the significance testing results to their paper before publication. ","- A new model for event and argument detection and typing that uses distant supervision signals in the form of additional prompt templates.
- The model presents impressive performance (though its impressiveness is questionable until statistical analysis results will be presented) and comparable scores to sota and strong baseline models.
- The paper is well written and detailed. ","The main weakness is that this method can only be applied to the event types and argument roles defined in the ACE dataset (which are not a lot). There are newer event-type datasets such as MAVEN (https://aclanthology.org/2020.emnlp-main.129/) that include many more events but cannot be used in this framework because they don't include the type definitions that are required for the prompting design.
No statistical analysis of the results is reported although a comparison between models is made. Sentences like ""The performance gap becomes more significant for the extremely low-resource situation."" should not be written without statistical proof of significance. ","I would suggest trying to come up with templates based on MAVEN types too, this will cover many more event types. ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The main weakness is that this method can only be applied to the event types and argument roles defined in the ACE dataset (which are not a lot). There are newer event-type datasets such as MAVEN (https://aclanthology.org/2020.emnlp-main.129/) that include many more events but cannot be used in this framework because they don't include the type definitions that are required for the prompting design.
No statistical analysis of the results is reported although a comparison between models is made. Sentences like ""The performance gap becomes more significant for the extremely low-resource situation."" should not be written without statistical proof of significance. 
I would suggest trying to come up with templates based on MAVEN types too, this will cover many more event types. "
ARR_2022_135_review,This draft reports an empirical study comparing the performance of various entropy estimators on natural language data as well as on artificially generated data. ,The draft investigates a problem that has not been previously considered in the NLP literature. The draft is also very valuable under a methodological perspective for entropy estimation. ,"Entropy results are provided using the unigram distribution, which is a very poor language model. ","As far as I can say, this is the first investigation in the NLP literature of various entropy estimators, which makes the draft very valuable.  My main concern is that entropy results are provided limited to the unigram distribution.  It is well-known that unigram distribution is a very poor model for sentence distributions in natural language.  There is no mention in the draft of more powerful language models, such as N-gram (N>1), combined with various smoothing and back-off techniques, why?   Figure 1: it would be nice to see some comments on those local minima in the figure occurring at N=10^5 for English, German and Dutch for both CS and NSB. Do you have any explanation for this? Same thing for HMT, between N=10^2 and N=10^3.  Typos l.166-67: This should should seem l.219: trigramma function > trigamma function Appendix A: sometimes you write p-hat, and sometimes you write p-hat_MLE l.546: truncated sentence?  Appendices C1 and C2:  I think at l.563 the reference should be to Table 3, not Table 4.  And reference to Table 4 should be placed into Appendix C2.  l.569-71: I cannot parse this sentence ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",None ,"Entropy results are provided using the unigram distribution, which is a very poor language model. 
As far as I can say, this is the first investigation in the NLP literature of various entropy estimators, which makes the draft very valuable.  My main concern is that entropy results are provided limited to the unigram distribution.  It is well-known that unigram distribution is a very poor model for sentence distributions in natural language.  There is no mention in the draft of more powerful language models, such as N-gram (N>1), combined with various smoothing and back-off techniques, why?   Figure 1: it would be nice to see some comments on those local minima in the figure occurring at N=10^5 for English, German and Dutch for both CS and NSB. Do you have any explanation for this? Same thing for HMT, between N=10^2 and N=10^3.  Typos l.166-67: This should should seem l.219: trigramma function > trigamma function Appendix A: sometimes you write p-hat, and sometimes you write p-hat_MLE l.546: truncated sentence?  Appendices C1 and C2:  I think at l.563 the reference should be to Table 3, not Table 4.  And reference to Table 4 should be placed into Appendix C2.  l.569-71: I cannot parse this sentence "
ARR_2022_135_review,"Entropy estimates are a fundamental quantity of interest for answering data-driven information-theoretic questions. However, it is known that the estimation of entropy from raw data can be quite challenging knowing that the MLE estimator in expectation underestimates entropy and the power-law governing linguistic data cannot be effectively captured by such estimates. This study is an empirical comparison of various entropy estimators on natural language data and fills an important research gap. ","Strengths: 1) The paper addresses an important research gap in the computational linguistics literature. 
2) It studies a set of 6 different estimators of entropy and their empirical comparisons using both synthetic data and natural language data. 
3) The authors have also replicated recent results that used the plug-in estimator to approximate entropy to show how the results overrepresent the actual effects compared to what could have been when using a power-law sensitive entropy estimator. ",The paper doesn't have any weaknesses. ,"Some typos to correct.
Line 045: challening -> challenging Line 310: ""See Fig 2"" could be written in parentheses, instead of making it a separate sentence. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"
Some typos to correct.
Line 045: challening -> challenging Line 310: ""See Fig 2"" could be written in parentheses, instead of making it a separate sentence. "
ARR_2022_135_review,"This paper compares three entropy estimators with the simplest plug in entropy estimator on linguistic data of CELEX corpora and synthetic data.  The authors compared the quality of estimators on unigram distribution, information study on the association between gendered inanimate nouns and their modifying adjectives, and finally with similarity between grammatical gender partitions between languages. 
Authors conclude that other estimators are more reliable than the plugin estimator. ",This paper investigates some entropy estimators whose application to linguistic data have not appeared in the literature. ,"1. References Although, the authors claim that this is the first literature, there have been abundant application of entropy estimates on linguistic data. Since Brown's Computational Linguistics paper 1983, some related papers have appeared in ACL milieu, but the main debate appeared elsewhere, especially, discussed in the journal of MDPI Entropy, some were even archived in the book forms. Those include multiple articles about the bias of plugin entropy, therefore, authors are recommended to go through them.  Authors are requested to correctly situate this work within the large history of studies on bias of plug in entropy, which is no more than one kind of work to apply entropy estimate to linguistic data.
2. Soundness of this paper The authors claim at the bottom of p.2, about the plugin estimator,     by Jensen's inequality, this estimator is, in expectation,     a lower bound on the true entropy. 
However, by information theory, any entropy estimator is destined to be no more than an upper bound of the true entropy. In Appendix A, the author draws conclusion by mixing the true entropy and the estimated entropy, where plugin entropy is no more than an estimated entropy. Please see the definition of cross entropy.
3. Experiments This paper only reports the partial and summarized output for each experiment. For example, only the ""bests"" were mentioned, but authors are recommended to show the concrete performances of each estimator.  To investigate the quality of entropy estimates, much more fundamental analysis are conducted within the previous literature, before going on to concrete application such as described in 4.2 and 4.3.  Such includes tests on other corpora than CELEX, how bias depends on the length of corpora etc. For tips, please see the previous litterature. ","p. 3 first paragraph, ""should"" replicates just below formula (7). ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. References Although, the authors claim that this is the first literature, there have been abundant application of entropy estimates on linguistic data. Since Brown's Computational Linguistics paper 1983, some related papers have appeared in ACL milieu, but the main debate appeared elsewhere, especially, discussed in the journal of MDPI Entropy, some were even archived in the book forms. Those include multiple articles about the bias of plugin entropy, therefore, authors are recommended to go through them.  Authors are requested to correctly situate this work within the large history of studies on bias of plug in entropy, which is no more than one kind of work to apply entropy estimate to linguistic data.
2. Soundness of this paper The authors claim at the bottom of p.2, about the plugin estimator,     by Jensen's inequality, this estimator is, in expectation,     a lower bound on the true entropy. 
However, by information theory, any entropy estimator is destined to be no more than an upper bound of the true entropy. In Appendix A, the author draws conclusion by mixing the true entropy and the estimated entropy, where plugin entropy is no more than an estimated entropy. Please see the definition of cross entropy.
3. Experiments This paper only reports the partial and summarized output for each experiment. For example, only the ""bests"" were mentioned, but authors are recommended to show the concrete performances of each estimator.  To investigate the quality of entropy estimates, much more fundamental analysis are conducted within the previous literature, before going on to concrete application such as described in 4.2 and 4.3.  Such includes tests on other corpora than CELEX, how bias depends on the length of corpora etc. For tips, please see the previous litterature. 
p. 3 first paragraph, ""should"" replicates just below formula (7). "
ARR_2022_220_review,"This submission proposed a model for offensive span detection (OSD). Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD. ",- Good writing and well organized - Interesting topic of offensive span detection ,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain - Insufficient evaluation:    - only one dataset is used   - lack of detailed dataset statistics   - the comparison is not persuasive ,"Although OSD is an interesting topic, the submission should address the following issues before it can be published: - Only one dataset is used for evaluation, is there anything more?
- Is it possible to add more evaluations about the synthetic data itself? In the current paper, the comparison is between the whole pipeline and other baselines. Not sure how much performance is made by the data itself, or the training mode itself?
- Is it possible to generalize the application? For example, from the OSD to general opinion and aspect detection?  - The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?
- The definitions of usefulness and diversity seem quite intuitive. Any motivations to justify this definition, or is there any alternative form of definition? ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.,,"- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain - Insufficient evaluation:    - only one dataset is used   - lack of detailed dataset statistics   - the comparison is not persuasive 
Although OSD is an interesting topic, the submission should address the following issues before it can be published: - Only one dataset is used for evaluation, is there anything more?
- Is it possible to add more evaluations about the synthetic data itself? In the current paper, the comparison is between the whole pipeline and other baselines. Not sure how much performance is made by the data itself, or the training mode itself?
- Is it possible to generalize the application? For example, from the OSD to general opinion and aspect detection?  - The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?
- The definitions of usefulness and diversity seem quite intuitive. Any motivations to justify this definition, or is there any alternative form of definition? "
ARR_2022_23_review,"This paper consists of two main parts. First, the authors constructed MedLAMA, a biomedical knowledge probing benchmark dataset based on the UMLS knowledge graph. Second, the authors proposed a contrastive-probe method based on retrieval-based probing and self-supervised contrastive learning. When the authors applied several conventional proving methods with existing pre-trained language models (PLMs) to MedLAMA, other methods showed poor performances, because they have weaknesses in handling multi-token answers. The authors insisted that the proposed contrastive-probe method achieved significant improvement on probing with MedLAMA data. The authors also performed extensive experiments including human expert evaluation to demonstrate characteristics of their dataset and method. ","- The authors constructed a new biomedical knowledge probing benchmark dataset MedLAMA.
- The authors proposed a contrastive-probe method, which can be used without labelled data. ","- The performance of the contrastive-probe method in the main result using MedLAMA was significantly better than other methods (mask average), while the performances using BioLAMA were similar between mask average and the contrastive-probe method. Thus, the authors need to clearly show why the performances in two datasets are different.    - The authors mentioned “2Prompt-based probing approaches such as Auto-Prompt (Shin et al., 2020a), SoftPrompt (Qin and Eisner, 2021), and OptiPrompt (Zhong et al., 2021) need additional labelled data for fine-tuning prompts, but we restrict the scope of our investigation to methods that do not require task data.“ However, it might be better to show the performances of these methods on the MedLAMA dataset, in order to show the validity of the MedLAMA dataset.
- Additionally, it seems to be an unfair comparison because only the contrastive-probe method was additionally pre-trained for the cloze-style task. ","- It would be better that comparison with BioLAMA is placed in the main result, not in the Appendix. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The performance of the contrastive-probe method in the main result using MedLAMA was significantly better than other methods (mask average), while the performances using BioLAMA were similar between mask average and the contrastive-probe method. Thus, the authors need to clearly show why the performances in two datasets are different.    - The authors mentioned “2Prompt-based probing approaches such as Auto-Prompt (Shin et al., 2020a), SoftPrompt (Qin and Eisner, 2021), and OptiPrompt (Zhong et al., 2021) need additional labelled data for fine-tuning prompts, but we restrict the scope of our investigation to methods that do not require task data.“ However, it might be better to show the performances of these methods on the MedLAMA dataset, in order to show the validity of the MedLAMA dataset.
- Additionally, it seems to be an unfair comparison because only the contrastive-probe method was additionally pre-trained for the cloze-style task. 
- It would be better that comparison with BioLAMA is placed in the main result, not in the Appendix. "
ARR_2022_23_review,"This paper presents a carefully curated biomedical probing benchmark - MedLAMA. The authors transform the relations in the UMLS knowledge graph to form triples for probing PLMs’ biomedical knowledge. The curated benchmark consists of 19 relations with 1000 examples each. The authors find that existing probing methods fail to achieve meaningful performance on MedLAMA. Therefore, they propose to “rewire the PLM parameters for retrieval”. They propose Contrastive-Probe that fine-tunes the PLM with little data from the original pretraining corpora. The proposed method improves ACC@1 and ACC@10 on MedLAMA to 7%/27%. The authors further conduct experiments for different PLMs, layers, and different relations. Finally, they perform an examination by an expert on the predicted answers and find that the automatic evaluation might underestimate the performance of the models. ","- The paper is well-written and easy to follow. The presentation of the ideas is clear.
- This paper presents a biomedical probing benchmark that might reinforce the research in this direction.
- The paper proposes a simple method to make PLMs more suitable for retrieval probing, hence improving the probing results. The improved results make the comparison on the benchmark more meaningful. ",The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper. ,"- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem? ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper. 
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem? "
ARR_2022_158_review,"This paper analyses whether task-specific transformer models (i.e., fine-tuned BERT models) are successful at brain encoding tasks. The authors compare the performance of encoding models across a wide range of NLP tasks and discuss the differences between encoding fMRI stimuli from reading vs. from listening to stories. They also analyse the results for various language specific brain regions. ","The paper provides initials answers to an important open questions the field of brain encoding tasks from language stimuli. 
The methodology seems appropriate and sound. 
The paper has a well written introduction that summarizes the current state of the field. The paper clearly distinguished between brain encoding and decoding tasks for clarification. ","This is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.
My main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels. 
This is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion. ",- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent. ,3.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"This is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.
My main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels. 
This is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion. 
- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent. "
ARR_2022_242_review,"The paper introduces a framework for learning a taxonomy using cognitive data, e.g., fMRI data. The authors focus on the NLP domain and compare the performance of this method with the previously used visual Taskonomy method (Saaty, 1987, Zamir et al., 2018). They find that their method performs better than random and that it provides taxonomy results that seem intuitive. While the method doesn't provide performance as high as the one in Zamir et al. (2018), it requires less compute and might therefore function as a potential alternative. ","The authors take an influential taxonomy idea from the visual domain and explore it in the domain of NLP. The learned taxonomy is based on cognitive data, providing an interesting approach to the task. ","In my view, the paper would benefit from refocusing on exploring its main contributions. First of all, the abstract states that the method results in ""competitive [performance] to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018)"". When inspecting the results, the performance is better than random but still far from the performance of the AHP method. The core contribution, therefore, seems to be that their method doesn't require expensive transfer learning between tasks. This point should be highlighted and explained further: Where does this become especially relevant and to what extent does it matter? The authors write that more advantages over AHP will be shown later (line 533ff) but it remains unclear to me when and where we will learn about those.
Secondly although the CogTaskonomy framework is introduced as a combination of two separate methods, there is no analysis how important their combination is. CRA seems to be more important than CNM and in combination with the BERT model, their combination barely matters. Is there an intuition why one would be more important than the other? Similarly, a comparison of which cognitive data is used might be interesting as well (i.e., fMRI vs. eyetracking data, for instance).
Finally, the taxonomy itself could be an interesting contribution but its exploration remains very limited. The authors visualize the taxonomy learned by one component of the CogTaskonomy framework (CNM) which also seems intuitive (Figure 4). However, it remains unexplored whether this is the same for all tested methods or even just what it would be for the complete CogTaskonomy framework.
Overall, I found the paper slightly difficult to follow. Making the contribution more pronounced and how this specifically connects to the results and future opportunities might help with this. A more direct method comparison with respect to data needed, training pipeline and other important points of distinction would have also helped me. There were also a couple of conclusions that didn't follow for me from the data which I'll specify in the comments and suggestions. ","*Conclusions that didn't quite follow for me:* - line 511ff: I can't follow why a higher performance of TinyBERT over BERT suggests that TinyBERT ""mak[es] sentence representations more relevant to individual tasks [through knowledge distillation] and hence result[s] in better task similarity estimation.""
- line 551ff: ""RSA is able to be adapted to NLP task structure detection"" seems to be already argued with a single configuration where it outperforms random.  - line 572ff: ""This is consistent with our previous finding in the main results that TinyBERT (with KD) captures more task-relevant knowledge than BERT for task relation detection."" This is argued based on the finding that ""with a small number of cognitive signals (voxels), TinyBERT for CNM can achieve a good task ranking score"" (line 566ff). Is there a way to make this more intuitive?
*General comments:* In the contributions (line 110), the authors state that the ""CNM is able to learn stable task relations"" (line 128f). However, this is only one of two components of the the proposed CogTaskonomy framework. What about the CRA?
The phrase ""sentence-level textual stimuli of cognitive data"" (e.g., line 259f) is very obscure. I recommend expanding on it once to clarify.
I didn't understand the paragraph 4.1 Cognitive Dataset. Specifically, it's not intuitive to me why the voxels had to be randomly selected (line 400f).
I think it would be useful to mention to the reader that high scores are undesirable in the TRS metric (e.g., Table 1 and Figure 3).
The authors try to make an argument across architectures but only based on BERT architectures which seems a weak generalizability claim. I would suggest either weakening the phrasing or providing more models.
*Typos:*  - abstract: CogTaxonomy -> CogTaskonomy - line 83f: clarify that these terms are introduced by the authors and are not pre-existing parts - line 265: concepts - line 557: more robust ",2.5 ,No,1 = No usable datasets submitted.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",None ,"In my view, the paper would benefit from refocusing on exploring its main contributions. First of all, the abstract states that the method results in ""competitive [performance] to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018)"". When inspecting the results, the performance is better than random but still far from the performance of the AHP method. The core contribution, therefore, seems to be that their method doesn't require expensive transfer learning between tasks. This point should be highlighted and explained further: Where does this become especially relevant and to what extent does it matter? The authors write that more advantages over AHP will be shown later (line 533ff) but it remains unclear to me when and where we will learn about those.
Secondly although the CogTaskonomy framework is introduced as a combination of two separate methods, there is no analysis how important their combination is. CRA seems to be more important than CNM and in combination with the BERT model, their combination barely matters. Is there an intuition why one would be more important than the other? Similarly, a comparison of which cognitive data is used might be interesting as well (i.e., fMRI vs. eyetracking data, for instance).
Finally, the taxonomy itself could be an interesting contribution but its exploration remains very limited. The authors visualize the taxonomy learned by one component of the CogTaskonomy framework (CNM) which also seems intuitive (Figure 4). However, it remains unexplored whether this is the same for all tested methods or even just what it would be for the complete CogTaskonomy framework.
Overall, I found the paper slightly difficult to follow. Making the contribution more pronounced and how this specifically connects to the results and future opportunities might help with this. A more direct method comparison with respect to data needed, training pipeline and other important points of distinction would have also helped me. There were also a couple of conclusions that didn't follow for me from the data which I'll specify in the comments and suggestions. 
*Conclusions that didn't quite follow for me:* - line 511ff: I can't follow why a higher performance of TinyBERT over BERT suggests that TinyBERT ""mak[es] sentence representations more relevant to individual tasks [through knowledge distillation] and hence result[s] in better task similarity estimation.""
- line 551ff: ""RSA is able to be adapted to NLP task structure detection"" seems to be already argued with a single configuration where it outperforms random.  - line 572ff: ""This is consistent with our previous finding in the main results that TinyBERT (with KD) captures more task-relevant knowledge than BERT for task relation detection."" This is argued based on the finding that ""with a small number of cognitive signals (voxels), TinyBERT for CNM can achieve a good task ranking score"" (line 566ff). Is there a way to make this more intuitive?
*General comments:* In the contributions (line 110), the authors state that the ""CNM is able to learn stable task relations"" (line 128f). However, this is only one of two components of the the proposed CogTaskonomy framework. What about the CRA?
The phrase ""sentence-level textual stimuli of cognitive data"" (e.g., line 259f) is very obscure. I recommend expanding on it once to clarify.
I didn't understand the paragraph 4.1 Cognitive Dataset. Specifically, it's not intuitive to me why the voxels had to be randomly selected (line 400f).
I think it would be useful to mention to the reader that high scores are undesirable in the TRS metric (e.g., Table 1 and Figure 3).
The authors try to make an argument across architectures but only based on BERT architectures which seems a weak generalizability claim. I would suggest either weakening the phrasing or providing more models.
*Typos:*  - abstract: CogTaxonomy -> CogTaskonomy - line 83f: clarify that these terms are introduced by the authors and are not pre-existing parts - line 265: concepts - line 557: more robust "
ARR_2022_95_review,"This paper presents a new dataset called FairytaleQA, which consists of 10,580 expert-annotated questions from 278 children-friendly stories. One key feature of this dataset is that each question is annotated with the specific reading comprehension skill required to answer it. The authors run several versions of BART-based QA models to show that the model cannot obtain human performance. Additionally, they use FairytaleQA for training question generation models and show that it can be used to ask high-quality reading comprehension questions. ",- This paper introduces a reasonable-sized dataset for QA in narratives. The dataset (if available) would be a useful resource for the NLP community.  - A good review of existing datasets that focused on stories and narratives. It covers all the related datasets that I am aware of.  - The categorization of reading comprehension skills in Section 3.2 is useful.  - The paper is fairly easy to follow. ,"- One major concern is that the contribution of this paper to the QA and NLP community seems mediocre. To me, the difference between FairytaleQA and other reading comprehension datasets on stories and narratives is not obvious. Although the additional annotations about reading skills could be counted as one highlight, this categorization is still coarse-grained to me since it seems nothing more than question types (I assume the categorization might be easily inferred from the question surface form, e.g., ""How did ... feel?"" belongs to the ""Feeling"" category).  - Some details of the annotation process are still vague to me. In Line 342, it claims that the ""texts were broken down into small sections based on their semantic content by our annotators."" The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?
- Another worrying fact is that: from Table 5, we see the performance gap between the QA model and the human is not very big. The BART model fine-tuned on FairytaleQA is 0.533 Rouge and 0.536 F1, which is already around 82% of the human performance (0.651 / 0.644). This suggests that FairytaleQA might not be challenging enough. The possibility is that, with some additional engineering efforts on the QA model, FairytaleQA becomes yet another QA dataset that gets solved within a matter of weeks that does not truly evaluate the phenomena it sets out to. ","Minor Suggestions:  - I think explicit and implicit questions might not be a good naming. Factual detail questions and inductive questions might be more clear?  Putting together all the strengths and weaknesses, I suggest this paper would benefit from another round of revision to address the above concerns before publishing. Otherwise, it might become yet another incremental reading comprehension dataset that might be addressed within a couple of months. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- One major concern is that the contribution of this paper to the QA and NLP community seems mediocre. To me, the difference between FairytaleQA and other reading comprehension datasets on stories and narratives is not obvious. Although the additional annotations about reading skills could be counted as one highlight, this categorization is still coarse-grained to me since it seems nothing more than question types (I assume the categorization might be easily inferred from the question surface form, e.g., ""How did ... feel?"" belongs to the ""Feeling"" category).  - Some details of the annotation process are still vague to me. In Line 342, it claims that the ""texts were broken down into small sections based on their semantic content by our annotators."" The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?
- Another worrying fact is that: from Table 5, we see the performance gap between the QA model and the human is not very big. The BART model fine-tuned on FairytaleQA is 0.533 Rouge and 0.536 F1, which is already around 82% of the human performance (0.651 / 0.644). This suggests that FairytaleQA might not be challenging enough. The possibility is that, with some additional engineering efforts on the QA model, FairytaleQA becomes yet another QA dataset that gets solved within a matter of weeks that does not truly evaluate the phenomena it sets out to. 
Minor Suggestions:  - I think explicit and implicit questions might not be a good naming. Factual detail questions and inductive questions might be more clear?  Putting together all the strengths and weaknesses, I suggest this paper would benefit from another round of revision to address the above concerns before publishing. Otherwise, it might become yet another incremental reading comprehension dataset that might be addressed within a couple of months. "
ARR_2022_157_review,"As I am a returning reviewer for the paper I will keep any of my previous points that still hold for this version of the paper and add new ones where appropriate.
I thank the authors for their response and the changes in the paper. I found much of the response (and changes) to be reasonable, with many of my complaints (especially regarding lack of information on the human evaluation) addressed in this version of the paper. The statistical significance tests are appreciated and they seem to support the initial version's claims.  ====== 	 This paper proposes a novel approach to the task of knowledge-grounded dialogue generation, that additionally exploits commonsense external knowledge augmented with named-entity triples extracted from co-reference chains detected in the dialogues themselves. The proposed approach additionally employs a multi-hop attention layer over the dialogue history and corresponding external knowledge to inform the decoding process.
The paper presents both automatic and human evaluation over 2 datasets, and discusses a case study. ","- The augmentation of common sense knowledge and the application of multi-hop attention over the dialogue history and unstructured knowledge for the purposes of knowledge-grounded dialogue generation are novel and interesting.
- The proposed approach is experimentally shown to outperform previous work in automatic evaluation. While automatic evaluation is known to be unreliable in single-reference generation tasks, human evaluation results also indicate that the proposed approach outperforms related work on multiple criteria that effectively measure knowledge precision and recall. ",- Human evaluation suggests that the proposed methods underperforms in fluency. This could be due to their use of a GRU as a decoder. Replacing the decoder with a pretrained language model similar to related work should help fluency. ,"- The paper now has an ablation study that supports each components efficacy, but it's presentation can be improved. Please present the ablation study results in a table and clearly state how each experiment performed numerically. As it is the comparison is quite vague for most of the configurations.
- The D.2 appendix mentions two human annotators. Based on the author responses, that number should be four.
- I suggested for the previous version that the paper would be stronger if it also presented results on other knowledge-grounded benchmarks, e.g. DSTC9 and the Doc2Dial benchmark. The author's response to this was reasonable, and I would suggest that the include their reasoning in the paper as a potential limitation of the approach. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"- Human evaluation suggests that the proposed methods underperforms in fluency. This could be due to their use of a GRU as a decoder. Replacing the decoder with a pretrained language model similar to related work should help fluency. 
- The paper now has an ablation study that supports each components efficacy, but it's presentation can be improved. Please present the ablation study results in a table and clearly state how each experiment performed numerically. As it is the comparison is quite vague for most of the configurations.
- The D.2 appendix mentions two human annotators. Based on the author responses, that number should be four.
- I suggested for the previous version that the paper would be stronger if it also presented results on other knowledge-grounded benchmarks, e.g. DSTC9 and the Doc2Dial benchmark. The author's response to this was reasonable, and I would suggest that the include their reasoning in the paper as a potential limitation of the approach. "
ARR_2022_323_review,"This paper focuses on the problem of fine-gained sentiment analysis, i.e., aspect sentiment triplet extraction (ASTE), and propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully exploit task-dependent relations and the linguistic features of word-pairs for enhancing the performance of the triplet extraction in ASTE. Specifically, a biaffine function with constraints is used to embed different relations of word pairs for multi-channel graph construction and then aggregate information over such graph, meanwhile a refined strategy is proposed for measuring the match degree of word pairs. The experimental results on the benchmark datasets demonstrate the effectiveness of the proposed model. ","1. The usage of rich relations and linguistic features and the construction of multi-channel graph for aspect sentiment extraction problem are novel, which is useful for task-dependent word representation learning and may be benefit for the researchers who encounter similar problems. 
2. The experiments presented in the paper are extensively evaluated. ","1. The proposed model seems to be similar to the multi-relational GCN model in (“Modeling relational data with graph convolutional networks”, 2018), please give the more discussion about the difference of such two works; 2. For Eq. (6), the learning of each channel of each element in adjacency matrix A via the biaffine attention module is unclear, especially for non-numeric linguistic features like POS combination or syntactic dependency types. 
3. The proposed end-to-end framework seems to be a little ad-hoc, such as the motivation of the selection of four types of linguistic features is obscure, meanwhile the linguistic features are extracted by external toolkits, and thus I wonder the influence of toolkit errors on the performance of proposed model. Furthermore, different linguistic features with constraints (R^{pos}, R^{dep}, R^{tbd}, R^{rpd}) may have different influences on the performance of the proposed model, an ablation study is needed for analysis, such as the independent influence of different linguistic features, and the cross-influences of different linguistic features like feature pair (POS, tree-based) or other combinations. In addition, the proposed model need to aggregate the information from different channel, and thus I wonder how it handle the situation if the values of different linguistic features are not in a same scale? 
4. The refined strategy seems to be a coarse strategy, which is only useful for the relations like “A”, actually I wonder the analysis of the refined strategy on different types of relations of word-pairs, which is benefit for better understanding the influence of refined strategy; ","Minor errors: the definition of POS has different meanings, e.g., Part-of-Speech for linguistic features, and a type of 10 self-defined relations of word-pairs, suggest to distinguish such two definitions with different symbols. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",None. ,"1. The proposed model seems to be similar to the multi-relational GCN model in (“Modeling relational data with graph convolutional networks”, 2018), please give the more discussion about the difference of such two works; 2. For Eq. (6), the learning of each channel of each element in adjacency matrix A via the biaffine attention module is unclear, especially for non-numeric linguistic features like POS combination or syntactic dependency types. 
3. The proposed end-to-end framework seems to be a little ad-hoc, such as the motivation of the selection of four types of linguistic features is obscure, meanwhile the linguistic features are extracted by external toolkits, and thus I wonder the influence of toolkit errors on the performance of proposed model. Furthermore, different linguistic features with constraints (R^{pos}, R^{dep}, R^{tbd}, R^{rpd}) may have different influences on the performance of the proposed model, an ablation study is needed for analysis, such as the independent influence of different linguistic features, and the cross-influences of different linguistic features like feature pair (POS, tree-based) or other combinations. In addition, the proposed model need to aggregate the information from different channel, and thus I wonder how it handle the situation if the values of different linguistic features are not in a same scale? 
4. The refined strategy seems to be a coarse strategy, which is only useful for the relations like “A”, actually I wonder the analysis of the refined strategy on different types of relations of word-pairs, which is benefit for better understanding the influence of refined strategy; 
Minor errors: the definition of POS has different meanings, e.g., Part-of-Speech for linguistic features, and a type of 10 self-defined relations of word-pairs, suggest to distinguish such two definitions with different symbols. "
ARR_2022_33_review,"The paper describes a new approach towards MeSH label prediction, utilizing the title abstract journal relative information. The proposed model combines BiLSTMs, Dilated CNNs and GCNNs to extract features from abstracts, titles and the mesh term hierarchy respectively. Limiting the search MeSH space with information extraction from metadata (such as other articles published in that journal) allows for a boost in performance by building dynamic attention masks. The final model shows good performance compared to related approaches, one of which uses the full article. ",- Utilized information past the document itself to limit the MeSH search space - Introduces novel end-to-end architecture that can be used in other tasks involving scholarly articles - Achieves good performance compared to related approaches. ,- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. ,"- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  - A few minor typos, proof reading should fix them. Nothing major. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. 
- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  - A few minor typos, proof reading should fix them. Nothing major. "
ARR_2022_33_review,"This paper introduces a new method for MeSH indexing, combining multiple methods including, but not limited to, dilated CNNs, masked attention, and graph CNNs. Overall, the proposed approach makes substantial improvements over prior state-of-the-art methods. For example, Micro F1 improves over BERTMeSH from 0.685 to 0.745. Similar improvements are also found for the example-based measures (e.g., example-based F1). Furthermore, a comprehensive ablation study was performed, showing that the label feature model has the largest impact on model performance, yet, other parts of the method still impact performance substantially. ","Overall, the paper is well-written and easy to read. Furthermore, the improvement over prior work is substantial. It is neither easy nor trivial to make such considerable performance improvements for MeSH indexing, especially for Micro F1. For instance, BERTMeSH [1] only improves DeepMeSH [2] by only 2% in Micro F1 [1] after five years of work. Hence, seeing a Micro F1 near 0.75 is a huge breakthrough.
References: [1] Peng, Shengwen, et al. ""DeepMeSH: deep semantic representation for improving large-scale MeSH indexing."" Bioinformatics 32.12 (2016): i70-i79.
[2] You, Ronghui, et al. ""BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text."" Bioinformatics 37.5 (2021): 684-692. ","Overall, there are three major weaknesses in this paper.
First, the paper uses a custom training and validation dataset pulled from PubMed, making comparisons difficult. Using the data from the yearly BioASQ shared tasks would be better to use their data so new methods are more easily comparable. I understand this is common in similar studies (e.g., by BERTMeSH [3]), but a standardized dataset seems possible and useful.
Second, while the hyperparameters are discussed, it is not clear whether hyperparameters were optimized for the baseline models. What were the chosen parameters? Was the validation dataset used to optimize them similarly to the proposed method? If so, why is the standard deviation not reported for the baseline models (e.g., in Table 1)? Given the substantial performance differences between the proposed model and prior work, this additional information must be reported to ensure fair comparisons.
Third, while this may be the first paper to use GCNNs for MeSH indexing, it is widely used for similar biomedical text classification tasks (e.g., ICD Coding). For instance, [1] directly combines BiLSTMs with GCNNs and label features in a very similar manner to the method proposed in this paper, albeit with exceptions such as [1] does not use dilated CNNs. Furthermore, that work has been expanded on to better understand the impact of the GCNNs and whether they are needed [2]. Hence, the paper would substantially help if the related work sections were expanded to include citations with similar methodologies. In my opinion, the ""Dynamic Knowledge-enhanced Mask Attention Module"" is one of the most innovative parts of the paper and should be highlighted more in the introduction.
References: [1] Chalkidis, Ilias, et al. ""Large-Scale Multi-Label Text Classification on EU Legislation."" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.
[2] Chalkidis, Ilias, et al. ""An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
[3] You, Ronghui, et al. ""BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text."" Bioinformatics 37.5 (2021): 684-692. ","Page 3, Line 240-252: There are a few variations of LSTMs [1]. Is the one used in this paper the same as the 1997 paper? 
  Page 2, Line 098-100: The phrase ""latent semantics"" is unclear. It may help the paper if that phrase is expanded, e.g., does this mean the contextual information from combining multiple layers of neural networks?
Page 4, Line 287: I believe ""edges are implement MeSH hierarchies"" should be ""edges represent relationships in the MeSH hierarchy"" Page 6, Line 416-417: I believe the phrase "", and we converted all words are lowercased"" Should be "", and we converted all words to lowercase"" References: [1] Graves,A. et al. (2012) Supervised Sequence Labelling with Recurrent Neural Networks. Vol. 385. Springer, Berlin. ",3.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,"3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"Overall, there are three major weaknesses in this paper.
First, the paper uses a custom training and validation dataset pulled from PubMed, making comparisons difficult. Using the data from the yearly BioASQ shared tasks would be better to use their data so new methods are more easily comparable. I understand this is common in similar studies (e.g., by BERTMeSH [3]), but a standardized dataset seems possible and useful.
Second, while the hyperparameters are discussed, it is not clear whether hyperparameters were optimized for the baseline models. What were the chosen parameters? Was the validation dataset used to optimize them similarly to the proposed method? If so, why is the standard deviation not reported for the baseline models (e.g., in Table 1)? Given the substantial performance differences between the proposed model and prior work, this additional information must be reported to ensure fair comparisons.
Third, while this may be the first paper to use GCNNs for MeSH indexing, it is widely used for similar biomedical text classification tasks (e.g., ICD Coding). For instance, [1] directly combines BiLSTMs with GCNNs and label features in a very similar manner to the method proposed in this paper, albeit with exceptions such as [1] does not use dilated CNNs. Furthermore, that work has been expanded on to better understand the impact of the GCNNs and whether they are needed [2]. Hence, the paper would substantially help if the related work sections were expanded to include citations with similar methodologies. In my opinion, the ""Dynamic Knowledge-enhanced Mask Attention Module"" is one of the most innovative parts of the paper and should be highlighted more in the introduction.
References: [1] Chalkidis, Ilias, et al. ""Large-Scale Multi-Label Text Classification on EU Legislation."" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.
[2] Chalkidis, Ilias, et al. ""An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels."" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.
[3] You, Ronghui, et al. ""BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text."" Bioinformatics 37.5 (2021): 684-692. 
Page 3, Line 240-252: There are a few variations of LSTMs [1]. Is the one used in this paper the same as the 1997 paper? 
  Page 2, Line 098-100: The phrase ""latent semantics"" is unclear. It may help the paper if that phrase is expanded, e.g., does this mean the contextual information from combining multiple layers of neural networks?
Page 4, Line 287: I believe ""edges are implement MeSH hierarchies"" should be ""edges represent relationships in the MeSH hierarchy"" Page 6, Line 416-417: I believe the phrase "", and we converted all words are lowercased"" Should be "", and we converted all words to lowercase"" References: [1] Graves,A. et al. (2012) Supervised Sequence Labelling with Recurrent Neural Networks. Vol. 385. Springer, Berlin. "
ARR_2022_147_review,This work proposes a pipeline for combining existing general knowledge resources with domain specific knowledge extracted from the web via constructed query templates and evaluates the contributions of the domain specific knowledge in two training approaches: a) selecting relevant sentences and b) generated relevant sentences in the context of counseling dialogue applications. ,"+A well written paper with clear organization, goals, and results +Clear task definition +Reasonable choices for different aspects of the process and justification for each of the pipeline elements.  +Good understanding of models, resources and prior work.
+A pipeline that can inspire others to replicate and improve. ","-The chosen evaluation metrics are neither appropriate nor insightful for the task. However, I understand that the authors made the best they could to provide some evaluation based on standard and community accepted metrics and that there are no standard metrics for the task. It is still unsatisfying that a well thought out pipeline has not been convincingly evaluated.
-The human evaluation helps somewhat but is very limited because it lacks comparisons with other methods.
-Use of existing resources -- lack of scientific novelty ",I would strongly advise the authors to provide a discussion/comparison with respect to: https://aclanthology.org/2020.sigdial-1.2.pdf Consider improving the work with a more sophisticated human evaluation on the actual dialogue task. Can you recruit volunteer patients? ,"4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,"3 = From the contents of the submission itself, I know/can guess at least one author's name.","4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.","It is not clear to me if the following paper is not cited and compared with to preserve anonymity (i.e., some authors are the same). 
https://aclanthology.org/2020.sigdial-1.2.pdf If not, it's an important omission in related work and comparison of models. If this is the case, I would lower my overall rating. ","-The chosen evaluation metrics are neither appropriate nor insightful for the task. However, I understand that the authors made the best they could to provide some evaluation based on standard and community accepted metrics and that there are no standard metrics for the task. It is still unsatisfying that a well thought out pipeline has not been convincingly evaluated.
-The human evaluation helps somewhat but is very limited because it lacks comparisons with other methods.
-Use of existing resources -- lack of scientific novelty 
I would strongly advise the authors to provide a discussion/comparison with respect to: https://aclanthology.org/2020.sigdial-1.2.pdf Consider improving the work with a more sophisticated human evaluation on the actual dialogue task. Can you recruit volunteer patients? "
ARR_2022_147_review,"The authors present an approach for knowledge enhanced counseling reflection generation. It uses dialogue context as well as commonsense and domain knowledge for generating responses in counseling conversations. Two methods for knowledge integration are proposed: a retrieval-based method and a generative method. Experimental results show that both methods for knowledge incorporation improve the system's performance.
CONTRIBUTIONS: (1) The authors propose a pipeline that collects domain knowledge (medical) through web mining and apply it to build up a counseling knowledge base. 
(2) The authors use the domain knowledge they collected along with commonsense knowledge bases for the task of reflection generation. 
(3) The authors analyze different types of commonsense and domain knowledge, as well as their effect on the generation task. ","- Overall, the paper is clear in its objectives and methodology followed. The work is well structured, easy to read and follow.
- The authors show empirical success of their approach.
- The overall story is convincing. The proposed approach is tested with reasonable models and appropriate experiments. The experimental results are promising, demonstrating the effectiveness of the proposed method. Thus, the paper makes valuable contributions to the field.
- The approach is well motivated and addresses a problem that is relevant to the community. ","- Lack of illustrative examples regarding the model outputs.
- Some details regarding the knowledge collection process have been omitted (see ""Questions"" below). ","QUESTIONS: - Fig. 2: Why did you discard the ""anatomy"" category?
- l. 221: How many query templates did you specify in total?
- l. 227: What's the size of the set of knowledge candidates?
- l. 550: Did you calculate the agreement between the annotators? Were the annotators authors of the paper?
MINOR: - Try to better align the figures with the text.
- fix punctuation: l. 336, l. 433, l. 445, l. 534 - Table 2: The highlighting of the numbers does not correspond to the caption (""highest scores are in bold, second highest scores in italic"") ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Lack of illustrative examples regarding the model outputs.
- Some details regarding the knowledge collection process have been omitted (see ""Questions"" below). 
QUESTIONS: - Fig. 2: Why did you discard the ""anatomy"" category?
- l. 221: How many query templates did you specify in total?
- l. 227: What's the size of the set of knowledge candidates?
- l. 550: Did you calculate the agreement between the annotators? Were the annotators authors of the paper?
MINOR: - Try to better align the figures with the text.
- fix punctuation: l. 336, l. 433, l. 445, l. 534 - Table 2: The highlighting of the numbers does not correspond to the caption (""highest scores are in bold, second highest scores in italic"") "
ARR_2022_333_review,"This paper introduces the first Thai Nested NER dataset, with a large size (largest non-English dataset) and high inter agreement. Along with introducing how they annotate the dataset, they also train three English SOTA models on the dataset to set a baseline performance for future works. ",- The dataset could be very useful for future works. It's large with high quality.  - Experiments are good. Different models have been evaluated. ,"- The writing is really poor. Many places are very confusing. The figures are not clearly separated from the text, and it is confusing that where I should look at. Many sentences use the past tense, while other sentences use the present tense. The only reason that I would like this paper to be accepted is because of the dataset. The writing itself is far from a solid paper, and I suggest authors go over the writing again.
- This dataset is the first Thai N-NER dataset, and N-NER in Thai is a new task for the community, so it could be very insightful to know what specific challenges are in Thai, and what errors the models make. The paper provides an error analysis, but not deep enough. It would be insightful if the authors could list error patterns at a finer granularity. ","It is also unclear to me that why syllable segmentation could be useful for the annotation. Many of the readers do not know Thai, so I think more explanation is necessary.
For the writing part, for example, Section 3 is mixed with the past tense and present tense. Figure 2 is hidden in the text. I suggest the authors put all the tables and figures at the top of the pages. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The writing is really poor. Many places are very confusing. The figures are not clearly separated from the text, and it is confusing that where I should look at. Many sentences use the past tense, while other sentences use the present tense. The only reason that I would like this paper to be accepted is because of the dataset. The writing itself is far from a solid paper, and I suggest authors go over the writing again.
- This dataset is the first Thai N-NER dataset, and N-NER in Thai is a new task for the community, so it could be very insightful to know what specific challenges are in Thai, and what errors the models make. The paper provides an error analysis, but not deep enough. It would be insightful if the authors could list error patterns at a finer granularity. 
It is also unclear to me that why syllable segmentation could be useful for the annotation. Many of the readers do not know Thai, so I think more explanation is necessary.
For the writing part, for example, Section 3 is mixed with the past tense and present tense. Figure 2 is hidden in the text. I suggest the authors put all the tables and figures at the top of the pages. "
ARR_2022_333_review,"The paper is about the creation of a large dataset for Thai nested named entity recognition. The dataset is characterized by manually curated class and span annotations, and by the use of a very large class set, 104 classes. 
In addition, some experiments about using known methods and a new innovative task approach for NNER task are described and analysed. ","The paper introduces a new dataset which could be used by others to make different experiments. 
The paper introduces three different baselines for comparing the experiments with new state of the art models. An error analysis is also included. ","The paper covers too many topics and some of them are not clear enough. 
The paper describes the creation of the dataset, the characteristics of Thai that impact the task, the processing problems, the guidelines and human annotation procedures, evaluation and quality control of the manual annotation, five different experiments offered as baseline and 3 experiments with state of the art models, and error analysis. ( Note that there are four pages of appendices). However, some of the topics are not totally covered. For instance, for the description of Thai characteristics, and associated challenges. It is not clear to me the problem of segmentation and it is not clear what are the human annotators doing after the syllable segmentation. An example would help to see the problem in this context. ","Line 302, right column. "" to trained the model"" => to train the model ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The paper covers too many topics and some of them are not clear enough. 
The paper describes the creation of the dataset, the characteristics of Thai that impact the task, the processing problems, the guidelines and human annotation procedures, evaluation and quality control of the manual annotation, five different experiments offered as baseline and 3 experiments with state of the art models, and error analysis. ( Note that there are four pages of appendices). However, some of the topics are not totally covered. For instance, for the description of Thai characteristics, and associated challenges. It is not clear to me the problem of segmentation and it is not clear what are the human annotators doing after the syllable segmentation. An example would help to see the problem in this context. 
Line 302, right column. "" to trained the model"" => to train the model "
ARR_2022_333_review,"The paper proposes a new nested named entity recognition dataset for the Thai language. It is formed of 8 layers with 10 coarse-grained classes and 104 fine-grained classes. The dataset is formed of about 5K documents collected from two domains, news articles and restaurant reviews. The data was annotated using an annotation guideline and the annotation steps were explained. The dataset was evaluated using SOTA models. ",A nested NER dataset with a wide coverage was compiled for the Thai language. It is the first such dataset for the language. It may be useful for nested NER studies for that language and also for low-resource languages as a comparison. Quality of the annotations was checked. ,"There some parts left unclear during the compilation process. Please see below. 
Also the dataset submitted as supplementary material to the paper seems to be a small part of the whole dataset. ","My comments are as follows: - How did you decide the number of classes (10 for coarse-grained and 104 for fine-grained) and how did you decide these classes?
- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.
- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.
- Another related issue is, if the dataset was annotated on a syllable basis, how useful will be such an annotation for NLP tasks? For instance, relation extraction, where we usually extract the NEs and the find relations between them. By using syllable annotations, how can we find word NEs? Using a segmenter or tokenizer?
- How is the annotation on Thai NER datasets, on a syllable-basis or word-basis, and why?
- It is stated that the dataset includes 4,894 documents. In NER datasets, it is better to state the size of the dataset with the number of sentences. How many sentences does the dataset include?
- Were these 4,894 documents selected randomly? ",3.5 ,No,3 = Potentially useful: Someone might find the new datasets useful for their work.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",1 = They would not be able to reproduce the results here no matter how hard they tried.,,"There some parts left unclear during the compilation process. Please see below. 
Also the dataset submitted as supplementary material to the paper seems to be a small part of the whole dataset. 
My comments are as follows: - How did you decide the number of classes (10 for coarse-grained and 104 for fine-grained) and how did you decide these classes?
- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.
- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.
- Another related issue is, if the dataset was annotated on a syllable basis, how useful will be such an annotation for NLP tasks? For instance, relation extraction, where we usually extract the NEs and the find relations between them. By using syllable annotations, how can we find word NEs? Using a segmenter or tokenizer?
- How is the annotation on Thai NER datasets, on a syllable-basis or word-basis, and why?
- It is stated that the dataset includes 4,894 documents. In NER datasets, it is better to state the size of the dataset with the number of sentences. How many sentences does the dataset include?
- Were these 4,894 documents selected randomly? "
ARR_2022_85_review,This paper adopts consistent representation with contrastive learning and knowledge distillation to maintain the stability of the relation mebedding. The resulsts on two dataset prove the effectiveness of the method. ,The method of using contrastive learning and knowledge distillation is interesting. There was a slight improvement in the experiment. The method proposed is easy to reproduce. ,"1. Less experimental improvement. The effectiveness of the KL is unclear. 
2. Analyses of experiments are standard but inadequate. ","1. Can you explained more about the relationship of Memory, Memory Bank, Memory Knowledge and Embedding Knowledge in your paper? 
2. How ia the positive samples of comparative learning constructed in Section 3.6? 
3. There is an error in bold T4 in the lower table of Table1. So the promotion on the TACRED dataset is not consistent. 
4. Why does CRL (w/o KL) sometimes perform better than CRL in Table1? Is knowledge distillation really always effective? 
5. In line490-492, the explanation of why CRL (w/o CR) will degrade performance seems to conflict with the reason why using knowledge differentiation(see line 341-346). 
6. It is suggested that some quantitative analysis can be given, for example, the distance of inter-class. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. Less experimental improvement. The effectiveness of the KL is unclear. 
2. Analyses of experiments are standard but inadequate. 
1. Can you explained more about the relationship of Memory, Memory Bank, Memory Knowledge and Embedding Knowledge in your paper? 
2. How ia the positive samples of comparative learning constructed in Section 3.6? 
3. There is an error in bold T4 in the lower table of Table1. So the promotion on the TACRED dataset is not consistent. 
4. Why does CRL (w/o KL) sometimes perform better than CRL in Table1? Is knowledge distillation really always effective? 
5. In line490-492, the explanation of why CRL (w/o CR) will degrade performance seems to conflict with the reason why using knowledge differentiation(see line 341-346). 
6. It is suggested that some quantitative analysis can be given, for example, the distance of inter-class. "
ARR_2022_148_review,"This paper works on personalized language modeling in case that there is limited data for the target user (i.e. a new user). The authors propose that leveraging the data from similar users extracted from the corpus to enhance the personalized language modeling for a new user. They first introduce three methods to calculate the similarity between a target user and the selected anchor users, and then either leverage the data of similar anchor users to fine-tune the language model or interpolate the anchor user models based on the similarity. The experiments show that their methods can improve the perplexity and accuracy, and more analysis explores the trade-off between the amount of available data from similar users. ","1. The paper explores an interesting topic of personalized language modeling. 
2. The proposed methods are easy to implement. ","The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing. ","Suggestions: 1. Add more figures to clarify the ideas. 
2. Leave the training details (like how many hours it takes to train) in a single section like ""implementation details"". 
3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing. 
Suggestions: 1. Add more figures to clarify the ideas. 
2. Leave the training details (like how many hours it takes to train) in a single section like ""implementation details"". 
3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result. "
ARR_2022_252_review,The authors propose to use static semi-factual generation + dynamic human-intervened correction to get better performance on OOD and few-shot performance. The method inlcudes 2 major steps: 1) use a rationale extraction model trained on small amount of annotated rationales to highlight rationales and replace non-rationales with synonyms (semi-factual generation) and 2) ask human annotators to identify false rationales (use synonym replacement for the false span) and missing rationales (extract a subsequence) to generate as the new examples. The authors compare with CAD and show better in-domain and OOD performance. ,"- The idea of using semi-factual generation is novel and interesting.
- Using raitonale model to expose model reasoning and ask human annotators for minimal effort (only identifies errors and leaves generating new examples to models) can largely reduce the annotation effort.
- The result that show better OOD performance against CAD with fewer number of examples is interesting as the effort to annotate the data is much less. ","- Some experiments don’t add much to the analysis: Duplication (line 493) doesn’t seem like a reasonable baseline to compare to. It merely multiplies the 50 examples without adding any new information.
- The semi-factual generation on model generated rationale in the first step: the replacement could be done on missing rationales, This will remove the correct rationales form the example. It would be better if the paper address this (even simply pointing out that it’s not an issue).
- Some annotation protocals and their encessities are not explained. E.g., line 223-224 and line 230-231. ","- Typo line 1: rational-centric → rationale-centric - Was the notation in line 284 intentional? Would look nicer to use $x'$ instead.
- Not sure if *inductive bias* is the right term to characterize what the annotation captures. Personally, I think of it more as *invariance*. (This point does not factor in to the final score since it's more of a personal take.) ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.",,"- Some experiments don’t add much to the analysis: Duplication (line 493) doesn’t seem like a reasonable baseline to compare to. It merely multiplies the 50 examples without adding any new information.
- The semi-factual generation on model generated rationale in the first step: the replacement could be done on missing rationales, This will remove the correct rationales form the example. It would be better if the paper address this (even simply pointing out that it’s not an issue).
- Some annotation protocals and their encessities are not explained. E.g., line 223-224 and line 230-231. 
- Typo line 1: rational-centric → rationale-centric - Was the notation in line 284 intentional? Would look nicer to use $x'$ instead.
- Not sure if *inductive bias* is the right term to characterize what the annotation captures. Personally, I think of it more as *invariance*. (This point does not factor in to the final score since it's more of a personal take.) "
ARR_2022_252_review,"The paper presents a rational-centric framework with human-in-the-loop to boost model out-of-distribution performance in few-shot learning scenarios. The proposed approach uses static semi-factual generation and human corrections to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalization. Experimental results shows the superiority of the proposed approach on in-distribution and out-of-distribution settings, especially for few-shot learning scenarios. ","- Improving out-of-distribution model performance specially in few-shot learning settings is an important problem of real-life need in the NLP community.
- The proposed approach is simple and can be applied for NLP tasks where rationales can be easily identified and annotated. For instance, in sentiment analysis and text classification tasks.
- The proposed approach provides cost savings in comparison to alternative approaches for data augmentation, and provides strong out-of-distribution as well as in-distribution performance in few-shot learning settings. ","- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.
- It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.
- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. ","The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the ""bold"" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,3 = Potentially useful: Someone might find the new datasets useful for their work.,3 = Potentially useful: Someone might find the new software useful for their work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.
- It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.
- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. 
The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the ""bold"" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better. "
ARR_2022_286_review,"This paper presents an algorithm to detect if unargmaxable classes occur given a certain model. 
On top of that, the authors find that although theoretically possible, the ""stolen probability problem (SPP)"" rarely occurs in reality.
To expand a bit, the authors start by revisiting the SPP, which sometimes goes by the name of softmax bottleneck in literature. 
In its essence, the problem arises from the hidden dimension being smaller than the output vocabulary size. 
The problem basically states that the model can not output arbitrary distribution over the vocabulary.
Moving on, the authors propose an approximate algorithm that pre-filters candidates, and then an exact algorithm is applied to decide if a certain class is unargmaxable.
With the tools ready, the authors then examine various LMs and MT models for the SPP. 
Unsurprisingly, most of the models do not suffer from the problems and those who do, only fail to argmax some ""corner case"" subwords. ","The authors provided a nice geometrical intuition for the SPP. 
Although the problem is known before, I think the authors did a good job in presenting their take on the problem.
The authors propose an algorithm to find the unargmaxable classes which is a combination of an approximate filtering and exact search.
The authors examined a large quantity of LM and MT models and only found a limited number of models suffering from SPP. 
Additionally, for those models, they show that the problem is not severe as the classes are ""corner case"" subwords.
The paper is easy-to-follow.
The final comment in the future work section about the connection between the difficulty of model training and the difficulty of finding the unargmaxable classes is definitely interesting. ","While there exist many papers discussing the softmax bottleneck or the stolen probability problem, similar to what the authors found, I personally have not found enough evidence in my work that the problem is really severe. 
After all, there are intrinsic uncertainties in the empirical distributions of the training data, and it is quite natural for us to use a smaller  hidden dimension size than the vocabulary size, because after all, we call them ""word embeddings"" for a reason. 
I guess what I mean to say here is that the problem is of limited interest to me (which says nothing about a broader audience) because the results agree very well with my expectations. 
This is definitely not against the authors because they did a good job in showing this via their algorithm and the concrete experiments. ","I feel like the authors could mention and expand on the implications when beam search is used. 
Because in reality, especially that many MT models are considered in the paper, greedy search is seldomly used. 
In other words, ""even if greedy search is used, SPP is not a big deal, let alone that in reality we use beam search"", something like that.
Compared to the main text, I am personally more interested in the point brought up at L595. 
What implications are there for the training of our models? 
How does the gradient search algorithm decide on where to put the word vectors and hidden state vector? 
Is there anything we, i.e. the trainers of the NNs, can do to make it easier for the NNs?
Small issues: - L006, as later written in the main text, ""thousands"" is not accurate here. Maybe add ""on the subword level""?
- L010, be predicted - L034, personally, I think ""expressiveness"" is more commonly used, this happens elsewhere in the paper as well.
- L082, autoencoder - L104, greater than or equal to ",3.5 ,No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",5 = They could easily reproduce the results.,,"While there exist many papers discussing the softmax bottleneck or the stolen probability problem, similar to what the authors found, I personally have not found enough evidence in my work that the problem is really severe. 
After all, there are intrinsic uncertainties in the empirical distributions of the training data, and it is quite natural for us to use a smaller  hidden dimension size than the vocabulary size, because after all, we call them ""word embeddings"" for a reason. 
I guess what I mean to say here is that the problem is of limited interest to me (which says nothing about a broader audience) because the results agree very well with my expectations. 
This is definitely not against the authors because they did a good job in showing this via their algorithm and the concrete experiments. 
I feel like the authors could mention and expand on the implications when beam search is used. 
Because in reality, especially that many MT models are considered in the paper, greedy search is seldomly used. 
In other words, ""even if greedy search is used, SPP is not a big deal, let alone that in reality we use beam search"", something like that.
Compared to the main text, I am personally more interested in the point brought up at L595. 
What implications are there for the training of our models? 
How does the gradient search algorithm decide on where to put the word vectors and hidden state vector? 
Is there anything we, i.e. the trainers of the NNs, can do to make it easier for the NNs?
Small issues: - L006, as later written in the main text, ""thousands"" is not accurate here. Maybe add ""on the subword level""?
- L010, be predicted - L034, personally, I think ""expressiveness"" is more commonly used, this happens elsewhere in the paper as well.
- L082, autoencoder - L104, greater than or equal to "
ARR_2022_289_review,"The paper describes a novel prompt-based transfer learning approach for text generation i.e., PTG. Their method learns a fixed number of source prompts (represented as vectors) based on source generation tasks and then derives a target prompt for the target generation task. The latter is achieved using an adaptive attention mechanism that utilizes both task-level and instance-level information. The effectiveness of the proposed method is evaluated on a diverse set of source-target tasks under different experimental conditions that test how the approach performs under various data-scarce settings (defined w/ respect to the target task). Additionally, the source prompts will be released as an open-source library to enable future experimentation on prompt-based transfer learning in other tasks/domains. ","The main strengths of the paper are as follows: a) The paper outlines a novel transfer learning approach that employs prompt-based generation with pre trained language models; b) Experiments are comprehensive (covering a wide range of approaches: LM-based fine-tuning, prompt-based generation & transfer learning). The results are satisfying and robust across different conditions (datasets, metrics, data-sizes); c) The analysis is informative. It gives insights into the role of different components of the proposed approach through ablation experiments and further confirms the author’s intuition on the properties of the learned source prompts via semantic similarity analysis;   d) The authors will release their source prompts as an open-source library which can be used to test how the approach can be adapted/extended to model other generation tasks; e) Overall, the paper is clearly presented and is well written (I have several recommendations on how writing can be further fleshed out but I believe that all my comments can be reasonably addressed in the camera ready version). ","The (minor) weaknesses of the paper are: a) Several clarifications need to be made to ensure reproducibility (see detailed comments below); b) The authors should address/clarify the choice of the hyperparameter λ used in the approach (see [General Question] below); c) The paper does a good job of explaining how/where the proposed approach works. Yet, we are left unclear on its limitations. What are some target tasks that could potentially be hard to benefit from transfer learning under the PTG framework? If space permits, the authors could add a paragraph explaining their thoughts on “Limitations”. ","[General Question] The final matching score between the instance x and prompt p_t requires setting a hyperparameter λ that defines the relevant contribution of the task- and instance-level information. Can you expand on how you choose this parameter for each task? More crucially, how sensitive are your results on the choice of this parameter per task and dataset choice? Do you have results on how the performance varies across different choices of λ, or alternatively, do you have an intuition on what types of target tasks might benefit more from larger instance- vs. task-information contributions?
[Section 1] High-level comment: The problems of adaptation to (a) a new task vs. (b) a new domain are discussed in parallel. The introduction can be fleshed out a bit more to present the two problems separately and clarify/motivate how the proposed approach targets each of them. Additionally, the terms “domain” and “datasets” are used interchangeably throughout the paper (i.e., “domain” is line 46 vs. “datasets” in lines 62). Consider sticking to one term throughout the paper?
[Section 1] line 39: “corpus” -> “corpora” [Section 1] lines 55-58: Is this the first work that framed generation tasks as language modeling by predicting the next token given previous tokens?   [Section 1] line 78: Can you expand on what the “soft prompts” are? It is hard to understand what is meant out-of-context.
[Section 1] line 63: In the previous paragraph you refer to the problems of adapting to a new task and datasets. Starting in line 63 you are listing prompt-based methods that address only the first part of the issue. Consider clarifying what you mean by “this problem” in beginning of the  paragraph?
[Section 1] lines 84-86: Can you provide some more context on whether/how the difficulty of transferring relates to the similarity/nature of the two tasks (i.e.,  source and target)?  [Section 1] lines 89-94: Can you give examples of two different instances drawn from the same task that should be handled using different prompts? This is crucial, to understand why incorporating instance-level information is needed and can help motivate better your approach!
[Section 1] line 110: At this point of the paper it is unclear what “prompt clusters” referred to.  [Section 1] lines 128-134: Since this is a transfer setting that involved different levels of supervision from multiple tasks and domains (source vs. target datasets) it is not super clear what the “fully-supervised” and “few-shot settings” refer to. It becomes clearer later on in the experimentation section that those terms refer to target sets, but it would be better to clarify early on.  [Section 3] line 197: Another benefit of working with continuous prompts (that is also specific to your approach) is that it allows you to form prompt clusters, something that is not straightforward to do with discrete prompts. Maybe use this to explain/motivate why you opt for continuous prompts early on the problem formulation?
[Section 3] lines 224-227: Need to clarify: Are the source prompts learned jointly for all source tasks?
[Section 4] lines 242-248: Does that imply that both source prompts and clustered prompts are included in the pool? Can you clarify on what the size of the source prompt pool is (in a similar way you are introducing source prompt dimensions in the preliminary section)? To make understanding easier, I would start by outlining what the role of source prompts and cluster prompts is in the beginning of 4.1 and then expand on the more low-level details of how each of them is computed.  [Section 4] lines 379-401: The ‘Model Discussion’ subsection seems more of an outline of the approach rather than an actual discussion. Consider rephrasing it and move it in the beginning of section 4?
[Section 5] lines 425-446: When describing the baselines it would be useful to comment on whether each of them (a) employs transfer or not, (b) is prompt-based or not . Ideally, such a clusterings should be shown in the Tables and discussion in the results section as it seems there are multiple layers of comparisons made here.   [Section 5] lines 448-449: Which PLM is used by your approach? ( I saw that the answer to the question is BART based on details provided in the Appendix, but it is preferable that this context is provided in the Experimental Setup to help make comparisons with other baselines more meaningful) [Section 5] Subsection Header: “Fully-Supervised Setting” is not self-explanatory. Can you describe the two settings “supervised” and “few-shot” in the experimental setup before moving to the results discussion?
[Section 5] lines 470-482: It is unclear what the exact experimental setting looks like for each of the “cross-task” and ‘cross-dataset“ sets of experiments. I would move details on that in the Experimental Setup section and discuss only the results in this one. Additionally, I have the feeling that whatever is discussed in text in this paragraph could be more concisely presented in a Table. Can you explicitly show what the source and target tasks are for each of the  Tables 1 and 2?
[Section 5] line 468: I would replace the term “cross-dataset” with “within-task” to make it clear that the setting is constrained on datasets of the same nature (cross-tasks might as well refer to two datasets from different tasks).
[Section 5] 493-509: For each of the paragraphs describing the results, can you add specific references to the Tables the reader should refer to?  [Section 5] Tables 1 & 2:  The first 4 rows of Tables 1 and 2 collapse to the same settings and are thus repeated. Consider removing them from Table 2?
[Section 5] Tables 1 & 2: Apart from discussing the results of Table 1 and 2 in isolation, could you also add a small discussion on how your approach yields similar performances across the “cross-task” and “cross-dataset” settings? What are the implications of that? How does that inform future decisions when choosing the source tasks given a known target task?
[Section 5] Tables 3 & 4: It is very hard to read those Tables. I believe what is really important to get out of those numbers is how metrics change in a relative manner when moving to fewer and fewer samples. Consider, instead replacing those tables with graphs (with #of samples in the x axis) and present just one or two metrics in the main text (moving the rest to Appendix for completeness)?
[Section 5] line 558: You mentions that the drop is *significant*, did you employ significance testing for numbers presented in Table 5? If not, consider smoothing out the text and maybe just quantify the drops. ","4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",No,5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.,5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"The (minor) weaknesses of the paper are: a) Several clarifications need to be made to ensure reproducibility (see detailed comments below); b) The authors should address/clarify the choice of the hyperparameter λ used in the approach (see [General Question] below); c) The paper does a good job of explaining how/where the proposed approach works. Yet, we are left unclear on its limitations. What are some target tasks that could potentially be hard to benefit from transfer learning under the PTG framework? If space permits, the authors could add a paragraph explaining their thoughts on “Limitations”. 
[General Question] The final matching score between the instance x and prompt p_t requires setting a hyperparameter λ that defines the relevant contribution of the task- and instance-level information. Can you expand on how you choose this parameter for each task? More crucially, how sensitive are your results on the choice of this parameter per task and dataset choice? Do you have results on how the performance varies across different choices of λ, or alternatively, do you have an intuition on what types of target tasks might benefit more from larger instance- vs. task-information contributions?
[Section 1] High-level comment: The problems of adaptation to (a) a new task vs. (b) a new domain are discussed in parallel. The introduction can be fleshed out a bit more to present the two problems separately and clarify/motivate how the proposed approach targets each of them. Additionally, the terms “domain” and “datasets” are used interchangeably throughout the paper (i.e., “domain” is line 46 vs. “datasets” in lines 62). Consider sticking to one term throughout the paper?
[Section 1] line 39: “corpus” -> “corpora” [Section 1] lines 55-58: Is this the first work that framed generation tasks as language modeling by predicting the next token given previous tokens?   [Section 1] line 78: Can you expand on what the “soft prompts” are? It is hard to understand what is meant out-of-context.
[Section 1] line 63: In the previous paragraph you refer to the problems of adapting to a new task and datasets. Starting in line 63 you are listing prompt-based methods that address only the first part of the issue. Consider clarifying what you mean by “this problem” in beginning of the  paragraph?
[Section 1] lines 84-86: Can you provide some more context on whether/how the difficulty of transferring relates to the similarity/nature of the two tasks (i.e.,  source and target)?  [Section 1] lines 89-94: Can you give examples of two different instances drawn from the same task that should be handled using different prompts? This is crucial, to understand why incorporating instance-level information is needed and can help motivate better your approach!
[Section 1] line 110: At this point of the paper it is unclear what “prompt clusters” referred to.  [Section 1] lines 128-134: Since this is a transfer setting that involved different levels of supervision from multiple tasks and domains (source vs. target datasets) it is not super clear what the “fully-supervised” and “few-shot settings” refer to. It becomes clearer later on in the experimentation section that those terms refer to target sets, but it would be better to clarify early on.  [Section 3] line 197: Another benefit of working with continuous prompts (that is also specific to your approach) is that it allows you to form prompt clusters, something that is not straightforward to do with discrete prompts. Maybe use this to explain/motivate why you opt for continuous prompts early on the problem formulation?
[Section 3] lines 224-227: Need to clarify: Are the source prompts learned jointly for all source tasks?
[Section 4] lines 242-248: Does that imply that both source prompts and clustered prompts are included in the pool? Can you clarify on what the size of the source prompt pool is (in a similar way you are introducing source prompt dimensions in the preliminary section)? To make understanding easier, I would start by outlining what the role of source prompts and cluster prompts is in the beginning of 4.1 and then expand on the more low-level details of how each of them is computed.  [Section 4] lines 379-401: The ‘Model Discussion’ subsection seems more of an outline of the approach rather than an actual discussion. Consider rephrasing it and move it in the beginning of section 4?
[Section 5] lines 425-446: When describing the baselines it would be useful to comment on whether each of them (a) employs transfer or not, (b) is prompt-based or not . Ideally, such a clusterings should be shown in the Tables and discussion in the results section as it seems there are multiple layers of comparisons made here.   [Section 5] lines 448-449: Which PLM is used by your approach? ( I saw that the answer to the question is BART based on details provided in the Appendix, but it is preferable that this context is provided in the Experimental Setup to help make comparisons with other baselines more meaningful) [Section 5] Subsection Header: “Fully-Supervised Setting” is not self-explanatory. Can you describe the two settings “supervised” and “few-shot” in the experimental setup before moving to the results discussion?
[Section 5] lines 470-482: It is unclear what the exact experimental setting looks like for each of the “cross-task” and ‘cross-dataset“ sets of experiments. I would move details on that in the Experimental Setup section and discuss only the results in this one. Additionally, I have the feeling that whatever is discussed in text in this paragraph could be more concisely presented in a Table. Can you explicitly show what the source and target tasks are for each of the  Tables 1 and 2?
[Section 5] line 468: I would replace the term “cross-dataset” with “within-task” to make it clear that the setting is constrained on datasets of the same nature (cross-tasks might as well refer to two datasets from different tasks).
[Section 5] 493-509: For each of the paragraphs describing the results, can you add specific references to the Tables the reader should refer to?  [Section 5] Tables 1 & 2:  The first 4 rows of Tables 1 and 2 collapse to the same settings and are thus repeated. Consider removing them from Table 2?
[Section 5] Tables 1 & 2: Apart from discussing the results of Table 1 and 2 in isolation, could you also add a small discussion on how your approach yields similar performances across the “cross-task” and “cross-dataset” settings? What are the implications of that? How does that inform future decisions when choosing the source tasks given a known target task?
[Section 5] Tables 3 & 4: It is very hard to read those Tables. I believe what is really important to get out of those numbers is how metrics change in a relative manner when moving to fewer and fewer samples. Consider, instead replacing those tables with graphs (with #of samples in the x axis) and present just one or two metrics in the main text (moving the rest to Appendix for completeness)?
[Section 5] line 558: You mentions that the drop is *significant*, did you employ significance testing for numbers presented in Table 5? If not, consider smoothing out the text and maybe just quantify the drops. "
ARR_2022_289_review,"This paper proposes PTG to transfer learned soft prompts to boost the performance on target generation tasks. Compared with previous work on prompt transfer learning, PTG further demonstrates that (1) combining information from multiple source tasks and (2) introducing task-level and instance-level attention mechanisms are helpful. Despite the simple technical design, the experiments show that PTG achieves superior performance than the baseline methods on several NLG tasks. ","1. The paper is well written and well-motivated. 
2. The experiments demonstrate the superiority of the proposed method. Sufficient ablation studies are conducted to show the effectiveness of several core designs. ","1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.
2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.
3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.
4. ( Minor) The proposed method is tied to prompt tuning & generation tasks. It would be interesting if PTG could be extended to other parameter-efficient algorithms (e.g., Adapter and LoRA) and discriminative tasks.
[1] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer ","Typos: (Line 514) For each size, we sample and 5 different datasets and average over 2 training random seeds. ( remove the first ''and'') ","3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.,,,"1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.
2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.
3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.
4. ( Minor) The proposed method is tied to prompt tuning & generation tasks. It would be interesting if PTG could be extended to other parameter-efficient algorithms (e.g., Adapter and LoRA) and discriminative tasks.
[1] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer 
Typos: (Line 514) For each size, we sample and 5 different datasets and average over 2 training random seeds. ( remove the first ''and'') "
ARR_2022_193_review,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries. It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging. However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines. Besides, the related work section is quite sketchy. ","1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general. 
2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages. ","1. The descriptions of several important sentences are unclear, e.g., in “For all languages, we use the reduced version of Wikipedia database”, it is suggested to provide the links to the reduced version of Wikipedia database for all languages (except for English). To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora). 
2. Figure 1 should be illustrated in more details. Why only show the results of three languages in Figure 1? Are the results of other languages consistent with these three languages? 
3. The related work section is quite sketchy. There are limited reviews on collocations and topic models in recent years. One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations. As shown in (Lau et al., 2013), they considered four different bigram replacement methods. Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student’s t-test. Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods. Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams. ","1. The presentation can be improved, e.g., “To train word embeddings” (line 298) and “to obtain word embeddings” (line 301) are repetitive. 
2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014). Besides, the information of reference (Merity et al., 2016) are incomplete. ","2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. The descriptions of several important sentences are unclear, e.g., in “For all languages, we use the reduced version of Wikipedia database”, it is suggested to provide the links to the reduced version of Wikipedia database for all languages (except for English). To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora). 
2. Figure 1 should be illustrated in more details. Why only show the results of three languages in Figure 1? Are the results of other languages consistent with these three languages? 
3. The related work section is quite sketchy. There are limited reviews on collocations and topic models in recent years. One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations. As shown in (Lau et al., 2013), they considered four different bigram replacement methods. Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student’s t-test. Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods. Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams. 
1. The presentation can be improved, e.g., “To train word embeddings” (line 298) and “to obtain word embeddings” (line 301) are repetitive. 
2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014). Besides, the information of reference (Merity et al., 2016) are incomplete. "
ARR_2022_51_review,This paper proposes improving many-to-many NMT using a word-level contrastive loss. The paper also shows that the proposed approach’s translation quality correlates with how well sentences can be retrieved using the encoder’s output. ,"The experiment covers many different setups, with different number of language pairs, a wide variety of languages, and more than 1 neural architecture. The ablation is helpful for explaining where the improvement in NMT performance comes from. ","1.  The choice of the word-alignment baseline seems odd. The abstract claims that “Word alignment has proven to benefit many-to-many neural machine translation (NMT).” which is supported by (Lin et al., 2020). However, the method proposed by Lin et al was used as baseline. Instead, the paper compared to an older baseline proposed by (Garg et al., 2019). Besides, this baseline by Garg et al (+align) seems to contradict the claim in the abstract since it always performs worse than the baseline without word-alignment (Table 2). If for some practical reason, the baseline of (Lin et al., 2020) can’t be used, it needs to be explained clearly.
2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.
3. If the claim that better word-alignment improves many-to-many translation is true, why does the proposed method have no impact on the MLSC setup (Table 3)? Section 4 touches on this point but provides no explanation. ","1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ).
From the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?
2. For Table 3, are the non-highlighted cells not significant or not significantly better? If it’s the latter, please also highlight cells where the proposed approaches are significantly worse. For example, from Kk to En, +FA is significantly better than mBART (14.4 vs 14.1, difference of 0.3) and thus the cell is highlighted. However, from En to Kk, the difference between +FA and mBART is -0.5 (1.3 vs 1.8) but this cell is not highlighted. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.",,,"1.  The choice of the word-alignment baseline seems odd. The abstract claims that “Word alignment has proven to benefit many-to-many neural machine translation (NMT).” which is supported by (Lin et al., 2020). However, the method proposed by Lin et al was used as baseline. Instead, the paper compared to an older baseline proposed by (Garg et al., 2019). Besides, this baseline by Garg et al (+align) seems to contradict the claim in the abstract since it always performs worse than the baseline without word-alignment (Table 2). If for some practical reason, the baseline of (Lin et al., 2020) can’t be used, it needs to be explained clearly.
2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.
3. If the claim that better word-alignment improves many-to-many translation is true, why does the proposed method have no impact on the MLSC setup (Table 3)? Section 4 touches on this point but provides no explanation. 
1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ).
From the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?
2. For Table 3, are the non-highlighted cells not significant or not significantly better? If it’s the latter, please also highlight cells where the proposed approaches are significantly worse. For example, from Kk to En, +FA is significantly better than mBART (14.4 vs 14.1, difference of 0.3) and thus the cell is highlighted. However, from En to Kk, the difference between +FA and mBART is -0.5 (1.3 vs 1.8) but this cell is not highlighted. "
ARR_2022_230_review,"A comprehensive set of experiments are conducted which suggests that baseline wide MLPs with bag-of-words (BoW) input can perform just as well as recently popular graph-based models, such as TextGCN and HeteGCN, in topical text categorization. In addition to wide MLP, sequential BERT and DistilBERT models are fine-tuned, which overall yield state-of-the-art results on 5 well-known text categorization datasets. Parameter counts of the models as well as runtime are also compared, showing the effectiveness of wide MLP with BoW models. ","- A comprehensive comparison of models on text categorization datasets demonstrating the effectiveness of a relatively simple baseline model - Consideration of model size, runtime performance in addition to accuracy metrics - Clear description, detailed explanations, acknowledgement of potential limitations, good insights ","- Limited to multi-class topical text categorization, which is while important, is ultimately just one task - Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) ","- Can parameter counts/training time for graph-based models be provided?
- less classes -> fewer - Minaee et al. (2021) can be cited: Deep Learning–based Text Classification: A Comprehensive Review - Dataset characteristics discussed  (line 359-376) can be combined with Table 2.
- Missing period (line 224).
- Sentence line 315-319 is ungrammatical. ","4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Limited to multi-class topical text categorization, which is while important, is ultimately just one task - Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) 
- Can parameter counts/training time for graph-based models be provided?
- less classes -> fewer - Minaee et al. (2021) can be cited: Deep Learning–based Text Classification: A Comprehensive Review - Dataset characteristics discussed  (line 359-376) can be combined with Table 2.
- Missing period (line 224).
- Sentence line 315-319 is ungrammatical. "
ARR_2022_125_review,"This paper proposed the use of a set of auxiliary tasks to capture some form of interdependency between arcs in semantic dependency parsing in order to provide a simpler implementation that can obtain robust performance, which achieved near-SOTA and systematic performance on one English SemEval-2015 dataset and one French deep syntactic cyclic graphs. ","For a short paper, this paper is well organized and written. The main research question is motivated by efficiently and clearly discussing pros and cons of the previous work and how the current work is related and differs from them. The overall flow of the presentation of the motivations and arguments is smooth, making it easy to follow. The description of the experiments and the discussion of the results are clear and succinct. The use of auxiliary tasks to capture some form of interdependence between arcs for semantic dependency parsing while remaining a reasonable complexity and competitive results seems a contribution to me. ","1. A more explicit definition / description / explanation of what is meant by “systematic” performance gains or improvements seems to be necessary since this is one of the proposed advantages of the proposed method, and it would also be better if you could explicitly describe the connection between ""systematic performance gain"" and the robustness of model performance. The aforementioned two aspects could make the contribution of the proposed method more obvious and salient.   2. A brief discussion of the motivation and selection of the auxiliary tasks seem needed in Section 3. Some of the issues were described in the first paragraph of Section 3, which could be viewed as implicitly motivating the use of the auxiliary tasks, but it still seems not very sufficient: the jump from problem description to “Hence the idea of using auxiliary tasks taking into account all the heads'' seems abrupt. I think a brief elaboration on why these 4 tasks are designed and chosen would make the picture clearer and would also echo the argument mentioned at the beginning, which is to capture some form of interdependence between arcs. ","Content: 1. Line 027: what did you mean by “plain dependency parsing”? Syntactic dependency? It would be clearer if a reference is added here to provide an example.  2. Line 047: it would be better and more explicit if the task and/or dataset is mentioned as part of the “achieve state-of-the-art results” statement.  3. Lines 078-079 / Line 08: For clarity, it would be better if the evaluation metric is mentioned here to better understand the scale of the improvement; this would also be helpful to understand the results reported in this paper for comparability: the expression “labelled F-measure scores (LF1) (including ROOT arcs)” was used in Fernández-González and Gómez-Rodríguez (2020).  4. Line 079: Since ID and OOD are not widely used acronyms and this is the first time of them being used, it would be better to define them first and use ID or OOD thereafter: e.g. in-domain (ID) and out-of-domain (OOD)  5. Lines 130-135: The example wrt the mwe label can probably benefit from a visualization of the description, if the space is allowed. Also, the clause “which never happens in the training set” may benefit from a little rephrasing - if I’m not mistaken, this example was mentioned here to support the claim that “impossible sets of labels for the set of heads of a given dependent”. Does this mean that such a combination is incorrect and thus not possible? If so, saying that such scenarios never happen in the training set could mean that it is likely to conceptually have such combinations exist, but it’s just that it’s not seen in the training data.   6. Is there a specific reason for choosing 9 as the number of runs? ( Conventionally, 3, 5, or 10 are common).  Typos: - Line 018: near-soa > did you mean “near-state-of-the-art (sota)”? Define before using an acronym - Line 036: decision > decisions  - Line 062: propose > proposes   - Line 097: biafine > biaffine  - footnote2: a dot (.) is missing after “g” in “e.g” - only pointed this out as a typo as you used “e.g.” throughout the paper as in Lines 157 / 225 / 230 / 231  - Line 202: experimented > experiment - only pointed this out as a typo as you seem to use present tense to describe other people’s work as in Lines 047 / 078 / 081 - Line 250: provide > provides/provided; significative > significant (also caption in Table 1) Miscellaneous: 1. Inconsistent use of uppercase / lowercase in the title: tasks > Task; boost > Boost 2. Inconsistent intext citation styles:     - Lines 077-078 & 277-278: (Fernández-González and Gómez-Rodríguez, 2020) > Fernández-González and Gómez-Rodríguez (2020)     - Lines 276-277: (He and Choi, 2020) > He and Choi (2020) 3. Placement of footnotes: the footnotes follow the ending punctuation, as shown in the *ACL Template Section 4.1: Lines 104 / 166 / 170 / 231 / 239 / 249 / 250 / 266.  4. FYI - the link to the code in footnote 5 didn’t seem to work; it said “Transfer expired: Sorry, this transfer has expired and is not available any more”. Not sure if I need to register an account to get to the code.   5. Add the abbreviation “LF” after “labeled Fscore” on Line 245 so that the use of “LF” later can be attributed to. ","3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",No,1 = No usable datasets submitted.,4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"1. A more explicit definition / description / explanation of what is meant by “systematic” performance gains or improvements seems to be necessary since this is one of the proposed advantages of the proposed method, and it would also be better if you could explicitly describe the connection between ""systematic performance gain"" and the robustness of model performance. The aforementioned two aspects could make the contribution of the proposed method more obvious and salient.   2. A brief discussion of the motivation and selection of the auxiliary tasks seem needed in Section 3. Some of the issues were described in the first paragraph of Section 3, which could be viewed as implicitly motivating the use of the auxiliary tasks, but it still seems not very sufficient: the jump from problem description to “Hence the idea of using auxiliary tasks taking into account all the heads'' seems abrupt. I think a brief elaboration on why these 4 tasks are designed and chosen would make the picture clearer and would also echo the argument mentioned at the beginning, which is to capture some form of interdependence between arcs. 
Content: 1. Line 027: what did you mean by “plain dependency parsing”? Syntactic dependency? It would be clearer if a reference is added here to provide an example.  2. Line 047: it would be better and more explicit if the task and/or dataset is mentioned as part of the “achieve state-of-the-art results” statement.  3. Lines 078-079 / Line 08: For clarity, it would be better if the evaluation metric is mentioned here to better understand the scale of the improvement; this would also be helpful to understand the results reported in this paper for comparability: the expression “labelled F-measure scores (LF1) (including ROOT arcs)” was used in Fernández-González and Gómez-Rodríguez (2020).  4. Line 079: Since ID and OOD are not widely used acronyms and this is the first time of them being used, it would be better to define them first and use ID or OOD thereafter: e.g. in-domain (ID) and out-of-domain (OOD)  5. Lines 130-135: The example wrt the mwe label can probably benefit from a visualization of the description, if the space is allowed. Also, the clause “which never happens in the training set” may benefit from a little rephrasing - if I’m not mistaken, this example was mentioned here to support the claim that “impossible sets of labels for the set of heads of a given dependent”. Does this mean that such a combination is incorrect and thus not possible? If so, saying that such scenarios never happen in the training set could mean that it is likely to conceptually have such combinations exist, but it’s just that it’s not seen in the training data.   6. Is there a specific reason for choosing 9 as the number of runs? ( Conventionally, 3, 5, or 10 are common).  Typos: - Line 018: near-soa > did you mean “near-state-of-the-art (sota)”? Define before using an acronym - Line 036: decision > decisions  - Line 062: propose > proposes   - Line 097: biafine > biaffine  - footnote2: a dot (.) is missing after “g” in “e.g” - only pointed this out as a typo as you used “e.g.” throughout the paper as in Lines 157 / 225 / 230 / 231  - Line 202: experimented > experiment - only pointed this out as a typo as you seem to use present tense to describe other people’s work as in Lines 047 / 078 / 081 - Line 250: provide > provides/provided; significative > significant (also caption in Table 1) Miscellaneous: 1. Inconsistent use of uppercase / lowercase in the title: tasks > Task; boost > Boost 2. Inconsistent intext citation styles:     - Lines 077-078 & 277-278: (Fernández-González and Gómez-Rodríguez, 2020) > Fernández-González and Gómez-Rodríguez (2020)     - Lines 276-277: (He and Choi, 2020) > He and Choi (2020) 3. Placement of footnotes: the footnotes follow the ending punctuation, as shown in the *ACL Template Section 4.1: Lines 104 / 166 / 170 / 231 / 239 / 249 / 250 / 266.  4. FYI - the link to the code in footnote 5 didn’t seem to work; it said “Transfer expired: Sorry, this transfer has expired and is not available any more”. Not sure if I need to register an account to get to the code.   5. Add the abbreviation “LF” after “labeled Fscore” on Line 245 so that the use of “LF” later can be attributed to. "
ARR_2022_125_review,"This paper proposes to use multi-task learning to improve semantic dependency parsing. Based on the baseline biaffine dependency parser (Dozat and Manning, 2017), four auxiliary tasks are proposed: H (predicting the number of governors for each token), D (predicting the number of dependents for each token), S (predicting all the arc labels for each token in a classification manner), and B (predicting the number of incoming arcs with each label for each token). They evaluate the best configurations of multi-task learning (combinations of auxiliary tasks and final prediction flows) on one French and three English datasets and find that their methods outperform the vanilla biaffine parser.
Overall, the paper is clear to understand and seems to perform solid empirical experiments. However, the proposed method seems too trivial and underperforms the state-of-the-art parser on the English datasets by a large margin. More in-depth analyses are also needed. ","- Solid empirical experiments: Four datasets are used. Each result is averaged from 9 runs, and significance tests are performed.
- Clear writing: While some typos exist and the overall writing can be improved, the paper is very clear to understand. ","- Trivial method: These all four auxiliary tasks seem too trivial to achieve the objective of improving arc consistency prediction since all these tasks are still performed on each token independently.
- Lack of in-depth analysis: Using the B+H auxiliary tasks designed to predict the number of governors and dependents for each token only improves the prediction of the number of heads by 0.6 points. This improvement seems pretty trivial. It would be better if the authors could provide more thorough analyses of why these auxiliary tasks are helpful.
- Underperform SOTA by a large margin: The methods underperform the SOTA parsers by a large margin because no POS or lemma information is used, and the authors claim their methods are directly usable with their system. The paper would be much more solid if the authors could verify this hypothesis. ","- Line 079: Explain ID and OOD.
- Line 130: Explain DM dataset.
- Line 138-144: Why the accuracy of predicting the number of heads is related to the observation that zero-head tokens are predicted too frequently.  - Footnote 1: Why use the representation of the first subword of a word? Did you try using the average of all the subwords?
- Code link seems to be expired. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.","4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",,"- Trivial method: These all four auxiliary tasks seem too trivial to achieve the objective of improving arc consistency prediction since all these tasks are still performed on each token independently.
- Lack of in-depth analysis: Using the B+H auxiliary tasks designed to predict the number of governors and dependents for each token only improves the prediction of the number of heads by 0.6 points. This improvement seems pretty trivial. It would be better if the authors could provide more thorough analyses of why these auxiliary tasks are helpful.
- Underperform SOTA by a large margin: The methods underperform the SOTA parsers by a large margin because no POS or lemma information is used, and the authors claim their methods are directly usable with their system. The paper would be much more solid if the authors could verify this hypothesis. 
- Line 079: Explain ID and OOD.
- Line 130: Explain DM dataset.
- Line 138-144: Why the accuracy of predicting the number of heads is related to the observation that zero-head tokens are predicted too frequently.  - Footnote 1: Why use the representation of the first subword of a word? Did you try using the average of all the subwords?
- Code link seems to be expired. "
ARR_2022_351_review,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements. The evaluation results look good against baseline models. ,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study). The overall performance is good. ,"1. In the backward query of Fig 4, why the span is ""must visit"" instead of ""a must visit"" given the probability of the word ""a"" is 86%, which is the same as the word ""must""? 
2. What if the forward query and backward query give conflict prediction? For example in the example sentence of Fig 2, if the answer to the first query is only ""small"" (without ""good""), what will happen? The example in Fig 4 is too simple. 
3. What are the differences between two versions of ASTE-Data datasets? 
4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct? ","The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper. 
The variables in formula 1 and 2 should be explained, for example, what is pair_asp. ",2.5 ,No,1 = No usable datasets submitted.,1 = No usable software released.,1 = I do not have even an educated guess about author identity.,"3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.",,,"1. In the backward query of Fig 4, why the span is ""must visit"" instead of ""a must visit"" given the probability of the word ""a"" is 86%, which is the same as the word ""must""? 
2. What if the forward query and backward query give conflict prediction? For example in the example sentence of Fig 2, if the answer to the first query is only ""small"" (without ""good""), what will happen? The example in Fig 4 is too simple. 
3. What are the differences between two versions of ASTE-Data datasets? 
4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct? 
The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper. 
The variables in formula 1 and 2 should be explained, for example, what is pair_asp. "

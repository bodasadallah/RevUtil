- The paper is not so clear as the introduction part does not show much background about the cognitive models. The authors may need to refine the writing.  - This paper shows lots of analyses about the attention weights of the Transformer models. Although it presents an in-depth analysis, how to utilize these conclusions is still unclear. If the authors can present a more powerful attention mechanism which correlates better with human attention and improves task performance compared with those baselines, I will be more convinced of the conclusions. 
Comments:  1.  If a model correlates better with human attention compared to other models, does it show better task performance?
 2.  Does all the experiments keep the numbers of the parameters of the compared models the same?
Typos: Line 255 / Line 259:  p < .05 -->  p < 0.05 Line 266:  Correlations grouped by sentence length shows stable values around .6  (SST) and .4-.6 (Wikipedia) except for shorter sen-    tences where correlations fluctuate. -- > 0.6  0.4 0.6 

==================================================

- the main contribution of fine-tuning a multilingual wav2vec model is not very novel.  - If I understand correctly, only the wav2vec is fine-tuned but the other 3 models are trained from scratch, therefore the comparison across the 4 models might not be very meaningful as it is easy to guess that the fine-tuned one would perform best.
- detailed comparison and analysis across models are not shown (e.g: what errors does each model make? is there any particular error trend made by each model?) 
- One point I would like to read more about is why the srs language fails on the XLSR (wav2vec) model, authors mention this is due to an unusually fast convergence but does not explain why this happens. Is it due to the quality of srs dataset or the XLSR model? Shedding more light on this might give us more hints on when we should use wav2vec and when we should refrain from using it.
- As mentioned in the previous section, it would be interesting if authors can discuss more details of each model (e.g: what error each model make and when should we use each model) 

==================================================

I feel the choice of models used by the authors are different on 1 important aspect. It seems like except XLSR-53 all other models were trained from scratch. In my opinion there are 2 important factors in place - (1) single speaker (2) limited data on an endangered language. This can then lead to a variety of questions -  1) Which models (HMM/DNN , CTC , Enc-Dec) is better? 
2) Using pre-trained or training from scratch which is better? 
3) If using pre-trained is it better to use pre-trained model on many languages or just 1 language but multiple speakers is enough.  The results presented by the authors seem to answer these questions but only to some extent. This limits the contribution of this work. It would be great if they can add an additional "from scratch" vs "from pretrained" results for each of the model choices. 
1) Could you provide the source of the g2p rules. It's important for readers to replicate your results as the results are provided in PER and PER is also directly dependent on the ground truth phonemes generated by the g2p rule. 
2) Have you considered using a phone based model like Li et. al. 2020 and using that to then convert to phoneme based rules for the endangered language? 
3) XLSR-53 is a pre-trained acoustic model which is trained in a self-supervised manner. However there are many large multilingual or english models available that can be fine-tuned. It would be interesting to see how they work compared to XLSR-53. For example using the KALDI ASPIRE model, or models built in Dalmia et. al 2018, or multilingual models in ESPNET toolkit, or the hidden representations of the Allosaurus model in Li et. al. 2020; as done in -  https://arxiv.org/pdf/2104.01624.pdf. 

==================================================

1. A minor weakness of the paper is a convoluted affine mechanism which may not be easy to interpret.
2. Some more clarity is required in the method description. Please see the Comments section. 
1. Line no 84:- "...may be..." -> may have been 2. Line no 218:- are the given references for the text encoding the first ones to use such encoding? If not, then please give the appropriate reference. 
3. Line no 203:- Define MLP to avoid any confusion 4. Line no 761:- Rephrase "We specific seeds" 5. Another minor issue is it is not clear how the different spans and boundaries are detected 

==================================================

• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor. 
• The paper doesn’t show the quantitative differences between MT and MTPE datasets are not presented on the paper • The paper shows that machine translated data can be used as a proxy for human translated data, but only compare with post-editing approach which provides bias for the human editor.
• Line 131 “another” -> “other” 

==================================================

- It is unclear how the human annotations process is done (for example: how many annotators per sample and the acceptance criteria).
- The motivation of the three experimental settings is relatively weak. It can be better explained in the introduction. The settings are interesting if they are appropriately described and given a more substantial reason why they are important. For now, it looks like the authors are trying to explore new settings without supporting rationale.
- The authors do not explore the performance drop when using local entities in the experiments. 
Comments: 1. Please add more descriptions as the authors undergo the human annotation process. 
2. It would be better to add examples for each setting.
Questions: 1. What are the criteria for selecting the human annotation results for the post-edit stage? Did you apply additional quality assurance after adding the entities to the samples? And, how much changes after the post-edit stage? 
2. Why did the significant performance drop occur when using local entities in the experiments? 
3. Did you manage to measure the quality of the samples you collected using the proposed method? 

==================================================

Weaknesses: I have two major conerns with the work: - The first is whether the proposed method generalizes to other contexts. The experiments are limited to one domain (biomedical), one task (text classification) and a single teacher model (BioBERT) and it is not clear what happens if any of these parameters are changed. It would be especially important to report results on NER datasets as this is a standard benchmark for biomedical PLMs.    - The second concern is whether the reported experiments actually allow to conclude that the proposed method actually works as intended. Couldn't there be other reasons for the reported improvements of the domain-adapted models over the finetuned ones (-ft vs DoKTra in Table 2)? For instance, it would seem important to compare to using a general-domain teacher to make sure the improvements are actually due to transferred domain knowledge and not because of other factors related to distillation. Relatedly, what would happen when using an ALBERT-sized BERT model as student? This would be important to confirm that the improvements are actually because architectural improvements of the student were utilized. Also, it is mentioned that the student model is also finetuned with a calibration regularizer. It would be important to include ALEBRT-ft + calibration regularizer in the ablation study to make sure that the observed improvements do not stem from the calibration regularizer.
Additionally, I have also one minor comment. A comparison to domain-specific PLMs adapted with continued pretraining would be informative, as it would allow researchers to determine how much performance could be gained if they invested the resources to do that. For this, e.g. the RoBERTa model of [1] could be used. More generally, a comparison with state of the art on the used datasets is missing.
[1]: Gururangan, Suchin, et al. "Don't stop pretraining: adapt language models to domains and tasks." 
Is final classification performed after linear layer that equalizes dimensions or before? 

==================================================

I understand that Section 6 tries to getting more factors involved in the interpretation of the modal's capability. However, the target theses seem reasonable (to get a quick yes answer) yet require much more pages. They are two of the big questions in digital humanities, and I don't see any convincing arguments/reviews on the extended analysis that contributes. 
In general, I think this is a great pioneering work and could be fed into a much more interesting resources in the future. 
It would be better to introduce more about the background for readers who are not familiar with (ancient) Korean. For instance,  why is Hanja an extinct 'language' instead of a 'script'? If ancient Korean only wrote Hanja, why do we call it a 'language'? 
In addition, Table 4 would need more explanation which might really help classical humanities studies, e.g., why naive AnchiBERT only outperforms others on NER? etc. 

==================================================

1. The importance of context is well known and well-established in several prior work related to hate speech. While the paper cites works such as Gao and Huang, 2017 and Vidgen, et al., it just mentions that they don’t identify the role of context in annotation or modeling. The former definitely considers its role for modeling and the latter incorporates it in the annotation phase. Though this work performs analysis of corpus and model to study the role of the context, the claim of being the first work to establish the importance of context may be a little stretched. 
2. Table 2 includes several work but drops out Vidgen et al, 2021, which might be really similar to the dataset presented in this work though the size varies significantly here. So, why is this dataset not used as a potential benchmark for evaluation (for investigating the role of context in detection of hate) as well? 
3. Though MACE can be used to assess the competent annotators and eliminate redundant annotators, it could be challenging to use when it involves most ambiguous content. 
4.  Some of the analysis and results discussed  (for eg. section 6) might be specific to the tested Roberta model. More experiments using different architectures are needed to determine if the findings and errors that arise are consistent across different models and different settings. 
Claim of being the first one to recognize the importance of context might be too stretched. 
More experiments with multiple runs with different random seeds for the dataset split will help report the mean score and the standard deviation. This will help us understand the sensitivity of the model to the data split or order of training etc. 

==================================================

W1: In the process of collecting the Redding messages, a pair (parent, target) is considered, but a better justification of this decision is needed to explain why authors have not considered more response messages.
W2: In the process of annotation analysis, five annotators label each message. An explanation of this number is necessary.  W3: The analysis of the results is complete but didactic examples while explaining each case would help to interpret the discussion of the authors.  W4: An analysis of the difficulty of the annotators in identifying each class is not presented. Would be important to see in which cases the annotators do not agree and why.
W5: I believe that the annotation guide may be too short for annotators who are not familiar with this type of task. Have examples been provided to the annotators? 
- The word "context" in the title of the paper could be more specified. I suggest the authors replace it with "Conversational context", otherwise could be confused by the context of the sentence.  - What is the annotator agreement between the five annotators before employing MACE's method? Would be helpful to see this analysis in order to observe the difficulty of the task.  - How the authors have resolved the possible doubts of the annotators while the annotating process in MTurk?  - Please, mention the language you focus on to retrieve the Reddit posts.  - Please, move Table 1 to page 2.  - The link of the code is not available but the authors in a footnote affirm "Code and data available at anonymous_GitHub_link" - A dot is missing after the "Label distribution and linguistic insights" paragraph title.
- In paragraph 481, please show didactic examples where the "Parent" helps to identify the class of the "Target".  - Please, explain what do you mean by "Intricate text" in Table 8. 

==================================================

- I am not quite clear on which datasets were produced by the authors and which were prior work. I feel this should be described in more detail in the paper.  - What is the source on the amount of Chinese speakers? statista claims it's 16% of the world's population (https://www.statista.com/chart/12868/the-worlds-most-spoken-languages/).
- References to the supplementary material/appendix are missing in the main text and should be inserted in the appropriate locations. This is especially important for the data set details, as these are missing from the main text. 
- L236: Acronym IRB was not introduced - L460: specficanti-toxin -> specific anti-toxin - Alibaba QUAKE is sometimes spelled Alibaba KUAKE, is this on purpose? 

==================================================

Minor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later. 
Specific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA. 
2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice. 

==================================================

The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below. 
Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue. 
The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)
- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.  - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?
- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.
- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting "matched" for "not matched" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions. 

==================================================

1. My biggest concern is whether the templates are domain or dataset-specific. The motivation is to address the domain generalization on column operations, but it is implemented by manually crafted ad-hoc templates that could be tailored for the datasets. I understand this paper is the first step to tackle this problem. The authors also say that improvement does exist and automatic schema expansion is worth future study. 
2. I have not extensively studied the benchmarks in the text-to-SQL task. So I'm not sure whether repartitioning the SQUALL is the best choice or any other resources are also available. 
1. I have a question regarding the schema expansion for the synthetic dataset. According to my understanding,  Table 2 shows the templates used for the SQUALL. What are the templates used for the synthetic dataset? Are they the same as the formulas used in the dataset construction (Table 5)? If so, will this setting make the synthetic dataset easier? The figures in Table 3 demonstrates that the performance on the synthetic dataset is much better than that on the repartitioned SQUALL. 
2. I haven't seen any typos. The paper is clearly written. 

==================================================

1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, "What was <column> in <year>?".
2. The paper assumes strong domain knowledge about the column types and assumes a domain developer first creates a set of templates based on column types. With the help of these column templates, I think many approaches (parsers) can easily solve the problem. For example, parsers utilizing the SQL grammar to generate the output SQL can use these templates to add new rules that can be used while generating the output. Few such works are 1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. 
1. It will good if the authors can learn the templates for schema expansion from source domain data. 
2. Compare the proposed approach with methods which uses domain knowledge in the form of grammar. Comparing with below methods will show generality of ideas proposed in the paper in a much better way.
1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021. 

==================================================

1. Despite the good motivation, there is a lack of empirical justification for the collected dataset. As mentioned in the paper, there are some underlying principles adopted for data collection. It would be more convincing to present some evidences showing that the new dataset really achieves that goal, comprehensively evaluating children’s RC skills, in comparison to existing datasets (e.g. NarrativeQA).  2. The evaluation of QAG methods is problematic. The author adopted the ROUGE-L metric for evaluation, but there is no justification that this metric is reliable in the addressed task: How likely a QA-pair with high ROUGE score can faithfully evaluate children’s RC skills? Although they conduct human evaluation, the evaluation metrics are not closely related to the educational purposes. 
The author also mentions a user study: “A preliminary user study with 12 pairs of parents and children between the ages of 3-8 suggests that this application powered by our QAG system can successfully maintain engaging conversations with children about the story content. In addition, both parents and children found the system useful, enjoyable, and easy to use”. But no details are included, and hence it is hard to determine whether the user study successfully proves the claims in the paper.  3. There are lots of details of data collection that are missing in the paper. What are the sources of stories? What are the coding templates used? 
“we developed a new RC dataset targeting students from kindergarten to eighth grade” Isn’t this range too broad? Are they expected to process the same or similar levels of RC skills?  You have observed that the questions asked by education experts are open-ended. Why does the designed QAG system extracts answer candidates from stories directly?
Typos: Line 271: “question generation module (QG) module” Line 303: Missing citation of the Spacy library 

==================================================

-p. 3 the authors point out that general purpose QA are not appropriate for the education domain and that they solve this problem by developing a new dataset targeting students K-8th grade. This is a big range of grades where fundamental reading comprehension strategies and abilities apply. Reading comprehension questions for Kindergarteners are vastly different from 8th grade middle schoolers. 
-The authors overgeneralize when making claims about the problem they solve in many places. I advise the authors to be precise with their claims.  The current work does not solve the problem of QA in education. It makes a contribution for asking specific type of basic reading comprehension questions for the genre  fiction fairy tale genre. 
-It is not clear if the improved results reported in Table 3 are statistically significant. It would be useful to add a discussion as to whether the reported improvement was within expectation given the effort for annotating a specialized dataset. 
-p.5 It is not clear how the proposed classification task can be extended for ranking QA pairs beyond the specific dataset. The method relies on having available ground truth pairs. 
p.3 Casual Relationship --> Causal Relationship p.7/8 The human evaluation questions that are reported are 3  (readability, question relevancy, answer relevancy) but the intercoder reliability is evaluated on 4 dimensions. Which is the fourth dimension. Also, can you provide more details about how you computed Krippendoff's alpha? The reported agreement for 5 annotators on questions of relevancy is surprisingly high. Maybe it would be helpful to explain why you didn't choose answer accuracy. 

==================================================

The only weakness I see in this paper is a lack of description of the incurred cost for token-level conservative loss. Without being able to see the associated code and run the experiments it is difficult to understand what negative impact using token level contrastive learning has. How much slower does it make the training? How does it affect GPU memory usage? 
It would be beneficial to the community to release the software and associated models along with exporting the models to an engine that can take advantage of lower precision for increased inference speed. 

==================================================

The "Related Work" section is quite brief: it cites a number of papers, but mostly only in a list of references with very little context. It would help to give a more thorough comparison, particularly to the most similar approaches such as the Bhatia et al. (2020) paper which also addressed meta-reviews. 
See above for my main comments.
The font in Table 7 in particular is EXTREMELY small, particularly with the highlighting which makes it even harder to read. Similar issues also apply to the axes of the graph in Figure 5. 

==================================================

- The experiment section is not informative to the readers, because it is stating obvious results such as Transformer models being more “capable of capturing content-specific information”.
- The case study section is also not very informative. The authors evaluate whether the model assigns bigger attention weights to the appropriate control token when generating relevant sentences. However, they only show a single sample as an illustration. Such a single sample illustration is very vulnerable to cherry-picking and biases. Moreover, interpreting bigger attention weights as being more important is somewhat controversial in the field. It would be more helpful to the readers if the authors show diverse quantitative analyses.
- The authors show how different control sequences affect the generated outputs with only 3 examples, which is extremely limited to see whether the model can be controlled. 
- It will be interesting to drop the control sequences from the gold labels (or replace them with negative ones) one after one and see how the automatic metric changes. This will show whether the control tokens really do contribute to the model’s performance. 

==================================================

- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.
- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results. 
- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.
- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained. 

==================================================

Weaknesses: 1) The paper doesn't present any clear hypothesis and experimental set-up to validate their hypothesis. Neither do the authors explore the issue of unanswerability in MRC in exhaustive detail. They fail to account for previous works on this task (https://ojs.aaai.org/index.php/AAAI/article/download/4619/4497) 2) The claims made in the abstract are unsupported in the results. The authors claim that "it is shown that uncertainty outperforms a system explicitly built with an NOA option". But, the results of the Implicit system are inferior to that of the Explicit system. The explicit: option A system is not a meaningful system in itself. 
3) The results of the negative penalty scheme show that the proposed uncertainty measure is not that useful in common cases such as 3:1. The authors fail to explore reasons for this, neither do they attempt to experiment with other MRC datasets to see if this is ubiquitous. This makes the exploration incomplete. 
4) The writing of the paper is quite convoluted. The paper is not divided into motivation, hypothesis, proposal, experimental design, results, discussion and related work clearly. Most of the sections are a mix of several of these componenets. This undermines the readability of the paper significantly. 
1) Kindly include the details of the expected entropy calculations in the main paper to make the paper self-contained. 
2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore. 

==================================================

I think many members of our community will find this work rather niche.  It might be better suited for BEA.
The authors build on top of a system called ELECTRA which is a shared task entry that has not yet been released, or even published.  Therefore, it's difficult to understand the modifications made to ELECTRA described in this paper.
The authors should consider doing evaluations using other data sets besides ReClor to see if their augmentation strategy generalizes well. 
Your abstract is way too long.  You can shorten it starting at the sentence, "This paper investigates both of these issues by making use of predictive uncertainty", remove everything until you get to "It is shown", then replace "uncertainty. It is shown" with "uncertainty, and shows".  Also, in the last sentence of your abstract, "it is shown that" should be "we show that".
On line 050, replace "consistently observed" with "been used to create".
On lines 066-067, remove ", and of course..." to the end of the sentence.
On lines 074-076, you can remove everything prior to "answer uncertainty" and begin this sentence with "Answer uncertainty" for clarity.
On line 192, insert "give" between "and" and "no".
On line 194, "discourages" should just be "discourage".
On line 234, "increase demand" should be "increased demand".
On line 273, "ensembled-based" should just be "ensemble-based".
On lines 338-339, change "with unanswerable examples too" to "that also contain unanswerable examples".
In Table 2, can you clarify what "Paper" and "Others" are?  Which papers do these come from?
On line 466, insert "and" in front of "only".
Can you provide some examples content/question/answer-choice triplets from ReClor?  What is ReClor's domain? 

==================================================

The only real concern here may be how well other factors are controlled for in the user study. With the method described for extracting morally framed argument components on a given topic, it is possible that other differences may also be present. Some of these may be connected to the moral position -- certain moral positions may bring with them certain phrasing or aspects of the topic that are more appealing to a given audience. And some of these may arise by chance given the relatively small sample size. I do not view this as a major weakness, but it may benefit from some further consideration. 
The assignment of a single moral label to a sentence seems quite definitive. It may be more often the case that a sentence corresponds somewhat to two or more moral foundations, and this might be better modelled as a score for each foundation. Is this something that has been considered?
The sentence starting on line 279 ("Having considerable higher effectiveness on average signals the advantage of using the proposed dataset.") seems incomplete? 

==================================================

- I'm afraid the final number of participants in the user study (3 + 3) makes any drawn conclusions rather unsubstantiated; One would expect some confidence intervals (classical stats approach) or high-density intervals (with Bayesian approach) to see, how much we can trust the study in Table 5 and 6 - Not sure, how much IBM API is stable/reproducible. See a similar argument by Kilgarriff, A. (2006). Googleology is bad science. Computational Linguistics, 33(1):147–151 
Missing reference to Lukin et al. 2017 (EACL) How exactly do you formulate queries (378)?
Regarding the Ethical statement: Well... this is super weak argumentation: you rely on IBM's black box but at the same time claim only correct solution is to aim for being transparent to the user. So you weren't but still did the study. What should we take from it? 
On the other hand, I think the ethical statement section is mostly non-sense anyway, and I wouldn't know what to write here myself, if I were the author. There is no guarantee that transparency leads to non-abusive technology, to begin with. So I'm just going to ignore this section completely. 

==================================================

1. 	While the proposed method is more effective than the previous methods, the efficiency (e.g. training and inference speed) is not compared. 
Question: 1. 	For the end-to-end variant, are the retrieving and the reasoning also intertwined as in previous works? Any intuition on why the proposed method is better?
Typos and Formatting Issues: 1. 	Line 53: the publication year is missing from the citation for WebQSP. 
2. 	The caption of the table should be put on top. 
3. 	Line 435: a space is missing between “statement.” and “SR”. 
4. 	Line 576: proves -> prove. 

==================================================

1. The proposed path expanding method might not involve much novelty. It is very similar to the UHop model in [1], which also adopts an iterative style to extract reasoning path until reaching the END, and updates question representation with historical expanded relations.  2. The trainable SR module is based on RoBERTa, while NSM and GraftNet do not employ large pretrained language models. Since RoBERTa is generally considered more powerful than LSTM and MLP, it is likely that the high performance largely results from RoBERTa rather than the trainable SR strategy itself. Then the comparison between the setting with and without SR may be unfair. Also, it is questionable whether it is worth the much larger computation resources to use a RoBERTa-based SR module instead of the simple and effective PRR.  3. There may be some problems with the mathematical formulas. In equation 6, when the similarity s(q,r) is larger, the probability of r given q should be larger rather than smaller.
[1] UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering. NAACL’19 
1. I recommend citing the UHop paper, which is closely related to the proposed SR method.
2. In the experiments of this paper, it is assumed that the golden topic entities are given (as indicated in lines 187-188). So it is better that you mark whether the baseline models use golden topic entities or entity linking results in the result table, which is a variable need to be controlled when we compare the performance of KBQA models. For example, BAMNet, GraftNet, and PullNet use entity linking results on WebQSP, according to original paper.
3. The EmbedKGQA reports 66.4 Hits@1 in the original paper while Table 2 shows 46.0 Hits@1, which is much lower. Is there any explanation for this contradiction? 

==================================================

1. The experiments are held on a private datasets and the exact setup is impossible to reproduce. 
2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well. 
3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE? 
I urge the authors to release at least some part of the dataset to the wider public, or under some  end user-agreement.
Comments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task. 
2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations. 

==================================================

1. This paper seems to reduce the impact of semantic noise on text matching to "length divergence bias". From the first example in Table 1, it can be seen that what affects the model's prediction should be that T2 contains more other semantics (noise) such as "Canadian", "University of Waterloo grads Kaheer Suleman and Sam Pasupalak", which may lead to semantic representation drifting and then affects the TM models. In my opinion, the "length divergence bias" should only have a discrepancy in the length of the textual expressions rather than introducing additional semantics. 
2. Lack of details and intuitive examples of the constructed adversarial datasets. 
- Line 136-138: "... we down-sample each category to align with the average PosRatio of the whole test set". Could the authors provide more details about the adversarial data construction?
- Could the authors use some specific examples to analyze why the constructed adversarial training sets can improve the robustness of the TM models? 

==================================================

- This is the (n+1)st paper on discussing biases in models and datasets and it's not clear to me whether this specific bias hasn't been discovered before - not a single 2021 paper cited - models investigated (ESIM, etc.) are a bit old, no novel model is included - the explanation via probing is a bit trivial (this probing has been criticized btw., as unreliable [1]); it's also not clear to me why BERT performs best in the probing, but is least prone to the length diversion bias, questioning the apparent explanation - in l.237, authors write "except for one combination", but there are quite a few negative signs in Table 3, indicating that adversarial training is less often helpful - the losses from length bias are often small, e.g., 1 percentage point for BERT [1] https://aclanthology.org/2020.conll-1.8.pdf 
- Textual matching is also relevant for evaluation metrics [2], where similar biases (e.g., lexical overlap) are discovered. It would be interesting to extend this analysis and also consider models from such communities [2] https://aclanthology.org/2021.emnlp-main.701/ 

==================================================

Due to the difficulty to build a corpus as employed in this study, the amount of data used to train the machine translation systems is relatively small when compared to publicly available corpora used for the same language pairs by the research community. Thus, the conclusions drawn by the authors might be limited to low or average data settings.
Also, a popular method usually employed to mitigate the alignment mismatch between train and test directions is to add a tag per sample specifying the direction. Commonly used when adding back-translations to the training data, this method could contrast with the current set of experiments and provide an interesting comparison point. The question would be, is the machine translation system able to model the direction and potentially abstract from train-test or data-model alignment mismatch. 
Because speakers at the European Parliament might not be native speakers of the language in which the transcription are written in, wouldn’t the Europarl corpus contain text annotated as originals but from non-native speakers?
The information about the passive voice checker in English is missing.
Dark colors in tables tend to make the numbers difficult to read. 

==================================================

Below is a copy from the previous review: > - Need of choice of the two hyperparameters vectors w and v > - This choice, as evident from the authors experiments, is very important > - Process of obtaining v can be quite involved > - The proposed metric works only or gradient-based models > - Although the formulation is intuitive, the metric values themselves can be hard to interpret. 
The authors addressed most of the reviewers' comments made for the previous submission. This is an important direction of research and I believe in the current state the paper can be accepted for publication. 

==================================================

I was unable to get the provided code to work. I tried both on my Macbook and on a Linux-based computing cluster. To be fair, I did not try for very long (< 15 minutes), and I also did not have access to a GPU so I tried to run it on a CPU. It’s possible that was the problem, but it wasn’t stated that that was a requirement. It seems that if I understood the instructions in the readme properly, there were a few __init__ files missing. However, even after changing those, I ran into a number of other errors. The readme was also a bit sparse, ie “Play around with the results as you see fit”. I commend the authors for including the code and data with the submission, but I would have liked to see a script included already (i.e. not just a snippet in the readme) along with a brief description of any dependencies required beyond the requirements.txt and what one might expect when running the script.
Another weakness I felt, was a lack of description of previous/related work. They mention in the very beginning that "Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT", but didn't reference other work to provide similar counter evidence to the consensus they referenced in Rogers et al. (2020). As someone not familiar with this area of research, maybe there is not so much to cite here, but if that is the case, I feel it should be mentioned why there is no related work section. 
Citation should be formatted like so: "The consensus points to BERT-like models having some capacity for syntactic understanding (Rogers et al., 2020)." 

==================================================

A human evaluation of the "gaps" filled by the model would have been a plus for the paper. 
A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on: - How often invalid constraints are finally hypothesized by the model?
- How grammatical are the "filled gaps" proposed by the model given that in principle the model does not modifies human constraints?
- Which is the impact of using different ratios of raw/augmented sentences for training?
Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs?
The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation "are" used to guarantee that all 

==================================================

Usage of data and experiment design: WMT data contain human generated references, any reason why post-edited content was not used in the paper for experiments? Post-edited content would have been much closer to what your task is and there's plenty of available data out there which is post-edited.   This task of sequence refinement has been studied well in  1. Relation to Neural Automatic Post editing models  2. Sequence Refinement part of Levenshtein Transformer (LevT) by Gu. et. al. 2019 is not discussed when it is almost the same thing although the use case is different (for Automatic Post Editing). 
3. Why was there no discussion on non-autoregressive models? Text infilling approach could indeed solved with a non-autoregressive model like LevT. What are the other methods of text infilling, why aren't those compared with your cross-lingual text-infilling approach?
I am left with some other questions after reading the paper, I am writing them as weakness here but they are essentially points where this paper can improve.  L246: "Then we combine the sampled data D and the trivial data D as the augmented data to train the model P in our experiments." what is the ratio of this combination, is it 1:1, if so are you not giving more weight text infilling problem than translation? An ablation study on this combination would have been good.  In the real-world scenario, given that the sample size is low (200), is 65.79 (BiTIIMT) significantly better than 64.05 (LCD)? Same question for edit distance cost? 
L273: "In other words, we employ the standard beam search algorithm to yield Yb, which leads to a valid Y, without explicitly imposing the constraint during decoding" Is this always the case in all languages? If not, can you mention the accuracy here? 

==================================================

Although this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. 
The following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. 

==================================================

- The number of datasets used is relatively small, to really access the importance of different design decisions, it would probably be good to use further datasets, e.g., the classical GeoQuery dataset.
- I would have appreciated a discussion of the statistical properties of the results - with the given number of tests, what is the probability that differences are generated by random noise and does a regression on the different design decisions give us a better idea of the importance of the factors?
- The paper mentions that a heuristic is used to identify variable names in the Django corpus, however, I could not find information on how this heuristic works. Another detail that was not clear to me is whether the BERT model was fine tuned and how the variable strings were incorporated into the BERT model (the paper mentions that they were added to the vocabulary, but not how). For a paper focused on determining what actually matters in building a text to code system, I think it is important to be precise on these details. 
It would take some time to implement your task for other corpora, which potentially use different programming languages, but it might be possible to still strengthen your results using bootstrapping. You could resample some corpora from the existing two and see how stable your results are. 
If you have some additional space, it would also be interesting to know if you have discuss results based on types of examples - e.g., do certain decisions make more of a difference if there are more variables?
Typos: - Page 1: "set of value" -> "set of values" "For instance, Orlanski and Gittens (2021) fine-tunes BART" -> "fine-tune" - Page 2: "Non determinism" -> "Non-Determinism" 

==================================================

1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes.  2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide.  In the Appendix, "Model Design" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?
3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? 
In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: "... and is ..." - Line 400-401: check grammar - Line 426: "autometric" - Line 556-557: check grammar 

==================================================

Of course, training on human judgments directly is not a panacea. There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become "stale"? 
That being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.
The author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.
One small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4. My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct). 
Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation. While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of "staleness"/"neural nets are uninterpretable as metrics" that could apply in this setting hold less weight. 

==================================================

1. In the previous rounds, the reviewer qXqD mentioned that there are no more explanations for the claim `PLASM can generalize to models that are not aligned with DistilBERT/BERT'.  The authors added experiments on XLNet, which partially solves this issue. But I expect the authors to give a deeper analysis for it. Also, it is better for authors to include textCNN/LSTM (1 of them is enough) for text classification, which can show whether non-pretrained models can be used under PLASM.
2. The authors seems to leverage some existing techniques for AL methods, thus the technical novelty is not very high. 
- What's the time complexity of Tracin operation?
- It would be even better to evaluate more datasets for text classification (e.g. SST-2 or TREC-6).  - In PLASM, the authors propose to use pseudo labeling to mitigate the ASM problem. Maybe I do not understand correctly, but using pseudo-labeling for AL is not a new concept. The are many works that leverage this technique for AL. Some relevant papers includes: (1) Semi-supervised active learning for sequence labeling; (2) Rethinking deep active learning: Using unlabeled data at model training; (3) ATM:  An Active self-training framework for label-efficient text classification.  The author could probably elaborate more on the details of pseudo-labeling. 

==================================================

My main complaints are 1) the presentation of this work is a bit misleading to me, 2) I'm not convinced by the main arguments of this work.
For 1), it's not completely clear from early sections that this work focus on the setting where the gold alignments are given. I don't think this setting is the default text-to-SQL in the current literature, so it'd be better to clarify this as early as possible.  For 2), the main arguments of the proposed system are that attention cannot capture alignments well because it's at the token level. But isn't the sequence tagging model used is also at the token level?  The second argument is that attention is prone to overfitting. I'm not convinced that we should attribute the overfitting of current parsers to attention mechanisms, and this paper does not provide any justification.  Maybe the authors should emphasize their way of incorporating alignments (alignments as context, and alignment augmentation), which I think are the main contribution of this work. 
The two-stage process is very related to [1]. In general, the paper perhaps should also mention the line of work that treat alignments as latent variables, e.g., [2, 3] - [1] Compositional Generalization via Semantic Tagging - [2] Span-based semantic parsing for compositional generalization - [3] Learning semantic parsers from denotations with latent structured alignments and abstract programs 

==================================================

Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary?  Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. 
Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).
Minor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. 

==================================================

1. Sensitivity of results to random seeds is not clear. Would be helpful to report variance for finetuning under different random seed settings. 
2. The actual strengths or limitations of BitFit are unclear. To elaborate, in addition to reporting empirical numbers on multiple tasks an analysis of the kind of examples where BitFit performs well should be added. It would be helpful for the community to understand if there is a certain type of tasks/nature of examples,  where BitFit performs better or is comparable to full fine-tuning. 
Minor typo: Line 69: hard reason --> hard to reason 

==================================================

- The paper provides some unsubstantiated conjectures about the "fine-tuning as exposure of existing capabilities in LMs". Without adequate reasoning or references, it is hard to comprehend what the author(s) refer to in this case.
- Generalization gap: No reference to a graph or table that the Generalization gap talks about. However, I did understand later that the generalization gap referred to here is from validation to test in Table 1. The difference between full fine-tuning validation and test also seems to be within 3% from this table and given that experiments haven't been run over multiple seeds, the conclusion about the lower generalization gap is unformed. On QQP, both full fine-tuning and BitFit suffer very similar drops. 
    - Have the author(s) run the codes for multiple seeds and reported the mean? 
    - If not run for multiple seeds, is the <1% difference a significant result?
Overall, while this paper proposes an interesting empirical result, it makes some unsubstantiated claims and some of the conclusions from the experiments are not adequately justified. The paper could benefit from a more substantial discussion of fewer results in the main paper with a more detailed discussion deferred to the Appendix for other experiments. 
- Perhaps this point is not explicitly mentioned, but it seems like BitFit also has the added advantage of *not* introducing any new parameters to the model. This would have been a nice positive to stress on.
- Is there any reason why the paper stresses on Masked Language Models in particular? Did experiments not work out with an autoregressive LM? 

==================================================

1. While the paper focuses on generalizability and transferability of value classifier, however,  there needs more clarity on domain specificity. Though the paper talks briefly about the moral value expressions taking different forms in various domains, the BERT model used here doesn’t seem to take into account the domain information. Hence, the value classifier might emit the same moral value label for a moral value expression irrespective of the domain. 
2. Though the objective is not to compare classifiers but evaluate the cross-domain capability, the observation of catastrophic forgetting, despite several other reasons, could also be a result of lack of domain information leading to more misclassification errors. 
3. The evaluation sections reports results over 10 runs, but the paper doesn’t report the standard deviation that potentially provides more information about the sensitivity of the model to order or domains trained vs domains tested. 
4. The findings of the paper pretty much align with earlier understanding of how training on larger amount of data  from different or same domain helps improve generalizability or transferability. Though it is important to validate them in the moral value applications (which is limited),  it will be better if the paper clarifies what its core contributions are and how it varies from other general approaches/findings in domain adaptation or multi-task learning methods. 
More clarity on specific contributions of the paper. 
Explaining the limitations/assumptions and potential reasons behind specific findings would be useful. 

==================================================

Most of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task,  it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. 
Questions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? 
2. In Table 2, what about the result of  SpreadsheetCoder under 20% train set? 
3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?
Suggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. 
2. The order of terms in L288-L290 should be aligned with Figure 2. 
3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example. 
4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely.  Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 

==================================================

- Baselines chosen in this paper are not strong enough to show the performance brought by the introduced objectives. For example, under the TableQA settings, another model employing SQL named TaPEx (also mentioned in this paper) can be used to compare with the proposed model.
- To further evaluate the effectiveness of the proposed method, experiments on other tasks (e.g. cell type classification, table type classification) compared to TUTA should be conducted. 
- More clarification and novelty is needed for contributions in Sec 1.
- As mentioned in Sec 5, there is another model named TaPEx, which is also designed to improve reasoning skills in table pre-training. Why is there no comparison with TaPEx in the experiment section?
- TUTA employs several datasets to evaluate the model performance on CTC (cell type classification), why is there only one dataset used in this paper?
- Is there any deficiency/limitation of introducing these objectives? Will it cause a performance drop on other tasks such as Table Type Classification and CTC? 

==================================================

Unfortunately, the paper is very difficult to follow in parts particularly because of the extensive use of abbreviations.
Regarding word vs morpheme, what exactly is the conclusion reached? It is not entirely clear, yet it seems to be one of the main questions in the paper.
The final sentence in the paper claims that the proposed algorithm may help speed up annotation processes, and as such must run fast. That holds only true if the algorithm is run on-the-fly, and if human annotators are used to incrementally teach the algorithm. Otherwise the algorithm can be run independently of the annotators; one could pre-segment all data automatically and then present it to annotators for correction. 
In line 165, you don't explain "DP", although it is recoverable through the context. 

==================================================

There are no major weaknesses in this version of the paper. 
The weaknesses mentioned in the previous review have been addressed in the current version for the most part. 
I appreciate the authors taking the time to improve upon the paper. I believe this work would definitely facilitate the documentation process whether in progress or as a post-processing step. It would be really useful for a future version of this work to include a use case study on using this process and reporting how useful it was. This would definitely increase the value of this work among both NLP and language documentation communities. 

==================================================

- Unclear how mainstream news was filtered to be relevant for political conflict. It could be useful to have some more data filtering details and evaluation task descriptions (e.g. example input, output classes, etc) in the appendix - [minor weakness] Beyond the vocabulary and model performance tables, I want even more out of the comparison between ConfliBERT and BERT because the model is the main contribution (there's no new theoretical contribution, major methodological contribution, or dataset). [ suggestion] Is it possible to do a manual analysis of cases where BERT misclassifies a text and ConfliBERT does it correctly, or both models still struggle? Why does ConfliBERT have large improvements for some tasks but not others? 
Since ConfliBERT’s main gains seems to be from political conflict-relevant words in the vocabulary, how well will it generalize in a rapidly-evolving space? Would it need to be retrained as different political groups and organizations emerge? 

==================================================

- the writing of the paper can be improved in some sections (Introduction/Related work) to accommodate readers that are not experts on the task - missing details on corpus creation (instructions, agreement) 
- the paragraph from l90 to l116 is very unclear. It is explained in later paragraphs, but on first reading it is somewhat confusing - l121 - ``consists'' -> ``consists of'' or ``contains'' - can you include more details on the selection criteria for candidate examples from reddit (section 3.1). E.g.: what are the topics, what are the stereotype targets, what kind of filtering do you apply?
- can you include agreement statistics for the corpus in section 3.1 or 3.2?
- while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for "selecting relevant examples", in order to justify the use of RL in that setup over a simpler selection method 

==================================================

1. The multi-task learning approach defined in Section 4.1 is not described in detail. There are several ways of performing multi-task learning and the authors should be a little bit more precise in the description of their model. For a more systematic description of MTL approaches for Deep learning models, please refer to https://arxiv.org/abs/1706.05098. 
2. Multi-task learning generally struggles when tasks of different complexities are combined together (e.g., see GradNorm for a possible solution: https://arxiv.org/abs/1711.02257). On the other hand, the authors assume that multi-task learning is going to work without putting much effort into studying how each task affects the others. 
3. The authors perform an ablation where the model is trained using each neighbour dataset together with the reference dataset. However, to show the power of using multiple related tasks at the same time, it would be beneficial to see the effect on the overall performance, of different combinations of datasets. 
Regarding the issue 1) and 2), I believe the authors should better describe their MTL approach and properly localise it in the literature. Additionally, the authors should provide more details about the training regime and what is the effect of combining different tasks at once. In this sense, weakness 3) can be useful to solve all the others. In particular, I would suggest the authors take their `bert-base` classifier, and train using MTL with different dataset combinations. For instance, assuming that their reference task is S and the related tasks are T_1, T_2, ..., T_6, I would evaluate the following combinations: 1. T_1 + S 2. T_1 + T_2 + S 3. (...) 
4. T_1 + T_2 + ... + T6 + S (this is the version with all the datasets that is currently implemented in the evaluation) In this sense, Table 4 goes in this direction. However, I believe the evaluation should focus on the performance of the stereotype detection task only because is the reference task for this paper. 

==================================================

1. The authors presented a fine-grained evaluation set in this paper. However, the anti-stereotype that appears in previous datasets is missing in the constructed dataset. In addition, details of annotations are missing in this paper. Since stereotype detection is quite challenging, it would be important to discuss how to guarantee the annotation quality and whether annotators can reach an agreement on collected corpus.
2. Missing related baselines. Only PLMs are considered in this paper and other task-related baselines are missing.
3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set? 
1. More discussions about dataset construction should be provided, e.g., the time range of data collection, preprocessing strategies and quality control.
2. All "table" and "figure" in the context should be capitalized. 

==================================================

- The proposed method is quite simple. Its essence is the "pilot update" mechanism, which basically applies the inner loop of MAML twice, once to update the teacher, and once to update the student.
- The achieved performance boost is moderate.
- The comparative analysis in Table 2 shows that, with significantly more computational costs than the two compared baselines (PKD and ProKT), the model achieves a modest performance gain of about 0.5 absolute F1 points. This analysis, though very welcome, demonstrates that the proposed method suffers a large computational penalty for a small accuracy/F1 gain. 
This paper is a resubmission from ARR August, and it has not been significantly improved. The only major addition is Table 2 that describes the tradeoff between computational cost and model performance. Therefore, the majority of reviewers' comments from ARR August still holds (I was one of the original reviewers). 

==================================================


- The paper uses much analysis to justify that the information axis is a good tool to be applied. As pointed out in conclusion, I'm curious to see some related experiments that this information axis tool can help with.
- For Figure 1, I have another angle for explaining why randomly-generated n-grams are far away from the extant words: the characterBERT would explicitly maximize the probability of seen character sequence (implicitly minimize the probability of unseen character sequence). So I guess the randomly generated n-grams would have distant different PPL value with the extant words. This is justified in Section 5.4.
- It would be better to define some notations and give a clear definition of the "information axis", "word concreteness" and also "Markov chain information content".
- Other than UMAP, there are some other tools for analyzing the geometry of high-dimensional representations. I believe the idea is not highly integrated with UMAP. So it would be better to show demonstrate results with other tools like T-SNE. 

==================================================

Several aspects of the proposal are not clear from how it is presented in this paper. The method is based on a highly rich and complex manually-built dataset, which appears to be hard to adapt or scale to other scenarios, domains, abusive phenomena, targets, languages, and so on. The actual effort in terms of time and money is not known, and a discussion on the scalability of the resource is needed.
The annotation task also seems to be particularly subjective, dealing with individual perception of what is offensive and what is supposedly common ground knowledge. While the authors' claim "a person's identity or attributes have not played a critical role in existing OTD research", there are actually recent works on this direction, e.g. (taken from the perspective data manifesto): https://arxiv.org/pdf/2112.04030.pdf https://ojs.aaai.org/index.php/HCOMP/article/view/7473/7260 https://aclanthology.org/2021.emnlp-main.822.pdf Another, more fundamental issue, that I'd like to see discussed is that reasoning in this work is treated as a seq2seq problem and approached by neutral networks. This feels like a step back with respect to the large literature on semantic parsing and logical-based inference. With the abundance of ontologies, knowledge graphs, and automatic reasoning tools, it is odd that none of them is even mentioned, while the task is reduced to a series of machine learning steps, hindering the transparence of the process. 
Eleven pages of Appendix are an indication of a great effort that went into this paper. Perhaps parts of it would be better positioned into the main text, in particular the annotators' guidelines. 

==================================================

- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. 
Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349 

==================================================

One key motivation in the paper is to improve the quality of the sentence representations via improving uniformity. However, it is still a bit unclear to me if there is a strong correlation between uniformity and representation quality. Maybe we can perform some additional analyses to reveal this by varying negative proportion k? Note that Figure 6(b) seems to show that k does not have a high quality impact on some datasets. 
1. Figure 1: although it is very intuitive to use one random example to demonstrate the distribution of false negative examples, it would also be good to show the statistics over the whole dataset or a particular slice of the dataset. 
2. Line 414: it would be good to show the ratio of false negative examples that are filtered out. 

==================================================

I am not fully convinced about the way the false negative samples are identified. In the paper, the authors proposed to use SimCSE as a complementary model and identify false negatives as negatives that have higher semantic similarity over a threshold using SimCSE representation, and punish those “false negatives” with a 0 weighting. I would argue that using similarity score alone could not identify false negatives. It only identify negatives that are very close to the original sentence semantically. In fact, those are “hard negatives” which has been shown in many contrastive works as essential in learning a high-quality representation. 
The authors can also do a better job at explaining why starting with a Gaussian negatives and optimizing equation (3) and (4) can lead to negatives sampled from non-uniform semantic space. Any previous work or evidence to support that? 
1) In Figure 1, what is the corpus that the authors use to sample sentence pairs? 
		2) In addition to STS tasks, the authors could also explore a number of downstream tasks as implemented in SentEval to evaluate the quality of sentence representations. 
		3) It would be good to add statical analysis to the results in Table 1 to show significance. 

==================================================

- The usefulness of the proposed task is not clearly justified. In the abstract it is briefly mentioned that the proposed task helps with subsequent tasks, but the authors did not provide further discussions and references. Also, it seems that time and location can only be inferred for certain types of images (e.g., news images), and therefore it is not clear how this can be a generally useful task.
- The authors demonstrated that the proposed task requires some reasoning capability to solve, but they only evaluated basic vision-language retrieval models that do not seem to have the corresponding reasoning capability. There is also not too much analysis of the experiment result, especially on the result that shows time prediction is completely off. 
- The authors need to provide a compelling argument about the practical value of the proposed task and demonstrate that incorporating the corresponding reasoning capability improves the model performance.
- It might be useful to provide the chance accuracy so that the readers can have a better idea of how well the baselines perform.
- Considering that the task can also be formulated as a classification problem, it might be useful to consider a standard softmax classifier. This will also give us an idea about how much performance one can get without transfer learning. Also, the time prediction seems to be completely off; the softmax classifier might be useful for diagnosing the problem. 

==================================================

1. Some data annotations steps can possibly lead to some spurious labels like the use of geopy for location annotations can introduce annotation errors which could throw off models trained/evaluated on the dataset 2. example-F1 is an unweighted measure and hence weighs the prediction of the century and decade the same as month and date. Maybe using a weighted version of the same to penalize the harder to get labels could help make the evaluation more fair. 
3. Dataset size is a little limited for the use of the more recent and heavy multimodal models which might be needed to reason over a vast amount of open ended data and visual images 4. Simpler baselines like random guess, a simple guess based on a captioning model, more recent visual transformers for feature extraction etc can be added as more baselines 
1. Try using weighted example F1 to assign different weights to easier prediction labels like the century and harder ones like month and date. Similarly distinguish between location and time prediction  2. Try incorporating more diverse and larger data sources to expand the data beyond NYT and also enhance it in size 3. line 365: size larger than 50, should be defined on a reference image size since it could mean widely different things based on how big the image itself is. 
4. Add more baselines including simpler ones as reference 

==================================================

1. The idea of neural pipeline method for data-to-text generation is not entirely new. The novelty is limited in this paper. 
2. In the experiments, the paper only compare with a weak baseline COPY. There are a lot potential baselines are missing in the experiments. Some existing methods leveraging pre-trained language models (PLMs) should be compared as a baseline. For example, training the PLMs on WIKIFLUENT datasets and then test on the evaluation dataset is a straight forward baseline. 
3. The human evaluation part is quite confusing. No baseline methods are compared (for example COPY). 
4. The paper writing is really confusing. First is the paper structure. A lot of model descriptions are describe in the experiment part. And the section 5.4 is the description of ablation methods, which would be more clear by listing together with other baseline methods. 
In sentence aggregation model, the details are not quite clear. Given the input sentences, the supervision is comming from the delimiter term. What is the standard of defining the delimiter term? Is it only using strict sentence string matching or semantic matching?  There are some missing references in this paper. 
[1] Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Ferreira, et al., 2019. 
[2] Data-to-text with content content selection and planning. Puduppully, et al., 2019. 
[3] An architecture for data-to-text systems. Reiter et al., 2007.  Typos Line 409 we adopt BART-base for -> we adopt BART-base model for 

==================================================

I am not sure there are many weaknesses as such. Although conceptually simple the approach relies on several tools to create the Wikifluent corpus (e.g., decontextualization, sentence splitting) and domain expertise to turn the trips into facts, so even though it zero shot wrt generation task itself, it is not data-lean at all.  For this reason, I would have liked to see an assessment of how noisy these intermediate steps are. 
- Please give us some statistics on the templates, how many they are per dataset, how they were developed (by one or several humans, was there an algorithm? Is the process deterministic or is the any ambiguity?). Btw what you call templates in the Appendix should be facts (which you get by application of your templates in the input).  - In the introduction you should give more credit to Myrossef et al. As far as I am aware they were the first who proposed to convert data-to-text generation to text-to-text generation (their model is not zero-shot, and you have enough of a contribution here to give them appropriate credit).  -Perhaps the following dataset is of use for your coreference replacement task: https://arxiv.org/abs/2102.05169 - I would have been curious to see what happens if you applied your approach in a non zero shot manner, i.e., if you fine tuned (using your templates and the Wikifluent corpus) - The paper contains detailed comparison to related systems, it would have been useful to organize these more meaningfully (i.e., fully supervised vs zero-shot, fine-tuned or not), such groupings would help the reader follow the discussion of your results  - 

==================================================

My major concern of this paper is the innovativeness.  I think the contribution of the paper lies in more on the engineering side: results on more test-sets to avoid overfitting, speed comparison not only on 1-batch GPU setting but also on x-batch and multi-cpu settings, etc. However, while a lot of content is on engineering side, there is very few new findings on algorithm or model side: the CTC model is not new, and no new evaluation metrics is proposed. 
So I think the paper would be better to appear in related workshop, rather than a ACL long paper. 
1. In Table 3, the COMET score is added without enough explanation: why choose this metric and how it fits into this evaluation?   And with the observation of substantial difference in COMET and BLEU score, the author suggest that NAR models may rank poorly in human evaluation, why not add the human evaluation in the paper?  If it does rank poorly, it may reveal another important feature of NAR model. 

==================================================

1. The results on WebNLG seem only marginally better compared to the second best model by Li et al [1] (BLEU-4 61.88 -> 61.90, Chrf++ 79.1 -> 79.7, CIDEr score is lower). For DART, there seems to be a significant jump compared to the baselines; however, the model by Li et al [1] is absent from this comparison (Table 2). 
2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear.
[1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 
1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template. 
2. The original numbers in [2] for BART-* and T5-* do not exactly match the ones reported in table 1. Why is that?
[2] Investigating Pretrained Language Models for Graph-to-Text Generation (https://github.com/UKPLab/plms-graph2text) 

==================================================

+ They did not show the detailed method (of distillation training procedure) in the paper. They should include them in main text not in appendix. 
+ They should explain why the proposed model can deal with a causal abstraction. At least they should show some samples and analyze them. 
+ They should compare among the proposed methods and analyze these results. 
+ Performance of each teacher model should be shown in Table 1. 
+ They can use the following paper: Knowledge Distillation from Internal Representations [Aguilar, AAAI20] 

==================================================

1. While it is fair to say that two annotators might have different answers to the same question and both might be correct, it would be better to verify that the answers provided are all valid. The authors manually validate a small subset of the dataset but, for a high quality dataset, it would be better to validate all of it. 
2. The paper should include automatic metrics for the generation task. While the metrics have their own problems, it would be a good way to compare systems without expensive human evaluation. 
1. It would be good if you expand on the importance of the order of wh-words used for question generation. 
2. Line 112: Consider the ‘second’ example actually refers to the first example in Figure 1. 
3. I agree with the authors about the framing of the classification task, that it isn’t a realistic one. Maybe the paper would be better without it. 

==================================================

No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work. 
1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently. 
2. Any reason you did not consider the other GLUE / SUPERGLUE tasks? 
3. I think you should rename "null hypothesis" to something else, since "null hypothesis" could have a different meaning and is not specific to label verbalization. 
4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph. 

==================================================

- More details on how exactly the topic related chit-chat turns would have strengthened the paper. What are prompts provided to the blender bot and the impact of different prompts on the quality of generated data?
- Also, Blenderbot details for TOD simulation can be expanded in section 2.3. For instance, what is the impact of using mergeSGD vs TOD simulation on the overall quality ?
- The paper seems to lack details on performance of the intent detector model and QA models and their impact on the quality of the dialogs generated. It would be nice to have an ablation study on the quality of dialogs using different intent detectors (including the data used to train). 
During the transition turn, did the process also check if the user is requesting for more information or a question before switching to TOD setting ? 

==================================================

1. From the perspective of research, since the released dataset is automatically generated without further manual revision or annotation, it is hard to say that this work proposes a new research task. Furthermore, the two difficult problems (implicit intent detection and transition utterance generation) deserve a more closer study. 
2. From the perspective of models, the proposed method consists of a list of sub-models. The whole process is too trivial. 
The authors target an important problem that tries to combine the open-domain dialog and task-oriented dialog. Specifically, they provide a  clear objective for this combination, i.e. triggering the business opportunities. There are at least three basic sub-problems. 
(1) How to capture the interaction between the two types of dialogs? 
(2) How to determine the turning point? 
(3) How to transit properly?
All of these sub-problems need to be treated more systematically and carefully.  Other suggestions. 
(1) The introduction is a little wordy and sometimes confusing. For example, in line 78, "the conversation starts without any speciﬁc goal". This is confusing, since if a user has no specific goal, why does he or she chat with a salesperson? 
(2) This paper is highly related to dialog recommendation. In line 505-512, the authors claim "such systems is to only make entity recommendations instead of tasks...". From my point of view, there is considerable overlap between "make entity recommendations" and "transferring from chit-chat to task-oriented dialogues and completing a task the user may want". Specifically, in Liu 2020, they have also combined chitchat and task-oriented dialog, and achieved chit to task-oriented transition. Then, the only difference lies is complete the task, which is not the focus of this paper. 

==================================================

- while this dataset format/type is not mainstream, it is relevant for Table QA and NLG tasks 
The paper is well-written. I like the use of the initial example throughout the paper. The order of the Tables/Figures shown in the Appendices is somewhat confusing, having to jump back and forth across pages (although this may be due to their size and latex's automatic layout). Appendix B.3 is not referenced in the main paper.
Please revisit the following sentences for readability: line 137, 301, 498, 550 For Table 1, where examples are shown, why is the % not aligned as well for the 1st example (to E3/G3) while is is aligned for the 3rd (where proportion aligns to E3, shouldn't this be G3)? 

==================================================

It is still not quite obvious to me what additional challenges are posed by the hierarchical table structure in table QA. Although the authors mentioned three challenges in the introduction, it seems that they are not well-reflected in the experiments. I think Appendix B.4 is a good example to empirically show hierarchical structures indeed increases the difficulty of table QA. I suggest the authors put this part into the main paper, and I expect to see more results and analysis towards this direction to reveal how SOTA QA models fail in facing with complex hierarchical structures. 
I am not an expert of table QA. But to me, the authors seem to miss a straightforward baseline, that is to firstly flat the table into a text sequence like Figure 7 and then train a text-based QA model on it. For example, we can fine-tune the pretrained QA models like SpanBERT for table QA after flatting the table into a text sequence. In this case, we might be able to learn the hierarchical information through the self-attention mechanism built in Transformers. 

==================================================

My main concern is that they haven't quite demonstrated enough to validate the claim that these are demonstrating a causal role for syntactic knowledge. Two crticisms in particular: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion.  2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to "cheat" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure.
Finally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information.
Really, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. 
I got garden pathed reading line 331 "One could define other Z1 and Z2..." The author seems to really like the bigram "prior art." 

==================================================

1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.
2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.
3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. 
It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?
417: why is “as” included along with “were” and “are”?
295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within” 

==================================================

Cons: From translation accuracy perspective, the improvement from fixing the conflict is very incremental. This is clearly shown as the translation improvement is only +0.47 at its best. In other cases (e.g. in the specific case (EN-RO - Table 5)), the improvement is +0.04 (from 23.15 to 23.19), which I would say it does not imply anything that is particularly meaningful.  able 5 also shows that assigning B_s to 0 seems does not matter that much (i.e. it does not hurt the performance that much). This shows that despite the intuition, the conflict actually does not matter in reality.
There might be improvement in model calibration, but with the current paper it is not clear from Table 4 the improvement means big thing or not. 
Typo + writing - as shown in Table 6,7 for both BLEU 207 and chrF -> 6, 7 - As shown in Table 3 , MLS 212 achieves consistent improvement over the origi213 nal label smoothing in both the original and the 214 balanced multilingual translation dataset under all 215 translation directions -> Table 3, MLS - About ECE score: There should be an explanation from the paper.
- Writing style: the authors use the word outperform a lot in the paper. Clearly the improvement does not reflect an "outperforming" by anymeans. I suggest the authors lower the use of word... 

==================================================

The experimental setup is a bit weak in my view. The choice of baselines covers related work, but it doesn't cover possible variations of the model, such as a non-BERT ranking baseline. The SVM and LR baselines both use the "old" style of using representations. I would honestly expect this kind of investigation from a paper that emphasizes the role of the ranking constraint and even puts it in the title.  In addition, the presentation of the main results makes it difficult to see the strengths of each model. The graphs are cut off at around 250 training documents even though some datasets contain much more. It appears as if the rivaling models start to converge after a while. Yet the analysis in table 1 suggest that the new model indeed promises overall improvements that the others cannot achieve (for example 85% accuracy on ASRS). 
It would be helpful for me to address my two concerns above. This would strengthen the results vastly. 

==================================================

1. Experimental part is less convincing. The LR and SVM based baselines are too weak compared with deep learning approaches. 
2. Figure 2 does not show complete trends w.r.t the training documents. It seems that LwR-RC might be outperformed with RB-WAVG or even Lw/oR-BERT. Moreover, analysis of this trend is insufficiently discussed. 
1. Please provide some statistics of three benchmarks as truncation has been used in both sentence length and sentence number for each instance. 
2. Please reorganize the section of performance evaluation to highlight the strengths of the proposed methods. 

==================================================

It wasn't obvious to me why attention-based architectures weren't used for the task. I can imagine the NLP community would be interested in that, but that doesn't make the paper's contribution less interesting. Also, it wasn't clear to me whether the data would can be released, as this is one of the important contributions of the paper. 
- It took me a while to understand that "sign" and "gloss" refer to the same thing. Or am I still misunderstanding it?
- I couldn't understand why a multiclass multilabel approach couldn't be presented in parallel. Was it just left for future work? I assume it would be necessary for the tokenization task that the authors mention in the conclusion, would it not? 

==================================================

1. When conducting the evaluation, do you consider using the F1 score instead of accuracy to have a more comprehensive understanding of model performance? 
2. I notice there is a severe imbalance with the data you annotated. I think a balanced-data experiment is needed. 
3. I would expect a more detailed error analysis in the discussion part. 
I think those properties will interact with each other so the models may easily be influenced by them together. Do you consider generating or extracting contrastive pairs during model training and testing? 

==================================================

1. In the experiment on GLUE, there is no comparison of the proposed core-set based method with a simple local-pooling based method. Since this paper is inspired and follows this line of previous work, such a comparison is necessary.
2. The derivation of the optimization objective is a bit messy (Equation 1-3). For example, the model weights $\mathbf{w}$ are important optimization variables, but they are totally omitted in the derivation. Therefore, the derived objective for token selection (i.e., Equation 3) is not mathematically reasonable. For the revised version or resubmission, I would suggest the authors re-think or re-write this part. 
Comments: 1. The proposed method is a model acceleration technique rather than model compression technique (which reduce the model parameters or size). Instead of model compression techniques, I believe other acceleration techniques such as [1-2] should be cited and discussed.
[1]: Dynamic Early Exiting for Accelerating BERT Inference [2]: BERT Loses Patience: Fast and Robust Inference with Early Exit Questions: 1. How do the authors form clusters for tokens in Figure 1? It's unclear in Section 3.
2. In the experiments, did the authors preserve the [CLS] token for other baseline methods (i.e., Att, Rand)?
Typos: line 304: \mathcal{X} \cdot \mathcal{Y} ==> \mathcal{X} \times \mathcal{Y} line 309: |S| = \ell_j ==> |S_j| = \ell_j line 323: \mathcal{L}(x, y, \tilde{S}, S] ==>\mathcal{L}(x, y, \tilde{S}, S)] 

==================================================

- A bit unclear if LRA is actually a good evaluation setup for the method (no speed up are shown and the models used are too shallow) - The presentation of the theorem will be hard to follow for the average audience of an ACL conference. Key concepts should be introduced  - Unclear what the theorem means in theory and practice (what does the theorem gives us in practical terms, is it still valid in practice given the differences of the actual implementation) 
- Introduce key concepts in the discussion of the theorem - Explain what the theorem gives us in practical terms - Justify the differences from theoretical algorithm and practical implementation 

==================================================

- This paper lacks experiment comparisons with some very similar approaches, such as centroid transformers and representation pooling, although the authors claim that a thorough comparison is not required.
- The proposed method seems not to improve the training speed of traditional BERT in the pre-training stage. It means that we only apply this method in the downstream tasks. I wonder about the effectiveness of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). 
This paper is well written and easy to follow. The authors explore to speed up BERT by selecting core-set tokens and reducing the sequence length across different encoder layers. This idea is interesting, and the motivation is reasonable.  My main concern is the experiment comparisons with similar approaches, such as centroid transformers and representation pooling. Although these methods do not release the code, I think this comparison could strengthen the effectiveness of the proposed method. 
Besides, this method seems to only fit downstream classification and ranking tasks. Thus, I wonder about the performance of combining this method with other speedup ways, such as DeeBERT (dynamic early exiting). If we adopt the way of dynamic early exiting, the sequence length reduction across different layers seems to be marginal somehow.
The token selection algorithm used in the paper also reminds me of another way that combines the selective encoding method and Gumbel-Softmax for BERT.  Each token predicts the probability of surviving at each layer and adopt the Gumbel-Softmax to obtain the final token that feeds to the next layer.  This method may enhance the robustness of the proposed method, since it traverses more different combinations. 

==================================================

1. it utilizes quantization technology but does not compare with the other quantization approaches. 
2. there is still a softmax computation left in the presented approach. 
3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. 
4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. 
5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). 
6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. 
It's recommended to report the overall energy reduction of the full model, conduct experiments on more language pairs and with the big setting on a few of them. 

==================================================

- while the focus of the paper is on bridging resolution and the authors claim that transfer learning allows the model to incorporate procedural knowledge, this claim is not backed up by any kind of error analysis or qualitative examples that would show what the model learns exactly - the part that is novel wrt Feng et al 2021 is pretty much just the extension towards ingredient transformation and the transfer learning study (which is however less elaborate as the one in Xia&vanDurme) - while the models are reasonably state of the art (based on ELMo and in line with Lee et al 2018 and Feng et al 2021), newer research such as the Xia&vanDurme paper use more recent language models such as XLM-R as the base model, which may lead to better performance overall 
Xia and van Durme has been published at EMNLP 2021 and can be cited as such.
Section 4 413: "For evaluation, we use precision, recall and F1" - since MUC and B3 measures also define P/R/F1, it's good to mention here that it's P/R/F1 on individual (coreference or bridging) links. 
Section 6 555: "Table 4 shows ..." - the structure of Table 3 and Table 4 isn't very intuitive, since there is partial overlap in conditions and metrics but essentially it's one big collection of results (baseline, +joint, +transfer, +joint+transfer). It's not clear to me why the "overall" figure isn't computed based on the separate baseline classifiers 

==================================================

- There approach may have limitations w/r/t to model scale (e.g., \alpha approaching zero in Figure 2) -- although the approach is still additive on GPT-3 (one of the largest language models available) in Table 1. 
	* It is not clear that results are durable w/r/t to non-vanilla models (e.g., results in Table 4 are far short of SOTA on these tasks) -- is coherence boosting still additive for more sophisticated approaches. 
I liked this paper and believe that the NLP community would benefit from knowing about this simple, yet impactful approach.  It would be great to see if context boosting is still impactful versus more sophisticated approaches.  Two questions: 1.  Should Section 1.1 be Section 2.0 instead? 
2.  How does Table 3 in this paper relate to Tables 2 of Zhang 2020b (re: DigloGPT,Beam 345 seems to perform better)? 

==================================================

I feel the design of NVSB and some experimental results need more explanation (more information in the section below). 
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input? 
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI. 

==================================================

I see no major weaknesses. But there are some minor issues that could be improved.
-  The description of the dataset is incomplete. It will be helpful to describe some demographic information of the singers in the dataset, such as gender and age range. What is the percetange of male and female singers? Are English songs recorded by native speakers? Male and female singers may pose slightly different challenges, so it will be good to know this. 
 - Have the forced alignment results been manually checked? Montreal Forced Aligner works well with modal speech but may not work well on singing. Will this affect the evaluation results? 
- Figure 4 is not easy to interpret. All pitch contours are mixed together. Is it possible to hightly the specific areas where DTW and CTW make errors? In this way it will be easier to spot the errors and validate the alignment result.  - The same for Figure 3. Since this is a new distance measure, it might not be familiar to readers. It will be immensely helpful if a numerical example could be provided. For example, this could be "if a points falls into region n with an angle of 30 degree, what is the numerical distance between this point and the anchor point?" 

==================================================

1. L003 mentioned the proposed framework is a self-supervised learning framework. IIUC, the model still needs cross-modal (x-modal) alignment to train. Why is the proposed framework a self-supervised learning framework? 
2. It is unclear to me how the gradient back-prop in the equation of L165? 
3. Cross-modal code matching: The key of the proposed approach is the x-modal code matching. It seems the codebook should be large enough to cover all semantic information in the dataset. I wonder how to determine the codebook space? Would the initalization of the codebook affect the performance? What is the performance under different codebook size? 
4. The proposed approach achieves higher performance than the baseline. I wonder is it due to the additional codebook or the increased model capacity? 
5. The interpretation part is a little bit confusing. Fig.3 clearly shows the codebook is aligned with the action. However, even with Fig. 3, it is still hard to interpret the output embedding of f^A_{code}. Imagine that given an embedding and a codebook, I wonder how to interpret the embedding? 
I think this paper is well-written and easy to follow. 

==================================================

There are quite a few punctuation errors and grammatical errors (l. 41, 137, 457), as well as orthographic inconsistencies (e.g., versions of “full-text” l. 95, 343, Table 3, 459, etc.). Some of the findings are printed in italic letters in an inconsistent way (e.g., 391-396, 427-438, 555-561, but not 567-581), also, this makes reading a little inconvenient. Spaces between tables, formulas, and text are also inconsistent (e.g., l. 285, 372). 
Please correct the punctuation errors and review the consistency of words (orthography with or without dashes, upper and lowercase letters). Remove italic conclusions to ensure better readability. 

==================================================

-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here.  - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. 

==================================================

1. The models addressed in the experiments are limited; only the (fine-tuned) BERT-base models are used. It's not clear how generalizable the findings are to other transformer models (larger or smaller models; variants such as RoBERTa). 
1. No justifications for the way to measure the faithfulness of the attribution methods. In experiments, the faithfulness of the attribution methods are measured by calculating the correlation with the gradient x input method (Kindermans et al., 2016); however, it has been pointed out that this method can only capture the global behavior of the model in a very rough way (Sundararajan et al. ICML 2017). I am wondering that if the authors use a more accurate method, such as integrated gradient (Sundararajan et al.), will they get the same results? 
1. Insufficient description about qualitative analysis. How were two input sentences for qualitative analysis (Sec 4.5.5, Figure 1) determined? At least, the authors should mention whether these examples occurred during the fine-tuning process. Furthermore, the attribution values in Fig. 1 are 0 to 1. Did you normalize the vector norms? Also, does the attribution map for the input examples that failed to be classified also work well? 
1. Some points are slightly over-claimed. The proposed method does not consider feed-forward layers; however, some text is described as if the proposed method considers all components in transformer networks. 
    - Title: "Incorporating the whole encoder layer"     - l.08:  "incorporates all the components" 
- The authors can mention Geva et al. (EMNLP 2021) to justify excluding the Feed-Forward layer.
- Although the mixing ratio of the methods with Fixed Residual (W_ {FixedRes} and N_ {FixedRes}) is explained as 0.5, it is actually lower than 0.5 because attention also has the effect of preserving. 

==================================================

- In token attribution analysis task, most researchers will intuitively agree that self-attention plays a primary role in Transformer architecture. However, why is the sub-network on top of the self-attention block important, and what role the authors expect from them are not well-motivated. A qualitative experiment comparing the token attribution analysis with and without subsequent sub-network (i.e., FFN, residual connection #2, and output LN) would be helpful to elucidate the role of remaining components other than self-attention block.
- The work appears to be incremental: As mentioned in the paper, Kobayashi et al. (2020, 2021) proposed the analysis method leveraging attention block. While the proposed method extends the existing approach to the entire encoder block, GlobEnc seems to reuse the tactics used in the previous method without any distinctive improvement. 
- In Equation 6, it would be better to note what γ and ⊙ (element-wise product) represent.
- In Figure 1, it would be better to compare the attribution map of GlobEnc with the attribution map of other comparable methods (e.g., N_{res}). 

==================================================

This dataset is very similar to the FEVER formulation of fact-checking, it's just data from a different and more noisy domain, which is of course a more realistic problem setting for automated fact-checking. The approach the authors propose is still useful but not very novel.  At least some version of this data has been previously released in other publications. See https://aclanthology.org/2021.acl-short.86/ , https://aclanthology.org/2020.emnlp-main.623/ .
Briefly at the end of the abstract and In 5.1 (Stage-1 Results) in detail, you mention that your dense passage retrieval approach improves over a TF-IDF baseline. Why is that improvement significant enough to be published? Since you are only comparing with a simple baseline, It is difficult to tell how much improvement is good improvement.
The negative pairs used for training the dense passage retrieval model appear to be selected at random. This presents a known issue about negative sampling into the problem setting: because it's easier to distinguish negative example pairs than positive ones, your final model ends up learning to do the easier task. Since learning the discrepancy between irrelevant and randomly selected <claim, article sentence> pairs is much simpler than learning the similarity between relevant <claim, article sentence> pair, the model ends up not learning very much. 
How do you handle false negatives in your problem formulation? I couldn't find any references to automatic or manual deduplication of the claims in your data, and this data clearly may contain different versions of the same claim fact-checked multiple times. How do you make sure that the false negatives in your dataset do not disturb your evaluation? i.e., what if model A is performing better than model B in your task, but because model A is picking more on those false negatives and you only count performance based on true positives, you end up showing model B superior, where in reality model A is superior but your evaluation is noisy? 

==================================================

1. The methods are not novel as they are largely borrowing from existing work. 
2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. 
3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. 
4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. 
1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. 
2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? 
3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. 

==================================================

1. Their experiments compared methods trained with different datasets and different model parameters. It is still difficult to distinguish this model architecture is nice, or the larger dataset/parameters are good.
2. They only compare the previous method in the prefect pinyin settings though several works on abbreviated pinyin IMEs are mentioned in the related work section. 
It would be better to add a comparison of these.
3. I could not fully understand why people want higher pinyin IME accuracy with full use of V100 GPU. It might be possible to run this on the standard laptops in the future, though. 
l.299, please add the explanation of the abbreviation of "precision at top-K"-> "precision at top-k (P@K)". 

==================================================

- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. 
Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: "We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth." Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. 
  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. 
  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. 

==================================================

1. Although this work points out that shortest rationales are largely not the best for human understanding, the appropriate lengths are still subject to the datasets or even the instances. The length of meaningful rationales may largely depend on the density of the information related to the task. As pointed in Section 5, a more rigorous evaluation is needed to better understand what is a good rationale explanation.
2. This work does not report how "short" the rationales generated by prior works are. As shown in Section 1, recent works agree that good rationales should be "shortest yet sufficient", while this work seems to simply focus more on "shortest". This brings out the concern that whether the main question discussed in this work can really stand for the trend of current works on this task. 
         (a). I think one potential solution to handle this concern is that - by extending or shortening the golden rationales and see whether such perturbations outperform or underperform the original one. 
1. I would like to see some examples of the generated rationales at different length levels from the proposed methods, as well as the rationales generated by the baselines. Such examples can help the readers to better understand the influence of rationale lengths. 

==================================================

The performance for some tasks and some models increased significantly but this is not true in all cases. The fact was explored by the authors who hypothesised that a leading cause is catastrophic forgetting where the model forgets previous knowledge learned through pre-training to accept new information from the intermediate task and confirmed that small models were changed more.  But it does not explain all results (like Bert small better than large even after intermediate training). The authors did also not comment on the difference of behaviour of BERT and Roberta models. 
One table with all the results - before and after intermediate training would help readers to observe differences. 

==================================================

I thank the authors for conducting additional experiments on testing the utility of learned job title representations for the next job prediction task. However, the results reported in the updated draft have reinforced my concerns regarding the effectiveness of the learned job title representations. As Table 3 shows, BERT-based representations outperform on the task when compared with representations learned by the proposed solution under various settings. Even on the classification task, the performance of BERT and Word2Vec methods is reasonably well. Thus, it is not clear what, if any, are the advantages of using the proposed solution for learning job title representations. Some gains in performance, in some settings, could certainly be achieved but at the cost of significantly more computationally expensive solution compared to using off-the-shelf BERT or W2V representations. 
In sum, I would keep my earlier scores unchanged. While I appreciate the efforts undertaken by the authors to test on a new task, the results, unfortunately, are not infavor of the proposed solution. 

==================================================

Nothing serious comes to mind. This isn't exactly a groundbreaking paper in its methodology, but it's important to have this sort of experimental empirical work in the literature. 
Another study that showed limited correlation between model size and "zero-shot" performance is Hu et al 2020 ACL, A Systematic Assessment of Syntactic Generalization in Neural Language Models, https://arxiv.org/abs/2005.03692 . More generally, the paper could engage a bit more with the syntactic evaluation literature, much of which temporally precedes the semantics and commonsense evaluations included in the current paper.
The related work section conflates approaches that train a classifier on a model's internal representations (such as Tenney et al) "zero-shot" targeted behavioral evaluations such as Ettinger's and Goldberg's that use the model's original objective (word prediction), calling both of them "probing". It would be helpful to be clearer about this distinction and clarify which analyses in the paper follow which approach.
One suggestion to the authors is to change the title to something that is less focused on BERT, captures some of the more noteworthy empirical findings, and avoid "muppet" as a generic term for transformer-based language models.
l. 331: "same approach" - which one? Both?
l. 439: that that -> that Section 4.7: Consider reminding the reader what the age comparison task is -- I got a little confused here and had to search for the PDF for word "age". 

==================================================

- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
-  Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix. 
L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about "Type of Skills" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of "skill", "knowledge" and "attitude" correspond to "hard" and "soft" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider "skill" and "knowledge" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: "Pre-training" and "pretraining" are not spelled consistently. 

==================================================

- The analysis section is limited and confusing in some parts: 	- Section 4.1 describes only the annotation results, without any further discussion of the implications.
	- In Section 4.2, based on Figure 2, the authors conclude that rebuttal sentences are not aligned with review sentences. However, what I see is completely different. The majority of review-rebuttal pairs happen to have a positive correlation of more than 50% unless I am missing something here!
	- Section 4.3 wasn't clear for me. I didn't understand how the figure supported the claim that authors often interpret reviews in a way that supports their argumentative goals. 
- There is missing work of Cheng et al. 2021 (Argument Pair Extraction via Attention guided Multi-Layer Multi-Cross Encoding) - It wasn't clear to me why the authors needed to build their own software for the annotation task. I think the task is similar to any other discourse labeling and relation identification task, and there are a lot of open-source tools that can be used for such annotations.
- In the introduction, the authors mention something about annotators' training and calibration process, but this wasn't elaborated upon later. I guess some details on this process should be presented in the paper.
- In Table 6, the header contains abbreviations. It is good to point out what do they mean. 

==================================================

The paper presentation could benefit from a rewriting that focuses on introducing the problem of personalized KGC and motivating it. 
Authors should also discuss why it’s not enough to treat personal memory as just some additional knowledge candidates.
The corpus creation described in section 4.1 does not give enough details to understand exactly how knowledge and memory candidates are created. Authors should also give examples of the data that was collected.
The distillation process of closing the loop in the dual learning setup should be discussed in more detail. E.g., where is equation 17 used in Algorithm 1? 
There are many sentences that begin with “And …”. Just remove the ‘and’ and rewrite. ( e.g., line 92, 98).
Line 149 and line 163, remove repetition.
Line 173, cases Line 205, “Auxiliary task” This is later called a dual task in line 336. Are these the same?
Overall in the manuscript for random variables and equations it will be better to avoid brevity and be explicit with all variables being conditioned on. ( e.g., line 321-325). Also in line 216 the variable should be conditioned on the sampled $\widetilde{Z}^p$. Also in figure 2, It should be $q(Z^p | C, R)$ without $Z^k$, right? Please verify all variables and carefully review it all. Please don't leave this as an exercise for the user.
Line 248-268, Standard BERT description could be avoided to make space for discussing points more specific to your contribution.
Why is the pseudo knowledge label named {$P\bar}? Is this accurate?
Please explain the dataset collection with examples of how knowledge and memory is compiled. 

==================================================

Despite its strengths, there are also several areas that offer opportunities for improvement in future revisions: - In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward).  Revising this section could benefit the overall clarity of findings and subsequent takeaway points.
- Some aspects of the dataset are unclear.  For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?
- Some of the presented results seem very close, particularly in the ablation study.  It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings. 
Two typos that could be addressed: - Line 278: The word *the* is repeated - Line 321-323: The text seems to indicate that $p_{\theta}(Z^{p})$ is shorthand for $p_{\theta}(Z^{p})$.  Should the second instance of this be something else? 

==================================================

- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?
- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). 
- In lines 198-220, explanation of $\phi$, $\psi$ and $\pi$ is not clear. Can they be better explained or incorporated in Figure 2?
- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.
- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper. 

==================================================

The main weakness of this paper is that it is unnecessarily cryptic. Although it does not have hard maths, there are things that could be explained only with words easily (page 3 for instance).
In the empirical section, I would add error analysis, significance tests between different configuration, especially in sections 6.1-6.3 
a) "Table-based verification is more challenging than unstructured-text-based" I would suggest to you that the right thing to say is "Table-based verification faces different challenges than unstructured-text-based...." Text analysis has been around for quite a long time, while [web] table analysis for around 15 years.
b) "Section 2 "Task Formulation" I would re-design it as a "Research Question(s)" where you say mostly the same thing but in a clearer way, including the main finding. "
c) How do you know that a row in a table is actually true? :)
d) What hardware did you require?
e) Some history of table mining forerunners would be nice in the extra page.
f) What is the trigger-word set??? Did I overlook something? That second point was unclear to me. I think I needed to make my way until section 5.3 to get it. 

==================================================

The authors use the term “significant” several times in the paper but in fact, did not use any statistical test to check that the results are indeed significantly decreed. 
This problem gets bigger when looking at Tables 5 and 6; it is hard to see that the suggested techniques truly improve the models. Personally, I am not convinced that the Dual attention and knowledge distillation technique, suggested by the authors, improved the models (see for example lines 5 and 8 in table 5, the change in F1 score is ~2 points) I am wondering if you had a perfect ASR, was it still beneficial to use the two language modalities? I am not an expert on speech embedding but does it capture things like tone, speed, and nervousness? If the answers is negative, I think that this is a major weakness 
I think that the main strength of the paper is the use of the two language modalities. The dataset\task is nice, but it is not the main contribution in my opinion. 
I think that if the paper will be accepted then it will be required to perform the right statistical test in order to prove that indeed the methods works 

==================================================

- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called "random work". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be "Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. " the combination" -> "combine" - L411 "train the model with one iteration over the corpus". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 "useful inductive bias"), you could check if embeddings of L2 have interpretable topological relations, such as analogy. 

==================================================

- My main concern with this paper is the proposed question-rewrite strategy. I’m not convinced this method covers a sufficiently large number of questions because out of 100 passages (or \~3.7K questions based on Table 4 in the appendix), 23% become invalid (\~850 questions), out of which 44% are attributed to unresolved coreference (\~375 questions). As reported at L373, the proposed method rewrites 12% of the questions for all models (\~45 questions per model), which hardly has a significant effect.
- I don’t understand why generating question-in-context lags behind the question-rewrite method. The question-rewrite method may make a question ungrammatical (as pointed out in $\S$B) because the original entity can be a clause or the coreference resolution tool may make mistakes. On the other hand, the question-replace model is trained to generate contextualized questions and its output is more likely to be grammatically correct. 
L238-L241: The sentence seems to be repeated but with different agreements. 

==================================================

It seems that the proposed re-writing approach makes re-written questions context-independent and self-contained (i.e., easier for the models). For example, "What songs were in Parade" could be answered without history information. Moreover, the proposed mechanism seems to kill the naturality of a conversation. Can you please elaborate on this?
With human evaluation (shown in Table 2), the number of unanswerable questions is higher across all models. I wonder if this is an artefact of humans conversing with models or the fact that humans (annotators) did not have access to the full passage. 
As a followup work, it would be insightful to conduct a similar study on different CQA datasets 

==================================================

1. The previous LRLMs perform poorly on suffix identification compared to the RoBERTa-based model, which is trained with this task. The LRLMs may have better results when also trained with this task. The authors need to show their task is more useful than vanilla language modeling task on more downstream tasks, such as QA or summarization. 
(1) The authors claim that the ChapterBreak (suffix identification) is more chanllenge on long-range understanding tasks. However, in order to show the effectiveness of the suffix identification task task, the long-range Transformer model pretrained with the task need to have powerful performance on the downstream understanding tasks than vanilla language modeling. The authors can give more results about that, following previous works (Big-Bird or Longformer)? 

==================================================

However, the work leaves a few things to be desired, let me order them from most to least important.
- I am not clear about the goals of the paper. The authors argue that a better understanding of the discourse structure of long-form answers can benefit the long-form QA systems or the evaluation of long-form answers. However, this end goal is not covered in the paper. The authors did not perform further experiments to show whether a long-form QA system can benefit from that discourse-level information to generate better answers.  - The six sentence-level discourse roles defined by the authors are still coarse-grained to me. It is also not surprising that most long-form answers are consist of answer summary, auxiliary information, and examples. I did not find many motivating insights from the analysis that could potentially benefit the future work of long-form QA.  - Section 5 is good to me. It analyzes the weaknesses of current long-form QA systems in discourse structure. However, the authors only analyzed 52 examples. Such a small sample size may not be enough to make a statistically significant observation. 
Putting together all the strengths and weaknesses I believe the authors proposed a new angle to study long-form QA and the NLP community will benefit from the annotated corpus. However, there are some problems that do not allow me to accept the current version. I think there are two directions the authors can work on to improve the paper:  - Conduct more in-depth or fine-grained analysis on the discourse structure of long-form answers, which hopefully would provide more insightful observations that can benefit the long-form QA community. For example, analyzing how the discourse structure of long-form answers differs in different domains and in different question types.  - Make this paper a closed loop. That is to show the additional discourse structure information can benefit the long-form QA system in terms of either improving its performance or facilitating the evaluation of long-form answers. 

==================================================

3) The explanation for baselines lacks the details of training settings, resulting in difficulty in reproducing. 
How to train the LOG-CaD and MASS models for complex definitions and the ACCESS and MUSS models for simple definitions? 
Which datasets did the authors use for those models?
4) It is hard to say that the performance improvements by the proposed method are significant. 
From the ablation study, SimpDefiner trained without LM and TR must be identical to the baseline MASS model. 
However, the baseline MASS model achieves 24.00 BLEU score on Complex whereas SimpDefiner trained only with definition generation achieves 25.02. 
The authors should explain this difference. 
Furthermore, the output of the baseline MASS model should be evaluated on Simple without applying ACCESS or MUSS. 
If the baseline MASS model achieves 25.02 BLEU score on Complex, it would also achieve 13.66 BLEU score on Simple as shown in Table 6, which is higher than the performances of pipelines on Table 3. 
5) The explanation for the inference should be described in Section 3, although Section 1 implies that SimpDefiner produces not only a simple definition but also a complex one. 
Also, Section 5.1 should indicate that complex definitions from Gen. Decoder are evaluated on Complex (from the OD test set) and simple definitions from Rec. Decoder are evaluated on Simple (from the OALD test set).
6) The expression of "fully unsupervised" in Section 1 is a bit misleading, because an alternative standard dictionary containing complex definitions can be used for training.
7) Conducting multiple runs with different seeds is preferable. 

==================================================

1) I miss an analysis for which specific examples encoding multiple passages help. Adding an analysis where models that encode passages independently of each other fail compared to the proposed approach would be helpful. Similarly, this is a weak point in the motivation of the paper: it is not immediately clear to me why encoding passages simultaneously should be beneficial. You remain very vague here (e.g., line 046) and do not provide convincing arguments.  2) A central argument of the paper is that encoding passages simultaneously is helpful. Why do you restrict yourself to a maximum sequence length of 512 by using BERT? There is pretrained language models e.g., based on Longformer where you could encode more passages. Providing more insights how the number of passages influence performance (beyond what is mentioned in paragraph around line 243) would make the paper stronger. 
1) Why do you use uncased PLMs?  2) Potentially relevant but not cited work: Chu et al. 2021 (https://openreview.net/pdf?id=8smkJ2ekBRC), Wang et al. 2019 (https://aclanthology.org/P19-1132.pdf) 3) Please fix the bibliography and cite published versions rather than arxiv versions (e.g., Loshchilov et al. 2017). 

==================================================

- The biggest concerns/confusions during the reading are the lack of implementation details of the proposed methods. They should have been described in the implementation details in Section 4.1.   1) For the interpolation method, how the \lambda has been set? 
2) For the dropout, thru the reading of the response letter, my understanding is that multiple stochastic masks (w/ 0 and 1) are applied to a document presentation from an encoder. Herein, what is the dropping rate? How many masks have been generated?
- I am not sure TQA is a good enough benchmark for the proposed method.  It can be seen from Table 1, none of the competing deep models could outperform BM25. Though the proposed DAR could boost DPR, I am not sure if such gain is meaningful. 
- The proposed methods are novel and intriguing. The confusion mostly comes from the implementation details and the results. Why not extend the efforts to a full paper and properly add all of the details given the extra space? 

==================================================

Some methodological details seem to be rather underspecified in the paper: e.g., what are "similar results" in line 216, or what is "much more difficulto to train" in line 221, or how many is actually "many updates" in line 284, and what is "subjective assessment" in line 394?
The paper only presents an evaluation on German and may or may not work in the actual low-resource settings. In particular, what are the exact features used? As far as I know, IPA-based features may be tailored towards English or other Indo-European languages, which may be a problem for low-resource languages. 
Line 517: typo - "improveD". 
Line 542: I assume Dutch and Finnish were removed to remove the phonemes that are also present in German, but it would be useful to say that explicitly. Also, isn't there overlap between German and the other training languages? 

==================================================

The main story is convincing, but some experiments are lacking or incomplete.
(1) In line 96, the authors state that articulatory features could be beneficial than phoneme identities. While in the experimental section, the authors discuss the results on embedding similarities (articulatory features v.s. phoneme identities) and the convergence time with alignment examples. Indeed, attention alignment could reveal the results, but the reviewer would more want to see the results from your subjective results and also the convergence time for a fully trained model with specific features.
(2) In section 4.2.1, It would be good to show when the low-resource model could have a matched performance as the baseline. 
(1) It's not clear why the author would use the phoneme embedding from a tacotron 2 model as gold data, which seems contradictory to the paper's statement. Suppose the articulatory features are learned to be very similar to phoneme embedding. In that case, it's difficult to say the learned feature could be better than phoneme embedding (discussed in 4.1.2).
(2) The reviewer is interested in how the LAML works in settings where the data size for each language is imbalanced. 

==================================================

1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance "I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558). 
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria? 
1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation? 
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant. 
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching). 

==================================================

Although this paper proposes a unified framework, the design of the pre/post-nets for each task is not novel. In addition, the quantized method has been explored in the previous work. The primary contribution of this paper is to integrate the existed strategies into a unified framework. 
The amount of the training data and the model size are important for pre-training. The dominant pre-trained methods usually report the results and release the models with the different settings. I suggest the authors do it, which will improve the influence of the work. 

==================================================

- I do not see any major weaknesses of this paper but there are a minor points.
- The authors evaluated the model on speech-to-text, text-to-speech and speech-to-speech tasks but did not include text-to-text tasks in the evaluation. Yet this evaluation could be important for understanding the advantages/limitations of SpeechT5. 
- Vased on what is written in this paper, it seems that the authors do not state any plans to release the code and the pre-trained weights. SpeechT5 is a complex model with several pretraining tasks and ultilizes a lot of computing resources. It might not be easy to replicate the model for a significant number of researchers. If the authors can consider releasing the model, it will benefit more of the research community.  - As speech and texts are projected into the same latent space, it will be interesting to see an analysis of the joint embedding space to better understand how the model learns. 

==================================================

- The performance gain by using NoisyTune on XTREME and GLEU is very small (less than 1 point in accuracy or F1 score).  - No standard deviation scores are reported, and the comparison of the performance gain vs. standard deviation is not reported.
- The methods compared in the experiments are only with using global noise. 
### Comments - I suggest further experimenting with various hyperparameters and would the addition of NoisyTune be complimentary to hyperparameter tuning.
- Since the motivation of this paper is avoiding overfitting during fine-tuning, I would be very curious to see NoisyTune in few-shot settings, where overfitting is more of an issue.
- [Minor] For Figure 4, preferable to have legends outside the plot.
### Questions to the Authors - Are different hyperparameters (e.g., drop out rate) explored? Would it be possible that tuning one of the hyperparameters leads to larger gains than using NoisyTune?
- What is the standard deviation across multiple runs for e.g., Table 1 and Figure 2? 

==================================================

- The details of how to apply K-Means methods to obtain the pseudo labels when using the CCL and how the number of clusters will affect the final performance is missing. 
- Given that sentence length might affect in Table 1, additional statistics of the pre-trained sentence length versus the might be good to provide.  - Could the author provide a more detailed description of how to construct the pre-training phrases and how many pre-training phrases are present and how these phrases overlapped with the downstream tasks? 

==================================================

1. It seems that the biggest contribution of the paper is to apply contrastive learning to topic modeling, which is of limited novelty. 
2. Batch is a sampling method, so what is the major difference between batch and the proposed one? This limits the performance significantly. 
Suggestions: 1. It is a little bit confusing in the assumptions in section 1. First of all, “The phrase semantics are determined by their context.”. As for the examples in Figure 1, the semantic of phrase “ United States” is fixed and not influenced by the context. I think the writers want to express: if we mask “United States”, we can still infer the mask phrase by its context. 
2. Writing should be strengthened. 

==================================================

- ~I was a bit disappointed to see no error analysis presented in this paper. It would be nice to see some trends and insights into cases when this proposed approach fails to perform well.~ - ~The experimental results seem to be missing significance testing. In absence of it, it's rather unclear whether some small absolute performance improvements are statistically significant. The paper would also benefit from a greater discussion of these close cases. Such a discussion is currently missing.~ - ~A qualitative analysis of the generated domain confused examples would help the reader better understand what these examples could be. Such an analysis is also missing from the paper.~ 
~How does the issue of instability of DANN relate to the solution proposed by [1] to learn an asymmetrically relaxed alignment? My feeling is that their method would address some of this, plus it's a stronger baseline regardless as it also takes label shift into consideration. Perhaps, try and include it as a baseline?~ ~[1] Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. " Domain adaptation with asymmetrically-relaxed distribution alignment." In International Conference on Machine Learning, pp. 6872-6881. PMLR, 2019.~ It might still be nice to include this in the camera ready. 

==================================================

- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds.  - Another concern is that the method may not be practical. In fine-tuning, THE-X firstly drops the pooler of the pre-trained model and replaces softmax and GeLU, then conducts standard fine-tuning. For the fine-tuned model, they add LayerNorm approximation and d distill knowledge from original LN layers. Next, they drop the original LN and convert the model into fully HE-supported ops. The pipeline is too complicated and the knowledge distillation may not be easy to control.  - Only evaluating the approach on BERTtiny is also not convincing although I understand that there are other existing papers that may do the same thing. For example, a BiLSTM-CRF could yield a 91.03 F1-score and a BERT-base could achieve 92.8. Although computation efficiency and energy-saving are important, it is necessary to comprehensively evaluate the proposed approach. 
- The LayerNorm approximation seems to have a non-negligible impact on the performances for several tasks. I think it is an important issue that is worth exploring.
- I am willing to see other reviews of this paper and the response of the authors.  - Line #069: it possible -> it is possible? 

==================================================

1. Subtle biases as a result of rejecting samples:     - fetched toxic sentences for rewriting (line 215),     - toxicity classifier (line 218)     - annotator bias     These choices introduce biases into the datasets (unavoidable). The paper would benefit from attempting to quantify how useful a model trained on ParaDetox is in a more general setting. e.g. performance on other (non-parallel) toxicity datasets. 
     2. I’m uncertain about the generality of the methodology. The pricing and application are platform-specific (toloka.yandex.com) and language-specific (English). Would this be possible using another crowdsourcing platform? Or in another language? 
3. The experimental details, for the classifier and proposed Bart-model are unspecified. In particular, details regarding data splits, fine-tuning configuration, and evaluation are missing. 
### Comments and questions - How many annotators were involved in the annotation processes?
- How many samples are enough to get reasonable results with the data produced using your pipeline? You show results on the subsets of ParaNMT, but how does performance scale with the samples in ParaDetox?
- In the name of reproducibility: What embeddings are used to compute the cosine similarity?
- In the paragraph on line 267, it is described that samples that the classifier scores above the threshold of 0.8 are included. How much does this affect the precision of the classifier? W.r.t. the $F_1$ score of 0.76.
- Do you intend to share non-detoxifiable samples? This to me seems like a highly useful resource that could benefit the community. Knowing when something can’t be detoxified isn’t something that can be inherently modeled by the proposed models.
### Style I found the nested parentheses on lines 041-043 hard to read. 

==================================================

- There are some technical aspects of the paper that weren't clear to me:   * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
  * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
     * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) 

==================================================

- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer).  - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper). 
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion. 

==================================================

- This paper does not provide a clear and explicit definition of historical-comparative linguistics. Not all *ACL audiences familiarize with this area, except the member of special working group in limited workshop scope.
- It is hard to follow what are the opinion piece the authors want to share with the readers through this paper. The paper is more likely reporting what the authors are currently do and what they plan to do, but still lacks the reflection points. 
It is better if the authors can relate the discussion points with the more general context. Does the linguistics issues discussing in this paper applied only for the languages in South Asia? 

==================================================

Although one of the main arguments made in the paper is that the biggest obstacle for NLP for the South Asian languages is data scatteredness rather than data scarcity, these two concepts seem to be mixed in some places. E.g., the introduction first mentions “This data scarcity persists despite long native traditions of linguistic description, [...]”, but then it says “the the most basic problem in NLP/CL work on South Asian languages is not data scarcity, but data scatteredness”. Why mention the data scarcity issue then, rather than going more into detail on the scatteredness aspect? Similarly, section 3 begins with “Having established the issue of data scarcity”. I wish the distinction between these two concepts would be more clear throughout the paper. 
Typos: l. 287: serveral → several l. 315: missing space 

==================================================

1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that "We have discarded long samples (>1024 token length) from validation and testing data as well.". I think it is not appropriate to throw any examples from the test set. 

==================================================

•	It is not clear why does the user decoder at time step t uses only the information till time step t from the agent decoder and why not use the information from all the time steps?
•	The motivation of applying the attention divergence loss to force attention similarity is still not clear to me. What happens if att^a_u is made equal to att^u_u . I couldn’t find any model ablation which justifies this loss as well.  •	The human evaluation is not clearly able to identify if the model improvements actually help as the results are close and not consistent across models (although reasoning has been provided). 
Paper is well written and organized. Additional experiments to justify the the attention divergence loss can help the paper. More examples in case studies can better help in understanding the contributions. 

==================================================

The authors claim that the alpha entmax is slow because it requires sorting. I checked the code of  $\alpha$-entmax implemented in https://github.com/deep-spin/entmax and find that the top-k approximation is the default option with K=100 as the default setting. In this case, `torch.topk` is called in the code, and only K numbers are sorted. Therefore, if the author position the proposed method as a fast approximation to the original  $\alpha$-entmax, I think the authors shall perform more systematical comparison with the top-k approximation.
In the paper, the only comparison with top-k approximation is Figure 2 (bottom, center). However, it's actually difficult to make a conclusion for comparing performance and speed based on this figure. It seems that the proposed 1.5-ReLU is still faster in the first 20 hours of training, then the gap closes. Also, as a key performance indicator of the proposed method, it is desirable to have the actual numbers. For example, averaged seconds per batch for training and averaged seconds per 1k decoding requests for inference. It will be even better to profile just the computation time of softmax /  $\alpha$-entmax /  $\alpha$-ReLU operators.
I hope to see more detailed comparison with the top-k variant of $\alpha$-entmax on (1) translation performance (at least WMT14 En-De) and (2) execution speed (can be seconds per 1k batch, will be better if reporting the running time of just the $\alpha$-entmax/$\alpha$-ReLU layer). I might change my assessment according to the results.
Another concern is on the distribution, in Figure 7, authors showed that the sums are still concentrating to a certain value. However, the deviation is not small according to this figure. Will this impact the correctness of the language model scores (log p)? Some applications are relying on the scores for reranking purposes. 
- For evaluation, the datasets that the authors use are pretty old datasets. I'm not against using WMT14 En-De, however, WMT13 En-Ru is rare in the research community recently. 

==================================================

- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.  - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings. 
- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better.  Missing References: Syntactically controlled paraphrase generation:  Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control  <-- this is a contemporaneous work, but would be nice to cite in next version.  Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization 

==================================================

(1) It is not very useful to detect the adversarial examples generated in advance by a particular attack algorithm because an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown. 
(2) It seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is normally not sufficient for such modeling. 
(3) It is unrealistic to assume that a defender knows which attack algorithm is used unless the authors show that the adversarial examples generated by different attack algorithms share some common features that can be leveraged to detect them. 
(4) The method used to create the dataset seems very simple. 
(5) In the experimentation, the authors mentioned that "… our method is relatively susceptible to PWWS". But the authors did not give any reasonable explanation. 
The authors would better evaluate the performance of the defense method combined with the proposed detection model compared to existing defense methods.
Some typos: (1) counter measure --> countermeasure (2) relatively little efforts --> relatively few efforts (3) be harmful for --> be harmful to (4) detecting adversrial misspellings --> detecting adversarial misspellings (5) require training nor validation --> require training or validation 

==================================================

- What do we learn overall? That visual information, from captions or images, together with contrastive learning, is slightly beneficial for downstream tasks underlying visual commonsense? I am  missing insights into       - why the other pretraining strategies fail           - why there is, overall, basically no difference in performance between pre-training on captions or on images (shouldn't the latter even be more informative than captions?) 
         - why pre-training on visual-linguistic data is harmful for general NLU instead of just not beneficial. 
     - It is positive that such a range of different visual-linguistic models have been examined. However, from their descriptions in Section 3 I am not sure I can assess whether these models are fairly comparable.  - Text-based comparison baseline: BERT-base is a weak model compared to BERT-large or RoBERTa -- would the conclusion that visual information can be integrated and leads to better commonsense reasoning behaviour hold when these stronger purely text-based LMs were used? 
- l222: *the* fully - l224f: *the* low-resource, *the* train data, *set* the size - l281: call *it* - l483: This *suggests* - Equ. ( 2): double-check i vs. j in denominator 

==================================================

1. The effect of different corpus: The sizes of different text corpus are different. But in the current experimental setup, authors didn't consider this factor. I'm also a bit confusing about why the authors choose MSCOCO as the caption dataset instead of the larger Conceptual Captions. 
2. Lack of knowledge probing tasks: Since the paper mainly studies how to evaluate the "knowledge" transfer, it is natural to consider adopting knowledge probing task such as LAMA (Petroni et al., 2019) to evaluate whether models already study the knowledge. 
Most comments are mentioned in the weakness part. I suggest authors add the knowledge probing tasks in the updated version and conduct some qualitative analysis to show if the models indeed learn the visual knowledge and which kinds of knowledge they learn more. 

==================================================

The primary one I would mention is when I read the abstract and start of the paper, I was excited about and hoping this paper would include analysis that attempted to quantitatively compare the English and French examples to extract instances of differing bias. The paper does this very well qualitatively and with careful human analysis, but not the sort of quantitative comparison I was expecting - I think this would be an exciting area for future work.
Also, is there a notion of statistical significance that could be applied here to understand whether the biases we see are greater than expected by chance? 
For clarity I would add significantly to captions; for instance, Table 4 should specify the scale of the scores and that e.g. a score of 50 indicates an absence of bias. Also double-check some of your numbers. For instance, the text says "Multilingual BERT is the model with the lowest degree of bias, with a score of 53 for English and 50.17 for French," but the table says 52.9 for English, so why the difference in significant figures?
This is likely a topic for future work, but a core question that arose for me is why multilingual BERT displayed so much less bias than the other models. Is this something that could be explored further? 

==================================================

While I have a generally favorable opinion of this paper, I have two issues with it that if addressed would improve the paper in my opinion.
First, I agree with the reasoning and premise, but I think the paper is missing a quite obvious connection to causal inference. Another way of explaining the current limitation of attribution evaluation is that these methods are almost never compared against counterfactuals (the only counterexamples I can think of are the causal model explanation papers). That is, we need benchmarks that include counterfactual examples where we know what is the type of manipulation that was done, and compare model predictions between the original examples and the counterfactual examples.
More broadly, we can think of model explanation as a causal estimation problem, and evaluate methods as we do in causal inference (sensitivity analysis and the likes). While this paper is not the place to invent new evaluation methods, I think that it’s worthwhile to highlight the potential connections between all these failures and the lack of causal thinking in the model interpretability literature.
Second, I think that the discussion points are a bit too vague and are not as thought out as the described logic failures. Especially in 3.3, I agree that we should want models that are robust to perturbations, but it is not a problem of evaluation methods, it’s a problem of model robustness. We do want our attribution methods to capture such inconsistency if it exists, this is exactly why we use it. 
The paper is generally very well-written, but may benefit from an additional quick round of editing.  Some typos for example:  Line 616: “make it more robustness” 

==================================================

I find it difficult connecting section 5 to the issues of hype and overclaiming in general. Are they about the long-term effects of overclaiming or hype? In my opinion, it is more related to AI risk research and potential risks of misuse of AI. It would be better to add one or two sentences to explain how they are connected to overclaiming. 
An additional practice to avoid underclaiming is to define clearly, before conducting experiments, the desired behaviours of a model in order to properly evaluate if the model "succeeds" or "fails".
Note: This paper is an opinionated paper and the replicability score does not applied, although literature used to draw the conclusion is provided. 

==================================================

1. The proposed approach to pretraining has limited novelty since it more or less just follows the strategies used in ELECTRA. 
2. It is not clear whether baselines participating in the comparison are built on the same datasets that are used to build XLM-E. 
1. From the results in Table 1, we can see that XLM-E lags behind baselines in "Structured Prediction" tasks while outperforms baselines in  other tasks. Any possible reason for such a phenomenon?
Some typos and grammatical errors. 
1. " A detailed efficiency analysis in presented in Section 4.5". Here "in" --> "is" 2. " XLM-E substantially outperform XML on both tasks". Here "outperform" --> "outperforms" 3. "... using parallel corpus" --> "using parallel corpora" 

==================================================

1. It is well known that ELECTRA-style is efficient and the TLM-based model is helpful to the cross-lingual representation. It is no surprise that combining both methods would work. 
2. The TLM-based pre-trained method required translation pairs. I understand most of the baseline required translation pairs, too. However, instead of using translation pairs, I would like to see more research try to achieve better cross-lingual transferability without using translation pairs. 
3. There is no comparison between the usual relative position bias and gated relative position bias. 
1. missing baseline in Fig.3 and Fig.4: I would like to see the comparison including InfoXLM an XLM-Align. 
2. missing baseline in table 5, 6: I would like to see the comparison including InfoXLM and XLM-Align. 
3. I'm interested in the difference between the results of using usual relative position bias and gated relative position bias. 

==================================================

- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.  - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.
- It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.  - In general, there are quite a few things missing -- details provided in comments section. 
**Typos:** - Background section: "...high fidelity thank to…" -> "...high fidelity thanks to…" - Background section: " … Fang et al., 2019).Many…" -> " … Fang et al., 2019). Many…" - Figure-1: "...which integrated to into…" -> "...which integrated into…" **Comments:** - Author did not mention how the initial durations of phonemes are obtained.
- Are durations of phonemes predicted in frames or seconds?
- Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.
- There is no mentioning of train, valid and test set splits in the dataset section.
- In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?
- How do you represent the reference mel-spectrogram at phoneme level?
- Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? 

==================================================

1. The sample that was used for the diversity evaluations evidently was based on 'three phonemes in randomly selected 11 utterances', this seems a bit puzzling as written. Either my understanding of what the sample consisted of is wrong and it could be clarified, or this decision could be briefly justified (e.g. was it the same 3 phoneme types in each utterance or a random 3 phonemes in each, presumably there was some logistical or theoretical reason not to use the full length of the 11 synthesised utterances, and is it safe to assume this sample is representative enough of diversity in the whole?) 
1. Figure 1 perhaps has space to make some of the text a bit larger 

==================================================

There is of course a fair amount of creativity in the pairs, which may yield some strange inferences. There's something about these paired sentences with the sarcastic reading ("bright as coal", "easy as kindergarten for a newborn") that seem particuarly forced: I wouldn't expect these sentences to be very common in everyday language, and then perhaps LM performance is expected, as they wouldn't have seen these things before. I feel they're a strange kind of sentence with a somewhat difficult semantics even for humans. For the pilot pair, I'm not sure I would get the intended meanings for the difference between "ballet dancer" and "modern dancer" - I feel the "dancer" alone could yield either reading. So there may be quirks in the dataset that make it tricky to interpret exactly what's going on with model performance.
I'm curious about the strong performance of the RoBERTa model - could it be that this kind of classification task isn't particularly difficult, if we give the model enough training and data? I think the paper shows clear difficulties in the generation and inference aspects, but perhaps classification is getting better. 
It should be noted that there are also lots of recent work on novel, challenging NLI sets, which the authors seem aware of. There are some recent and contemporary works that also deal with figurative language (Chakrabarty 2021, "Figurative Language in RTE", Poliak 2018, https://aclanthology.org/D18-1007/) the authors should be aware of: in my mind, this work provides a novel, high-quality dataset, they could benefit from clarifying how their work is distinct from these.
A possible question: this work seems focused on starting with metaphors and getting literal interpretations. I think there are also significant applications the other way, particularly in metaphor generation, where the literal interpretations can be used as input and the metaphors as output. 

==================================================

I guess that there are theories about figurative languages. How can section 3.2 be grounded among such theory? If he authors could ground their results in relation to such theories, I think this paper will have even larger impact. 
Just a conjecture, but I am a non-native and I am certain that I am worse than GPT in these tasks. 
I guess that a non-native human cannot do well in these tasks, in general. 
 If you crowdsource the performance on Fig-QA among non-natives, it might reveal what could be  necessary to train LMs.
-It would be good if Fig-QA is  built for  other languages as well.
-Although I did not find any datasets nor software attached, I highly recommend Fig-QA to be publicised. 

==================================================

The paper "A howling success or a working sea? Testing what BERT knows about metaphors" by Pedinotti et al, Blackbox NLP 21, should be cited here if possible. 
I suspect the forced-choice setting makes a significant difference to human interpreters, as suggested in ft. 2 "The opposing meanings help to avoid ambiguity in the correct answer, make the task intuitive for human annotators, and help prevent annotation artifacts that have plagued other NLI datasets." Seeing both possible answers makes the pragmatic scale on which the metaphor is being interpreted very explicit (for metaphor interpretation as structural mapping of conceptual domains, see eg Gentner and Bowdle 2008). For example, "He hustles like he's a billionaire's son" (Table 1) might be interpreted as a description of how he gets things done (by using his contacts and social privilege) or of how hard he works (not hard because he doesn't need to). The scale "not at all - hardcore" selects the meaning. Similarly, "She was as embarrassed as a kid who got an A" might mean she is ashamed of being called a nerd, or proud of her academic success. Natural examples of creative metaphors with unexpected scalar mappings are not hard to find (for instance, the American F4 Phantom jet was nicknamed the "Flying Brick"; while as a conventional metaphor, "flies like a brick" means "cannot fly at all", the intended meaning is that the Phantom was unaerodynamic but had a very powerful engine).
While the paper notes that humans do not have a problem processing metaphors (sec 7.3), I believe these results are for metaphors in context, and often for conventional ones, not creative ones. I am not sure of whether similar results apply to creative metaphors, especially out of context. 

==================================================

- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings. 
- I would recommend to clarify notation when it comes to GPT-2. There's the GPT-2 architecture and the GPT-2 pre-trained language model. Saying that CLIP uses GPT-2 as it's language model (line 84) might lead readers to assume it uses the pre-trained GPT-2. Which is not the case. It will be helpful if the authors are more explicit and consistent when referring to the CLIP language encoder.
- In lines 507-514 authors talk about how CWEs lose mutual information with the the input word and cite Voita et al (2019). Additionally, they argue how this might not be the case for the CLIP [EOS] token. I'm not sure if the finding of Voita et al (2019) applies to the CLIP representations as no language modeling objective is used during CLIP training. So mutual information with the input might be higher for all tokens, not just the [EOS] token. This is something that could be checked. 

==================================================

- Experiments need further improvements. StableMoE converges faster than other MoE methods, but the total compute budgets are not revealed and remain unclear. The inference speed comparisons are also missing.
- The overall contribution of StableMoE is strong, but the contribution of the balance loss is marginal. Comparison (either empirical results or theoretical justifications) to existing load-balancing methods are missing. 
- Does the routing fluctuation problem exist in common MoE models or just the one compared (BASE Layer)? The experiment baselines include Hash Layer and Switch Transformer, but the paper only reports the token-to-expert assignment changes for the BASE Layer model.   - How is the balance loss different from the one in Switch Transformer? The two losses are both encouraging the model to assign the tokens to experts in a more balanced way.  - The main results compare the training FLOPS, how about the total compute budget/costs for convergence? And also the inference speed is unclear.
- For "still uses original assignment scores as input, so the gate signal can also be learned in training stage 2" (line 215 to 216), the routing strategy is fixed, so how the gate single could be also learned during stage 2? 

==================================================

None of the datasets presented are actually new, rather these are just new splits and a packaging of existing resources into a benchmark. 
The tasks require input texts of drastically different lengths, which a single model might not be very good at. 
The baselines are a little bit old and arbitrary (no encoder-decoder models and only one type of hierarchical modeling). 
LexGLUE feels like kind of an odd name for a legal focused dataset 2 Line 231-232: s/access/assess 3 It feels odd to try to make only have english a desirable property of the benchmark, instead of a limitation 3.2  what does it mean articles cannot be violated? 
are these cases normally split chronologically? While this ensures a lack of overlap, it seems possible that each court produces a slightly different distribution of decisions. Is this type of out of distribution evaluation intended?
4.2 Why are all of the models tested encoder-only? Why no encoder-decoder models like T5? 
BERT-like models etc. not that good at storing sentence encoding representations in the CLS token (see e.g., Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders (Liu et al 2021) or Condenser: a Pre-training Architecture for Dense Retrieval (Gao & Callan 2021), so difficult to conclude these baselines are particularly strong. In general, hierarchical modeling seems to be its own task area, where both different pretraining techniques might produce better sentence encodings and different methods of hierarchical modeling may produce better overall results. 
This baseline seems to combine tasks of various different lengths – do you expect the same models to be good at both? Is it even important for one model to be good at both?
5.2: feels a bit hard to say that your result clearly favors the legal models -- 13 total results, 6 of which the legal models do the best Why is there no “overall” score reported, like there is with GLUE? 

==================================================

- Creating/Selecting a dialogue containing all slots is not easy when the dialogue is especially in multi-domains. 
  - In multi-domain dialogue, the length of dialogue is often long and the information of schema is more complex. 
  - To encode whole information for the long and complex dialogue, there is another challenge regarding sequence length.
- Moreover, the long token length of the SDT prompt yields computational complexity.
- Limited analysis on the number of dialogue examples. ( I think it is hard to increase the number of example dialogues for the same reason.) 
- LLMs in Section 6: Did you mean the Large Language Model? I think it is not a normal expression.
- How do the SDT prompts work on in-context learning? ( one-shot inference without gradient update using GPT-3.) 

==================================================

1. Some of the details and analysis are not clear, please see comments 1,2,3,4 2. Some of the ablation studies are unclear or problematic. See comments 5,6 
1. line 130, It is confusing why SGD dataset requires manually created dialog examples for prompts; But for MultiWOZ, it just chooses the example from the training set. please explain why. 
2. line 175 seems lacking of experimental results to support it. 
3. line 121, the paper lacks details for how to design and use prompts for dialog with multi-domains/services. According to line 140,  I only can guess it split the dialog state with multi-domain/service into separate examples, correct? please explain more. If possible, please show some examples in the Appendix for easy understanding. 
4. Error analysis in 5.2 is crappy, more quantitive error analysis will help. 
5. Table 5, the models above the line are using slot descriptions, while the SDT-seq is using name only. It seems they are not directly comparable. Although we may guess that description is more robust than name or slot-IDs, as shown in previous works[1,2]. Please explain more, and also list all name-based results on SGD-X. 6. line 165, finetuning T5-seq with prompt examples is meaningless given there are only #services new examples in prompt. Instead, for the combination of using description + example, could you simply append those example-based prompt after description-based prompt.  This new experiment will help to show how the interesting combination will help.   [1]Cao, Jie, and Yi Zhang. " A Comparative Study on Schema-Guided Dialogue State Tracking." Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021. 
[2]Lee, H., Gupta, R., Rastogi, A., Cao, Y., Zhang, B. and Wu, Y., 2021. SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. arXiv preprint arXiv:2110.06800.
7. The model names T5-ind, T5-seq are misleading. Maybe using Desc-ind/seq, Eg-ind,Demo-ind/seq? 

==================================================

- It would be nice to show more experimental results of the proposed method from more aspects such as an ablation study or parameter analysis.
- There is still a clear gap between the private embeddings and non-private ones. For some baseline approaches the embedding performances are basically random guessing. The paper failed to explain this tradeoff and potential solutions. 
- One question: x' is sampled within certain distance from x in the embedding space, does that mean they are semantically/syntactically similar? If so, how much high-level concepts from the documents can be hided? 

==================================================

1. As an outsider, I wonder whether some natural baselines could have been explored. For instance, given that the method uses sentence embeddings after a clustering transformation, I wonder what would happen if cluster centroids were directly used instead. Similarly, I wonder what's the need for computing the Tucker Depth, and not just using the closest vector in terms of cosine similarity. 
L404: It might be worth recalling the reader that k is the number of sentences at this point, given that this fact was introduced much earlier on and not used until then. 

==================================================

-  The paper does not seem to have any comparison with previous work on this topic at all. They directly do different experiments using their own architecture. Simple baselines (e.g., document length) that are commonly used for this problem can be used as a comparison point, in a STL setup, where the predicted variable can be different (holistic score, individual trait scores), keeping the text representation constant.
- The paper misses a discussion on the limitations of the current approach. For example, the authors commented in the response pdf to one of the reviewers that performance difference across traits is due to topical variation. The modeling process does not have any specific component to account for such topical variations resulting in different scores. It could be a potential limitation. I am not saying a paper is bad because it has a limitation. I think acknowledging limitations gives a more holistic perspective for the reader about the approach. 
I reviewed the previous version of this paper, and most of the minor comments I mentioned have been addressed in this version.  One other comment:  - How are the trait  scores obtained for the prompts that did not have them in the original dataset? The authors claim they took from another source, but understanding how they are created is relevant for this paper.  - I think having a few points of comparison to your approach will give a better perspective for us as readers. Comparison between LSTM and BERT is good, but not enough, as  this topic still has a dominant approach of combining some linguistic features with neural models, for modeling overall score. For individual traits too, predicting the trait score instead of individual score, keeping rest of the set up same, may give you a quick comparison point, and make the paper more complete. 

==================================================

- (minor) Studying the correlation between inter-class JS divergence and performance is a good point.  However, I found its results not entirely convincing because it depends on the presence of two of the six tasks: CoLA and SST-2.  CoLA is a quite different task, and the authors also report the correlation without this task, which becomes much smaller.  At that point the presence of a positive correlation entirely depends on the presence of SST-2.  Or to phrase it differently, no such correlation is observed in the subset of tasks {QQP, MRPC, QNLI, RTE}. 
- The authors have taken into account all my previous comments, and plan to look into the correlation point above. 

==================================================

- do not provide a new system improving over the RoBERTa baseline system for the edit intention task 
The paper provides a new data set for editorial edits with clear description on how the data set was assembled and how it can be used. 
Suggestions for improving the paper: - provide another SOTA approach for the edit-intention task going beyond the RoBERTa baseline - signal earlier that the automatic metrics do not correlate with the human metrics. The automatic evaluation configuration section is little bit hard to follow. Some of the explanations in the appendix could help to improve the readability of this section. I'm still not sure though, what Overall indicate in Table 7 - Is the FELIX system useful? First, it does not learn when the revisions are complete (having the system is Figure 2 is not very informative) and looking at the suggestions provided by the samples in the appendix, another concern may be raised. FELIX generates frequently hallucinations that potentially are contradictory to the initial text. 

==================================================

My only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page. 
- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as "Description" or "Symbolism". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?
- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writer’s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!
- L275: this inclusion criteria -> these inclusion criteria - L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader. 

==================================================

1. The proposed NAR model is almost an encoder part of a transformer model, papers [1,2] are used an encoder-only model for NAR. The novelty is not clear described.  [1] Non-Autoregressive Text Generation with Pre-trained Language Models [2] Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation 2. Lack of an ablation analysis to show the gain from each change. For example, with the special blank token (the difference between this model and [1]) vs without it. 
1. The three observations taking a lot of space in the introduction are very general, the reader may want to know more about the specific issue this paper is targeted, and the contributions made in this work. 

==================================================

- It is unclear why SEC-BERT is better than FIN-BERT. The only difference seems to be the financial documents that BERT is pre-trained on. The paper fails to analyze why the different choice of documents leads to a large difference in performance.
- The technical novelty of SEC-BERT is limited, but it is fine to me since the model is not the only contribution in this paper. 
Why FIN-BERT is much worse than SEC-BERT?
What about the other XBRL tags that are not in the 139 tags? If this task is for real applications, the other tags are also necessary, right? 

==================================================

- Result in Table 1 are substantially lower than SOTA results, casting doubt on usefulness of this approach.
- Approach appears to be very incremental to T5, leading to questions about novelty.
- A clearer ablation studying comparing the seven potential combinations of the three differences to T5 is missing.
- The array of GLM models evaluated (e.g., GLM_{Doc}, GLM_{Sent}, GLM_{410M}, etc.) were not well motivated and did not add much to the presentation. 
I generally liked the paper, but I am not sure that the results (as presented) make a compelling argument to add another model to the transformer-based “pantheon of models.”  It would be great to see how larger GLM models compare to SOTA results, given that hardware resources do not appear to be a constraint.
Typo: Should “BRET” in line 65 be “BERT”? 

==================================================

It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. 
Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:           Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> "we to ...." 

==================================================

W1: Considering that E2E model benefits from mostly training over pseudo-labels (“distillation”), the resulting model actually represents another pipeline, which requires the text-NER. This hurts the main arguments favoring the E2E approach over the pipeline model.
W2: For the E2E model, for methods that do not use pseudo-labeling, improvements are rather limited. This point needs to be discussed more comprehensively, especially to understand what makes the difference when looking at the comparison for “Un-Sp” external data type 
- Different fonts for the captions make it harder to read. I suggest using a consistent font for the caption and the main text.  - A sentence that introduces the findings is missing before enumerating points in line 433 - Positions of the figures are not fitting with the explanations, which makes the paper harder to read. 

==================================================

1. Missing related work: A closely related line of work is collective entity linking that considers multiple mentions in the same document. I think the proposed model is a special case of collective entity linking. However, the paper does not mention this line of work. 
2. No error analysis is performed. So it’s hard to tell in which cases the model does not perform well. An error analysis would help to identify potential limitations of this model and pave way for future model improvements. 
1. What is the target KB size used for MedMentions in the experiments? Is it the number of unique entities in the labeled partition (as shown in Table 2) or is it the entire UMLS? 
2. What is used as ‘descriptions’ of entities in MedMentions? To the best of my knowledge, every entity in UMLS does not have a description. 
4. How are the training and inference batches constructed? Do they contain mentions from the same document to leverage coreferent mentions as much as possible? Since a fixed batch size (B=128) is used, there is a chance that a document will be split into several batches, or a batch may contain mentions from very different documents. A clear description of how batches are constructed would be helpful. 
4. Does the model need to compute \psi(m_i, e) for all e in the target KB during inference? If that is the case, then how does it impact the efficiency of the model during inference? 

==================================================

Although I am leaning towards accepting the paper, I still doubt the contribution of the picker for the generator. This picker only affects the generator if the T5 encoding representation is altered accordingly. I think the connection between the picker and the generator is somewhat weak. Meanwhile, the proposed `soft` labeling method seems useless, compared with the straightforward `hard` labeling. Considering the `hard` labeling method seems to just pick overlapped words between the incomplete utterances and the rewritten utterances, the nolvety of the proposed method seems smaller.
Besides the above, the contribution part (line 99-111) is not well structured. I would recommend to add experimental contributions to the third point. 
There are several minor typos that should be fixed in the revision: - line 080: incorrect use of quotation marks and repeated quotation marks.
- line 151: why the length of R is not equal to the one of U?
- line 164: sing -> single - line 177: copes -> copies - line 195: `the` entire input - line 205: `the` next section - line 292: you could use \softmax to represent the symbol - line 310: you should use \dot between \alpha and L_picker - line 318: Table 1 should be at the top of this page - line 325: UIR -> IUR - line 329: what does it mean for `the joint model and the generator`? I think the generator should be part of the joint model?
- line 364: incorrect use of quotation marks - Table 3: redundant `RUN-BERT` - Table 6: remove bold style on the number `39.2` since it is not the best one 

==================================================

- The selective attention is rather straightforward.
- The motivation of proposing three new probing tasks is not obvious to me. The authors can just simply use the tasks in Caglayan et al. And also the color-based probing is almost the same as the Color Deprivation in Caglayan et al. 
A general comment: For MMT task, it is already known that the vision model does not contribute much to the standard MMT task. It seems quite artificial to me to create these probing tasks to draw conclusions for MMT. Since we know a stronger vision model helps image captioning, so it is foreseeable that a stronger vision model is helpful when visual information is missing. Can the authors elaborate on how significant the conclusion is and why we need such deliberately designed probing tasks? 

==================================================

While I appreciate their effort in building pre-trained models for long sequences, I do have the following concerns: - The results by other models (Table 2, Table 4 bottom) are taken from the original papers whose experiments might not be conducted under the same computational limit (i.e., GPU/TPU memory). It is likely that the improved performance is due to the authors' better computational resource that allows longer inputs. Also, it is unclear if techniques that can reduce memory usage (e.g., gradient checkpointing) are used by the authors in this paper to reach longer inputs.
- In Table 4 and Figure 2, due to the lack of T5 models pre-trained with the Gap Sentence Prediction objective, It is unclear the better performance by LongT5 is due to the Transient Global Attention or the Gap Sentence Prediction pre-training objective.
- There lacks some ablation studies for the Transient Global Attention (e.g., is it necessary to use all global tokens?). 
- Would be better if the input lengths used by different comparisons are listed. 

==================================================


The manuscript is centered on a very interesting and timely topic, which is also quite relevant to the themes of ARR. Organization of the paper is good and the proposed method is quite novel. The length of the manuscript is about right.
The manuscript, however, does not link well with recent literature on AI for mental health, e.g., see sentic computing for patient centered applications. Also, latest trends in mental disorder detection with attentive relation networks are missing. Finally, check Ji et al.’s recent review of suicidal ideation detection.
The manuscript presents some bad English constructions, grammar mistakes, and misuse of articles: a professional language editing service (e.g., the ones offered by IEEE, Elsevier, or Springer) is strongly recommended in order to sufficiently improve the paper's presentation quality for meeting the high standards of ARR.
Finally, double-check both definition and usage of acronyms: every acronym, e.g., CNN, should be defined only once (at the first occurrence) and always used afterwards (except for abstract and section titles). Also, it is not recommendable to generate acronyms for multiword expressions that are shorter than 3 words, e.g., ED (unless they are universally recognized, e.g., AI). 

==================================================

The paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? 
2- "with more than 50 words are kept to increase" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? 
3- "Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019)." - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & Hürriyetoğlu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... 
1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. 
2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. 
4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. 
5- Do you use sentences or whole posts as the unit to be annotated?
English language: "pubic posts" -> public posts "a ED model must" -> "an ED model must" 

==================================================

- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.
- There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.
- There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1. 
- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.
- It would be valuable to investigate the reason behind the high impact of the LDND distance.
- I would recommend removing singletons from Figure 4.
- Add statistical significance tests in Section 5.
- Cite Pires et al., 2019 in the 2nd paragraph in the introduction. 

==================================================

One of my concerns is that though it's nice to have results over such a large number of language pairs, only looking at the overall trend might not be enough to reveal certain interesting patterns. There can be too many influencing factors, some of which are explored in this work, while some others might also be important, such as training data size, quality of annotation, etc. Especially, when looking at the source languages, it seems that a large portion of them belong to Indo-European, which may make the overall results biased. Maybe it would be nice to explore a small subset of languages from different language families.
It seems that "pretraining-inclusion" is a major predictor, which may be not surprising. Nevertheless, there might be a hidden factor inside: if a certain language is not included in the pre-training, the vocabulary of the pre-training model might not even have items for the words in this language, which might be split into strange pieces by the subword segmenter. In addition, source-target vocabulary overlapping may be another important factor to look at. I think there should be more exploration and analysis in these aspects. 
There is a related work [1] performing similar experiments and analyses as in this work, but with more tasks and ranking features (though not using pre-trained models). It would be helpful to check it.
The prediction of POS tagger mostly depends on the lexicons themselves. It might be interesting to explore simpler models with static (cross-lingual) word embeddings, especially for the languages that are not included in the pre-training of XLM-R. [1] (Lin et al., 2019) Choosing Transfer Languages for Cross-Lingual Learning. 

==================================================

1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. 
Comments & questions: - Abstract: The sentence in lines 12-17 ("After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version") is cumbersome and can be made clearer.  - Do you perform re-annotation for the expanded dataset as well? The text now says "..and applying the same preprocessing" (line 355) - this point can be made more clear.
- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?
Typos: - Line 47: "constitinga" -> "consisting" - Line 216: "classifies" -> "classify" 

==================================================

- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. 
  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. 
  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small. 
  * The sizes are Train: 6465, Val: 646, Test: 646. 
  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. 
  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. 
#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world 

==================================================

When comparing the proposed method to the Compacter, it essentially has a similar number of parameters (only 10% lower) and achieves a similar performance across the datasets. It also likely has a considerably larger computation requirement, as the model needs to run twice for each sentence. 
Therefore, the paper lacks a clear reason for why the proposed method is in any way an improvement over the previously proposed Compactor.
There are quite a lot of grammatical errors, especially in the introduction. The paper should be proof-read by a more fluent English speaker.
The contributions in the introduction refer to 1.55M as the parameter count for adapter-based methods, but omits Compacter which is also adapter-based and has considerably fewer parameters.
Table 2 refers to 2 adapter-based methods: Adapter and Compacter. Table 3 refers to 1 adapter-based method: Adapter. But the text in 4.4 states that the adapter-based method in Table 3 is actually the Compacter. This is confusing and it is currently unclear whether the results in Table 3 are indeed for the Compacter model or not. 
On line 198 it is explained that the second layer has a task-specific prompt, while the first layer has an instance-specific prompt. But RoBERTa has more than 2 layers; it is currently unclear what happens in the other layers.
The introduction claims that the downside of existing prompt-tuning methods is that they are "incompatible with the traditional LM objective". It is unclear what that means exactly and why the LM objective would be relevant here. The LM objective is only used for pre-training these models and as far as I can see the paper does not try to evaluate this model as a LM. 

==================================================

There are two main concerns from me . The first one is that how to generate the adversarial training set with code is not clearly discussed and  the detail of adversarial training set is not be provided. The second concern is that the reason of excluding 25% of the word phrases said in the last paragraph in 4.1 and the reason of using first 50 examples from the OntoNotes test set is not be fully discussed. 
1.  For the Appendix H section, it should be reorganized which is difficult to follow. 

==================================================

1. In my view, ACTUNE tries to search a small subset of the vanilla dataset with high coverage. It seems to explore a sub-optimal within "small price", which may have limitations that overdraft the potential with more data. 
2. The comparison between ACTUNE and baseline models seems not fair. According to Section 2.1, the number of labeled samples is 100 (initial labeled data) + 1000 (labeling budget b) = 1100. But the baseline models only have 1000 labeled data. 
3. The presentation of experimental results can be improved:  (1) The figures of experiment results are not well presented. To clearly recognize the details of Fig. 1, Fig. 3, Fig. 4, and Fig. 5, they need to be zoomed in to 300%.  (2) Insufficient color contrast in Fig. 3(c) makes distinguishing the last two lines difficult. 
Comments: 1. The definition of diversity is unclear. Eq. (5) introduces the inter-class diversity, but In Eq. (8), diversity means sampling from different clusters. 
2. The usage of the momentum-based memory bank is confusing. For example, as shown in Line 268, if $f(x; \theta^t)=[0.05,0.95]$, and $m_t$ in Eq. (10) is larger than $m_L$ (suppose to be 0.8), then the score of class 1 in $g(x;\theta^t)$ is larger than 0.8 times 0.95, which is larger than 0.5. In this case, how can predictions in the memory bank affect the current prediction? 
3. I think it would be better if the authors could provide more descriptions in Figure 6. For example, Line 549 says, "Figure 6(e) and 6(f) show the results". What can the reader learn if he/she is not familiar with this visualization? The same problem also appears in Line 1176.
Some typos do not affect understanding: 1. Line 122: we -> We 2. Line 188: result -> results 3. Line 475: Fig. 5(b) and Fig. 1(f) -> Fig. 1(b) and Fig. 1(f) 

==================================================

- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).
- CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed. 
- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.
- Notation of BERT_\theta and BERT_\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer.  - How are negative sampled for BERT_\epsilon?
Additional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.
- Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.
Typos: -	Line 259: “cotation” -	Line 285: Missing “.” 

==================================================

1. The writing still needs improvement. There are only some minor revisions compared to the last version, where they fix some typos, moving part of human evaluation results into the main part, and change few expressions to make them more clear. However, some major problems still exist. 
1) The descriptions of the model are difficult for readers to follow, including complex notations and some unclear definitions (e.g.,  the different styles mentioned). 
2) Although it adds more details about human evaluation, there is still no clear definition of metrics pklg and lklg. And I also have concerns about their justification.
2. An ablation study is still missing. Without such analyses, readers will have no idea about how each component contributes to the final performance. E.g., how the style adapter affects the generated responses, or how it performs if one kind of latent is removed. 
Please check the weakness section.
1. A case study with only one sample is insufficient for a competent paper in the dialogue area. Consider adding more samples and putting more highlights on the samples to show your superiority.
2. Still missing some references that utilize pretrained model in an encoder-decoder architecture for auxiliary-information-grounded dialogue generation.  Typo: L492: Sec ?? 

==================================================

- The four logical flaws in dialogue systems are derived from “the observation of interactions between advanced dialogue systems and humans” (L272-273). More information about these observations is helpful. For instance, in which dialogue models these observations were made? How often do they occur? Are there other flaws too? The last question can be important for future research.
- The paper can also benefit from an error analysis. What are the circumstances that DEAM fails? To what extent does failure in AMR parsing cause an issue in DEAM?
- The AMR abstraction from syntax may attribute to inadvertent inconsistencies. In Figure 4 top-right, I can see that the verb tense is changed from past to present. In this particular example, this may not be much of an issue, but for some cases, this can lead to unintended inconsistency. For example, if the conversation is about a deceased person, past tense is presumably used frequently and only a tense change to present can make the conversation flawed. 
- L85: FED dataset -> the FED dataset - L307: AMR-to-Text model -> the AMR-to-Text model - L346: “apparently” weakens the point. I suggest removing it - L350: The sentence about question-type utterances sounds detached from the paragraph.
- L407: PersonaChat dataset -> the PersonaChat dataset - L462: \citet should be used - In Figure 4 bottom-left, the first sentence should remain the same (i.e., it should start with “He was” rather than “They are”) 

==================================================

- There is no compelling advantage of adding audio data to pre-training: text-only pre-learning (SNER+ST1) is better than or the same as text-only pre-learning (SNER+SAT).
- The idea was assessed only on one task (NER), albeit with three varieties. A more extensive assessment and deeper analysis is expected from a long article. 
- Tables 1&2: please add standard deviations since you anyway have already run each model at least three times - Table 1, caption: consider removing "calculated with the scikit-learn library. ( Pedregosa et al., 2011)", because an average can be calculated anywhere (say in Excel), and you don't need the scikit-learn for that.
- Figure 4: if the y-axis is indeed in percents, then the ticks should be 0, 1, 2, etc. Otherwise, one can think that the improvements are in hundredths of a percent (very small).
- line 499: corpus (David, 2020) ~~corpus~~ 

==================================================

- Some definitions and statements are not clear or well justified.
- Lack of clarity in the definition of the input/outputs for each subtask 
063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
092 Missing reference for wordnet definition 112-114 "The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1." - > It doesn't seem clear.
115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention "mango" and "nutrient" and do not seem to be concepts according to your definition.
188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?
Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity. 

==================================================

- My main criticism is that the "mismatched" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected "pristine" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.   - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise) 
- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant? 

==================================================

- Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?
- Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning 
- The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.
- Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated 

==================================================

The paper is not very clearly writtem. Too much information is moved to Appendix. 
Some important details of data collection and data generation are omitted. 
The main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. 
I did not notice many typos. However, the paper organization can be improved. 
For example, data generation and data collection can be described in the same section. 
Also, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. 
Below are some detailed comments/questions:  1. What "in-the-wild" term means? 
2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). 
3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. 
4. Conclusions from Table 4 are missing. 
5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. 

==================================================

1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable. 
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations. 
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied. 
1) It's better to adopt experiment settings consistent with previous work. 
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved. 
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021. 

==================================================

The authors presented an excellent response and addressed all the concerns in my previous review GAJd in their revised manuscript. In particular, the authors added experiments on the new Shakespeare dataset, used extra automatic metrics to evaluate their approach and found consistent trends, clarified some questions I had about the modeling, added comparisons to recent few-shot style transfer approaches.
I have increased my score to 4. It would be nice to move some of the new results into the main body of the paper with the extra 9th page, especially the experiments on the Shakespeare dataset. 
Several references are missing their venues / journals / arXiv identifiers, you can get the correct bib entries for papers from https://aclanthology.org, Google Scholar or arXiv. 

==================================================

There is a slight imbalance between the background and the description and discussion of results in the paper. Given the very strong set up, the final part of the paper seems somewhat underwhelming. Further discussion on the results and their significance, with more space allocated to what they mean for further work and applications would have made for an even stronger paper.
At alpha=.49 inter-annotator agreement is low, which is not unexpected given the complexity of the task. However, further discussion on how this could be addressed (e.g. with better annotation manuals or training) is missing. 
Line 458: equally effected -> equally affected Overall, no other issues with the writing, which is excellent. I would advise the authors to dedicate a bit more space on discussing the results and their significance for further work and applications, perhaps shortening the background sections slighlty. 

==================================================

- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion 

==================================================

- much of the approach  is based on combining existing published work with respect to the dictionary construction and candidate generation so it makes it difficult to assess how much contribution has been made by the authors.
-  Some issues on readability - There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - no error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened. 
-  Some issues on readability -  There are many acronyms such as CUI(Concept Unique Identifier ), BC5CDR (BioCreative V CDR corpus), MeSH.    I know what they are but the paper should be self contained and acronyms when introduced first should be elaborated on.     i.e UMLS Dictionary Fine-Tuning (UMLS -DTF) - never made obvious in the text.    also Section names - often they are just numbers (see 5.4 or see 4.2 ) - but these should be Section 5.4  and Section 4.2 - consider adding into future work an error analysis of spurious  or missing mentions across all datasets especially - BC5CDR - the abstract is too long and should be shortened.
- line 207 - equation - should one of the L* have the t subscript?  Or am I wrong? 

==================================================

- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.  - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines. 
- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers? 

==================================================

1. The few-shot performance is evaluated over 5 randomly sampled different training and dev splits, it's better to also report the performance variance across 5 splits. If the variance is high, it might be better to include an analysis of how to pick training/dev data for few-shot learning / what kind of data will be more helpful for the performance. 
2. The proposed method gets large improvement compared with VL-T5, which has almost the same architecture. I'm also interested in how much of the improvement comes from different pre-training objective and how much of the improvement comes from better hand-craft prompt design (if we consider VL-T5 as using dataset-specific prefixes as hand-craft prompt). 
Typos: Line 166: "or significantly improves", no "or"?
Suggestions: Related paper: Learning to Prompt for Vision-Language Models. Zhou et al. 

==================================================

1. While calculating the speedups, the authors are showing the FLOPs in the figures. Note that, since the proposed method requires each example to pass through a smaller model first and then on the bigger model, the two sequential steps actually _increase_ the processing time for those examples that are passed to the larger model. I suscept that if we show the speedup figures in the "time" dimension, it will be less favorable than "FLOPs." So I'd be happy if the authors could also provide these figures in the "time" dimension. 
2. Some of the names seem not consistently referred to in the text. See "comments, suggestions and typos." 
1. If I understand correctly, in Line 283, there seem to be typos on the operators in the equation -F(x;f) < t = log(...). 
2. There is an inconsistency in the definition of "logits" between Line 251 (where f_y(x) could negative, and are regarded as logits) and Line 391 (where logits is the normalized conditional probability of the i-th class.) 

==================================================

1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead. 
2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5. 
1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing. 
2. How would this approach scale to batch size > 1? 
3. What is the required memory overhead to keep  4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc. 

==================================================

1. The approach is derivative, meaning it takes two existing approaches and combines them. It's not bad. I'm fine with derivatives if they work. 
2. I have a feeling that the authors also took two precautions against immediate derivatives from their approach, and that gave them a couple of pages to add to their paper: paragraphs 3.3.1 Softmax-Based Mechanism and  3.3.2 Entropy-Based Mechanism explain why they did not use the softmax score and the entropy score, and that gave them one more paragraph: 4.1.1 Ablation Studies. But I'm also fine with this strategy. 
Consequently, earlier exists require -> exits Our method is easily.. orthogonal to many other existing methods (and later: To investigate the orthogonality of E-LANG with others)- I don't really get the meaning of 'orthogonal' and  'orthogonality' in this context. Seems that it is in the same direction with the general trends. 
and are also applicable for encoder-decoder -> is also 

==================================================

This is no necessarily a weakness:  The approach proposed here reminds me a lot about coarse-to-fine inference [1, 2], where the coarse/weak model is used to drastically pruning the search space in order to speedup inference.  Although these approaches were developed for structured prediction tasks and were designed for non-neural models, it would still be nice to have discussion about the connection between E-lang and this line of work.
[1] Improved inference for unlexicalized parsing [2] Structured Prediction Cascades 
It would be interesting to show/compare some examples that are routed to the swift model by energy/softmax/entropy based approaches. 

==================================================

Despite the motivation and presented statistics, the paper could transmit a better overview of the proposed challenge. As it is, I find it hard to evaluate its merits. Among other improvements, examples of the data for each task and its annotation would help.
The reason for balancing the test sets regarding the year of publication, not the training, is not clear.
After the abstract and introduction, I was expecting more about the presence of hate speech in the data, but it does not go further than reporting the % of documents filtered for being tagged as abusive by an existing model. 
Although, at a high level, tasks are well-motivated, what makes the authors think that other researchers will adhere to a challenge in this format?
Why using RoBERTa as the baseline? What were the alternatives?
Not clear how the perplexity is applied in the evaluation of the RetroGap challenge. This measure should be further explained for self-containedness.
Could a model trained for the RetroGap task contribute to better OCR?
Typos: consists in -> consists of (several times until section 5) pdf2dvju -> pdf2djvu described in Section ) avoid contractions like didn't and can't 

==================================================

1. The "baseline" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset). 
2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages. 
3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below) 
L76: incomplete sentence "... and ." 
I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing. 

==================================================

1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned. 
2. Baselines chosen by the paper for comparison appear to be weak. 
3. The paper could also have been better organized. 
Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning. 
2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data.
Regarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup.
Regarding paper organization: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity. 
2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec. 2. 

==================================================

I regret that the authors do not make comparisons with training N  N-to-1 systems from scratch. Training would be more costly, but would BLEU be better? ( or worse, due to the models never seeing some language directions).
Generally speaking, there is nothing strikingly innovative in the paper.
The text has many typos; and the paper could be clearer (see my recommendations below). 
The way you present your results do not sell them very well. It is a bit hard to tell at first glance from figure 1 if your method is working or not. At the very least, you should present an average of the improvements over all the languages (or maybe a separate average for english-centric and non-english centric pairs). The caption of figure 1 mention some number but you do not explain what they are: are they the average improvement over all languages?
You say your training data is 3.7B, but is that 3.7 B words? 3.7MB? 3.7B sentence pairs?
You mention doing some preliminary experiments showing that "complete many-to-many training is still as challenging as one-to-many". I would have liked that you detail a bit more your findings.
It is not clear to me why results in Table 2 and Table 1 are different for the pivot-based baselines. Is it because they are actually distilled in table 2?
I found section 2.2 a bit confusing. You seem to be trying a lot of unrelated things and presenting them simultaneously (training strategies, decoder architecture,...). And then you suddenly mention distillation in the Result section. You should restructure this a bit (maybe with one or two additional sections) to make clearer what you are doing and why/. Many typos: l076: "and ."
Figure 1 caption: baslines -> baselines l123: "graduation accumulation" -> gradient accumulation l127:  "graduation accumulation" -> gradient accumulation l 212:  "the the issue" l254: "Table 1"  -> I think this should be Table 2 

==================================================

I don't find particularly concerns with this paper, though I have questions about several experimental details (see section below for more information). 
1. In lines 366-367, it's said the relative weights for combining CTC and seq2seq losses are different during training and decoding. Why is it the case? 
2. For results shown in Sections 4.3 and 4.4, is it possible to add error bars? 
3. What is the size of the proposed model, compared to baselines? 
4. In table 3, for the proposed model, WER for the audio-visual setting vs. audio-only setting are close (2.6% vs. 2.7%). Do authors have more justifications on going with multi-modal for this task? 

==================================================

- The contributions of the paper are not very clear.
- The explanation of the methods is not well explained.
- The performance improvement from audio-only to audio-visual is very small (WER: 2.7 to 2.6) and there is significance analysis. This raises a big question on the necessity to add the visual modality in this way and also justification on the extra computation for such a small gain. 
Questions/Comments: - Why did the authors did not include experiments with external LM for their approach? This will make a better comparison with other models using external LM.
- Line 183: “we truncate the first convolutional layer in MoCo v2” -> why?
- In the Audio-only model, what are the differences from Watanabe et al., 2017? This is very important to note the contribution of this work.
- Line 250: “a canonical transformer decoder” -> what is it?
- Line 254: “is arguably a decoder” -> why arguably?
- Line 577: the authors discuss about limited ImageNet data for pretraining. For self-supervised pretraining, the authors are not limited to labeled images as in ImageNet. They can use any image for pretraining. So, I’m not sure why the authors discuss this as a problem.
Typos: - Line 326: “publicly AVSR” -> publicly available AVSR - Line 527: “randomly crop” -> randomly cropping 

==================================================

The meaningfulness of these conclusions strongly depend on PPDB being true paraphrases. If they aren't paraphrases in all dimensions and in the strictness sense, it's unclear that one is truly controlling for meaning. In many example paraphrases shown in the paper, it's clear that one wouldn't not expect representation to be the same for even an ideal representation. For example, "are mad" and "'re out of your mind" can and often do have different meanings. Even for the highest rated paraphrases such as "where did they come from?" vs "where are they from" can mean drastically different things, and it's unclear whether a representation that has an identical interpretation of those two phrases is what we really want. 
I would recommend revisiting strong assumptions being made in this paper. No two sentences mean exactly the same thing, even before discounting noise from PPDB. How these analyses are affected by differences in sentence or phrase meanings should be a big part of the discussion. 

==================================================

1. While the paper details a lot of "How"s - the "Why"s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.
2. The author mention uniformity / alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https://arxiv.org/pdf/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https://arxiv.org/pdf/1708.06320.pdf) 
1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read "The input of LaCon contains two parts consisting of the text and all the labels for the task) 

==================================================

- The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.
- Code is unavailable. It's hard to reproduce the results.
- Writing should be improved and double-checked. 
Please refer to the comments above.
Grammar errors:  - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?
- line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ? 

==================================================

It would be difficult to detect the positions of sentiment words if an overall sentence (more than one word) has become noisy due to ASR. The same would be true if there are inserts and deletes in the resulting noisy sentence. This approach may fail in such cases. What is the solution for the same?  Some qualitative analysis of the results and/or error analysis will be useful to understand these type of cases better. The case study section can be replaced with qualitative and /or error analysis. 
The last sentence of the abstract “Furthermore, our approach can be adapted for other multimodal feature fusion models easily.” needs more explanation somewhere in the paper. This is a very vague sentence without any proper basis.  There is no mention of making the built datasets publicly available. Though the dataset can be built to reproduce the results it would be better if authors can share the same publicly. 

==================================================

1. The contribution of this paper is slightly less: the improvement on prediction effect of SWRM seems to mainly come from the capability of BERT in detecting the position of sentiment words and predicting correct sentiment words. 
2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates. 
3. Insufficient datasets and explanation of experimental settings: this paper only used one dataset (CMU-MOSI) for evaluation, which cannot fully compare the performance and robustness between SWRM and baseline models. Particularly, CMU-MOSI is one of the datasets employed by Dumpala et al. (2018), and there is a larger dataset named CMU-MOSEI in (Dumpala et al., 2018). It is suggested to add the CMU-MOSEI dataset for evaluation. Moreover, the experimental part doesn’t mention the setting of hyperparameters. 
4. Lines 308-319 are quite confused to me. The definition of notations should be largely improved for clarity. 
1. Line 220: is absent -> are absent; Line 259: in the training and testing phrases -> in the training and testing phases; Lines 310-311: k_i is defined as the number of sentiment words, but k_i is not used elsewhere; Line 352: Subsequently, We -> Subsequently, we. 
2. As mentioned above, I think that supplementary datasets are important for better comparing SWRM and baselines. Moreover, the effect of sentiment word detection and correction of SWRM should better be shown. 
3. From my opinion, instead of correcting sentiment word errors from the ASR models, improving the performance of the ASR models is a thorough solution to guarantee the effect of the MSA models in practice. 

==================================================

Unfortunately, the final data set contains imbalanced classes, something the authors aim to address in future versions of the data set.  I wouldn't use this as a reason to reject this paper, however.
Some in our community may find this work, and its domain, rather niche; this paper would be a great fit for the BEA workshop. 
Can the authors mention the dates during which the data was collected?  Since this was such a big manual effort, I wouldn't be surprised if the bulk of the work was done in 2021 on data collected in 2020, for instance. This is also important since the domain is computer networking which changes fairly rapidly.
On line 005, insert "many" between "by" and "Automatic".
On line 040, change "interesting" to "useful".
On line 054, "in the last decades" should read "over the past decades".
On line 154, "detrimental for" should be "detrimental to".
The last sentence of section 2.2, beginning with "Lastly, structured collections..." seems out-of-place here.  Should this be a separate paragraph?  Or can you do more to tie it in with the preceding sentences?
On line 395, "refined for multiple years" should be "refined over multiple years".
In this field, it's typical to refer to learners' responses to questions as "responses" rather than "submissions".  Just a minor thing you may want to consider :) 

==================================================

1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data? 
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail. 
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback. 
Moreover,  Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers.  3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. 
Line 277: “The may be attributed…” -> “This may be attributed… 

==================================================

I am concerned about the novelty of the methods. This method simply encodes additional structure information into the Transformer models in a standard way. In this paper, both hierarchical position embedding and section title embedding are borrowed from other papers, so I would say the contribution of this paper is not substantial.
The paper has some unclear parts: (1) In the line 232, I think DPE is never defined. 
(2) Segment Embeddings in Figure 1 is not discussed. I believe it is not same to the segment embedding in BERT. 
(3) Regarding section title embedding, I believe section titles are not single vector, how to add it to BOS? 
Since this is a text generation task and automatic metrics (such as rouge) are not very reliable, I was wondering if you had a human evaluation result? 

==================================================

-Why did the authors decide to experiment with their model on CNN/DM? Hierarchical modeling of documents make sense on long documents such as arXiv and Pubmed. Is there any reason to indicate that it also work on shorter input documents such as CNN/DM…?
-In the analysis section, BERTSUMEXT has been compared with HiStruct+ (Roberta-base) with the claim that HiStruct+ improves the SOTA. Is it a fair comparison as the BERTSUMEXT and HiStruct+ (Roberta-base) do not use the same LMS. In other words, it is still unclear where the improvements of HiStruct+ (Roberta-base) vs. BERTSUMEXT are coming from? Are they because of the hierarchical info injection, or simply because of Roberta-base LM?
-The paper does not include the human evaluation process, which is an integral part of the summarization system setup. This gets, especially, higher priority as the results of the HiStruct+ model vs. baselines are close to each other. 
  -While results are promising when comparing HiStruct+ with the reproduced baselines (Table 2), the proposed model still does nor outperform the abstractive SOTA. 
The methodology section needs re-working and ideas could be better presented. 
Some important information such as the number of extracted oracle sentences should come directly under the experimental setup, not in the Appendix. 

==================================================

1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date? 
2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice. 
1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients? 
2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients? 
3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures. 
4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature. 
5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric. 

==================================================

The novelty of the approach is limited. 
The attempt to train the document retrieval and outcome prediction jointly is unsuccessful. 
The related work section needs to be expanded as it omits important prior work on both Patient-Specific Literature Retrieval and Clinical Outcome Prediction.
Your attempt to train the retrieval and outcome prediction jointly seems unsuccessful, have you considered experimenting with different approaches, such as  Retrieval-Augmented Language Model pre-training and fine-tuning?
There are too many references to the appendix in the paper. For example, it will be helpful to include more detail in section "Learning To Retrieve Using Outcomes", as opposed to referring to the appendix.  It will be helpful to include ethical concerns, detailing the limitations of the approach regarding practical, clinical application. 

==================================================

The paper makes several significant claims about best-first search and recombination being much more efficient than beam search, but the discussions on actual run-time are lacking. In the appendix the authors note that the best-first search code could be optimized relatively easily, but one question that doesn’t seem to be answered is: compared to the baseline approaches, how much longer does it take to generate outputs, at least as the method is implemented now?
While the potential for significant improvement on quality scores is there due to the significantly higher portion of the search space explored, the authors don’t actually attempt any kind of re-ranking, which leaves an open question: are we able to actually extract the best output sequence from all the generated candidates? Did the authors try any simple re-ranking mechanisms to do this?
For evaluating diversity, one drawback of using distinct unigrams and bigrams is that it weights all n-grams that appear the same, which does not take into account the fact that infrequent n-grams the same. Thus, it would be good to also include Ent-n, and entropy-based metric introduced by Zhang et al (2018) that does account for this. 
One previous decoding method used the combine similar outputs was to over-generate candidates, and then perform post-decoding clustering to group similar candidates together (Ippolito et al., 2019). While the methods proposed in this paper are distinct, these methods should be mentioned in the related work.
Missing Citations: - Generating informative and diverse conversational responses via adversarial information maximization (Zhang et al., 2018) - Comparison of Diverse Decoding Methods from Conditional Language Models (Ippolito et al., 2019) 

==================================================

The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases. 
- Figure 2, it is not clear about "merge target". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, "including that it optimizes for the wrong objective". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work.  - In figure 5, the y-axis label may use "Exact Match ratio" directly.
- Line 409, could you cite the "R2" metric?
- Appendix A, the authors state "better model score cannot result in better hypothesis". You'd better state clearly what idea hypothesis you want. " a near-optimal model score" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph? 

==================================================

1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. 
2) Lacking strong baselines both on entity-level factuality evaluation and summarization. 
1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? 
2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? 
3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? 
4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). 

==================================================

The case study does not provide very strong evidence for the superiority of ICM with respect to the other measures. A paper like this one, whose  ambition is to make a strong methodological contribution, should have a better coverage of real-world evaluation.  It would be made stronger if the authors could test their metric in one or two further domains, and extend the pool of evaluated classifiers to architecture in which the distribution of labels in the development set also plays a role. 
While defining the formal desiderata, probably for reasons of space, the authors do not provide enough intuitive description of what the target of the different principles is. I believe that the paper would be made much stronger if the authors would keep the somewhat didactic attitude that characterizes the overview. 

==================================================

- The modeling of the mixture of multiple aspects is not explored deeper enough, and thus the so-called "first to explore" (in the Introduction) sounds more like an overclaim to me.
- The qualitative results (Table 5) and human evaluation both show that there are still limitations of the proposed method and the improvement is not so obvious.
- Given the arguments about the downstream applications (Line 116-117), I would like to see more downstream performance of such controllable text generation models. Think about how you would connect the proposed models to real NLG applications, and show the advantage of your model. I would say style transfer could be a good start. 
N/A. The authors have addressed most of my previous comments and suggestions. 

==================================================

- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.
- For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.
[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 
Please consider adding new human evaluation results and baselines as mentioned in weaknesses. 

==================================================

1. The experiments are not convincing. For example, the authors use the development dataset to evaluate the results. Test data is not publicly available. The reason is insufficient. In addition, four different query relations is small. It is hard to convince the effectiveness of DILR. 
2. This work is not clearly written. For example, the second paragraph of section 5.1 should be rewritten and divided into more than one paragraph. 
1. What is DNNs, in Section 1 Introduction ? 
2. The rule is "if A is in B and B is part of country C, then A is in country C". How to solve the example by DILR? Specifically, how to process the example by Hierarchical Attentive Reader and Multi-hop Reasoner ? 
3. BERT is a strong baseline model from Table 1. Do you explain why DILR-BERT is stronger than BERT in detail? 

==================================================

- Complicated system with a narrow scope (but that seems to be the current issue with other neuro-ILP work too)   - Complex attention mechanisms that are hard to interpret - The Complexity vs Accuracy tradeoff may not be worth it (e.g. BERT is a much much simpler model with a marginal drop in accuracy). The interpretability of the soft rules could be a potential benefit of the DILR approach, but it is still limited (no interpretation on relations, mostly attention-based entity chains) 
- #54: Min et al 2019 is a decomposition-based approach not a Neural Module Network - #55: Doesn't your attention-based approach also implicitly encodes relevant contexts into unnamed relations?
- #277: What is i and j here refer to? It is only introduced later on the next page. It would help if every term is clearly defined. I would even be okay if the equations are only spelled out in the appendix with the main paper only using some meaningful function names. E.g. B_ij = subj_context_attn(S, C) - It would help a lot in terms of understanding if the indices are not shared. E.g if "i" refers to a token in the subject, don't use it to also refer to the index in the answer candidate later. This will make the reader's life much easier since []_{ij} would then always refer to the attention between the subject and context. We don't have to carefully look around each equation to understand the meaning of each term.
- In 4.2, please think about what are the key ideas that you want the reader to know vs what are the key details needed for reproducibility. Try to move the latter to an appendix.  - The description of the BERT model is unclear. You get the representations for the query-subject OR the candidate? Shouldn't you be getting a representation for query-subject-candidate-context for each candidate and context? Or do you get multiple vector representations and concatenate them? 
  - Are all the models re-trained on the same subset? 

==================================================


QUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using  standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?
- Section 5.1: "... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?
TYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. 

==================================================

The writing could be further improved, including the structure organization and detailed concept clarification. It's also unclear why V&L models are able to store visual commonsense effectively (Is it really from the images or just from the captions?). 
1. It's better to add more vision-and-language models including VisualBERT and ViLBERT, since they are also pre-trained with MLM objective. 
2. It seems not reasonable to me for the formula in Adjective Projection part. If we want to estimate whether the object is large or small, we need to first calculate the similarity between "large" and the object, then "small" and the object, respectively. Then we should calculate the difference between the two similarity instead of using the similarity between the object embedding and the difference between "large" and "small" embeddings. 
3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues. 
4. Paper title is a bit overclaimed and also does not cover one of the main topics reporting bias. 

==================================================

1. Previous studies find that the mis-calibration problem is closely related with model size (Guo et al., 2019; Wang et al., 2020). In this work, the authors did not conduct experiments using larger NMT models, making the results less convincing. 
2. Table 1 shows that the proposed method only brings marginal improvements in terms of Pearson’s correlation. 
3. Table 2 shows that the learned confidence score can result in better BLEU score. To my knowledge, model confidence can affect the BLEU score with difference beam sizes. I wonder wether the proposed method can improve BLEU in larger beam size (i.e., beam size = 100). Moreover, in Table 2, the authors did not compare the proposed method with graduated LS (Wang et al., 2020), which is a closely related baseline. 
In Table 4, I wonder whether "the model probability" is estimated using MC dropout. Since MC dropout is very useful in OOD detection. Please provide more details. 

==================================================

- In Table 1, although Conf outperforms TP, but the advantage of D-Conf comparing to D-TP is modest.  Given these numbers are Pearson's correlation, practically if the correlation cannot be improved by around 10% in absolute term, it's unlikely people will adopt it for estimating confidence scores given the extra cost on model training, inference cost and hyperparameter optimization.
- The hyper-parameter is crucial for the proposed loss in this paper. If lambda is too strong, then no confidence will be learned, c_t is always 1. If lambda is too small, perhaps the model will always produce c_t = 0 to peak the ground truth. However, critically, I was not able to find information about how the authors found/optimized the hyper-parameter. 
Many mistakes, for example Eq.6 does not have the summation. And also grammar/spelling mistakes. 

==================================================

Regarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that "Through careful observation we find that all positive examples have the same length ..".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by "syntactic structures".  It would be great to have some examples or more descriptions on what does "syntactic structures" referring to.
Regarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.
Regarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. 
Is the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.
This paper created a subset by filtering based on length of the positive/negative example pairs. How big is this new subset? 

==================================================

The biggest concern regarding this paper is the lack of human verification. That is treating humans (or a group of annotators) as a black-box (same as neural network model) and checking whether the proposed transformation indeed produces valid comparison. The transformation is all based on intuitions and without human validation,  it's hard to determine whether it will be valid and therefore we do not know whether the satisfy value is indeed meaningful. 
In the introduction, the paper says "reliance on ground truth data limits the quantity and quality of test cases we can produce". In my opinion, ground truth data will increase the quality of the test cases. I was wondering why the author believes that annotation limits the quality of the test cases. 

==================================================

The paper is framed as pitting, to some extent, distributional information against the phonological hierarchy. But these kinds of phonological constraints tend to be soft and violable. So I was not convinced by the takeaway of the first set of experiments using the phonological classifiers. What does this add on top of just examining the distributional patterns of phonological features in the compounds and EEs?  The motivation for the neural experiment was also a bit confusing to me. Essentially, it shows that you can predict the ordering of B and C in ABAC based on the training data, because B and C occur in that same ordering elsewhere in the corpus. This doesn’t seem particularly informative as a result.
As the paper points out, there seem to be two routes to learning this information: (soft?) phonological constraints and the distributions in texts. But I’m not fully convinced that this work breaks ground theoretically or methodologically. 
Might want to check out work by Emily Morgan and Roger Levy on binomial pairs, predicting the ordering of items in a phrase like “peanut butter and jelly”. 

==================================================

I was not able to find any major concerns. Some minor comments are written in the next section. 
- I am a little unconvinced by the discussion in lines 477 to 486. I speculate that sentence embeddings learned by DiffCSE focus more on superficial information than SimCSE in order to solve replaced token detection, but the result is the opposite. Why?
- It would be insightful to investigate the impact of the model size of the encoder. In the past few years, I have repeatedly observed that a method that works well on small models has only a marginal impact on large models. This paper will be further strengthened if the effectiveness of the proposed method is shown even when large-scale encoders are employed. 

==================================================

Technique is limited to problems that can be modeled by equation trees.  A lot of paper real estate is given to an analysis that basically shows:  - undertrained models don’t work - only using part of the encoding function (the bottom N layers) doesn’t work I don’t think this analysis will be of much use to the ACL community.  It seems like the cosine similarity of lower layers in figure 3 are relatively high, while the t-SNE visualizations in Figure 2 are more mixed. Do you think t-SNE is accurately representing the latent space? 
The paper would benefit from connections to prior work on BERTology. An intro to this line of research can be found at https://huggingface.co/docs/transformers/bertology 

==================================================

The main weakness of this paper is the motivation, which is extremely    imprecise. The pedagogical literature cited in lines 36-46 is opaquely    summarized and does not lend sufficient grounding for introducing the    problem: "mathematics is about patterns and not merely about numbers";    "...students explore patterns, not just memorize procedures". The authors    distill these insights into their hypothesis, where they posit that    "semantics affects problem-solving". The word "semantics" is used    extremely liberally here, and it is was unclear to me what the authors    actually meant here (vs. prototype equations). The analysis and discussion    offered by Section 3 is difficult to follow because of this, and it is not    clear how instances "similar semantics" are tracked or computed (see, e.g.    Figure 3). Upon several re-readings, it became apparent that what the authors    actually refer to is lexical overlap (at least as evidenced by Probs C and D    in Figure 1). Due to this, the representation clusters for each prototype    equation tend to be expanded by the problems with high lexical overlap across    classes. This is more or less straightforward, but more care needs to be    devoted to making the language precise and less lofty. 
Comments:    * When investigating cosine similarity, authors should consider the work that    investigates MLM embedding spaces, namely    https://aclanthology.org/D19-1006.pdf and    https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the    anisotropy phenomenon.     * It would be interesting to see how the embedding spaces look like for BERT      out of the box.
   * It is important to make distinction between `procedures' and 'patterns'    Questions:    * What layer are the plots in the top row in Figure 2 from? Conversely, what      epoch are the plots in the bottom from?
   * Relevant to above point: how are "similar semantics" calculated? What is      meant by `semantics'? This sort of language not useful. ( lines 216-20)    * Lines 250-52: what does this mean?     Style/typos:    * Line 46 - hypothesize instead of `think'    * Line 55 - citation for encoder/decoder    * Line 155 - representations 

==================================================

1) Lack of interpretability: There could be more of a discussion of why "semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. 

==================================================

1) Although the proposed method is indeed interesting, the  authors do not make any attempt to compare it to any other prior methods. They limit themselves to showing that the method produces statistically significant linear regressions. But this is, to my mind, insufficient as empirical evaluation. Why not implement a simple baseline (for example, character frequencies, etc) and compare to it? This would make the paper more persuasive.
2) The strange results for "p --> b" sound change deserves more explanations beyond just noting that this is a rarer phoneme than the others. If the method is not able to trace sound change in 1/3 cases, it must be shown that it is still better than anything else (baselines).
3) The method is entirely based on written signal (spelling). It is understandable that we lack proper phonological datasets, but I would like to see more discussion on how this spelling proxy might influence the performance of the proposed method. 
l. 066: "phonology: Inspired by" --> "phonology. Inspired by" May be, reposition Figures 1 and 2 to use the page space more efficiently?
Did you try cosine distance instead of Euclidean? It might perform better in the end.
The authors several times mention "neural methods that make use of dense character representations" (which they don't use). I believe that the word "neural" is redundant here. Dense representations can be produced without using any neural networks (for example, by applying SVD to PMI vectors).
These papers, although not 100% on-topic, might still be mentioned in the related work: https://aclanthology.org/W19-4713/ https://aclanthology.org/W19-4732/ 

==================================================

1. The idea is incremental. Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers.
    [1] Deaton, Jon, et al. "Transformers and pointer-generator networks for abstractive summarization." ( 2019).
    [2] Prabhu, Nikhil, and Katharina Kann. " Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies."
2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version. 
1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage? 

==================================================

1. The paper does not clearly introduce the dataset. For example, what is average length of each text in NewsEla? If the text is long, the paper should also mention how the text is encoded by BERT when it exceeds the maximum length. 
2. As the paper already pointed out, the nature of pairwise model restricts the inference for downstream applications, so that the contribution of this paper is not very strong. 
- L400: citation for "for evaluating ranking or information-retrieval tasks in literature"?
- L502: Is NPRM (0.999, 0.995, 0.990, 0.948) better than regBERT (0.999, 0.997, 0.994, 0.977)?
- L613: "while" -> "" 

==================================================

- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.
- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? 
- Are "slugs" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.
- "In this background" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. "With this background", "Given this background", etc.
- 2 Related Work - "result in" --> "results in" - 4.4 Evaluation - "w" --> "we"? It's unclear what is meant.
- Limitations of NPRM - "while" --> "why" (or just get rid of "while") 

==================================================

- Overall, the paper somewhat lacks novelty. Most ideas presented in the paper have already been considered before (for example, end-to-end training; enhancing retriever with knowledge distillation).  - Specifically, knowledge distillation was previously considered for retrieval (from a reader rather then a re-ranker; [1]) but a proper reference is not given.
- The claim which DPR and BM25 scores aren't comparable is not entirely correct (Line 191). [ 2] show that a hybrid retriever over DPR and BM25 is actually able to improve over both.
- FiD [3], a state-of-the-art reader for open-domain QA (NQ & TriviaQA) is a missing baseline. Indeed, it seems like the results of Fid are similar to Re$^2$G. If this is indeed the case, it should be reflected in the paper.
[1] Izacard and Grave. Distilling Knowledge from Reader to Retriever for Question Answering. 2020 [2] Ma et al. A Replication Study of Dense Passage Retriever. 2021 [3] Izacard and Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering 
- Did you try to marry FiD with your reranking framework, thus eliminating the need for expensive cross-attention in the decoder (by lowering the number of retrieved passages fed to FiD)? 

==================================================

I am wondering how novel the application of the knowledge distillation from the reranker as teacher model to provide labels to the DPR model is, see Hofstätter et al."Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation" (https://arxiv.org/abs/2010.02666).
Furthermore I am missing an ablation of training the reranker with the combined scores of the positive passages (line 277) in comparison to the standard way the reranker is trained in Nogueira et al.. I would also appreciate an ablation study of combining DPR and BM25 results with reranking in comparison to established solutions like combination with Reciprocal Rank Fusion (see Chen et al. "Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models") or combining the scores with a linear combination of the BM25 and DPR score (see Karpuhkin et al. "Dense passage retrieval for open-domain question answering") I am also wondering why only 5 of 11 existing datasets of the KILT leaderboard are selected, I would be interested why you exactly choose these datasets. 
I think the overall writing is clear and understandable however I thought it could be better organized in some sections: Line 207-2018 describes dense retrieval in the section about re-ranking, I think dense retrieval should be described beforehand and not in the section "Reranking" The formulas in Line 310 are not described and embedded in the text, I think describing them in the neighbouring text would make clear why the formulas are there. 

==================================================

Some evaluations results are mixing. Please see the detailed comments below. 
Question: 1. In line 257, it's strange that including the collected LIV-EN parallel data for finetuning actually makes the NMT system perform worse. Could you provide more discussions/explanations on this?
2. Could you provide some insight why the multilingual model is "noticeably weaker" (line 236) on the ET→En and LV→EN evaluations?
3. It's interesting that the LV→EN model performs better than the ET→EN model, especially in section 2 authors mentioned that Livonian and Latvian languages are similar in many aspects.  minor: 1. Maybe the information that "the translation is done by hired experts (section 3)" can be added to the footnote 2 on page 2 (section 1)? I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.
2. Line 194, maybe I'm not familiar with the context, could you explain what does "implement the support of ..." mean?
3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.
4. Line 210, on what ET, EN, LV data is the Sentencepiece tokenizer obtained on? Those in the 4-language parallel corpus?
5. Line 255: typo, "perform performed" 

==================================================

- The experiments in the paper do not necessarily directly answer the research questions as posed in the introduction 
With an additional page, it would be nice to focus on some nice-to-have analysis to make it clearer what to take away from the experiments for those interested in this language pair, or others: *why* might there be a slight preference for pivoting through Estonian? Is this simply because the Estonian model performs slightly better than the Latvian before pivoting, and have nothing to do with the appropriateness of contact language vs typologically related language? Are there any reasons you can find for why finetuning is worse than pivoting? Are there any ways (besides downstream BLEU) to how use of a contact language or same language family affects the model?  - Table 2 layout is current a bit difficult to read; consider adding additional row and column headers saying source/target, pivot vs multilingual vs finetuning, to make it easier to match results to experiments - Would not call 8.92 a "very respectable BLEU score"; perhaps write that it is "improved" instead - The way the multilingual model's vocabulary is chosen (25k units taken from imbalanced multilingual sources, size-wise) is likely to hurt performance on Livonian-English; it might make more sense to check to see what vocabulary granularity is being used for Livonian (if all or mostly characters, for example, that suggests the multilingual vocab is not appropriate, and perhaps a set of smaller BPE vocabs for each language should be created and then joined after) 

==================================================

1.The novelty is a bit limited for the small focused task. 
In the information extraction area, recently a lot of work has been explored to replace the traditional classification methods with the generative models. It is not novel enough to directly adapt to another task. For the major contribution of this work,  the special design of inputs and outputs could well capture the characteristics of the task. Fine-tuning multilingual LMs enables good generalization for zero-shot cross-lingual transfer. If the language-agnostic templates could be applied to other structure prediction tasks with good improvements, the contribution could be larger. Currently this paper is okay but not novel enough. It would be better to discuss the potential of language-agnostic templates (html tag). 

==================================================

The authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. 
- L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557). 

==================================================

Although the paper is interesting and strongly supported by experimental results, understanding the details is tough. For one, the difference between TGDFA+orig and TGDFA is not very clear in what is meant by the original bilingual GDFA. An illustration (page limit permitting) may be helpful to understand the difference.
It is unclear if HELFI training set uses 3 languages or 84 as listed in the appendix. 
Apart from the concerns above, it is better to have some insights on how more other language pairs not in the test set correlates with increased performance. This can be achieved by experimental setups which include or exclude other languages not in the test set. For example, in HELFI, there can be one setup which includes only Finnish, Greek, and Hebrew and another that includes all the 84 languages in the Appendix A. The Appendix A is awkward as the section heading is not in one page with the entire content, which can be achieved by the typesetting. 

==================================================

1. Punctuation only includes four marks (.,!?), but others are more common (specifically ":). Why not include these? ( non-scientific source: http://www.viviancook.uk/Punctuation/PunctFigs.htm).
2. Table 4, if I understand the experiment correctly, seems unfair. You're comparing sentence segmentation from models trained on mostly punctuated text to your model, which expects unpunctuated text. A fairer comparison would be to run the original text through a pretrained punctuation restoration model. 
What is the point of the taxonomy when you have four punctuation marks? ~L130 The annotator disagreement might be interesting for the different punctuation marks. I expect high agreement for question marks but much lower for commas. 

==================================================

It is unclear if the additional punctuation mark of exclamation mark adds any significant theoretical difficulty to the problem apart from the lack of trained models. In fact, it would play a role in poor performance of models that have not been trained on dataset without that punctuation mark, which doesn't truly highlight the flaw in these models. The drawn conclusion that this is due to the "different nature of data" thus does not hold much value. 
Further, in the field of PR, it seems like there is plenty of scope for creation of synthetic datasets by simply removing punctuation marks from arbitray English texts, movie transcripts and so on. Comparative study of such synthetic datasets, and an analysis of why human-annotated sets might be of more value, if at all, should be performed. 
Creation of synthetic datasets on large scale, by removing punctuation marks seems possible. This option should be explored. 
Models trained on such synthetic datasets can also be compared on the human-annotated datasets. These might serve as a better baseline than models trained on data without the same set of punctuation marks. 

==================================================

- Although the proposed approach does bring WSD improvements, it is rather incremental. This is probably not a "weakness" per se, just that the paper is not an eye-opener.  - The evaluation is done on English data only, which leaves some doubts about how this would work with other languages. 
L. 135: " linguistic law exists in many corpora" --> probably  "linguistic law is manifested in many corpora"? The whole statement is a bit strange: Zipf's law is well known to be manifested not "in many corpora", but in _all_ human languages, and thus, in any imaginable corpus of natural language texts. 

==================================================

A critical limitation is how practical the method is given the nature of model expansion. This method is studied under a still relatively synthetic setup. There are a lot of problems that I can imagine for real-world large scale models such as GPT3. To mention a few: (1) How should we decide when to expand the model? In reality, these models are trained on O(100B) data, covering a significant portion of the whole internet. So when do we know a new data source emerges? How to detect this? ( 2) Large scale models are trained using sharding systems of thousands of accelerators, it is a challenging technical question to expand them in a way described in this paper. ( 3) Recent works have shown it is possible to continue training language models for either language understanding [1] or additional applications such as code synthesis [2]. Therefore, perhaps these simple approaches are sufficient enough for good generalization.
[1] Finetuned Language Models Are Zero-Shot Learners. Wei et al., 2021. 
[2] Program Synthesis with Large Language Models. Austin et al., 2021. 
How is the prompt token computed at inference time? Is it included using nearest neighbor search or not included at all? 

==================================================

I have reviewed this paper in the previous round and was happy to see that the authors have addressed the previously found weaknesses: - Method: The method section has been re-structured and as a result became much easier to read.
- Human evaluation: The appendix now provides more extensive information, including details on annotator consent.
- The applications of this work are described more clearly - One thing that is still missing is a short discussion of possible limitations. 
- Some crucial information is only presented in the appendix (most importantly the details on the evaluation metrics, which cannot be understood without reading the appendix). This should be included in the main paper if accepted. 

==================================================

1. 	Innovation is weak, only the modification of existing methods. Although existing cross-lingual NER methods don’t consider multi-task learning, it has been widely used in transfer learning/unsupervised learning. 
2. 	The experiment is not sufficient. A key contribution of this work is the teacher-student framework. But little experiments are carried out to prove its effectiveness. It seems that MTST in ablation study aims to prove it but the description “multiple-teacher to single-teacher” and “teacher and student have the same neural network structure” is confusing. 
There are some grammatical mistakes, such as a)	in line 115, it should be "that introduces an entity similarity evaluator"; b)	etc. 

==================================================

1. The authors unitize multilingual mBERT as basic sequence feature extractor to achieve the sequence embedding representation, why not try other multilingual pre-trained language models, such as mBART, ERNIE-M, mLUKE and so on. 
2. The languages in CoNLL dataset are some rich-resourced languages indeed, the authors need to test MTMT model on some low-resourced languages.
[1] Ri R, Yamada I, Tsuruoka Y. mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models[J]. arXiv preprint arXiv:2110.08151, 2021. 
[2] Ouyang X, Wang S, Pang C, et al. Ernie-m: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora[J]. arXiv preprint arXiv:2012.15674, 2020. 
This paper has done a solid work on Cross-Lingual Named Entity Recognition, however, some questions remain to be clarified.
Please try more multilingual feature extractors and test MTMT model on some low-resourced languages. 

==================================================

1. I am concerned about the intuition/motivation of the Similarity evaluator teacher model. Paper reported a concrete example in lines 70-85. There can be a possibility of negative transfer. Let’s consider a counter-example: suppose for a low-resource language LR with sentences S1 = {w1, w2, w3, w4}, S2 = {w1, w5, w3, w4} and S3 = {w1, w6, w3, w4}. Where (w2, w5) and (w2, w6) are considered as token pairs by Similarity teachers. The English NER model predicts correct labels Lc for w2 and incorrect label Lic for w5 and w6. While training with the student model, it is possible that the similarity evaluator teacher may provide incorrect supervision for w2 as the other two tokens have incorrect labels. 
2. It is unclear how many tokens are uniformly sampled from each sentence while Entity Similarity Pairs Construction and how the token pairing has been done? There should be a label distribution analysis of the source language training dataset used for the Similarity evaluator teacher model. 
1. Some sentences are confusing. For example (1) lines 7-11 sentence. In particular, what is the meaning of the phrase “identical single task across both domain.” 
2. In line 23, it should be: …on the three datasets…. 
3. In lines 523-524, it is unclear what embedding distribution is considered from the student model; add more detail. 

==================================================

1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' 

==================================================

- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.
- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.
- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper. 
Some typos: - 001: "The latent variables" -> "Latent variables" - 154: "efficiently to compute" -> "efficient to compute" - 299: "We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder" - unclear - 403: "langauge" -> "language" 

==================================================

1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
2. Replicability of method is not clear, there's no indication that the code will be released. 
- I might have missed the specific in the paper; hopefully, I get CKMT = "Compact-network K-nearest-neighbor MT" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = "Pruned CKMT", hope I get that right too. 

==================================================

1. From the results in Table 2, it shows that in ALBERT, with null prompt, the performance largely decreases (from 8 #Wins to 1), which raises the question of the necessity and robustness of null prompting. Moreover, it shows that prompt-based finetuning with BitFit can achieve comparable or even better performance, with memory efficiency introduced. It suggests that only using BitFit is a more wise choice. 
1. It may be better to conduct more experiments across different models. 
2. It may be better to analyze the principle of the success of null prompts instead of only experimental evaluations. 

==================================================

As the authors mention, the dataset is limited to simulated dialogues of simple objects, rather than human objects, but this is a preliminary study. In addition, it is not specified whether the data and code will be publicly released, which is the primary contribution. Some of the baselines used were weak. Other stronger baselines should be considered, e.g., SUMBT, DSDST, GCDST, SOM-DST, CSFN-DST, MinTL, and Pegasus. 
- line 249: "tuned" -> "tune" - line 325: is a word missing after "linear"?
- line 343: "recovers" -> "recover" - please clarify what is meant in line 375 by "harmful" and annotation bias in line 373 - line 494: "Asides" -> "Aside" - lines 528-529: should "only improves the results significantly" be "insignificantly"?
- line 541: "We observed that" is repeated twice - line 584: please elaborate on what is meant by filtering the videos and training a new model, as well as how to interpret Table 5 

==================================================

1. The authors provide the performance improvement of the proposed approach on the OOV dataset but do not show how the performances on the NER datasets change (if they). 
2. I am not an expert in the field, so I don't know other span-based approaches other than the ones the authors cite, but it seems very expensive in terms of speed classifying all the possible spans in possibly long input sentences. 
line 128: BN has never been defined. 
line 216: subdividing -> subdivide? 
line 420: the the -> the line 430: MOreover -> Moreover line 436: 5Our -> 5 Our There are other typos that I did not mention, like parentheses starting right after the word without any space, more in general, the paper needs a good session of typos polishing. 

==================================================

1. The use of BERT architecture for this task has already been explored in R^2 BERT [Yang et al. 2020]. Existing work [1] explores use of contextualised representations of multiple sentences for document classification, so the idea of multi-scale representation may not be completely novel.
2. For document-level representation, the model uses only first 510 tokens. Ironically, the authors claim to get most improvement on prompt 1,2,8 for ASAP dataset (all of which have >510 tokens) 3. Some statements in the discussion are misleading. in [518-521], the authors mention "We further re-implement BERT^2 proposed by (Yang et al., 2020), and the performance is not so strong as the published state-of-the art like models 9, 10 and 12.". The BERT^2 model is same as model 10. If the authors are trying to claim that their own implementation of BERT^2 is not as well-performing as the published result, there could be some implementation issues not taken care of resulting in sub-par performance.
4. For the loss functions, the authors should either give references to existing work which proposes these losses (for any task) or explicitly indicate that it is their contribution.  [1] Lu, Jinghui, et al. "A Sentence-Level Hierarchical BERT Model for Document Classification with Limited Labelled Data." International Conference on Discovery Science. Springer, Cham, 2021. 
1. The citation of R^2 BERT in table 2 is incorrect. 
2. The work R-DROP has been mentioned in introduction without giving any context (it is a dropout strategy) 3. The equations for attention pool are incomplete (terms under the summation missing). It makes them hard to understand. 
4. The grammar in the paper writing could be improved. 

==================================================

1. The writing needs to be improved. Structurally, there should be a "Related Work" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the "Introduction" and "Related Work" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of "Those systems are not state-of-the-art", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). 
Comments: 1. Please keep a separate "Related Work" section. Currently "Introduction" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points ("Traditional AES", "Deep Neural AES" and "Pre-training AES") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays.  Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. " Multi-task Learning for Automated Essay Scoring with Sentiment Analysis". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference. 

==================================================

1- It is not clear how this task and the approach will perform for new information (updates) about an event (even if it is contradictory to what is known about an event) 2- The approach operates on clusters. New events may not have clusters.  3- Measurement of the performance maybe highly affected by the way the evaluation dataset is created.
4- Comparing random and logistic regression based baselines to something that is empowered by BERT is not fair. Even if it is a baseline.
5- I appreciate the effort spent for this paper. It is a lot of work and some parts are described very well. However, many steps are either missing or could be much clear. 
1- "because events are more important to storytelling": Do you have any investigation or reference for this? Sentiments may be playing significant role as well.
2- The concepts "topic" and "event" are confusing as they are used in the paper. The approach is based on events. The datasets utilized are based on topics. I see the details of the datasets and I understand it. But this is an issue in the whole article.
3- How is the entire doc used to create the knowledge graph in the subsection "KG representation"?
4- resoltuion -> resolution 

==================================================

The paper is easy to follow, but the presentation can be improved. Below is a list of minor points, and they can be fixed rather easily.
- The paper does not defend itself well enough. Instead of claiming "both training and predicting processes are language independent," it should openly acknowledge that the approach won't be applicable to certain language orthography that is not very syllabic. This is addressed in the conclusion, but should appear early.
- The training loss is not explicit. The description in section 3 is not detailed enough.
- The paragraph and the table right above section 4.1 are good to have, but they feel cherry-picked. The paper should consider discussing other benefits, such as how easy it is to add new words to the model.
- Section 4.1 is weak, if not hurting the paper. There should be a deterministic mapping (or something very close to deterministic) for Japanese Hiragana, and the results definitely do not help the claim that the approach is language independent. The approach would not be effective if we apply it to Japanese Kanji or Mandarin Chinese. 
Though there is a accompanying repository, I still think it's worth talking about the model architecture, training hyperparameters, and the training loss.
The model is an energy-based model. Given the limited space, there probably isn't much to be said, but it might be worth mentioning so that the readers are aware of this. 

==================================================

- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric. Please refer to the question. It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric. 
In Eq. 3, does T* include all the slots-values in gold and predicted belief states or just the predicted and gold slots-values in current turn?
How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)? If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA. I think you should include evaluation results with the AGA metric. 

==================================================

Weaknesses of the paper: - results could have been discussed in more detail - some factors like time zones could have also been considered when selecting data 
1. Do you think that the location of a country within the time zone affects the results? If a country is located at the westernmost part of a specific time zone, there might be more than one hour difference between the sunrise/sunset with regard to the easternmost part. 
2. Have you thought of visualizing the data on a map? This might be helpful for discovering some patterns in the data. 

==================================================

- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant.  - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021)  rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.   - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models.  - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) 
Most of my comments are given above.  Details : some references have no venue -> please correct this.     misc{hsu2021robust,       title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},        author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},       year={2021},       eprint={2104.01027},       archivePrefix={arXiv},       primaryClass={cs. SD} } 

==================================================

- Lack of novelty:    - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models. 
  - Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER. 
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives. 

==================================================

Weaknesses: - Retrieving (avg # sentences) * 100 sentences (see section 3.3) instead of just 100 sentences seems to be a bit of a cheat. For a strict comparison to DPR, Top-20 and Top-100 performance should be reported with exactly those numbers of retrieved elements and without post-processing on larger sets of retrieved passages. One could argue that allowing for more expensive passage retrieval is what is giving the improvements in this paper, other than for SQuAD where the lower granularity does seem to be helping, except it doesn’t help as much as BM25.
- The idea of having more granular representations for passage retrieval is far from new. The authors do cite DensePhrases (Lee et al. 2021), but don’t mention that it’s already at lower granularity than passage level. They could also cite for example ColBERT (Khattab et al. 2021).
- The big improvement reported in Table 2 for SQuAD “Single” is a bit confusing since it relies on a Top-20 number that is much lower that what is reported on the DPR paper (although this seems to be a common problem). On the positive side, the number reported for SQuAD “Multi” matches the DPR paper. 
Suggestions: - Line 91: the authors claim that contrastive conflicts are *the* cause for bad performance on SQuAD, but the statement seems unjustified at that point. It might make sense to refer to later results in the paper. 

==================================================

- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. 

==================================================

1. This paper lacks a discussion on its novelty, e.g., why continuous latent variables are better than discrete latent variables for solving the one-to-many mapping problem. Though empirically the proposed DialogVED performs better than PLATO, DialogVED incorporates more other techniques (e.g. n-stream self-attention, memory scheme, masked language modeling, free bits) than PLATO. It is not very clear to me why we should choose those techniques to train a continuous variational encoder-decoder. Maybe some ablation studies on the training objective and model architecture can help explain it. 
2. The model evaluation does not show the model's superiority in solving the one-to-many mapping problem in open-domain dialogue response generation. Distinct-1/2 are corpus-level metrics and are not showing the diversity of generated responses given the same context. To verify their claim, the authors may consider using self-bleu and the ratio of unique generated sentences to better evaluate the diversity. 
1. It is unclear to me what's the input to the prior network. Fig 1 suggests the [CLS] token is at the end of the context, but line234 says the [CLS] token is at the beginning of the context. 

==================================================

- The evaluation of the paper could be made stronger by using some of the standard datasets for terminology translation (e.g. wmt21 shared task) and evaluation metrics (Alam et al. 2021).
- The description of the alignment embedding seems a bit under-specified. Am I understanding it correctly that the constraints in the target sentence get an additional index that gets embedded (similar in concept to positional embeddings)? Are unaligned words in the constraints marked in a special manner? Are these embeddings recomputed in every refinement step of the LevT? 
- Line 232: The are surely sentences where not every bucket is represented, right? Would it be then more correct to say that you have **approximately** 6x the data? Or am I misunderstanding something?
- Line 248: This explanation, while plausible, could be relatively easy to check just by looking at the words themselves. Have you done that? Have you tried filtering those words out (e.g. using stopword lists or similar) as they are unlikely to appear as constraints in real life situations?
- Line 263: It would be better to use l_i.
- Subsubsection starting at 324: Would it make sense to use the lexicon information directly if available (which it is for some test conditions) and resort to automatic tools only if necessary? Related to this, have you measured how sensitive the method is against alignment errors? This is also specially relevant for out-of-domain settings, where the alignment model is also operating in out-of-domain conditions.
- Table 4: Please also include bold numbers for the baselines of previous work. Specifically for WMT17-WIKT the best result in terms of BLEU is actually in the baselines. 

==================================================

- No comparison across languages (only De target language) - While testing on the full En-De test set, the improvements are minor 
*Typos:* - Table 1: htten --> hatten - line 155: An --> a - line 166: For NATs --> seems redundant, an NAT --> a - Figure 1: I am confused by frequency buckets. Does the x-axis represent the **inverse** frequency of constraint? Otherwise, I am genuinely confused. And while sampling self-constrains, do you sample from the word buckets based on the sentence itself or do you consider ALL possible words (from all sentences) in the bucket?  *Additional questions:* - During training you sample 0-3 reference tokens as constraints, since the BPE is used, does it mean that it might be the case that only part of the word is considered as a constraint? Or do you handle such a situation in some way?
Thanks! 

==================================================

1. One problem I can notice is the scalability issue of the proposed method.  As mentioned in section 4.3, step-01 of training required labeled parallel sentences, limiting the proposed approaches to the few high-resource languages. 
2. The proposed model lacks a comparison with baseline(s) from the literature. For example, a baseline that uses external knowledge like LAKM (Yuan et al. 046 (2020); line-46) or Liang et al. (2021; line-50) can be taken. It is not necessarily the proposed model that should outperform these baselines but to get an idea of where this approach stands compared to previous work. Other possibilities to use the more popular pre-trained model like XLM-R or mT5 (encoder only). 
3. Many of the reported scores (particularly for mBERT) in Table-2,3 & 4 are close to baseline; the statistical significance test is recommended for reliability 
1. It is hard to read numbers in Figure-3 and Figure-5 with a printed copy. 
2. In Line 492, the referred table number is incorrect. 
3. Author(s) can plan to investigate the unsupervised approach as a future extension of this work which will be more promising. 

==================================================

1.Adding the random selection is not a new idea. The randomness of the layer selection could cause the unstable of performance. It is better to show more results such as the convergence analysis. Random selection might not be the best option here.
2.The performance is not convincing. The improvements under some settings are not significant.  3.The new approach could not find the best subset (subset mapping) since the proposed model just selects m layers randomly. But it is not clear if we should ignore it or solve it later. From my opinion, the attention-based approaches with best mapping search may achieve better performance. 
The whole approach lacks the evidences to prove the usefulness. Comparing to ALP-KD, the improvement is not significant. 

==================================================

1. Though experts have higher annotator agreement, they are not guaranteed to perform 100% accuracy. What is the consistency score between expert annotators? 
2. The consistency and accuracy are confusing. What is the difference between arg-wise consistency and accuracy? The intuition is that given the low pred-wise consistency, the label annotation agreement should be even lower because all labels related to inconsistent predicates are different. 
3. Are the consistency and accuracy scores calculated before or after experts' revision? 
1. L70-71, researches ... focus ... make -> research ... focuses ... makes 2. L389, augment -> argument 

==================================================

While I do see the benefits of generating adversarial attacks and the advantages of building tools that guard against them, I find this line of work, and this paper as part of it, problematic.  In NLP, there is a growing interest in stress-tests that measure model performance in counterfactual scenarios (see CheckList, Counterfactual Invariance to stress tests, etc.). As opposed to adversarial attacks, such papers try to design counterfactual examples that we know we want to be robust to, and carefully measure model sensitivity to them. As this is a causal problem (estimating the effect of a counterfactual on an observed outcome), such methods examine performance with respect to modifications that we can reason about (i.e. flipping gender/race etc.). Reading this paper, I’m still not convinced that we should put such emphasis on examples that are modified based on word-level perturbations generated by a correlation-based search method. As such, I believe that training models to be robust to such attacks is somewhat tangential. Generally speaking, do we have a good reason to believe that these adversarial attacks are realistic? What do we learn from them on the model and its robustness and reliability?
If I put aside the power of adversarial attacks in uncovering model vulnerability, I do find this paper interesting and useful within this context. I suggest authors better address the causal nature of generating counterfactual examples, and try to analyze the sensitivity of their method to the type of attacks generated by SHARP. In doing that, this paper can certainly contribute to the growing literature on model vulnerability and robustness. 
The paper is generally well-written, but may benefit from an additional round of editing.  Some typos for example:  Line 588 - “investigate the sensitivity“ Line 590 -  “as an search problem” Line 595 - “guarantees a better“ 

==================================================

This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets.  At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this "data quality audit" work, because this work sets a precedent/"standard" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. 
- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - "Limitations" section in the Appendix could be moved to the "Impact Statement & Ethics" section. 

==================================================

- Training is performed on each dataset separately. This may result in the models being quite domain-specific, i.e., limiting their generalisation ability.  - Method is limited to 2 sequences (minor point) 
- PISP objective: I can see how patch-based swapping may help for learning a better intra-modal association in the case where the patches depict some objects the text refers to. But what is the assumption regarding the benefit of general patch-based (as in CLIP-ViL) swapping for the sequencing task (general intra-modal understanding is already the motivation for SMRM, isn't it?)?
- Sect 4.2.3: Maybe an illustration would help to explain this part, I found it hard to understand.
- Sect 4.3: The output of the BERSON decoder is simply the order, i.e., integer values (e.g., 0 and 1 in your case of sequences of 2?), isn't it?
- Section 5: Why did you not also compare against a pre-trained ResNet version, as you did for CLIP (image-only)?
- Section 5: How would you explain the gap of the effectiveness in the Image-Only setting on WikiHow vs. RecipeQA? For the other settings/models/modalities, the difference in effectiveness is comparably marginal.  **Typos, Grammar, Style, and Presentation Improvements** - l135: "consists" -> "which consists" or "consisting" - Section 3.2: The amount of footnotes could be reduced.  - l318: select *the* same - l.466: *the* text-only 

==================================================

1) It is unclear how one should do the smoothing of LR in the “Smooth” variant. The authors proposed some values in the reasonable range but without any discussion on how they are decided. It would be great if there is a methodical approach or a description of how this is done. 
2) Layer conductance is not a good way to measure vanishing gradients. It is a measure of neuron importance for a given input. While the result of conductance analysis is in the form of “gradients”, it is a different concept from the gradients the model receives during training. Hence, I would argue that the layer conductance analysis is actually not measuring what the authors aim to measure and the conclusions should be revised. 
3) MSLR requires knowing the unimodal optimal learning rates in advance - which means we have to spend resources to tune unimodal models and discard them in the end. It would be interesting to see how this compares to directly tuning a late-fusion model with separate LR for each modality, especially with a similar or less compute budget. 
1) Relating to weakness 2), the authors need to properly measure gradients during training and validate their hypothesis about vanishing gradients, which would be easy to measure with norm of gradients. If the authors do not have the resources to conduct this analysis in time, I suggest the authors to revise their claims on vanishing gradients as layer conductance does not measure that. 
2) I suggest the authors to add a brief discussion about how to pick learning rates in the “Smooth” variant; 3) Typo - line 440, “...from the all the joint…” -> “...from all the joint…” 4) Typo - line 561, “while MSLR while achieves…” -> “while MSLR achieves…” 

==================================================

A study focusing on grammatical error correction should include an error     analysis that attempts to characterize the types of a mistakes the model     makes. The authors likely had to go forego this step due to space     constraints, but without it the insights produced here are generally     limited.  Furthermore, the authors do not include any sort of context for what kind of     performance the models are expected to achieve for these tasks in favorable     circumstances, e.g. in a supervised learning regime. Without this     information, it is difficult to ascertain if the out-of-the-box models'     probing performance is at all good, or if it pales in comparison to simple     supervised baselines. I imagine this would be easy to achieve by fine-tuning     a model on some splits of the dataset, or training another architecture for     the task, e.g. a BiLSTM.  Lastly, it would be useful to see how their models fare on downstream tasks,     how they compare with other Chinese-language MLMs like RoBERTa, and what     implications this might hold for Chinese MLM design. Indeed, the authors     provide this information in the appendix, but nothing is shown in the main     body of the paper, and the discussion is limited to a sentence. 
- How is P@10 calculated for spans of length 2+? This should be mentioned.
- Is the [MASK] token applied to the omitted characters in the insertion task?
Line 249: Michelle, not Micheal 

==================================================

1. The write-up has many typos and some formulas/explanations are confusing. 
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. 
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. 
4. More strong baselines should be included/discussed in the experiments. 
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. 
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. 
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? 
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.” 
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? 
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? 
12. Line 450: L_{pc} or L_{cp}| in Eq. 7? 
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”? 
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? 
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? 

==================================================


Here attach the comments about addressing my previous review.  A1: Would you give some suggestions based on current results about which part can be discarded first if computational efforts are constrained?
A2: "We believe any functions that first rise then decrease can also obtain similar results" -> Do you have any other instantiation? Also, if this argument can be clarified, this sentence should be added to the paper.
A3:  Based on the explanation, it seems that ND cannot be adapted to beam search, as it cannot achieve better performance. Thus, the discussions from Line494 may be problematic.
Overall, the revision towards my previous review is not clear and the arguments provided in the responses are not supported by sufficient discussion or experimental results. I would still keep my score to be 3. 

==================================================

1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. 
2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. 
Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? 
2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? 
3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? 

==================================================

- There is unfortunately not a whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered a hyperparameter. While I have no problem with what is currently in the paper, I am just not sure that this is enough to form a long paper proposing a new method. I think if this paper couched itself as more of a 'analysis/experimental' type of paper, expanding its analysis even further (and maybe expanding its scope to other tasks besides just abstractive summarization), it could be solid contribution. Unfortunately, the paper is presented more as a 'methods' paper, with the new method being proposed simply being a change in hyperparameters. 
- What hypothesis test are you using for computing the significance in table 2 and 3?
- Are there other tasks where temperature smoothing could be beneficial? 

==================================================

From Table 6, the difference of attention pattern results in a great difference in novel-n-grams ratios and length in the generated teacher summarization, but this does not necessarily translate to a better student summarization system in Table 2, further explanation or better ablation studies might be needed to understand how the length, novel-n-grams ratio, the different dataset will affect the distillation effects.  Given that this paper focuses on sequence distillation, wondering how the beam size, length penalty of the teacher model will affect the sequence distillation results? 
It may be good to know if a better teacher model (BART-large, PEGASUS-large) produces better pseudo-labels for summarization distillation. 

==================================================

The problem caused by difference in training data and test data has been studied, although they might not focus on "temporal" aspect. 
There are a number of papers, where terms like "covariate shift", labeling adaptation, and instance adaptation (e.g., Jiang and Zhai (2007) and Shimodaira (2000). See below). It would be interesting to see the connection between the arguments of the current paper and such papers.
- Jiang and Zhai. Instance Weighting for Domain Adaptation in NLP. ACL. 2007.
- Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.
TD score is defined in Section 2.3, but there is no intuition or justification about why it is defined like this, at least in the same section. Descriptions about intuition can be found in the later sections, so this is simply a matter of organization of the paper.  It is not clear whether TD scores for different performance metrics can be compared with each other, across different tasks. There should be a discussion on the justification (or maybe limitation).
Some specific examples of temporal misalignment would help readers understand the paper, if they (the examples) are presented in Introduction. 
D in the text body should be in italic, consistently. 

==================================================

- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. 
If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. 
- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? 

==================================================

I want to see a baseline that tests spelling bee on representations specifically optimized for morphology or spelling, so that we can see what the performance of this probe would be on complete information about spelling. As is, it's not clear what a true upper-bound performance would be for such a probe.
 I felt that the conclusions drawn from the attempt to train with additional spelling information were not well justified. After all, if training with the additional information actually damaged performance, the authors would not have concluded that the model doesn’t use the information or that the information was somehow harmful or misleading. Instead, they would rightly assume that the particular procedure they were using to add that information was not providing it in a structure that the model could easily use. However, because it doesn’t change  performance at all, the authors conclude that the model is able to acquire everything it needs without direct observation of the spelling. It’s not clear to me that these results contradict the idea that some information about character composition might be able to help a model.
There needs to be more detail on the implementation of spelling bee. 
I felt that this paper could do with more citations to the existing literature on morphological/character composition in neural models (e.g., https://aclanthology.org/P17-1184/) 

==================================================

- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses. 
- One thing I wasn't sure I understood is whether the "smoothed" inputs are used on their own to train the model, or if they're used in addition to the standard inputs. Lines 236-239 make me think they're used in addition. This is fine, but should be emphasized more explicitly.
- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not. That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose? 

==================================================

- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector. 
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data? 

==================================================

This paper does not make a significant contribution. Smoothed Representation and Mixup Strategy are not proposed by the authors. Moreover, the strategy is too simple to combine the one-hot representation and smoothed representation with a weight parameter lambda.
Three low-resource sentence classification tasks are used in the experiments. Even more lower-resource task classifications are missed, such as MRPC in GLUE.  It lacks the configuration of lambda, I believe this can greatly affect performance.
I am still unsure about the motivation, the high probability of "average" is only due to MLM. Nevertheless, the semantic meaning of "average" is learned by MLM and a downstream task like sentiment analysis would only care about its semantic meaning. 
One of the biggest problems is that motivation is not convincing. I think it would be better to have examples of different predictions using your text smoothing approach. In Figure 2, how does the "average" likely affect the sentiment analysis label?
Though some of the tasks aren't very useful, I believe more classification tasks should be conducted. In my experience of GLUE, SST and MNLI are two of the most resourceful sentence classification tasks compared to other tasks in GLUE. 

==================================================

__1. Lack of significance test:__      I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results. 
     __2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)? 
     __4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result? 
     __4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample? 
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation 

==================================================

- The MCTS algorithm itself in the context of text generation is not explained very well, which makes it hard to understand the proposed algorithm - The need for a discriminator for any possible constraint can be quite limiting to the applicability of the algorithm - There lacks a formal runtime analysis. Speed during decoding is often quite important and it's unclear what the extent of the computational drawbacks of this algorithm are - There have been a number of recent works also using MCTS for language generation. I am not very familiar with these works so it is hard for me to tell how novel this paper is in context 
- I don’t quite see where p(x|c) fits into equation 1. Is it just s_i? This could be made more clear - In terms of plug and play controlled generation methods that do not require fine-tuning of an LM, a citation is missing for Pascual et. al. 2021. Schick et. al 2021 also use a discriminator at each time step to control generation for certain attributes - Line 467: the standard definition of perplexity used in language modeling divides by the number of tokens, and so is not dependent on length - Line 228: token -> tokens 

==================================================

What specific problem that previous approaches cannot solve but this paper can resolve? Or in terms of what aspect, discriminator-guided decoding can do better (not just score)? For example, every constraint is modeled as one discriminator, will it mean the model is better modularized? 
- could you give a detailed example of what "constraint" is used in this paper? It is not clear how multiple discriminators can enforce constraints simultaneously.
- MCTS is mainly used in RL since the state transition and reward function are probability distributions. It is unclear why you need MCTS to do a search, why not just depth-first search, best-first search, A* search, or any heuristic search? Could you highlight the necessary steps that can achieve discriminator-guided decoding?
- Do you assume you can naturally have every constraint as well as the corresponding discriminator?
- you can put all the HTTPS links as footnotes.
- Line 513 "Go of RAM" ->  "GB of RAM" 

==================================================

1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. 
2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. 
5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.
Reference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 
the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. 
Line 300-304: q and d are confusing 

==================================================

The authors do not give any explanation or definition of the evaluation metric used. 
The results are presented really briefly, without discussion on the differences observed for the 18 english zero-shot evaluation datasets (while it is one of the interest of the BEIR benchmark). 
The authors should spend some time on analyzing the results obtained for the different datasets. 
For example, why the results for LaPraDor (unsupervised and FT) are lower than Late interaction for some datasets (FEVER or CQADupStack) while they are better for others ? Why the results are almost the same for Re-ranking, LaPraDoR unsupervised and FT while they are really differents for FEVER for example ? 
On the contrary, I found it really difficult to draw any conclusion from a case study with two examples. 
In the "Carbon footprint" section, the authors mention "All emitted carbon dioxide has already been offset". What do that mean exactly? 

==================================================

1. The main contribution of the proposed approach lacks novelty. Since BM25 has strong zero-shot capability, it seems that the combination of the BM25 serves mainly to enhance the zero-shot capability of the model. The proposed ICoL is mainly an integration of some existing methods. 
2. The authors do not illustrate why it is better to keep representations in the same hidden space, it also has not been experimentally verified that they are in the same hidden space. 
3. The link between the ICoL and BM25 weighting is not fully worked out. 
1. In line 300-301 "{ d i,1−, . . . , d i,n−} is a set of randomly sampled query negatives", query->document? 

==================================================

This paper does not have any glaring concerns in my opinion.
Weaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?
- The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. 
Missing reference for "First there is no existing data that can be used to quantify how much private information is revealed by a LM". Maybe rephrase as, "no standalone dataset to evaluate privacy of training data in a LM"? 
Carlini, Nicholas, et al. "Extracting training data from large language models." 30th USENIX Security Symposium (USENIX Security 21). 2021.
Explain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words.
Is this an error? 
"Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7." 
Should this be "the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3."?
KL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N. 

==================================================

- The structure of the whole paper is not grasped at a glance due to the presentation being less clear. 
Suggestions - This reviewer thinks that the indicator for adversary should be 'it' rather than he (line 374, 448).
- In the usage of cases 'LM+KL+MI', 'LM+MI', and 'LM+KL', LM is commonly placed in, which means the authors can abbreviate it by using only KL and MI. It would make the whole passage more readable.
- In Section 4, it would be better if some paragraphs are split, and the meaning of results would be better presented if organized into a single table with the interpretation.
Typos - Please correct citation format to inline, such as in line 262.
- There are many usages of 'casual' language modeling. Think this is a typo. 

==================================================

The amount of material (in particular, mathematical proofs) presented goes beoynd an 8-page *ACL paper, in my opinion.
The method is defined as "semanti-driven", but I think this is very misleading: the approach doesn't directly deal with semantics, but rather with word labels. If the approach was truly semantic, the model wouldn't be able to distinguish between complete synonyms.
I do not have enough expertise to evaluate the novelty of the actual model presented. 
- Line 100: why is this a class of neural networks, and not a neural network?
- The conclusion is extremely short, the paper could benefit from some discussion.
- Line 476: typo, compare our models _to_ - Line 510: Figure 8 is mentioned, but the text doesn't say this is actually in the Appendix.
- Table 3 is labelled as Figure 3. As a result, Figure 4 should be Figure 3, etc.
- Line 431: I couldn't understand which n-grams were excluded: unigrams, bigrams, but also bigrams+trigrams? What exactly is the latter type? 

==================================================

Please take a look at some questions on the equations and confusing phonemes below. 
\mathbb{Q}_K is defined at L894 as a class of functions, while Q_K is the K-th code distribution in the codebook. \mathbb{Q}_K^NN is defined at L899. \mathbb{Q}_K^NN must be a subclass of \mathbb{Q}_K, because the definition of \mathbb{Q}_K does not have any requirement on the functions q. (Question 1) Should \hat{\theta} and \hat{q} of the left hand of Eq. (14) at L14 be \hat{\theta}_n and \hat{q}_n? Should L905 be corrected accordingly?
(Question 2) In L908, Q_K and Q_K^NN are mentioned. But, I suppose they should be \mathbb{Q}_K and \mathbb{Q}_K^NN instead. Should L910 be corrected accordingly?
(Question 3) What is q_k in L911? I suppose it should be Q_k instead. But, even in that case, I could not follow the transformation from L910 to L911. Please clarify the transformation.
I also have questions about confusing phonemes.  (Question 4)  What is the precise definition of “error probability” of (10a)? While the confusion between nasals /n/ and /m/ is mentioned in the main part of the paper, there seem other pairs with even higher error probabilities. Among them, the pair of /ch/ and /ah/ looks very different from each other. Why do they have such high error probabilities? 

==================================================

1) If I understand correctly there is a need to know the word and phoneme segment boundaries for this task. This is a pretty strong assumption and can be unreliable for many languages. The experimentation done by the authors use both ground truth and provided segmentation which I think is good to show that the technique works even with a segmental model. But the authors should rephrase term "mild assumption". 
2) Details about the model training and dataset is missing. Which will make this work accessible to a smaller set of research community. It would be great if the authors can provide code or additional details about the model. 
1) Regarding the related works -- "there is a long line of work that use supervised, multilingual systems" -- it would be good to acknowledge some of the older works too. 
2) Following up on that, there are works that recognize articulatory features, or directly predict phones -- mentioning some of those works would also be useful. 
3) For the results in 5b, it would be good to add some models from the above work for comparison. As different communities would be interested in different aspects of this paper. 
4) There is a recent work on unsupervised speech recognition at Neurips 2021 (https://arxiv.org/pdf/2105.11084.pdf) which does something similar but without the need for segmental acoustic models. It would be good to make a contrast or have a discussion about that for the readers to have a better understanding. 

==================================================

The methodology section could be strengthened by further justification of some of the design decisions; for example, what models were considered in addition to T5 (line 148-152).  Were there other tasks that were considered beyond the three examined (line 153-154).
The evaluation section could benefit from further motivation for the three experiments shown; were other considered? Were there other datasets that were considered? 
How did you determine what questions you were going to ask the annotators? How is it related to what has been done in related research?
What was the process for identifying annotators? Why only three annotators?  Glad to see that the "full annotation data" will be available, and I'm assuming that any other required data will also be available to other researchers as well. 

==================================================

1. First of all, compared with other excellent papers, this paper is slightly less innovative. 
2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. 
3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. 
4. It lacks case study to show which document-level translation errors are improved by the proposed method. 
1. It is suggested to add a structure diagram to illustrate your proposed method. 
2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. 
3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. 

==================================================


- The authors mention using `the latest WMT evaluation sets`, which currently would be from WMT2021, but if/when the paper is published the `latest` would already be different, so it would be good to specify the exact year.
- The paper mentions using both BPE and Sentencepiece - are they used on top of one another or different for each training data set? If so, then why? This part is unclear... - Is the motivation for using 64K token vocabulary for Europarl the larger size of the training data? The IWSLT data set has a far more diverse variety of languages/writing systems/characters, so it would seemingly make more sense use a larger vocabulary for that instead. 

==================================================

1. While the authors mention leaving this to future work, the study of how many groups to divide each layer into is missing and how sensitive is model performance to the choice of number of groups is missing → how does the proposed group dropout compare to just a regular dropout with group size = 1; I think this is important because the paper proposes “Latent Group Dropout” and whether group dropout is needed or not is a key question that is not experimentally verified. 
2. “ heuristic multi-task group dropout” (HMGD) captures most of the improvements between Transformer vs proposed LaMGD with a 0.5 BLEU difference between HMGD and LaMGD - It is worth making a case that the simpler approach HMGD is also a win 3. It is unclear if there was an attempt to tune the sizes and dropout for the adapter modules in the adapters baseline 4. Table 5 - very few pairs (7 out of 32) show significant gains over the Transformer baseline - it is actually unclear how much language specific model capacity helps on this TED benchmark at all. Even the Adapters baseline is very similar to the Transformer baseline. 
1. Regarding “Finally, we set the weight of the entropy term to 0.0001 in the training loss in every experiments.” - ideally, you should refer back to the exact term in the exact equation described in the previous sections 2. I don’t understand what “0^nd” means in section 3.1.4  3. Boldface in Table 3 doesn’t always refer to the best BLEU per column 

==================================================

The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words. I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be). 
- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.
- the discussion of Arabic use of diacritization is not accurate. Arabic "dots" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are.  Check out  https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf - I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger).  Any thoughts on that? 

==================================================

- While the authors have now edited the paper to motivate the syntactic paraphrasing task much better, my comments on a lack of motivation for architectural and modeling choices as well in deriving insights from the results still hold.  - The newly added results for the SGCP baseline look very different from the numbers reported in Kumar et al., 2020. The metrics reported in this paper for SGCP are significantly worse than even the copy baselines which makes me a bit doubtful about validity of the results reproduced by the authors.
Overall, through the revision the authors seem to have partially addressed my major issues with the first version but a significant portion is still unaddressed as pointed out above. I request the authors to try to rectify these in their final submission, especially the second point on the performance metrics for the SGCP baseline. 
1. In section 4.1 how is template encoder different from parse-tree encoder? Templates are also constituency parse trees right? 
2. It would have been useful if in Table 5, examples of the templates would have also been given. 

==================================================

Though the paper is detailed, I recommend the following areas to be addressed (a) Retrieval of the similar templates: The authors should clarify on why the query text and the whole query parse is important to retrieve templates? Is  it always the case that given a query, we can retrieve templates other than the own query template? Are all top K retrieved templates meaningful concerning the query or some thresholding on similarity is needed? 
(b) Human evaluation needs some more elaboration. The author should report the inter-annotator agreement along with the results.   (c) Syntactic evaluation: At the time of evaluating TED, did the authors use the top most retrieved template? They should elaborate on the process (d) Table 1: The performance of (Si_SCP) is slightly better than guiG in ParaNMT-small where as for QQP-OS the gains are higher. Is there any specific reasons or observation regarding the same? 
(e) Though the paper is focused on syntax guided paraphrase generation, it would be nice if the authors can discuss Si_SCP’s gain in comparison to unsupervised paraphrase generations [Krishna, Kalpesh, John Wieting, and Mohit Iyyer. " Reformulating Unsupervised Style Transfer as Paraphrase Generation." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.] 
While I can see a detailed work in the paper, I would recommend the authors to address the points mentioned in the weaknesses. 

==================================================

Despite the good performance of the probing classifiers, it is difficult to determine whether PASs generalizations are learned, or whether a kind of separate rote memorization of each predicate take place. 
to ascertain that model learn significant generalizations, researchers tend to employ control tasks. Unfortunately, in this paper, the authors reduced the  procedure to initializing the weights of the language model at random. I do not think this is a principled way to ascertain that the intermediate representation actually encodes the property at hand. The authors should  use selectivity instead. It is computed by subtracting the performance of the probe with re-assigned labels from the reported proxy task performance. 
the authors are asked to refer to Hewitt & Liang, (2019): "Designing and Interpreting Probes with Control Tasks" for more details. 
The paper can be further improved if the authors uses selectivity in their experiments. 

==================================================

- The PGD adversarial training baseline might be a weak baseline as the authors restrict the number of gradient steps to 5. I would recommend to run PGD with more steps, at least for one of the downstream tasks, to provide a stronger baseline. 
- I don't understand what Figure 1 shows. Also, It's not clear to me how the paragraph starting from line 183 connects to the previous discussion. Maybe you can clarify this?
- Which attack was used to compute accuracy under attack in Figure 2? Could you add additional lines showing accuracy under attack against more than one method? This would illustrate if the optimal flood level generalizes across attacks.
- Figure 3 is a bit hard to read. Maybe move MRPC to a separate figure and put it in the appendix?
The authors might be interesting in this concurrent work (https://arxiv.org/abs/2104.04448) from the computer vision community  that studies the relationship between adversarial robustness and flatness. 

==================================================

Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?
2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.   3) Briefly discussing what kinds of data augmentation techniques  ( ♥ )  other works use in Table 4 would be great (and maybe why not used in this paper). 
Line 17: add a space "datasets. We" --> "datasets. We" 

==================================================

-- the contribution is probably not significant outside GEC community 
l.17 -- no space after period l.66 "it will be taxing" -> "it will be difficult" l.260-265 "most of the sentences are considered to contain grammatical errors in the previous annotation, but a considerable part of them are not corrected in our annotation" You can make such comparison only on the same data, thus I compare the number of erroneous sentences for NLPCC18(Orig) and MuCGEC(NLPCC18), but the number of correct sentences has increased only by about 80. May be it is more convincing to provide the average number of errors per sentence?
l.266-267 -> NLPCC18 has the shortest sentences, whereas CGED sentences are much longer.
l.267-270 This is possibly because, since HSK is an official Chinese proficiency test, candidates tend to use long sentences to show their ability in Chinese use -> ...because of the fact that HSK is ..., so candidates l.471 -472 I am sure whether I understood the terminology: do you call redundant errors the cases when the text contains more characters than its correction. May be, it is better to call them insertion and deletion errors? 

==================================================

- **Inefficient design of target segmentation**: If I understand correctly, all input segments are assigned a target segment. This probably causes unnecessary summarization of irrelevant input segments - if the text is massive and the gold summary is very short, why not ignore most of the input segments and only select sufficient input segments to cover the targets? Also, this allows targets to be duplicated over different input segments.  - **Missing analysis of model behavior**: It would be useful and interesting to know what exactly is happening at each stage, e.g. how much text is produced at each stage, and how noisy or detailed intermediate summaries look like.
- **Missing analysis of empirical running time** 
**Suggestions** - Line 249: “Huge time costs” sounds very informal, maybe change to “considerable running time” or similar - Please indicate if f-score, precision or recall is used when ROUGE is mentioned - Follow-up on weakness 1): ignoring input segments could be implemented by assigning “empty” targets, e.g. using a single special token that when predicted would trigger a segment to be discarded in the next step.
**Questions** - Table 9 in Appendix: what does “keyword emerged in gold summary” mean?
- “Separate model for each stage” is mentioned, but I am still not sure whether this means 2 (fine and coarse) or N models?
- Does the number of input segments at each coarse stage always stay the same as it was in stage 1, before reaching the fine-grained stage? 

==================================================

The only thing I can complain about is the rather general framing of the dataset in the introduction. To me, it seems that "contextual descriptions" is a bit too vague and generic as a term for describing the contribution. The contexts explored in this work are very specific distractor-like types of contexts where an image appears in combination with extremely similar images. This is close to the classical referring expression setting that investigates descriptions of objects in the context of similar distractor objects. The resulting expressions may be called discriminative descriptions/unambiguous references or something along these lines. 
My suggestion would be to mention the connection to referring expressions somewhere in the paper and to think about refining the intro/notion of context a bit.
Yu, Licheng, et al. "Modeling context in referring expressions." European Conference on Computer Vision. Springer, Cham, 2016. 
Mao, Junhua, et al. "Generation and comprehension of unambiguous object descriptions." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. 

==================================================

In my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.
I had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results   directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these   subsets. 
Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. 

==================================================

1. The authors claim that this is a novel formulation of the NLI task but cite a prior paper that formulates this task the same way. The novelty is in the generative implementation and the scope of the claim should be reduced. 
2. I might have missed this, but the actual implementation of how p(y|B) is set to a uniform distribution during inference is not described. I would be happy to retract this point if the other reviewers saw this described somewhere. Additionally, how is training with a uniform prior actually implemented (Table 3)? 
3. One experiment I would like to see (and is missing at the moment) is a performance comparison of all models on challenge sets such as ANLI since they were constructed specifically to weed out models that perform well on the original test set using spurious features. 
4. Is there a reason for using only BERT over RoBERTa since the latter is generally a stronger baseline? Could a model pretrained on a larger and more diverse dataset be less prone to structural bias? 
5. The authors do not state if the code will be released. 
L50: "Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems.": Such as?
L291: Is BERT trained with forward masks when using the autoregressive loss? 

==================================================

- Based on the empirical analyses (Table 2), it is not clear if pure worst-case-aware sampling ($\phi=0$) is consistently better than pure loss-proportional sampling ($\phi=1$). It seems the results on mBERT and XLM-R give different conclusions. The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.
- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental. 
- Line 183: it would be good to show what languages are used in the PLM pretraining and if there is a difference of the proposed method between the seen and unseen languages.
- Table 2: there are only three values of $\phi$ listed, and there are no difference between $\phi=0.5$ and $\phi=1$. It would be good to show more details on how $\phi$ affects the final performance.
- Table 3: which PLM is used in this experiment? What is the $\phi$? 

==================================================

1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper. 
2. The results in the analysis with varying homogeneity of training samples (Section 4) are conflicting: they don't support the hypothesis that this method improves more when some language types are underrepresented in the training samples. Therefore, it's unclear why this method improves over baseline methods, i.e. does it help the model learn from skewed training samples or is it something else? 
3. I'd expect a discussion of how this method relates to and differs from existing methods on zero-shot cross-lingua transfer. For example, meta-learning methods optimize the initial parameters [1] or optimization algorithms [2] during fine-tuning toward better performance on outlier/unseen languages.
[1] Zero-Shot Cross-Lingual Transfer with Meta Learning. Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein. 2020.
[2] Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. Weijia Xu, Batool Haider, Jason Krone, Saab Mansour. 2021. 
For the analysis with varying homogeneity of training samples, would it help to look at performance on the romance and the outlier languages separately? Also, it might also be helpful to consider the typological distance between different language genera. 

==================================================

Although the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR). 
1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.
2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.  3. The paper does not refer to the table 3 when discussing the results in section 4. 

==================================================

The implications discussed in the paper apply *if* one were to try to project continuous prompts to discrete space using GPT-2's (or any other) embedding matrix, but, as far as I am aware, no one has attempted to interpret continuous prompts in this way. The paper could be strengthened by more explicitly discussing *why* we should be concerned with this particular method of prompt interpretation: 1. Do the authors think we should be especially concerned about nearest-neighbor, embedding matrix projections because that's how language model outputs and word2vec are calculated? This seems to be the case, but could be made more explicit. It's not clear why the use of an embedding matrix at the output layer of language models implies this would be a popular way of discretizing continuous prompts. 
2. Is it because the particular discrete projection studied is the simplest that the authors believe it might be used in the future? 
3. Do the authors think their hypothesis generalizes to other methods of discretely interpreting prompts? If so, that could also be made more explicit. 
1. Space permitting, it may be beneficial to move the finding of Appendix B to the main paper as I found this interesting. 
2. Line 246 is difficult to parse. Possibly split this into two sentences. 

==================================================

The idea of using boundary information is not completely innovative. For example, the idea of effectively using boundary information has been proposed in Shen et al.(2021).
The experiments don’t prove the claims. Firstly, the paper lacks adequate verification for the proposed idea, and evaluated it with only one baseline. Secondly, the paper used RoBERTa while the previous methods used BERT. Thirdly, Yu et al. (2020) conducted the experiments on GENIA, but the paper doesn’t. 
The paper lacks adequate verification for the idea. The paper claims that the boundary smoothing can be easily integrated into any span-based neural NER systems, but the paper only integrated it into the Yu et al. (2020). The paper should further evaluate the boundary smoothing idea in several SOTA approaches.
In addition, the idea of effectively using boundary information has been proposed in Shen et al.(2021), which constructs soft examples for partially matched spans, and trains a boundary regressor with boundary-level Smooth L1 loss.
In the experiments, I have some concerns: (1)	The paper uses RoBERTa for the experiments, while the baselines used BERT. It may be unfair. 
(2)	Why the Baseline outperforms Yu et al. (2020) on ACE 04 and ACE 05 greatly, but has comparable performance on OntoNotes 5 and CoNLL 2003? Yu et al. (2020) also conducted experiments on the GENIA dataset, but the paper doesn’t report the results on this dataset. 
(3)	According to the Table 4, the improvement of BS against LS is marginal.  Some statements are not very accurate. For example, Line 313, the submission says “this work is among the first to introduce a span-based approach to Chinese NER tasks and establish SOTA results.”, however, Shen et al. 2021 proposed a span-based approach previously, and conducted the NER experiment on Chinese Weibo dataset. 

==================================================

1. The problem formulation is flawed. The authors argue that one major motivation of this paper is to learn schema in a data-driven way other than laborious manual schema engineering. However, on the proposed four datasets, I feel that the schemas are somehow easy to learn. For example, on E2E, WikiTableText and WikiBio, only two columns are involved. According to my understanding, there is no discrepancy between training and testing w.r.t. the table schema. Things on the fourth dataset, Rotowire, feel more complicated. There are two tables, containing around 5 and 9 columns respectively. It's not clear whether a difference exists between training and testing. And the evaluation metric only considers the cell values while ignoring the schema. That's another reason I feel the schema is fairly easy to learn in the proposed setting. Furthermore, under the current setting, this problem sounds more like another type of machine reading that the model is asked to fill in the table as the schema is easy to predict. So I highly suggest considering schema generalization where the schemas differ between training and testing and additionally evaluating schema prediction. 
2. It's not surprising that the major performance contribution comes from the pretrained language model (BART). But compared to that, the gain obtained from the proposed method is marginal (Table 5). 
1. In Section 3 (line 247-252), I am wondering tables are divided into three types. For me, one type (the column header) should work. 
2. Several typos exist in the paper. 
3. Like I suggest in the weaknesses, more exploration of schema learning and generalization will make this paper much stronger and more interesting. Another workaround is to rephrase the motivation to make this paper focus more on the non-header part. 

==================================================

1. The paper need further polish to make readers easy to follow.
2. In Table 6, the improvement of method is marginal and unstable.
3. The motivation of this new task is not strong enough to convince the reader.  Is it a necessary intermediate task for document summarization and text mining (as stated in L261)?
4. It directly reverse the table-to-text settings then conducts the experiments on four existing table-to-text datasets. More analysis of the involved datasets is required, such as the number of output tables, the size/schema of the output tables. 
Questions: 1. L68-L70, Is there any further explanation of the statement "the schemas for extraction are implicitly included in the training data"?
2. How to generate the table content that not shown in the text?
3. Why not merge Table 1 and Table 2? They are both about the statistics of datasets used in experiments.
4. What’s the relation between text-to-table task and vanilla summarization task?
5. How to determine the number of output table(s)? Appendex. C don't provide an answer about this.
6. What’s the version of BART in Table3 and Table 4?
Suggestions: 1. The font size of Figure 2 and Figure 3 is too small.
Typos: 1. L237: Text-to-table -> text-to-table 2. L432: "No baseline can be applies to all four datasets" is confusing.
3. Table 3: lOur method -> Our method 

==================================================

1. This paper proposed three different types of data augmentation methods. Both hard negative and semi-hard negatives focus on replacing the entity mentions. However, it is not well-explained why this can help to cluster the relational expression. Does this lead to a model which focuses on entity mention detection rather than relation patterns? From my understanding, replacing entity mentions of the same context should not largely change the relation semantics.
2. I think back translation plays a key role in this paper. I think it is necessary to add an ablation study to exclude the hard and semi-hard negatives. In particular, how does the model perform when both formula 1 and 2 only include t and t^p and use in-batch negatives (similar to contrastive learning). 
1. It is not clear to me why hard negatives and semi-hard negatives are generated from the positive sentence rather than the original sentence. Wouldn't there exist error propagation if the back translation is noisy? 
2. It seems that T-REx is a distantly supervised dataset with no human labels. am I correct? If so, is it possible to evaluate over annotated dataset? ( For example TACRED or FewRel?) 

==================================================

1. The paper doesn't show how tertiary claim classes and visual claim classes can help in fake news detection.  So it leads to the question, why do we care the proposed label classes in claim detection? 
2. The annotated dataset size is limited for the use of largely used heavy multi-modal models. And while claims are related to 3 topics, they are still limited. 
3. Experiments include fine-tuning on ALBEF but not fine-tuning on CLIP. 
4. The dataset has conflicts in annotated data. Why not adding more annotators? 
1. Try to show the value of the dataset -- to show the use of tertiary claim classes and visual claim classes can actually help in fake news detection performance. Also, it will be interesting to see whether using the classification results on visual claim classes can help the classification performance on tertiary claim classes. 
2. Increase the number of annotators each data to avoid conflicts. If possible, annotate more data. 
3. In experiment, it can also do fine-tuning on CLIP. 

==================================================

- Less of a weakness, but I do slightly disagree with the authors' focus on instance-level rather than system-level correlation (see below) 
In Section 2.4, the authors argue that metrics should be ranked in terms of instance-level judgments. While I acknowledge the clear advantages of this, particularly related to sample size, I would encourage the authors to also include system-level scores in the leaderboard. Ultimately, in the other half of the leaderboard, metrics will be used to make system-level comparisons. Even if there is high instance-level correlation, there could be different patterns at the system-level, for example if a metric has a small but consistent bias for or against a particular system's outputs. I wonder if perhaps there is some sort of compromise way to rank metrics that incorporates both instance-level and system-level scores. 

==================================================

- Several important aspects of the approach are underspecified: How are the meaning representations segmented into individual facts (Section 2.1)?  What are the inputs/outputs to each stage of the system (Sections 2.2 to 2.4)?  How are the lexicalization, aggregation and post-editing tasks performed (e.g., is this just a T5_small model)?
- The development and exact form of the loss function (Eq 1) is unclear.
- The label inference process seems spurious (Section 2.6).  Some examples and statistics would be helpful. 
The work seems to be presented as an engineering task and provides limited insight into the fundaments of how decomposing the task help language models to consume meaning representations to generate text.  In particular, exploring the potential relationship between lexicalization and missing slot error would have made for a much stronger paper.  Also the level of specificity in Section 2 obscured the exact nature of the approach.
The terms s_x and v_x are not clearly defined.  Are these slots and value bindings, respectively?  If so, how are is each derived? 

==================================================

1. ** Poor efficiency**. The proposed method contains three different modules, each containing an unshared PLM so that the amount of parameters is three times that of the End2End method. As T5 is good at multi-task learning, I think the author should consider how to improve the performance of the model in the case of parameter sharing, rather than simply stacking multiple models. 
2. ** Time-consuming**. The proposed method requires recurrent generation so that the speed of response generation will be significantly reduced, especially when the amount of attribute-value pairs is relatively large. It should be noted that the NLG model is only a part of the dialogue system of the pipeline structure, and time efficiency also needs to be taken into consideration. 
3. ** Lack of strong baselines**. The baseline for comparison is only E2E-T5, which is a naive E2E-NLG method and may not be strong enough. Therefore, the improvement of the proposed method over a single T5 model(E2E-T5) may not be a very valuable conclusion.  I think the author should investigate and compare more few-shot NLG methods to enhance the persuasiveness of the experimental results. 
4. Lack of some additional insight into the effectiveness analysis for three modules. 
5. The innovations in this paper are more like tricks for improving metrics. 
**Suggestions**: See above.
**Questions**: In table 1, The MER indicators of +aggregation and +post-edit are both higher than 1.1, while the MER of +selection is very low. From Line.402-Line.405, the selection is selecting the one with lower MER from aggregation and post-edit. Why the best MER result is reduced to 0.14 after selection 

==================================================

The main concern to me is about the theoretical analysis, which seems too brief to reveal the intrinsic mechanism of the proposed methods, details are listed as follows.
- The corollary in line 245 assumes that $\mathbf{w}^*_{i}$ $(1 \leq i \leq n)$ are drawn i.i.d. from a distribution on $[-1, 1]$. Is this. i.i.d. assumption holds for the proposed networks? Besides, I not that the range of $\mathbf{W}$, $(-0.5, 0.5)$ is not equal to that of $\mathbf{w}^*$'s. Could this difference effect that derivations?
-  In the proposed method, the authors claim that $N$ sparse student modules have probabilities of $p_1, p_2, p_3, ..., p_N$ to substitute the corresponding teacher layers separately. But these probabilities have not been considered in the theoretical derivations. I guess these can affect the results.
- The nonlinear transformations in the neural works, such as $exp$ and $tanh$ have not been considered in the theoretical derivations. To my understanding, these are non-trivial results and should not be omitted to obtain the Eq. (10) directly.
Another concern is why the reported results are all on the dev set but not on the test set, which is uncommon or maybe I have missed some thing. 
---------Suggestions---------   1. The detailed derivations of corollary and the derivations of Eq. (10) are important, which should be displayed in detail in the appendix or on the main pages.  2.  The Figures and Tables can be moved on the top for better readability.  3. Significance test can be conducted for the results of Tab. 1-3.
----------Typos----------   - line 249: $\mathbf{W}$ --> $\mathbf{w}$. - Eq.(8) $c$ has not been defined.
- line 269: missing the right $|$. - The title of Tab. 3 is the same as that of Tab. 1. 

==================================================

1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets. 
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system. 
3. ( minor) It is unclear how the authors arrived at the different components of the "scoring function," nor is it clear how they arrived at the different threshold values/ranges. 
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content. 
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.   2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)? 
3. It will be nice to see some examples of the system on actual texts (vs. other components & models). 
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well. 

==================================================

While the results are promising, the innovation of this paper is somewhat limited as the approaches are quite well known.   The paper indicates that only gold cue information is used for scope resolution (I. 219). Now, the question is what happens if you use predicted cues? Is it the case that a few wrong cues can give devastating results on focus detection?  More details should be there for the approach "negation-focused data collection" approach. It seems additional cues are collected only from a biomedical domain. Therefore, the model AugNB can be biased to perform better on the biomedical datasets only. 
Further, there should be more information on CueNB model as well. How is the masking objective utilized on top of AugNB? 
Numbers in Tables 2 and 3 look more crowded. I would round to one decimal point in these tables. 

==================================================

The paper has incremental or limited novelty as the question generation from passages using BART is already a known, Bi-encoder for QA already exists. 
Contributions of the paper are quite trivial. 
Bi-encoder+Unsupervised QA didn't perfrom great. So whether choosing QA as the pre-training is helping QA task or not is not very clear. 
what is the motivation behind independently encoding questions and passage is not very clear. isn't that a shared common encoder would help the model learning better question tokens to passage token matching? 
For QA task, It seems that the cross-encoder + MRQA is doing better than proposed QUIP. 
Highlight the best numbers in table 7.  - Have a strategy to filter out best quality question from the set of synthetically generated questions.
- May be design a curriculum learning paradigm for pre-training so that the batch sampling is done in a manner that helps the pre-training learn from easy to complex questions. 

==================================================

1. Although the translators may be highly professional, different translators usually have different choices, so producing annotations by using 1 translator per language does not seem enough. 
2. Also no annotation training details and guidelines are shared, and this is important as professional translators usually have issues with such kind of non-standard tasks. No information about the tool used. 
3. It would have been good to have the number of items per language pair when providing the statistics of the annotated dataset. 
The example on the first page sounds like figurative use of the word "to march". This is an important aspect of mismatched translations. 

==================================================

1)	The data labeling methods transform the prediction score in a ranked way. However, different evaluation approaches may produce different order. In this paper, only one evaluation approaches are used to generate the prediction score. 
2)	Writing should be improved as the paper is not easy to follow. 
1)	As an abbreviation, PLM in line 67 should be defined first 2)	The authors should give some intuitive case about hypothesis, source and reference in experiments 3)	The introduction of Hard MRA need improvement. I thought the monotonic means the sequence h->s->r. Because h->s is not allowed, the hard MRA forbids the h->s, s->r, and h->r 4)	Line081, learning-xbased should be learning-based 

==================================================

The paper is very dense, with a lot of theoretical material, may be more suitable for a ML conference or a journal rather than *ACL conference. Having said that, I lack expertise in ML and may not have a good feel for where this kind of studies belong. 
- Line 036 - "probabilities of becoming the next word" reads awkward to me - Line 046 - should be "man" instead of "king"?
- Line 166 - typo, "thoery" - Line 214 - theorem 2 could benefit from providing an intuitive explanation - Line 280 - typo, "there existS" 

==================================================

The paper really struggles to find a case where the proposed approach is better than the regular MoS. The improvement on natural language seems to be small, and the paper needs to work on a synthetic language to show the gap between MoS and the proposed approach. The paper then looks into a QA task where the uncertainty is high to showcase the benefits of the proposed approach, but the gain seems to be small. Either natural language does not exhibit the variability that fits the theory, or the problem of modeling is not a lack of uncertainty in the output distribution but requires something else.
The use of the term bimodal suddenly appears in section 5, and the use of the terms hints that it is the multimodality of the distribution that causes the problem. Indeed, this has been pointed out in Chris Bishop's seminal paper, Mixture Density Networks. However, the connection between multimodality problem and the theory in the paper is missing. It would be much better to draw the connection in section 2, and signpost a little in section 5.
There are minor presentation issues. For example, it is not until section 2.2 did I start to understand why the problem stated in the introduction exists. it would be much better to explain the problem in words in the introduction. As another example, the figure contains too many colors, and the colors don't particularly mean anything and are a bit distracting. 
I would talk about theorem 1 informally in the introduction. I would also remove unnecessary colors in the figures.
Theorem 2 reads a bit abstract, so it would be much better to relate the theorem with the running, queen-king-woman-man example.
I also think that the argument about the limitation of MoS, in the second paragraph of section 3.2, is a little dense. It might be better to add more redundancy to that paragraph, using different ways to explain the same concept. 

==================================================

- Generally, the evaluation is rather "narrow" -- it compares only with variations of the same model (either full cross or split at the same level). It may be helpful to include more baselines to put these results in more context. The baselines presented, however, are rather strong -- with DistilBERT achieving 0.390 MRR@10 on MS MARCO (compared to Nogueria & Cho's 0.365 using BERT-large). 
 - The evaluation on CAR appears to have been conducted using "automatic" labels (based on the reported number of positive passages per query), even though human-provided labels are available. In general, further details about which version of CAR was used should be provided. 
 - Equivalent performance is determined statistically via the failure of a one-sided t-test. Equivalence testing (e.g., TOST) could be used instead to strengthen the statistical claims. 
 - It would be nice to have seen this work on other base models-- particularly a T5-based model. 
- This work may feel more "at home" at an IR conference -- I'm not sure how interesting this will be to a broader *CL audience. That being said, I like this work and would be happy to see it presented at an ARR venue as well. 
 - Can this approach be used with the ColBERT model, where the ranking scores are based only on the similarities within the last layer? 

==================================================

We can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. 
If you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. 

==================================================

I found several minor problems and uncertainties in the previous version of the paper, but the authors managed to address practically all of these in their revised version. My only remaining reservation thus is towards the claimed but not demonstrated language-agnosticity of the presented approach, which I find to be too strong a claim (or maybe I have a different understanding of what "language agnostic" means). 
In their response to the previous reviews, the authors list the following improvement: "We describe the Twitter data acquisition and cleanup process.", but I have not found this improvement in the current version (but I admit I might have simply overlooked it; all I am saying is I did not find it at the places where I would expect it). 

==================================================

1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. 

==================================================

The paper organization could be improved. I understand that there should be details about the creation, evaluation and analysis of CORWA. With the space limit, some evaluation tables are moved to supplementary material. Table 6 might be more important but it is not in the main body. It is possible to make Figure 2,6 smaller to save some space. 
What is the goal for section 6? This section solves the task of full related work generation, but hard to understand how this is related to the main work.
Table 1: the width of the table borderline seems different in each row. And this table has a different style/format with other tables. 

==================================================

The empirical evaluations in this work are difficult to interpret, and I am not sure what the key takeaways are. The metrics used in this work (AUC score, recall at K) are non-standard for the tasks under consideration, so it is difficult to interpret the results in the context of word-level QE evaluation. Although it is interesting that sentence-level QE models contain some information about probable word-level translation errors, it is unsurprising that sentence level models would learn to focus on target tokens that are likely to be wrong and I think the paper at least needs more analysis or insight into the details of what is going on to be useful for other researchers in the context of QE (see comments for some ideas).
As presented, the work cannot be framed as a new approach to word-level QE because it does not use the same evaluation metrics, and the approach cannot label tokens in a MT hypothesis as Good/Bad, so any comparison with supervised word-level QE approaches is not informative. 
Possible Analyses / experiments: - what types of errors do the feature attribution models find/miss, what do these models tell us about what sentence-level models are learning?  - what are the dynamics between source and target token attribution? do some inputs attribute more responsibility to the source, while others attribute more to the target hypotheses? why? ( the notion of an "attribution budget" could be useful here) - do feature attribution approaches completely miss some types of word-level errors? does this show that sentence level models aren't learning that information? or does it indicate a problem with the data generation method of the word-level QE task? 

==================================================

It would be great if the code is indeed released if the paper is accepted. This would help a lot with the reproducibility of the method proposed in the paper. Other than that, it is unclear how the approach described here compares with the ones proposed for the Explainable Quality Estimation shared task which propose similar approaches. If the paper is accepted it would be good to position this in relation to these approaches. 
- Any idea about the difference in performance between using XLMR-base and XLMR-large? 

==================================================

1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction.  2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement. 
1. In Table 2, the ablation experiment has a setting only add "local Constraint" but without "Refinement Mask". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position. 

==================================================

The main concern is the measure of the inference speed. The authors claimed that "the search complexity of decoding with refinement as consistent as that of the original decoding with beam search" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).
Even with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens/sec for Transformer-base model)?
How do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.
The score will be increased if the authors can address the concern.
[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. 
1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.
2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?
3. Is PPL a commonly-used metric for storytelling? 

==================================================

It is not clear when deletion operation is used in the refinement period. 
1. How to ensure that the consecutive refined BEP tokens can be valid whole words after de-BPE operation? 
2. I suggest show the the effect of the proposed model with beam search during the refinement stage for the translation quality, although with inferior inference speed. 
3. Although achieving an improvement, what are the main advantages of this proposed over some simple and effective methods without additional parameters and speed reduction, such as back-translation and R-drop? 
4. The ratio between the generation and the refinement probabilities in the whole training objective is 1 in the equation 12. Is it the optimal setting? 

==================================================

- Comparison of results with equivalent methods described on related work are missing. For example, post-editing or multiple pass decoders.
- The general idea is well described but several details are not clear in the paper (refer to comments for details). This information is important for understanding and reproducibility, and it is necessary for properly reviewing the work. 
1 . Line 050: A common way to address the issue is using beam search.
2. Lines 076-079 Question: Do you replace a word independent of the context? For example, if you replace w_i, what happens with w_{i-1} and w_{i+1}. This could create incoherent text. I will assume that it is better to replace the whole phrase instead of independent words. 
3. Lines 197-198 Question: How do you calculate the joint probability P(y_1, y_2, ..| y_{<i}, X) if this part is not autoregressive?  I suggest adding the equation to the paper for clarity.
4. Line 197 Question: What it means the “0”? 
5. Lines 185-191 Question: Same question as in (2).  It is not clear if you replace the whole phrase or each word? 
6. Lines 199-207. The decoding algorithm mixing greedy and beam search should be added as pseudocode for clarity. 
7. Equation 12. I assume that P_g and P_r shared the same parameters, so their calculation varies only for the use of different masks. Does this imply making two passes of the decoder? Please clarify these points in the paper. 
8. As far as I understand, the refining mask is used only for training. Is this correct? 

==================================================

1. the authors show that the bias appears only in two out of seven language directions, which pretty much depends on the quality of the MT systems that produced the MT outputs on which the QE systems were trained. It should be clear that for other language pairs or even datasets, other or no de-biasing may be needed. 
2. the ru-en shows biasing towards the source sentence. Although this language direction is included in the results, there are no conclusions related to it or efforts for debiasing of this case. 
3. In section 3.1 there is a lot of space used for a dataset that is already known, unless this refers to newly released data together with this paper (confused a bit). 
4. In section 3.3: what was the process of the manual annotation? How many annotators worked on that, what was their expertise, language fluency. Were there any inter- or intra-annotator statistics? 
Will the authors make the software available in a repository? 

==================================================

1. Given that this work aims to investigate the partial input bias of QE models, more other representative QE models are required to demonstrate that the partial input bias problem is important and the proposed methods are effective. The authors only conduct experiments using one QE model, making the results less convincing. 
2. Figure 1 shows that the partial input bias problem is not severe in some other language pairs, such as RO-EN and ET-EN. The authors did not provide any detailed explanation of this phenomenon, which weakens the motivation to address the partial input bias for QE models. 
1. I suggest to conduct more experiments using more QE models on a wider range of language pairs. 
2. Some examples may make it better to understand why partial input bias exists. 

==================================================

1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. 
Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ? 

==================================================

- The proposed framework is based on the previous template-based information extraction method. 
The template-based method's main drawback is the significantly increased training/inference cost (corresponding to the number of event types). The increase in the category of events (such as MAVEN with 100+ types) will exacerbate the problem.
- For extreme low-resource settings (1%), different data sampling may heavily affect the experimental results. 
It is better to conduct experiments multi-times on different subset samples. 
Some detailed questions about model inference: - Multi-events: Can DEGREE and DEGREE(ED) deal with multiple events of the same type in the same sentence? Although we can predict the same input sentence for different types one-by-one if a sentence contains multiple events of the same type, the same input (template and sentence) will correspond to various outputs. For example, some sentences have two death events. This is not a problem for other generation methods because they are trigger-driven (TANL, BART-GEN), or generate all extracted events in one step (Text2Event).
- Converting generated results to events: Are there some generated results that cannot be parsed into events? Generation methods are more uncontrollable than span extraction and span classification methods for event extraction. For example, the generated span maybe not appear in the input sentence.
There are many template methods for information extraction. 
It is suggested that authors should cite these articles: - Template-Based Named Entity Recognition Using BART. Findings of ACL_IJCNLP 2021 - Reading the Manual: Event Extraction as Definition Comprehension. spnlp@EMNLP 2020 

==================================================

The main weakness is that this method can only be applied to the event types and argument roles defined in the ACE dataset (which are not a lot). There are newer event-type datasets such as MAVEN (https://aclanthology.org/2020.emnlp-main.129/) that include many more events but cannot be used in this framework because they don't include the type definitions that are required for the prompting design.
No statistical analysis of the results is reported although a comparison between models is made. Sentences like "The performance gap becomes more significant for the extremely low-resource situation." should not be written without statistical proof of significance. 
I would suggest trying to come up with templates based on MAVEN types too, this will cover many more event types. 

==================================================

Entropy results are provided using the unigram distribution, which is a very poor language model. 
As far as I can say, this is the first investigation in the NLP literature of various entropy estimators, which makes the draft very valuable.  My main concern is that entropy results are provided limited to the unigram distribution.  It is well-known that unigram distribution is a very poor model for sentence distributions in natural language.  There is no mention in the draft of more powerful language models, such as N-gram (N>1), combined with various smoothing and back-off techniques, why?   Figure 1: it would be nice to see some comments on those local minima in the figure occurring at N=10^5 for English, German and Dutch for both CS and NSB. Do you have any explanation for this? Same thing for HMT, between N=10^2 and N=10^3.  Typos l.166-67: This should should seem l.219: trigramma function > trigamma function Appendix A: sometimes you write p-hat, and sometimes you write p-hat_MLE l.546: truncated sentence?  Appendices C1 and C2:  I think at l.563 the reference should be to Table 3, not Table 4.  And reference to Table 4 should be placed into Appendix C2.  l.569-71: I cannot parse this sentence 

==================================================

1. References Although, the authors claim that this is the first literature, there have been abundant application of entropy estimates on linguistic data. Since Brown's Computational Linguistics paper 1983, some related papers have appeared in ACL milieu, but the main debate appeared elsewhere, especially, discussed in the journal of MDPI Entropy, some were even archived in the book forms. Those include multiple articles about the bias of plugin entropy, therefore, authors are recommended to go through them.  Authors are requested to correctly situate this work within the large history of studies on bias of plug in entropy, which is no more than one kind of work to apply entropy estimate to linguistic data.
2. Soundness of this paper The authors claim at the bottom of p.2, about the plugin estimator,     by Jensen's inequality, this estimator is, in expectation,     a lower bound on the true entropy. 
However, by information theory, any entropy estimator is destined to be no more than an upper bound of the true entropy. In Appendix A, the author draws conclusion by mixing the true entropy and the estimated entropy, where plugin entropy is no more than an estimated entropy. Please see the definition of cross entropy.
3. Experiments This paper only reports the partial and summarized output for each experiment. For example, only the "bests" were mentioned, but authors are recommended to show the concrete performances of each estimator.  To investigate the quality of entropy estimates, much more fundamental analysis are conducted within the previous literature, before going on to concrete application such as described in 4.2 and 4.3.  Such includes tests on other corpora than CELEX, how bias depends on the length of corpora etc. For tips, please see the previous litterature. 
p. 3 first paragraph, "should" replicates just below formula (7). 

==================================================

- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain - Insufficient evaluation:    - only one dataset is used   - lack of detailed dataset statistics   - the comparison is not persuasive 
Although OSD is an interesting topic, the submission should address the following issues before it can be published: - Only one dataset is used for evaluation, is there anything more?
- Is it possible to add more evaluations about the synthetic data itself? In the current paper, the comparison is between the whole pipeline and other baselines. Not sure how much performance is made by the data itself, or the training mode itself?
- Is it possible to generalize the application? For example, from the OSD to general opinion and aspect detection?  - The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?
- The definitions of usefulness and diversity seem quite intuitive. Any motivations to justify this definition, or is there any alternative form of definition? 

==================================================

- The performance of the contrastive-probe method in the main result using MedLAMA was significantly better than other methods (mask average), while the performances using BioLAMA were similar between mask average and the contrastive-probe method. Thus, the authors need to clearly show why the performances in two datasets are different.    - The authors mentioned “2Prompt-based probing approaches such as Auto-Prompt (Shin et al., 2020a), SoftPrompt (Qin and Eisner, 2021), and OptiPrompt (Zhong et al., 2021) need additional labelled data for fine-tuning prompts, but we restrict the scope of our investigation to methods that do not require task data.“ However, it might be better to show the performances of these methods on the MedLAMA dataset, in order to show the validity of the MedLAMA dataset.
- Additionally, it seems to be an unfair comparison because only the contrastive-probe method was additionally pre-trained for the cloze-style task. 
- It would be better that comparison with BioLAMA is placed in the main result, not in the Appendix. 

==================================================

This is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.
My main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels. 
This is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion. 
- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent. 

==================================================

- One major concern is that the contribution of this paper to the QA and NLP community seems mediocre. To me, the difference between FairytaleQA and other reading comprehension datasets on stories and narratives is not obvious. Although the additional annotations about reading skills could be counted as one highlight, this categorization is still coarse-grained to me since it seems nothing more than question types (I assume the categorization might be easily inferred from the question surface form, e.g., "How did ... feel?" belongs to the "Feeling" category).  - Some details of the annotation process are still vague to me. In Line 342, it claims that the "texts were broken down into small sections based on their semantic content by our annotators." The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?
- Another worrying fact is that: from Table 5, we see the performance gap between the QA model and the human is not very big. The BART model fine-tuned on FairytaleQA is 0.533 Rouge and 0.536 F1, which is already around 82% of the human performance (0.651 / 0.644). This suggests that FairytaleQA might not be challenging enough. The possibility is that, with some additional engineering efforts on the QA model, FairytaleQA becomes yet another QA dataset that gets solved within a matter of weeks that does not truly evaluate the phenomena it sets out to. 
Minor Suggestions:  - I think explicit and implicit questions might not be a good naming. Factual detail questions and inductive questions might be more clear?  Putting together all the strengths and weaknesses, I suggest this paper would benefit from another round of revision to address the above concerns before publishing. Otherwise, it might become yet another incremental reading comprehension dataset that might be addressed within a couple of months. 

==================================================

- Human evaluation suggests that the proposed methods underperforms in fluency. This could be due to their use of a GRU as a decoder. Replacing the decoder with a pretrained language model similar to related work should help fluency. 
- The paper now has an ablation study that supports each components efficacy, but it's presentation can be improved. Please present the ablation study results in a table and clearly state how each experiment performed numerically. As it is the comparison is quite vague for most of the configurations.
- The D.2 appendix mentions two human annotators. Based on the author responses, that number should be four.
- I suggested for the previous version that the paper would be stronger if it also presented results on other knowledge-grounded benchmarks, e.g. DSTC9 and the Doc2Dial benchmark. The author's response to this was reasonable, and I would suggest that the include their reasoning in the paper as a potential limitation of the approach. 

==================================================

1. The proposed model seems to be similar to the multi-relational GCN model in (“Modeling relational data with graph convolutional networks”, 2018), please give the more discussion about the difference of such two works; 2. For Eq. (6), the learning of each channel of each element in adjacency matrix A via the biaffine attention module is unclear, especially for non-numeric linguistic features like POS combination or syntactic dependency types. 
3. The proposed end-to-end framework seems to be a little ad-hoc, such as the motivation of the selection of four types of linguistic features is obscure, meanwhile the linguistic features are extracted by external toolkits, and thus I wonder the influence of toolkit errors on the performance of proposed model. Furthermore, different linguistic features with constraints (R^{pos}, R^{dep}, R^{tbd}, R^{rpd}) may have different influences on the performance of the proposed model, an ablation study is needed for analysis, such as the independent influence of different linguistic features, and the cross-influences of different linguistic features like feature pair (POS, tree-based) or other combinations. In addition, the proposed model need to aggregate the information from different channel, and thus I wonder how it handle the situation if the values of different linguistic features are not in a same scale? 
4. The refined strategy seems to be a coarse strategy, which is only useful for the relations like “A”, actually I wonder the analysis of the refined strategy on different types of relations of word-pairs, which is benefit for better understanding the influence of refined strategy; 
Minor errors: the definition of POS has different meanings, e.g., Part-of-Speech for linguistic features, and a type of 10 self-defined relations of word-pairs, suggest to distinguish such two definitions with different symbols. 

==================================================

- Threshold is said to have a very big impact but is not discussed in detail with different ablations. How does threshold affect computational complexity (outside of performance)?  - Some of the design choices are not explained well (e.g. why IDF-weighting) - Training time (epochs) and computational complexity of the kNN and GCNN component is not discussed. 
- Equations 10 & 11 should be H_{abstract} instead of D_{abstract}? If not, when is H_{abstract} used?  - There is a significant drop in performance for MeSH terms when metadata are not available, leading to a worse performance than other methods (Ablations-d). In case of new journals or preprints, is this the expected performance?  - With the tuned threshold, how many MeSH terms are not selected during the dynamic masking on average in the different data splits? What is the hierarchical level of these terms?  - A few minor typos, proof reading should fix them. Nothing major. 

==================================================

-The chosen evaluation metrics are neither appropriate nor insightful for the task. However, I understand that the authors made the best they could to provide some evaluation based on standard and community accepted metrics and that there are no standard metrics for the task. It is still unsatisfying that a well thought out pipeline has not been convincingly evaluated.
-The human evaluation helps somewhat but is very limited because it lacks comparisons with other methods.
-Use of existing resources -- lack of scientific novelty 
I would strongly advise the authors to provide a discussion/comparison with respect to: https://aclanthology.org/2020.sigdial-1.2.pdf Consider improving the work with a more sophisticated human evaluation on the actual dialogue task. Can you recruit volunteer patients? 

==================================================

- Lack of illustrative examples regarding the model outputs.
- Some details regarding the knowledge collection process have been omitted (see "Questions" below). 
QUESTIONS: - Fig. 2: Why did you discard the "anatomy" category?
- l. 221: How many query templates did you specify in total?
- l. 227: What's the size of the set of knowledge candidates?
- l. 550: Did you calculate the agreement between the annotators? Were the annotators authors of the paper?
MINOR: - Try to better align the figures with the text.
- fix punctuation: l. 336, l. 433, l. 445, l. 534 - Table 2: The highlighting of the numbers does not correspond to the caption ("highest scores are in bold, second highest scores in italic") 

==================================================

- The writing is really poor. Many places are very confusing. The figures are not clearly separated from the text, and it is confusing that where I should look at. Many sentences use the past tense, while other sentences use the present tense. The only reason that I would like this paper to be accepted is because of the dataset. The writing itself is far from a solid paper, and I suggest authors go over the writing again.
- This dataset is the first Thai N-NER dataset, and N-NER in Thai is a new task for the community, so it could be very insightful to know what specific challenges are in Thai, and what errors the models make. The paper provides an error analysis, but not deep enough. It would be insightful if the authors could list error patterns at a finer granularity. 
It is also unclear to me that why syllable segmentation could be useful for the annotation. Many of the readers do not know Thai, so I think more explanation is necessary.
For the writing part, for example, Section 3 is mixed with the past tense and present tense. Figure 2 is hidden in the text. I suggest the authors put all the tables and figures at the top of the pages. 

==================================================

The paper covers too many topics and some of them are not clear enough. 
The paper describes the creation of the dataset, the characteristics of Thai that impact the task, the processing problems, the guidelines and human annotation procedures, evaluation and quality control of the manual annotation, five different experiments offered as baseline and 3 experiments with state of the art models, and error analysis. ( Note that there are four pages of appendices). However, some of the topics are not totally covered. For instance, for the description of Thai characteristics, and associated challenges. It is not clear to me the problem of segmentation and it is not clear what are the human annotators doing after the syllable segmentation. An example would help to see the problem in this context. 
Line 302, right column. " to trained the model" => to train the model 

==================================================

There some parts left unclear during the compilation process. Please see below. 
Also the dataset submitted as supplementary material to the paper seems to be a small part of the whole dataset. 
My comments are as follows: - How did you decide the number of classes (10 for coarse-grained and 104 for fine-grained) and how did you decide these classes?
- It will be useful explaining briefly the currently available NER datasets for the Thai language in the related works section.
- It was stated in Section 3.1 that the data was annotated on a syllable-basis in order to avoid the automatic word segmentation problems. But, in the rest of the paper, all the explanations were given on a word-basis. Section 3.5 indicates that words were segmented using an automatic tokenizer. Also, the example in Table 2 was annotated on a word-basis. This point should be clarified.
- Another related issue is, if the dataset was annotated on a syllable basis, how useful will be such an annotation for NLP tasks? For instance, relation extraction, where we usually extract the NEs and the find relations between them. By using syllable annotations, how can we find word NEs? Using a segmenter or tokenizer?
- How is the annotation on Thai NER datasets, on a syllable-basis or word-basis, and why?
- It is stated that the dataset includes 4,894 documents. In NER datasets, it is better to state the size of the dataset with the number of sentences. How many sentences does the dataset include?
- Were these 4,894 documents selected randomly? 

==================================================

1. Less experimental improvement. The effectiveness of the KL is unclear. 
2. Analyses of experiments are standard but inadequate. 
1. Can you explained more about the relationship of Memory, Memory Bank, Memory Knowledge and Embedding Knowledge in your paper? 
2. How ia the positive samples of comparative learning constructed in Section 3.6? 
3. There is an error in bold T4 in the lower table of Table1. So the promotion on the TACRED dataset is not consistent. 
4. Why does CRL (w/o KL) sometimes perform better than CRL in Table1? Is knowledge distillation really always effective? 
5. In line490-492, the explanation of why CRL (w/o CR) will degrade performance seems to conflict with the reason why using knowledge differentiation(see line 341-346). 
6. It is suggested that some quantitative analysis can be given, for example, the distance of inter-class. 

==================================================

The overall writing is poor. The whole paper lacks logic when organizing their content and is poorly structured, making it difficult to read, especially when they describe their experimental results. It is hard to learn something useful because of the writing. 
Suggestions: 1. Add more figures to clarify the ideas. 
2. Leave the training details (like how many hours it takes to train) in a single section like "implementation details". 
3. Organize the results in a more reasonable way, e.g., highlight what we can learn from each result. 

==================================================

- Some experiments don’t add much to the analysis: Duplication (line 493) doesn’t seem like a reasonable baseline to compare to. It merely multiplies the 50 examples without adding any new information.
- The semi-factual generation on model generated rationale in the first step: the replacement could be done on missing rationales, This will remove the correct rationales form the example. It would be better if the paper address this (even simply pointing out that it’s not an issue).
- Some annotation protocals and their encessities are not explained. E.g., line 223-224 and line 230-231. 
- Typo line 1: rational-centric → rationale-centric - Was the notation in line 284 intentional? Would look nicer to use $x'$ instead.
- Not sure if *inductive bias* is the right term to characterize what the annotation captures. Personally, I think of it more as *invariance*. (This point does not factor in to the final score since it's more of a personal take.) 

==================================================

- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.
- It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.
- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. 
The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the "bold" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better. 

==================================================

While there exist many papers discussing the softmax bottleneck or the stolen probability problem, similar to what the authors found, I personally have not found enough evidence in my work that the problem is really severe. 
After all, there are intrinsic uncertainties in the empirical distributions of the training data, and it is quite natural for us to use a smaller  hidden dimension size than the vocabulary size, because after all, we call them "word embeddings" for a reason. 
I guess what I mean to say here is that the problem is of limited interest to me (which says nothing about a broader audience) because the results agree very well with my expectations. 
This is definitely not against the authors because they did a good job in showing this via their algorithm and the concrete experiments. 
I feel like the authors could mention and expand on the implications when beam search is used. 
Because in reality, especially that many MT models are considered in the paper, greedy search is seldomly used. 
In other words, "even if greedy search is used, SPP is not a big deal, let alone that in reality we use beam search", something like that.
Compared to the main text, I am personally more interested in the point brought up at L595. 
What implications are there for the training of our models? 
How does the gradient search algorithm decide on where to put the word vectors and hidden state vector? 
Is there anything we, i.e. the trainers of the NNs, can do to make it easier for the NNs?
Small issues: - L006, as later written in the main text, "thousands" is not accurate here. Maybe add "on the subword level"?
- L010, be predicted - L034, personally, I think "expressiveness" is more commonly used, this happens elsewhere in the paper as well.
- L082, autoencoder - L104, greater than or equal to 

==================================================

1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.
2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.
3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.
4. ( Minor) The proposed method is tied to prompt tuning & generation tasks. It would be interesting if PTG could be extended to other parameter-efficient algorithms (e.g., Adapter and LoRA) and discriminative tasks.
[1] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer 
Typos: (Line 514) For each size, we sample and 5 different datasets and average over 2 training random seeds. ( remove the first ''and'') 

==================================================

1. The descriptions of several important sentences are unclear, e.g., in “For all languages, we use the reduced version of Wikipedia database”, it is suggested to provide the links to the reduced version of Wikipedia database for all languages (except for English). To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora). 
2. Figure 1 should be illustrated in more details. Why only show the results of three languages in Figure 1? Are the results of other languages consistent with these three languages? 
3. The related work section is quite sketchy. There are limited reviews on collocations and topic models in recent years. One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations. As shown in (Lau et al., 2013), they considered four different bigram replacement methods. Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student’s t-test. Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods. Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams. 
1. The presentation can be improved, e.g., “To train word embeddings” (line 298) and “to obtain word embeddings” (line 301) are repetitive. 
2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014). Besides, the information of reference (Merity et al., 2016) are incomplete. 

==================================================

- Limited to multi-class topical text categorization, which is while important, is ultimately just one task - Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) 
- Can parameter counts/training time for graph-based models be provided?
- less classes -> fewer - Minaee et al. (2021) can be cited: Deep Learning–based Text Classification: A Comprehensive Review - Dataset characteristics discussed  (line 359-376) can be combined with Table 2.
- Missing period (line 224).
- Sentence line 315-319 is ungrammatical. 

==================================================

- Trivial method: These all four auxiliary tasks seem too trivial to achieve the objective of improving arc consistency prediction since all these tasks are still performed on each token independently.
- Lack of in-depth analysis: Using the B+H auxiliary tasks designed to predict the number of governors and dependents for each token only improves the prediction of the number of heads by 0.6 points. This improvement seems pretty trivial. It would be better if the authors could provide more thorough analyses of why these auxiliary tasks are helpful.
- Underperform SOTA by a large margin: The methods underperform the SOTA parsers by a large margin because no POS or lemma information is used, and the authors claim their methods are directly usable with their system. The paper would be much more solid if the authors could verify this hypothesis. 
- Line 079: Explain ID and OOD.
- Line 130: Explain DM dataset.
- Line 138-144: Why the accuracy of predicting the number of heads is related to the observation that zero-head tokens are predicted too frequently.  - Footnote 1: Why use the representation of the first subword of a word? Did you try using the average of all the subwords?
- Code link seems to be expired. 

==================================================

1. In the backward query of Fig 4, why the span is "must visit" instead of "a must visit" given the probability of the word "a" is 86%, which is the same as the word "must"? 
2. What if the forward query and backward query give conflict prediction? For example in the example sentence of Fig 2, if the answer to the first query is only "small" (without "good"), what will happen? The example in Fig 4 is too simple. 
3. What are the differences between two versions of ASTE-Data datasets? 
4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct? 
The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper. 
The variables in formula 1 and 2 should be explained, for example, what is pair_asp. 

==================================================


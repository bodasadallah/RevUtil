{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING 08-01 07:57:00 utils.py:569] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
                        "INFO 08-01 07:57:00 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)\n",
                        "INFO 08-01 07:57:02 selector.py:80] Using Flashinfer backend.\n",
                        "INFO 08-01 07:57:03 model_runner.py:680] Starting to load model google/gemma-2-9b-it...\n",
                        "INFO 08-01 07:57:04 selector.py:80] Using Flashinfer backend.\n",
                        "INFO 08-01 07:57:04 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1447f91832ba4db2959a2bc3f3c5f17f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 08-01 07:57:37 model_runner.py:692] Loading model weights took 17.3781 GB\n",
                        "INFO 08-01 07:57:38 gpu_executor.py:102] # GPU blocks: 9934, # CPU blocks: 780\n",
                        "INFO 08-01 07:57:42 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
                        "INFO 08-01 07:57:42 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
                        "INFO 08-01 07:57:59 model_runner.py:1181] Graph capturing finished in 18 secs.\n",
                        "/fsx/homes/Abdelrahman.Sadallah@mbzuai.ac.ae/mbzuai/review_rewrite/notebooks\n"
                    ]
                }
            ],
            "source": [
                "from vllm import LLM, SamplingParams\n",
                "import os\n",
                "import sys\n",
                "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASHINFER\"\n",
                "\n",
                "llm = LLM(\n",
                "    model=\"google/gemma-2-9b-it\",\n",
                "    gpu_memory_utilization=0.9,\n",
                "    max_model_len=4000\n",
                ")\n",
                "\n",
                "# %%\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "module_path = Path(os.path.abspath(\"\")).parent\n",
                "print(module_path)\n",
                "sys.path.append(str(module_path))\n",
                "from data_processing.filter_reviews import filter_reviews\n",
                "from data_processing.semantic_segmentation import split_paragraph\n",
                "\n",
                "\n",
                "import pandas as pd\n",
                "from prompts import PROMPTS\n",
                "from tqdm import tqdm\n",
                "# reviews = pd.read_csv(\"../../data/reviewer2_ARR_2022_reviews.csv\")\n",
                "split_reviews = pd.read_csv(\"../../data/reviewer2_ARR_2022_manual_split_reviews.csv\")\n",
                "tokenizer = llm.get_tokenizer()\n",
                "sampling_params = SamplingParams(\n",
                "    temperature=0.0, top_p=1, max_tokens=32,\n",
                "    stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 374/374 [13:36<00:00,  2.18s/it]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "aspects = ['Actionability','Constructiveness or Politeness','Credibility or Verifiability','Specificity']\n",
                "cnt = 0\n",
                "for i, row in tqdm(split_reviews.iterrows(), total=split_reviews.shape[0]):\n",
                "\n",
                "    cur_split_review = row['split_review']\n",
                "    cur_split_review = cur_split_review.split('$$$')    \n",
                "    num_of_points = len(cur_split_review)\n",
                "    for aspect in aspects:\n",
                "        aspect_desc = PROMPTS[aspect]\n",
                "        # print('evaluting aspect:', aspect)\n",
                "        aspect_score = 0\n",
                "        scores = []\n",
                "        for j,review in enumerate(cur_split_review):\n",
                "            # print(f'evaluting review:{j} out of {num_of_points}')\n",
                "\n",
                "\n",
                "            prompt = PROMPTS['binary_score_prompt'].format(aspect=aspect, aspect_description =aspect_desc, review=review)\n",
                "\n",
                "            conversation = [\n",
                "                            {\n",
                "                                'role': 'user', 'content': prompt\n",
                "                            }\n",
                "                        ]\n",
                "            c = tokenizer.apply_chat_template(\n",
                "                        conversation,\n",
                "                        tokenize=False,\n",
                "                        add_generation_prompt=True\n",
                "                    )\n",
                "            inputs = [c]\n",
                "            outputs = llm.generate(inputs, sampling_params, use_tqdm= False)[0].outputs[0].text.strip()\n",
                "\n",
                "            \n",
                "\n",
                "            if  outputs != '1':\n",
                "                outputs = '0'\n",
                "            # print(outputs)\n",
                "            aspect_score += int(outputs)\n",
                "            scores.append(outputs)\n",
                "\n",
                "        split_reviews.at[i, aspect + '_average'] = aspect_score / num_of_points\n",
                "        split_reviews.at[i, aspect] = ','.join(scores)\n",
                "\n",
                "    # print(reviews.iloc[i])\n",
                "    cnt += 1\n",
                "    # print(cnt)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Index(['Unnamed: 0', 'paped_id', 'paper_summary', 'summary_of_strengths',\n",
                            "       'summary_of_weaknesses', 'comments,_suggestions_and_typos',\n",
                            "       'score_overall', 'score_best_paper', 'score_datasets', 'score_software',\n",
                            "       'author_identity_guess', 'confidence', 'score_replicability',\n",
                            "       'ethical_concerns', 'focused_review', 'Actionability',\n",
                            "       'Constructiveness or Politeness', 'Credibility or Verifiability',\n",
                            "       'Specificity', 'split_review', 'Actionability_average',\n",
                            "       'Constructiveness or Politeness_average',\n",
                            "       'Credibility or Verifiability_average', 'Specificity_average'],\n",
                            "      dtype='object')"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "split_reviews.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "split_reviews.to_csv(\"../../data/reviewer2_ARR_2022_manual_split_reviews.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
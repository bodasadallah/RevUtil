review_id,source,focused_review,review_point,human_actionability,human_specificity,human_verifiability,human_politeness,llm_actionability,llm_specificity,llm_politeness,Notes,actionability_tim,specificity_tim,verifiability_tim,politness_tim,Notes Tim,llm_verifiability,llm_feedback_verifiability
905,ICLR_2023_2312,"Weaknesses
1. Literature Review
The paper regrettably fails to acknowledge a vast body of related literature, on (i) intention-conditioned trajectory prediction, (ii) variational graph methods for trajectory prediction, and (iii) models that explicitly model social interactions for forecasting. At the very least, these references ought to be mentioned and discussed for a diligent representation of the research space, even if the methods are not directly compared against.
(i) Intention-Conditioned Trajectory Prediction:
[R1, R2, R3] talk about intention-conditioned trajectory prediction for autonomous vehicles. Apart from the data the methods are applied to, the architectures can be applicable to, and are relevant for, the problem being addressed here. Crucially, the DROGON paper defines intention explicitly (more on this in Weakness 2. below).
(ii) Variational Graph Methods:
[R4] from the Neurips I Can't Believe It's Not Better Workshop explicitly deals with graph conditional variational methods for multi-agent trajectory prediction. The results in that paper are very relevant for this research area and should be included.
(iii) Encoding Social Interactions:
Graph and other stochastic methods that encode social interactions between agents have been long applied to trajectory and behavior forecasating problems. [R5] explicitly incorporates a spatiotemporal graph for incorporating social interactions between agents. [R6] more recently explicitly takes a meta-learning approach for modeling the dynamics unique to a group for probabilistic forecasting. A sports team is a group, and if each team is viewed as having unique social dynamics resulting from the team's strategy then [R6]'s core modeling idea is directly applicable. The cue in [R6] terms is simply player location here. Their modeling of social influence of other agents is also permutation invariant, a limitation this paper claims about existing methods. References:
[R1] DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning - Choi et al.
[R2] Intention-Driven Trajectory Prediction for Autonomous Driving - Fan et al.
[R3] LOKI: Long Term and Key Intentions for Trajectory Prediction - Girase et al.
[R4] Graph Conditional Variational Models: Too Complex for Multiagent Trajectories? - Rudolph et al.
[R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al.
[R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
2. Unsupported claims and definitions
The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
3. Notation
There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , . . . ]
. See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.","2. Unsupported claims and definitions The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.",0.0,1.0,0.0,0.0,nan,nan,nan,Do we need to include the full review as well?,1.0,1.0,-1.0,0.0,nan,-1,"the review point contains several claims, such as the lack of definition for agent intentions and causality, and the assertion that the paper does not model causal relationships explicitly. however, the reviewer does not provide any justification or attempt to verify these claims. they do not reference any specific sections of the paper or provide examples to support their assertions. the review point lacks any explanation or evidence to substantiate the claims made, which is necessary for a score of 1. therefore, the review point falls into the category of making claims without providing any supporting information or justification."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion why values for p different from 2 are not interesting to consider.,0.0,1.0,0.0,0.0,nan,nan,nan,nan,1.0,1.0,-1.0,0.0,nan,0,"the review point ""i'm missing a discussion why values for p different from 2 are not interesting to consider"" does not contain any claims. it is a general statement indicating the absence of a discussion in the paper, which is a factual observation. the reviewer is not making a subjective judgment or suggestion but is instead pointing out a missing element without expressing an opinion or providing justification. therefore, this review point does not require any verification or justification."
612,ICLR_2021_971,"Weaknesses
The proposed algorithm is not parameter-free (unlike SNIP, which is virtually parameter-free), is quite complicated (and I imagine difficult to implement), and there is little justification for certain components of the method, e.g., the dynamic scaling function (and choices of lambda), whether the simplification of m_{t-1} = … = m_{T} is mild enough. It is not clear to me how a practitioner can run the proposed algorithm in a parameter-free way without having to conduct ablation studies of their own first, especially since, as the authors note, “We observed that the penalty parameter was difficult to tune properly, either being too aggressive at pruning, or too passive” as the justification for the dynamic scaling function
Parts of the paper are too dense and notation-heavy, and this hurts readability and understanding significantly, e.g., Lemma 1, paragraph regarding the introduction of the saliency function on pg. 2.
The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity
The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.","2. The presented experimental results are not very compelling. For example, in Table 3, we see that BEP 1e-4 achieves a ~.4% improvement over SNIP and GRASP, at the cost of ~7-8.4 more hours of training time. This calls into question the effectiveness of the proposed approach -- which is, at the end of the day, meant to speed up training + pruning by removing unnecessary components of the network early on. Clarity The paper is reasonably well-written and organized overall. It was clear that the authors compressed some of the mathematical expressions/lemmas (e.g., statement of Lemma 1), which is somewhat understandable given the page limit, but this hurt readability and understandability.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,There are two points in here,1,"the review point contains several claims. firstly, it claims that the experimental results are not compelling, specifically highlighting the trade-off between improvement and training time in table 3. this is a subjective judgment about the effectiveness of the proposed approach. secondly, it claims that the clarity of the paper is affected by the compression of mathematical expressions, which impacts readability and understandability. the reviewer attempts to justify the first claim by providing specific data from table 3, comparing bep 1e-4 with snip and grasp, and discussing the trade-off between improvement and training time. for the second claim, the reviewer explains that the compression of expressions is understandable due to page limits, but still negatively affects clarity. these attempts to justify the claims meet the criteria for verifiability."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,0.0,1.0,1.0,0.0,"[A] Leaning to give a 1 here.
[V] I am assuming that ""in this case"" makes it clear to the author to which part the review is referring to, even though it is missing here. ",0,"the review point ""what is the relationship between the cdf and percentile rank in this case? is there a way to express one with the other?"" does not contain any claims. it is a general question asking for clarification or further explanation regarding the relationship between the cdf and percentile rank. the reviewer does not make any subjective statements, suggestions, or requests for changes, nor do they provide any opinion or stand that would require justification. therefore, this review point is a normal sentence that does not need verification."
4288,NIPS_2020_251,"* The NF assumption was not discussed as compared to a standard SSM which uses additive measurement noise. Placing the emission noise *before* the nonlinearity is a crucial move; otherwise filtering is not tractable. I would have appreciated further discussion of the impact of this. It's possible that this technique can be applied as a drop-in replacement in many models to avoid awkward approximations such as EKF, UKF and PF; however this conclusion is not immediate from the work presented in this paper. * As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning. * The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer. * NKF does not show markedly better performance than the GP-Copula model in the main experiments.",* NKF does not show markedly better performance than the GP-Copula model in the main experiments.,-1.0,1.0,0.0,0.0,nan,nan,nan,nan,-1.0,1.0,-1.0,0.0,nan,0,"the review point ""nkf does not show markedly better performance than the gp-copula model in the main experiments"" is a factual statement about the results presented in the paper. it does not contain any subjective judgment, suggestion, or request for changes. the statement simply describes an observation that can be verified by looking at the experimental results in the paper. therefore, it does not require any justification or verification from the reviewer."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,1.0,0.0,1.0,0.0,nan,-1,"the review point makes a claim that the language in the paper is often unclear, specifically citing an example from line 211. the reviewer suggests that the phrase ""aup stops moving entirely"" is ambiguous and implies that it refers to the aup learning a policy where the agent remains immobile. however, the reviewer does not provide any justification or attempt to verify this claim. they do not explain why they believe the language is unclear or why their interpretation of the phrase is valid. therefore, the review point contains a claim but lacks any supporting explanation or verification."
3368,NIPS_2020_1285,- Only one constraint value is selected for each environment (50% of the speed attained by an unconstrained PPO agent - how and why was 50% chosen?). I would be interested in seeing whether FOCOPS consistently exceeds performance and satisfies constraints compared to other methods for different constraint levels. - Only one constraint is used for the experiments. How would FOCOPS perform empirically when there are multiple constraints?,- Only one constraint is used for the experiments. How would FOCOPS perform empirically when there are multiple constraints?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,1.0,1.0,-1.0,0.0,nan,0,"the review point ""only one constraint is used for the experiments. how would focops perform empirically when there are multiple constraints?"" does not contain any claims. it is a factual statement about the paper, noting that only one constraint was used in the experiments, followed by a question regarding the performance of focops with multiple constraints. this is a request for further information or clarification rather than a subjective statement or suggestion for change. therefore, it does not require any justification or verification."
2693,NIPS_2019_436,"Weaknesses 1. Proposed method is relatively simple extension - involves using typical prototype for class in addition to transformation of class word-embeddings. 2. The benefit of incorporating semantic information is largely in the 5-class, 1-shot learning case (3.5% Mini-Imagenet and 2.75% Tiered-Imagenet accuracy gain compared to state-of-the-art LEO applied to regular few-shot learning scenario) and there seems to be very little gain beyond that number of shots. Comments The proposed setting and method requires that word embeddings are known for all train and test classes. Is it a more realistic scenario for few-shot learning that word-embeddings are only available for train classes, as this removes requirement that model that can only be used to learn about concepts that we already have word-embeddings for? Is semantic information as defined in paper applicable to few-shot settings beyond image-classification?","2. The benefit of incorporating semantic information is largely in the 5-class, 1-shot learning case (3.5% Mini-Imagenet and 2.75% Tiered-Imagenet accuracy gain compared to state-of-the-art LEO applied to regular few-shot learning scenario) and there seems to be very little gain beyond that number of shots. Comments The proposed setting and method requires that word embeddings are known for all train and test classes. Is it a more realistic scenario for few-shot learning that word-embeddings are only available for train classes, as this removes requirement that model that can only be used to learn about concepts that we already have word-embeddings for? Is semantic information as defined in paper applicable to few-shot settings beyond image-classification?",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,Multiple points in here,-1,"the review point contains several claims, such as questioning the realism of the scenario where word embeddings are known for all train and test classes, and whether semantic information is applicable beyond image classification. however, the reviewer does not provide any justification or attempt to verify these claims. they do not offer any evidence or reasoning to support their suggestions or questions, such as why the proposed scenario might be unrealistic or how semantic information could be applied in other contexts. therefore, the review point lacks verifiability as the claims are not substantiated with any form of explanation or validation."
4026,NIPS_2020_159,"UPDATE: Thank you for your response, which addressed most of my concerns. The only issue is that only controlling the number of leaves can still be problematic since the depth of the also matters [1]. [1] Reyzin L, Schapire RE. How boosting the margin can also boost classifier complexity, ICML 2006. ============================= I have several concerns and questions: 1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner. 2. The empirical results are also not convincing to me: 1) the results are only averaged over three runs, which is insufficient to me. 2) I would also like to see the standard deviations of the average accuracies. 3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets. 4) I was also wondering if the conclusion still holds with other base learners. 3. We know that AdaBoost is just a special case of gradient boosting with the exponential loss. Therefore, my feeling is that the analysis is just about the gradient boosting with different loss functions, not the behavior of gradient boosting itself.","1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner.",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,-1,"the review point contains a claim regarding the best way to ensure model complexity is the same for base learners, suggesting the use of a decision stump. this is a subjective statement that requires justification. however, the reviewer does not provide any explanation or attempt to verify why using a decision stump would be the best approach. the review point lacks any supporting information or reasoning to strengthen this claim. therefore, it does not meet the criteria for verifiability as the reviewer did not attempt to justify their claim."
1193,ICLR_2023_1294,"weaknesses:
1: The best feature of CLIP is the generality, that is, being able to recognize any image without pre-defined/fixed classes. MUST adapt the CLIP model to a specific dataset (which is the main purpose of this paper). A simple solution is claimed by authors in the limitation section: ""There exists a simple way to address this concern: gather unlabeled image from all the domains of interest, and perform MUST to learn a single model that can generalize to multiple domains."" It would be really great to have 1-2 such experiments to verify this hypothesis.
Minor weaknesses:
2: The paper does not reach out to theoretical backup to explain why MUST works.
3: Adding results on ImageNet-Sketch will further strengthen this paper.
4: A related work [1] is worth discussing.
[1]: Test-time training with masked autoencoders, NeurIPS 2022","4: A related work [1] is worth discussing. [1]: Test-time training with masked autoencoders, NeurIPS 2022",0.0,-1.0,-1.0,0.0,nan,nan,nan,nan,1.0,0.0,0.0,0.0,nan,-1,"the review point ""a related work [1] is worth discussing. [1]: test-time training with masked autoencoders, neurips 2022"" contains a claim that the related work is worth discussing. this is a subjective statement indicating the reviewer's opinion that this particular work should be included in the discussion. however, the reviewer does not provide any justification or explanation for why this related work is significant or relevant to the paper being reviewed. there is no attempt to validate the claim by mentioning how this related work connects to the paper's content or its potential impact on the paper's findings. therefore, the review point lacks verifiability as it does not include any supporting information to strengthen the claim."
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ",1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used.,-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,-1.0,1.0,1.0,0.0,nan,-1,"the review point makes a claim by suggesting that there is no direct relationship between fairness consideration and the risk metric used, which implies a subjective judgment about the paper's content. however, the reviewer does not provide any explanation or attempt to justify this claim. there is no mention of evidence, examples, or reasoning to support the assertion that the fairness implications are merely a side effect of hyperparameter settings and not directly related to the risk metric. therefore, the review point lacks verifiability as the claim is not substantiated in any way."
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.",- The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information.,-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,Is this really a weakness?,-1,"the review point contains claims about the clarity of the paper and the informativeness of figures 1 and 2, particularly figure 2. these are subjective statements that express the reviewer's opinion on the quality and effectiveness of these sections. however, the reviewer does not provide any explanation or attempt to justify why they find the introduction clear or why figure 2 is particularly informative. there is no mention of specific elements or aspects of the figures or introduction that contribute to these claims. therefore, the review point lacks any attempt to verify or justify the claims made."
44,ICLR_2022_1016,"Weaknesses: 1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time. 2. For previous SVD-based and NS-based methods, computation processes for forward and backward propagations are consistent. However, this work adopts Matrix Taylor Polynomial or Matrix Pade Approximation for forward propagations and uses approximate Lyapunov equation for backward propagation, leading variance in forward and backward propagations. The authors would better make some discussions about this issue. 3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it.
[r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021.
Other comments: 1. It is clear that MPA involves matrix inverse, which is very GPU-unfriendly. As stated in the paper: ""Moreover, we note that the matrix inverse can be avoided, as Eq. (13) can be more efficiently and numerically stably computed by solving the linear system"". The authors would better provide more detailed computation and analysis. 2. How to compute the coefficients of p m and q n
for the Matrix Pade Approximation in equation (12)? Do forward operations of MPA in Table 1 contain computation of coefficients p m and q n
? 3. Does Equation (2) lack a (·){sym} operation for / f r a c l U
? 4. Is equation (11) missing a term z^{k} in the left side? 5. I am not sure why sign(B) in equation (21) can be calculated as identity matrix? 6. P{M} and Q_{N} are used to approximate the Taylor series. If I am not misunderstanding, does I- Q_{N}^{-1}P_{M} replace Q_{N}^{-1}P_{M} in Eqn. (13)? and do the terms I-X replace X in Eqn. (12)?","3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it. [r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021. Other comments:",1.0,1.0,-1.0,1.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,-1,"the review point contains a claim that the authors should make some discussions about the efficiency of bp of mpa, as shown in table 6, and the use of svd and mtp/mpa in forward and backward propagations. this is a subjective statement suggesting an addition to the paper, which qualifies as a claim. however, the reviewer does not provide any explanation or justification for why such a discussion is necessary or beneficial. they do not attempt to validate their suggestion with any reasoning or evidence. therefore, the review point lacks verifiability as the claim is not supported by any form of justification."
4854,NIPS_2020_653,"The key issue is in Eq. 6, which appears to indicate that the normalising flow transformation is applied at each timepoint independently. This renders the model somewhat trivial (a Kalman filter with nonlinear outputs), extensively studied (e.g. in the EKF literature). It is well known that linear dynamics in a Kalman filter is a discretisation of an underlying continuous dynamical system, and you could use unevenly spaced observations if desired. Here are some possible claims that would render the work more interesting: 1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution. 2.) Having a normalising flow that depends on the value of the Weiner process at all past time steps (which allows much richer temporal dependencies). 3.) Arguing that the presently described process is surprisingly effective, by looking at a more empirical examples, and comparing to SOTA performance from referenced papers. Eq 12 might do some of this, but it is extremely unclear.",1.) Inclusion of the Jacobean in the output transformation renders the ML solution a better characterisation of the Bayesian solution.,-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point ""inclusion of the jacobean in the output transformation renders the ml solution a better characterisation of the bayesian solution"" contains a claim that the inclusion of the jacobean improves the characterization of the bayesian solution. however, the reviewer does not provide any explanation or attempt to verify this claim. there is no supporting information or reasoning given to justify why the inclusion of the jacobean leads to a better characterization. therefore, this review point lacks verifiability as the claim is not substantiated with any form of justification or evidence."
4349,NIPS_2020_1636,"While the general idea of the paper is appealing and has been evaluated extensively, the presentation of the methodology is lacking in clarity at times. After reading section 3, some issues could have been addressed more clearly: • Regarding line 171/172: what do the authors mean by “regret reaches the plateau”? Can this be quantified? • If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown? • How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.","• If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown?",0.0,0.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,-1.0,0.0,nan,-1,"the review point makes a claim by questioning the validity of the leftmost leaf being the 'best' leaf in the constructed tree. this is a subjective statement that implies a need for justification or evidence to support the claim. however, the reviewer does not provide any explanation or attempt to verify this claim. they simply pose a question without offering any reasoning or evidence to support their skepticism. therefore, the review point lacks verifiability as the reviewer did not mention anything to strengthen their claim."
4495,NIPS_2020_1367,"1) In my experience, it is commonly collapsed when training the network with BN directly normalized by moving statistics. Batch renormalization technique [1] can address this problem. If you have used batch renormalization in the experiments, it is better to cite it in the experimental part. If you used a new technique to avoid training from collapse, please give the details. [1] Ioffe,S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in neural information processing systems, pp. 1945–1953, 2017. 2) I think it might need more comparison/ablation experiments to show the characteristics of StochNorm. There are no experiments of comparing stochastic normalization to each branch batch normalization with the current mini-batch statistics or the moving statistics. The parameter selection probability $p$ is significant, and it is better to offer the parameter sensitivity analysis in the experiments. 3) StochNorm is orthogonal to other fine-tuning methods, which is claimed to be an advantage. However, when observing Table 3 and Table 4, I find the performances of L^2_SP + StochNorm and DETAL + StochNorm on dataset Stanford Cars is even inferior to that of single StochNorm, which might be inconsistent with the claim. 4) A typical NeurIPS paper commonly offers us insights with theoretically grounded analysis. This work is relatively weak in this aspect.",4) A typical NeurIPS paper commonly offers us insights with theoretically grounded analysis. This work is relatively weak in this aspect.,-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,1.0,-1.0,-1.0,0.0,"[A]: Can kind of be argued in both ways. Reviewer wants more theoretically grounded analysis. However, this is very abstract.",-1,"the review point makes a claim by stating that the work is ""relatively weak"" in providing theoretically grounded analysis, which is a subjective judgment about the paper's quality. however, the reviewer does not provide any explanation or attempt to justify this claim. there is no mention of specific examples or reasoning to support why the analysis is considered weak. therefore, the review point lacks verifiability as it does not include any effort to validate the claim made."
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,1,"the review point makes a claim that the authors' understanding of safe reinforcement learning is inaccurate, as it only focuses on avoiding negative side effects during training, while it should also consider safety at deployment. the reviewer attempts to justify this claim by referencing garcía et al., 2015, which is cited in the paper, to support their argument that safe rl encompasses more than just avoiding negative side effects during training. this reference serves as an attempt to verify the claim made by the reviewer. therefore, the review point contains a claim and provides some form of verification."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,1,"the review point contains a claim that it is crucial to list all the features in detail in the final version for clarity and replicability of the paper. the reviewer justifies this claim by stating that the authors' response provided more details on this point, implying that the current level of detail in the paper is insufficient. this justification attempts to validate the claim by highlighting the importance of detailed feature listing for the paper's clarity and replicability. therefore, the review point includes a claim and provides an explanation to support it."
3735,NIPS_2020_1660,"- The work heavily builds on the previous work of Cutkosky and Orabona [10]. In particular, the power of the proposed BCO method really relies on the paramter-free nature of the scale learning interface in ""Interface 3"". So this diminishes its significance from the side of technical contributions. - In line 72, the authors argue two new ideas ""appropriate surrogate loss function"" and ""a new one-point gradient estimator with time-varying parameters"". I admit that the design of surrogate loss is clever, while the gradient estimator seems standard, with only a slight and nature twist for the problem. So it is not appropriate to emphasize much on that.","- In line 72, the authors argue two new ideas ""appropriate surrogate loss function"" and ""a new one-point gradient estimator with time-varying parameters"". I admit that the design of surrogate loss is clever, while the gradient estimator seems standard, with only a slight and nature twist for the problem. So it is not appropriate to emphasize much on that.",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,-1,"the review point contains claims regarding the novelty and emphasis of the two ideas presented by the authors: the ""appropriate surrogate loss function"" and the ""new one-point gradient estimator with time-varying parameters."" the reviewer expresses an opinion that the surrogate loss design is clever, while the gradient estimator is standard and should not be emphasized much. however, the reviewer does not provide any justification or attempt to verify these claims. there is no explanation or evidence given to support why the surrogate loss is considered clever or why the gradient estimator is deemed standard. therefore, the review point lacks verifiability as the claims are not substantiated with any form of reasoning or evidence."
1353,ICLR_2023_3031,"Weaknesses: - The technique seems to be in its early stages and it seems as if tuning needs to be performed for every new dynamical system and setting. First, the different training phases are difficult to follow and makes me wondering if the pipeline is robust enough. Also, please explain how was chosen to use a 4-layer U-Net for the Lorenz-63 and only a 3-layer U-Net for the Lorenz-96. - Network structure: it is not clear which losses are minimized at the different phases, in particular, the perturbator+flow operator is not clear. From eq. 11, it looks as if the dynamics and the data fidelity losses were applied on the same current state x_hat, while my understanding was that L_rec was calculated on the perturbator's output while L_dyn was calculated on the flow operator's output. If the hybrid loss is calculated on the final output of the 2 blocks, I don't se how we can enforce the decoupling of the two goals as stated. - It is difficult to understand the size of the input data and latent data. In particular, please give axis labels on Figure 1 images, as it first looks like a 2D spatial problem, while later it is explained that the input is of size time and location. Please also clarify what does T represent in Figure 2: I guess the time dimension, and in this case where is the location dimension? - Please explain best the following sentence: 'For the case in which the prior dynamics are unbiased': in a real setting, how can we know if the prior dynamics are biased or not? - 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive? - Why stopping at 8 and 10 blocks for the experiments, while it looks as if the results are always improving with more blocks?
Typos: Specifically, The perturbator
The perturbator uses the observations and labels it has learned to perturb the reconstructed states to make it deviate from the original flow --> please rephrase
Figure 3: the color do not match Figure 1, as here the colors are also linked to training/no training. Maybe use another sign to indicate training/no training, as a red line surrounding the box.",- 'espilon(t) represents the white Gaussian noise.' --> isn't it reductive?,0.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point ""espilon(t) represents the white gaussian noise.' --> isn't it reductive?"" contains a claim, as it suggests that the representation of epsilon(t) as white gaussian noise might be overly simplistic or lacking in detail. however, the reviewer does not provide any justification or explanation for why they believe this representation is reductive. there is no attempt to validate the claim or provide context or examples to support the assertion. therefore, the review point lacks verifiability."
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,-1,"the review point contains claims regarding the paper's shortcomings, specifically the lack of description on how to quantify the top-k frequent interaction pairs and the limited results and discussions presented in section 5.4. these are subjective statements that require justification. however, the reviewer does not provide any explanation or attempt to verify these claims. they do not offer any reasoning or evidence to support their assertions about the paper's deficiencies. therefore, the review point lacks verifiability as the claims are not substantiated with any form of justification or validation."
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness.",0.0,0.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,-1,"the review point makes a claim that the study among different adversarially trained models is missing, which leads to an unclear trade-off among robust trained models. it suggests that the trades model may improve both robustness and back-door robustness. however, the reviewer does not provide any explanation or attempt to verify this claim. there is no mention of any evidence, examples, or reasoning to support the assertion that the study is missing or that the trades model could improve both aspects. therefore, the review point lacks verifiability as the reviewer did not attempt to justify their claim."
5339,NIPS_2020_1039,"1) It could be better if a more comprehensive comparison of the asymptotic performances among on/off-policy methods. Off policy methods enjoy better sample efficiency at the cost of higher computation burden. Maybe an additional table could be provided in the appendix. Personally, I don’t actually expect a gap between the on/off-policy methods. 2) Although the usage of current f-function in the f-divergence is justified, it will still be interesting to see a comparison if the alternative f-functions were adopted. An empirical comparison would further justify the usage of the author’s current choice. Minor: 1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}. 2) Notations in sec. 8.6 are inconsistent: sampling from \pi is switching back and forth between $a \sim \pi(s)$ and $a \sim \pi(\cdot | s)$. 3) At the end of Algorithm box 1, does J_{\nabla \theta} J_{reg} (\pi_{\theta}) intends to mean that the gradient of \theta on J_{reg}? 4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?","1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}.",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,1.0,1.0,-1.0,0.0,nan,-1,"the review point states that there is an error in the integration domain in equation 12, specifically in the proof of lemma 2. this statement implies a claim that the current integration domain is incorrect and should be changed. however, the reviewer does not provide any justification or explanation for why the integration should be over \(\mathcal{s} \times \mathcal{s}\) instead of \(\mathcal{s} \times \mathcal{a} \times \mathcal{s}\). there is no attempt to validate this claim with any reasoning, examples, or references. therefore, the review point contains a claim but lacks any supporting information to strengthen it."
4364,NIPS_2020_285,"- In Section 2, it is assumed that the state and action spaces are finite. Is this assumption really necessary? It might be quite limiting since policy gradient methods are typically employed when dealing with continuous state-action spaces. Moreover, the transition model is defined as deterministic. Is this assumption necessary? - Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case. - Proposition 5: This is more curiosity than an issue. Are there some sufficient conditions to enforce that Var(R) > 0 along the whole learning process? It seems to me that as we get close to the optimum we are going to prefer less stochastic policies, thus we slow down convergence. Do you think we can still converge asymptotically if deterministic policies are allowed? Anyway, in policy search, we could even limit to stochastic policies. Maybe in such a case, we can have a non-zero guaranteed improvement and, consequently, converge in a finite number of iterations. I think the paper would greatly benefit from a discussion on these points. - Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers","- Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,0,"the review point expresses confusion about the notation used in proposition 6, specifically regarding the improvement operator for different cases. this is a request for clarification rather than a claim or judgment about the paper's content. the reviewer does not make any subjective statements or suggestions for changes; instead, they are seeking an explanation to resolve their confusion. therefore, this review point does not contain any claims that require justification or verification."
4874,NIPS_2020_62,"- The paper's narrative is based around POMDPs, but the experimental evaluation does not really stress the capability of the method in that respect. Evaluation is done on pixel-based control, which is PO of course, but we have know that a lagged observation of a few time-steps can make the state fully observable quickly. (See the appendix of [1]). Hence, we do not know how the method fares in environments where the state uncertainty has to be actively reduced by the agent. Therefore I think the paper overstates the results. It is easy to get out of this, however, since one can just drop the POMDP claim. - The justification of the overall approach could have been improved. For me personally (and the optimal control community) it is obvious that we want some kind of state estimation when we use control, as most–if not all–practical problems are PO. But the paper could have done a much better job at its justification. E.g. a very noisy sensor that requires a few time steps waiting to correctly estimate a quantity makes such approaches necessary. The authors suffer from the fact that the RL community is somewhat focused on Mujoco-like benchmarks, which are representative of only a very small fraction of practical optimal control problems. But the authors could have chosen to use a different suite of environments, such as EscapeRoomba or MountainHike, which would illustrate this. If the authors had chosen to conduct experiments that tackle much more relevant POMDP problems, I'd have given an increased score. - I would have enjoyed an ablation whether AISOC/MaxEnt is necessary. [1] **CURL: Contrastive Unsupervised Representations for Reinforcement Learning** Michael Laskin*, Aravind Srinivas*, Pieter Abbeel. Thirty-seventh International Conference Machine Learning (ICML), 2020.","- The justification of the overall approach could have been improved. For me personally (and the optimal control community) it is obvious that we want some kind of state estimation when we use control, as most–if not all–practical problems are PO. But the paper could have done a much better job at its justification. E.g. a very noisy sensor that requires a few time steps waiting to correctly estimate a quantity makes such approaches necessary. The authors suffer from the fact that the RL community is somewhat focused on Mujoco-like benchmarks, which are representative of only a very small fraction of practical optimal control problems. But the authors could have chosen to use a different suite of environments, such as EscapeRoomba or MountainHike, which would illustrate this. If the authors had chosen to conduct experiments that tackle much more relevant POMDP problems, I'd have given an increased score.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,1.0,1.0,1.0,0.0,nan,1,"the review point contains several claims, such as the need for better justification of the overall approach, the suggestion to use different environments like escaperoomba or mountainhike, and the assertion that the rl community is overly focused on mujoco-like benchmarks. the reviewer attempts to justify these claims by providing examples, such as the scenario of a very noisy sensor requiring state estimation, and by suggesting alternative environments that could better illustrate the paper's relevance to practical pomdp problems. these examples serve as an attempt to validate the claims made, thus meeting the criteria for verifiability. therefore, the review point includes claims and provides some form of explanation or verification for them."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations.,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point makes a claim by stating that there is a missing discussion on the numerical aspects of the l2 wasserstein distance computations. this is a subjective statement suggesting that the paper should include additional content. however, the reviewer does not provide any justification or attempt to verify why this discussion is necessary or how it would benefit the paper. there is no explanation or evidence provided to support the claim that the discussion is missing or needed. therefore, the review point lacks verifiability as the claim is not substantiated with any reasoning or evidence."
3549,NIPS_2020_1360,"The idea of the paper is simple and the motivation of the approach is well justified. However i have some concerns on the novelty of the work and the comparison with existing results. In particular I have the following concerns: Main Issues: I am concern regarding two of the main contributions of this work. I believe that some of the claims on the main contributions section are not novel. 1) The fact that the stochastic EG did not converge was shown with a counterexample in Chavdarova et al. [2]. However, in line 82, the authors mentioned that their approach on counterexample is an improvement because they show that the non-convergence persists for any error distribution with positive variance. In my opinion the proposal counterexample is not really a main contribution of this work. This work builds upon the counterexample of Chavdarova et al. [2] and use it to propose a convergent variant of the EG. 2) In line 55 the authors claim: ""Prior to our work, last-iterate convergence rate for bilinear min-max games had only been studied in the deterministic setting."" This is not true. In ICML 2020 Loizou et. al proposed the analysis of stochastic Hamiltonian methods showing last-iterate convergence for stochastic bilinear games and some classes of non-convex non-concave games. They also proposed a variance reduced method showing linear convergence (which is much faster then the O(1/t) rate of the DSEG in this setting). Other Issues: 3) The authors mentioned that the error bound condition is satisfied for two large classes of problems: Strongly monotone operators and Affine operators. However the definitions of the above two problems were never explicitly given. In addition for the affine operators it is mentioned that $\tau$ is the minimum non-zero singular value of the matrix. Which matrix? the authors need to be more rigorous. 4) In line 116 the authors start using capital letters for the notation (see $X_t$). until this point everything was lower case. 5) In experiments the only method presented is the DSEG with different values for $r_\gamma$ and $r_eta$. This is not adequate. The method should compare with other methods that guarantee convergence for the classes of games under study, like the stochastic Hamiltonian methods mentioned above. Even if the DSEG will be slower it will be really helpful for the reader. In addition the indicators of the lines in the Figure 3 are not distinguished (for colour-blind readers)",4) In line 116 the authors start using capital letters for the notation (see $X_t$). until this point everything was lower case.,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"the review point simply states an observation about the use of notation in the paper, specifically noting a change from lowercase to uppercase letters starting at line 116. this is a factual statement about the content of the paper and does not include any subjective judgment, suggestion, or request for change. there is no claim made that requires justification or verification. therefore, this review point does not contain any claims."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization.,0.0,0.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point, ""i'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization,"" contains a claim. the reviewer is implicitly suggesting that the paper should include a discussion on the computation of further derivatives for marginal likelihood hyperparameter optimization. this is a subjective statement indicating that the reviewer believes this discussion is necessary. however, the reviewer does not provide any justification or explanation for why this discussion is important or how it would benefit the paper. there is no attempt to validate the claim or provide context for its inclusion. therefore, based on the criteria, this review point lacks verifiability."
1261,ICLR_2023_4259,"Weaknesses
Linear latent dynamics have been used in object-centric models before
No quantitative evaluation with respect to object tracking
Several baselines missing
No ablations whether all the complex components are necessary
1. Novelty
I have some concerns about novelty here. Linearizing the latent dynamics is not a new idea. It's one of the main ideas behind Contrastive Predictive Coding (van den Oord 2018) and has been used in the context of object-centric video models in VideoMONet (Weis et al., JMLR 2021) for predicting the next frame. Although these works don't frame it in the language of Koopman operator theory, the main idea seems very similar. Thus, the novel idea in this work seems to me the interpretation and manipulation of the eigenvalues of the linear transition matrix. Shouldn't this also be possible with previous methods like VideoMONet?
2. Evaluation
The authors evaluate only with respect to reconstruction and all further demonstrations of manipulating the dynamics are only qualitative based on individual examples. However, object-centric models have problems tracking objects properly through occlusions (Weis et al. 2021). Thus, we don't know whether and how well the approach actually works except for individual hand-picked examples.
3. Ablations missing
The authors framework is a mix and match of components proposed in earlier works, such as object masks, occlusion modeling, separating appearance from pose, tracking objects and modeling their interactions. However, to what extent each of these components actually do what they promise and whether they are necessary for the problems studied by the authors remains unclear. Wouldn't a substantially simpler approach accomplish the same on such simple datasets? I think it would make much more sense to modify existing approaches by replacing only the component that models the latent dynamics and showing that performance can be retained while gaining interpretability or -- if this is not possible -- showing which other components are necessary to make the linear latent dyanmics possible.
4. Baselines
Several strong baselines for object-centric dynamic models like OP3 (Veerapaneni et al. 2020), VideoMONet (Weis et al. 2021) or SAVi (Kipf et al. 2022) seem to be missing.","4. Baselines Several strong baselines for object-centric dynamic models like OP3 (Veerapaneni et al. 2020), VideoMONet (Weis et al. 2021) or SAVi (Kipf et al. 2022) seem to be missing.",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point states that several strong baselines for object-centric dynamic models are missing, specifically mentioning op3, videomonet, and savi. this is a claim, as it suggests that the paper should have included these baselines. however, the reviewer does not provide any justification or explanation for why these particular baselines are necessary or how their absence impacts the paper's contributions or findings. there is no attempt to validate the claim by discussing the relevance or importance of these baselines in the context of the paper. therefore, the review point lacks verifiability as it does not strengthen the claim with any supporting information."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","-The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. -README file for the dataset [Authors committed to add README file] - General Discussion:",0.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as the explanation of methods being too detailed, the lack of mention of other work, and the repetition in method sections. additionally, there is a claim about the need for a readme file for the dataset. however, the reviewer does not provide any justification or explanation for these claims. they do not attempt to verify why the explanation of methods is considered too detailed or why the repetition is problematic. similarly, there is no explanation for the necessity of a readme file. the review point lacks any attempt to validate these claims, resulting in a lack of verifiability."
5776,NIPS_2018_600,"weakness of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts. Positives: + The paper is well organized and easy to follow. + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound. + Good performance. Negatives: - Less discussion on the linear version of CGNL using dot product for f. - Missing fundamental comparison to the simple ResBlock. The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns. - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ]  = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling.  Such discussions as above would help the readers to further understand the methods and to further extend the method. - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of Z = BatchNorm( X * W_z ) + X, while the linear CGNL is Z = BatchNorm( s * X * W_theta * W_z ) + X  = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance. [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works. ",+ The paper is well organized and easy to follow.,-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point ""the paper is well organized and easy to follow"" contains a claim about the paper's organization and readability. however, the reviewer does not provide any explanation or evidence to support this claim. there is no attempt to justify why the paper is considered well organized or easy to follow, such as mentioning specific sections, the logical flow of arguments, or clarity of presentation. therefore, the review point lacks verifiability as the claim is not substantiated with any form of justification or verification."
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point makes a claim by stating that the authors make a strong connection to sil and florian's work, emphasizing the supervised vs. unsupervised difference. this is a subjective statement that requires justification. however, the reviewer does not provide any explanation or attempt to verify this claim. they do not mention any relevant information or examples to support their assertion about the connection or the emphasis on the supervised vs. unsupervised difference. therefore, the review point lacks verifiability as the claim is not substantiated with any form of justification or evidence."
4170,NIPS_2020_490,"I think more details and discussion are needed to justify and evaluate some claims in the paper. 1. In Figure 5, it is said that ""two children spawned at an early time t_s in the chaotic training regime arrive at two different loss basins"", while ""two children spawned at a later time t_s in the stable training regime arrive at the same loss basin"". While I do agree with this claim, a simple explanation for it may just be that as t_s becomes larger, the distance between a child and its spawning parent becomes smaller and smaller. To see this, note that as the risk decreases, the gradient norm also becomes smaller, and the child is trained for T-t_s epochs, which also shrinks as t_s becomes larger since T seems to be fixed. Therefore we can expect the child and parent to eventually lie in the same basin as t_s becomes larger, and I wonder what is the new conclusion we can draw from this experiment. 2. In Figure 6, at the last points of the curves, is \tilde{t} equal to T? If so the green curves should meet the red curves since there is no additional training, while if not I wonder the details. Regarding the claim that the learned NTK can outperform full deep learning, it seems that the kernel can indeed drives the training error smaller, but the improvement in test error is very small and may be negative as in Figure 12. I also wonder if the curves in Figures 6, 11 and 12 represent the average of multiple parent and child runs?","2. In Figure 6, at the last points of the curves, is \tilde{t} equal to T? If so the green curves should meet the red curves since there is no additional training, while if not I wonder the details. Regarding the claim that the learned NTK can outperform full deep learning, it seems that the kernel can indeed drives the training error smaller, but the improvement in test error is very small and may be negative as in Figure 12. I also wonder if the curves in Figures 6, 11 and 12 represent the average of multiple parent and child runs?",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims and subjective statements. the reviewer questions whether the green and red curves should meet in figure 6, implying a claim about the expected behavior of the curves. they also express skepticism about the claim that the learned ntk can outperform full deep learning, suggesting that the improvement in test error is minimal or negative. additionally, the reviewer questions whether the curves in figures 6, 11, and 12 represent averages of multiple runs, which implies a claim about the methodology used in the paper. however, the reviewer does not provide any justification or attempt to verify these claims. they do not offer any evidence or reasoning to support their assertions or questions. therefore, the review point lacks verifiability as the claims are not substantiated with any form of explanation or validation."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature).,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains a claim that the authors have not demonstrated whether the percentile rank correlates with perceived lightness on the same scale of perception, implying a need for additional analysis or data. however, the reviewer does not provide any justification or attempt to verify this claim. they suggest that relevant numbers might be found in the literature but do not elaborate on why this is necessary or how it would support their point. therefore, the review point lacks any attempt to strengthen the claim made."
3556,NIPS_2020_936,"Weaknesses: - the paper could gain of having a thorough discussion on the generalization properties of the proposed learning procedure (and if no gain, that should be stated as well); - is is not clear how (not) stringent is Assumption 3 regarding the hypothesis classes, whereas it is key for the results provided; in addition the finite class claim related to floating point arithmetic could be developed a bit more; - it is not clear to me what the main strength of MixBoost is: -- is it related to the computational savings? -- is it related to the generalization performances of the method? -- is it related to the use of Random Fourier Features? - regarding Random Fourier Feature: wouldn't an even accelerated method such as Fastfood (Le et al. 2013) preferable to the sampling proposed here?","- the paper could gain of having a thorough discussion on the generalization properties of the proposed learning procedure (and if no gain, that should be stated as well); - is is not clear how (not) stringent is Assumption 3 regarding the hypothesis classes, whereas it is key for the results provided; in addition the finite class claim related to floating point arithmetic could be developed a bit more; - it is not clear to me what the main strength of MixBoost is: -- is it related to the computational savings? -- is it related to the generalization performances of the method? -- is it related to the use of Random Fourier Features?",1.0,1.0,-1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as the need for a thorough discussion on the generalization properties of the proposed learning procedure, the lack of clarity regarding assumption 3, and the ambiguity about the main strength of mixboost. however, the reviewer does not provide any justification or attempt to verify these claims. they do not explain why a discussion on generalization properties is necessary, why assumption 3 is unclear, or why understanding the main strength of mixboost is important. the review point lacks any supporting information or reasoning to strengthen these claims. therefore, it does not meet the criteria for verifiability as the reviewer has not attempted to justify their claims."
1707,ICLR_2023_1914,"Weaknesses (1) The framing of synergies and their neuroscientific context is somewhat lacking. The premise of the paper is that muscle synergies can be predicted from the cortical inputs, e.g. We applied our method to the corticomuscular system, which is made up of corticospinal pathways between the primary motor cortex and muscles in the body and creates muscle synergies that enable efficient connections between the brain and muscles.”, however synergies are thought to be generated in the spinal cord and some of their first characterization was in frogs, a species without a motor cortex. One reason this is problematic is that the paper is framed as revealing new synergies, e.g. “However, the conventional approach uses only muscle activities (observed phenomena) to capture the muscle synergies, and there may still be unexplored muscle synergies that remain hidden” However, based on the model design it seems like it should be detecting a subset of the muscle-only synergies. Moreover, synergies is largely defined in a muscle-centric way. It is certainly the case that discovering cortico-muscular shared synergies is interesting, but the framing is very different. (2) There are no details provided on the TCN training, and importantly how data was split up for train and test splits. This is especially important for the SCI experiments. (3) There are multiple alternative models that could be considered, for instance performing NNMF on a linear or nonlinear prediction of EMG from ecog activity. The specific motivation for the increased number of parameters and model structure of the TCN is not provided. One of the appeals of NNMF is its simplicity, allowing it to be used across paradigms and contexts, and this TCN introduces a lot of added complexity, with only minimal gains in VAF at high numbers of syneries. (4) For the SCI experiments – there is no ground truth present and so it is impossible to evaluate which technique is ‘correct’. As noted above, without knowledge of how much data is required for model training, it is hard to know if the increase in number of synergies observed is a result of
Nits: - ‘connectivity’ is misleading, as it isn’t using the structural connections between the brain and body. - Figure 6: would help to have an estimate of variance for the number of synergies, e.g. from using different subsets of data to train/test.","- ‘connectivity’ is misleading, as it isn’t using the structural connections between the brain and body.",-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point ""‘connectivity’ is misleading, as it isn’t using the structural connections between the brain and body"" contains a claim. the reviewer is asserting that the term ""connectivity"" is misleading in the context of the paper because it does not refer to the structural connections between the brain and body. however, the reviewer does not provide any explanation or attempt to justify why they believe the term is misleading. there is no additional information or reasoning given to support this claim, which is necessary for verifiability. therefore, the review point lacks any attempt to verify the claim made."
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.","3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as questioning the experimental runtimes and suggesting that the decoders could be costly. the reviewer also claims that the paper does not substantially improve over previous works and that the clarity of writing can be improved. however, the reviewer does not provide any justification or attempt to verify these claims. they do not offer evidence or reasoning to support their concerns about the experimental runtimes or the comparison with previous works. the mention of the author's response addressing some concerns is noted, but it does not provide a justification for the claims made. therefore, the review point lacks verifiability as the claims are not supported by any explanation or evidence."
4026,NIPS_2020_159,"UPDATE: Thank you for your response, which addressed most of my concerns. The only issue is that only controlling the number of leaves can still be problematic since the depth of the also matters [1]. [1] Reyzin L, Schapire RE. How boosting the margin can also boost classifier complexity, ICML 2006. ============================= I have several concerns and questions: 1. Line 97: what does “same size” mean? We know that in order to make a fair comparison, we must make sure that the model complexity of base learners is the same. For decision trees, does it mean the same number of leaves? To me, the best way could be just using a decision stump as a base learner. 2. The empirical results are also not convincing to me: 1) the results are only averaged over three runs, which is insufficient to me. 2) I would also like to see the standard deviations of the average accuracies. 3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets. 4) I was also wondering if the conclusion still holds with other base learners. 3. We know that AdaBoost is just a special case of gradient boosting with the exponential loss. Therefore, my feeling is that the analysis is just about the gradient boosting with different loss functions, not the behavior of gradient boosting itself.","3) The experiments are only evaluated on one data set. To make the conclusion more convicting, the author should make a comparison on more data sets.",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point makes a claim by suggesting that the authors should evaluate their experiments on more datasets to make the conclusions more convincing. this is a subjective statement indicating the reviewer's opinion on how the research could be improved. however, the reviewer does not provide any justification or explanation for why using more datasets would make the conclusions more convincing. there is no attempt to validate the claim with examples, references, or reasoning. therefore, the review point lacks verifiability as it does not include any effort to support the claim made."
4568,NIPS_2020_812,"Significance and novelty of the contribution. Some of the assumptions that the authors then chose to make made the paper a less interestsing than I had hoped. making the investigation very deep, but narrow: * ...that mapping means creating some form of quantized top-down view of the environment. I don't see why a human readable map should be in any way optimal for a learnt system. Other work indeed shows you can decode a such a map from a hidden representation, but the choice of representation / architecture is very important for agent performance. * using MapNet as the one example of fully learn system. I appreciate that this choice might be motivated by the fact that the agent uses a mapping represenation compatible with the others leveraged in the paper. At the same time I think it is fair to say thatt the conclusions the authors derive from the experiments are not warranted. * the unusual reward structure using r_closer - what happens without this signal, in a setting that is less artificial? Overall I think the overall impact of the paper is limited by the artifically rescricted scope of the experiments. In particular, the conclusions about the learnt system are not convincing, because of the choice of agent design and representation, and lack of comparisons with baselines in the literature - that is, I think the results we see are like this because of the choice of agent, rather that something fundamental you have found. I would love if the authors could convince me otherwise in the rebutttal.",* using MapNet as the one example of fully learn system. I appreciate that this choice might be motivated by the fact that the agent uses a mapping represenation compatible with the others leveraged in the paper. At the same time I think it is fair to say thatt the conclusions the authors derive from the experiments are not warranted.,-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains a claim that the conclusions derived from the experiments using mapnet are not warranted. however, the reviewer does not provide any explanation or attempt to justify this claim. there is no mention of specific reasons or evidence to support why the conclusions are considered unwarranted. the review point lacks any attempt to verify or substantiate the claim made about the conclusions. therefore, based on the evaluation criteria, this review point contains a claim but does not include any effort to strengthen or justify it."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art.,-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as the paper being ""well-written and well-organized,"" ""clearly positions the main idea and proposed approach related to existing work,"" and ""experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art."" however, the reviewer does not provide any explanation or evidence to support these claims. there is no attempt to justify why the paper is considered well-written or how the effectiveness of the proposed approach is demonstrated. therefore, the review point lacks verifiability as the claims are not substantiated with any form of justification or verification."
493,ICLR_2022_1823,"Weaknesses
The main weakness is that the relationship between ADU and constituency tree is not clearly described. Is it true that ADU are often phrases that occur in the constituency tree? How often does this happen? Does the new BERT-based model adhere to constituency constraints? Is the BENEPAR parser appropriate for this data? Based on Trautmann et al.’s comments on grammaticality and clauses, my intuition is that an ADU is almost always a phrase in the tree, in which case it is somewhat less surprising that it helps this task (and maybe it should be helping even more). If this is the case, it’s worth considering related work in distant supervision to include.
The treatment of constituency trees is haphazard. 1) At times it is not clear if dependency or constituency trees are being used; 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all; 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree.
(low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.","3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree. (low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims. firstly, it suggests adding more work on constituency tree representation to the related work section, which is a subjective suggestion. secondly, it questions the claim that batching heterogeneous graphs is a ""major difficulty"" of the work, implying skepticism about the authors' assertion. however, the reviewer does not provide any justification or attempt to verify these claims. there is no explanation or evidence provided to support the suggestion of including additional related work or to substantiate the skepticism regarding the difficulty of batching heterogeneous graphs. therefore, the review point lacks verifiability as the claims are not supported by any form of justification or validation."
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","- the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets.",-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point makes a claim that the experiments conducted demonstrate the proposed method's effectiveness in terms of accuracy and interpretability for a significant majority of the studied datasets. however, the reviewer does not provide any explanation or attempt to verify this claim. there is no mention of specific results, examples, or any form of justification to support the assertion of effectiveness. therefore, the review point lacks verifiability as it presents a subjective statement without any supporting evidence or reasoning."
1212,ICLR_2023_4249,"Weaknesses: • The novelty of this paper might be slightly insufficient. My understanding is that the proposed model can be considered as an extension of previous GraphCL. Despite the introduction of cross-view reconstruction, it seems that cross-view reconstruction exists in previous graph representation learning studies. • The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions: • Why are two data augmentation graphs needed? • The proposed model is involved in multiple losses. How to determine the weight values of different losses? Could the fixed weight values be generalized to different datasets?","• The performance might be still somewhat limited. The authors compared many methods under different settings with GraphCV on multiple datasets. However, it can be found that GraphCV is comparable to or even underperforms the best baseline in many cases. While GraphCV has better performance than baselines in some cases, the advantages are not significant enough. Other questions:",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as the assertion that ""graphcv is comparable to or even underperforms the best baseline in many cases"" and that ""the advantages are not significant enough."" these are subjective statements that require justification. however, the reviewer does not provide any specific evidence or explanation to support these claims. they do not mention any data, examples, or reasoning to substantiate their opinion on the performance of graphcv compared to the baselines. therefore, the review point lacks verifiability as the claims are not supported by any attempt at justification."
4732,NIPS_2020_1809,"- Note sure whether the authors intend to release code also upon acceptance but the statement in line 270 is a little unclear. If code is only available during the review phase, this is a clear minus. - The degree of novelty is pretty small as the framework is well known and only a tiny aspect is changed. - The paper contains a lot of known material on the one hand but has a lot of references to the Appendix which makes the paper a little hard to digest. I would suggest to remove textbook material on EP in favor of including some more material on the Wasserstein distance. - That said, I'm not sure whether the page on the locality property is enlightning and really surprising. This could in principle be part of the Appendix and leave more space for an algorithmic discussion of the required computations for the variance update. - EP suffers from stability problems when the moment updates are not numerically accurate e.g. as a result of quadrature approximations. I'm missing a discussion on the numerical aspects of the L2 Wasserstein distance computations. - I'm missing a discussion on the marginal likelihood and its accuracy. - I'm missing a discussion of whether and how further derivatives of the site update can be computed in order to perform marginal likelihood hyperparameter optimization. - I'm missing a discussion why values for p different from 2 are not interesting to consider. - The manuscript does not provide evidence whether the proposed divergence measure is better suited in cases where EP has ""deficiencies"" according to the authors. MCMC experiments have shown that EP with KL is surprisingly accurate. The paper lacks a comparison in this respect. The missing convergence proof for EP is clearly an issue but the 2nd and 3rd paragraph seem as if EP is a buggy approach per se. Please provide concise and concrete examples where EP with KL is problematic and demonstrate that EP with WD is any better.",- I'm missing a discussion on the marginal likelihood and its accuracy.,0.0,-1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"the review point ""i'm missing a discussion on the marginal likelihood and its accuracy"" does not contain any claims. it is a general statement indicating the absence of a discussion on a specific topic within the paper. the reviewer does not make a subjective statement or suggest any changes; they simply note the lack of content. therefore, this review point does not require any justification or verification."
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ",- doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent.,1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains claims regarding the need to change specific wording in the paper, such as ""doomed"" to ""deemed"" and the suggestion to remove ""hence"" in line 74. these are subjective statements indicating the reviewer's opinion on the appropriateness of the language used. however, the reviewer does not provide any justification or explanation for these claims. there is no attempt to validate why ""doomed"" should be changed to ""deemed"" or why ""hence"" should be removed. the review point lacks any supporting information or reasoning to strengthen these claims. therefore, it does not meet the criteria for verifiability as outlined in the scoring rubric."
44,ICLR_2022_1016,"Weaknesses: 1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time. 2. For previous SVD-based and NS-based methods, computation processes for forward and backward propagations are consistent. However, this work adopts Matrix Taylor Polynomial or Matrix Pade Approximation for forward propagations and uses approximate Lyapunov equation for backward propagation, leading variance in forward and backward propagations. The authors would better make some discussions about this issue. 3. The authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it.
[r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021.
Other comments: 1. It is clear that MPA involves matrix inverse, which is very GPU-unfriendly. As stated in the paper: ""Moreover, we note that the matrix inverse can be avoided, as Eq. (13) can be more efficiently and numerically stably computed by solving the linear system"". The authors would better provide more detailed computation and analysis. 2. How to compute the coefficients of p m and q n
for the Matrix Pade Approximation in equation (12)? Do forward operations of MPA in Table 1 contain computation of coefficients p m and q n
? 3. Does Equation (2) lack a (·){sym} operation for / f r a c l U
? 4. Is equation (11) missing a term z^{k} in the left side? 5. I am not sure why sign(B) in equation (21) can be calculated as identity matrix? 6. P{M} and Q_{N} are used to approximate the Taylor series. If I am not misunderstanding, does I- Q_{N}^{-1}P_{M} replace Q_{N}^{-1}P_{M} in Eqn. (13)? and do the terms I-X replace X in Eqn. (12)?","1. In this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time.",1.0,1.0,-1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains a claim suggesting that the authors should compare mtp/mpa+ns based bp and ns based fp + lya in terms of accuracy and running time to determine which method is most important for fast matrix square root. this is a subjective statement that implies a suggestion for additional analysis or experimentation. however, the reviewer does not provide any justification or explanation for why this comparison is necessary or how it would contribute to the paper's findings. there is no attempt to validate the claim or provide reasoning for the suggestion, which is required for a score of 1. therefore, the review point lacks verifiability as per the criteria."
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","10. Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”).",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims, such as the suggestion to include a simple example of cross-domain translation difficulties and the request to summarize major directions for domain adaptation in nmt. these are subjective statements indicating the reviewer's opinion on what should be added to the paper. however, the reviewer does not provide any justification or explanation for these claims. there is no attempt to validate why these additions would improve the paper or why they are necessary. the review point lacks any supporting information or reasoning to strengthen the claims made. therefore, it does not meet the criteria for verifiability as the reviewer did not attempt to justify their claims."
914,ICLR_2023_1657,"Weaknesses:
The paper starts with grandiose claims of tackling ""open-ended learning"". However, open-ended learning involves learning to perform across diverse environments. But the definition of open-ended learning in this work seems restricted only to learning different skills in a given environment. In experiments, it is mostly restricted to goal conditioned environments and learning a goal conditioned policy.
""But where do goals come from? Almost always, they are sampled from a fixed distribution over a predefined goal space; i.e. they come from an engineer."" There are numerous works where the goals are NOT generated from a fixed distribution (listed in the references below)
The previous statement makes us believe that in this work, the goals are not generated from a fixed distribution. However, a few paragraphs later, the authors note that ""In this second challenge — the one we focus on — agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery."" i.e, this work focuses on learning a goal conditioned policy from pre-defined goals.
""In social episodes, a social partner suggests a novel goal to the agent and decomposes it into two consecutive sub-goals: 1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities."" The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments.
One of the contributions listed is: ""an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion"". This seems like a standard active learning setting and not a novel contribution.
References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018","1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities."" The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments. One of the contributions listed is: ""an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion"". This seems like a standard active learning setting and not a novel contribution. References: [1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020 [2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017 [3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021 [4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021 [5] Automatic curriculum learning through value disagreement. Zhang et al, 2020 [6] Exploration via hindsight goal generation. Ren et al, 2019 [7] Automatic goal generation for reinforcement learning agents. Florensa et al, 2018",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point contains several claims. firstly, it claims that maintaining a list of all discovered and to-be-reached goals is not tractable in most environments. secondly, it claims that the active learning mechanism described is a standard setting and not a novel contribution. however, the reviewer does not provide any justification or evidence to support these claims. there is no attempt to explain why maintaining such lists is not tractable or why the active learning mechanism is not novel. the references listed do not directly support these claims, as they are not used to substantiate the reviewer's points. therefore, the review point lacks verifiability as the claims are not justified or verified in any way."
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,0,"the review point ""only one generative model is tested here - do results change with other models? say a simple gmm or a sparse coding based one?"" does not contain any claims. it is a general question asking whether the results would differ if other models were used, such as a gaussian mixture model (gmm) or a sparse coding-based model. this is a factual observation about the paper's content, specifically noting the use of only one generative model, and it does not express an opinion or suggest a change. therefore, it does not require any justification or verification."
3390,NIPS_2020_875,"- Some of the claims made by the authors seem imprecise (see below), and the presentation could be more clear/streamlined. - Parts of the proposed approach seem somewhat ad-hoc, and would benefit from better empirical or theoretical motivation (see below). - Some of the baselines in the experiments seem weak or missing.",- Some of the baselines in the experiments seem weak or missing.,-1.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,-1,"the review point ""some of the baselines in the experiments seem weak or missing"" contains a claim that the baselines used in the experiments are inadequate or absent. however, the reviewer does not provide any explanation or attempt to justify this claim. there is no mention of specific baselines that are weak or missing, nor is there any reasoning or evidence provided to support the assertion. therefore, this review point lacks verifiability as the reviewer has not made any effort to validate their claim."
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?",- No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling.,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2482,NIPS_2021_277,"Weaknesses:
The first two rows in Table 1 show that the proposed ITC loss significantly improves the performance. I wish to hear discussions and comments on where the ITC could be useful. Is it specified to the proposed vision-transformer-related VLP models, or it could be extended to the Vilt-like structure [21] with linear projection only, or even to the conventional VLP methods [13,2,3] with detector features.
The ablation of the model architecture might be necessary to understand the source of improvements, i.e., from the proposed losses and distillation, or the change in model architecture and patch size. Specifically, it might be necessary to ablate 1) different model sizes, e.g., small, base, large; 2) different patch sizes and numbers, e.g., ViT/32, /16, and input image size; 3) the influence of model initialization in Sec 3.1.
A minor concern is the comparison to SOTA in Table 4. The compared methods are the BASE version of the SOTA [3,8]. The LARGE version of [3,8] yields similar performance to the reported numbers.
A comment: The contribution of the momentum distillation and the analyses in Section 4 limit at applying the related techniques (moving average teacher in semi-supervised learning [33], the mutual information maximization perspective in NLP [A]) onto the VLP methods. Nonetheless, the proposed techniques and analyses do help the VLP study.
[A] A mutual information maximization perspective of language representation learning
The limitations and societal impacts are discussed in the conclusion section.","2) different patch sizes and numbers, e.g., ViT/32, /16, and input image size;",-1.0,1.0,0.0,0.0,nan,nan,nan, This needs the rest of the review to annotate,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.",- What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision.,0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?","- The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2969,NIPS_2022_1846,"Weaknesses and Questions 1. For the distribution estimation, this paper uses three Gradient flow networks to learn different parameters. According to Section 4.3, the networks learn specific parameters for each class using different inputs (i.e., x ¯ j
). That is to say, the network F 1
will output c j j = 1 n for n
classes. However, in Line 142, the authors point that the parameter c
is shared between all classes. How to unify this c
? 2. How to update F 2
, and F 3
via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1
can be trained. 3. Some experimental details are missing. 3.1. What is the ratio of training data D t
to validation data D v
in the training stage. 3.2. What is the value of initial c
. 4. In Table 2, the metric-based baseline FEAT performs similar accuracy to the proposed method. It’s better to discuss the superiority of the method in terms of time consumption. According to Algorithm 2, the upper bound Eq. (14) simplifies the training of classifiers, but Eq. (16) is still difficult to compute. Typo: In Table 2, the description does not match the content, e.g., ""Euclidean Metric"" (or ""Hyperbolic Metric"") and ""Model"".",3. Some experimental details are missing. 3.1. What is the ratio of training data D t to validation data D v in the training stage. 3.2. What is the value of initial c .,0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3661,NIPS_2020_1466,"- Regarding the comparisons in Table 1., I would have liked to see comparisons with [9], [10] and/or [11]. SMPL and ASAP are good, but they are blended with an initialization from the authors' method and a comparison with DHBC alone is not completely convincing. - The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data. - The authors decided to limit themselves to human shapes in this work, although there are no modelling assumption that clearly require this. While I can to some extent understand this decision, it would still be interesting to see some qualitative generalization results to other classes of shapes. I understand that your method requires the human template for training, but similar methods [11] also show at least some examples that are not human or only close to human shape.","- The method requires the ground truth correspondences as supervision which is a strong assumption. Many related methods ([11, 13, 40] in the paper) investigated the unsupervised setting at least to some degree and therefore allow for training on non-synthetic data.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
315,ICLR_2022_2903,"Weaknesses / Questions for authors ===
""Coherence"" seems to be an important topic of the paper. It is mentioned often, and Section 3.1 provide a formal definition of epsilon-coherence. However, in the empirical results this didn't seem to be mentioned anymore. It made me wonder: what is the use of the formal definitions in Section 3.1? I was expected this to be used to evaluate the models. Maybe it did, but then it wasn't clear to me, so in any case I think the relation between the experiments and Section 3.1 can be made clearer.
I found the explanation of the newly proposed relation decoder the ""Dynamic Comparator"" too short. It was difficult for me to understand what it did. More importantly: it seems to outperform all other models very convincingly, but I could not find any discussion on why you believe this is the case. So I felt the empirical results deserve more detailed reflection. Perhaps an ablation study would be interesting?
The empirical studies by itself are also somewhat one-sided (MNIST --> BlockStack). Did you also try the other direction? I.e., train on the BlockStack domain and then evaluate on MNIST? More generally, I was wondering whether the empirical setup is actually similar to how we humans learn. Can we really learn a relation such as ""larger than"" from one domain, and can we directly apply it to another? This is actually not directly clear to me. I can imagine we learn a relation in multiple domains, after which we are able to generalize to new domains.
All appendices seem to be missing from the PDF.
=== Detailed comments for improving the paper ===
Section 2:
I liked the formalisation and I think it was well presented, also the example was helpful.
Section 3:
""Z is a latent variable, itself draw from marginal p_z"" --> ""draw"" should be ""drawn"". Also, I believe p_z is a prior instead of a marginal.
In Definition 3: ψ S ( S )
is undefined, I supposed it means ψ S ( S ) = ψ S ( s ) s ∈ S
, i.e. all encoding of S?
It could be helpful to add a sentence explaining the measure on substructures, e.g., saying that this value will be close to 1 if the soft-structure is similar to the structure.
""It is straightforward to show that ∑ S σ . . . = 1
"" --> what are we summing over here? All possible interpretations of \sigma use S? Please clarify this.
""If we have a theory \Tau over \sigma"" --> I don't get this, isn't a theory defined over the language which contains the relations?
Section 4:
Overall I found this section quite difficult to follow and I wasn't sure the details were relevant (but I could be wrong here). You could consider removing some technical details here (and moving them to the appendix), so that you have more space for possible additional experiments.
The grounding s i j k
is used but only in the next paragraph it is explained what it means, so please define it beforehand.
It is not explained what an ""atomic subformula"" is.
Section 5:
""given a domain images"" --> remove ""a""
""latent space ."" --> remove space
The explanation on the DC is extremely dense, and I'd like to understand better why you developed it this way and why you think it works well.
Section 8:
""ordinaliety"" --> ordinality",".. = 1 "" --> what are we summing over here? All possible interpretations of \sigma use S? Please clarify this. ""If we have a theory \Tau over \sigma"" --> I don't get this, isn't a theory defined over the language which contains the relations? Section 4: Overall I found this section quite difficult to follow and I wasn't sure the details were relevant (but I could be wrong here). You could consider removing some technical details here (and moving them to the appendix), so that you have more space for possible additional experiments. The grounding s i j k is used but only in the next paragraph it is explained what it means, so please define it beforehand. It is not explained what an ""atomic subformula"" is. Section 5: ""given a domain images"" --> remove ""a"" ""latent space ."" --> remove space The explanation on the DC is extremely dense, and I'd like to understand better why you developed it this way and why you think it works well. Section 8: ""ordinaliety"" --> ordinality",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1305,ICLR_2023_1603,"Weaknesses - 1. The main drawback of the design is that it relies heavily on a centrally available dataset. One of the two primary goals of the system is to handle non-IIDness in the data, which raises the question - how does the performance of F2L depend on the quality of the root dataset at the server. How well does the root dataset represent the non-IIDness present among the clients? How is scalability affected if the root dataset is not updated to well represent the newly joined clients? More experiments are required to convince the reader that the system can do well even when the root dataset does not exactly represent the data distribution among the clients. 2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance? 3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature? 4. F2L relies on switching between LKD and FedAvg after sufficient convergence has happened. How is this threshold chosen? What can be a general way to choose this value for any dataset? 5. Figure 3 shows that a student can outperform a teacher in F2L. This experiment was performed on EMNIST. Does this observation hold in general, independent of the dataset? If not, what conditions does this depend on?",3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4665,NIPS_2020_789,"- My main concern is that, I don’t see the benefits of modeling the data as a union of subspaces, where each subspace corresponds to a class, when the representation space is *learned*. In particular, since these subspaces won’t be orthogonal in practice, on real data. In an unsupervised setting, to recover the subspaces, one needs to perform subspace clustering, which is a hard problem and computationally expensive to perform. In a supervised setting, where estimation of the subspaces is easy, one needs to do nearest-subspace-classification which is more intricate than linear classification. In stark contrast, a linear head trained with a cross-entropy loss learns a representation space with approximately linearly separable regions for each class. As a consequence, classification is simple (linear) and Lp distances in representation space are meaningful (which is not necessarily the case when the classes lie on a union of subspaces). - I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this. - While the theoretical analysis reveals interesting properties of the learned representation, it completely ignores the relationship between the individual data points and their representation, defined through the feature extractor. It is well-known that the structure and properties of the extractor crucially impact the learned representation, possibly even more than the loss, see e.g. [ZF14]. [c] Elsayed, Gamaleldin, et al. ""Large margin deep networks for classification."" Advances in neural information processing systems. 2018. --- Update after rebuttal: Thanks to the authors for their response. I now better see the benefits of encouraging orthogonality between class regions in the feature space, which is why I increased my rating. However, I'm still not sure whether the theoretical result is useful to explain what is going on, as I still believe the network architecture is crucial for the structure in the feature space. Furthermore, as pointed out by the other reviewers, the method seems to have many similarities with previous methods, which should be discussed more precisely.","- I acknowledge the encouraging results regarding robustness of the representations learned with the proposed method. However, there are many other methods which can make neural networks with linear classification head more robust, for example [c]. Therefore I believe a union of subspace structure is not fundamentally required to achieve this.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?",6. What do error bars represent? How often were experiments repeated with different random seeds?,0.0,0.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5646,NIPS_2018_809,"Weakness: - The uniqueness of connecting curves between two weights would be unclear, and there might be a gap between the curve and FGE. A natural question would be, for example, if we run the curve findings several times, we will see many different curves? Or, those curves would be nearly unique?  - The evidences are basically empirical, and it would be nice if we have some supportive explanations on why this curve happens (and whether it always happens). - The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding)  Comment: - Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research. - To be honest, I'm not sure the first curve-finding part explains well why the FGE work. The cyclical learning rate scheduling would perturb the weight around the initial converged weight, but it cannot guarantee that weight is changing along the curve described in the first part.","- The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding) Comment:",-1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2557,NIPS_2019_220,"Weaknesses: 1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method. 2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference. 3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer) ","3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5336,NIPS_2020_814,"- Only applies to overdetermined least-squares (LS) problems, which is a bit uninteresting at this point. We can solve these quite well already. - Practical performance improvement by using orthogonal transforms is slight (only apparent as sketch becomes larger, where it is less useful; and in the regimes studied by numerical simulations, the decay factor rho was very small for all methods, so convergence was super fast for everything and the differences were not that great) - Everything is in the asymptotic regime, which is not obviously useful, and not very standard for sketching results - The language of the paper oversells things, since it keeps referring to things as ""optimal"" without qualifying that everything is asymptotic.","- Practical performance improvement by using orthogonal transforms is slight (only apparent as sketch becomes larger, where it is less useful; and in the regimes studied by numerical simulations, the decay factor rho was very small for all methods, so convergence was super fast for everything and the differences were not that great) - Everything is in the asymptotic regime, which is not obviously useful, and not very standard for sketching results - The language of the paper oversells things, since it keeps referring to things as ""optimal"" without qualifying that everything is asymptotic.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5043,NIPS_2020_443,"- Apart from ridge regression, there are some regression models like lasso regression the authors might try. - For Table 1, the notation should be aligned with its description. - Overall, I am quite curious why 2v2 accuracies for all hypotheses are just slightly better than the random chance. Why is that? Correct me if I am wrong, thanks.","- For Table 1, the notation should be aligned with its description.",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3366,NIPS_2020_639,"The relevance of this paper is entirely unclear, for multiple reasons: 1. The author themselves state ""This work does not present any foreseeable societal consequence."", raising the question why we should we care about this work in the first place. 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise. 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.","3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.",0.0,-1.0,-1.0,-1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5592,NIPS_2018_55,"weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved. 1) The statements in the MNIST experiment such as ""While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable."" are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims. 2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.  3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"". 4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions:   I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence.   line 218: an CAE -> a CAE   line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.","4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions: I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence. line 218: an CAE -> a CAE line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019. However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.",0.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
71,ICLR_2022_1155,"Weaknesses: 1. This work is motivated by “the common misconception that adversarial examples are high-frequency noise”; however, such an understanding has already been questioned in the literature. For example, in (Tsuzuku & Sato, 2019), it shows “adversarial perturbations do not necessarily lie in high-frequency spots” by experiments; in (Yin et al. 2020), it says “adversarial examples are not strictly a high frequency phenomenon”; particularly in Bernhard et al. (2021), it questions “some preconceived hypothesis” that “adversarial perturbations as a pure HSF phenomenon with data-agnostic spatial frequency characteristics”. Therefore, the motivation should be better justified. The contribution is more like additional evidence of the ongoing debate, but rather a new frequency-based understanding of the “common misconception”. This should not be overclaimed. 2. This work reveals that adversarial examples are dataset dependent; however, this is a commonly accepted point in the community, and therefore the idea and the conclusion do not seem new. In addition, the demonstration of this point with only CIFAR-10 and ImageNet-derived datasets does not seem sufficient. More datasets should be considered including the simplest one (MNIST), and others such as CIFAR-100, SVHN, Fashion-MNIST. 3. Only PGD attacks are investigated – it is unclear if the observations also occur with other attacks, e.g., C&W, auto-attack. 4. It claims the “observations overlap with insights from the concurrent work by Bernhard et al. (2021)”; however Bernhard et al. (2021) appeared on arXiv in April 2021. The authors may want to highlight the differences and justify this point. According to my understanding, the main messages of two papers are the same. This will dwarf the contribution of this paper. 5. When measuring the importance of different frequency components and attacking low-frequency components, how to guarantee the imperceptibility? Should not the threat models be re-defined? How to make the trade-off between imperceptibility and attack successfulness? 6. The measure of average noise gradient over the entire dataset should be further justified. Why is not the average noise gradient of a specific class, given the intuition that the frequency properties of images from different classes may be so different that the averaging may be misleading. 7. When designing adversarial training with frequency-based perturbation, how to find the frequency subspace? How is PGD attack restricted to the same frequency bands?","5. When measuring the importance of different frequency components and attacking low-frequency components, how to guarantee the imperceptibility? Should not the threat models be re-defined? How to make the trade-off between imperceptibility and attack successfulness?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks:",-1.0,0.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
597,ICLR_2021_2678,"Weakness –
W0 – There is no comparison to existing metrics, e.g., IS and FID to clearly show the advantages as the authors claimed in the paper, e.g., fewer samples and low variance. I think people now are using the combination of IS and FID in their experiments to measure quality and diversity. How are these two new metrics better than the existing combination? There is a bit disconnected between the concepts of the three proposed metrics, and I think it's still fair to compare the proposed metrics (visual quality and mode diversity) with the combination of IS and FID.
W1 – The visual quality and mode diversity metrics use the face detection/verification frameworks which are quite specific to face datasets that have ground-truth labels. Can they apply to other domains rather than faces? If so, does it cause any bias?
W3 – What is X − 1
in Eq. 6. Is it the inversion of matrix determination or number the division of the number of samples?
W4 – Mistakes in Eqs. 3 and 10 and Table 3, should be gradient of ∇ x ^
instead of ∇ η .
W5 – It looks like the paper is a bit rushed in this submission, there are sufficient missing details of implementations, e.g., hyper-parameters, batch size, detail of architecture, … are not provided. These factors may also have effects on the training of GAN. It would be interesting to have studies on these in future work.
W6 – The paper's experiments are limited to one low-resolution dataset with standard GAN architectures. It's important to evaluate various GAN architectures and datasets for GAN assessment papers. It would be interesting to conduct the experiments also on state-of-the-art GAN models, e.g., SN-GAN, BigGAN, ProGAN, and StyleGAN, and with high-resolution datasets as well.
W7 - Some metrics are interesting and likely to be valuable in the future, but can be more polished on experiments and comparison. It would be great if the current measures can be developed to extend for other datasets, e.g., CIFAR, ImageNet.
W8 – The paper has derived some mathematics of turning points, but what is its meaning and how is it useful to derive this equation?","3 and 10 and Table 3, should be gradient of ∇ x ^ instead of ∇ η . W5 – It looks like the paper is a bit rushed in this submission, there are sufficient missing details of implementations, e.g., hyper-parameters, batch size, detail of architecture, … are not provided. These factors may also have effects on the training of GAN. It would be interesting to have studies on these in future work. W6 – The paper's experiments are limited to one low-resolution dataset with standard GAN architectures. It's important to evaluate various GAN architectures and datasets for GAN assessment papers. It would be interesting to conduct the experiments also on state-of-the-art GAN models, e.g., SN-GAN, BigGAN, ProGAN, and StyleGAN, and with high-resolution datasets as well. W7 - Some metrics are interesting and likely to be valuable in the future, but can be more polished on experiments and comparison. It would be great if the current measures can be developed to extend for other datasets, e.g., CIFAR, ImageNet. W8 – The paper has derived some mathematics of turning points, but what is its meaning and how is it useful to derive this equation?",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2228,ACL_2017_130_review.json,"Weaknesses: The paper suffers from several drawbacks 1. The paper is hard to read due to incorrect usage of English. The current manuscript would benefit a lot from a review grammar and spellings. 
2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100? 
3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence? 
4. The datsets are from a biomedical domain. No domain specific tools have been leveraged. 
5. Since dataset class distribution is unclear, it is unclear to determine if accuracy is a good measure for evaluation. In either case, since it is a binary classification task, F1 would have been a desirable metric. 
6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant. ","2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100?",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5538,NIPS_2020_1623,"1. Lacking insight on setting the hyper-params: Table 2 (b) shows E_s = 60 will have the best performance but in the experiments in Section 2.4, E_s is set to 70. Is there any insight or explanation on the reason why to do such changes? 2. Not enough model structures in the experiments to support the conclusion: As mentioned by one of the baselines [24], previous works utilize a various set of different architectures, which hinders a fair comparison. And this paper only uses ResNet34@CIFAR10/100, and ResNet50@ImageNet. Although this paper reproduces some baselines in the setting above, it would be better to provide more comparison on other model structures and use the reported numbers in the baselines.","2. Not enough model structures in the experiments to support the conclusion: As mentioned by one of the baselines [24], previous works utilize a various set of different architectures, which hinders a fair comparison. And this paper only uses ResNet34@CIFAR10/100, and ResNet50@ImageNet. Although this paper reproduces some baselines in the setting above, it would be better to provide more comparison on other model structures and use the reported numbers in the baselines.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","7. Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.",- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2040,ARR_2022_143_review,"Weak: 1. 	More examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. 	Some higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. 
Detailed questions and comments: 1. 	Table 1, what are the hardware used? Model sizes? For “speed comparison”. 
2. 	Figure 1, what are the labels for horizontal and vertical axis? 
3. 	Lines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? 
4. 	Can you give real-world examples and attach them to your Figure 2? 
5. 	Figure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. 
6. 	Lines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? 
7. 	Line 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. 
8. 	Table 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? 
9. 	Table 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? 
10. 	Section 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ","2. Figure 1, what are the labels for horizontal and vertical axis?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4364,NIPS_2020_285,"- In Section 2, it is assumed that the state and action spaces are finite. Is this assumption really necessary? It might be quite limiting since policy gradient methods are typically employed when dealing with continuous state-action spaces. Moreover, the transition model is defined as deterministic. Is this assumption necessary? - Proposition 6: I am a little confused about the notation. Equation (13) employs the improvement operator for the value-based case, but the remark in the subsequent line is stated for the improvement operator for the trajectory-based case. - Proposition 5: This is more curiosity than an issue. Are there some sufficient conditions to enforce that Var(R) > 0 along the whole learning process? It seems to me that as we get close to the optimum we are going to prefer less stochastic policies, thus we slow down convergence. Do you think we can still converge asymptotically if deterministic policies are allowed? Anyway, in policy search, we could even limit to stochastic policies. Maybe in such a case, we can have a non-zero guaranteed improvement and, consequently, converge in a finite number of iterations. I think the paper would greatly benefit from a discussion on these points. - Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers","- Proposition 2: This is also a curiosity. The optimal policy, in the considered policy space, is a fixed point of the operator. There can be other fixed points? If so, do the authors think that is possible to characterize the space of fixed-points? Are there some conditions under which the fixed point is unique? ***Minor*** - lines 50 and 62: s_{t_1} -> s_{t+1} - Equation (3) goes beyond margins - line 64: reporting the formal definition of d^\pi might help - Equation (24) there should be a \propto instead of = - Proposition 5: there should be a statement, not just a formula - The notation of Proposition 9 does not match that used in the proof (z vs f(R)) - Figure 2: not very readable in grayscale, I suggest using different linestyles or markers",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1822,ARR_2022_288_review,"-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. 
The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ","- While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1822,ARR_2022_288_review,"-	While this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. 
-	Although ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. 
-	The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. 
The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. 
The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. 
Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ","- The paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case. The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. The advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. Minor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases?",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5776,NIPS_2018_600,"weakness of the non-local (NL) module [31] that the correlations across channels are less taken into account, and then formulate the compact generalized non-local (CGNL) module to remedy the issue through summarizing the previous methods of NL and bilinear pooling [14] in a unified manner. The CGNL is evaluated on thorough experiments for action and fine-grained classification tasks, exhibiting promising performance competitive to the state-of-the-arts. Positives: + The paper is well organized and easy to follow. + The generalized formulation (8,9) to unify bilinear pooling and non-local module is theoretically sound. + Good performance. Negatives: - Less discussion on the linear version of CGNL using dot product for f. - Missing fundamental comparison to the simple ResBlock. The authors nicely present the generalized formulation toward CGNL by unifying the two previous works of bilinear pooling and non-local module. Though the kernelized (non-linear) correlation function f is well theoretically motivated, the actual form of f that achieves the better empirical performance is a âlinearâ form (dot product). In this regard, the reviewer has the following concerns. - Less discussion about the linear form. If the reviewer correctly understands the CGNL formulation, the linear function f of dot product f (line 204) can greatly simplify the CGNL into Y = X * W_theta * tr[(X*W_phi)â * (X*W_g)] = X * W_theta * tr[(XâX)* W_g* W_phiâ]  = s * X * W_theta, where s = tr[(XâX) * W_g * W_phiâ]= tr[(XâX)* W] is just a scalar and W = W_g*W_phiâ. This reformulation would be beneficial from the following viewpoints. > It reduces the parameters from {W_theta, W_phi, W_g} to {W_theta, W}, which facilitates the implementation. > It is closely related to squeeze-and-excitation (SE) module [9]. The above formulation can be regarded as a bilinear extension of SE from âsqueezeâ viewpoint since it âsqueezesâ the feature map X into the bilinear form of XâX while SE simply employs an average-pooling.  Such discussions as above would help the readers to further understand the methods and to further extend the method. - Missing comparison. Based on the above discussion, one can think that the baseline for the linear CGNL is a simple ResBlock of Z = BatchNorm( X * W_z ) + X, while the linear CGNL is Z = BatchNorm( s * X * W_theta * W_z ) + X  = BatchNorm( s * X * W_tz ) + X. The only difference is the scaling factor s that is also build on X. Through batch normalization, such a scaling might be less effective (during the training) and thus by comparing these closely-related methods, the authors have to clarify its effectiveness of CGNL empirically. Due to this concern, the reviewer can not fairly evaluate the impact of the method on classification performance. [After Rebuttal] The reviewer appreciates the authorsâ efforts to perform the comparison experiments in such a short rebuttal period. The comparison with the standard ResBlock clarifies the effectiveness of the proposed method as well as helps us to further understand how it works. ",+ Good performance. Negatives:- Less discussion on the linear version of CGNL using dot product for f.,0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1193,ICLR_2023_1294,"weaknesses:
1: The best feature of CLIP is the generality, that is, being able to recognize any image without pre-defined/fixed classes. MUST adapt the CLIP model to a specific dataset (which is the main purpose of this paper). A simple solution is claimed by authors in the limitation section: ""There exists a simple way to address this concern: gather unlabeled image from all the domains of interest, and perform MUST to learn a single model that can generalize to multiple domains."" It would be really great to have 1-2 such experiments to verify this hypothesis.
Minor weaknesses:
2: The paper does not reach out to theoretical backup to explain why MUST works.
3: Adding results on ImageNet-Sketch will further strengthen this paper.
4: A related work [1] is worth discussing.
[1]: Test-time training with masked autoencoders, NeurIPS 2022",2: The paper does not reach out to theoretical backup to explain why MUST works.,0.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4288,NIPS_2020_251,"* The NF assumption was not discussed as compared to a standard SSM which uses additive measurement noise. Placing the emission noise *before* the nonlinearity is a crucial move; otherwise filtering is not tractable. I would have appreciated further discussion of the impact of this. It's possible that this technique can be applied as a drop-in replacement in many models to avoid awkward approximations such as EKF, UKF and PF; however this conclusion is not immediate from the work presented in this paper. * As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning. * The qualitative experiments seemed particularly artificial; I did not learn much here beyond the fact that the implementation broadly seems to work. If these are indicative of a real-world problem, it would be helpful to make this clearer. * NKF does not show markedly better performance than the GP-Copula model in the main experiments.","* As a simple example, consider a univariate example where $f$ is a sigmoid, and the true $z = -5$, hence $\E[y] ≈ 0$. If $y$ is observed with additive noise of +0.2, the inferred $z = f^{-1}(0.2) ≈ -1.4$, which may cause substantial problems for inference and learning.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
352,ICLR_2022_2232,"Weaknesses
The writing should be improved overall. Issues throughout include grammar, misuse of commas, capitalization (as in reference to algorithm 1 on page 6), technical details left out, poor formatting.
There seems to be a lack of novelty in the modeling approach: the same training schemes, DNN model, etc have been developed before, in papers that are cited by this one.
The results are rather limited and do not seem to show a clear advantage over standard techniques.
Some sentences have unclear meaning, e.g.: Page 1 - ‘a manual adaptation and business knowledge are needed…” - ‘its dynamic dimension reflects directly the demand change over the learning steps…” Page 2 - ‘Before we deep dive in the model architecture and present its main components, we should briefly highlight some problem-related concepts’ - the narration is too casual Page 5 - Equation 13 label is cut off - means that violation is given -infinity in what sense? A large floating point number? Page 6 - Training: Sample rollout and greedy rollout - are these ever defined? What is the baseline function used in the REINFORCE algorithm? Page 7 - Google OR-Tools baseline - what are the details of the implementation? Is it the CP-Sat solver? A specialized solver? - What is the meaning of the numbers in Table 1? Is lower better? - Why does RNN-RL appear twice in Table 2 with different results? - Is OR-tools called with a solver timeout? Or it is allowed to run to completion? - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method?
General questions:
Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case?
Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper?
What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.","- The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method? General questions: Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case? Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper? What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.",1.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.",1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.",- Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way?,0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5136,NIPS_2020_44,"The main weaknesses which make me a little less excited about this work are mostly related to the scalability of the algorithm(s) and the applications of their techniques. - Scalability: Barrier-Greedy++ is not scalable but achieves a slightly better approximation (by a factor of 2) compared to the relatively more scalable Barrier-Greedy. However, Barrier-Greeedy itself though is linear in n, is cubic in r (which is the size of the largest feasible set). Even if r is a fraction of n (say, 10% of n), the algorithm is still cublc in n which could be very slow for n in range of a million instances. - Motivating applications: Given that greedy algorithm already achieves a factor of k for multiple matroid constraints (which occur mostly in practice), I do not see this algorithm being useful in only the matroid case. Moreover, I do not see any guarantees in the *only* knapsack case (i.e. if there is no matroid constraint). As far as I have seen even in the related work of Mirzasoleiman et al and Feldman et al, they either have only matroid constraints or only knapsack constraints. I would really like the authors to clarify this. In my opining, this paper would be significantly strenghted if this algorithm applies to only multiple knapsack constraints. It seems the algorithm also has an additional assumption of (l \leq k) whcih the authors later relax (but this part was not super clear in the paper).","- Scalability: Barrier-Greedy++ is not scalable but achieves a slightly better approximation (by a factor of 2) compared to the relatively more scalable Barrier-Greedy. However, Barrier-Greeedy itself though is linear in n, is cubic in r (which is the size of the largest feasible set). Even if r is a fraction of n (say, 10% of n), the algorithm is still cublc in n which could be very slow for n in range of a million instances.",-1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.","- The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner?",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
196,ICLR_2022_1224,"Weakness] 1. One of the experimental results can be interpreted differently so that one of the contributions can be just a misreading of the experimental results. 2. Some parts of the paper require more explanations and formal descriptions about the concepts.
[Comments] 1. About Figure 2b, as far as I understand, the authors tested two attack methods (single-point attack and multi-point attack) against two ResNet50 models (regular and robust) over the same set of target data points. (If they are tested on different sets of target data points, it could be an unfair comparison.) Then, against the regular ResNet50 model, the single-point attack can find much better perturbations (than the multi-point attack) for the same set of target data points. Doesn’t this just mean that “the multi-point attack is not effective against the regular ResNet50 model” rather than “adversarially trained model is more vulnerable (than the regular ResNet50 model) against the multi-point attack”? In other words, the main contribution that “certain black-box attack can perform better against adversarially trained model” is just a misreading of the experimental result and there is another possible interpretation that “the multi-point attack is not effective against the regular model”. (The reason for this another interpretation could be the reason that is provided by the authors; a regular model has a less smooth boundary.) Also, the comparison with DeepFool (Figure 3b) says that the robustness gain is bigger for white-box attacks (constantly 17) compared to other black-box attacks. However, since the robustness gain is defined as a ratio, this value can be small just because black-box attacks cannot find a small enough perturbation for regular models. Again, we still don’t know whether this is because of the vulnerability of adversarially trained models or because of the poor performance of the black-box attacks. I suggest the authors check the average perturbation sizes to show that the black-box attacks do not perform poorly against the adversarially trained model. 2. First, robustness gain is only briefly described in the introduction and I cannot see any formal definition of it. Even though experiments are the only way to compute the robustness gain, it is better to describe it formally (an ideal & theoretical definition. If there is any reference that supports the concept, cite it.) and explain the intuition behind the concept. Second, more details are needed for the robustness gain experiment. The main motivation for this experiment is unclear. η
(which must be the robustness gain) was not defined before it is mentioned. 3. Lastly, I don’t think that robustness gain is a good measure to make comparisons. This is because the robustness gain is a ratio so it can be decreased by the poor performances of black-box attacks against a regular model. In other words, it is unclear whether the results come from the vulnerability of an adversarially trained model against black-box attacks or the results just come from the poor performance of the attacks against a regular model.","2. First, robustness gain is only briefly described in the introduction and I cannot see any formal definition of it. Even though experiments are the only way to compute the robustness gain, it is better to describe it formally (an ideal & theoretical definition. If there is any reference that supports the concept, cite it.) and explain the intuition behind the concept. Second, more details are needed for the robustness gain experiment. The main motivation for this experiment is unclear. η (which must be the robustness gain) was not defined before it is mentioned.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ","- Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret?",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
280,ICLR_2022_586,"weakness I see is the way of presenting the results in Fig. 3. All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this article.","3. All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this article.",0.0,-1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1646,ICLR_2023_1255,"Weakness:
The description of the developed methodology is unclear. It is hard to follow the work. The notation and equation formulation are complicated. Together with the corresponding explanation, they should be largely improved.
The long range information is the main advantage of the proposed method, however, there are no corresponding experimental discuss or theoretical analyses in the manuscript.
The computation complexity analysis is a big concern. The proposed method uses more parameters and computation steps during message passing, this would bring lots of computation burden. However, from the real dataset experimental results in table 4, the performance promotion is slight. 4 The novelty isn't particularly high. Most techniques used were proposed and demonstrated before.
How about experimental results on the PDBbind dataset.",4 The novelty isn't particularly high. Most techniques used were proposed and demonstrated before. How about experimental results on the PDBbind dataset.,-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4450,NIPS_2020_1451,"1. Unlike the works HaoChen and Sra and Nagaraj et.al, this work uses the fact that all component functions f_i are mu strongly convex. 2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples. 3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.",2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples.,1.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4812,NIPS_2020_1627,"- While the theoretical results seem correct, it is not clear to me the advantages of this approach compared to previous work, in particular, gradient Q-learning (GQ). On line 110, it is written that the assumptions are not as stringent but I am not convinced that this is the case. Could the authors clarify this point? - Assumption 1 does not seem very natural to me. If I am interpreting it correctly, it assumes that we have a fixed replay buffer of data on which we are doing updates, as in the offline batch RL setting. It is not specified which policy is used to collect this data and I would expect certain assumptions on this behavior policy. - I do not think assumption 4 would be extremely realistic in practice, but I think this is acceptable in this case since it seems like this assumption is mainly made to provide more insight on the performance of the algorithm (in a special case). As such, I would be more comfortable if it was presented as such, instead of trying to justify it in practice. For example, line 196: ""We note that Assumption (IV) does not impose any additional constraint on the features considered, since we can make them orthogonal and scale them to ensure that the latter assumption holds."" I do not find this argument convincing since we may be given features that do not satisfy this condition in practice or features may be changing over time if they are being learned. - I would have appreciated some more explanation concerning the intuition of the two-timescale updates. Line 121: ""...but instead match the projection of the output along the feature space, much like the “pre-conditioning” step from [1]."" seems to hint at the idea but it is not clear to me exactly what this means. - One of the contributions is listed as ""A better theoretical understanding for the use of the target network in DQN."" (line 112). I think the paper could have included more explanations about this point. Currently, I cannot find much discussion about how CQL informs us about DQN. - For the empirical experiments, it is not clear how the hyperparamters were selected. On line 222, it is simply written what the step sizes are but the algorithms can be sensitive to this choice. It would be much more informative if sweeps over the two step sizes were done and the algorithms' sensitivity to them was assessed. Full parameter sweeps seems feasible in this case due to the simplicity of the environments. - For the mountain car experiments, it is surprising that only 3 training runs were done (line 255) for each algorithm. I would expect many more for such a simple environment, say, at least 10. Currently, the standard errors are very large and it is difficult to make meaningful comparisons between the algorithms.","- I would have appreciated some more explanation concerning the intuition of the two-timescale updates. Line 121: ""...but instead match the projection of the output along the feature space, much like the “pre-conditioning” step from [1]."" seems to hint at the idea but it is not clear to me exactly what this means.",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5493,NIPS_2020_393,"Even though the paper is well written, simple to follow and provides an extensive set of experiments. However, the paper is relatively dense and a lot of further insight can be found in the appendix (which could almost already make another paper). Nevertheless, certain aspects of the analysis seem to be brushed over or left out: - Section 4 starts with the statement that it has been shown that deep ensembles indeed perform Bayesian Model Averaging, however no part of Section 3 directly addresses this point. It is clear that ensembles perform some sort of model averaging, however it would be interesting to further discuss the form q(w|D) takes in deep ensembles and which implicit assumptions about the prior are made and whether deep ensembles recover the true p(w|D) in the limit of infinite models in the ensemble (Sampling perspective?) or whether they are closer to a variational approximation and which implicit assumptions are made about the form of q(w|D). - Fig 3 offers a comparison of different predictive distributions and it is claimed that an increase in samples does not improve the estimate of the predictive distribution for SVI. However, I believe that this is an unfair comparison as this estimate is strongly influenced by the form of variational approximation used. Therefore, an approximation with little expressive power can only gain little improvements by further samples, whereas more complicated variational distributions might cover multiple modes in the weight distribution and could benefit from more samples. This should me more thoroughly examined.","- Section 4 starts with the statement that it has been shown that deep ensembles indeed perform Bayesian Model Averaging, however no part of Section 3 directly addresses this point. It is clear that ensembles perform some sort of model averaging, however it would be interesting to further discuss the form q(w|D) takes in deep ensembles and which implicit assumptions about the prior are made and whether deep ensembles recover the true p(w|D) in the limit of infinite models in the ensemble (Sampling perspective?) or whether they are closer to a variational approximation and which implicit assumptions are made about the form of q(w|D).",1.0,1.0,1.0,1.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2578,NIPS_2019_1348,"Weaknesses: 0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven. 1. The paper suggests that using EHRM has fairness implications. These fairness implications are obtained as a side effect of using different hyperparameter setting for the skewness of the human risk distribution. There is no direct relationship between fairness consideration and the risk metric used. 2. In the Introduction, the authors choose to over-sell their work by presenting their work as a ""very natural if simple solution to addressing these varied desiderata"" where the desiderata include ""fairness, safety, and robustness"". This is a strong statement but incorrect at the same time. The paper lacks any connection between these objectives and the proposed risk metric. One could try to investigate these connections before claiming to address them. 3. One example of connection would be the definition of Calibration used in, for example, Kleinberg et al. and connect it to a human calibration measure and derive a Human risk objective from there as well. It is a straightforward application but the work lacks that. 4. There are no comparison baselines even when applying to a fairness problem which has a number of available software to get good results. Agarwal 2018: ""A Reductions Approach to Fair Classification"" is seemingly relevant as it reduces fairness in classification to cost-sensitive learning. In this case, the weighting is done on the basis of the loss and not the group identities or class values, but it may be the reason why there is a slight improvement in fairness outcomes. Since the EHRM weights minorities higher, it might be correlated to the weights under a fair classification reduction and hence giving you slight improvements in fairness metrics. 5. There were a few typos and some other mistakes: - doomed -> deemed (Line50) - Line 74: Remove hence. The last line doesn't imply this sentence. It seems independent. ","0. My first concern is the assumption that a human risk measure is gold standard when it comes to fairness. There are many reasons to question this assumption. First, humans are the worst random number generators, e.g. the distribution over random integers from 1 to 10 is highly skewed in the center. Similarly, if humans perceive a higher risk in the tails of a distribution, it doesn't necessarily mean that minimizing such risk makes the model fair. This still needs to be discussed and proven.",1.0,1.0,1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5748,NIPS_2018_394,"Weakness: - The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP). - As the authors also implied, MC is not enforced in the âright wayâ (columns not normalized). I would like the authors to report their MC performance with column normalization for completeness. ","- The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP).",-1.0,0.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2969,NIPS_2022_1846,"Weaknesses and Questions 1. For the distribution estimation, this paper uses three Gradient flow networks to learn different parameters. According to Section 4.3, the networks learn specific parameters for each class using different inputs (i.e., x ¯ j
). That is to say, the network F 1
will output c j j = 1 n for n
classes. However, in Line 142, the authors point that the parameter c
is shared between all classes. How to unify this c
? 2. How to update F 2
, and F 3
via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1
can be trained. 3. Some experimental details are missing. 3.1. What is the ratio of training data D t
to validation data D v
in the training stage. 3.2. What is the value of initial c
. 4. In Table 2, the metric-based baseline FEAT performs similar accuracy to the proposed method. It’s better to discuss the superiority of the method in terms of time consumption. According to Algorithm 2, the upper bound Eq. (14) simplifies the training of classifiers, but Eq. (16) is still difficult to compute. Typo: In Table 2, the description does not match the content, e.g., ""Euclidean Metric"" (or ""Hyperbolic Metric"") and ""Model"".","2. How to update F 2 , and F 3 via minimizing Eq. (16)? When the classifiers are fixed, it seems that only the network F 1 can be trained.",0.0,1.0,0.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3401,NIPS_2020_1223,"*Temporal logic as such is often useful when considering infinite traces. Signal temporal logic is quite useful in the case of finite-time traces when dealing with continuous time systems. Neither of them are under consideration here, and I think this is the biggest draw back. *Novelty is quite limited. Much of the ideas in the paper have been introduced before. I will list some out here (which hasn't been discussed in the paper): 1. Writing STL specifications in terms of DNF specifications/using logical operators has been done previous in works such as: a) https://arxiv.org/abs/1703.09563 (they use DNFs/MILPS, which are equivalent), b) https://openreview.net/forum?id=BklC2RNKDS *Projecting outputs to satisfy constraints has been considered in works such as: a)https://arxiv.org/pdf/1805.07075.pdf, b) https://arxiv.org/abs/1801.08757 c)https://arxiv.org/abs/1603.06318. In fact, if you're only looking at finite+discrete time traces, this work is quite similar to c. Without considering infinite traces, the STL specifications reduce to first-order logic specifications as considered in c already. 2. There is a rich history of consider temporal logic specifications, and even STL specifications in the CPS community. As such, the specifications introduced here are not novel/new. 3. Propositions 4.1, 4.2 and 4.3 are trivial/obvious to the best of my knowledge. 4. There's very little discussion in terms of related work regarding enforcing STL/TL specifications for learning systems -- there have been several papers attempting to do this. a) https://robotics.sciencemag.org/content/4/37/eaay6276.full b) https://dorsa.fyi/publications/sadigh2014learning.pdf c) https://dl.acm.org/doi/abs/10.5555/3306127.3331994 d) https://openreview.net/forum?id=BklC2RNKDS","*Temporal logic as such is often useful when considering infinite traces. Signal temporal logic is quite useful in the case of finite-time traces when dealing with continuous time systems. Neither of them are under consideration here, and I think this is the biggest draw back. *Novelty is quite limited. Much of the ideas in the paper have been introduced before. I will list some out here (which hasn't been discussed in the paper):",0.0,1.0,-1.0,0.0,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3013,NIPS_2022_1564,"Weaknesses:
1.The main part can be more concise (especially for the introduction part)and including empirical results.
2.Given the new introduced hyper-parameters, it is still not clear whether this new proposed method is empirically useful. How to choose hyper-parameters in a more practical training setting?
3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue.
The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.","3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue. The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5328,NIPS_2020_1036,"* I was surprised to see, after the authors touted advantages of using a consistent training and test implementations, that the results of the (UN+UQ) system were significantly worse than those of the ""inconsistent"" solution introduced by Balle et al 2017 (UN+Q). Only when the softened quantizer is added (UN+UQ+SR) do we see a relatively small improvement. Why? Are there potential ways to improve this? * This makes one wonder about how much the UQ noise matters. In particular, it would be instructive to see a comparison to (UN+SR). Given the previous comment, one might suspect this would lead to even better performance - and thus that the UQ methodology, despite its mathematical interest, is not of practical value. * Although I think it's important and interesting, this is a pretty heavy and narrowly-focused topic for the NeurIPS community.","* This makes one wonder about how much the UQ noise matters. In particular, it would be instructive to see a comparison to (UN+SR). Given the previous comment, one might suspect this would lead to even better performance - and thus that the UQ methodology, despite its mathematical interest, is not of practical value.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5646,NIPS_2018_809,"Weakness: - The uniqueness of connecting curves between two weights would be unclear, and there might be a gap between the curve and FGE. A natural question would be, for example, if we run the curve findings several times, we will see many different curves? Or, those curves would be nearly unique?  - The evidences are basically empirical, and it would be nice if we have some supportive explanations on why this curve happens (and whether it always happens). - The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding)  Comment: - Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research. - To be honest, I'm not sure the first curve-finding part explains well why the FGE work. The cyclical learning rate scheduling would perturb the weight around the initial converged weight, but it cannot guarantee that weight is changing along the curve described in the first part.","- Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5563,NIPS_2020_1167,"The motivation and illustration are not clear. The details are as follows, 1. Why the RANet can capture more context information than SPP and previous attentional models? The authors claim that RANet naturally provides the spatial and category relationship of pixels to construct the contextual representations, but the category information in RANet may be not accurate, which could results in error guiding information. 2. After obtaining the contextual representation $O$ as described by Eq.(8), how to get the final segmentation map? 3.In Eq.(1), $B_{i \arrow j}$ denote a set of pixels on the line, what is the direction of the line? Vertical?horizontal？or oblique？","3.In Eq.(1), $B_{i \arrow j}$ denote a set of pixels on the line, what is the direction of the line? Vertical?horizontal？or oblique？",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5592,NIPS_2018_55,"weakness of this paper lies in the evaluation. Although it is a great thing that this paper uses more datasets than MNIST, the evaluation can be much improved. 1) The statements in the MNIST experiment such as ""While results without an CAE are quite convincing, the CAE clearly improves the pertinent positives and negatives in many cases. Regarding pertinent positives, the cyan highlighted pixels in the column with CAE (CAE CEM PP) are a superset to the cyan-highlighted pixels in column without (CEM PP). While these explanations are at the same level of confidence regarding the classifier, explanations using an AE are visually more interpretable."" are problematic. These are quite subjective statements, and some form of quantitative evaluation across subjects is required for such claims. 2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.  3) This paper did not provide enough information regarding how the evaluation was done in the brain functional imaging experiment. It seems that the only sentence is ""With the help of domain experts"". 4) c, \beta, and \gamma are important parameters for the proposed approach. The main paper did not discuss the choice of these parameters at all, and the supplementary material only gives procedural information. It would be great if this paper provides more thoughtful discussions on the choices of these parameters, or maybe the insensitivity of these parameters if that is the case. Overall, I really like the idea of this paper and believe that this paper should be accepted. Given the space limit of NIPS submissions, one possible way to improve the paper is to drop one experiment and make the other two experiments more solid. Minor presentation-related suggestions:   I like the introduction overall, but the first sentence seems a bit out of nowhere and statements such as ""Explanations as such are used frequently by people"" are questionable and at least requires better evidence.   line 218: an CAE -> a CAE   line 252: spend -> spending I have read the review and it would be useful if the user can clarify how some set operations in the formulation apply to continuous variables.","2) In the procurement fraud experiment, it seems that the experts like everything that the algorithm shows. Risk evaluation seems a non-trivial problem. It is unclear whether these experts or humans are good at this task. Also, given the sample size, it is unclear whether the difference in Table 1 is statistically significant.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5632,NIPS_2018_1007,"Weakness: Which brings us to various unclear parts in the paper. First of all, there are key claims that are hard to justify. For instance: ""a key strength of neural models is their effectiveness at efficient transfer"". I am sure you'll find a lot of disagreement here especially when you are not working in the vision community where ImageNet models transfer fairly well. This is not the case with models trained with RL (DQN or policy gradients) because the gradients are a lot noisier and the representations learning is more difficult. Another example being, ""A3C algorithm [â¦], because it is simple, robust and stable"" is hard to swallow given the term ""asynchronous"" in the algorithm. The paper's treatment of ""transfer"" is quite unclear too. Transfer learning has a long history and would refer to multiple surveys on transfer in RL [1,2] to better place their objective. Moreover, we can expect that the NIPS audience wouldn't know as much about symbolic AI and RDDL description, so use of terms like fluents without defining them first, leaves things unclear to the reader. Similarly, even thought the components of the architecture are clearly explained individually, their exact combination and how exactly the losses are setup is quite unclear. I hope the authors are atleast planning on releasing their code for easier replication. There are quite a few components in the proposed method. Whether they are warranted can only be checked by experimental verification. The paper is quite unclear about the exact nature of these problem domains - what's the observation space like, what are the possible number of problems that are generated in a domain, etc. (One can look it up on IPPC but making the paper clear would be better). Moreover, since these are planning problems it's hard to say if DeepRL algorithms like A3C are right baselines to benchmark against. The paper _is_ using the model after all (transition). Wouldn't it be useful to at least show the standard methods used in IPPC and their efficiency. Do we even gain on anything at all by showing transfer abilities if the total time taken by standard planning algorithms for each problem domain is less than learning via DeepRL? Moreover it's unclear what were the parameters of A3C algorithms itself - number of workers/batch size etc. It doesn't look like Figure 2 and Figure 3 show averaged runs over multiple seeds (maybe fill color to show standard error?) nor is there any standard deviation for Table 2 results. So although the inclusion of ""SAD"" seems pretty big as an improvement, I can't make a strong claim given how much variance there can be with DeepRL experiments.  # Minor things:  - Line 85: Missing citation? - Line 329: Not sure if Table 2 suggests that. Possibly meant to be Figure 3? [1]: http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf [2]: https://hal.inria.fr/hal-00772626/document ",- Line 329: Not sure if Table 2 suggests that. Possibly meant to be Figure 3? [1]: http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf [2]: https://hal.inria.fr/hal-00772626/document,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”",3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.","- What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4575,NIPS_2020_1719,"Just 3 concerns: 1. Equation 11 is expressed without the regularization terms ... and it is stated that extending it with the same is straightforward. Is that really so? It is not really that straightforward when projecting. SO authors must expand on this a bit. 2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more. 3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.","3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5331,NIPS_2020_790,"- While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48]. - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix. - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution. - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.",- I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2829,NIPS_2022_158,"Weakness
Not sure what the main take away is. The goal appears to be to understand the neural encoding in the retina, but after that the analysis and results, there is no attempt to tie these back to neurobiological mechanisms. It seems one could, but the paper just ends with the statement, ""our results are in strong agreement with observed retinal data,"" which leaves you hanging.
Specific issues:
The difference of Gaussians model in eq. 8: it mentions that the center position of each kernel is different for each neuron, but is this also learned? not mentioned.
Section 3: linear model in the continuum limit - this is very unclear. what is being continuized? space? The integral is over frequency space - not following what's going on. principal vectors a_1, a_2 and reciprocal vectors b_1, b_2 - what are these?
Section 4.1: "" power spectral density can be well approximated by a product of spatial and temporal power-law densities"" - Dong & Atick is cited, but curiously the claim the exact opposite, it is not separable.
Figure 4, panel A shows striking clustering in temporal spectral centroids - they are all stacked neatly in tight columns, no scatter. is this what emerges from the learned filters, or is somehow the quantization imposed?
The mosaics are interesting to look at, but not clear what to take away from this.
Overall this seems like a very promising direction, I want to like this paper, but I find it a bit confusing and lacking a clear message.","8: it mentions that the center position of each kernel is different for each neuron, but is this also learned? not mentioned. Section 3: linear model in the continuum limit - this is very unclear. what is being continuized? space? The integral is over frequency space - not following what's going on. principal vectors a_1, a_2 and reciprocal vectors b_1, b_2 - what are these? Section 4.1: "" power spectral density can be well approximated by a product of spatial and temporal power-law densities"" - Dong & Atick is cited, but curiously the claim the exact opposite, it is not separable. Figure 4, panel A shows striking clustering in temporal spectral centroids - they are all stacked neatly in tight columns, no scatter. is this what emerges from the learned filters, or is somehow the quantization imposed? The mosaics are interesting to look at, but not clear what to take away from this. Overall this seems like a very promising direction, I want to like this paper, but I find it a bit confusing and lacking a clear message.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4787,NIPS_2020_611,"I think this paper would benefit from a more comprehensive connection to existing ideas in the field of biologically-plausible learning. Specifically: 1. There is a body of literature on biologically plausible methods for training feedforward ANNs with labels, see [R1] and [R2] for example. 2. Biologically-plausible learning rules that do not require label information have been studied in [R3] for shallow networks, and in [R4] for training multiple hidden layers of representations. The latter work, although targeting similarity search as a downstream task and not classification, also extensively uses divisive normalization for images. Similarly to this work, this seems to be important for achieving high accuracy (precision). 3. The empirical evaluations (Table 1) need to be compared with previously published results in the biologically-plausible settings. For instance, [R3] reports accuracy on fully connected network for MNIST better than 98.5%. Refs [1,3] report slightly better than 50% accuracy for fully connected architectures on CIFAR-10. Refs: R1. https://arxiv.org/abs/1412.7525 R2. https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full R3. https://www.pnas.org/content/116/16/7723 R4. https://arxiv.org/abs/2001.04907","2. Biologically-plausible learning rules that do not require label information have been studied in [R3] for shallow networks, and in [R4] for training multiple hidden layers of representations. The latter work, although targeting similarity search as a downstream task and not classification, also extensively uses divisive normalization for images. Similarly to this work, this seems to be important for achieving high accuracy (precision).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3649,NIPS_2020_791,There are several issues here which I would like the authors to address: * Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all. * What is the relationship between the CDF and percentile rank in this case? is there a way to express one with the other? * The experiments show that in a controlled setting (where a clear target patch and template patch are defined) it is possible to explain several illusions. One thing which is common to all the illusions is that the target patch is flat - what about cases where the patch to explain may have some structure? like the Kanitze triangle? this would make a much more convincing case for the method. * The authors show that the percentile rank correlates with the perceived *relative* lightness (for example) but they do not show if this is actually at the same scale of perception - do subjects report the same change in lightness perception? (I'm sure these numbers can be found in literature). * Only one generative model is tested here - do results change with other models? say a simple GMM or a sparse coding based one?,* Could the authors comment on the use percentile rank? I understand the reasoning behind it more or less but this is not explained in the paper at all.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1812,ARR_2022_174_review,"1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes. 2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide. In the Appendix, ""Model Design"" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?
3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? 
In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar ","- Line 130-131: check grammar - Line 248: ""... and is ..."" - Line 400-401: check grammar - Line 426: ""autometric"" - Line 556-557: check grammar",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4575,NIPS_2020_1719,"Just 3 concerns: 1. Equation 11 is expressed without the regularization terms ... and it is stated that extending it with the same is straightforward. Is that really so? It is not really that straightforward when projecting. SO authors must expand on this a bit. 2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more. 3. This agnostic formulation in this case is related with solving a minmax problem ... in ""robust MDPs"" or even in adversarial games.... Authors should make an attempt to discuss the same. Finally, while the evaluations are acceptable they definitely have room for improvement, at least in the way the results are discussed. -- that is sub-par. Ideally the authors must define evaluation goals as in what is measured and for what reason.. what is it that they are trying to show. Then when discussed the tables/plots connect the discussion back to the goals.",2. Justification provided for not comparing against Pareto-frontier search methods is very loose and ambiguous. and not convincing in anyway. Authors should expand on this more.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3471,NIPS_2020_679,"he main weaknesses of the paper are: * the lack of comparisons to other model-based exploration approaches to disentangle the impact of the model-based aspect vs the novelty measure on performance. * the lack of empirical comparisons and discussion of ICM (Pathak et al., 2017) and RND (Burda, 2018). * the lack of statistical tests to support empirical comparisons. * the lack of an ablative study with respect to the 6 different losses used by the proposed approach.","* the lack of empirical comparisons and discussion of ICM (Pathak et al., 2017) and RND (Burda, 2018).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5616,NIPS_2018_232,"weaknesses - Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","- Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- How does the method scale as the corpus size or hidden dimension size is increased?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
905,ICLR_2023_2312,"Weaknesses
1. Literature Review
The paper regrettably fails to acknowledge a vast body of related literature, on (i) intention-conditioned trajectory prediction, (ii) variational graph methods for trajectory prediction, and (iii) models that explicitly model social interactions for forecasting. At the very least, these references ought to be mentioned and discussed for a diligent representation of the research space, even if the methods are not directly compared against.
(i) Intention-Conditioned Trajectory Prediction:
[R1, R2, R3] talk about intention-conditioned trajectory prediction for autonomous vehicles. Apart from the data the methods are applied to, the architectures can be applicable to, and are relevant for, the problem being addressed here. Crucially, the DROGON paper defines intention explicitly (more on this in Weakness 2. below).
(ii) Variational Graph Methods:
[R4] from the Neurips I Can't Believe It's Not Better Workshop explicitly deals with graph conditional variational methods for multi-agent trajectory prediction. The results in that paper are very relevant for this research area and should be included.
(iii) Encoding Social Interactions:
Graph and other stochastic methods that encode social interactions between agents have been long applied to trajectory and behavior forecasating problems. [R5] explicitly incorporates a spatiotemporal graph for incorporating social interactions between agents. [R6] more recently explicitly takes a meta-learning approach for modeling the dynamics unique to a group for probabilistic forecasting. A sports team is a group, and if each team is viewed as having unique social dynamics resulting from the team's strategy then [R6]'s core modeling idea is directly applicable. The cue in [R6] terms is simply player location here. Their modeling of social influence of other agents is also permutation invariant, a limitation this paper claims about existing methods. References:
[R1] DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning - Choi et al.
[R2] Intention-Driven Trajectory Prediction for Autonomous Driving - Fan et al.
[R3] LOKI: Long Term and Key Intentions for Trajectory Prediction - Girase et al.
[R4] Graph Conditional Variational Models: Too Complex for Multiagent Trajectories? - Rudolph et al.
[R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al.
[R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.
2. Unsupported claims and definitions
The paper doesn't actually define agent intentions and causality in the specific setting, so there is no reasonable way to evaluate whether the proposed method actually models intentions. The intention-conditioned trajectory works I've mentioned talk about intention over long- and short- time horizons, where e.g. the former is in terms of goal destinations. Here the paper is talking about team sports with player intentions but simply states that this results from communication. What does intention mean here? Also, the paper claims to model causal relationships, but I can't see any explicit causal factors modeled of learned in the graph structure. There might be other exogenous variables explaining trajectory behavior.
3. Notation
There are a few notational errors. For instance, the variable used for the sequence cannot be the same as the individual elements: x < t = [ x 1 , . . . ]
. See [R4] for this. In many places there exist grammatical errors and incomplete sentences. Please do a pass to fix these.",- Rudolph et al. [R5] Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction - Mohamed et al. [R6] Social Processes: Self-Supervised Meta-Learning over Conversational Groups for Forecasting Nonverbal Social Cues - Raman et al.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”","4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
475,ICLR_2022_3205,"Weaknesses
This method trades one intractible problem for another: it requires the learning of cross-values v e ′ ( x t ; e )
for all pairs of possible environments e , e ′
. It is not clear that this will be an improvement when scaling up.
At a few points the paper introduces approximations, but the gap to the true value and the implications of these approximations are not made completely clear to me. The authors should be more precise about the tradeoffs and costs of the methods they propose, both in terms of accuracy and computational cost.
On page 6, it claims that estimating v c
according to samples will lead to Thompson sampling-like behavior, which might lead to better exploration. This seems a bit facetious given that this paper attempts to find a Bayes-optimal policy and explicitly points out the weaknesses of Thompson sampling in an earlier section.
Not scaled to larger domains, but this is understandable.
Questions and minor comments
Is the belief state conditioning the policy also supposed to change with time τ
? As written it looks like the optimal Bayes-adaptive policy conditions on one sampled belief about the environment and then plays without updating that belief.
It is not intuitive to me how it is possible to estimate v f
, despite the Bellman equation written in Eq. 12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct?
I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.","12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct? I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2606,NIPS_2019_1408,"Weaknesses: - The paper is not that original given the amount of work in learning multimodal generative models:   â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior.   â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths: - The experimental results are nice. The paper claims that their MMVAE modal fulfills all four criteria including (1) latent variables that decompose into shared and private subspaces, (2) be able to generate data across all modalities, (3) be able to generate data across individual modalities, and (4) improve discriminative performance in each modality by leveraging related data from other modalities. Let's look at each of these 4 in detail:   â (1) Yes, their model does indeed learn factorized variables which can be shown by good conditional generation on MNIST+SVHN dataset.   â (2) Yes, joint generation (which I assume to mean generation from a single modality) is performed on vision -> vision and language -> language for CUB,   â (3) Yes, conditional generation can be performed on CUB via language -> vision and vice versa.  Weaknesses: - (continuing on whether the model does indeed achieve the 4 properties that the authors describe)   â (3 continued) However, it is unclear how significant the performance is for both 2) and 3) since the authors report no comparisons with existing generative models, even simple ones such as a conditional VAE from language to vision. In other words, what if I forgo with the complicated MoE VAE, and all the components of the proposed model, and simply use a conditional VAE from language to vision. There are many ablation studies that are missing from the paper especially since the model is so complicated.   â (4) The authors have not seemed to perform extensive experiments for this criteria since they only report the performance of a simple linear classifier on top of the latent variables. There has been much work in learning discriminative models for multimodal data involving aligning or fusing language and vision spaces. Just to name a few involving language and vision:     - Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016     - DeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013 Therefore, it is important to justify why I should use this MMVAE model when there is a lot of existing work on fusing multimodal data for prediction. ****************************Clarity**************************** Strengths: - The paper is generally clear. I particularly liked the introduction of the paper especially motivation Figures 1 and 2. Figure 2 is particularly informative given what we know about multimodal data and multimodal information. - The table in Figure 2 nicely summarizes some of the existing works in multimodal learning and whether they fulfill the 4 criteria that the authors have pointed out to be important. Weaknesses: - Given the authors' great job in setting up the paper via Figure 1, Figure 2, and the introduction, I was rather disappointed that section 2 did not continue on this clear flow. To begin, a model diagram/schematic at the beginning of section 2 would have helped a lot. Ideally, such a model diagram could closely resemble Figure 2 where you have already set up a nice 'Venn Diagram' of multimodal information. Given this, your model basically assigns latent variables to each of the information overlapping spaces as well as arrows (neural network layers) as the inference and generation path from the variables to observed data. Showing such a detailed model diagram in an 'expanded' or 'more detailed' version of Figure 2 would be extremely helpful in understanding the notation (which there are a lot), how MMVAE accomplishes all 4 properties, as well as the inference and generation paths in MMVAE. - Unfortunately, the table in Figure 2 it is not super complete given the amount of work that has been done in latent factorization (e.g. Learning Factorized Multimodal Representations, ICLR 2019) and purely discriminative multimodal fusion (i.e. point d on synergy) - There are a few typos and stylistic issues: 1. line 18: ""Given the lack explicit labels availableâ -> âGiven the lack of explicit labels availableâ 2. line 19: âcan provided importantâ -> âcan provide importantâ 3. line 25: âbetween (Yildirim, 2014) themâ -> âbetween them (Yildirim, 2014)â 4. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite interesting. - The paper did a commendable job in attempting to perform experiments to justify the 4 properties they outlined in the introduction. - I can see future practitioners using the variational MoE layers for encoding multimodal data, especially when there is missing multimodal data. Weaknesses: - That being said, there are some important concerns especially regarding the utility of the model as compared to existing work. In particular, there are some statements in the model description where it would be nice to have some experimental results in order to convince the reader that this model compares favorably with existing work: 1. line 113: You set \alpha_m uniformly to be 1/M which implies that the contributions from all modalities are the same. However, works in multimodal fusion have shown that dynamically weighting the modalities is quite important because 1) modalities might contain noise or uncertain information, 2) different modalities contribute differently to the prediction (e.g. in a video when a speaker is not saying anything then their visual behaviors are more indicative than their speech or language behaviors). Recent works therefore study, for example, gated attentions (e.g. Gated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 or Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning, ICMI 2017) to learn these weights. How does your model compare to this line of related work, and can your model be modified to take advantage of these fusion methods? 2. line 145-146: ""We prefer the IWAE objective over the standard ELBO objective not just for the fact that it estimates a tighter bound, but also for the properties of the posterior when computing the multi-sample estimate."" -> Do you have experimental results that back this up? How significant is the difference? 3. line 157-158: ""needing M^2 passes over the respective decoders in total"" -> Do you have experimental runtimes to show that this is not a significant overhead? The number of modalities is quite small (2 or 3), but when the decoders are large recurrent of deconvolutional layers then this could be costly. ****************************Post Rebuttal**************************** The author response addressed some of my concerns regarding novelty but I am still inclined to keep my score since I do not believe that the paper is substantially improving over (Wu and Goodmann, 2018) and (Tsai et al, 2019). The clarity of writing can be improved in some parts and I hope that the authors would make these changes. Regarding the quality of generation, it is definitely not close to SOTA language models such as GPT-2 but I would still give the authors credit since generation is not their main goal, but rather one of their 4 defined goals to measure the quality of multimodal representation learning.","- The paper is not that original given the amount of work in learning multimodal generative models: â For example, from the perspective of the model, the paper builds on top of the work by Wu and Goodman (2018) except that they learn a mixture of experts rather than a product of experts variational posterior. â In addition, from the perspective of the 4 desirable attributes for multimodal learning that the authors mention in the introduction, it seems very similar to the motivation in the paper by Tsai et al. Learning Factorized Multimodal Representations, ICLR 2019, which also proposed a multimodal factorized deep generative model that performs well for discriminative and generative tasks as well as in the presence of missing modalities. The authors should have cited and compared with this paper. ****************************Quality**************************** Strengths:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4349,NIPS_2020_1636,"While the general idea of the paper is appealing and has been evaluated extensively, the presentation of the methodology is lacking in clarity at times. After reading section 3, some issues could have been addressed more clearly: • Regarding line 171/172: what do the authors mean by “regret reaches the plateau”? Can this be quantified? • If the tree is constructed as described, it is questionable that the leftmost leave is actually the ‘best’ leave. Can this be shown? • How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.","• How is the problem treated that SVM can lead to many distinct areas in the described methodology? Consider the case of the 1D sine function and we have data points only at increments of pi. K-means would result in two clusters, i.e., the points with values +1 and -1, respectively. Then, SVM would potentially cluster the domain in the two classes resulting in alternating regions for each class. What would be the resulting domain for TuRBO then? In the very beginning, the authors mention that only deterministic objective functions are considered. It is not clear how this statement fits to the the main result: the optimization of policies on the MuJoCo tasks which are known to be inherently stochastic objectives. Does LA-MCTS depend on the deterministic assumption? If yes: why does it work well in practice on stochastic functions, and if not: Why assume it then? The empirical performance of LA-MCTS is impressive. However, the method combines many different building blocks and as such introduces many additional hyperparameters. Though an ablation study was performed, the performance of the method depends drastically on the choice of hyperparameters. As such, the practicability of the approach is limited as an additional layer of parameters needs to be tuned in addition to the BO parameters. It is not exactly clear, why the authors call their method ‘latent actions’ as these are just the decision boundaries from the SVM classifier. No theoretical work is presented. ------------------------------ After reading the authors response: Thank you for the detailed response to the raised concerns as well as the additional experiments. Tree construction: Being the best node in expectation is something different then being the best node. This should be made more clear in the main paper. Further, Figure 10 does not really help to make this issue more clear as for example the evaluated points are missing in the plot. How's the initial purple region selected when no data is available? Also, using a contour-plot to visualize the objective function would help to understand the figure better. Deterministic assumption: if no component depends on being deterministic, than I'd highly recommend removing this from the main paper in the beginning. Also, just using a sample mean of 5 rollouts does not lead to a deterministic function but just reduces the variance by a factor of 5, which can still be relatively high for RL tasks especially as the outcome does not necessarily follow a uni-modal distribution. Also, please make the use of multiple rollouts more transparent as this simplifies the RL problem drastically. Minor: Appendix A.1: Hit-and-Run and Gibbs sampling do not require the region to be a convex polytope. Overall: The approach presented in this paper shows great potential but the quality of the paper is not yet at the level of a top-tier conference.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.",3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4450,NIPS_2020_1451,"1. Unlike the works HaoChen and Sra and Nagaraj et.al, this work uses the fact that all component functions f_i are mu strongly convex. 2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples. 3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.",3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5331,NIPS_2020_790,"- While the theoretical bounds are nice, no algorithmic results for efficiently estimating the CRS model is given. I suppose that this all relies directly on the machinery developed for CDM in [48]. - I also find the utilization of the page limit very suboptimal. Main discussions such as those about the discrepancy of the optimization should not appear in the Appendix. - One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution. - Given the parameter scaling of the model (quadratic in the number of items) it might not be easily applicable in practice. In fact I find the simulation results confusing. It is not clear why CRS has such nonlinear performance. I do not find the explanation on line 522-527 satisfactory.","- One important limitation is that the theorems provide guarantees only for datasets of *full* rankings. In many practical scenarios, the data would consist of partial rankings over (many small) subsets of items. Similar guarantees for this case would be very helpful, and would strengthen the contribution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?",2.How fast does this algorithm run or train compared with other baselines?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
275,ICLR_2022_1955,"weaknesses ablation of representation is missing: It could be argued that the most gains are from the new GNN architecture. It could be instructive to try out the same model with the same architecture that G2G, GLN, or NeuralSym uses. Or maybe with DRGAT, G2G, GLN, or NeuralSym would also perform better?
baselines is missing: I would suggest to add
Sacha et al - MEGAN model https://arxiv.org/abs/2006.15426
Sun et al https://arxiv.org/abs/2007.13437
Seidl et al https://arxiv.org/abs/2104.03279 to the results table, which outperforms SemiRetro in top10 accuracy.
notes: In the introduction, the authors write: Fortunately, with the rapid accumulation of chemical data, machine learning is promising to solve this problem (Szymkuc et al., 2016; Coley et al., 2018; Segler et al., 2018). - However, Szymkuc et al 2016 argue against the use of data-driven approaches, so I would suggest to not to cite them in this context.","- However, Szymkuc et al 2016 argue against the use of data-driven approaches, so I would suggest to not to cite them in this context.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2316,ACL_2017_216_review.json,"Weaknesses: 1. Compared to Balikas COLING16's work, the paper has a weaker visualization (Fig 5), which makes us doubt about the actual segmenting and assigning results of document. It could be more convincing to give a longer exemplar and make color assignment consistent with topics listed in Figure 4. 
2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?
- General Discussion: The paper is well written and structured. The intuition introduced in the Abstract and again exemplified in the Introduction is quite convincing. The experiments are of a full range, solid, and achieves better quantitative results against previous works. If the visualization part is stronger, or explained why less powerful visualization, it will be more confident. Another concern is about computation efficiency, since the seminal LDA work proposed to use Variational Inference which is faster during training compared to MCMC, we wish to see the author’s future development. ","2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2396,NIPS_2021_311,"Weaknesses - The paper leaves some natural questions open (see questions below). - Line 170 mentions that the corpus residual can be used to detect an unsuitable corpus, but there are no experiments to support this.
After authors' response All the weakness points have been addressed by the authors' response. Consequently I have raised my score. In particular:
The left open questions have all been answered.
There indeed is an experiment to support this, thanks to the authors' for clarifying this, that connection was not clear to me previously.
Questions - Line 60: Why do you say that e.g. influence functions cannot be used to explain a prediction? The explanation of a prediction could be the training examples whose removal (as determined by the influence function) would lead to the largest score drop for a prediction. - How does the method scale as the corpus size or hidden dimension size is increased? - What happens if a too small corpus is chosen? Can this be detected? - What if we don’t know that a test example is crucially different, e.g. what if we don’t know that the patient of Figure 8 is “British” and we use the American corpus to explain it? Can this be detected with the corpus residual value? - In the supplementary material you mention how it is possible to check if a decomposition is unique. Do you do this in practice when conducting experiments? How do you choose a decomposition if it is not unique? What does it imply for the experiments (and the usage of the method in real-world applications) if the decomposition is not unique?
Typos, representation etc. - Line 50: An example of when a prototype model would be unsuitable would strengthen your argument. - Footnote 2: “or” -> “of” - Line 191: when the baseline is first introduced, [10] or other references would be helpful to support this approach - Line 319: “the the” -> “the” - Line 380: “at” -> “to”?
A broader impact section could be added. In a separate section (e.g. supplementary material), there could be an explicit discussion on when the method should not be used, e.g. as shown in Figure 8, the American corpus shouldn’t be used to explain the British patient. Also see last question above – what if we don’t know that the patient is British? Can this be detected? This should also be discussed in such a section.",- What happens if a too small corpus is chosen? Can this be detected?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5529,NIPS_2020_1796,"While the result is interesting, many of the design decisions behind the models and training procedures seemed poorly motivated and discussion on their nuances lacking. - Why represent the rewards as a function of a VAE used to encode exploratory frames in the environment? How does more or less exploration impact the effect of the auxiliary rewards learned this way? - What is the impact of environment ordering during training? Are there any curriculum effects at play? Why train with such a curriculum in the first place? An ablation demonstrating the necessity of this approach would contextualize this decision. - The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42). - It seems the claim that safe reinforcement learning focuses on just avoiding negative side effects during training is inaccurate. As stated in the abstract of a paper the authors cite (García et al, 2015), safe RL is also concerned with safety at deployment. - The language is often unclear, e.g. Line 211: ""AUP stops moving entirely."" The reviewer believes this is referring to the AUP learning a policy in which the agent remains immobile. - The discussion around the theoretical results (3.2) does not add much insight to the experiments and results presented in the paper. - The contribution is not very novel, as it is simply applying AUP as presented in Turner et al, 2020 to another environment, with little to no modification. - Not clear why Lines 55-57 are included in the related work, as they do not seem particularly relevant to safe RL.","- The language stating ""the agent learns R_AUP"" was confusing (Line 138). The reviewer believes this language was meant to describe ""training the Q_i functions"" corresponding to each R_i. This confusing language was used in the caption for the reward learning curves in the supplementary materials, which seems supported by the statement starting on Line 202: ""In the supplementary material..."" - Some key concepts are not defined, for example ""initial state reachability"" (Line 42).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2582,NIPS_2019_933,"weaknesses: + I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms.  Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold. ",- A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2055,ARR_2022_17_review,"- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant. - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021) rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.  - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models. - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) 
Most of my comments are given above. Details : some references have no venue -> please correct this.   misc{hsu2021robust,    title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},    author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},    year={2021},    eprint={2104.01027},    archivePrefix={arXiv},    primaryClass={cs. SD} } ","- Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) Most of my comments are given above. Details : some references have no venue -> please correct this. misc{hsu2021robust, title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training}, author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli}, year={2021}, eprint={2104.01027}, archivePrefix={arXiv}, primaryClass={cs. SD} }",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4011,NIPS_2020_182,"1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion. 3. Technical details in this paper is a bit hard to follow. It is better to given a neural network diagram or a pseudo-code algorithm to help readers between understand the details of the proposed framework. 4. In line 216, it is said that 1,000 source sets are generated. However, in line 224, MAE is only averaged over 100 source sets, which is contrary to previous description. 5. There are many typos in this paper, e.g., - Line 40: “and and reture” - Line 127 and Line 142: “Appendix ??”","1. The utilization of the approximation in (10) is not properly validated. For example, the error between the approximation of the deep neural network and the original Mori-Zwanzig memory term is not evaluated. 2．In the section of numerical experiments, different baselines are compared in different tasks. However, choosing them in these tasks is not well justified. For example, InfluLearner is only compared in the task of Infection probability and influence function estimation. Obviously, by combining with the classical greedy algorithm, it can be compared in the task of Influence Maximization. Thus, why choosing these compared algorithms in different tasks needs more discussion.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3644,NIPS_2020_1156,"- Other papers have done somewhat similar things, e.g. Duncker & Sahani (2018) have condition-dependent latents as well as single-trial latents. - The paper is somewhat light on experimental results, and the conclusions are somewhat abstract (on *average* there is less overlap between noise and sensory coding dimensions, but there seems to be substantial trial-to-trial variability) - I don't have any other major concerns besides the typical complications and drawbacks of Gaussian Processes. It would be useful to consider different choices for the GP kernel function. Also more discussion on how to choose parameters like the lengthscale of the kernel would be useful. (In the original GPFA paper the authors place a prior over the lengthscale... Is that done here or is the lengthscale a user-determined constant?)","- The paper is somewhat light on experimental results, and the conclusions are somewhat abstract (on *average* there is less overlap between noise and sensory coding dimensions, but there seems to be substantial trial-to-trial variability) - I don't have any other major concerns besides the typical complications and drawbacks of Gaussian Processes. It would be useful to consider different choices for the GP kernel function. Also more discussion on how to choose parameters like the lengthscale of the kernel would be useful. (In the original GPFA paper the authors place a prior over the lengthscale... Is that done here or is the lengthscale a user-determined constant?)",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1455,ICLR_2023_4735,"Weaknesses: 1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied. 2) There are many pretrained models for various tasks, more pretrained models are encouraged to study for VQA. 3) I am still unclear about the motivation of intra-consistency and inter-divisibility loss related to quality assessment.","1) The contribution of this paper is to exploit the knowledge from the multiple pretrained deep models for VQA, while the practical method is doubtful. First, I am wondering why there are different pretrained models for different datasets. It is better to have a deeper analysis or insight into the fusion mechanism of various pretrained deep models. How the different pretrained models contribute to the quality-aware feature extraction is also valuable to be studied.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
429,ICLR_2022_1745,"Weaknesses Major
The authors have some fundamental issues and misconceptions about the BootstrappedDQN (BootDQN) algorithm. Section 2 mentions that BootDQN addresses exploration by running multiple behavior policies in the environment; if so, then it would be true that the method would have no reason to solve sparse reward tasks a priori as described by the authors. However, the reason for BootDQN's success in exploration is because of posterior sampling [8] and the fact that the ensemble is an approximate posterior over the optimal action-value function of the MDP, conditioned on all agent interactions observed thus far. It is precisely this principle that allows the algorithm to address sparse-reward tasks [2]. The authors' description of Thompson sampling in BootDQN as an ad-hoc heuristic seems incorrect given the rigorous theoretical guarantees that accompany randomized least-squares value-iteration algorithms [3]. The authors are also confused about the use of replay buffers in BootDQN, claiming that data from each head is held separate. There is exactly one replay buffer used in BootDQN and Bernoulli masks are sampled and stored with each transition for implementing the statistical bootstrap. Overall, the connection the authors draw with BootDQN in this work seem rather disingenuous; the proposed algorithm is simply applying ensembles of actor-critic pairs with no connection to the statistical bootstrap, unlike BootDQN. Renaming the algorithm and rephrasing the contribution seem appropriate.
A more critical issue concerning BootDQN and the proposed BHER algorithm is that the former is a purely value-based RL algorithm that maintains a posterior distribution over the optimal action-value function. In contrast, the latter is an off-policy actor-critic algorithm where, naturally, the critic is meant to be an estimate of the action-value function induced by the actor policy. While the empirical results of this paper confirm empirical benefits of this ensembling heuristic (the ablation in Figure 5 shows that this is responsible for most of the BHER performance), the authors have offered no real justification for this ensemble actor-critic algorithm. What is the point of representing epistemic uncertainty over the actor and critic in this manner?
I don't find the so-called counterintuitive prioritization to be counterintuitive at all. It seems natural that hindsight transitions will serve the agent well only when there is little uncertainty in the associated optimal behavior under the relabeled goal. Can the authors explain why sampling based on higher variance seems to still maintain reasonable performance in three of seven environments shown in Figure 5? Minor
In the first paragraph of Section 4.2, the authors describe an instance of Prioritized Experience Replay (PER) [6] where the variance of the critic ensemble is used to prioritize transitions sampled from the replay buffer. They seem to confuse this technique with methods for intrinsic motivation [7] based on curiosity and Random Network Distillation. Clarity Strengths
The authors provide ample details about their experimental setup for reproducibility of their results. Weaknesses Major
Overall, the paper is not well written. There are numerous grammatical errors throughout, genuinely too many for me to sensibly list them all out here. Oftentimes, these errors are missing articles (for example, ""it uses successful trajectories generated by agent as expert demonstrates"") or incorrect phrases (""on the contrast"", ""we inference all the Q-values""). Normally, I wouldn't bother nitpicking at a small handful of these, but there are too many throughout the entire body for what may end up being a published conference paper. Minor
The authors should remove the phrase ""importance sampling"" that is used twice in the paper to, in my reading, talk about the importance of sampled goals, rather than the Monte-Carlo technique of the same name. Originality Strengths
The authors demonstrate a good instinct in examining how other techniques used in deep reinforcement learning might further improve the efficacy of HER. Weaknesses Major
Fundamentally, this paper rests on the idea of using ensembles and prioritized experience replay together, neither of which is new to (deep) reinforcement learning [1,4,5,9]. Though I do not know of any prior work that has explored this specific combination, it would not surprise me if such prior work already exists.
More importantly, there are other options for leveraging such ensembles that have not been addressed in this work [1,4]. Similarly, the authors only consider variance-based prioritization schemes, rather than the traditional prioritization based on TD-error or any recent variants of PER. Demonstrating that the authors' specific choices in the proposed approach are better than these existing approaches to ensembling and PER would dramatically improve what so far seems to be a rather incremental algorithm. Minor
While it is appropriate for the related work section to focus on HER, it should also acknowledge the two fundamental innovations of this paper (ensembles and prioritization schemes) and provide an overview of related work for these areas as well. Significance Strengths
The only positive I can glean from this paper are the empirical results which seem to support the use of ensembling in actor-critic algorithms. Figure 5 shows a marginal drop in performance when the proposed counterintuitive prioritization scheme is not used. That said, it is not clear that this paper advances our understanding of ensembling in deep RL any more than prior work. Weaknesses Major
Given the lack of comparisons mentioned above, it's difficult to assess how impactful the proposed approach will be. With the breadth of existing work on the topic, I'm unconvinced that this will add any novel insights into how practitioners use ensemble methods in reinforcement learning. The prioritization scheme, while slightly interesting on the surface, doesn't seem to be a critical ingredient to the proposed algorithm's success based on the ablation studies shown.
I don't believe any of the experiments have shown results for regular DDPG without the use of HER. Having this baseline in place is important as it communicates the extent to which HER is even necessary for achieving a reasonable level of performance in each of the examined environments. Minor References
Lee, Kimin, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. ""Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning."" In International Conference on Machine Learning, pp. 6131-6141. PMLR, 2021.
Osband, Ian, and Benjamin Van Roy. ""Why is posterior sampling better than optimism for reinforcement learning?."" In International conference on machine learning, pp. 2701-2710. PMLR, 2017.
Osband, Ian, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. ""Deep Exploration via Randomized Value Functions."" J. Mach. Learn. Res. 20, no. 124 (2019): 1-62.
Peer, Oren, Chen Tessler, Nadav Merlis, and Ron Meir. ""Ensemble Bootstrapping for Q-Learning."" arXiv preprint arXiv:2103.00445 (2021).
Saphal, Rohan, Balaraman Ravindran, Dheevatsa Mudigere, Sasikant Avancha, and Bharat Kaul. ""SEERL: Sample Efficient Ensemble Reinforcement Learning."" In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1100-1108. 2021.
Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized Experience Replay."" In ICLR, 2016.
Singh, Satinder, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. ""Intrinsically motivated reinforcement learning: An evolutionary perspective."" IEEE Transactions on Autonomous Mental Development 2, no. 2 (2010): 70-82.
Strens, Malcolm. ""A Bayesian framework for reinforcement learning."" In ICML, vol. 2000, pp. 943-950. 2000.
Wiering, Marco A., and Hado Van Hasselt. ""Ensemble algorithms in reinforcement learning."" IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38, no. 4 (2008): 930-936.","124 (2019): 1-62. Peer, Oren, Chen Tessler, Nadav Merlis, and Ron Meir. ""Ensemble Bootstrapping for Q-Learning."" arXiv preprint arXiv:2103.00445 (2021). Saphal, Rohan, Balaraman Ravindran, Dheevatsa Mudigere, Sasikant Avancha, and Bharat Kaul. ""SEERL: Sample Efficient Ensemble Reinforcement Learning."" In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1100-1108. 2021. Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized Experience Replay."" In ICLR, 2016. Singh, Satinder, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. ""Intrinsically motivated reinforcement learning: An evolutionary perspective."" IEEE Transactions on Autonomous Mental Development 2, no.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1944,ARR_2022_248_review,"It is a bit confusing about the word error rates in Fig. 3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. 
Some grammar issues need to be reviewed. For example, at line 394 in section 4.2:      Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ...."" ","3. The description in the paper use word error rate (WER) all the time. But in Fig. 3, the authors used (100 - WER) as the vertical axis. It is not clear about the motivation of doing this. Note: the WER can exceed 100% in some cases. Some grammar issues need to be reviewed. For example, at line 394 in section 4.2: Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the com396 bined version (7 entity tags) -> ""we to ....""",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5339,NIPS_2020_1039,"1) It could be better if a more comprehensive comparison of the asymptotic performances among on/off-policy methods. Off policy methods enjoy better sample efficiency at the cost of higher computation burden. Maybe an additional table could be provided in the appendix. Personally, I don’t actually expect a gap between the on/off-policy methods. 2) Although the usage of current f-function in the f-divergence is justified, it will still be interesting to see a comparison if the alternative f-functions were adopted. An empirical comparison would further justify the usage of the author’s current choice. Minor: 1) In eq. 12 (the proof of Lemma 2), in the third from the last line, the integration should be over \mathcal{S} \times \mathcal{S} instead of \mathcal{S} \times \mathcal{A} \times \mathcal{S}. 2) Notations in sec. 8.6 are inconsistent: sampling from \pi is switching back and forth between $a \sim \pi(s)$ and $a \sim \pi(\cdot | s)$. 3) At the end of Algorithm box 1, does J_{\nabla \theta} J_{reg} (\pi_{\theta}) intends to mean that the gradient of \theta on J_{reg}? 4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?",4) Can the authors slightly justify the first equation in sec. 3.2? Why is that an equality instead of an inequality?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
352,ICLR_2022_2232,"Weaknesses
The writing should be improved overall. Issues throughout include grammar, misuse of commas, capitalization (as in reference to algorithm 1 on page 6), technical details left out, poor formatting.
There seems to be a lack of novelty in the modeling approach: the same training schemes, DNN model, etc have been developed before, in papers that are cited by this one.
The results are rather limited and do not seem to show a clear advantage over standard techniques.
Some sentences have unclear meaning, e.g.: Page 1 - ‘a manual adaptation and business knowledge are needed…” - ‘its dynamic dimension reflects directly the demand change over the learning steps…” Page 2 - ‘Before we deep dive in the model architecture and present its main components, we should briefly highlight some problem-related concepts’ - the narration is too casual Page 5 - Equation 13 label is cut off - means that violation is given -infinity in what sense? A large floating point number? Page 6 - Training: Sample rollout and greedy rollout - are these ever defined? What is the baseline function used in the REINFORCE algorithm? Page 7 - Google OR-Tools baseline - what are the details of the implementation? Is it the CP-Sat solver? A specialized solver? - What is the meaning of the numbers in Table 1? Is lower better? - Why does RNN-RL appear twice in Table 2 with different results? - Is OR-tools called with a solver timeout? Or it is allowed to run to completion? - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method?
General questions:
Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case?
Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper?
What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.",- Is OR-tools called with a solver timeout? Or it is allowed to run to completion?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3459,NIPS_2020_1464,"- significance and novelty: is quite similar to closely related work [13,14] (and maybe PathNet), but does not compare to them. -- The discussion says ""the major difference between [13,14] and ours is that they treat their network depends on the task, which might not be suitable for online continual learning setting."" So the only difference is that they may underperform. This paper either has to compare to [13,14] (especially [13] is very similar and open-source), or show numbers in efficiency (FLOPs, test time, etc) to show that they cannot be used efficiently. - claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?","- claims and evaluation -- fairness of number of parameters: As far as I can tell, HAT has 7.1M params, A-GEM uses ResNet18 which is 11M params. This paper uses 43M params which is not fair. It is also not fair to say all methods have similar efficiency at test time, since multiple blocks in the same layer can be on. -- The exploration trick improved performance quite a lot. It is great that this paper did an ablation study, but since it has a lot of hyperparameters and design choices (why H^1/2 not H, why sigmoid, how are gamma/kappa/epsilon chosen), it is unclear how the design choices are made and how the hyperparameter was tuned, as this would tweak the balance between learning and remembering. An analysis of sensitivity to hyperparameters is preferred. -- The paper is not *entirely* an online method, since the model is pre-trained on the first task (with multiple epochs) to make NAS training work. --- Was line 253 epochs referring to this? Or did this paper train anything else with more than 1 epoch?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4223,NIPS_2020_1003,"1. The study among different adversarially trained models is missing, thus the trade-off is unclear among robust trained models. For example, the TRADES model may improve both the robustness and back-door robustness. 2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off. 3. Experiment details missing. It is unclear to the reviewer whether the data for the adversarial training is poisoned or not. Would adversarial training still work under poison data? Would that mean successful backdoor attack (weak back-door robustness) also reduce the adversarial robustness? Maybe a figure showing the trade-off under this setting is missing. 4. Too few steps of attack for adversairal attack (only 5 to 10 steps), it is may not access the true adversarial robustness.","2. Following the point above, it is unclear whether the trade-off still holds when the models that are partially adversarial robust. Since the results are present in two extreme without the middle results. For example, models with 10%,20%, 30% adversarial robustness accuracy. A curve with some reasonable resolution is needed to show the trade-off.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
207,ICLR_2022_1625,"Weakness
--- The overall technical contribution is limited. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2].
-- The use of the sine activation function is not well supported by the experiments. In table 1, sine function shows marginal improvement over tanh, and in table 5 of the appendix, using tanh shows much better performance over sine. Furthermore, tanh tends to become saturated during backpropagation of a long sequence. I would suggest the author also compares with ReLU.
-- Presentation in the paper can be improved:
The modified Adam optimizer is listed as one of the technical contributions. However, it is not stated clearly what is the modification and how does it compare to the original Adam optimizer.
The name of the robots are presented in Fig. 7 of Sec. 4.5 but are referenced multiple times in previous sections and table 1. It would be helpful to clarify the name of the agents early on.
Reference of Fig. 5 at the end of Sec. 4.2 seems to be incorrect, as no comparison of SGD and Adam is shown in Fig. 5.
[1] Lee, Joonho, et al. ""Learning quadrupedal locomotion over challenging terrain."" Science robotics 5.47 (2020). [2] Zhao, Allan, et al. ""RoboGrammar: graph grammar for terrain-optimized robot design."" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.","5. [1] Lee, Joonho, et al. ""Learning quadrupedal locomotion over challenging terrain."" Science robotics 5.47 (2020). [2] Zhao, Allan, et al. ""RoboGrammar: graph grammar for terrain-optimized robot design."" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-16.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1305,ICLR_2023_1603,"Weaknesses - 1. The main drawback of the design is that it relies heavily on a centrally available dataset. One of the two primary goals of the system is to handle non-IIDness in the data, which raises the question - how does the performance of F2L depend on the quality of the root dataset at the server. How well does the root dataset represent the non-IIDness present among the clients? How is scalability affected if the root dataset is not updated to well represent the newly joined clients? More experiments are required to convince the reader that the system can do well even when the root dataset does not exactly represent the data distribution among the clients. 2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance? 3. Figure 2c shows the performance of F2L when a client is injected into the system midway during the training. F2L can be seen to perform better than vanilla FL. Can this be attributed to knowledge distillation? How would it compare with Fed-Distill? How sensitive are the observations with respect to the knowledge distillation parameters - lambda and temperature? 4. F2L relies on switching between LKD and FedAvg after sufficient convergence has happened. How is this threshold chosen? What can be a general way to choose this value for any dataset? 5. Figure 3 shows that a student can outperform a teacher in F2L. This experiment was performed on EMNIST. Does this observation hold in general, independent of the dataset? If not, what conditions does this depend on?",2. Table 1 shows that F2L performs significantly better than Fed-Distill. The lower performance of the other benchmarks can be attributed to the fact that they do not leverage any information from a root dataset. What essentially leads to this improvement with respect to Fed-Distill? Do they both use the same root datasets? Is Fed-Distill well tuned for best performance?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
493,ICLR_2022_1823,"Weaknesses
The main weakness is that the relationship between ADU and constituency tree is not clearly described. Is it true that ADU are often phrases that occur in the constituency tree? How often does this happen? Does the new BERT-based model adhere to constituency constraints? Is the BENEPAR parser appropriate for this data? Based on Trautmann et al.’s comments on grammaticality and clauses, my intuition is that an ADU is almost always a phrase in the tree, in which case it is somewhat less surprising that it helps this task (and maybe it should be helping even more). If this is the case, it’s worth considering related work in distant supervision to include.
The treatment of constituency trees is haphazard. 1) At times it is not clear if dependency or constituency trees are being used; 2) Table 1 should be made more clear, what do the percentage values indicate, and why not use the full tree at all; 3) It is worth adding to the related work more work in constituency tree representation, such as but not limited to Yang and Deng 2020 that also use GNN to represent constituency tree.
(low priority) Some technical details are concerning as described. For example, it is true that batching heterogeneous graphs may be somewhat more challenging than batching similar length sequences, but it is hard to believe this is one of the “major difficulties” of this work. If it is such a challenge, then it may have warranted further discussion about tradeoffs in architecture selection and impact on speed or performance.",1) At times it is not clear if dependency or constituency trees are being used;,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4008,NIPS_2020_404,"- What strikes me as surprising is the direct usage of the function f as the part embedding and point embedding. This needs further justifications, as ideally a BAE-like network should assign a hard per-part occupancy score of {0,1}. e.g. if the point coordinate belongs to the wing of an airplane, then the ideal f should output a one-hot vector. Clearly networks struggle in doing so, thus creating non binary outputs. However, this is more of a bug than a feature. Yet, the authors here exploit this approximation by making the network encode per-point subtleties into this approximated 1-hot. My intuition tells me this is not a principled way of doing it and perhaps a point embedding should be an intermediate prediction from which another few layers convert it into a part embedding? - The CR loss assumes all sampled points on the source have a corresponding point on the target. However, as explained by the authors, often part may appear and/or disappear. How is this being addressed? minor: line 109: a more detailed definition of f^-1 is recommended. In particular, it should be made explicit that f^-1 takes as input both the semantic embedding as well as the shape code and retrieves the coordinate. references: Probably relevant is this recent work that also uses implicit neural function to register between a partial and a full shape: ""The Whole Is Greater Than the Sum of Its Nonrigid Parts""","- The CR loss assumes all sampled points on the source have a corresponding point on the target. However, as explained by the authors, often part may appear and/or disappear. How is this being addressed? minor: line 109: a more detailed definition of f^-1 is recommended. In particular, it should be made explicit that f^-1 takes as input both the semantic embedding as well as the shape code and retrieves the coordinate. references: Probably relevant is this recent work that also uses implicit neural function to register between a partial and a full shape: ""The Whole Is Greater Than the Sum of Its Nonrigid Parts""",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4072,NIPS_2020_1186,"Theoretical Grounding: - Since the regret bound relies on bounding the state reconstruction error at each point in a finite-length sequence, it seems clear to me that the bound only applies to MBRL algorithms that rely exclusively on full-episode rollouts for planning (e.g. PILCO). This criticism in particular seems to apply to the practical algorithm used in the submission, since it relies on a parametric value function approximator to make the planning horizon more tractable. - In practice we don't know \beta _a priori_. How should \beta be chosen? Empirical Evaluation: - The method is not evaluated in a stochastic environment. If you have set out to solve the issue of conflation of epistemic and aleatoric uncertainty, you should evaluate your method in a stochastic environment, not deterministic Mujoco environments. As it is the experiments give the impression of a bait-and-switch. - No comparison to competing methods only an ablation of the proposed exploration strategy with greedy improvement and approximate Thompson Sampling. - No ablations of β (presumably a crucial design choice). - No demonstration that their dynamics models as implemented satisfy their calibration assumption. Significance/Impact: - Information on the specific implementation details is fairly sparse (e.g. what learning rate and batch size did you use for your dynamics models? Did you reuse a previous implementation of MPO?). Reproducing the authors' findings would likely prove very difficult. MBRL is notorious in the machine learning field for reproducibility issues. If you (the authors) had to reimplement your method from scratch tomorrow, what details would you need?",- No ablations of β (presumably a crucial design choice).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1231,ICLR_2023_3550,"Weaknesses
I am not entirely sure about the use of individual training accuracy gain as part of the metric, I think we should technically look at user-level accuracy, where we have at least a single sample held out test for a given training sample thereby measuring generalization accuracy gains. I think measuring the gain on training samples is a bit meaningless as technically the default accuracy on those samples could be 100% for that user, if we do like a KNN classifier.
The measurement of privacy expenditure is inconsistent, there should just be 4 privacy budget epsilons and all experiments done on those, as opposed to different epsilons and even different ways of reporting. Like for one method epsilon is reported,for another the standard deviation of the noise added is reported (which could be converted to epsilon and I think should be for presenting results). All this said, I find stacking all these inconsistent guarantees together like that in Figure 3 in appropriate.
I am not sure I fully understand the two lines of reasoning for why the naive definitions of IF are bad, in section 3.2: • A training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users’ data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense. • A training algorithm is individually fair if the difference in privacy risks between any pair of users is small. This definition can be satisfied by reducing privacy risks with privacy-preserving ML because the differences are small if all users’ privacy risks are small. However, strong privacy protection with DP is known to degrade classification performance --> There are two issues with this: a) the only way to get similar privacy risks is not by applying privacy preserving methods, there could be other ways out there, so to say that this is achieved by strong DP guarantees is a bit inaccurate, and b) there are recent papers that show DP can be achieved with little loss to accuracy [3-5]
I am not really sure what the conclusion “necessity of the proposed IF for NNs'' really means. If it means we need improvements, there are some group level improvements, why not try them and then test? Improvments like [1-2]. I think the fact that the only conclusion from the paper is that individual level fairness is not good in privacy preserving methods is a bit repetitive gien prior work.
[1] Tran, Cuong, Ferdinando Fioretto, and Pascal Van Hentenryck. ""Differentially private and fair deep learning: A lagrangian dual approach."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021.
[2] Fioretto, Ferdinando, et al. ""Differential Privacy and Fairness in Decisions and Learning Tasks: A Survey."" arXiv preprint arXiv:2202.08187 (2022).
[3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12. (ICLR 2022)
[4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13. (ICLR 2022)
[5] Tramer, Florian, and Dan Boneh. ""Differentially Private Learning Needs Better Features (or Much More Data)."" International Conference on Learning Representations. 2020.",• A training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users’ data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2297,ACL_2017_71_review.json,"Weaknesses:  -The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. 
  -README file for the dataset [Authors committed to add README file] - General Discussion:  - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. 
  - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)  - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. 
  - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. 
  - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference ","- Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
811,ICLR_2021_1181,"Weaknesses
1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3412,NIPS_2020_1180,"Here are the following things that I think are missing from this work and should be addressed: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture. 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach. 3. I have some reservation about the evaluations. Please see the next section for specific details.","1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4916,NIPS_2020_1560,"The paper has a number of issues which are beyond the authors control. Most are caused by the extreme publication speed in the field and the limitations of the conference publishing model. Indeed all of my major concerns are of this type: 1. The use of ImageNet-V2 as primary example: The main issue I find in the paper is the use of ImageNet-V2 as the primary example for “natural distribution shift”. There is indeed a large gap between model performance on ImageNet and ImageNet-V2 but almost all of this can be attributed to subtle effects that arise in the dataset replication process as described by Engstrom et. al. 2020. Using this to criticize the paper is however unfair as Engstrom et. al. 2020 was published on 19. May 2020 only a week before the abstract deadline. ### Post-rebuttal comments: I did not know Shankar et. al., ICML ’20. Thanks for pointing out that reference. I have to look more deeply into this but judging from a quick read their results do indeed change my perception on the performance gap in ImageNet-V2. Nevertheless I think ObjectNet is the more obvious example and should be front and center. As Djolonga et. al. 2020 show it's the least correlated with other benchmarks with a very different design making it (in my current view) the most interesting of the selected benchmarks. 2. The definition of “distribution shifts arising in real data”: While the distribution shift from ImageNet to ImageNet-V2 has mostly been explained by Engstrom et. al. 2020 those to ObjetNet, ImageNet-vid-robust and YTBB-vid-robust can reasonably be expected to be real and existent. They do however only cover a subset of distribution shifts arising in real-world images. As to what is missing take for example ImageNet-R which was recently introduced by Hendrycks et. al. 2020. ImageNet-R shows real-world images not of the original objects but of different artistic renditions like paintings or sculptures. In this case some robustness interventions do have an effect. So the discussion of what constitutes a “natural distribution shift” has to be more nuanced. But as before this information was not available to the authors at time of submission because Hendrycks et. a. 2020 was published 3 weeks after the submission deadline. What turns this situation absurd is the fact that Hendrycks et. al. 2020 heavily builds upon what can readily be assumed to be an earlier version of the present article (not cited here to keep the double-blind mechanism as much intact as possible). ### Post-rebuttal comments: Thanks for including ImageNet-R even though it doesn't make the story easier. The dedication to completing the testbed is really amazing. 3. Big Transfer (BiT) models are missing from the analysis: The recently published Big Transfer model family (Kolesnikov et. al. 2019) was shown to have stunning generalization properties. The most interesting model of that family, BiT trained on the JFT300 dataset, has however not yet been publicly released. After seeing the L2-NoisyStudent model perform so well it would have been interesting to see if BiT-JFT can live up to it’s hype. Instead the authors of BiT have released their own robustness study using partly similar methodology as in the paper presented here (Djolonga et. al. 2020). This should not imply that Djolonga et. al. 2020 is biased or does something wrong but simply illustrate how fast paced the field has become. ### Post-rebuttal comments: Thanks for including BiT-M and reaching out to the authors. As I said above the commitment to completeness is great! 4. Too much information for 8 pages: It is pretty obvious that the amount of content presented in this paper is more than fits 8 pages in the NeurIPS template. I think the authors did a good job presenting their work in that format but when reading the paper it is still noticeable that there was much more content than could fit. It becomes even more obvious when reading the appendix which is full of exciting experiments that provide valuable information but have a good chance of being overlooked there. I want to repeat here that these problems are beyond the authors control. Most of it is caused by the huge amount of related work that was done in parallel and the conference submission system makes it impossible to publish longer papers or significantly update them during the review process. As a result I think it would be unfair to judge the submission based on these flaws. I would still appreciated if the authors could adapt their interpretations and related work prior to submission. Thus the following suggestions contain points regarding the above mentioned issues: 1. Use ObjectNet instead of ImageNet-V2 as the go to example (especially in Figures 1 & 5 as well as in Section 4) 2. Discuss different possibilities to select distribution shifts in real data in the introduction. State and motivate your choice. If I was asked I’d call them distribution shifts to ImageNet like images, as opposed to sketches, renditions, images with specific environmental factors like nighttime scenes or images taken in bad weather etc. which have a specific and easy to point out distribution shift. I think the second to last paragraph of the broader impact statement does a good job in justifying and contextualizing this approach and could be used here. 3. Follows thereof: Try to be a bit more specific as the chosen “natural” distribution shifts are just a subset of what is possible. The paper sometimes reads as if it covered all natural distribution shifts while it doesn’t (especially in section 1). Smaller suggestions: 4. Place “Dataset shifts” before “Consistency shifts” in section 3.1.1 or mention the video datasets in the first section 5. Change the description of “Image corruptions” in 3.1.2 removing the statement that you used corruptions from Geirhos et. al. 2019 which according to the appendix were not used and either don’t mention the number of corruptions (38) or explain why it’s 38 and not 19. I’d probably just remove that number as the nuanced discussion of “in memory” and “on disk” corruptions is only mentioned in the appendix. 6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”",6. Specify which dataset \rho is computed on in section 41. “Dataset shifts”,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1629,ICLR_2023_840,"Weaknesses:
Some important related works are missing: [1] tackles temporal causal discovery with Neural ODEs that would be able to handle inconistent sampling intervals, [2] performs joint structure learning and data imputation, [3] performs temporal causal discovery using the NOTEARS framework for continuous DAG learning. Methods based on the same framework have been applied to static data for joint causal discovery and data imputation [4]. All these weaken the novelty of this paper.
The paper mentions that it is based on Granger causality. However, the current formulation also allows for an interpretation as an additive noise model: ie x i = f ( p a i ) + e i
. Could you please comment on this? This interpretation would also allow for the identification of the temporal causal graph A 0 , τ
rather than just the summary graph A ^ = m a x t A t .
Please add some comments about the difference between Granger causality and ANMs or PCMCI that also identify the temporal causal graph.
It is unclear how τ m a x
is chosen. Is this assumed to be known? What if this isn't known?
All experiments use missing data. It would be great to see a baseline comparing to datasets with full observability. Misc:
What's the intuition of using the moving average as a training signal for the imputation network?
For the graph discovery stage - do you calculate an expectation over multiple graph samples or is this amortised over different batches? Or do you use the same graph sample for optimising this loss?
Please explain ZOH earlier in the text.
Eq 3: what is e
? What's the assumption about it? This might make or break the use of the L2 loss.
Please pay attention to the use of \citep and \citet.
Eq 5: You use inconsistent τ = 0. . . and τ = 1 , . . . .
p5 just above eq 11: I believe n 2
should be n 3 .
p2: "" to conduct causal inference and .."" - should this be ""causal discovery""? Causal inference tackles the question of inferring causal estimates (e.g. ATEs).
[1] Bellot, Alexis, Kim Branson, and Mihaela van der Schaar. ""Neural graphical modelling in continuous-time: consistency guarantees and algorithms."" International Conference on Learning Representations. 2021. [2] Morales-Alvarez, Pablo, et al. ""VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups."" arXiv preprint arXiv:2110.08223 (2021). [3] Pamfil, Roxana, et al. ""Dynotears: Structure learning from time-series data."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [4] Geffner, Tomas, et al. ""Deep End-to-end Causal Inference."" arXiv preprint arXiv:2202.02195 (2022).",... p5 just above eq 11: I believe n 2 should be n 3 .,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1073,ICLR_2023_642,"Weaknesses:
Unclear notations. The authors used the same notations to write vectors and scalars. Reading these notations would be challenging to follow for many readers. Please consider updating your notations and refer to the notation section in the Formatting Instructions template for ICLR 23.
The framework impact is unclear. The authors mentioned that the case of intrinsic but known bias and variance is often the case in computational neuroscience and neuromorphic engineering. This is the main motivation for their approach. However, the framework provided is limited to specific cases, namely, white noise and fixed bias. The authors argue that their assumptions are reasonable for most cases computational scientists and neuromorphic engineers face, but they don’t provide evidence for their claims. Clearly, this framework provides an important way for analyzing methods such as perturbed gradient descent methods with Gaussian noise, but it’s unclear how it can help analyze other cases. This suggests that the framework is quite limited.
The authors need to show that their choices and assumption are still useful for computational neuroscience and neuromorphic engineering. This can happen by referring to contributing and important works from these fields having known bias and variance with Gaussian noise.
In the experiments, the used bias is restricted to having the same magnitude for all weights ( b 1 →
). Can we reproduce the results if we use arbitrary biases? It would be better if the authors tried a number of arbitrary biases and averaged the results.
The paper is not well-placed in the literature. The authors didn’t describe the related works fully (e.g., stochastic gradient Langevin dynamics). This makes the work novelty unclear since the authors didn’t mention how analyzing the gradient estimator was done in earlier works and how their contribution is discernible from the earlier contributions. Mentioning earlier contributions increases the quality of your work and makes it distinguishable from other work. Please also refer to my comment in the novelty section.
Missing evidence of some claims and missing details. Here, I mention a few:
It’s not clear how increasing the width and/or depth can lower the trace of the Hessian (Section 2.1). If this comes from a known result/theory, please mention it. Otherwise, please show how it lowers the trace.
The authors mentioned that they use an analytical and empirical framework that is agnostic to the actual learning rule. However, the framework is built on top of a specific learning rule. It’s unclear what is meant by agnostic in this context (Section 1).
The authors mentioned in the abstract that the ideal amount of variance depends on the size and sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. However, the authors didn’t mention the sparsity dependence anywhere in the paper.
The authors mentioned in a note after the proof of Theorem A.5 that it is also valid for Tanh but not Sigmoid. However, the proof assumed that the second derivative is zero. It’s unclear whether a similar derivation can be developed without this assumption. However, the authors only mention the relationship with the gain of ϕ ( . ) .
More information is needed on how the empirical likelihood of descent is computed (Fig. 7).
The use of MSE should be mentioned in Theorem A.3 since it’s not proven for any loss function. In addition, the dataset notation is wrong. It should be D = { ( x 1 , y 1 ) , . . . , ( x M , y M ) }
, where M
is the number of examples since it’s a set containing input-output pairs, not just a single pair.
The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead.
A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1).
The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work.
It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary.
The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30.
It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance.
The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results.
Alternative hypotheses can be made with some experiments. The experiment in Fig. 3.A needs improvement. The authors mention that excessive amounts of variance and/or bias can hinder learning performance. In Fig. 3. A, they only show levels of variance that help decrease loss. An alternative explanation from their figure is that by increasing the variance, the performance improves. This is not the case, of course, so I think the authors need to add more variance curves that hinder performance to avoid alternative interpretations.
Minor issues that didn’t impact the score:
There are nine arXiv references. If they are published, please add this information instead of citing arXiv.
What is a norm N
vector? Can you please add the definition to the paper?
You mentioned that the step size has to be very small. However, in Fig. 1, the step size used is large (0.02). Can you please explain why? Can this be an additional reason why there is no smooth relationship between the values of the variance and performance?
No error bars are added in Fig. 4 or Fig. 7. Can you please add them?
In experiments shown in Fig. 3 and Fig. 5, the number of runs used to create the error bars is not mentioned in Appendix B.2.
A missing 2 D
in Eq. 27.
In Theorem A.3 proof, how the input x
has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d
, not d ?",".. , ( x M , y M ) } , where M is the number of examples since it’s a set containing input-output pairs, not just a single pair. The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead. A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1). The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work. It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary. The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30. It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance. The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results. Alternative hypotheses can be made with some experiments. The experiment in Fig.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4133,NIPS_2020_73,"1. While I find the combination of single-image 3D reconstruction and GAN interesting, I am concerned about the technical contribution of the paper. It seems that each component is similar to previous works. The single-image 3D reconstruction network is almost identical to [24], and the GAN network also are standard. It feels like the contribution of the paper is just a combination of these two tasks. 2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network. 3. Why using the sinusoidal encoding in the network? How does it compare to directly using the (u, v) coordinates? Overall, I like the results of the paper. However, I am not fully convinced about the choice of the framework, particularly for the questions discussed in point 2. The technical contributions of the proposed method is also not significant to me.","2. Another solution to the proposed task here is that first training a 2D GAN to generate new 2D images of specific category, and then directly run the single-image reconstruction network such as [24] to generate textured mesh from the input image. The paper should include a comparison to this baseline. My sense is that currently GAN can generate very high-quality 2D images from sampled latent codes and text. It should be easy to directly generate resonable textured meshes from the high-quality 2D images. It is not clear to me why the proposed framework would outperform this baseline, considering that the performance of the proposed method is also bounded by the performance of single-image 3D reconstruction network. In addition, this alternative solution would be more flexible than the proposed method, since you can use arbitrary GAN network to generate 2D images without re-training the reconstruction network.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1433,ICLR_2023_2880,"Weaknesses: 1. The first question is that the evidence of the motivation is not direct. Since the problem to be solved is that “a predictor suffers from the accuracy decline due to long-term and continuous usage”, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. 2. Another question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match. 3. Is there any related work about the mixed-batch method?",3. Is there any related work about the mixed-batch method?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
531,ICLR_2022_1119,"weaknesses, starting from the most significant ones.
Assumptions and Threat Model? This is probably the only “true” problem of the paper, which should be absolutely rectified. I was not fully able to understand the assumptions made by Tesseract. Does it work “only” against the “directed deviation attack” proposed by Fang et al.? Or does it also protect against different attacks? In general, Section 2.2, Threat Model, is not very comprehensive. The authors should better expand this section by clearly pointing out all the assumptions and requirements of the proposed method. This is especially true because the Fang et al. attack was proposed in 2020, and some of its assumptions are not yet well-known. Specifically, this statement is suspicious: “We assume a full-knowledge (white-box) attack where the attackers have access to the current benign gradients.”. Does it mean that Tesseract only works under this assumption? I.e., the attacker knows, and exploits, the current benign gradients? This is a rather “unrealistic” assumption: I understand the willingness to work against “worst case” scenarios; yet, if such “worst case” scenarios are not realistic in the first place, then what is the purpose of the proposed mechanism? What benefit is there in protecting against an attack that will never happen in the first place? I invite the authors to restructure this section by using the common taxonomies adopted in adversarial ML papers [I].
Problem or Feature Space attacks? The authors perform their experiments on four well-known datasets: MNIST, CIFAR, Shakespeare, FEMNIST; for each dataset, a different (deep) ML model is targeted. Three of these datasets are of images, whereas Shakespeare contains text data. There are different ways to create “adversarial examples”, depending on the ‘space’ where the perturbation is applied. As far as I am aware, the adversarial examples considered in this paper to perform the poisoned updates are created in the feature space. It would be a lot more interesting if at least one evaluation included adversarial examples generated in the “problem” space [A]—or, at the very least, considered samples generated by “physically realizable” adversarial perturbations [B]. I acknowledge that the method should work even in these circumstances, as the proposed Tesseract defense is agnostic of the process used to apply the perturbation. However, considering the strong relationship with (real) security that permeates the paper, I believe that a more convincing use-case would dramatically improve the quality of the paper. This is also motivated by the current state-of-the-art: after almost a decade of adversarial attacks, more recent efforts are leaning towards evaluation that consider more realistic circumstances, where the attacker is constrained by the limitations of the real world; this is even more true in “distributed system” scenarios, such as Network Intrusion Detection Systems, which bear a strong relationship with federated learning (e.g., [C, D, E, F]). As such, I invite the authors to perform an additional “proof-of-concept” experiment where they consider adversaries with constrained capabilities. This is also motivated by the fact that some perturbations may yield different effects when created in the problem space (as shown in [A]).
Tradeoff? A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients. Even if the baseline performance does not decrease, what is the overhead of the proposed method? For instance, in Table 2 the authors report some results for “Attack=None”, which I assume represent the accuracy when no attack takes place. However, all the rows of these experiments (namely, FedSGD, Tesseract, Faba, FoolsGold, FLTrust) consider hardening FL techniques; for instance, on MNIST the proposed Tesseract has an accuracy of 92.52 when no attack takes place—the best among all other defences. Despite being appreciable, I am interested in knowing the performance when NO defense is applied. Surely, the test accuracy in a “fully trusted” FL setting should be superior than 92.52. Hence, I ask: what is the ‘cost’ of Tesseract?
Lack of a concrete use-case. I believe that the paper could be further improved with a concrete use-case, where the authors explain, step-by-step, how a (single, or multiple) attacker can compromise a federated learning system, and how the proposed method can help in solving such problem. Hence, I request the description of a concrete use-case explaining the abstract scenario reported in Figure 1. Such use-case can be at the basis of the “constrained” attack that I invite the authors to perform in my ""problem space perturbations"" suggestion.
Some additional issues:
• In the Introduction, the authors state: “To counter this threat, a set of approaches has been developed for countering Byzantine clients in FL…”. I believe that “Byzantine Clients” is a wrong term: what is countered by Tesseract are not byzantine clients, but ""unloyal"" clients, that are “against” the byzantine clients (at least by referring to the well-known problem of the byzantine generals, which should agree on a method to reach consensus in the presence of unloyal generals).
• The caption of Figure 1 has a typo “c out of m clients maybe be malicious”.
• In Figure, the gradient “LM_{c-1}” is out of place.
• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?
• Figure 1 appears before Figure 2, but in the text it is referenced after Figure 2.
• Putting Figure 2 so early on is very confusing. The “flip score” is a measure introduced for the first time in this paper. As such, any reader would be thrown off by such graphs before reading the paper, meaning that the findings of Figure 2 are difficult to interpret---during the Introduction---, as the flip score has not been defined yet. As such, such graphs are ultimately meaningless: I have to trust the authors that they correspond to “interesting” observations and “fair” experiments, which is not scientific.
• The presentation and notation in the “Flip-score” (page 5) is very ugly and difficult to follow.
• Section 5 should be merged in Section 6
• W.r.t. Table 2, the authors state “We see that TESSERACT is the winner or 2nd place finisher in 7 of the 12 cells (benign + two attacks * 4 datasets)”. This should be better highlighted. I only see three bold values for Tesseract in Table 2.
• W.r.t. Table 2, the authors state “We have not shown the test loss curve for Krum aggregation because of the large loss values.”. I invite the authors to report such values in Table 2, because the different “formats” of the three subtables (None, Full-Krum, Full-Trim) make this table very hard to interpret.
EXTERNAL REFERENCES
[A]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[B]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Conaml: Constrained adversarial machine learning for cyber-physical systems."" Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021.
[F]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.
[I]: ""Wild patterns: Ten years after the rise of adversarial machine learning."" Pattern Recognition 84 (2018): 317-331.","• In Section 2, the authors state “Our simulation of federated learning consists of m clients, each with its own local data, but the same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1”. Is there a minimum amount of “m”?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2329,ACL_2017_318_review.json,"Weaknesses: 1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. 
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. 
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? 
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. 
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. 
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. 
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"". 
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2? ","3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5163,NIPS_2020_1524,"Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - ""philosophical note"" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.","3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the ""desired"" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1904,ARR_2022_306_review,"1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? 
2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. 
3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. 
4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. 
1. In lines 382-384, it is mentioned that ""We have discarded long samples (>1024 token length) from validation and testing data as well."". I think it is not appropriate to throw any examples from the test set. ","1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1972,ARR_2022_357_review,"- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.
- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.
- No error analysis is provided. 
- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.
Typos: - one conclusions --> conclusion ","- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3946,NIPS_2020_1659,"The ability of EvolveGraph to uncover known dynamic relations is not explored in as much detail as it could be. More specifically, the one synthetic experiment designed to evaluate this is somewhat simple, in that all relations change from ""active"" to ""inactive"" for all entities at the same moment in time, and this switch happens once. What happens when relations change at different times for different variables? What happens if the re-encoding gap is ""out of sync"" with the actual change in relations? How well does the model perform if relations change multiple times aperiodically? These questions are not explored here. There are a few modeling decisions which are made that are not explained or explored either. The ones that stick out to me: - The observation model has learned attentional coefficients that seem to be static across time. Do these contribute meaningfully to model performance? Also, doesn't the fact that these coefficients are static mean that they ""pre-determine"" the impact some variables have on others in a data-agnostic manner? - A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance? - How many modes does the model predict, and how does performance vary as the number of predicted modes changes? Right now, it's difficult to understand if the performance improvements are primarily due to modeling multi-modality, modeling dynamic relations, or both. These criticisms are relatively minor, however; there is enough present in this work for it to be a worthwhile publication.",- A different prediction mode is selected for each variable for every time step. What happens if modes are re-evaluated less often? How do the frequency of mode selection and relation re-prediction relative to each other impact final performance?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1232,ICLR_2023_3724,"Weaknesses:
The paper has low readability. A lot of the issues are certainly fixable but in its current form, it is confusing enough to distract from evaluating the technical contributions of the paper. Certain examples are:
“DeVAE surpasses 2% for β-TCVAE and 9% for β-VAE.” In what terms exactly?
Many instances in the introduction talking about spreading the conflict of disentanglement and reconstruction over time and space is not easy to follow and understand.
“However, in this work, we get rid of calculating TC by leveraging the narrow information bottleneck (Tishby et al., 1999; Burgess et al., 2018) to find efficient codes for representing the data, which promotes disentanglement.” This is pretty confusing right where it is in the introduction and only becomes somewhat clear after reading the method section.
The authors claim that DynamicVAE suffers from Information Diffusion problems. If that is the case, wouldn’t that result in low disentanglement scores or at least high variance across different seeds for DynamicVAE? But, that’s not the case in Figure 2.
The experiment for high-dimensional latent space is weak. 1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space?
A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set?
What are the layer embeddings? They seem to be an important component of the method but are never explained.
How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper?
Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.","1024 dimensional latent space for dSprites seems unrealistic, it ideally should be for a dataset that requires high-dimensional latent space. And, there are no quantitative numbers. Why is DeVAE worse for low dimensions? A more high-level question would here is — what exactly in DeVAE makes it more compatible for handling high-dimensional latent space? A lot of design decisions are unexplained. How are the hyperparameters for other methods chosen? Are the beta values chosen for DeVAE selected based on test performance or on a validation set? What are the layer embeddings? They seem to be an important component of the method but are never explained. How much is the computational overhead because of the hierarchical latent space and how does it compare to other methods that are compared within the paper? Why is DeVAE not compared with FactorVAE and Cascade-VAE, they both seem highly relevant as well.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2353,NIPS_2021_1788,"Weaknesses: - The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3805,NIPS_2020_1707,"There are some concerns: 1. In line 82, authors should provide more explanations why they assumed linear constrains. How does it compare with non-linear combination in terms of performance and optimization speed. 2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability. 3. How does the cheap upsampling method (bicubic in the paper) influence the result? What is the limitations of upscaling factor, say will it fail if the factor is 8? 4. More comparisons and results from RAISR should be presented. 5. Experiments on image denoising and deblocking is very limited, lacking quantitative comparisons on benchmarks and intuitive explanation of this generalization.",2. How to prove the pre-defined dictionary is over-complete? How to compare the hand-crafted filters with learned filters? Experiments on Set5 is limited in data size and generalization ability.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5651,NIPS_2018_195,"Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently. - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed. - Line 177 typo: ""batchsized"" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2. - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions: - Will the authors be open-sourcing the code for the experiments? - Have you experimented with a constant \beta? - Have you experimented with having _both_ BN and VCL?  Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. ",- Have you experimented with having _both_ BN and VCL? Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5726,NIPS_2018_245,"Weakness] 1: Poor writing and annotations are a little hard to follow. 2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.  3: The motivation is to solve when the question doesn't focus on the most obvious visual concept when there are synonyms and homographs. However, from the experiment, it's hard to see whether this specific problem is solved or not. Although the number is better than the previous method, it will be great if the authors could product more experiments to show more about the question/motivation raised in the introduction.  4: Following 3, applying MLP after GCN is very common, and I'm not surprised that the performance will drop without MLP. The authors should show more ablation studies on performance when varying the number of facts retrieval, what happened if we different number of layer of GCN? ","2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1656,ICLR_2023_1511,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.","- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3604,NIPS_2020_356,"1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community. 2. Evaluation conducted using 102 scenes from the simulated dataset. There is no mention of why or how these scenes were selected, whether they are complex enough, or whether they pose a significant challenge. Will their proposed method scale to any environment? Some statistics about the selected scenes are necessary. How big are the selected scenes? How many rooms? How many of each target object in each scene? List the scenes used in the appendix for reproducibility. --POST-REBUTTAL--- I am happy with the author's response on this. 3. Though great to have reported real world transfer, the experiments run seem to be minimal and the evaluation premature. --POST-REBUTTAL--- I am happy with the author's response on this. 4. There is overall lack of a deeper discussion about the importance and implications of this work for the wider research effort in autonomous agents and how the authors expect the proposed method to scale. In addition, the paper is overly focused on results from the datasets and challenge, with little interpretation of these results. 5. No standard deviations are reported in the quantitative results. ---POST-REBUTTAL--- The authors have not addressed this concern. I strongly believe that results without any notion of deviation or error (without a good reason for their absence) are not reliable. 6. I am not sure if I missed it, but I could not find an explanation of the “Random” baseline. Is this random at the level of the low level action (local policy) or random at the level of the long-term goal? Also, the distance to success values reported in Table 1 are not that much larger than that achieved by the proposed model. This is of course difficult to judge because of the lack of more scene information or results interpretation. --POST-REBUTTAL--- I am happy with the author's response on this. I understand now that this is indeed a challenging task - looking at the very low success rate of baselines.","1. I am a bit concerned about how significantly novel this work is, as it brings together many existing methods. As a final result the algorithm presented in the paper seems to do great, but I am not sure whether it is a significant scientific contribution that can benefit the wider community. To me, it seems to be a well engineered approach that was designed to solve the CVPR 2020 Habitat Object Goal Navigation Challenge. --POST-REBUTTAL--- Having considered the authors' response and reviewer's discussion, I can see that there is value in the specific approach for the embodied indoors navigation community.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2313,ACL_2017_657_review.json,"Weaknesses: one of the main weaknesses of the paper lies in the fact that the goals are not clear enough. One overall, ambitious goal put forward by the authors is to use approaches from experimental psychology to interpret LSTMs. 
However, no clear methodology to do so is presented in the paper. On the other hand, if the goal is to validate sociological assumptions, then one should do so by studying the relationships between gender markers and the written justifications, independently on any model. The claim that ""expected gender differences (are) a function of theories of gendered self-construal"" is not proven in the study.
- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up.
As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed. ","- General Discussion: if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that ""these probabilities are valid ways to interpret the model"". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up. As mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1569,ICLR_2023_303,"Weaknesses: 1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it? 2. Regarding the proposed hierarchical patch embedding, can authors give more explanation about the specifical design as 2 consecutive MLP with ratio 3, how authors reach this design. As this is a critical module proposed, can authors provide some ablation studies about it, e.g. why uses a different MLP ratio of 3 as latter stages uses 4 instead? Will allocate computations differently to 56x56, 28x28 influence the final performance? What computation percentage should be used in patch embedding v.s. latter stage (14x14) for better performance?","1. There are several part of descriptions not clear enough. a. Table 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. b. Stage definition missing, though can be referred, but better have clear definition. c. Can authors also add Param and Flops to Table 2, as changing components will change computation as well. d. Incomplete sentences: Page 5, Step (c’) ‘causes a significant accuracy’; Sec 3.3 ‘all tokens in Stage ? are symmetric’ e. Merging operation details? One can only infer it from figure 1, can authors add some descriptions about it?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5633,NIPS_2018_197,"weakness of the paper: its clarity. From the presentation, it seems evident that the author is an expert in the field of computer algebra/algebraic geometry. It is my assumption that most members of the NIPS community will not have a strong background on this subject, me including. As a consequence, I found it very hard to follow Sect. 3. My impression was that the closer the manuscript comes to the core of algebraic geometry results, the less background was provided. In particular, I would have loved to see at least a proof idea or some more details/background on Thm. 3.1 and Cor. 3.2. Or maybe, the author could include one less example in the main text but show the entire derivation how to get from one concrete instance of A to right kernel B by manual computation? Also, for me the description in Sect. 2.4 was insufficient. As a constructive instruction, maybe drop one of the examples (R(del_t) / R[sigma_x]), but give some more background on the other? This problem of insufficient clarity cannot be explained by different backgrounds alone. In Sect. 3.2, the sentence ""They are implemented in various computer algebra systems, 174 e.g., Singular [8] and Macaulay2 [16] are two well-known open source systems."" appears twice (and also needs grammar checking). If the author could find a minimal non-trivial example (to me, this would be an example not including the previously considered linear differential operator examples) for which the author can show the entire computation in Sect. 3.2 or maybe show pseudo-code for some algorithms involving the Groebner basis, this would probably go a long way in the community. That being said, the paper's strengths are (to the best of this reviewer's knowledge) its originality and potential significance. The insight that Groebner bases can be used as a rich language to encode algebraic constraints and highlighting the connection to this vast background theory opens an entirely new approach in modelling capacities for Gaussian processes. I can easily imagine this work being the foundation for many physical/empirical-hybrid models in many engineering applications. I fully agree and applaud the rationale in lines 43-54! Crucially, the significance of this work will depend on whether this view will be adopted fast enough by the rest of the community which in turn depends on the clarity of the presentation. In conclusion: if I understood the paper correctly, I think the theory presented therein is highly original and significant, but in my opinion, the clarity should be improved significantly before acceptance, if this work should reach its full potential. However, if other reviewers have a different opinion on the level of necessary background material, I would even consider this work for oral presentation. Minor suggestions for improvements: - In line 75, the author writes that the ""mean function is used as regression model"" and this is how the author uses GPs throughout. However, in practice the (posterior) covariance is also considered as ""measure of uncertainty"". It would be insightful, if the author could find a way to visualize this for one or two of the examples the author considers, e.g., by drawing from the posterior process. - I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well? - I am uncertain about the author's reasoning on line 103. Does the author want to express that the mean is a sample from the GP? But the mean is not a sample from the GP with probability 1. Generally, there seems to be some inconsistency with the (algebraic) GP object and samples from said object. - The comment on line 158 ""This did not lead to practical problems, yet."" is very ominous. Would we even expect any problem? If not, I would argue you can drop it entirely. - I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.","- I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3433,NIPS_2020_1241,"1. The analysis on neural networks are direct results induced from the kernel method results. It may not be tight, and can even suffer from the curse of dimensionality. Hence, the different between two types of models showed in the theorems may not fully characterize the actual performance difference. Neural networks can possibly perform much better than the bound. 2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?","2. While the theorems requires the activation function to be smooth, in the numerical experiments ReLU are used. Maybe it is more illustrative if smooth activation function can be used. Is there a reason for not using tanh, sigmoid, etc?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
796,ICLR_2021_2043,"Weaknesses: - The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.
- Clarify the difference between S-Full and S-PGD from Experiments section. Since S-Full also uses T-step PGD, how it is different than S-PGD? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric, the performance gains are very minimal and not very significant than the baseline OE (Hendrycks et al., 2019b) under two metrics AUROC and AUPR.
Final thoughts: The proposed method is clearly motivated. Although the performance gains on adversarial robustness is significant, there are critical points yet to be addressed. Therefore, I marginally accept this paper.
Post rebuttal: The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper.","- The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner. It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate. In case such attack is infeasible, please provide the rationale behind that.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3455,NIPS_2020_725,"- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks. - One particular concern is that in this setup, as shown Figure 4a, a strong Hebbian learning rule leads to a much faster convergence than gradient descent, presumably because Hebbian learning quickly memorizes the training set. It might very well be that Hebbian learning leads to more progressive differentiation in tasks where it results in equal or slower convergence than gradient descent.",- One limitation of the present work is that it considers a very simple hierarchical task whose categories are perfectly linearly separable using the given features and a simple two-layer linear network. It remains to be seen whether the lessons obtained here will generalize to harder tasks and non-linear networks.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3000,NIPS_2022_1200,"Weakness: Originality:
1.I want to know if this paper is the first time to study the problem of the robust collaborative inference, where there are both arbitrary agents and adversarial agents. The arbitrary agents are easy to identify. However, I’m afraid the proposed method achieves a similar performance to identify the adversarial agents compared with baselines.
From Eq.(5), the framework aims to find a combined feature l
which is on the manifold and is near h
. The manifold projection could get a similar results for the adversarial sub-features. Could the authors discuss more about it? Writtings:
1.After so many times of reading, I guess I understand this paper. The authors introduce their method in Section 2.3, which is very simple. However, it relies block-sparse structure which is detailed stated in Section4. This could cause confuse when understanding the proposed method.
2.The notations are confusing. For example, h and l
both denote the feature. Why not use a letter (or with its variants)?
the citation format may be ICLR rather NeurIPS.
Theoretical analysis:
1.This paper provides an extensive theoretical analysis. In fact, I suggest the authors discuss more what the analysis means. Compared with baselines, why CoPur could do better.
2.Could the authors give an intuitive explanation about the effect of the sparsity α
on CoPur? Experiments:
1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c
and different Ω a d v ?
2.More analysis is helpful, for example, The comparison on optimization efficiency.
The authors discuss the limitations in Appendix. I have no other suggestions.","1.From the ablation studies, CoPur achieves a better performance compared with the manifold projection, what if there are different Ω c and different Ω a d v ?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1170,ICLR_2023_3572,"Weakness: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body. 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).","1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5211,NIPS_2020_1428,"- Novelty: This work can appear as incrementally innovative since Sobolev descent and unbalanced optimal transport were already present in the literature. This work is essentially the merging of these two existing works, resulting however in a new contribution. - Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.",- Empirical evaluation: The experiments are still a bit toyish and even on these toyish examples do not really outperform existing results.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3081,NIPS_2017_330,"Weaknesses
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed
In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation. ",- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
85,ICLR_2022_3099,"Weaknesses
W1: The setting seems to be limited and not well justified. 1) It only consider ONE truck and ONE drone. Would it be easy to extend to multiple trucks and drones? This seems to be a more interesting and practical setting. 2) What is the difference of this setting versus settings where there are multiple trucks? Are there methods solving this setting, and why are they not working in TSP-D? 3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.
W2: It is not clear why exactly an LSTM-decoder is better than an attention-based decoder. The paper justifies that ""AM loses its strong competency in routing multiple vehicles in coordination"". However, AM decoder still conditions ""on the current location of the vehicle and the current state of nodes"". Thus, I don't think it overlooks the interaction between different vehicles. It depends more on how you design the decoder. Compared to attention, an LSTM essentially adds to the historical decisions to the policy, not the interactions between vehicles. Therefore, it is not clear why exactly LSTM-decoder is better, and the justification is quite vague in the paper.
W3: Except for AM, NM by Nazari et al. (2018) has also been an important counterpart of the proposed HM. However, it is not compared as a baseline. Whereas I understand that not every baseline should be compared, but NM is mentioned a few times throughout. If historical information is important in decoding an action, why is it not important in encoding a state? Because of this, the empirical evaluation is not totally convincing to me.","3) In the second paragraph of section 2.1, the two assumptions that ""we allow the drone to fly for an unlimited distance"" and that, ""Only customer nodes and the depot can be used to launch, recharge, and load the drone."" seem to be contradicting? If you allow unlimited distance, why would the drones still need to be recharged? Am I misunderstanding something? Because of the limited setting, it may not be of interest to a large audience.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
587,ICLR_2022_1057,"Weakness:
Experiments: 1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments. 2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?",2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1723,ICLR_2023_2202,"Weakness: 1. The results are not stunning. The multi-task training does not lead to consistently better performance on all graph-structure prediction tasks. The performance improvement on downstream tasks is not effective. 2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.","2. Some experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2250,ACL_2017_104_review.json,"Weaknesses: - Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion: ",- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2803,NIPS_2022_1523,"Weakness:
1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data.
[1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.
2 Unclear model design: The model architecture and learning details are fragmented or missing. The authors could either provide a plot of model illustration, pseudo-code table, or code repository. Considering that Neurochaos Learning is not a well-known method, it is important to demonstrate integrated details to facilitate reproductivity.
3 Experimental design: The experiments regarding Coupled autoregressive (AR) processes and Coupled 1D chaotic maps etc. don't seem to be well-motivated. Could the authors reason why particularly using such setting to investigate cause-effect of time series. Lastly, the comparison to a five-layer neural network seems to be less convincing, given the rapid developments of deep learning architectures.
Yes, the authors fairly discussed the limitations of the method. The potential negative impact may not be applicable to this study.","1 Causality: I think the main drawback of this manuscript is the discussion of causality. In line 25, the authors claim that causality has been mathematically defined by Wiener et.al.. it would be nice to explicitly give the definition here, as reviewers may not familiar with this definition. Importantly, the nuance of causality definition varies from literature [1] to literature [2]. Without presenting the exact definition of causality quoted in this paper and discussing related definitions, it makes the readers hard to understand the main idea. In terms of 'classification of cause-effect', I am not sure if this terminology makes sense or not. What does it mean by classifying cause-effect (later causality detection is brought in line 38)? I believe the authors should discuss its connection to causal variable identification. This also relates to the fact the study is conducted on observational data. [1] Peters J, Janzing D, Schölkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017. [2] Hernán M A, Robins J M. Causal inference. 2010.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5224,NIPS_2020_1363,"- Even if MLE was better than DuetRL in objective evaluations, it is not a reason to leave DuetRL out of subjective evaluations. - The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references. - The quality and reusability of Chinese folk music transcriptions are not discussed. I think that the dataset is fine however, the usage of this dataset has to be justified by the authors, at least part of the supplementary. See the next three comments below: 1) Folk music is transmitted orally; scores are transcriptions into a single melody. These transcriptions do not typically include heterophony and other stylistic features like glissandi, ornaments, dynamics contrasts, etc. that characterize the folk song genre or style. The authors should discuss the limitations of the lack of performance elements in the transcriptions. 2) Another important aspect is the collection process and purpose of the transcriptions and if they are suitable for music generation? Given that the data is retrieved from Essen folk collection, is it possible that European scholars who prepared transcriptions with a Western classical music perspective and without adequate understanding of the characteristics of Chinese folk music? Are we sure that the transcription process retained the melodic characteristics? Such transcriptions are sometimes useful for descriptive reasons from a ""Western"" point of view, and they are not intended for prescriptive work, e.g., to be used in performance practice. (https://www.amherst.edu/system/files/media/1770/Seeger%252520-%252520Prescriptive%252520and%252520Descriptive%252520Music-Writing.pdf). There is no discussion if these transcriptions are re-usable for rendering music. To the authors' defense, I explored the dataset and Essen archives for more evidence (https://wiki.ccarh.org/wiki/EsAC) and consulted an ethnomusicologist specializing in Chinese music to validate the dataset. The response was that the transcriptions sound OK albeit that lack the performance elements as described above. Nevertheless, - since it is going to be the first paper on the topic - the authors need to include these discussions as part of the study (paper or the supplementary). 3) There are many different Chinese folk music traditions, which significantly differ in characteristics due to region, ethnicity, and era. This is not acknowledged by the authors, except in Section 4.1: ""It comprises 2250 traditional Chinese folk songs, from Han, Natmin, Shanxi, and Xinhua origins."" However, the origins are confusing: Han is an ethnicity; Shanxi is a province; Xinhua is a small county or the official state-run press agency, and the transcriptions listed under Xinhua are from all over China and modern-day Mongolia; I am not sure what Natmin is. This is a sloppy explanation from the authors' side.","- The musical motivation and background are explained weakly. There are no musicological sources cited for counterpoint (lines 15-17) and Chinese folk music (lines 18 - 20). We do not know the composers who use incorporate counterpoint to Chinese folk melodies or their prominence (line 20). Are these compositions Western classical music compositions, part of another classical tradition (Beijing opera?), or something else (film music?)? As far as I understand, variations of Chinese folk music have been a common theme throughout the development of Western classical style in China since the 20th century (e.g., He Luting's compositions)? Is there a demand to create more ""counterpointed"" folk music, e.g., for music education, film/game music generation, for commercials? In short, The readers should be able to appreciate these details in the introduction and be able to read further from additional references.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2656,NIPS_2019_346,"weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.","1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3138,NIPS_2017_15,"WEAKNESS AND CONCONERN
   	1) As already mentioned in previous section, the description lacks certain levels of details for complete reproduction, for example, how is the physics engine implemented, it's understandable that the authors left out some details with proper reference, however it is not very clear, in paricular as physics engine is an important component in the system, how the engine is set up and the set up affect the results.
   	2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.
   	3) The authors should also cite recent work on frame prediction. this is very related.","2) In addition to 2), there are concerns about the evaluation protocols for the billiard cases. why didn't the authors compared the results to previous results on the datasets other than the proposed baseline (sample perception model + repeating object dynamics) in the paper; for the block stability prediction, are the settings comparable to ones in the previous results. Those are important details to shed more lights to see if the proposed fully reconstruction and simulation approach did make a differences on a specific task over existing method, in particular the end-to-end without reconstruction as in the block stability prediction, though the authors can argue that full construction may be easier for rendering more articulated prediction results.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3611,NIPS_2020_878,"In short, the paper misses some critical details and some explanations are not clear. After reading the paper and the supplementary materials, I still believe it is hard to reproduce their results. - The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem. - The meaning of the shaded regions of Figure 2 (right), Figure 5 (left), and Figure 6 (left) is never explained in the caption or main text. Morevoer, it neighor represents standard deviation or the range between the best and worst case because the line with the same color can go outside the shaded region. For example, the solid red line in Figure 5 below the red shaded region between 60 and 90 trained models. - In the equation after line 156: ""\sigma ( A H^l W H^l)"" should be ""\sigma( A H^l W )"". Is this a typo? Otherwise, this is not a graph convolution layer. I have low tolerance with a typo in the ""only one"" equation in a paper. - It is not clear how the adjacency matrix A is defined in line 155. A_ij is the edge of node i to node j or node j to node i? Although I can figure it out from Figure S1 in supplementary materials, it is not clear in the main paper. - Following the previously point, based on Figure S1, the graph convolution operation propogates the features ""backward"" in the graph (in the opposite direction of an edge). I wonder if the authors can explain why you made this decision instead of adding edges for each node to the ""global"" node and following the edge direction to do graph convolution? - The original GCN (Kipf and Welling) uses normalized adajancy matrix with renormalization trick. However, this paper uses the original adjacency matrix without explantation or ablation study. I wonder if there is a reason behind not using the conventional graph convoltuional operation.","- The outputs of the binary relation predictor model shown in Figure 4 are not symmetric. Let F be the model and m1 and m2 be two model archietectures, (p1, p2) = F(m1, m2) and (p2', p1') = F(m1, m2). It is clear that p1 != p1' and p2 != p2' since the model just concatenates two graph embedded vectors and feeds it to a linear classifier. I wonder how the authors handle this problem.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
225,ICLR_2022_2575,"Weakness:
1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?
2). 80.0% top-1 accuracy on ImageNet is not hard. Does the design principle can scale well across different (data size, model parameters, FLOPs)?
3). I don't know which of the proposed components contribute the most to the overall performance. What can be concluded from Appendix Figure 7 and Figure 8?","1). Isn't Eq (1) encourages more randomized selection? What is the motivation of WGM, if it is refined by SRM?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2468,NIPS_2021_783,"Weaknesses: 1. The introduction of the knowledge coefficient matrix may raise the concern of privacy leaks and the communication cost. 2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting. 3. To my best knowledge, there are some other personalized FL works (Hierarchical Personalized Federated Learning for User Modeling, WWW'21; Exploiting Shared Representations for Personalized Federated, arXiv:2102.07078), which need to be listed as baselines.",2. The use of the public data in step 2 in Figure 1 conflicts with the general FL setting.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5707,NIPS_2018_845,"Weakness - Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks. - One of the main advantages from the simulated random-walks is parallelization (at least for sequence generation) and scalability. Section 3.5 is not enough for scalability argument.  * Detailed comments - Due to [28] and the attention usage in the language models, the attention is generally regarded per instance (or sentence/sequence) while here the attention is globally defined. The proposed method has its own value as the authors describe in the paper, but the terminology can lead to some confusion. - Some references such as [16] need to be polished. - Section 3.5 seems somewhat contradictory to the original purpose of trying the graph embedding because it will cut out the information that SVD cannot preserve. Also, whether Section 3.5 is just an idea or it is actually implemented to be used in experiments is not clear. It would be interesting to see the difference in performance between full-matrix context and SVD-approximate context. - Section 3.6 seems more promising to generalize the proposed idea beyond the simple random-walks. - While the authors sweep C for node2vec, the other hyperparameters are not mentioned where it is hard to believe that the default values of the other hyperparameters work best across the datasets. Mentioning the detail about C without mentioning the others naturally raise those questions, so the authors need to add some description.","- Despite the comparison with node2vec, which uses the second-order Markov chain for its random-walk sequence, the authors only deal with the first-order Markov chain. It means that D in nodevec is not be obtainable by the power-series in the proposed method. Despite this difference, all the random-walks are regarded the same as simple random walks.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5813,NIPS_2016_95,"Weaknesses 1. The time complexity of the learning algorithm should be explicitly estimated to proof the scalability properties. 2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained. ","2. In Figure 4, the time complexity for TRMF-AR({1,8}) and TRMF-AR({1,2,â¦,8}) seems to be the same. The reason should be explained.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4583,NIPS_2020_1678,"1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested. 2. I am not an expert in the are, but my impression is that the novelty of the work is somewhat limited. In particular the novelty is mainly to refine the framework of Dai et al. [4] on the particular problem, and to introduce the components of noise predictor and importance sampling for scalability.","1. The method is mainly heuristic, there is no guarantee for the performance of the new method. Accordingly the quality of the method can be judged only empirically on the datasets that have been tested.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4000,NIPS_2020_1878,"- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors? - Authors DBSCAN++ show cases where DBSCAN++ is even providing better clustering partition than for DBSCAN. Could the authors perform a similar analysis here with SNG-DBSCAN? - DBSCAN++ is also applied to outlier detection. How is performing SNG-DBSCAN compared to DBSCAN++ in this context? - Which parameters are used with DBSCAN for time and space consumption comparison? - Would it be possible to have empirical results on datasets for which DBSCAN is actually working? It seems that DBSCAN is performing well only for Still. Hence DBSCAN and SNG-DBSCAN are equally performing bad on the remaining sets...","- Empirical evaluation is a bit too weak for me. The comparison with DBSCAN++ is not fair. Even if DBSCAN++ and approach proposed by the authors come up with the same bound for runtime complexity O(sn**2) with n the number of nodes and s the sampling rate, this is definitely not fair to use the same sampling rate for edges (authors' approach) and DBSCAN++. Hence could the authors provide experiments for which sampling rate is optimized for both competitors?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4808,NIPS_2020_1318,"1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space). 2. In a similar vein, I was envisioning a simple extension of SG-MCMC methods where we just do occasional random walks/Brownian motion in the original space (using Gaussian noise and ignoring the energies entirely) using some relatively cheap heuristic to detect if the fancier iterate sequence is stuck in local optima. Something like that would help explore the entire space (triggering some iterations of random walk if heuristic flags a local optima) without changing the underlying energy used in the real Sg-MCMC at each iteration. Wonder how that would compare to a method like the one in the paper. 3. Following from these, it would have been nice if the authors included synthetic experiments with asymmetric modes (one small mode to one side of a 2d space and one taller one on the other side) where the real mean would not be close to the 'center' and thus would not be easily approximable by a flatter version of the energy. Comparisons with something like the approach in 2. above would help understand the main insight of the paper better. 4. Runtime plots in the experiment sections on real data would be nice to have, for example plots of energy/error vs wall clock time / iteration count as provided for the synthetics. 5. Some insight on how to choose \delta_{u} and partition count for real datasets (like the values mentioned on line 173 for synthetic) would be nice to have.","1. My main concern is that using a flattened surrogate energy in this fashion is suitable for most sampling situations. The main reason is, by construction our iterates are not following the true distribution particularly closely; for example a plot of the samples obtained in the synthetic experiments (figs 2c--d) would look quite different from the original. While this does allow the algorithm to bounce out of local optima, the deviance from the true energy would make samples obtained after convergence to not be super useful. For point estimation situations, we might be able to get away with these samples for cases where the multiple modes of the real energy are sort of symmetric (as in the synthetic Gaussian experiments); it seems that even if we use a 'flattened' energy (can be thought of as lower peaks with higher elevation between them), the original distribution's symmetry would be essentially preserved and the mean / other point estimates would be close enough. But flattening energies with skewed distribution of modes might not be as accurate, as the flattened version might have a mean closer to the 'center' of the space, but the original would be closer to one of the modes near the periphery (am visualizing a simple 2-d space).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1482,ICLR_2023_3023,"Weaknesses: 
 1. The proposed technique does not seem to predict uncertainty. To the best
  of my understanding, it predicts where the original model is making
  errors. An easy way to see this would be to consider a point where the
  model is confident but inaccurate, i.e., a bias. The predicted mask
  should cover this point, even though it is not an uncertain prediction
  of the model. The problem is actually in the definition of
  ``uncertainty''. Equation 1 (or 3) do not necessarily only correspond to
  uncertainty. It corresponds to expected error for a sample. It also
  includes the contribution of bias. In this regard, I think the
  uncertainty positioning may not be very accurate. Consequently, the
  comparison with a method that computes confidence intervals may not be
  appropriate. Here, I should note that predicting expected error or the error is not a
  bad target. However, the difference between this and uncertainty
  estimation should probably be made clear. 
 2. I am not sure about the contribution of the corollary 1. The result is
  not very surprising in my opinion. The mask that aims to minimize the
  ``uncertainty'' is bound to be related to the ``uncertainty''. 
 3. It is unclear how authors estimate the expectation in Equations 4
  or 5. This is over y x variable. However, in Equation 6, they drop the
  expectation and simply take only one sample to compute this
  expectation. Using this I am assuming the model learns to predict the
  error and not the expected error. As a result, the model may not be able
  to identify an uncertain prediction at a pixel for a given sample since
  for that pixel, there may be only 1 output that happens to be close to
  the ground truth. 
 4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0,
  m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} =
  \infty. It is unclear how authors deal with this. Furthermore, the
  intuition of this specific calibration form is unclear. ","4. Equation 8 yields a suspicious behavior. When m_{\theta} = 0, m_{\lambda} = \lambda. However, when m_{\theta} = 1, m_{\lambda} = \infty. It is unclear how authors deal with this. Furthermore, the intuition of this specific calibration form is unclear.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4527,NIPS_2020_84,"- I think the definition of ""near convex"" is quite restrictive towards the necessities of the sensitivity framework. However, it is presented as a very general and natural class of functions. The limitations are not clearly discussed in my opinion. - related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.",- related work: see below - the SVM result depends on regularization and additionally on the structure of the data which seems quite restrictive to rely on both relaxations.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2141,ARR_2022_210_review,"1) Improvements are overstated. The abstract mentions "" two-fold improvements"" in perplexity, whereas the actual improvements are tiny. What exactly did you mean by 2 fold improvement?
2) Perplexity is not a suitable metric for the task in question. What you are highlighting is the probability distribution between likely candidates for top-K prediction of a softmax, which is not adequately measured by perplexity. There are other metrics, such as MRR which is mentioned in the appendix. Perhaps switching evaluation based on that would vastly improve the experimental section of your paper. The problem you are highlighting is more linked to ranking rather than perplexity.
There are many counter examples where perplexity can be very high but the model ranks the vocabulary correctly. E.g. with |V| = 100 assign the top probabilities as ""woman"" with prob 2/100, ""king"" with prob 1/100 and spread the remaining prob mass over the remaining 98 words - this has extremely high entropy but correct ranking.
3) Readers unfamiliar with the softmax bottleneck problem will struggle to understand the parallelogram example, or understanding theorem 2. More real estate should be allocated in the main paper to explaining those two, so that the reader is not forced to look at the appendix to understand the main body of the paper.
4) Line 307-320, it is not at all clear how you guarantee that the top-k words would always fall into different partitions. Could you elaborate on that. How are the partitions computed?
5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. 
There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger.
The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful.
Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format.
Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.
- Line 195: plane -> hyperplane - Figure 1 caption: middle -> midpoint - Line 1453: ""the tanh removes the magnitude of facets"" -> Could you explain this in more detail? I could not make sense of why tanh ""removes the magnitude"" - what do you mean specifically by ""magnitude"" here?
- Line 1457: Why is invertibility important? My reading of section F.1 is: We removed tanh because it worked better empirically (which sounds fine to me, provided it doesn't change the rankings of your proposed models and the baselines).
- There is a mistake in rows ""MFS - Multi partition"" and ""MFS - Multi Input"" in Table 2. MFS - Multi partition has 1 #P and 9 #I according to the table while Multi Input has 4 #P and 1 #I which does not make sense.
Rephrasing: Line 063: ""output the woman or king."" - > ""output woman or king"" Line 121: ""is sometimes not be able"" -> ""is sometimes not able"" Line 1271: ""the GPT-2"" -> ""GPT-2"" Line 235: ""multi-mode"" -> ""multimodal"" also line 263. 
Line 911: ""As in section 5"" -> ""As in Section 5"" ","5) Section 5, you have to give some examples of the datasets, so that the reader has any chance to parse table 4. It's incomprehensible without the extra knowledge in the appendix. There is no explanation why the probability distribution of the top-k results in the softmax is an issue. The reader of the paper would struggle to understand why this is an issue if the top-1 rank is correct in the vast majority of cases. More motivation here would make the paper stronger. The paper completely ignores the bias term in the softmax. The bias term could change the orderings of the of the output layer and potentially ameliorate (or deteriorate) the issue. You should discuss it, or make it clear that you are not analysing it (Like Demeter et al 2020) The choice of H and W is not really explained. More details would be helpful. Why are the captions of table 3 and table 4 going above the figure and not below? This is unusual for ACL paper and I am not sure if it's acceptable format. Missing reference: The main point of theorem 1 has been discovered several times in history: See Cover(1967) and I.J. Good & T.N. Tideman(1977) who counted the number of possible rankings given N and d. See https://rangevoting.org/WilsonOrder.html for a comprehensive discussion of the multiple discoveries.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2707,NIPS_2022_776,"Weakness:
1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
4.(Minor) Line 121, minimized -> minimize.
[1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.",3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1885,ARR_2022_24_review,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? 
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? 
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? 
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? 
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes. 
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"". 
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. 
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ","- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5162,NIPS_2020_1750,"- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence. - Choice of methodology: 3.1: The proposed experimental methodology is poorly justified. The pruning methodology (3.1) does not seem to directly correspond to previous approaches which have found found that pruning increases generalization. The experiments are ran on two unconventional networks on CIFAR-10, using unconventional pruning schedules. These networks reach much lower accuracy than expected for CIFAR-10 (85%-87% test accuracy), possibly due to the fact that these networks are relatively overparameterized for CIFAR-10 (a CIFAR-10 ResNet-20 has 0.27M parameters and typically reaches about 91% test accuracy; the ResNet-18 in the paper has 11.5M parameters, and the VGG-11 has about 110M) and L1/L2 regularization is disabled. It is therefore hard to extrapolate these results beyond the two CIFAR-10 networks and their bespoke hyperparameters chosen in the paper. - 3.3: the choice of duration of holding parameters at zero, and the conclusions drawn from it, don't seem justified from the data: the drop in test accuracy from ""Zeroing 50"" does not seem to be the same as the drop in accuracy from ""Prune_L"", and it is unsurprising to see that any regularization technique improves test accuracy for these heavily overparameterized CIFAR-10 networks, so the claim that ""pruning-based generalization improvements in overparameterized DNNs do not require the model’s parameter count to be reduced."" (lines 232-233) is hard to extract from just this experiment on these networks. Overall: the work would be significantly strengthened by having much more well-justified and clearly presented methodology for the networks that are used, the experiments that are performed, and the conclusions that are drawn from those experiments. # UPDATE AFTER AUTHOR RESPONSE The fact that the results seem to hold almost exactly as strongly on the ResNet-20 as they do on the ResNet-18 seems to imply that the results are not just equivalent to adding regularization into an over-parameterized and under-regularized network, and do in fact provide some effect when applied alongside standard regularization (as opposed to my suspicion that the effect would disappear or even reverse when applied alongside standard regularization). I do still think that there is more work to be done on that front (making more precise exactly the relationship between regularization and this effect), but the rebuttal did satisfactorily address my main concern with the paper. Regarding the addition of Kendall Tau, I am satisfied that they do show a correlation, even with the relatively noisy data. Overall, I'm raising my score to a 6. I still think the paper would be improved by a more thorough discussion of the relationship to other more standard forms of regularization, and I'm hesitant to trust that the methodology will be made substantially more clear without seeing a revised version of the paper, but enough of my concerns were addressed by the experiment on the standard ResNet-20 that I no longer see a strong reason to reject the paper.","- Methodology clarity: As an empirical paper, methodology should be forefront. As it is, there are not enough methodological details in the main body of the paper to reproduce the results (how the pruning is performed, the pruning schedule, how the gaussian noise is applied, how many batches in an epoch, etc.). The methodological details in Appendix A help, but are not clearly presented enough to be able to reproduce the results with confidence.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1486,ICLR_2023_3777,"Weakness
1: The technical novelty is limited and some details are confusing.
Slimmable networks are a special case of widely studied one-shot NAS (e.g., [R2, R3, R4, R5, R6]), which only considers the width dimension (see discussion in OFA [R2]). There are many techniques to deal with interference among subnetworks. Specifically,
“Slow start” belongs to progressive training in one-shot NAS. For example, OFA proposes a “Progressive Shrinking” strategy, which starts with training the largest sub-network and then progressively fine-tunes the network to support smaller sub-networks by gradually adding them into the sampling space.
“Online distillation” was originally proposed in US-Nets [Yu et al., 2019b]. Apart from the inplace distillation, it also proposes the sandwich rule.
“loss reweights” aims to assign larger weights for sub-networks with large widths. However, it violates the training objective of slimmable networks. The objective is to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural configuration, rather than only training an accurate large “supernet”. This is evidenced in Table 2 (e), where adding loss reweighting makes R-50(0.25) perform worse, so what’s the meaning there?
2: Another concern is what are the fundamental differences between self-supervised and supervised training for slimmable networks? This is not clear to me as all the training techniques used are common practices in supervised training.
3: What’s the relationship between unsupervised NAS (e.g., [R4, R5]), including the contrastive self-supervised one (e.g., [R6])?
4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?
5: The paper lacks mathematical modeling for the gradient divergence issue which leads to the optimization difficulty claimed by the authors. I think there are only four possible widths and it is not difficult to analyze the gradient magnitude and directions using SGD with maths formulations. Also, some theoretical analysis on convergence is expected, even assuming a linear neural network is fine [R1].
6: The experiments are far from enough to justify the effectiveness of the proposed method.
6.1: The results are merely based on the ResNet-50 backbone. However, I would like to see more ResNet backbones such as R-101 and R-152. More importantly, experiments on Vision Transformers, such as ViT-B in MoCo v3, must be included in the experiments.
6.2: The paper only evaluates the representation quality using linear probing. However, it must evaluate transfer learning performance which is the standard practice in self-supervised learning (e.g., in MoCo v3). For example, experiments on dataset transfer and downstream tasks such as dense segmentation and detection on COCO and ADE20k are needed.
6.3: How about training the whole network (width 1.0) first then using network pruning (e.g., [R7]) to obtain small networks (width 0.25, 0.5, 0.75)? As this strategy can avoid the interference issue during training.
6.4: It lacks comparisons with methods dealing with sub-network interference, such as switchable BN [Yu et al., 2019], sandwich rules [Yu et al., 2019b] and many others.
7: The discussions and references in related work are far from enough. There are few discussions with single-shot NAS and unsupervised NAS methods. In addition, as I point out in the technical novelty part, the differences and advantages with the related work must be discussed.
8: Writing also needs to be improved.
8.1: What is the definition of the “main parameters” in the introduction?
8.2: In Sec. 3.2, “..., where L
is the loss function”. It should be defined in Eq. (1) where it first appears.
8.3: Many grammar issues. I only point out a few. “Slimmable neworks” in Sec. 2; “server performance degradation” in Sec. 3.2.
9: In Sec. 3.2, authors argue that the two ratios in Figure 3 should be large enough. “In Figure 3f, ..., are larger than 1.0 by a clear margin”. It does provide a clear concept of how large is good enough. In my opinion, it also depends on the network architectures and self-supervised learning frameworks. So Figure 3 may not be statistically significant. References:
[R1]: “On the optimization of Deep Networks: Implicit Acceleration by Overparameterization”, ICML 2018
[R2]: “ONCE FOR ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT”, ICLR 2020
[R3]: “BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models”, ECCV 2020
[R4]: “Are Labels Necessary for Neural Architecture Search?”, ECCV 2020
[R5]: “Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?”, NeurIPS 2020
[R6]: “Contrastive Self-supervised Neural Architecture Search”, Arxiv 2021
[R7]: “Resrep: Lossless cnn pruning via decoupling remembering and forgetting”, CVPR 2022","4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that “To ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice” as all sub-networks with different widths should be equally important. Otherwise, what’s the meaning of slimmable networks there?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4671,NIPS_2020_1856,"* See section (Clarity). * 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this). * For better or for worse, there is a lot going on -- and it feel like the pages 5 and 8 were crammed up. I wonder if maybe the theory of OT (pages 3-4)can be reduced to 1 page, so that there can be a more thorough discussion, and explanation of the results and metrics rather than having the reader refer to the Supplementary Material consistently for some minor details.",* See section (Clarity).* 2 observers are used in human psychophysical task and the number of trials is not stated. Also not stated if observers were authors or naive to goals/outcome of experiment (unless this is mentioned in the Supplement and I might have missed this).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3428,NIPS_2020_747,"I have a few concerns regarding this paper. 1. How practical/realistic is the threat model? In the paper, chapter 2.2, the NoBox attack demands the training set. In the real world however, isn't the training set even more precious than the trained models? 2. In the AEG objective, the generator needs to get the gradient to be trained. Would AEG still applicable to the non-differentiable robust classifiers, such as: [1] Countering Adversarial Images using Input Transformations [2] THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES [3] Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples The common point of these approaches is they all incorporate some sort of in-differentiability. 3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST. 4. One experiment I'd like to request: (i)- get a model trained on some dataset at epoch N, N+1, N+2... N+k (ii)- use the generator to generate pertubed imagess to attack all of them. (iii)- show the effectiveness of the attack. This should be a more realistic scenario and it aligns with the main point. 5. A portion of the experiment has compared the NoBox attack to the other attacks. These are generally under different threat model assumptions. However the main claim of the paper is that the NoBox is capable of attacking different models in the same function space. It would be better if the authors can present the NoBox's effectiveness attacking more diversified trained neural network models.",3. The experiments. (Maybe my misunderstanding) Many published papers in this field used ImageNet (and the top-1 score) to benchmark the effectiveness of the attack or the robustness of the defense. However this paper the experiments are limited to only CIFAR and MNIST.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5171,NIPS_2020_880,"1. I could not see a strong motivation for explicitly enforcing sparsity on architecture parameters. This is because there are already many works trying to decouple the dependency of evaluating sub-networks on the training of supernet (i.e., making the correlation higher). This means that we have ways to explicitly decouple the network evaluation with supernet training without adding a sparsity regularizaiton. 2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2. 3. The one-stage method proposed basically focusing on training network weights W after the training of architecture parameters is converged. However, similar idea can also be achieved in other differentiable NAS framework, where one can continue training the supernet weights after the architecture remains little change. For example, in GDAS, after the entropy of edges is well minimized, the sampled architecture will be close to determnistic, and one can keep training W to obtain the optimal weights. Moreover, other one-stage methods like [4] are not compared nor discussed. ======================== After reading the author's response, most of my concerns have been addressed. I choose to accept this submission. ======================== [1] Guo, Zichao, et al. ""Single path one-shot neural architecture search with uniform sampling."" ICLR 2020. [2] X. Dong and Y. Yang. Searching for a robust neural architecture in four gpu hours. CVPR 2019. [3] Bender, Gabriel, et al. ""Understanding and simplifying one-shot architecture search."" ICML2018. [4] Cai, Han, Chuang Gan, and Song Han. ""Once for all: Train one network and specialize it for efficient deployment."" ICLR 2020.","2. Properly understanding Table 2 requires more experiment details. As far as I know, weight-sharing methods require the BN to be re-calculated [1] to properly measure the Kendall correlation. Other works that can reduce the gap between supernet and sub-networks (e.g. [3]) or can make the edges activated to be sparse like GDAS [2] are not compared. Moreover, there seems no explanation in main content regarding Table 2.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1908,ARR_2022_64_review,"- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:   - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction   - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line ""544"", I still quite understand why using entity representations can reduce language bias. ","- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3403,NIPS_2020_585,"I think the major problem of this paper is that the novelty is limited. It seems that nearly all the components in the proposed method have been used, and the combination manner is also not so novel. 1. The two-stage method is widely used to deal with partial label examples. The first stage in this paper adopts the widely used label probagation strategy to obtain labeling confidence for partial label examples. However, SSPL [22] also adopts this strategy (which obtain labeling confidence for both partial label examples and unlabeled examples). So there seems no novelty for the first stage. 2. For the second stage, a modified maximum margin formulation is introduced, which can jointly enable the induction the induction of predictive model and the estimation of labeling confidence over unlabeled data. The formulation of the second stage mainly follows [17]. There are two differences between this paper and [17]: Firstly, this paper uses the obtained labeling confidence of partial examples to give different weights on the losses of different examples. Secondly, the unlabeled data is used in the formulation by using the widely-used manifold regularization, so that the labeling confidence of unlabeled data will be jointly estimated. Generally, I think the improvement over [17] is intuitive and reasonable. But I think that the novelty is not enough, especially on such a prestiguous venue NeurIPS. 3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage. 4. This paper may not effectively deal with large-scale datasets because it uses alternating optimization and needs to construct a similarity graph in advance.","3. The alternating optimization is a common optimization solution, which has also been adopted by a related paper [21]. But unlike [21], this paper does not provide any theoretical analysis or empirical evidence about the convergence of the modified maximum margin formulation in the second stage.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3980,NIPS_2020_1314,"1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP. 2. In Section3.1 the authors give a thorough comparison between the discrete Gaussian and discrete Laplace. The conclusions deduced by the discrete distribution are almost the same as the prior conclusions of the continuous distribution. 3. This paper does not compare their methods with the exponential mechanism(Frank McSherry and Kunal Talwa 2007). Exponential mechanism can also output a discrete answer for a query.","1. In my opinion discrete Gaussian mechanism proposed by this paper is not a big innovation, since the discrete Laplace distribution has been introduced to DP in (GRS12) and be used in the 2020 US Census. Applying discrete Gaussian noise to DP is a natural derivational work of the discrete Laplacian noise to DP.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
608,ICLR_2021_1900,"weaknesses:
Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq. (2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 2) Regarding Eq. (5), what the intuition of the adaptive matrix (i.e., (log_{\mu+1} (N_i+1))^{\delta^{\tau}}) when i = j should be provided to the authors.
The major issue of this paper is the experimental evaluations. 1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic. 2) Some state-of-the-art methods are not involved in the experimental comparisons, such as [ref1-ref5]. Moreover, the accuracy of the proposed method cannot outperform these methods.
Minor issues:
There are several typos and writing problems in this paper. For example, on Page 3, ""Dubey et al.Dubey et al. (2018)"", and ""Chen et al.Chen et al. (2019)"". On Page 4, ""PC Dubey et al. (2018)"". On Page 8, ""And also solves the long-tailed problem by an adaptive matrix term.""
[ref1] Weakly Supervised Fine-grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning, CVPR 2020.
[ref2] Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019.
[ref3] Fine-Grained Visual Classification via Progressive Multi-Granularity Training of Jigsaw Patches, ECCV 2020.
[ref4] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.
[ref5] BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition, CVPR 2020.","1) The classification accuracy on these fine-grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. Thus, the effectiveness of the proposed method is problematic.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2060,ARR_2022_1_review,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. 
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ","- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3525,NIPS_2020_324,"The experiments are performed on two simple environments: 1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.","1) an oil discovery problem which involves the agent minimizing costs in an unknown noisy 1D cost function, and 2) an ambulance relocation problem which involves controlling a fleet of k ambulances, to minimize costs (movement and time) to respond to incoming requests. While AdaMB performs similarly to epsilonMB with less resources in these problems, it is not clear if the proposed method would scale to more complex problems with larger state-action spaces. The proposed method AdaMB does not seem to perform any better than AdaQL. Both perform similarly and maintain a similar number of regions on average. The paper lacks a thorough comparison of AdaMB and AdaQL and it is not clear if AdaMB is better than AdaQL in any aspect.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
114,ICLR_2022_1971,"Weakness: Method:
1. Verification and Optimization:
The proposed HNPF method is for verification (e.g., check whether a given solution x is Pareto optimal), but not for optimization (e.g., find an (approximate) Pareto solution x). It needs an extra search method, such as random sampling in this work, to first generate a large number of feasible solutions to cover the whole search space. Therefore, the underlying optimization is indeed random sampling (independent from HNPF), which could be extremely inefficient for a non-trivial search space. It is not suitable to put and compare the proposed HNPF method with other optimization methods that can directly find the (approximate) Pareto solution.
Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems.
2. The Reason to Build the Model:
HNPF needs to first build a neural network to check whether a given solution x satisfies the Fritz-John Condition (FJC), which requires a large number of training samples (e.g., 11K for a two-dimensional problem). The learned model is mainly used to classify whether the extra randomly sampled solutions (e.g., 9K) are weak Pareto optimal or not. The reason for model building, such as the advantage over the simple FJC rule-based classification, is not well motivated and justified in this work.
The proposed Pareto filter in stage 2 is not discussed and compared with other related nondominated sorting algorithms (e.g., [2]).
3. Necessary Condition for Pareto Optimality:
The KKT[3] and FJC[4] are two types of first order necessary conditions for (local) Pareto efficiency (Pareto optimality). In my understanding, the multi-objective optimization based MTL algorithms mentioned in this work (Sener & Koltun, 2018; Lin et al., 2019a; Mahapatra & Rajan, 2020; Ma et al., 2020; Navon et al., 2021) mainly use the gradient-based multi-objective optimization methods (e.g., MGDA) [5-7], which is based on the KKT condition. For these methods, in each update step, the gradient can be written as a linear combination of the gradient for each objective with adaptive weights derived from the KKT condition. Therefore, they are all different from the simple linear scalarization with fixed weights. All the claims and analyses in this work that the previous works use simple linear scalarization is not correct.
The FJ condition is also for local Pareto convergence, similar to the KKT condition. The global convergence property is solely due to random sampling that only works for extremely low-dimensional problems. It is unfair to say the proposed algorithm can overcome the local convergence of other gradient-based methods. In addition, the proposed algorithm heavily depends on the Fritz-John condition, but the original work [4] is not cited.
4. Linear Scalarization and Convex Pareto Front:
It is well-known that the simple linear scalarization cannot find the non-convex part of the Pareto front [8]. This finding leads to the seminal work on NBI scalarization (Das & Dennis.,1998), which is indeed one fundamental work that inspires the proposed method in this work (section 4, first sentence). The claim ""it is incorrect to state that LS itself fails if the Pareto front is non-convex"" (appendix, page 15) is questionable.
Since the proposed HNPF can only verify whether a given solution is weak Pareto optimal, its ability to find the whole Pareto front totally depends on the extra sampling method (e.g., random sampling) to generate all the Pareto solutions (might be infinite). It is misleading to indicate the proposed HNPF method itself can find the whole Pareto front. In addition, since the Pareto set has measure zero and infinite cardinality, the random sampling + HNPF method can at most find a dense approximation to the Pareto set. Experiment:
5. Algorithms for Comparison:
All the multi-objective optimization based MTL algorithms are designed for optimizing a deep neural network with millions of parameters. They implicitly depend on the assumption that the deep neural network has good properties (e.g., no bad local optimum [9][10]) on its loss functions, which is consistent with other gradient-based single-objective optimization methods. They are not designed to find the global Pareto front for low-dimensional problems.
For low-dimensional problems, it is more suitable to compare with the model-free multi-objective optimization methods such as the multi-objective evolutionary algorithm [11,12] and multi-objective CMA-ES [13]. If the model building is allowed, Multi-Objective Bayesian Optimization (MOBO) algorithms can have a very good sampling efficiency for the low-dimensional problems [14,15]. It is also very common to conduct non-dominated filtering at the end of those model-free algorithms or MOBOs (e.g., only keeping the current non-dominated solutions).
6. Training + Sampling:
The proposed method needs to first sample 11k solutions to train the neural network model, then randomly generate extra 9K solutions for filtering. Is there any advantage over simply using FJC to filter 9K (or 11k + 9K) randomly sampling solutions?
7. Figure from Other Works:
Many figures in the main paper and the appendix are directly borrowed from other works. I think this is not appropriate even the credits are given to the original works. Reference:
[1] Ruchte, Michael, and Josif Grabocka. Multi-task problems are not multi-objective. arXiv preprint arXiv:2110.07301, 2021.
[2] Roy, Proteek Chandan, Kalyanmoy Deb, and Md Monirul Islam. An efficient nondominated sorting algorithm for large number of fronts. IEEE transactions on cybernetics 49, no. 3: 859-869, 2018.
[3] Kuhn, H. W., and A. W. Tucker. Nonlinear Programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 481-492. University of California Press, 1951.
[4] Da Cunha, N. O., and E. Polak. Constrained minimization under vectorvalued criteria in finite dimensional spaces. Journal of Mathematical Analysis and Applications, 19(1), 103–124 ,1967.
[5] Fliege, Jorg, and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research 51, no. 3: 479-494, 2000.
[6] Fliege, Jorg, and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on SQP techniques. SIAM Journal on Optimization 26, no. 4: 2091-2119, 2016.
[7] Desideri, Jean-Antoine. Multiple-gradient descent algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique 350, no. 5-6: 313-318, 2012.
[8] Das, Indraneel, and John E. Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems. Structural optimization 14, no. 1: 63-69, 1997.
[9] Kawaguchi, Kenji. Deep learning without poor local minima. NeurIPS 2016.
[10] Kawaguchi, Kenji, and Leslie Kaelbling. Elimination of all bad local minima in deep learning. AISTATS 2020.
[11] Deb, Kalyanmoy, Amrit Pratap, Sameer Agarwal, and T. A. M. T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation 6, no. 2: 182-197, 2002.
[12] Zhang, Qingfu, and Hui Li. ""MOEA/D: A multiobjective evolutionary algorithm based on decomposition."" IEEE Transactions on evolutionary computation 11, no. 6: 712-731, 2007.
[13] Igel, Christian, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evolutionary computation 15, no. 1: 1-28, 2007.
[14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020.
[15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.","1: 1-28, 2007. [14] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. NeurIPS 2020. [15] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations. NeurIPS 2020.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2507,NIPS_2021_537,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3487,NIPS_2020_1312,"- The contribution of local voxel-bounded implicit fields is not motivated and evaluated enough. The authors showed that voxel embeddings improve the performance. However, an experiment that shows the performance with different resolutions (partly done with the progressive training evaluation) would improve the paper. - Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution. - For the experiments on large-scale scenes and dynamic scenes, qualitative results for existing methods are missing. Furthermore, quantitative results are missing for these experiments (also for existing methods). - The geometric result in the ScanNet experiment does not seem to be very good. Therefore, it would be interesting to see how accurate the represented geometry is and how it affects the overall performance of the method. - The effect of the early termination is not properly evaluated in the paper. - The initial grid resolution seems to be already very high. It would be interesting to see how this affects the method. - There should also be comparisons to DeepVoxels since this work also leverages local voxels as a feature representation.","- Similarly, the contribution of progressive training is not evaluated enough. In order to be a valid contribution, the authors need to show that the progressive training is not only better than one round of training at the initial resolution but also better than one round of training at the increased resolution. As it is done now (in the ablation study), there is the possibility that the improved performance is due to the higher voxel resolution.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1654,ICLR_2023_1765,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.","- Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1526,ICLR_2023_3808,"Weaknesses: • The writing of the paper is at times confusing. For example, it is unclear of the significance of the \cap architecture. No detail is included in the paper. • The rationale of dividing the sequence of intermediate results into 11 sets is not provided. • No ablation studies are provided for \mu and \lambda, and the size of each set. In fact, their values are not provided in the paper (unless I missed them). • Using different sets and varying \phi appear to have a similar effect. However, there is no discussion on this issue. • It appears that there are two types of sets (Table 1 and 2), one for style and one for content. But in the paper, they are regarded to be the same, which is quite confusing. • There is insufficient discussion of the novelty of the proposed method, e.g. what the proposed method can do that other recently proposed style transfer methods cannot do? For example, in Fig. 6, the only discussion is one single sentence on p. 5 stating that the proposed method preserve the semantic structure of the content image and simulate the strokes of the style image from the style domain. Such a statement, which is quite generic, can be easily applied to other style transfer methods. • There is no quantitative comparison, e.g. style loss, user study, and efficiency, between the proposed method and other methods. • One issue not discussed is the importance of using the diffusion model. What can it provide to style transfer that no other methods can? • There are some occasional typos and grammatical mistakes.",• The rationale of dividing the sequence of intermediate results into 11 sets is not provided.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5012,NIPS_2020_1049,"- The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE. - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1. - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.",- The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2844,NIPS_2022_477,"Weaknesses:
1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.
3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?
4.How is gamma in the Sinknhorn-Knopp affect the performance?
5.How does the class distribution estimate for PRODEN in Figure 1?
Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment.
Limitations: The experiments need to be further improved.",5.How does the class distribution estimate for PRODEN in Figure 1? Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment. Limitations: The experiments need to be further improved.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1317,ICLR_2023_3854,"Weakness:
1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.
2.The novelty of the paper is limited as some parts seems like a direct combination of TD3 and SAC, but the paper does not provide detailed insight of why SAC works in complex envs while TD3 works in simple envs.
3.Theorem 2 assumes that Yi follows a uniform distribution, which might be too strong in practice.
4.The experimental results are not quite convincing. TD3 fails to learn in Humanoid-v2, can you explain the reason?","1.The paper claim that it can reduce the overestimation bias compared with TD3. SD3(NIPS 2020), DATD3(AAAI 2022) also can reduce the underestimation bias of TD3. However, the paper does not compare these methods in terms of both estimation bias and performance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1875,ARR_2022_2_review,"-	The authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. 
-	Given that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? 
-	It would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. 
-	It was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. 
-	In addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. 
-	In Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. 
-	In Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? 
-	In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? 
-	The fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). 
-	In Section 4.3.3, how do you deal with the RoBERTa token limit? 
-	In Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. 
-	In Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. 
-	It would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ","- In Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5730,NIPS_2018_857,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo?  [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018.  ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.",- The writing should be improved. Some points in the paper is unclear to me.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
229,ICLR_2022_410,"Weaknesses:
. Theorem 4.2: For the downstream classification, the loss is upper and lower bounded in terms of the L_NCE loss. The authors provided comparison with Saunshi et al. (2019) from the technical perspective. Is there any intuitive explanation on how to evaluate the classification performance in terms of contrastive learning (loss)?
. Assumption 4.5 (intra-class connectivity): This assumption is strong. Without the label information, it seems impossible to derive such augmentation set. Please add discussion on the practicality of this assumption, and show an example on some datasets if possible.
. Proposition 4.7: Based on the proof provided in the appendix the conclusion not only relies on the existence of such augmentation set (Assumption 4.5), but also that such augmentation should be applied to intra-class samples, ie, t_i(x_i) = t_j(x_j). This kind of operation is impractical without the label information. Please add comment on that.
. In the experiments, RandomResizedCrop is used to illustrate the relationship between Aug Strength and ACC(ARC). The best performance for different datasets all achieves at Aug Strength = 0.92. Any comments on that? eg., in terms of data augmentation for intra-class samples at Aug Strength = 0.92?
. In practice, there are different kinds of data augmentation, eg, flipping, rotation, and scaling. The authors only showed results on RandomResizedCrop. Can you show results for other data augmentation types? Do you have similar conclusion as that for RandomResizedCrop?
. Different data augmentation types are often used together in practice (eg, randomly pick two augmentations from the augmentation set for the raw image). Then how to apply the proposed analysis in such practical case? In particular, how to measure the Aug Strength?
. The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping). 1) The study on applying the analysis to existing contrastive learning algorithms is, however, preliminary (only with RandomResizedCrop). 2) Based on the proposed analysis how to find the sweet spot of data augmentation for contrastive learning is crucial, but this is not discussed.",". The authors emphasized the importance of the data augmentation design for intra-class samples (ie, perfect overlapping).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2403,NIPS_2021_1571,"How do you validate the estimated uncertainty maps, in the absence of GTs for the same ?
Wonder what is the expansion of the acronym ""D_CBase"" used ? What does it mean ?
What is MAE in fig. 4 ? Not defined anywhere in main doc. - got it in Supple though.
Fig. 9 of Supple material - how do you validate these complexity scores ? Even the GT or human evaluation is not provided. What are the typical ranges of complexity values you get by the entropy based measure ? How do you thus claim it to be consistent with human visual perception ?
Some typos: line 169: ......local minimum, it solve the following estimating.... - change to : it solves the....
Some more references may be added as:
Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011;
Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.","- change to : it solves the.... Some more references may be added as: Bayesian Learning via Stochastic Gradient Langevin Dynamics; Max Welling, Yee Whye Teh, ICML 2011; Variance Reduction in Stochastic Gradient Langevin Dynamics; Avinava Dubey, Sashank J. Reddi, Barnabas P´oczos, Alexander J. Smola, Eric P. Xing, NIPS 2016.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3498,NIPS_2020_944,"1. Weighted retraining is not new. The cross-entropy method (De Boer et al., 2005; Neil et al., 2018) maximizes the expectation E_p(x)[f(x)] of the objective function f(x) when sampling from a policy p(x) by periodically retraining p(x) on the samples with the highest reward, e.g. those with a reward above a quantile cutoff (i.e. using a stepwise weighting function). Instantiations of the cross-entropy method include DbAs (Brooks et al) and FBGAN (Gupta et al). Reward weighted regression (RWR) (Hachiya et al) is another existing optimization technique that employs weighted retraining. Angermueller et al. (http://arxiv.org/abs/2006.03227) recently employed these techniques as baselines for high-dimensional discrete optimization. 2. The described rank-based weighting function is not new. See RankGAN (Lin et al. 2017) or LeakGAN (Guo et al. 2017) for an example. 3. The evaluation is missing important baselines such a DbAs, FBGAN, RWR, and model-based optimization. 4. Chemical design task: It is unclear how the optimization trajectory of ‘original’ was obtained. How were new data points sampled from JT-VAE? Why does the trajectory stop at 250? 5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning. 6. What do error bars represent? How often were experiments repeated with different random seeds?","5. In addition to JT-VAE, I would also like to see a comparison with GCPN (You et al) and reinforcement learning.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5083,NIPS_2020_734,"- Main math section/derivation could be improved for clarity: it's a very simple idea but it is presented in an obfuscated way that requires a lot of energy from the reader to go through. - It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper. - The gains obtained by the proposed method modification are not that large.","- It feels Section 6 doesn't belong in this paper, and that the extra space could be used for extending the other parts of the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3452,NIPS_2020_1505,"Even though the paper reads well in its current form, I found during the first reading that the presentation of the conceptual messages (which are the most important part in my opinion) were overshadowed by the algorithmic comparison between GOLEM and NOTEARS. The paper is dense and it is not necessarily clear what the take-away messages of the paper are. - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions. - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.",- I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4673,NIPS_2020_1254,"- The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for ""realistic"" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated. - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.","- The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1943,ARR_2022_247_review,"- The authors should more explicitly discuss other work/data that addresses multi-intent sentences. Footnote 6 discusses work on multi-intent identification on ATIS/MultiWOZ/DSTC4 and synthetically generated multi-intent data (MixATIS and MixSNIPS), but this is not discussed in detail in the main text. - Additionally, footnotes are used FAR too extensively in this paper -- it's actually very distracting. Much of the content is actually important and should be moved into the main body of the paper! Details around parameter settings etc. can be moved into the appendix to make space (e.g., L468).
- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.
- The ontology definition and annotation scheme itself is glossed over in this paper, although it is a major contribution. The authors should help quantify the effort required and comment on the feasibility of scaling their high-quality annotation to other domains. 
Comments: - The paper in general is very dense (and thus difficult to get through in parts). The authors frequently include numbered lists in the paragraphs that might be easier to read as actual lists instead of in paragraph form (where appropriate).
- 163: This statement is unsupported ""First, the models went back to focusing on single-turn utterances, which..."" - Footnote 6: As described in the weaknesses section, the authors should more explicitly describe these works and provide examples of how their work aims to improve on them.
- 196: Need more description here -- many parts of the proposed NLU++ ontologies are also highly domain specific (e.g., intents like ""spa"" and ""card"").
- Table 4: Should include other attempts at multi-intent datasets here (DSTC4, MixATIS, etc.).
- Table 8: Some of the ""description-questions"" shown are ungrammatically, e.g., ""is the intent to ask about some refund?"", or ""is the intent to ask something related to gym?""
- Could the annotation scheme be easily scaled up to more domains? How much effort would be involved in ontology definition and annotation?
Typos: - 166: Space after footnote 5.
- 340 (and later): ""Data/Domain Setups"" -> ""Setups"" could either be ""Setup"", or ""Settings""/""Configurations""? ","- Some of the intents do not really confirm to standard definitions of an intent, e.g., ""card"" (Fig 1). This does not actually describe the ""intent"" behind the utterance, which might traditionally be something like ""confirm_arrival"". "" Card"" in this case could be considered more like a slot and maintain a similar level of genericness. On the other hand, intents such as ""less_lower_before"" may be overloaded. While it makes sense to try to make slots more generic so they can be reused across new domains, the authors can more explicitly articulate their reasoning behind overloading/over-specifying intents.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5689,NIPS_2018_680,"weaknesses are: 1) The statistical aspects are a bit eluded: is the given algorithm robust to statistical imprecision? 2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?","2) The algorithm runtime in N^6 can still be quite slow, are there other heuristics that are faster, or can this be improved in special cases?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5198,NIPS_2020_1687,"- Since the proposed optimization is non-convex, there is no guarantee for the correctness of the bounds. - Perhaps the most important missing result in this work is confidence intervals for the bounds. - At some parts, for instance the choice of function family for p_\eta, it seems that the only criteria for the choices in the model is to make the optimization task efficient and no other justification is provided. - Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose. - In the experiments, only two cases are considered: linear Gaussian case and a second case in which the treatment is again linear and the outcome is generated by 0.3X^2−1.5XC+e. It seems necessary to consider other instances of non-linear cases as well. - The choice of the outcome equations (X-6C+e and 0.3X^2−1.5XC+e) look random. Was there any specific reason for this choice?","- Is there any intuitions or guidelines for choosing the response functions? I thought MLP should be a good choice, but the resulting bounds seem to be loose.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2019,ARR_2022_292_review,"1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior. ","1) Lack of interpretability: There could be more of a discussion of why ""semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers"". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5808,NIPS_2018_630,"Weaknesses: - While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score. ","- The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
589,ICLR_2022_3053,"Weakness:
It’s difficult to identify the real novelty/contribution of the proposed method. The core techniques such as importance score-based pruning and full model distillation (essentially self-distillation) have been explored in the network pruning literature.
The rules designed for parameter redistribution are draw from the observations in Fig. 4. However, there is no ablation study to validate its effectiveness as compared to other design rules.
The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction.
Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters).
There is a lack of comparison with other pruning methods.
Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).","4. However, there is no ablation study to validate its effectiveness as compared to other design rules. The proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction. Compared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters). There is a lack of comparison with other pruning methods. Ablation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2404,NIPS_2021_655,"weakness about their results in the final section. However, I have some concerns on the paper. First, the related works might have been cited adequately, but the authors could have done better comparing with them. Second, there is a concurrent paper that covers many of the results in this paper. I vote for a weak reject.
The concurrent paper shall not be a concern. After discussion I would raise my vote to accept. Limitations
The recent paper [1] studies the more general tensor bandit, where the reward function can be more general low-rank p-tensors. In particular, the p = 2
case in [1] seems cover the main contributions (Theorem 1-3) in this paper. Besides, Algs. 1 and 2 in [1] look quite similar to Alg. 1 in the submission, but the ones in [1] are much more general. For example, the ones in [1] can solve general ⟨ θ ⋆ , a t ⟩ p
reward, while the submission only studies ⟨ θ ⋆ , a t ⟩ 2
. The authors may need to compare their results with those in [1].
The lower bound (Theorem 2) seems to be the same as [2, Theorem 6] by putting r = 1
. The authors might need to point it out more explicitly, and it would be strange to list it as a contribution.
Other Comments
Line 72, what is x x ?
At Line 125, the idea of using curvature to obtain sqrt regret for ETC could date back to even earlier papers, such as [3].
Societal Impact
The paper is mainly theoretical, and I don't see any potential negative societal impact. Bibliography
[1] Huang, Baihe, Kaixuan Huang, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, and Jiaqi Yang. ""Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."" arXiv preprint arXiv:2107.04518 (2021).
[2] Lu, Yangyi, Amirhossein Meisami, and Ambuj Tewari. ""Low-rank generalized linear bandit problems."" In International Conference on Artificial Intelligence and Statistics, pp. 460-468. PMLR, 2021.
[3] Rusmevichientong, Paat, and John N. Tsitsiklis. ""Linearly parameterized bandits."" Mathematics of Operations Research 35, no. 2 (2010): 395-411.",1 and 2 in [1] look quite similar to Alg.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5037,NIPS_2020_1663,"Besides the concerns about the assumptions (discussed in Strengths), my other major comment is related to experiments. 1. It seems that the proposed approach was not compared against any existing imputation method. Without doing so, it is very difficult to see the real value of the work. 2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice. 3. I find the results in Section 6.2 a bit weak. Do the results echo findings reported in the literature? Such comparisons could be useful.","2. In Section 6.1, the authors ""artificially delete intervals completely at random with probability 0.2."". I am wondering how 0.2 was chosen? Based on my experiences, the probability could be much higher than 0.2 in reality. How does the proposed method work when it is the case? Some parameter analysis (the robustness of the work with respect to the probability) would be nice.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4199,NIPS_2020_630,"- Authors miss to cite and comment some very related work in network interpretability such as: Bau et al, CVPR 2017. Network Dissection: Quantifying Interpretability of Deep Visual Representations. I think the work in Network Dissection is directly relevant to the topic discussed in the paper, particularly for its hierarchical analysis as well as focus on ImageNet. - It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics. - Why did authors restrict themselves to a smaller number of categories? I think it would be interesting to understand the behavior on a larger category set. - Have authors evaluated qualitatively some of the metrics? I think it would be interesting to see whether, additionally to the quantitative evaluation, the images have some features in common. After rebuttal: The authors have addressed most of my concerns and I will update my score to 7.","- It would also be interested to get analysis on multiple runs and other datasets. The paper is very interesting and has good insights, but the reader might wonder whether this effects are artifacts specific of some particular training run or this particular dataset. I think it would be good to have more variability to better understand the dynamics.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2300,ACL_2017_371_review.json,"Weaknesses: - The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section ",- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5033,NIPS_2020_1579,"- The presented analysis is limited to a single decision affected by a confounding variable. It is a sensible assumption in many cases, but I got dissapointed by the fact that the current analysis doesn't seem to **conceptually** extend to the general case. - The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one. - Really, the paper presents a **family of bounds** parameterized by Gamma, but Gamma isn't an easily interpretable quantity. Can this be rephrased in terms of a more standard notion (e.g. an information quantity)?",- The bounds seem really hard to compute. It would have been nice had the paper provided a (perhaps more conservative) yet simpler expression alongside the current one.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4330,NIPS_2020_1680,"The work is already quite extensive and more cannot be reasonably expected. Still, perhaps the authors can comment on these issues: 1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1. 2) While Manchev and Spratling 2020 claim that their difference target prop for RNNs outperforms BPTT in (simpler) 4 of 5 tasks, why does DRL which is an improvement on DTP not come closer to backprop in these tasks? 3) Would a correction different from GN targets enable to come closer to backprop? 4) In lines 177-178, the authors write 'This damping interpolates between the pseudo-inverse and the transpose of Jf ̄ i,L , so for large λ, GN targets resemble gradient targets.' I would assume that the authors would optimize over lambda as a hyperparameter, but I did not find it in the list of hyperparams in Tables S1-S4. WHy not? In any case, it would be good to explore the dependence of performance on lambda explicitly in a (supplementary) plot and how this impacts performance expecially given that for 'large λ, GN targets resemble gradient targets'.",1) Following up on lines 245-246 'GNT on nonlinear networks does not converge to a true local minimum of the loss function' and the experimental gap between DRL+variants and backprop in section 4 / Table 1.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4757,NIPS_2020_1819,"- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction. - The results are not significantly higher than that of the closest previous work on English. On Chinese it is, but one has to wonder whether that might be due to implementation or hyperparameter differences rather than the particular model. - There are some missing ablations for quantifying where the performance gains come from, and for justifying some design choices (see details below).",- The approach is not that novel compared to previous incremental parsing approaches. The main controbution is a more expressive parameterization of parsing action prediction.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3203,NIPS_2020_1444,"1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point. 2. In Theorem 1, it seems that the algorithm can only achieve meaningful results when the bias term decreases very fast with respect to t. In some other cases (such as when the bais is fixed), the regret is linear. 3. In Theorem 2, to achieve meaningful results, one has to know the value of v in advance, which is generally impossible.","1. Although the authors motivated their algorithm from real-world problems, it seems to me that it is very difficult to implement Algorithm 1 to real-world applications (since we have to solve the problem in Eq.(7)). Moreover, since the decision set K can be of any form, I think it’s not easy to sample an action x_t\in K after we get p_t. It would be better if the authors can add more discussion on this point.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1580,ICLR_2023_3912,"Weaknesses: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.","1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5533,NIPS_2020_1458,"- As shown in Table1, the proposed Element-weighted TriHard loss actually does not perform better than several existing strategies. Although combining EWTH with HTH or HNTH gets improved results, the effectiveness of the designed method of this paper is incremental. - the best value of t is varying for different methods and datasets (Supplementary materials), so it is too sensitive to selected. - Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.","- Mathematical symbols in the Section of Method have some problem. For example, x_k under equation (5) has not been mentioned earlier. Because there are so many formulas, it's best to define all the symbols clearly in the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3185,NIPS_2020_1801,"1. A paragraph/section on 'Problem setting' is missing (no notation, variables introduced), the definiton of the energy function itself is not clear, is it E(\dot, \dot) or E(\dot)? Is y missing? What is the difference between y and y'? I think the whole section 2 can be rewritten in a more clear way (better flow and references from text books, relevant papers). 2. The key observation, starting at line 92 can be made more formal, or maybe show an syntetic experimet for validation of the claim. 3. Recent work (https://arxiv.org/abs/1906.02845) show that denisty based models fail at OOD, I wonder if the authors have tried similar experiments and if the same issue arises with the energy-based score (since it should be aligned with the in-domain density). If OOD from more complex (CIFAR/SVHN) -> simpler datasets (MNIST) works this will show one more advantage of the energy-score. 4. Were other architectures explored besides WideResNet? 4. a. Maybe add a toy example on synthetic data (such as two concentric rings or moons dataset) to showcase the method works with a simple network as well. 5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?",5. Details in Appendix B state that results are averaged over 10 runs. Why is there no indication of standard deviation? Were these 10 runs done over random hyper-paramter configuration for all baselines or 10 runs with the best selected hyperparameters' values?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
88,ICLR_2022_3123,"Weaknesses:
The proposed algorithm is lack of novelty. Compared to the previous video transformer (ViVit, TimeSformer), the difference of this work is: 1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention. 2) this work introduce additional residual connection (with learnable ratio) between attention block from adjacent transformer blocks. 3) the final output is weighted average of the output from each transformer block while the TimeSformer take the output of last block as the final representation.
This paper would need more evidence to support its claim. For example, it's not clear why we should have the current layout of the network. How a set of spatial transformer block followed by a set of temporal transformer block is better than alternating design? considering the cross-stage self-attention could work in both cases.
The improvement of performance is minor. Take the ViVit as the baseline. If we are going to compare between the best performance between ViViT and this paper, the improvement is minor (81.3% from ViViT vs. 81.8% from this paper). If we are going to compare the performance using the same input resolution (16x224x224), the performance of this paper is still on par with ViViT (80.6% from ViViT vs. 80.1% from this paper). In terms of the computational cost, it is wrong to compare GFLOPs between two methods with different inference views. As the performance of any method will not be linearly increased with number of inference views, it is not fair to put 4x3 views for ViViT and 1x3 views for this paper. As a result, the number of 8.6% does not make too much sense. At last, I would suggest to include MViT[1] as one of the reference, which is the SOTA video transformer so far.
The proposed work should be evaluated in at least one or two more video benchmarks. The nature of K400 and K600 is the same. I would suggest also include Something-Something-V2 or Epic-Kitchen datasets.
[1] Multiscale vision transformers, ICCV 2021","1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
198,ICLR_2022_32,"Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper","- The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4113,NIPS_2020_558,"- Further experiments: I would've liked to see more experimental results on additional graph problems. This could drive your point further by showing more examples of how the loss function can be constructed. - Scalability bottleneck: GNN training can only scale up to some level. Thoughts on whether your method can be scaled up despite this limitation? - Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.","- Architecture choices/tuning: In lines 219-227, how did you select this particular architecture? B.2 in the appendix did not clarify this. There are many ingredients to your architecture but it is difficult to figure out what's necessary and what's not. Perhaps an ablation study or detailed results on the validation sets can help here.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3237,NIPS_2020_1454,"- The novelty seems limited. The idea of building correlation in low-res and then refine for or facilitate the high-res results might be new in the literature for correspondence search, but it is quite common and has been widely adopted in previous work doing stereo match, where the main job is also to find correspondence but along epipolar line. - The main contribution of this paper, IMO, is to run 4D convolution on low-res correlation volume, which saves computation and possibly achieve comparable performance. If so, the experiment showing the saving of computational resources, e.g. gpu runtime, flip-flop, memory, must be given. - Similar multi-scale approach in stereo matching often runs fast at the cost of losing accuracy, since the correlation volume in low-res is not as informative as the high-res one, and it is not easy to fix if some mistakes are made in low-res. However the experiments show that the result is even better than SOTA. It would be good to add more explanation and analysis. - It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.","- It would be nice and inspiring to show some qualitative results, possibly with zoomed-in view, for cases where previous methods failed but okay with the proposed method. Also, it's good to show some failure cases and analysis the limitation.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5051,NIPS_2020_1889,"- The performance of the method is still sub-optimal on large scale datasets, and hence there is a scope of improvement. - The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.","- The proposed method is somewhat similar to the earlier work [35] in terms of overall training and prediction algorithms, and some finer details such as usage of data-dependent methodologies for grouping is used.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
663,ICLR_2021_2455,"Weaknesses: In spite of the strengths mentioned above, there are a few questions that are confusing. 1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2. As for the theoretical analysis: a) For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b) For Theorem 1, the core assumption is ‘the expected loss function as a function of beta belongs to a Sobolev space’, which is confusing. Could you provide some explanations of this assumption or give some examples of it? c) Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3. As for the proposed objective function: a) As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing? b) I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).
[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives.","1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4778,NIPS_2020_770,"1. All components, BGV and TFHE, are borrowed from other papers. Similar to Chimera, the switching mechanism is not proposed by the authors. Even transfer learning has developed for a long time. The authors are suspect of just ensembling these ideas together. The reviewer doubts the novelty of the proposed method. 2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.",2. The authors did not explain why FHESGD equipped with BGV performs worse than their system equipped with TFHE-BGV because the authors claim that the BGV is better than TFHE. The reviewer expects more analysis of mechanics. Minor: Fig. 5 and Fig.6 are supposed to compare with Chimera.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2812,NIPS_2022_1430,"Weakness:
One major issue with this paper is clarity of text and definitions. Examples:
Use of <s> as star token: this symbol is widely used as start-of-sentence in the literature
Equation 3, P ( < s > | x ) = ∑ y ∈ A P ( y | x )
, I think there should be some notion of time in this equation, otherwise I am not sure if rest of model makes sense. Second, if this is time dependent posterior, is it simply 1.0 - P(<b> | x, t)?
text issues:page 5, ""is is useful think ...""
Many issues in Table1: 4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean? 4.b one to the last row, there is 0.1/5.9, does this model perform 0.1 on clean or is it a typo?
The definition of the randomly split the words in bottom of page 6 is not clear to me.
The notation pDrop, how about p_drop ?
Another major concern I have is the significance of modeling presented in this paper. I think the change is not very far from original CTC, it only changes state transition of CTC, this can be done for any other model, like RNNT. While important, I am not sure if it is significant.
Finally, there is a major concern about baseline comparisons:
All the experiments presented are on simulated dataset created by authors, no previous numbers are reported on these partially labeled dataset. The main comparison is with fully labeled data. I don't think this sufficiently evaluate STC. Why not reporting some number on some other datasets, like the ones presented in reference 2 or 3.
STC is not compared against other methods for recovering partially labeled data. For example one baseline can be creating pseudo label from CTC + LM for missing labeled and train with partial+pseudo labels. Maybe this will do as good as STC?","4.a row starting with ""TRANSFORMER[36]"": there is some number under LM (2.5/5.9), what does this mean?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1572,ICLR_2023_3811,"Weaknesses
Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow. P. 3, p. 4, l.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5570,NIPS_2020_133,"(-) Some claims are way too strong. *“we study and prove the incapability of standard activation functions to extrapolate” is debatable. The proof is mainly related with ReLU and tanh. * After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.","* After Corollary 1, row 164: “the proposed activation function is a more general method than the ones previously studied”. In what sense a periodic function is more general? (-) The main problem is the evaluation of the method. The paper does not show that the proposed method provides clear benefits against existing methods. Experiments do not show that Snake activation is better than the existing activation functions - ReLU and their variants (e.g. Leaky ReLU) on any benchmark database. From Figure 5, I can conclude that Swish and Leaky ReLU are comparable with the proposed method (or even better) for CIFAR-10. Thus, why these methods are not also used in the last two experiments (6.2 and 6.3) where Snake activation obtains the best results? The same omission is hold for MNIST (figure 3). The above reflects my understanding, and I may have missed something. But if I am correct, the experiments in this paper fail to demonstrate the usefulness of the method.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4522,NIPS_2020_988,"The introduced method relies on sparse distributions, which is a quite strong assumption. While the authors address the main implications of this assumption, I think there should have been an even more detailed discussion/empirical evaluation to increase the impact of this work in the community: - in the semi-supervised learning experiments in section 5.1 you use a VAE model which is relatively simple and by now quite outdated. Do you expect these results to generalize to more complex architectures? For example, if I took any of the SOTA semi-supervised deep generative models and just replaced the softmax with the sparsemax would you expect similar improvements? - how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data? - since sparsemax is such a core component of this method, it would be useful to add some details on its forward/backward passes and their computational complexity wrt the softmax.",- how does this method behave with challenging tasks that may contain many ambiguous data points? Would the model just use lots of loss evaluations throughout the whole training procedure (and not only in the beginning as in your experiments) or would the sparsity assumption make the model learn to be certain even for ambiguous data?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
141,ICLR_2022_1421,"Weakness:
For the Per-SSFL framework, the local (personalized) model and global model are used. The memory consumption aspect should be discussed. For resource-constrained edge clients, high memory cost could be an issue.
Although it mentioned in the implementation setting that the client number selected per round is 10, it is not clear how many total clients are used in the FL setting.
In Table 1, what is the FL method under the supervised setting?
In Figure 2, what does (0.5) for SSFL on IID and SSFL on non-IID mean?
As can be seen in Figure 2, the convergence rates for the IID and non-IID cases are quite similar. Can you provide an explanation for that?
After reading Appendix D and Figure 10 in Appendix, the experimental setting on GLD-23K is still not quite clear. For example, how the local training set for each client is generated? What about the label distribution? Also, it seems that the number of clients used on the GLD-23K is different from that on CIFAR-10.
For ease of comparison and implementation, it would be good to evaluate the method on more commonly used datasets such as CIFAR-100 and Tiny-ImageNet and other datasets besides vision datasets (e.g., text) for FL.
It would be interesting to see the SSFL results under different numbers of selected participant clients. Since λ
is an important parameter that balances consensus and personalization, its effect should be studied.
Minor issues:
Section 2.1, “… the local empirical risk over the heterogeneous dataset D k
.” -> D k
Figure 10 appeared in Sec. 5.2 and 5.3, it should be Fig. 2 and Fig. 3.
In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent.
Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)”
A careful proofread of the paper is highly recommended.","3. In Figure 2(a), the colors for the curve (SSFL on non-IID) and its legend are different (pink vs. red). It should be made consistent. Sec. 3.2, “… contemporary self-supervised learning frameworks (e.g., SimSiam, SwAV, BYOL)” -> should be “… (e.g., SimCLR, SwAV, BYOL)” A careful proofread of the paper is highly recommended.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
157,ICLR_2022_939,"Weaknesses
Novelty is somewhat low.
Discussion of gradient explosion seems inaccurate.
Some discussion of related work is missing, particularly about related works in the model-based RL literature.
Sometimes they are stating things as facts without providing evidence.
There was no detailed discussion of how the results with the current simulator relate to other recent simulators such as Brax. Are the wallclock times similar? The results seem mixed, but having a detailed discussion would have been useful.
The simulator is fast, so it may have been better to do more than 4 environments. Recommendation
I recommend accepting the paper as I have not seen differentiable simulators used for tasks of the difficulty considered here (while also taking advantage of differentiating through the simulator). I was considering a score of 6 or 8, but gave 6 for now.
Discussion of points brought up
Novelty: It seems the contribution is primarily one of engineering, and they don't propose any surprising new idea. The idea of truncated backpropagation is old. Moreover, the policy training scheme resembles that of the the Dreamer algorithm (Hafner et al. 2019). Dreamer also uses short horizon rollouts together with a terminal value function, and backpropagates through these short horizons to optimize the policy. The differences are only: Dreamer uses lambda return weighting during the short horizons (why didn't you use this?), and the method of constructing the sequences is different (Dreamer samples start states from a replay buffer and performs model rollouts from these states, while the current work splits an episode into chunks). I am surprised that Dreamer was not discussed when explaining the methodology. Certainly it should be mentioned that there are prior works using a similar policy optimization procedure (with slight variations).
Discussion of gradient explosion: There are other earlier works that do a more detailed job of discussing the gradient and landscape issues, such as PIPPS (Parmas et al. 2018), which should have been cited and discussed (moreover your methodology was very similar to these previous works). In your work, if gamma were 1, the value function were perfect and the policy were deterministic, the gradient you compute with your method should be exactly the same as the gradient that is computed using BPTT. From this point of view, your discussion is insufficient, as you do not explain why the loss landscape and gradients end up being smooth despite this fact. I can think of two possible reasons: 1. The value function is an approximator that ends up being smooth because of limited capacity to model the complicated landscape. 2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance.
For the other points see the raw notes at the bottom of this section.
Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions
Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?
Q2. In Figure 2, did you use a deterministic or stochastic policy? Was this the same policy as was used during training the value function? If the policy was stochastic, then how did you evaluate the landscape? This would require sampling many trajectories with the same policy and averaging. Are the scales on the left and right figures the same?
Q3. Did you do any ablation study of the policy noise? Does the method still work when the policy is deterministic? How much does the performance drop?
Additional notes made during reviewing
""for systems ranging from robots (e.g., Cheetah, Shadow Hand) to complex anima- tion characters (e.g., muscle-actuated humanoids) using only high-level reward definitions."" Please provide references.
""A differentiable simulator provides accurate first-order gradients of the task performance reward with respect to the control inputs."" This is speculation. You provide no evidence. Problems with accuracy can arise when the task performance depends on a sampled initial state (so that the gradients are inherently stochastic). Perhaps change to ""may provide"".
""However, despite the availability of differentiable simulators, it has not yet been convincingly demonstrated that they can effectively accelerate policy learning in complex high-dementional and contact-rich tasks, such as some traditional RL benchmarks."" While this may be the case for differentiable simulators, there are several model-based RL works that showed effective learning (e.g., Dreamer. The difference with a simulator is just that the model does not have to be learned. I think the claim here is downplaying such previous contributions. Also, ""dementional"" should be ""dimensional"".
""There are several reasons for this: 1.(), 2.(), 3.()"" These reasons are stated as facts, while they are speculations. Perhaps, ""possible reasons"" would be better. At least points 1 and 3 should be the same for model-free RL, so are they really the reason? No references were provided.
""Because of these challenges, previous work has been limited..."" How do you know that those were the challenges that limited the applicability of the previous methods? The publications themselves do not seem to note your reasons as the reason why they limited their experiments. For example, the PODS paper says their method overcomes the issues of exploding gradients.
""In addition, we propose a truncated learning window to shorten the backpropagation path to address problems with vanishing/exploding gradients and reduce mem- ory requirements."" This is known as ""truncated backpropagation"". It is plagiarism to claim that you ""proposed"" this.
In Equation 1, please provide the definition of the Jacobian's and gradients. Usually, the gradient is a row vector, whereas you are using a column vector.
""This makes the reward function smoother..."" What do you mean by this? The reward function is the same in all cases.
""In addition, we apply state normalization as is common in RL algorithms."" Please explain what ""state normalization"" is.
""First, the terminal value function absorbs the discontinuity of long dynamics horizons and early termination into a smooth function, as shown in Figure 2 (Right)."" This explanation is incomplete. If you had no discount factor, a perfect value model and a deterministic policy, your computed policy gradient would be exactly the same as that of BPTT.
""Finally, the use of short horizons allows us to update the actor more frequently, which, when combined with parallel differentiable simulation, results in a significant speed up of training time."" Do you have an ablation study showing that it speeds up?
""In contrast, our method scales well due to direct access to the true gradients from differentiable simulation."" You don't have access to true gradients when you are using a stochastic policy. It may be better to explain this by referring to the fact that reparamterization gradients are often more accurate when computing gradients of smooth functions.","2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance. For the other points see the raw notes at the bottom of this section. Parmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018, July). PIPPS: Flexible model-based policy search robust to the curse of chaos. In International Conference on Machine Learning (pp. 4065-4074). PMLR. Questions Q1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
339,ICLR_2022_2842,"Weaknesses: 1. In introduction section, author claimed ""Representation Topology Divergence (RTD) score which measures a dissimilarity between two point clouds of equal size with one-to-one correspondence between points"". The question is how to explain ""one-to-one correspondence between points"", what does ""one-to-one"" mean here? It seems this is an important part of contribution, but there is no specific explanation about this so it is confusing me.
The writing is not clear. For example, some concepts in section 2 are not well explained, such as Barcode, Vietoris Rips complex and homology group are not be familiar with ML community. It should be better to add some figures and intuitive explanation about these abstract topology terms.
3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score?
It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?","3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score? It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
698,ICLR_2021_1682,"weaknesses
+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.","- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
873,ICLR_2023_507,"Weaknesses
I encourage the authors to put forward the general notion of sparsity that is assumed across the paper (as defined in S ( w )
) early on in the introduction so that reader can follow the ideas put forward in Fig. 1.
One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent.
From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal)
By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?","1. One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent. From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal) By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2056,ARR_2022_18_review,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7. The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1, and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages? 
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda = 1.5 nA -> na (short vowel) ",2. No details on the. multi-task learning mentioned in Section 4.4 are available.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1900,ARR_2022_334_review,"- There are some technical aspects of the paper that weren't clear to me:  * L271: Was the same fine-tuned RoBERTa model, which was used as a toxicity classifier, used to embed the paraphrased sentences from ParaNMT to check their cosine similarity to decide if they should be processed through the retrieval pipeline?
 * L292: Which BPE tokenizer are you referring to? The RoBERTa Byte-level BPE tokenizer? 
   * It wasn't clear to me if the 671 parallel sentences which were used as a blind test were part of the ParaDetox 12,000 examples or not. If not, were they created through the generation pipeline or the retrieval pipeline (i.e., from ParaNMT)? Are you planning on releasing an actual train/dev split with the dataset?
 * Hyperparameters weren't mentioned to replicate experiments for fine-tuning BART.
 * Although the Data Collection Pipeline section (Section 3) was clear, some parts of the paper were hard to follow. 
I think the paper would benefit from another round of revisions to fix some typos. It would also be helpful to the readers to know the specifics of the various experiments conducted (e.g., what embeddings were used? what BPE tokenizer? what were the hyperparameters used to fine-tune BART?) ",- There are some technical aspects of the paper that weren't clear to me:,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2242,ACL_2017_699_review.json,"Weaknesses: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1. 
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,    The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc. 
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845). ","4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4173,NIPS_2020_1855,"Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as ""undecided"", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the ""raw"" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have ""training the neural representation"" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.","- More a comment: I found the ""Algorithm 1"" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1587,ICLR_2023_2147,"Weakness
While the proposed shortcoming of MPJPE and ECE metric makes intuitive sense, I find the proposed method is quite disconnected from the main motivation. It is hard for me to find how the design choices made for cGNF relate to a better measure of the underlying distribution. The proposed “training using subset of observation” is close to a masking strategy and the training loss is a fairly standard NLL loss for normalizing flow in pose estimation [2]. As a result, I could not make a connection between the objective of obtaining a better-clibrated model and the actual proposed method (is the innovation in the architecture? I could not make a connection there either). While I find the analysis interesting and well-designed and quantifies a known pose estimation issue well (that SOTA methods often do not measure uncertainty well), the method does not seem to draw insight from it.
The claimed to estimate both conditional (which I see) and marginal (which I do not see) using the cGNF model needs to be further explained.
I also do not see how this is a “zero-shot density estimation problem” and how randomly using a subset to train can lead to this. (It could be me not understanding it and if the authors could further elaborate on this I could consider raising the score.).
While the paper focuses on quantifying uncertainty and occlusion in pose estimation, few examples and results were actually shown showcasing the strength of the model. It would significantly strengthen the claim if extensive visual examples could be shown the benefit of the model (e.g. uncertain 2D keypoints actually correspond to the more spread-out hypothesis, and, equally important, that 2D keypoints with little ambiguity leads to a model that is closer to the mean).
Question to authors
During the analysis of the miscalibration behavior in using minMPJPE, the models’ samples’ deviation from the median is used to construct the error distribution. The difference between the ground truth and the median is then used to approximate the ground truth error.
I am not sure how the second part approximates the actual uncertainty in the ground truth samples. Each ground truth sample m
has a unique uncertainty associated with it. For instance, occluded poses lead to more significant errors. In equation (3) the summation term lumps all of them together and forms a distribution. This amounts to measuring the uncertainty at a per-joint level ( ϵ m , k
) and not per sample level. On the other hand, the uncertainty of the model is done at a per-sample model. I understand that it would be difficult to measure the uncertainty at a per-sample model for ground truth data, but the current model seems questionable.
[1] Wehrbein, T., Rudolph, M., Rosenhahn, B., & Wandt, B. (2021). Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188.
[2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , 11585-11594.
Small error: right above page 3 equation (3), should it be ϵ m , k ∗ ?","2021 IEEE/CVF International Conference on Computer Vision (ICCV), 11179-11188. [2] Kolotouros, N., Pavlakos, G., Jayaraman, D., & Daniilidis, K. (2021). Probabilistic Modeling for Human Mesh Recovery.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3293,NIPS_2020_1480,"1. The novelty of Theorem 1 is not entirely clear. The authors argue that the classical another parametrization B, Theta = (C Omega^{1/2}, Omega^{1/2}), which also enables joint convexity. Therefore, Theorem 1 doesn't appear to be a significant result in itself. This may be a lemma instead. 2. It is not clear why the classical parametrization does not allow penalty functions. One can include penalties on C by using the classical parametrization and levering C=B Theta^{-1}, e.g., || B Theta^{-1}||_{1,2} where 1,2 is the group L1 penalty. 3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage. 4. This work needs a more detailed comparative analysis in order to prove the superiority of the proposed approach. In particular, paper is lacking an adequate computational complexity and run-time analysis with respect to the existing methods, e.g. the standard parametrization and penalty approach. A similar weakness also exists in the numerical results section, where comprehensive empirical comparisons are lacking. The supplementary material has a numerical table, which shows a very incremental improvement and is not conclusive. 5. In proving Theorem 3, the authors employ standard methods such as restricted eigenvalues. These methods are already known to extend to non-convex objective functions and does not specifically hold for the claimed convex formulation.","3. It looks like the authors argue the advantage of convexity in section 2.1, but later on introduce non-convex regularizers, which in my opinion destroys the purpose. The resulting overall problem is non-convex. It is very unclear what convexity of a part of the objective function provides. Moreover, without any empirical comparison with the standard parametrization, it is hard to claim an advantage.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1870,ARR_2022_14_review,"-The idea makes sense for the long document summarization, but I’m wondering what the others have done in this area with a similar methodology? What does the system offer over the previous extract-then-generate methodologies? This is troublesome considering that the paper does not have any Related Work section, nor experimenting other extract-then-generate with their proposed model.
- The extract-then-generate can be re-phrased as a two-phase summarization system that can be either trained independently or within an end-to-end model. The choice of baselines is a bit picky here considering the methodology. The authors should report the performance of other similar architectures (i.e., extract-the-generate or two-phase systems) here. - While results are competitive on arXiv, some of the baselines are composed of less parameters and obtain better performance.
-The paper lacks in providing human analysis, which is an important part of current summarization systems as to revealing the limitations and qualities of the system that could not be captured by automatic metrics.
- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. 
In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim. ","- The paper misses some important experimental details such as the lambda parameters values, how the oracle snippets/sentences are picked, and etc. It could be improved. In the introduction part, the authors have made this claim: “We believe that the extract-then-generate approach mimics how a person would handle long-input summarization: first identify important pieces of information in the text and then summarize them.” It will be good to provide a reference for this claim.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2144,ARR_2022_61_review,"1. While their method addresses some of the issues with existing work on diverse paraphrase generation and controlled text generation, none of the existing methods are used to compare to their proposed method. This makes the results of the paper less convincing as the baseline considered is a simple sequence to sequence paraphrase generator without any control over the desired output. 
1. In section 2.3, it is assumed that the variance of the distribution p(q|s) is sentence independent. How is this variance estimated? Is it determined using the sample variance of the quality values in the training data? Since quality is a 3 dimensional vector, is the complete covariance matrix approximated or a diagonal covariance matrix is assumed (which I guess won’t be a reasonable assumption for this problem)? 
2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims. ","2. Was there any significance testing done for the results on automatic metrics and human evaluation? Since some of the values between the baseline / gold standard are close to the method’s metrics, it would help solidify the claims.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1678,ICLR_2023_1654,"Weaknesses:
I believe the paper has a couple of fundamental weaknesses as it stands that should be addressed before it could be accepted at ICLR. 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons. 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in ""seq distance to function"" relationships - this is especially important in relation to the point about the Novelty measure above.
4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page ""AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation."" And there are papers writing about this eg Pak et al 2021","1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4544,NIPS_2020_1461,"1. The title seems misleading, the method from my understanding is a way to use GNNs for learning natural laws, or understand GNNs in terms of symbolic models, rather than extracting symbolic models from general deep learning models. If the latter is their main claim, it would be helpful if the authors show the utility of this model on more general tasks, other than natural laws. 2. The paper lacks a strong related work section, missing key details about models that have been used previously for relevant tasks. For example, the authors have not discussed how other papers like Neural Relational Inference or using Generative Models for their tasks. 3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model. 4. It would be helpful if the authors address the natural question how utilizing this model for a general deep learning model is more interpretable or generalizable especially in the cases of noise? 5. The paper needs to add more details in the main draft. Key details are in the appendix, the authors should include majority of Section A.1 in the main paper rather than the appendix. The simulation details should be explained more, are the authors generating time series datasets, the number of samples, length of the time series, how the trajectory is processed etc. While some details are covered in the appendix, these details are essential to understand the task and model.","3. The paper's main claim is interpretability and generalization, however while they reliably show that their model fits the ground truth equation, the authors do not show robustness or generalization on the simulation data at hand, which would be a better indicator of the claims of the model.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2574,NIPS_2019_175,"Weaknesses: 1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.  ","5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2182,ACL_2017_614_review.json,"Weaknesses: - I don't understand effectiveness of the multi-view clustering approach. 
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                  - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub? ","- The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4413,NIPS_2020_1552,"* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs. * Relation to relevant prior work is not mentioned and elaborated. For example, Rajasegaran, et al. ""Random path selection for continual learning."" NeurIPS'19 also propose a similar masking based approach to learn non-overlapping paths for dissimilar tasks. Similarly, PathNet (Evolution Channels Gradient Descent in Super Neural Networks) selectively masks out irrelavent model paramters. These papers should be cited and disucssed (preferably compared against) in this manuscript. * To my understanding, the notion of similar and dissimilar tasks is not accurate. E.g., the prior works on task incremental learning have both sets of similar and dissimilar tasks. (E.g., consider CIFAR100 classes in GEM - NeurIPS'17). In fact the considered set of similar and dissimilar tasks is not too different from the ones considered in earlier works. Specifically, consider a seminal work from Li & Hoeim, ""Learning without forgetting"" (TPAMI), where different datasets such as ImageNet/Places365/VOC/CUB/Scenes/MNIST are considered in continual learning experiments). Nevertheless, the proposed splits and dataset choices should be properly motivated and the authors should also report some experiments on previously considred protocols for fair benchmarking against existing methods. * The annealing strategy is somewhat similar to controller proposed in iTAML (iTAML : An Incremental Task-Agnostic Meta-learning Approach - CVPR'20). * The approach assumes that the task ID is known beforehand. Although this is consistent with some prior works, isn't it a bit restrictive in practical settings? It would be good to explain some application scenarios where tasks ID can be known to motivate the readers. * Equation 3 is wrong, it should be explicitly written. * The caption of Figure 1 should have some description for the MTCL architecture (a) as well.","* The dataset choice seems arbitrary. Since authors are defining a new setting, they should elaborate why specifically FEMNIST and FCelebA are used to create similar and dissimilar pairs.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4950,NIPS_2020_1842,"1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper. 2. The claimed effect of ProtoMod needs more evaluations. The authors provide some qualitative results in Figure 3 and Fig Q.3 & Q.4 (from supp.) to show the proposed ProtoMod can localize the attributes like 'black back' and 'yellow breast'. What concerns me is whether the model is indeed learning the specific attributes (black back) or only the partial semantic like 'black' or 'back'. The part localization experiments (Fig 2 and Table 3) may partially show the model can learn the part information ('back', 'belly') from attributes but does not show if the model can distinguish between 'black back' or 'yellow back'. Neither the attribute similarity maps from Fig 3 and Fig Q.3 & Q.4. I think some qualitative results of the same images with locations of attributes only differ partially (e.g. 'black back' vs. 'yellow back' vs. 'black breast') can further verify whether the ProtoMod is learning the specific attributes or just the color or parts. --------- Ref: R1: Zhu, Pengkai, Hanxiao Wang, and Venkatesh Saligrama. ""Learning classifiers for target domain with limited or no labels."" ICML 2019. --------- Updates: the authors address my concerns in the rebuttal. I raise my score to accept.","1. The novelty of the paper is limited. Learning attribute localization with divergence and concentration losses (L_{AD} and L_{CPT}) for ZSL has been explored in other works like [58]. Similar ideas have also been proposed in previous works like [R1], where the authors also learns part localization and attribute prototypes without explicit supervision and applies to zero-shot learning. I would encourage authors elaborate the differences between these works and clarify the specific novelties and contributions proposed in the paper.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1345,ICLR_2023_1035,"Weaknesses: 1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc.
The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments.
I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e. 600 episodes for each dataset. This is helpful for indicating the chance of handling such hard tasks in real-life scenarios where hard and easy tasks are mixed.
[1] Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. AAAI 2022 [2] Cross-domain few-shot learning with task-specific adapters. CVPR 2022 [3] Exploring Efficient Few-shot Adaptation for Vision Transformers. TMLR 2022","1. While the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc. The paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments. I wonder if it is possible to measure the average ‘hardness’ of the original experiment setting on MD, i.e.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2214,ACL_2017_66_review.json,"Weaknesses: The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).
- General Discussion: The current presented approach, in comparison to existing approaches, is promising. ","- General Discussion: The current presented approach, in comparison to existing approaches, is promising.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4320,NIPS_2020_386,"The weaknesses are : 1. It is not clear how the reparametrization differ from the original parametrization with low rank covariance in terms of predictive performance. 2. It is not clear why the more expressive diagonal covariance is less predictive than the scaled identity covariance. 3. Why the latter covariance is more computationally efficient than the former for deep neural networks? 4. Given that the ELRG-VI has worse accuracy as in the Modern CNNs, what is the advantage of using this approximate posterior? 5. Evaluation of the quality of the posterior approximation by predictive performance is not appropriate as in many cases MAP can give more accurate predictions.",3. Why the latter covariance is more computationally efficient than the former for deep neural networks?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5685,NIPS_2018_639,"Weakness: - I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.  - How does the reader know that the reward definitions of rho for this tasks necessitates a non-convex reward function. Surveillance and data collection has been studied in POMDP context by many papers. Fortunately/unfortunately, many of these papers show that the increase in the reward due to a rho based PWLC reward in comparison to a corresponding PWLC state-based reward (R(s,a)) is not that big. (Papers from Mykel Kochenderfer, Matthijs Spaan, Shimon Whiteson are some I can remember from top of my head.) The related work section while missing from the paper, if existed, should cover papers from these groups, some on exactly the same topic (surveillance and data collection).  - This basically means that we have devised a new method for solving non-convex value function POMDPs, but do we really need to do all that work? The current version of the paper does not answer this question to me. Also, follow up question would be exactly what situation do I want to use the methodology proposed by this paper vs the existing methods.  In terms of critisim of significance, the above points can be summarized as why should I care about this method when I do not see the results on problem the method is supposedly designed for.  ","- I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
304,ICLR_2022_113,"Weaknesses: - The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase ""coarse-grained images"" is inaccurate, the ""coarse-grained"" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)",- It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1661,ICLR_2023_3449,"Weaknesses
1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.). [1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2896,NIPS_2022_1666,"Weaknesses:
I cannot give a clear acceptance to the current manuscript due to the following concerns:
1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work.
For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work.
2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1.
3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper.
In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.
4. Advantage of NAR(heatmap) + RL + Meta-Learning: From Table 1&2, for TSP1000, the generalization performance of AM (G: 31.15, BS: 29.90) trained on TSP100 is not very far from the testing performance of DIMES without meta-learning (27.11) directly trained on TSP1000. It could be helpful to check whether the more powerful POMO approach[4] can have a smaller performance gap. Reporting the results for POMO and DIMES without meta-learning for all instances in Table 1 could make the advantage of the NAR(heatmap) + RL approach in DIMES much clearer.
Hottung et al.[5] shows that POMO + Efficient Active Search (EAS) can achieve promising generalization performance for larger TSP instances on TSP and CVRP. The comparison with POMO + EAS could be important to better evaluate the advantage of meta-learning in DIMES.
[1] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, Thomas Laurent, and Xavier Bresson. Learning tsp requires rethinking generalization. arXiv preprint arXiv:2006.07054,2020.
[2] Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.
[3] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. NeurIPS 2018.
[4] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. POMO: Policy optimization with multiple optima for reinforcement learning. NeurIPS 2020.
[5] André Hottung, Yeong-Dae Kwon, and Kevin Tierney. Efficient active search for combinatorial optimization problems. ICLR 2022.
Yes, the limitations have been adequately addressed in Section 5 Concluding Remarks. I do not see any potential negative societal impact of this work.","1. Inaccurate Contribution: One claimed contribution of this work is the compact continuous parameterization of the solution space. However, as discussed in the paper, DIMES directly uses the widely-used GNN models to generate the solution heatmap for TSP[1,2] and MIS[3] problems, respectively. The credit for compact continuous parameterization should be given to the previous work [1,2,3] but not this work. For TSP, Joshi et al.[1] have systemactilly studied the effect of different solution decoding (e.g., Autoregressive Decoding (AR) v.s. Non-autoregressive decoding (NAR, the heatmap approach) and learning methods ( supversied learning (SL) v.s. reinforcement learning (RL)). To my understanding, the combination of AR + SL, AR + RL and NAR(heatmap) + SL have been investigated in Joshi et.al. and other work (e.g., PtrNet-SL, PtrNet-RL/AM, GCN), but I am not aware of othe work on NAR(heatmap) + RL. The NAR + RL combination could be the novel contribution of this work. 2. Actual Cost of Meta-Learning: The meta-learning (meta-update/fine-tuning) approach is crucial for the proposed method's promising performance. However, its actual cost has not been clearly discussed in the main paper. For example, Table 1 reports that DIMES only needs a few minutes to solve 128 TSP500/TSP1000 and 16 TSP10000 instances. However, at inference, DIMES actually needs extra meta-gradient update steps to adapt its model parameters to each problem instance. The costs of the meta-gradient steps are 1.5h - 10h for TSP500 to TSP10000 as reported in Appendix C.1. Since all the other heuristic/learning methods do not require such meta update step, it is unfair to report that the runtime of DIMES is only a few minutes (which should be a few hours) in Table 1. 3. Generalization v.s. Testing Performance: To my understanding, all the other learning-based methods in Table 1 are trained on TSP100 instances but not TSP500-TSP10000 as for DIMES. Therefore, the results reported in Table 1 are actually their out-of-distribution generalization performance. There are two important generalization gaps compared with DIMES: 1) generalization from TSP100 to TSP10000, 2) generalization to the specific TSP instances (the fine-tuning step in DIMES). I do see it is DIMES's own advantages (direct RL training for large-scale problems + meta fine-tuning) to overcome these two generalization gaps, but the difference should be clearly clarified in the paper. In addition, it is also interesting to see a comparison of DIMES with other methods on TSP100 (in-distribution testing performance) with/without meta-learning.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5265,NIPS_2020_1700,"- There are a number modelling details that are not entirely clear in the paper (see below). - When comparing Gumbel-CRF directly to Perturb-and-MAP MRF, performance is very similar.",- There are a number modelling details that are not entirely clear in the paper (see below).,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3860,NIPS_2020_1228,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.","* Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2038,ARR_2022_256_review,"1. 	I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this! 
2. 	More baselines should be contained such XLM, XLMR, mBART. 
1. The teacher model should also be evaluated to verify whether the distillation process is necessary. 
2. Typo. L-347. ' Figure.4' ","1. I don’t understand how and why the student model is taught by the teacher model. The teacher model is only trained on the source language. When applied to the target language, does it mean that we directly input the target sample into the teacher model to get the teacher distribution? If so, this means that the mBART teacher model is able to conduct NER task in the target language. So why don’t we just use the teacher model to conduct zero-shot cross-lingual NER? I also didn’t see this baseline exists. Hope the author can explain this!",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5573,NIPS_2018_641,"weakness.  First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless.  Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD.  The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes.  I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication.  Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.  4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse. ","2. ln.55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5305,NIPS_2020_1519,"Here are some parts that the paper could potentially improve: - Some typos: e.g. in line 41-43, MLE should come first and SM should come second? - For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way. - Furthermore, in line 173, I noticed that the paper update phi for K times on the same minibatch. Is this a special design? Why not use different batches (which seems to be less biased)? - Also in the paragraph following theorem 2, the paper claims the theorem provides insights into implementation. According to the theorem, the gradient estimation becomes less biased when N is larger. Is this consistent with your empirical observation? I didn't find ablation study on the hyper-parameter K. - Practical usefulness: I understand that the aim of the paper is not to establish a new SOTA. But still I wonder if the proposed method provides any additional practical benefits. It would be cool if the paper can demonstrate this. For example, is there any interesting results if we do Langevin sampling on both image space and latent space? Is it possible to do controllable image generation by manipulating or interpolating the latent variables? These make it different from a standard EBM. Also is it scalable to higher dimension such as CelebA 128x128? - Usually to make score matching work for images, we need to apply noise annealing on the images [1]. Is it necessary for the proposed method? [1] Generative Modeling by Estimating Gradients of the Data Distribution","- For theorem 2, it would be more interesting to explore the setting where G(theta, phi) is not strongly convex (i.e. a weaker assumption), although the assumption is acceptable if it is necessary for making things feasible. Also it seems there is a missing dependence of the bound on the batch size in theorem 2 and corollary 3, are you assuming infinite batch size here? Usually, SGD with biased gradient also depends on the batch size in a non-negligible way.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3822,NIPS_2020_179,"1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack. 2. Suppose the evaluation reflects the model robustness, the region for “small perturbations” is quite small. In CIFAR10, the proposed mechanisms only work on par with adversarial training at epsilon=0.001. I would think even epsilon=8/255 on CIFAR10 are not perceptible by humans. 3. In Table 1, it will be interesting to integrate the proposed fixations with SOTA models. The reported RESNET result is not the SOTA ones. 4. In the gradient obfuscation part, it will be good to clarify what does the authors “verified”. 5. Last paragraph in the conclusion generalizes the work a bit too much. 6. There is no adversarial training baseline in ImageNet10, ImageNet results.","1. Given adversarial accuracy is an upper bound of true robustness, I am not sure whether the adversarial attack in the experiments is strong enough to truly evaluate the robustness. In Figure 3, it is only using 5-20 step PGD (no random restarts, not running for longer steps). It might not be sufficient. I am not sure whether I should trust the results and analysis from the weak attack.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3001,NIPS_2022_489,"Weaknesses:
Concern regarding representativeness of baselines used for evaluation
Practical benefits in terms of communication overhead & training time could be more strongly motivated
Detailed Comments:
Overall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.
The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.
Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.
Robustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.
Some confusion in notation: - Algorithm 2, L8 should be t = 1,…,H (for horizon)? - L100, [M] denotes the set of LBs?
Minor notes: - Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.
[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.","- L100, [M] denotes the set of LBs? Minor notes:",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
924,ICLR_2023_4092,"Weakness: 1. The writing and setting up of this paper are not very clear to me. The motivation does not convincing to me. 2. The novelty of the proposed method seems trivial. Contrastive learning, embedding alignment and etc have been widely discussed in the previous literature.
3. For XQR performance, it is not surprising that with the additional selected XOR finetuning examples, the performance will boost further. 4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.",4. The ablation of the paper is not ready. It would be better to see the parameter comparison or computing comparison between the proposed method and the existing methods. It is also not clear to me that the performance boost is from the proposed alignment components or other mentioned adjustments such as filtering and etc.,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4420,NIPS_2020_341,"- For theorem 5.1 and 5.2, is there a way to decouple the statement, i.e., separating out the optimization part and the generalization part? It would be clearer if one could give a uniform convergence guarantee first followed by how the optimization output can instantiate such uniform convergence. - In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.","- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
5664,NIPS_2018_185,"Weakness: ##The clarity of this paper is medium. Some important parts are vague or missing. 1) Temperature calibration: 1.a) It was not clear what is the procedure for temperature calibration. The paper only describes an equation, without mentioning how to apply it. Could the authors list the steps they took? 1.b) I had to read Guo 2017 to understand that T is optimized with respect to NLL on the validation set, and yet I am not sure the authors do the same. Is the temperature calibration is applied on the train set? The validation set (like Guo 2017)? The test set? 1.c) Guo clearly states that temperature calibration does not affect the prediction accuracy. This contradicts the results on Table 2 & 3, where DCN-T is worse than DCN. 1.d) About Eq (5) and Eq (7): Does it mean that we make temperature calibration twice? Once for source class, and another for target classes? 1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain? 2) Uncertainty Calibration From one point of view it looks like temperature calibration is independent of uncertainty calibration, with the regularization term H. However in lines 155-160 it appears that they are both are required to do uncertainty calibration. (2.a) This is confusing because the training regularization term (H) requires temperature calibration, yet temperature calibration is applied after training. Could the authors clarify this point? (2.b) Regarding H: Reducing the entropy, makes the predictions more confident. This is against the paper motivation to calibrate the networks since they are already over confident (lines 133-136). 3) Do the authors do uncertainty calibration on the (not-generalized) ZSL experiments (Table 2&3)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 4) Do the authors do temperature calibration on the generalized ZSL experiments (Table 4)? If yes, could they share the ablation results for DCN:(T+E), DCN:T, DCN:E ? 5) The network structure: 5.a) Do the authors take the CNN image features as is, or do they incorporate an additional embedding layer? 5.b) What is the MLP architecture for embedding the semantic information? (number of layers / dimension / etc..) ##The paper ignores recent baselines from CVPR 2018 and CVPR 2017 (CVPR 2018 accepted papers were announced on March, and were available online). These baseline methods performance superceed the accuracy introduced in this paper. Some can be considered complementary to this work, but the paper canât simply ignore them. For example: Zhang, 2018: Zero-Shot Kernel Learning Xian, 2018: Feature Generating Networks for Zero-Shot Learning Arora, 2018: Generalized zero-shot learning via synthesized examples CVPR 2017: Zhang, 2017: Learning a Deep Embedding Model for Zero-Shot Learning  ## Title/abstract/intro is overselling The authors state that they introduce a new deep calibration network architecture. However, their contributions are a novel regularization term, and a temperature calibration scheme that is applied after training. I wouldnât consider a softmax layer as a novel network architecture. Alternatively, I would suggest emphasizing a different perspective: The approach in the paper can be considered as more general, and can be potentially applied to any ZSL framework that outputs a probability distribution. For example: Atzmon 2018: Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning Ba 2015: Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions  Other comments: It will make the paper stronger if there was an analysis that provides support for the uncertainty calibration claims in the generalized ZSL case, which is the focus of this paper. Introduction could be improved: The intro only motivates why (G)ZSL is important, which is great for new audience, but there is no interesting information for ZSL community. It can be useful to describe the main ideas in the intro. Also, confidence vs uncertainty, were only defined on section 3, while it was used in the abstract / intro. This was confusing. Related work: It is worth to mention Transductive ZSL approaches, which use unlabeled test data during training, and then discriminate this work from the transductive setting. For example: Tsai, 2017: Learning robust visual-semantic embeddings. Fu 2015: Transductive Multi-view Zero-Shot Learning I couldnât understand the meaning on lines 159, 160. Lines 174-179. Point is not clear. Sounds redundant. Fig 1 is not clear. I understand the motivation, but I couldnât understand Fig 1. ","1.e) It is written that temperature calibration is performed after training. Does it mean that we first do a hyper-param grid search for those of the loss function, and afterward we search only for the temperature? If yes, does it means that this method can be applied to other already trained models, without need to retrain?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3447,NIPS_2020_486,"The method is relatively straightforward and some directions could be explored further. For instance, by using fully connected transformations, typical convolutional weight-sharing is not utilised. It would be good to discuss downsides to fully connected Woodbury transforms, and possible alternative formulations that would utilise convolutional weight-sharing. Further, the models utilised in the experimental section are quite small. As a result, the NLL performance is not very good compared to newer flow-based models. In addition the gains in NLL are quite small, and it would be better if the authors included standard deviations over multiple runs. Minor: - If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck. - For a better overview it would be nice to have a table showing the complexity for different methods for their forward/inverse/logdet in one place.","- If possible, I would advice the authors to include the ""changing bottleneck"" experiment in the main paper. This experiment relates to the required size of the bottleneck.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
3749,NIPS_2020_755,"Sometimes the presentation is dense: a table, for example, would be a more efficient way to compare the derived rates with past results. There are a few discoveries I wish the authors would discuss a bit more, including: - the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible. - showing the bias-variance decomposition explicitly, at least one, would be nice - can you explain why the kernel is redundant when beta=2 (line 204)? - Since the claimed lower bound is novel, can you explain what is new about the construction?","- the generalization to ""adversarial noise;"" e.g. explain why this generalization is plausible.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1451,ICLR_2023_2368,"Weaknesses: 1. There is no theoretical guarantee that the discoveries resulting from the sparse network architecture are unique. 2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here. 3. It lacks model complexity analysis and comparison. Given different levels of biological entities' intra and inter interactions, I am worried about the model's real applicability.","2. The paper missed the description of how to quantify the top-k frequent interaction pairs, which is discussed in section 5.4. As the paper claims, one advantage of this framework is novel discovery. However, very limited results and discussions are presented here.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4087,NIPS_2020_310,"The main weakness of the paper is its lack of focus, which is most evident in empirical evaluations and theoretical results that don’t seem relevant to the main ideas of the paper. I don’t think this is because the empirical and theoretical results are not relevant, but because the paper emphasizes the wrong aspects of these results. To reiterate, the main idea of the paper is that the representations learned when minimizing the InfoNCE loss may be useful for continual learning in cases where the environment dynamics don’t change too much but the reward function does. A secondary idea is the addition of the action information to the InfoNCE. About the last experiment in the procgen environment (Section 6.4), the section reads as an attempt to demonstrate the main algorithm (DRIML) is the best. Not only is this not true because nothing conclusive can be said with such few runs, but it obfuscates more interesting findings and relevant information. - First, it would be useful to provide some relevant information about why these evaluations were performed in the procgen environment. This choice is important for the main hypothesis because procgen environments are procedurally generated. Hence, if we hypothesize that DRIML will learn a robust representation that captures the environment dynamics and will be better suited to overcome the random variations in the environment, then we would expect DRIML to perform better than other models that are not explicitly designed this way, such as C51. This is indeed what happens, but the text does not emphasize what the main hypothesis is and why this environment is relevant. - Second, there are some interesting findings that are not emphasized enough in Table 1. The impact of the action information on the performance of DRIML is striking. In some environments such as jumper, the performance almost tripled. Additionally, it is possible that the advantage that DRIML has over CURL is due to the action information. Here, it would be good to emphasize this fact and leave it for future work to investigate whether CURL would benefit from including the action information into its architecture. These two additions would make the argument stronger because instead of a simple comparison to determine which algorithm is best, the emphasis would be on the two main ideas of the paper that motivate the DRILM agent. About the first and second experiments (Section 6.1 and 6.2), these three sections are great for demonstrating that DRIML is indeed working as intended. However, it is often difficult to tell what is the main takeaway from each experiment because the writing doesn’t emphasize the appropriate parts of the experiments. - In Section 6.1, it seems that the wrong plots are referenced in Lines 217 and 218. The paragraph references FIgure 2b and 2c, but it should be referencing 2a and 2b. Moreover, it would be useful to have more details about these two plots: what are the x and y axis, what would we expect to see if DRIML was working as intended, and why do the plots have different scales? For Figure 2c, it is not clear why it is included. It seems to be there to justify the choice of alpha = 0.499; if this is the case, it should be explicitly stated. Figure 2d is never referenced and it’s not clear what the purpose of this figure is, so it should be omitted. - In Section 6.2, it isn’t clear what architecture is used in the experiment and how the DIM similarity is computed. An easy fix for this is to move most of the information about the Ising model from the main text to the appendix (Section 8.6.1) and move the information about the architecture to the main text. In fact, the appendix motivates this experiment fairly well in Lines 511 to 513: “If one examines any subset of nodes outside of [a patch], then the information conserved across timesteps would be close to 0, due to observations being independent in time.” You can motivate the hypothesis of this experiment based on this statement: if the DIM loss in Equation (6) is measuring mutual information across timesteps, then we would expect its output to have high measure inside of the patches and a low measure outside of the patches. This would make it very clear that the DIM loss is in fact working as intended. About the theoretical results, the main issue is the organization and the lack of connection between the theoretical results and the main ideas of the paper. - In terms of organization, it seems odd that Theorem 3.1 is introduced in page 3, but is referenced until page 6 after Proposition 1. It would be easier on the reader to have these two results close together. - More importantly, it is not clear what the connection between the theoretical results and the main idea of the paper is. The proposition is used as evidence that the convergence rate of \tilde{ v_t } is proportional to the second eigenvalue of the Markov Chain induced by the policy. However, I don’t follow the logic used for this argument since the proposition tells us that if \tilde{ v_t } and v_t are close, then v_t and v_\infty are also close. Combined with Theorem 3.1, this tells us that v_t will converge to v_\infty in a time proportional to the second eigenvalue of the Markov Chain and the error between v_t and \tilde{ v_t }, but it says nothing about the convergence rate of \tilde{ v_t } to v_t. Even if this was true, it is not clear how this convergence rate relates to the continual learning setting, which is the motivating problem of the paper. One could make a connection by arguing that in environments where the dynamics don’t change but the reward function does, the convergence rate of the InfoNCE loss remains unchanged. However, this is not what is written in the paper. This, in my opinion, is the weakest part of the paper, to the point where the paper would be better off without it since it is not adding much to the main argument. Perhaps this proof would be better suited for a different paper that specifically focuses on the convergence rate of the InfoNCE loss. Finally, there are a few architectural choices that are not well motivated. -It is not clear why the algorithm uses 4 different auxiliary losses: local-local, local-global, global-local, and global-global in Equation (7). To justify this choice, it would be useful to have an ablation study that compares the performance of DRIML with and without each of these losses. - Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once. - Third, it’s not clear what architecture is used for the DRIML agent. Line 11 in the abstract mentions that the paper augments the C51 agent, but line 259 says that “all algorithms are trained… with the DQN... architecture,” yet Table 2 in the appendix (Section 8.5) shows hyperparameters that are not part of the DQN or C51 architectures. For example, gradient clipping, n-step returns, and soft target updates (tau in Table 2) are not original hyperparameters of the DQN or C51 architectures. The n-step return is more commonly associated with the Rainbow architecture (Hessel et. al., 2018) and the soft target updates correspond to the continuous control agent from Lillicrap et. al. (2016). There should be some explanation about these choices and, more importantly, the paper should clarify whether the other baselines also use these modifications. Of particular interest to me is the motivation behind gradient clipping since it is not used in any of the 4 architectures mentioned above; is this essential for the DRIML agent? - Finally, how were all these hyperparameters selected? Neither the main text or the appendix provide an explanation for these choices of hyperparameter values.","- Second, in Algorithm Box 1, it is not clear why each auxiliary loss is optimized separately instead of optimizing all of them at once.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
2710,NIPS_2022_327,"Weaknesses
I think the paper is hard to follow and read, in particular there are a lot of statistical details and some of the machine learning details can be lost in there. There is excessive use of alternative notation, for example η
is used for the conditional density p(Y = 1 | X = x), I think it is better to use probabilistic notation directly and not introduce additional greek letters as makes the notation hard to follow. Same can be said for risk definitions.
Overall about readability, I think the paper should be rewritten to put the terms that are usable for implementation from a machine learning perspective (like binary and multi-class uncertainty components), and leave derivations and proofs for the supplementary. The audience of NeurIPS is largely machine learning people, and the paper notation and arguments could be simplified so it is easier to follow for this audience. The statistical details are important but after reading this paper, if I want to implement this method, to me it is not clear what parts I should use or directly how this method works conceptually.
- The results in the paper only present out of distribution detection results as main result, and classification with rejection on text datasets. There are additional results on the supplementary but I believe these are minor. Overall I think there are missed opportunities for evaluation, for example, calibration error of epistemic uncertainty (Figure 2 could be a start point).
- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.
- Since density estimation is computationally expensive, the authors resort to an approximation of the kernel density estimate with nearest neighbors in feature space. I think this is fine, but there is no evaluation of approximation quality or ablations on how to select an approximation for nearest neighbors and kernel density estimates, the authors just use HNSW for this purpose.
Minor issues
I think a diagram showing how the method works and its training/inference process would be an easy way to understand the overall proposed approach.
- I think the paper is missing references to the text datasets (CoLA, SST-2, and MRPC) and other datasets used in the text classification experiments.
- I am not sure what score is used for OOD detection, my guess is that it is the epistemic uncertainty U e
, and this should be explicit in the evaluation section.
As I mentioned in weaknesses, the paper does not mention that it uses an approximation for the kernel density estimate, and this could be a limitation that could be addressed in the future.
There are no other limitations mentioned in the paper about the proposed method, even as the checklist says the paper does discuss limitations (I do not see where, I might be wrong).
Overall I see no negative societal impact to discuss.","- The authors argue that one advantage of this method is that it can disentangle aleatoric and epistemic uncertainty, but this is only vaguely evaluated (only in a toy example in the supplementary), and not in the main paper. I think this is a missed opportunity as this is an important topic that is often overlooked, and there are opportunities for comparison with Kendall and Gall. 2017, which is a well known baseline for uncertainty disentanglement. I recommend that the authors also consider how to evaluate the aleatoric and epistemic uncertainty separately and show that they behave as expected.",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
4202,NIPS_2020_630,"- more analysis on other network architectures are required. analyzing merely the ResNet and VGG family is not enough. the authors shall include results on other families such as DenseNet, GoogleLeNet, etc. - how do the authors estimate the local volume density with kNN (L116)? if one simply uses the region that contains k nearest neighbors to define density, it will have some issues. For instance, it will have a lot of discontinuities. Can the authors be more specific on this and provide more explanations? - I'm slightly confused about Fig. 2(a). the number of measurements for \chi^{l, l+1} is different to (more than) those of \chi^{gt} and \chi^{out}. what happened? there seems to be more variantions for \chi^{l, l+1} when the measurement scale is more fine-grained - does it apply to \chi^{gt} and \chi^{out} too? - the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?","- the authors have argued that the observation may open the opportunities to several interesting directions, such as helping us design better architecture, enabling more powerful training scheme, which I totally agree. But it would be better if the authors can actually show some preliminary results on some of these directions. This will make the submission way more solid. Currently I feel like the depth of the paper can be improved a bit. The observation is cool, but then what?",nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL
1686,ICLR_2023_1725,"Weakness: • This method utilize a DETR pretrained on COCO which limits the application of this method. For some common used inpainting dataset like CelebA, FFHQ, Places, Paris street View, LSUN, it is hard to acquire the corresponding segmentation map to train the DETR. • The author claims ‘COCO-panoptic is more challenging than center-aligned datasets’, but the author should prove the generalization of the proposed method on other domain datasets like face (FFHQ)、buildings (Paris Street View). • One important application of inpainting is object removing while this method aims for new object generation. What about the performance of the proposed method on the object removing? • I find in most figures (Fig 1, Fig 3-5, Fig 8-9), there exist an obvious color discrepancy between the ground truth and the results of this paper. Can you make an explanation? • When processing large missing area, does the segmentation completion network still has the ability to generate new objects or just inpaint with background contents? • How is the model complexity and inference time compared to other methods?",• How is the model complexity and inference time compared to other methods?,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,NO_LABEL,NO_LABEL

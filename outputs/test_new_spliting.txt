Focused review:

Weaknesses:
The main weakness of the paper is that it is unpolished. It seems like it needs a several rounds of edits before it is ready.
Ablations section represent hyperparameter tuning rather than ablation. And its’ setup is optimistic, since the same data is used to tune parameters as the data is then used for presenting the results.
The actual ablation which is missing would be to check whether binning the raw data would give any advances. In general, I am missing intuition behind projected features. The paper does attempt to explain the average, but the whole quantized projection thing would be great to grasp more why it may be a good idea to use (on top of the empirical evidence provided). Details:
Arguable statements:
Introduction, first paragraph. “no anomalies are seen in training” – it is an assumption that is not always true. It is actually makes a problem a little bit easier if one has a luxury of being sure that the training data is free from anomalies. One may have just unlabelled data which may or may not contain anomalies. In fact the authors contradict to this statement themselves, when describe SMAP and PSM datasets in appendix C.
“This demonstrates that the main challenge in time series anomaly detection is not the representation, but rather finding the correct classifier without supervision.” – or one may say that with better representation any decent classifier would work. In fact the next sentence does exactly this, says that with the better representation those simple methods that didn’t work with the first representation now work.
Exp. 1. “while the raw data is not linearly separable, DPW features are able to linearly separate” – if they were able to linearly separate they would have achieved 100% accuracy.
Section 5. “as windows that are well separated in time are roughly independent” – quite arguable statement as what about seasonal time series for example?
Section 5. “Given the above analysis, we conclude that the average of the features of all window in a time series approximately follows the Gaussian distribution” [The main theoretical justification of the method] – the above analysis is rather vague and imprecise. As discussed above the IID assumption is quite questionable, moreover, the Central Limit Theorem also states that the Gaussian distribution is achieved only when the number of samples are infinite. Of course, in practise we can observe the distribution close to Gaussian much earlier, but no further explicit empirical evidence is provided in the paper on that. Only the exp 2 and 3 are used as evidence.
Section 5, last sentence. “… distance does not admit a simple Gaussian interpretation… and therefore was not used for time series anomaly detection” – Presence of a Gaussian interpretation is not required for anomaly detection, therefore, this statement looks a bit odd.
General presentation issues:
Missing references in second paragraph of introduction
Missing details how window level accuracy is computed. Do the dataset have labels on the level of a particular time moment? From the rest of the text it seems that only the whole time series has a label.
Section 4. Exp. 1. “with 10 scales (s=10)” – s was introduced as a scale index, i.e., 10 scales would mean s \in {1, …, 10}, whereas s=10 would mean “with a scale equal to 10” [i.e. a single scale]. Which one is needed here?
Table 4 is wrongly referenced in Exp 1, 2 and 3.
Data for Exp 2. “We used the smaller datasets … ” – firstly, it seems that tables 1 and 2 contain the same set of datasets. Secondly, why for the first experiment it was possible to deal with memory requirements for the larger datasets and for the second experiment it becomes impractical apparently.
Exp 3 is missing details on “multiple adjacent windows” – how many?
Section 5. “constant shrinkage factor” – missing reference
Section 6.1. Results. “GOAD and NeuTraL achieved the best performance of all the baselines” – wrong statement. GOAD is outperformed by other baselines on all the datasets.
Section 6.2. Benchmark paragraph. It was called Datasets in Section 6.1
Section 6.2. Benchmark. “(so dimension r=54 in total)” – r was used before for the dimension of the projection.
Section 6.2. Baselines. The name ST-GCAE should be connected with Markovitz et al somewhere for the Baselines paragraph to make sense with the table 4.
Section 6.3. Baselines. Never in the previous text the names of the conferences were mentioned for the related work.
Section 6.4. Number of projections – What is the number of bins? What is the type of the projections?
Appendix B. Missing reference to Table 8. Missing details on how these error bounds were computed.
Unclear statements:
Window-level anomaly scoring. I am a bit confused here. First, score(a) is computed assuming \alpha_w = 1/n, but then we pretend \alpha_w is not a constant when we take a derivative over it?
Window-level anomaly scoring paragraph. “One interpretation…” – Why would the hyperplane have this vector?
Section 6.1. Dataset. “UEA datasets (obt)” – what is (obt)?
Section 6.1. Results. “It also … is well-grounded” – What does it mean?
Section 6.2. Benchmark. “Each observation…” – does it mean that the experiment is not trying to find anomalies for an individual person?
Section 6.3. Metric. What is point-adjust?
Typos/missing words/other minors:
Missing year in SVDD reference
Section 2. Second paragraph, 4th sentence. “This representation was also used by ” – missing reference apparently
Section 3. First paragraph. s is called a stride and a scale. Better to stick with one name for it.
Section 3. “Discertized” -> Discertized
Section 3, before eq (1). “The resulting projected matrix” – a vector?
Section 3, last paragraph. “maximal and minimum” – to be consistent it is better to use either “maximal and minimal” or “maximum and minimum”
Table 2. “single win” is not immediately apparent to correspond to single window
“OCC” acronym is not defined
Section 5. First sentence. “series series”
Section 5. 3rd sentence. “aa” -> “a”
Section 5. Paragraph starting with “We propose…”. Sentence starting with “While a the windows” -> “a” is redundant
Section 5. Method. “By taking the logarithm and neglecting the constant term,” – of what?
Section 6.1. Metric. “we USE the series”
Section 6.2. Benchmark. “as Markovitz et al. Markovitz et al.”
Section 6.2. Metric. “provided by the dataset” -> “provided in the dataset”
Section 6.3. Baselines. “in ICLR’22” -> “at ICLR’22”
Section 6.3. Baselines. “SMDSu et al.” – missing blank
Section 6.4. Effect of Gaussian density estimation. “but instead the [?] of each dimension”
Section 6.4. Effect of Gaussian density estimation. “We compare [WHAT?] using…”
Section 6.4. Effect of Gaussian density estimation finishes with two “. .”
Section 8. “making finding an accurate classifier even without supervision [POSSIBLE?]”
Appendix F. “Analysis in Sec ??”
Appendix G. Lee et al reference is missing a year.
Appendix H. “anomaly detection can be USED”

Review Point: 1. “while the raw data is not linearly separable, DPW features are able to linearly separate” – if they were able to linearly separate they would have achieved 100% accuracy. Section 5. “as windows that are well separated in time are roughly independent” – quite arguable statement as what about seasonal time series for example? Section 5. “Given the above analysis, we conclude that the average of the features of all window in a time series approximately follows the Gaussian distribution” [The main theoretical justification of the method] – the above analysis is rather vague and imprecise. As discussed above the IID assumption is quite questionable, moreover, the Central Limit Theorem also states that the Gaussian distribution is achieved only when the number of samples are infinite. Of course, in practise we can observe the distribution close to Gaussian much earlier, but no further explicit empirical evidence is provided in the paper on that. Only the exp 2 and 3 are used as evidence. Section 5, last sentence. “… distance does not admit a simple Gaussian interpretation… and therefore was not used for time series anomaly detection” – Presence of a Gaussian interpretation is not required for anomaly detection, therefore, this statement looks a bit odd. General presentation issues: Missing references in second paragraph of introduction Missing details how window level accuracy is computed. Do the dataset have labels on the level of a particular time moment? From the rest of the text it seems that only the whole time series has a label. Section 4. Exp.
==================================================

Focused review:

Weaknesses
Things I liked about this work:
an interesting idea: I really like the idea of intrinsic regularisation by using attention. It seems like a useful technique that may lead to exciting future work too.
a well structured paper: The overall narrative and flow of the paper is very well put together so it is easy to read and understand.
a good set of experiments: I also like the type and number of experiments ran and their overall presentation.
Things that can be improved:
misleading contribution: there are a few issues with the current state of the claimed contribution. Claiming as contribution the observation that dominance and collision can occur in dual-arm control is misleading. These are not novel observations and are therefore not a contribution to this work. In addition, the claim that the solution ensures preservation of safety in collaborative shared spaces is also a bit misleading because it is implicitly learnt over trial and error with no follow up guarantees. See below for more details.
insufficient number of seeds - 3 is not enough: although the paper does a great job at evaluating different aspects of this work, it uses 3 seeds per experiment which is significantly insufficient for model-free RL, especially when there are complex dynamics introduced from the manipulation set up and the bimanual nature of the tasks. I find this the biggest weakness of this paper.
overall clarity can be further improved: although the paper does a great job at explaining the methodology and presenting the achieved results, there are a number of unclear statements that can be further improved.
Claim and Contribution
One concern I have for this work is the practicality of the solution. A large part of the claim is on preserving safety in collaboration in shared spaces. This is truly an important problem in physical systems although not necessarily a big deal for simulation. Although there is nothing wrong with proposing a solution to this with simulated evaluation, it still seems important to somehow ground it to the context where it practically matters. However, as it stands, it is hard to see how this solution can be at all scaled in practice. Specifically, the approach only implicitly stimulates reduced collision through regularisation. This, however, does not prevent from collision in the process of learning and what is worse is that it does not provide any guarantees that there will be no collision once a policy has been extracted. This seems to deem the solution impractical for part of its claimed contribution. However, if there is another benefit to minimising collision in the context of the simulated task and collaborating agents then this should be explained in more details. Otherwise, the claim seems a bit conflicting.
In addition, the way the work by Chitins et al. is introduced makes me think this work is directly related to this problem. 'While this intrinsic reward encourages the two robots to collaborate for tasks that are hard to achieve by a single robot, it does not address the domination and conflict problems for efficient and safe manipulation.' Encouraging robots to collaborate seems directly related to addressing the domination problem (e.g. to not collaborate) so this seems like a direct candidate to compare against. If the methods are not comparable for some reason, then its reasons should be explained clearly.
The concept of robot collaboration describes the act of interacting agents and is thus inherently solving the problem of using a dominant agent, e.g. through coordination [1,2,6,7] for example. There is a lot of work in robotics done on safe collaboration in bimanual settings too [3,4,5] too, so it is not clear why this is a contribution for this work. Moreover, those works were not part of the literature review but perhaps some of them might help put things in perspective.
Furthermore, the summarised contribution on page 2, states that the proposed approach can solve the tasks more efficiently but this is not necessarily true when looking at the evaluation section. Requiring 10M as opposed to 11M environmental steps, for a 100 step long horizons, is hardly more efficient. Perhaps the solution results in faster learning but it does not seem more efficient.
Experimental Evaluation
I think this work does a great job at presenting the overall experimentation. I really like the breadth of evaluating the work but I have a few additional concerns. The main one is the small number of seeds used. To be able to confirm the utility of this method, I think it's important to run experiments with at least 10 seeds. Being a model-free approach, it is well known problem that the learning of a Q function can vary drastically even if the tasks were not manipulation heavy and not bimanual. The problem of interest in this work asks for more thorough evaluation with respect to the number of seeds. I would happily reconsider my recommendation if this gets fixed.
In addition, it seems like the proposed approach works better with a fully sparse reward as opposed to an informative one, is there a reason for this, it seems somewhat counter intuitive.
The overall study of the required finish steps is great but it seems like it is performed on a not-yet converged policies. In this case it is not clear if the reported improvement against the number steps required is really thanks to obtaining a more effective policy, synthesised with the help of the provided regularisation, or is merely a consequence of the not-yet converged policies. This brings me back to the comment made in claims and contribution, perhaps a more accurate statement would be to measure the speed of learning, e.g. as a function of the per-step normalised reward, as opposed to the speed of solving the task. It seems only meaningful to measure the speed of solving a task between two learning processes that have already converged to near-optimal policies. This is not the case according to Figures 3 and 4.
Minor comments
'One possible solution is to design a task-allocation reward function to encourage better coordination.' seems like a statement that needs a reference. References
[1] Lee, Y. et al Learning to coordinate manipulation skills via skill behavior diversification. ICLR 2019
[2] Hu, B. et al. Coordinated compliance control of dual-arm robot astronaut for payload operation. IJARS 2021
[3] Sadeghian, H. et al 2012. Global impedance control of dual-arm manipulation for safe interaction. IFAC Proceedings Volumes, 45(22), pp.767-772.
[4] Vick, A. et al. Safe physical human-robot interaction with industrial dual-arm robots. In 9th International Workshop on Robot Motion and Control (pp. 264-269). IEEE.
[5] Gams, A. et al. Coupling movement primitives: Interaction with the environment and bimanual tasks. 2014 IEEE Transactions on Robotics, 30(4), pp.816-830.
[6] Deng, M. et al, 2017, August. Reinforcement learning of dual-arm cooperation for a mobile manipulator with sequences of dynamical movement primitives. In 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM) (pp. 196-201). IEEE.
[7] Freek Stulp et al. Reinforcement learning with sequences of motion primitives for robust manipulation. IEEE Transactions on robotics, 28(6):1360–1370, 2012.

Review Point: 2014 IEEE Transactions on Robotics, 30(4), pp.816-830. [6] Deng, M. et al, 2017, August. Reinforcement learning of dual-arm cooperation for a mobile manipulator with sequences of dynamical movement primitives. In 2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM) (pp. 196-201). IEEE. [7] Freek Stulp et al. Reinforcement learning with sequences of motion primitives for robust manipulation. IEEE Transactions on robotics, 28(6):1360–1370, 2012.
==================================================

Focused review:

I have identified some weaknesses in the paper: - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned. 
 - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). 
What follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.
- What is "Fraction unanswerable" in Figure 5 and how is it obtained?
- I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25. 

Review Point: - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned.
Review Point: - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). What follows are some minor remarks, questions and comments:
Review Point: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.
Review Point: - What is "Fraction unanswerable" in Figure 5 and how is it obtained?
Review Point: - I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25.
==================================================

Focused review:

Weaknesses: - The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples. - I find this statement in the supplemental section D.4 questionable: "Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states? - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version.  - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model.  Minor comments and questions : - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from "scratch". (Though Figure 3 makes it clear that pretrained embeddings have little impact). - I think the authors risk overclaiming when they write "Existing language GANs... have shown little to no performance improvements over traditional language models", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM). 

Review Point: - The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples.
Review Point: - I find this statement in the supplemental section D.4 questionable: "Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?
Review Point: - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version.
Review Point: - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions :
Review Point: - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from "scratch". (Though Figure 3 makes it clear that pretrained embeddings have little impact).
Review Point: - I think the authors risk overclaiming when they write "Existing language GANs... have shown little to no performance improvements over traditional language models", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).
==================================================

Focused review:

Weakness:
For the previous VideoMAE, this paper argued that it failed to learn multi-frame interactions as the model can leverage visible areas to easily reconstruct the masked token. This part is not well supported due to the following reasons: 1. There is no well-proved connection between the ability of learning multi-frame interaction (or dependencies) and the difficulty of VideoMAE tasks. It would be good to show the evidence that the harder the reconstruction task the better the video understanding model will be received from VideoMAE. 2. Given the best masking ratio in VideoMAE is as high as 90%, there will be a large number of masked patches that cannot leverage the visible information.
Similarly, in Figure 3, harder reconstruction may not equal a better video understanding model.
In the introduction: ‘model with more motion details is critical to distinguish different actions’. This statement is not 100% correct. From the two-stream neural network to video transformers, it has been well explored that the importance of spatial and temporal information in action recognition is case by case, depending on the dataset. In this paper, the kinetics dataset is a spatially heavy dataset while the something-something dataset is a temporally heavy dataset.
In the abstract: this paper claims to track the moving object using optical flow. However, it seems that this paper uses a random tube masking strategy for picking masked tokens (sec 3.2.1). How to make sure the objects will be included in the masks.
The target name of the last block in Table 1 is not well aligned with the equation. Is ( z p , z s )
equivalent to the Traj (ours) ? is z s
equivalent to the HOG? as it consists of the HOG features.
This paper leverages the hand-crafted feature, which is known to be very expensive to compute. It is important to know how much extra cost is introduced.
This paper utilizes the tube masking, which masks the same spatial location over the time. In this case, how to deal with the moving object? as the object in the time step t may disappear in the same location of time step t+L.
When compute the trajectory information, how to deal with the moving background and incorrect optical flow caused by huge movement?

Review Point: 1. There is no well-proved connection between the ability of learning multi-frame interaction (or dependencies) and the difficulty of VideoMAE tasks. It would be good to show the evidence that the harder the reconstruction task the better the video understanding model will be received from VideoMAE.
Review Point: 2. Given the best masking ratio in VideoMAE is as high as 90%, there will be a large number of masked patches that cannot leverage the visible information. Similarly, in Figure 3, harder reconstruction may not equal a better video understanding model. In the introduction: ‘model with more motion details is critical to distinguish different actions’. This statement is not 100% correct. From the two-stream neural network to video transformers, it has been well explored that the importance of spatial and temporal information in action recognition is case by case, depending on the dataset. In this paper, the kinetics dataset is a spatially heavy dataset while the something-something dataset is a temporally heavy dataset. In the abstract: this paper claims to track the moving object using optical flow. However, it seems that this paper uses a random tube masking strategy for picking masked tokens (sec 3.2.1). How to make sure the objects will be included in the masks. The target name of the last block in Table 1 is not well aligned with the equation. Is ( z p , z s ) equivalent to the Traj (ours) ? is z s equivalent to the HOG? as it consists of the HOG features. This paper leverages the hand-crafted feature, which is known to be very expensive to compute. It is important to know how much extra cost is introduced. This paper utilizes the tube masking, which masks the same spatial location over the time. In this case, how to deal with the moving object? as the object in the time step t may disappear in the same location of time step t+L. When compute the trajectory information, how to deal with the moving background and incorrect optical flow caused by huge movement?
==================================================

Focused review:

Weaknesses: 1.It seems that there is no analysis about the influence of initial model (line 1) on the experimental results in this paper. In fact, the influence of initial model exists (assuming that the best model is set at the beginning). It is suggested to add analysis about the influence of initial model. 2.The experiment is too simple. "We train a convolutional neural network LeNet-5 using Fashion MNIST". Such a specific experiment may affect the degree of confidence of the experiment results. 3.This paper proposes two strategies(constant and sequential) for selecting anchor and miner groups. But it would be better if there were a strategy based on the performance of each client. 4.The analysis of g(i)t,k (line 19)is reasonable, but it is worth discussing whether a factor α is needed for second and third items. 5.For the analysis of Figure 1, it seems that only Communication Rounds=400 is considered(why is 400?). Such analysis seems to be one-sided, and there are other considerations in fact, such as the change rate of the training loss.

Review Point: 1.It seems that there is no analysis about the influence of initial model (line 1) on the experimental results in this paper. In fact, the influence of initial model exists (assuming that the best model is set at the beginning). It is suggested to add analysis about the influence of initial model.
Review Point: 2.The experiment is too simple. "We train a convolutional neural network LeNet-5 using Fashion MNIST". Such a specific experiment may affect the degree of confidence of the experiment results.
Review Point: 3.This paper proposes two strategies(constant and sequential) for selecting anchor and miner groups. But it would be better if there were a strategy based on the performance of each client.
Review Point: 4.The analysis of g(i)t,k (line 19)is reasonable, but it is worth discussing whether a factor α is needed for second and third items.
Review Point: 5.For the analysis of Figure 1, it seems that only Communication Rounds=400 is considered(why is 400?). Such analysis seems to be one-sided, and there are other considerations in fact, such as the change rate of the training loss.
==================================================

Focused review:

Weaknesses: The authors only proved the role of entropy in selecting data, but this paper does not elaborate on the motivation and advantages of introducing complex reinforcement learning to train a policy network.
Further Comments: 1. The authors use training set entropy as a reward to train a policy network for data selection. How is it different from directly using entropy and selecting data through threshold? What are their advantages and disadvantages? 2. In the code provided in the supplementary materials, the policy network is first trained on the data set to be selected. Does the policy network need to be retrained on the data set to be selected each time? Considering the generalization of reinforcement learning, this will limit the universality of the algorithm. 3. In Figure 5, the initial reward of SCA is much higher than A2C and VPG, and the trend is different from that of A2C and VPG. What is the reason for these phenomena? Is the value of reward related to the quality of data selection? 4. There are some clerical errors in the paper. For example, in second page, it should be “Pr(“nor”) = 1/2”. It is recommended that the author read the full text to correct such problems.

Review Point: 1. The authors use training set entropy as a reward to train a policy network for data selection. How is it different from directly using entropy and selecting data through threshold? What are their advantages and disadvantages?
Review Point: 2. In the code provided in the supplementary materials, the policy network is first trained on the data set to be selected. Does the policy network need to be retrained on the data set to be selected each time? Considering the generalization of reinforcement learning, this will limit the universality of the algorithm.
Review Point: 3. In Figure 5, the initial reward of SCA is much higher than A2C and VPG, and the trend is different from that of A2C and VPG. What is the reason for these phenomena? Is the value of reward related to the quality of data selection?
Review Point: 4. There are some clerical errors in the paper. For example, in second page, it should be “Pr(“nor”) = 1/2”. It is recommended that the author read the full text to correct such problems.
==================================================

Focused review:

Currently I am giving a score 8, mainly because the idea, motivation and storyline are exciting. But the draft’s Sections 3 & 4 remain unclear in several ways. My final score will depend on how the authors clarify the main questions below: -Section 3 appears to be too “high level” (it shouldn’t be, for the many new things discussed). For example, I was expecting to see how backpropagation was done for the two new layers, but they were unexplained (not even in the supplementary). Also, it is surprising that “fixing shift” as an important extension towards the authors’ claimed “coarse/fine flexibility” only takes five lines in Section 3. A true gem may be overlooked! -Section 4: it is totally unclear what are the dimensions of shift and add layers? For example, when you compare “ShiftAddNet” with ResNet-20, shall I imagine either shift or add layer to have the same dimension as the conv layer, for each layer? Or else? How about DeepShift/AdderNet? Are they fair-comparable to ShiftAddNets in layer/model sizes? - Section 4: The two IoT datasets (FlatCam Face [26], Head-pose detection [11]) are unpopular, weird choices. The former is relatively recent but not substantially followed yet. The latter was published in 2004 and was no longer used much recently. I feel strange why the authors choose the two uncommon datasets, that makes their benchmarking results a bit hard to sense and evaluate. There should have been better options for IoT benchmarking, such as some wearable health or mobile activity recognition data, or even some sets in UCI.

Review Point: - Section 4: The two IoT datasets (FlatCam Face [26], Head-pose detection [11]) are unpopular, weird choices. The former is relatively recent but not substantially followed yet. The latter was published in 2004 and was no longer used much recently. I feel strange why the authors choose the two uncommon datasets, that makes their benchmarking results a bit hard to sense and evaluate. There should have been better options for IoT benchmarking, such as some wearable health or mobile activity recognition data, or even some sets in UCI.
==================================================

Focused review:

1. The notations, equations in the method section are not clear. In Line 110 for instance, the equation $\Upsilon(x)=\{\Upsilon(x)_l\}$ is confusing. 2. The discriminator on the left side of Figure 1 is not the network used by the existing I2I methods (e.g., BicycleGAN concatenates the one-hot vector with the image as the input.) 3. Two highly-related frameworks targeting multi-domain I2I [1,2] are not cited, discussed, and compared in the paper. 4. In the table of Figure 3, it is not clear why training with partial adaptor performs worse than that of training without the adaptor? 5. Since the model is pre-trained from the BigGAN model trained on the natural images, what is the performance of the proposed method on the I2I tasks with unnatural images (e.g., face to artistic portrait)? [1] Lee, et al. "Drit++: Diverse image-to-image translation via disentangled representations.". [2] Choi et al. "StarGAN v2: Diverse image synthesis for multiple domains."

Review Point: 1. The notations, equations in the method section are not clear. In Line 110 for instance, the equation $\Upsilon(x)=\{\Upsilon(x)_l\}$ is confusing.
Review Point: 2. The discriminator on the left side of Figure 1 is not the network used by the existing I2I methods (e.g., BicycleGAN concatenates the one-hot vector with the image as the input.) 3. Two highly-related frameworks targeting multi-domain I2I [1,2] are not cited, discussed, and compared in the paper.
Review Point: 4. In the table of Figure 3, it is not clear why training with partial adaptor performs worse than that of training without the adaptor?
Review Point: 5. Since the model is pre-trained from the BigGAN model trained on the natural images, what is the performance of the proposed method on the I2I tasks with unnatural images (e.g., face to artistic portrait)? [1] Lee, et al. "Drit++: Diverse image-to-image translation via disentangled representations.". [2] Choi et al. "StarGAN v2: Diverse image synthesis for multiple domains."
==================================================

Focused review:

1. It is not fully clear how the top K similarity for each class is selected to help generates more reasonable class centers. Elaborating this mechanism would help the reader. 2. The authors only give the K value selection with a class number of 10 and 100, How to confirm the K when the class number is others? 3. In the Fig. 2, the , and fc need more explanations. 4. On page 6, line 242, MNIST needs citation.

Review Point: 1. It is not fully clear how the top K similarity for each class is selected to help generates more reasonable class centers. Elaborating this mechanism would help the reader.
Review Point: 2. The authors only give the K value selection with a class number of 10 and 100, How to confirm the K when the class number is others?
Review Point: 3. In the Fig. 2, the , and fc need more explanations.
==================================================

Focused review:

Weaknesses:
In table 2, does "GPW for all" mean that all neurons share the same learnable Vth? And does "SGP" mean that each layer has its own Vth? For the experiments shown in Fig. 2, does each layer have its own trainable Vth? For each experiment, the authors need to make clear statements on whether all the neurons share the same Vth, or each layer has its own Vth, or each neuron has its own Vth. I also suggest the authors to combine eqns. 6 and 11 in one place, and clarify the range of the summation in eqn. 11.
I think the catastrophic performance drop in fig. 2a is not hard to understand. Let me first guess that each layer has its own trainable Vth. For one layer, you need to sum up dL/dVth_i for each neuron i in that layer to get dL/dVth. Since dL/dVth_i, dL/du_i, and dL/dWij have the same order, dL/dVth will be hundreds of or thousands of times large than dL/dWij , depending on the model width. Therefore, a learning rate suitable for updating Wij may be too large for updating Vth. So we can easily overcome the issue by using a small learning rate for Vth (e.g., set init lr=0.1 and 0.001 for W and Vth, respectively). Your proposed method basically shrinks dL/dVth_i with a sigmoid function, so it has a similar effect as using smaller lr. Can you compare the two methods experimentally? Can you show the benefit of your proposed voltage-dependent shrinkage scheme?
I am concerned about the direct performance gain of the proposed method. The authors do not show the performance when disabling threshold training and keeping other settings the same. So it is not clear whether the sota results are achieved due to the proposed method or, say, network structures. Table 2 seems to show the direct effect of the proposed method, but the baseline uses a different network structure (the title of table 2 is misleading!). I would appreciate it if the authors could conduct ablation studies. Minor:
Fig.1: DSR-VGG11 adopts learnable thresholds, as repeatedly pointed out in the manuscript.
In eqns. 6 and 9, a minus sign is missed. ds/du and ds/dVth have opposite signs.

Review Point: 6 and 11 in one place, and clarify the range of the summation in eqn.
Review Point: 11. I think the catastrophic performance drop in fig. 2a is not hard to understand. Let me first guess that each layer has its own trainable Vth. For one layer, you need to sum up dL/dVth_i for each neuron i in that layer to get dL/dVth. Since dL/dVth_i, dL/du_i, and dL/dWij have the same order, dL/dVth will be hundreds of or thousands of times large than dL/dWij , depending on the model width. Therefore, a learning rate suitable for updating Wij may be too large for updating Vth. So we can easily overcome the issue by using a small learning rate for Vth (e.g., set init lr=0.1 and 0.001 for W and Vth, respectively). Your proposed method basically shrinks dL/dVth_i with a sigmoid function, so it has a similar effect as using smaller lr. Can you compare the two methods experimentally? Can you show the benefit of your proposed voltage-dependent shrinkage scheme? I am concerned about the direct performance gain of the proposed method. The authors do not show the performance when disabling threshold training and keeping other settings the same. So it is not clear whether the sota results are achieved due to the proposed method or, say, network structures. Table 2 seems to show the direct effect of the proposed method, but the baseline uses a different network structure (the title of table 2 is misleading!). I would appreciate it if the authors could conduct ablation studies. Minor: Fig.1: DSR-VGG11 adopts learnable thresholds, as repeatedly pointed out in the manuscript. In eqns.
Review Point: 6 and 9, a minus sign is missed. ds/du and ds/dVth have opposite signs.
==================================================

Focused review:

Weaknesses:
Comparing the occupational statistics computed by GPT2 vs those by the United States is very interesting and informative. However, the presentation on the methodology and the subsequent discussion is confusing to me. Particularly from section 3.4, I am not sure what “adj.” in equation (1) means and why “adj. Pred” is appropriate as a scaling factor. Would appreciate it if the authors could clarify and make this section clearer.
The analysis of intersection effects is interesting but I fail to see a clear presentation on statistical significance of these results. It may be clearer if the authors could specify p-values on some regressors and offer some discussions. From Table 3, I also do not believe that average pseudo-R2 is necessarily a meaningful measure for the individual factor.
The authors claim the contribution of “benchmarking the extent of bias relative to inherently skewed societal distributions of occupation associations”. However, I have some reservations as 1) the authors did not propose any quantitative measurement to the extent of occupation bias relative to real distributions in society; 2) the authors did not compare any models other than GPT2.
Several sections of the paper read confusing to me. There is a missing citation / reference in Line 99, section 3.1. The notation \hat{D}(c) from Line 165, section 3.4 is unreferenced.
The authors made great effort to acknowledge the limitations of their work.

Review Point: 2) the authors did not compare any models other than GPT2. Several sections of the paper read confusing to me. There is a missing citation / reference in Line 99, section 3.1. The notation \hat{D}(c) from Line 165, section 3.4 is unreferenced. The authors made great effort to acknowledge the limitations of their work.
==================================================

Focused review:

Weakness:
1 Lack of novelty. The proposed method that uses paired data with shared concepts to help constrain disentangled representation learning is a common method in various papers:
[1] Zero-shot Synthesis with Group-Supervised Learning, which proposes group-supervised learning to formalize paired data as graphs and mine the similarity between paired images and boost disentangled representation learning. This paper also proposes multiple applications such as data augmentation.
[2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning.
[3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes
[4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting
[5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer
Constraining of disentanglement can be conducted either in latent space or image domain with paired images sharing the same attributes.
The author should compare the proposed method with these similar methods and claim the advantage.
2 The claim of weekly supervised learning is not promising. The training still needs attribute labels of data, otherwise, the paired images can not build. This is full supervision instead of weekly supervision
3 Evaluation dataset is too simple, synthetic controllable dataset. The real-world and more complex dataset should be conducted. E.g., ilab-20M [6] dataset, where each image has content, pose, and background attributes. RaFD [7], a face dataset with controllable
4 More baselines: the method proposed in [1] GZS-Net and [2] can be used to compare.
Reference [1] Zero-shot Synthesis with Group-Supervised Learning
[2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning.
[3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes
[4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting
[5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer
[6] iLab-20M: A large-scale controlled object dataset to investigate deep learning
[7] Radboud Faces Database

Review Point: 1 Lack of novelty. The proposed method that uses paired data with shared concepts to help constrain disentangled representation learning is a common method in various papers: [1] Zero-shot Synthesis with Group-Supervised Learning, which proposes group-supervised learning to formalize paired data as graphs and mine the similarity between paired images and boost disentangled representation learning. This paper also proposes multiple applications such as data augmentation. [2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning. [3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes [4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting [5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer Constraining of disentanglement can be conducted either in latent space or image domain with paired images sharing the same attributes. The author should compare the proposed method with these similar methods and claim the advantage.
Review Point: 2 The claim of weekly supervised learning is not promising. The training still needs attribute labels of data, otherwise, the paired images can not build. This is full supervision instead of weekly supervision 3 Evaluation dataset is too simple, synthetic controllable dataset. The real-world and more complex dataset should be conducted. E.g., ilab-20M [6] dataset, where each image has content, pose, and background attributes. RaFD [7], a face dataset with controllable 4 More baselines: the method proposed in [1] GZS-Net and [2] can be used to compare. Reference [1] Zero-shot Synthesis with Group-Supervised Learning [2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning. [3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes [4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting [5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer [6] iLab-20M: A large-scale controlled object dataset to investigate deep learning [7] Radboud Faces Database
==================================================

Focused review:

1. It is good to see a study of the causes of hidden stratification problem before formally proposed GEORGE. According to the paper, there are two main causes, i.e. inherent hardness and data imbalance. However, it is unclear how the proposed GEORGE address these two causes. Any discussion about this would further improve this paper. 2. The computation cost of GEORGE might be high since it is a two-step framework. Any results to show the computation cost?

Review Point: 1. It is good to see a study of the causes of hidden stratification problem before formally proposed GEORGE. According to the paper, there are two main causes, i.e. inherent hardness and data imbalance. However, it is unclear how the proposed GEORGE address these two causes. Any discussion about this would further improve this paper.
Review Point: 2. The computation cost of GEORGE might be high since it is a two-step framework. Any results to show the computation cost?
==================================================

Focused review:

Weaknesses
Originality: Main Result 1 relies on known formulas for low-rank matrix factorization. It is not clearly explained what are the major technical challenges, if any, in obtaining this result.
Clarity: The community labels in (3) and the model (4) are such that the E X
does not have sparse columns if k
is small. For this reason, I feel the paper is more about the large k
version of the sparse clustering problem.
Edit 08/19: After discussion with authors, the previous point is resolved.
Clarity: From the main text alone, it is unclear how the information-theoretic threshold is obtained. The formula of the MSE is difficult to interpret, so I can't see if the threshold is a consequence of this. Some further explanation is needed here.
Quality/Clarity: It is difficult to assess the rigour of both main results, especially Main Result 2. This is because the appendix is not organized in a conventional way with a clearly demarcated proof of Main Results 1 and 2. For Main Result 2, I do not see in the Appendix any explanation of how the asymptotic algorithmic MSE is computed. I only see plots rather than arguments. I also do not see any derivation of the Bayes-optimal MSE or reference to known (rigorous) formulas.
Minor issues
Line 39: Equation (2) defines vectors that are (i) standard Gaussian with probability ρ
OR (ii) the zero vector with probability 1 − ρ
. I think what is meant is for there to be a random subset of zero entries, with the rest of the entries being Gaussian.
Main Theorem 1: Please take a careful proofread over this. Here v ∗ , u ∗ , and w
have not been defined in (10). Also Z u
does not appear in (10).
Consider making the boundaries bolder in Figure 1. Also I found the color of λ i t
to be hard on the eyes
Line 70: Typo "statitiscal"
Line 85: Typo "analyis"
Line 91: Typo "Statistics", change to "statistics"
Line 111: Change to "In particular, [10] conjectured and [11] proved..."`
Line 143-144: This comment is hard to understand because (38) is in the Appendix, and then I'm having trouble seeing the connection to (6).
Line 195: Typo "Invextigate"
Line 261: Replace "Despite of this fact" with "Despite this fact"
Summary of score
My score is due to concerns mostly about the rigor and partially about the novelty of this submission. I also feel there is a lack of clarity in explaining how the main results are obtained.
Update of score 08/19
The authors' rebuttal addressed my concerns about rigor and somewhat about novelty. I agree with other reviewers that the strengths and technical challenges of this paper are not highlighted enough in the main text. I also think further clarity is needed on the level of rigor and the asymptotic regime to which the results apply (which seems to be for k growing large and rho going to 0). I have raised my overall score from 3 to 4 because further serious revision is needed. I have also upgraded the soundness from 1 to 2.
1. I agree with the authors' assessment that there are no apparent potential negative societal impacts.
2. The weak recovery problem studied here is primarily of theoretical interest, and it is not clear if the AMP algorithm is useful for non-Gaussian problems. So practical impact may be limited.

Review Point: 1. I agree with the authors' assessment that there are no apparent potential negative societal impacts.
Review Point: 2. The weak recovery problem studied here is primarily of theoretical interest, and it is not clear if the AMP algorithm is useful for non-Gaussian problems. So practical impact may be limited.
==================================================

Focused review:

Weaknesses:
The paper "Few Sample Knowledge Distillation for Efficient Network Compression" [1] is highly related, and can be argued to be very similar to the inner knowledge distillation part. They both essentially try to match transformed students' features with teachers' features, if I'm understanding it right. This hurts the novelty and the quality of related work of the paper. I suggest including a short discussion of this.
There are two places that refer to other works that might be a bit exaggerated, despite I acknowledge these are advantages of the method: 1. Section 6. "even though most of the other methods fine-tune for more than 100 epochs". I think there are also many works that do not fine-tune for more than 100 epochs. 2. In related work, "To date, most pruning methods are restricted to sequential connections as non-sequential connections." I think there are a number of approaches that are capable of pruning ResNets, not in such a restricted way the text later described. At both places, I suggest the authors include references, including both positive and negative ones.
The pipeline of the whole implementation seems not very straightforward. It would be great if the code can be released.
Overall I still think Knapsack formulation is a valid development of pruning method, but the inner distillation part could be better acknowledged as using/borrowing from a previous method [1] in this context.
[1] Few Sample Knowledge Distillation for Efficient Network Compression. Li et al. CVPR 2020

Review Point: 1. Section 6. "even though most of the other methods fine-tune for more than 100 epochs". I think there are also many works that do not fine-tune for more than 100 epochs.
Review Point: 2. In related work, "To date, most pruning methods are restricted to sequential connections as non-sequential connections." I think there are a number of approaches that are capable of pruning ResNets, not in such a restricted way the text later described. At both places, I suggest the authors include references, including both positive and negative ones. The pipeline of the whole implementation seems not very straightforward. It would be great if the code can be released. Overall I still think Knapsack formulation is a valid development of pruning method, but the inner distillation part could be better acknowledged as using/borrowing from a previous method [1] in this context. [1] Few Sample Knowledge Distillation for Efficient Network Compression. Li et al. CVPR 2020
==================================================

Focused review:

Weaknesses: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper

Review Point: - The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool.
Review Point: - The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.
Review Point: - EINOPS does not consider operations involving two or more tensors - A comparison in terms of computation cost is missing in the paper
==================================================

Focused review:

- The claim "In this paper, we are thus interested in developing methods to cluster video datasets without manual supervision, substantially reducing the cost of labelling video data." is a bit misleading, given that the method is evaluated on existing datasets by ignoring the labels. Due to this, the method is trained on some human annotated data as the videos in these datasets (e.g., Kinetics) have had significant annotation in the selection of the temporal intervals and contents of the videos. They already belong to a set of actions. Instead, to support this claim, unlabeled and unannotated videos should be used. As the additional cost of labeling an action is quite small compared to the temporal selection of an action in a clip. - Some of the ideas seem quite similar to those introduced in Evolving Losses for Unsupervised Video Representation Learning [58]. It would be good to better clarify the conceptual differences between these works. Specifically, "As part of this, we wish to account for the fact that semantic classes are not all equally probable, and tend instead to follow a Zipf distribution [1, 39]. We then evaluate the quality of the discovered labels by matching them to the ones provided by human annotators, using datasets where ground-truth labels are known" seems very similar to the ideas presented in [58]. - It would also be good to compare against [58] in Table A.5

Review Point: - The claim "In this paper, we are thus interested in developing methods to cluster video datasets without manual supervision, substantially reducing the cost of labelling video data." is a bit misleading, given that the method is evaluated on existing datasets by ignoring the labels. Due to this, the method is trained on some human annotated data as the videos in these datasets (e.g., Kinetics) have had significant annotation in the selection of the temporal intervals and contents of the videos. They already belong to a set of actions. Instead, to support this claim, unlabeled and unannotated videos should be used. As the additional cost of labeling an action is quite small compared to the temporal selection of an action in a clip.
Review Point: - Some of the ideas seem quite similar to those introduced in Evolving Losses for Unsupervised Video Representation Learning [58]. It would be good to better clarify the conceptual differences between these works. Specifically, "As part of this, we wish to account for the fact that semantic classes are not all equally probable, and tend instead to follow a Zipf distribution [1, 39]. We then evaluate the quality of the discovered labels by matching them to the ones provided by human annotators, using datasets where ground-truth labels are known" seems very similar to the ideas presented in [58].
Review Point: - It would also be good to compare against [58] in Table A.5
==================================================

Focused review:

- This is a purely algorithm paper. The result is on a kind of sensitive analysis of the algorithm. Although this result may be useful when it is combined with machine learning, it is more suitable to be presented in an algorithm conference. - The definition of the prediction error is artificial by a technical reason. I know no machine algorithm which gives a bound on this kind of prediction error.

Review Point: - This is a purely algorithm paper. The result is on a kind of sensitive analysis of the algorithm. Although this result may be useful when it is combined with machine learning, it is more suitable to be presented in an algorithm conference.
Review Point: - The definition of the prediction error is artificial by a technical reason. I know no machine algorithm which gives a bound on this kind of prediction error.
==================================================

Focused review:

Some weaknesses/comments/questions of the proposed method in my opinion are as follows: 1) The improvement due to rotation seems small (ref. table 4) and the most of the improvement seems to be due to the gradient approximation. Please clarify. 2) How many additional learnable parameters are introduced while learning rotation matrices? Are they binary or floating point? Please provide a summary of new learnable parameters and actual (theoretical) improvement in memory and FLOPS. This is important to see as there are some parameters in the network is kept in floating point and additional parameters are introduced compared to the baselines. 3) What is the motivation for the specific form of gradient approximation (sec. 3.4) used in this paper? Why not simply use an existing function such as tanh and anneal using a temperature parameter to approximate the step function similar to [a]? 4) Is there any reason for explicitly learning the rotation matrices rather than penalizing the angle between floating-point and the binary vectors while learning? I believe, this approach will not have any additional learnable parameters and encourage the float vectors to align with the binary vectors. Please comment. 5) Is there any loss due to the introduction of the bi-rotation formulation? 6) What is the justification for Eq. 11? 7) Any overhead at test time? [a] Ajanthan, T., Gupta, K., Torr, P.H., Hartley, R. and Dokania, P.K., 2019. Mirror descent view for neural network quantization. arXiv preprint arXiv:1910.08237. Post rebuttal update: The rebuttal clarifies most of my initial concerns and authors have promised to add discussions in certain parts. The main one, improvement due to rotation on xnor-net (without gradient approximation) is interesting and I recommend including that in the paper. I increase the rating.

Review Point: 1) The improvement due to rotation seems small (ref. table 4) and the most of the improvement seems to be due to the gradient approximation. Please clarify.
Review Point: 2) How many additional learnable parameters are introduced while learning rotation matrices? Are they binary or floating point? Please provide a summary of new learnable parameters and actual (theoretical) improvement in memory and FLOPS. This is important to see as there are some parameters in the network is kept in floating point and additional parameters are introduced compared to the baselines.
Review Point: 3) What is the motivation for the specific form of gradient approximation (sec. 3.4) used in this paper? Why not simply use an existing function such as tanh and anneal using a temperature parameter to approximate the step function similar to [a]?
Review Point: 4) Is there any reason for explicitly learning the rotation matrices rather than penalizing the angle between floating-point and the binary vectors while learning? I believe, this approach will not have any additional learnable parameters and encourage the float vectors to align with the binary vectors. Please comment.
Review Point: 5) Is there any loss due to the introduction of the bi-rotation formulation?
Review Point: 7) Any overhead at test time? [a] Ajanthan, T., Gupta, K., Torr, P.H., Hartley, R. and Dokania, P.K., 2019. Mirror descent view for neural network quantization. arXiv preprint arXiv:1910.08237. Post rebuttal update: The rebuttal clarifies most of my initial concerns and authors have promised to add discussions in certain parts. The main one, improvement due to rotation on xnor-net (without gradient approximation) is interesting and I recommend including that in the paper. I increase the rating.
==================================================

Focused review:

Weaknesses: 1. It appears to be an incremental derivation from existing distributionally robust logistic regression (LR) with continuous features. 2. In terms of experimental results, the proposed formulation does not appear much better than the classical LR in terms of accuracy. Actually, it is not clear there is any significant advantages when compared to LR (Please see next comment). Without significant improvement, the new formulation is more like a conceptual exercise.
The comparative evaluation in lines 282-287 is not so convincing. Currently, only the number of best performance is counted;
“The table shows that for the unregularized model, both of our distributionally robust logistic regression achieve the lowest classification error in 78% of the instances, whereas the classical logistic regression achieves the lowest classification error in 61% of the instances. For the regularized model, the results remain at 78% (our model) vs. 61% (classical logistic regression). Overall, the non-robust models failed to achieve the lowest classification error in three of the instances, whereas the robust 287 models missed the lowest classification error in only one of the instances.”
This is analogous to ‘bean counting”. However, the variance is not considered. It would be desirable to show variance information. Maybe some statistical test can be used for judge if there is any significant difference between the LR with the DROs, or between the regularized r-LR with the regularized r-DROs.

Review Point: 1. It appears to be an incremental derivation from existing distributionally robust logistic regression (LR) with continuous features.
Review Point: 2. In terms of experimental results, the proposed formulation does not appear much better than the classical LR in terms of accuracy. Actually, it is not clear there is any significant advantages when compared to LR (Please see next comment). Without significant improvement, the new formulation is more like a conceptual exercise. The comparative evaluation in lines 282-287 is not so convincing. Currently, only the number of best performance is counted; “The table shows that for the unregularized model, both of our distributionally robust logistic regression achieve the lowest classification error in 78% of the instances, whereas the classical logistic regression achieves the lowest classification error in 61% of the instances. For the regularized model, the results remain at 78% (our model) vs. 61% (classical logistic regression). Overall, the non-robust models failed to achieve the lowest classification error in three of the instances, whereas the robust 287 models missed the lowest classification error in only one of the instances.” This is analogous to ‘bean counting”. However, the variance is not considered. It would be desirable to show variance information. Maybe some statistical test can be used for judge if there is any significant difference between the LR with the DROs, or between the regularized r-LR with the regularized r-DROs.
==================================================

Focused review:

Weaknesses: 1. In Theorem 2 and Corollary 2.1, there is an assumption on D t − 1 1 and D t 1
, but D t
is a variable in the iterative update. Similarly, Theorem 3 has the assumption D t − 1 , j η t − 1 ≤ D t , j η t
. How strong are these assumptions? Can they be satisfied during the iterations? 2. One of the key points in this work is that the parameter-wise update of B t
trades off between secant condition and weak secant condition. Intuitively, one would expect that the number of decoupled parameters L
has influence on the performance both theoretically and numerically. However, L
does not appear in the convergence analysis or the experiments. 3. In the experiments, it is claimed that the proposed method outperforms the compared first-order methods on convergence speed. However, it is only true in terms of epochs, but not true in terms of runtime. It is quite common that a quasi-second-order method converges faster in terms of iterations, but the real question is how efficient it is in runtime. 4. It is suggested that the notation for the Lipchitz constant in Theorem 3 should be changed, since L already denotes the number of decoupled parameters.

Review Point: 1. In Theorem 2 and Corollary 2.1, there is an assumption on D t − 1 1 and D t 1 , but D t is a variable in the iterative update. Similarly, Theorem 3 has the assumption D t − 1 , j η t − 1 ≤ D t , j η t . How strong are these assumptions? Can they be satisfied during the iterations?
Review Point: 2. One of the key points in this work is that the parameter-wise update of B t trades off between secant condition and weak secant condition. Intuitively, one would expect that the number of decoupled parameters L has influence on the performance both theoretically and numerically. However, L does not appear in the convergence analysis or the experiments.
Review Point: 3. In the experiments, it is claimed that the proposed method outperforms the compared first-order methods on convergence speed. However, it is only true in terms of epochs, but not true in terms of runtime. It is quite common that a quasi-second-order method converges faster in terms of iterations, but the real question is how efficient it is in runtime.
Review Point: 4. It is suggested that the notation for the Lipchitz constant in Theorem 3 should be changed, since L already denotes the number of decoupled parameters.
==================================================

Focused review:

Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).
Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.
[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. âModeling Relationships in Referential Expressions with Compositional Modular Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. âModeling Context Between Objects for Referring Expression Understanding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. âSubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.â In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378â86. Curran Associates, Inc.

Review Point: 1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
Review Point: 2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22] 3. It is unclear what SPAT means in Table.
Review Point: 2.4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts? 5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig.
Review Point: 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*) 6.
==================================================

Focused review:

My concerns about this paper concentrate on two main points. First, the main class of losses that the paper introduces, that of relative Lipschitz continuity (Def. 2.1) seems very closely related to that of Riemann Lipschitz Continuity (RLC) in Antonakopoulos et. al (2020). In particular, given that the losses are (RLC) then one can recover relative Lipschitz continuity via a direct combination of convexity and Cauchy-Schwartz inequality. Moreover, conversely every relative Lipschitz continuous loss can be seen as (RLC) if one chooses the respective Riemannian metric accordingly; this becomes even more evident for the example that the paper presents, if f(x)=x^{2} for x\in R, then one can straightforwardly choose the Riemannian metric in such a manner that the respective dual norm would be \|v\|_{x,\ast}=|v|/x and (RLC) follows. That said, this weakens significantly the contributions concerning FTRL and the like, since in Antonakopoulos et. al (2019) these results are already established. On the other hand, concerning the most intriguing part that of establishing logarithmic regret for the case where the loss functions are in addition relatively strongly convex, there is no obvious way to establish any relevant examples that satisfy simultaneously relative Lipschitz continuity and relative strong convexity, besides of course the euclidean ones. More precisely, assume that we have the static standard convex minimization problem and x^* is a solution. Then, relative strong convexity (Def 2.2) implies that for some constant M>0: <\nabla f(x),x-x^*>\geq M D(x^\ast,x) whereas due to Lu's (2019) relative continuity, we have for some L>0: <\nabla f(x),x-x^*>\leq L\sqrt{2D(x^*,x)} Hence, the above inequalities demand that the Bregman "distance" between a solution and the base point x remains bounded, which fails to be true for various practical examples like Poisson Inverse Problems and/or Support Vector Machines due to the singular behaviour of the regularizer (or its gradient) near the boundary of the feasible domain. That said, in order to justify the significance of the logarithmic regret result, the authors have to provide relevant examples of interest for this particular class of loss functions. In conclusion, in the blanket assumption part 2.1, the losses domain X is assumed to be closed. This assumption, despite the fact that it is typical for the Euclidean framework, a priori excludes various interesting problems (e.g. f(x)= -logx, x>0, the KL-divergence etc). Post rebuttal: The authors provided an example of a relatively continuous/relatively strongly convex function; therefore I increased my score. However, I think that the paper needs a revision according the following points: 1.Clarify the connection with existing work, in particular the paper of Antonakopoulos et.al. 2020. 2. Highlight in a more clear way the significance of logarithmic regret obtained for relatively continuous/relative strongly convex functions by providing a more extensive presentation and applications of the said class of fucntions.

Review Point: 1.Clarify the connection with existing work, in particular the paper of Antonakopoulos et.al. 2020.
Review Point: 2. Highlight in a more clear way the significance of logarithmic regret obtained for relatively continuous/relative strongly convex functions by providing a more extensive presentation and applications of the said class of fucntions.
==================================================

Focused review:

- As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading. - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.
- It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this. - In general, there are quite a few things missing -- details provided in comments section. 
**Typos:** - Background section: "...high fidelity thank to…" -> "...high fidelity thanks to…" - Background section: " … Fang et al., 2019).Many…" -> " … Fang et al., 2019). Many…" - Figure-1: "...which integrated to into…" -> "...which integrated into…" **Comments:** - Author did not mention how the initial durations of phonemes are obtained.
- Are durations of phonemes predicted in frames or seconds?
- Figure-1 did not mention how the proposed CUC-VAE TTS system works in the inference time. Moreover, it is hard to understand the color schema followed in the Figure-1, there is no legend.
- There is no mentioning of train, valid and test set splits in the dataset section.
- In Table-2 the baseline system received a better MOS score than the baseline + fine-grained VAE and baseline + CVAE, why is it? Whereas in Table-4 the baseline system show high MCD and FFE error than the baseline + fine-grained VAE and baseline + CVAE systems, why is it?
- How do you represent the reference mel-spectrogram at phoneme level?
- Did you use pre-trained HiFi-GAN to synthesize speech from the predicted mel-spectrograms? 

Review Point: - As a reviewer, I found the paper slightly difficult to read -- some long sentences can be rewritten to improve the clarity of the paper reading.
Review Point: - The subjective results are derived on a small set of utterances (11 audios) using a small number of listeners (23 subjects), this may not be substantial enough for statistical significance of the results published in the paper.
Review Point: - It is not clear why CUC-VAE TTS system with L=1 performed worse than baseline system -- an appropriate reason or further analysis may be required to validate this.
==================================================

Focused review:

Weakness:
1.The idea that uses replay buffer is not quite novel. It is common for rl methods trained with replay buffer. I think it is novel to use the learned critic in previous rounds, can this technique helps for single RL algorithms?
2.The performance curve of each algorithm during the training is missing.
3.For the experimental details, how is the preference vector decided in practice?
4.The critic function is related with the preference vector, can does the model generalize well on this variable?
5.How the performance change when the number of objectives increases?

Review Point: 1.The idea that uses replay buffer is not quite novel. It is common for rl methods trained with replay buffer. I think it is novel to use the learned critic in previous rounds, can this technique helps for single RL algorithms?
Review Point: 2.The performance curve of each algorithm during the training is missing.
Review Point: 3.For the experimental details, how is the preference vector decided in practice?
Review Point: 4.The critic function is related with the preference vector, can does the model generalize well on this variable?
Review Point: 5.How the performance change when the number of objectives increases?
==================================================

Focused review:

Weaknesses]: 1. One of the major concerns is the similarity of this paper to a prior work REDQ. The similarity is of various aspects, including the algorithmic tables, sentences, results, analyses and even the format! On top of that, the main idea is actually very much similar, and as far as I can tell the only interesting difference seems to be the replacement of Q value from the min value of a randomly sampled set in REDQ to the average of the min K values in AQE. The authors also don’t give a convincing explanation why this replacement can improve the performance, either theoretically or intuitively. 2. The most similar approach REDQ which the authors cited multiple times is not tested in Figure 4 of Appendix B. Compared with the baselines in that figure (SAC-5, TQC-5), REDP sounds much better in terms of reducing the Q estimation error. I am quite curious how REDQ on the bias. 3. It’s kind of strange that a model with low bias and std provides poor performance sometimes. For example, in figure 3, the orange line (K=10) has lower bias and std than the red line (K=16) but provides worse performance. The same phenomenon appears in Figure 5 (g) (between the blue line and red line) and Figure 5 (j) (orange line and red line), etc. The authors don’t provide an explanation. 4. Have the authors tried to adjust K dynamically, such as using a small K in the early stage to avoid overestimating and use large K in the late stage to reduce the estimation variance?
[typos] Not sure whether or not the authors intended to say “AQE improves the sample-efficiency performance of TQC and the asymptotic performance of REDP” in the abstract? Because TQC provides SOTA asymptotic performance while REDP achieves high sample efficiency...

Review Point: 1. One of the major concerns is the similarity of this paper to a prior work REDQ. The similarity is of various aspects, including the algorithmic tables, sentences, results, analyses and even the format! On top of that, the main idea is actually very much similar, and as far as I can tell the only interesting difference seems to be the replacement of Q value from the min value of a randomly sampled set in REDQ to the average of the min K values in AQE. The authors also don’t give a convincing explanation why this replacement can improve the performance, either theoretically or intuitively.
Review Point: 2. The most similar approach REDQ which the authors cited multiple times is not tested in Figure 4 of Appendix B. Compared with the baselines in that figure (SAC-5, TQC-5), REDP sounds much better in terms of reducing the Q estimation error. I am quite curious how REDQ on the bias.
Review Point: 3. It’s kind of strange that a model with low bias and std provides poor performance sometimes. For example, in figure 3, the orange line (K=10) has lower bias and std than the red line (K=16) but provides worse performance. The same phenomenon appears in Figure 5 (g) (between the blue line and red line) and Figure 5 (j) (orange line and red line), etc. The authors don’t provide an explanation.
==================================================

Focused review:

weakness when using GA-MLPs for solving graph problems as compared to GNNs. Along these line the paper the main results can be characterized as follows: 1) The paper identifies a specific instance of identifying non-isomorphic graphs that can be solved via a GNN but not by a GA-MLP framework. 2) The paper provides an empirical and experimental evaluation of the representation power of GNNs versus Graph-Augmented MLPs, and show a separation in expressive power between the two in terms of node level functions on rooted graphs. Specifically, they show that the set of functions that can be represented by a GNN of a certain depth) grows doubly exponentially in k, as opposed to only exponential growth of the function class when considering a similar GA-MLP architecture. They also empirically evaluate the difference in performance of the two models on community detection and counting walk problems.
To obtain the result in 1, the paper uses the recent equivalence between the computation in a depth-k GNN and the WL graph isomorphism test. The authors use this to construct two non-isomorphic graphs that the WL test can distinguish using 2 iterations, but on which a GA-MLP will produce the same augmented embeddings. To obtain the result in 2, the authors count the number of distinct rooted trees that can be produced during the computations of a GNN and GA-MLP, and show a gap between the case of a GNN and a GA-MLP.
My main main concern with the paper is that it does not provide sufficiently rigorous findings on either the theoretical or the experimental front. On the theoretical side, the paper establishes a somewhat expected performance gap between using a full GNN and an approximation such as a GA-MLP. Also it seems that the lower bounds for GA-MLPs only hold for the variant where the linear transformations are of the form A,A^2, and so on. Perhaps the authors can clarify this point? Further there are no sample complexity or generalization bounds provided. On the experimental front, it would have been much nicer to understand the tradeoffs in performance versus scalability or provide guidance on which models are more suitable for common, real-world GNN problems and applications. Counting attributed walks and community detection are not very representative problems for GNNs.

Review Point: 1) The paper identifies a specific instance of identifying non-isomorphic graphs that can be solved via a GNN but not by a GA-MLP framework.
Review Point: 2) The paper provides an empirical and experimental evaluation of the representation power of GNNs versus Graph-Augmented MLPs, and show a separation in expressive power between the two in terms of node level functions on rooted graphs. Specifically, they show that the set of functions that can be represented by a GNN of a certain depth) grows doubly exponentially in k, as opposed to only exponential growth of the function class when considering a similar GA-MLP architecture. They also empirically evaluate the difference in performance of the two models on community detection and counting walk problems. To obtain the result in 1, the paper uses the recent equivalence between the computation in a depth-k GNN and the WL graph isomorphism test. The authors use this to construct two non-isomorphic graphs that the WL test can distinguish using 2 iterations, but on which a GA-MLP will produce the same augmented embeddings. To obtain the result in 2, the authors count the number of distinct rooted trees that can be produced during the computations of a GNN and GA-MLP, and show a gap between the case of a GNN and a GA-MLP. My main main concern with the paper is that it does not provide sufficiently rigorous findings on either the theoretical or the experimental front. On the theoretical side, the paper establishes a somewhat expected performance gap between using a full GNN and an approximation such as a GA-MLP. Also it seems that the lower bounds for GA-MLPs only hold for the variant where the linear transformations are of the form A,A^2, and so on. Perhaps the authors can clarify this point? Further there are no sample complexity or generalization bounds provided. On the experimental front, it would have been much nicer to understand the tradeoffs in performance versus scalability or provide guidance on which models are more suitable for common, real-world GNN problems and applications. Counting attributed walks and community detection are not very representative problems for GNNs.
==================================================

Focused review:

1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
2. Replicability of method is not clear, there's no indication that the code will be released. 
- I might have missed the specific in the paper; hopefully, I get CKMT = "Compact-network K-nearest-neighbor MT" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = "Pruned CKMT", hope I get that right too. 

Review Point: 1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
Review Point: 2. Replicability of method is not clear, there's no indication that the code will be released.
Review Point: - I might have missed the specific in the paper; hopefully, I get CKMT = "Compact-network K-nearest-neighbor MT" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = "Pruned CKMT", hope I get that right too.
==================================================

Focused review:

I hope to see the following updates and discussions in the rebuttal, and I am happy to increase my review rating once my concerns are addressed. 1. In the ICLR publication "Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness", it was shown that Fine-Tuning is not the most effective approach to recover backdoor models, unless one has a sufficient amount of clean data to alleviate the backdoor effect. I would like to see how well the proposed attack is against the mode connectivity based defense proposed in the ICLR paper. 2. When reporting the attack/clean accuracy, it was unclear whether the authors were using data samples (and how many) from training data or testing data (unseen when training the backdoor model). I also would like the authors to address the robustness of the proposed approach when performing backdoor attack on testing (unseen) data. Intuitively, universal trigger pattern should be more robust to distribution shifts between training and testing data, and the input-aware apporach can be more sensitive (and cause attack to fail) if the generator cannot overcome the inherent distribution shift. Also, using testing (untrained) data makes more sense in all experiments. I hope the authors can clarity the data setting. 3.The probabilistic backdooring during model training according to Eq. (2) is worthy of more exploration. The authors only used rho_a=rho_c=0.1 in the experiments. I hope to see some parameter sensitivity analysis on these two parameters.

Review Point: 1. In the ICLR publication "Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness", it was shown that Fine-Tuning is not the most effective approach to recover backdoor models, unless one has a sufficient amount of clean data to alleviate the backdoor effect. I would like to see how well the proposed attack is against the mode connectivity based defense proposed in the ICLR paper.
Review Point: 2. When reporting the attack/clean accuracy, it was unclear whether the authors were using data samples (and how many) from training data or testing data (unseen when training the backdoor model). I also would like the authors to address the robustness of the proposed approach when performing backdoor attack on testing (unseen) data. Intuitively, universal trigger pattern should be more robust to distribution shifts between training and testing data, and the input-aware apporach can be more sensitive (and cause attack to fail) if the generator cannot overcome the inherent distribution shift. Also, using testing (untrained) data makes more sense in all experiments. I hope the authors can clarity the data setting.
Review Point: 3.The probabilistic backdooring during model training according to Eq. (2) is worthy of more exploration. The authors only used rho_a=rho_c=0.1 in the experiments. I hope to see some parameter sensitivity analysis on these two parameters.
==================================================

Focused review:

Weaknesses:
1. Could authors please analyze and comment on how complicated relations can be handled by RNs. Is it the case that RNs perform well for single hop relations such as "what is the color of the object closest to the blue object" which requires reasoning about only one hop relation (distance between blue object and all other objects), but not so well for multiple hop relations such as "What shape is the small object that is in front of the yellow matte thing and behind the gray sphere?". From the failure cases in table 1 of supplementary material, it seems that the model has difficulty in answering questions involving multiple hops of relations.
2. L203-204, it is not clear to me what do authors mean by "we tagged ... support set". Is this referring to some form of human annotation? If so, could authors please elaborate on what happens at test time?
3. All the datasets experimented with in the paper are synthetic datasets. Could authors please comment on how they expect the RNs to work on real datasets such as the VQA dataset from Antol et al.?
Post-rebuttal comments:
Authors have provided satisfactory response to my question about multi-hop reasoning. However, I would still like to see experiments on real VQA dataset to see how effective RNs are at dealing with the amount of variation real datapoints show (in vision as well as in language). So it would be great if authors could include results on the VQA dataset (Antol et al., ICCV 2015) in camera-ready.

Review Point: 1. Could authors please analyze and comment on how complicated relations can be handled by RNs. Is it the case that RNs perform well for single hop relations such as "what is the color of the object closest to the blue object" which requires reasoning about only one hop relation (distance between blue object and all other objects), but not so well for multiple hop relations such as "What shape is the small object that is in front of the yellow matte thing and behind the gray sphere?". From the failure cases in table 1 of supplementary material, it seems that the model has difficulty in answering questions involving multiple hops of relations.
Review Point: 2. L203-204, it is not clear to me what do authors mean by "we tagged ... support set". Is this referring to some form of human annotation? If so, could authors please elaborate on what happens at test time?
Review Point: 3. All the datasets experimented with in the paper are synthetic datasets. Could authors please comment on how they expect the RNs to work on real datasets such as the VQA dataset from Antol et al.? Post-rebuttal comments: Authors have provided satisfactory response to my question about multi-hop reasoning. However, I would still like to see experiments on real VQA dataset to see how effective RNs are at dealing with the amount of variation real datapoints show (in vision as well as in language). So it would be great if authors could include results on the VQA dataset (Antol et al., ICCV 2015) in camera-ready.
==================================================

Focused review:

1. High level comment: Since the proposed solutions are both based on a counterfactual as well as interventional query, it seems important to justify which framing is more appropriate for the recourse task. On the face of it, recourse is a counterfactual query rather than an interventional one. The justification and motivation for each potential formulation is unclear and it would be good if the authors incorporate that in their motivation. 2. Authors do not address the issue of feasibility in the level of detail as is warranted for recourse. This makes the formulation a little impractical for actual practice. Can the authors clarify the details of feasibility or enumeration of feasible actions? All comments in the paper allude to searching over potential intervention sets \mathcal{I}. However, there are causal dependencies in the graph which can determine allowable feasible sets. It is unclear how to address this challenge here. 4. Is the learned CVAE completely respect the causal graph? The procedure of training CVAEs for interventional recourse should be clarified in further detail. 5. In experimental evaluation, I did not see a comparison to existing baselines in recourse, neither a qualitative assessment of the type of recourses that the model learns.

Review Point: 1. High level comment: Since the proposed solutions are both based on a counterfactual as well as interventional query, it seems important to justify which framing is more appropriate for the recourse task. On the face of it, recourse is a counterfactual query rather than an interventional one. The justification and motivation for each potential formulation is unclear and it would be good if the authors incorporate that in their motivation.
Review Point: 2. Authors do not address the issue of feasibility in the level of detail as is warranted for recourse. This makes the formulation a little impractical for actual practice. Can the authors clarify the details of feasibility or enumeration of feasible actions? All comments in the paper allude to searching over potential intervention sets \mathcal{I}. However, there are causal dependencies in the graph which can determine allowable feasible sets. It is unclear how to address this challenge here.
Review Point: 4. Is the learned CVAE completely respect the causal graph? The procedure of training CVAEs for interventional recourse should be clarified in further detail.
Review Point: 5. In experimental evaluation, I did not see a comparison to existing baselines in recourse, neither a qualitative assessment of the type of recourses that the model learns.
==================================================

Focused review:

weaknesses.
Sections 2 and 3 present a workflow that is very similar to the most popular graph convolution. Obviously, it is better to focus on the proposed component.
several related works are missing such as ICML 21 Directed Graph Embeddings in Pseudo-Riemannian Manifolds, and ICLR 20 DIMENet
3 Technical analysis- The paper proposes Hermitian matrices as a substitute for graph Laplacian.
The major concern is that how the Hermitian matrix can distinguish directions. In the definition of phase matrix in line 103, how to tell (1) no link between u and v, from (2) bi-directional link between u and v ? Do they both equal to zero?
The phase matrix is as large as the adjacency matrix, there may be an efficiency issue when the graph is large.
4 Experiment- The proposed method has been evaluated on both synthetic and real-world graph datasets.
The results of link prediction well support the proposed method, but the performance on the node classification task is not significantly better than the baselines.
Since there might be an efficiency issue, efficiency evaluation is expected.
After rebuttal
The author well cleared my concerns and the score was updated.
describe how the proposed method distinguishes forward, backward, bi-direction, and no link.
discuss and study the efficiency problem: how much will the additional phase matrix and its multiplication take?
present the implementation of multiplication between complex matrices

Review Point: 4 Experiment- The proposed method has been evaluated on both synthetic and real-world graph datasets. The results of link prediction well support the proposed method, but the performance on the node classification task is not significantly better than the baselines. Since there might be an efficiency issue, efficiency evaluation is expected. After rebuttal The author well cleared my concerns and the score was updated. describe how the proposed method distinguishes forward, backward, bi-direction, and no link. discuss and study the efficiency problem: how much will the additional phase matrix and its multiplication take? present the implementation of multiplication between complex matrices
==================================================

Focused review:

Weakness:
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
- Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4. Comments:
The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained. 

Review Point: - The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
Review Point: - Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4. Comments: The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.
==================================================

Focused review:

Weaknesses: 1. It is confusing to me what the exact goal of this paper is. Are we claiming the multi-prototype model is superior to other binary classification models (such as linear SVM, kNN, etc.) in terms of interpretability? Why do we have two sets of baselines for higher-dimensional and lower-dimensional data?  2. In Figure 3, for the baselines on the left hand side, what if we sparsify the trained models to reduce the number of selected features and compare accuracy to the proposed model? 3. Since the parameter for sparsity constraint has to be manually picked, can the authors provide any experimental results on the sensitivity of this parameter? Similar issue arises when picking the number of prototypes. Update after Author's Feedback: All my concerns are addressed by the authors's additional results. I'm changing my score based on that. 

Review Point: 1. It is confusing to me what the exact goal of this paper is. Are we claiming the multi-prototype model is superior to other binary classification models (such as linear SVM, kNN, etc.) in terms of interpretability? Why do we have two sets of baselines for higher-dimensional and lower-dimensional data?
Review Point: 2. In Figure 3, for the baselines on the left hand side, what if we sparsify the trained models to reduce the number of selected features and compare accuracy to the proposed model?
Review Point: 3. Since the parameter for sparsity constraint has to be manually picked, can the authors provide any experimental results on the sensitivity of this parameter? Similar issue arises when picking the number of prototypes. Update after Author's Feedback: All my concerns are addressed by the authors's additional results. I'm changing my score based on that.
==================================================

Focused review:

Weaknesses:
I believe the paper has a couple of fundamental weaknesses as it stands that should be addressed before it could be accepted at ICLR. 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons. 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in "seq distance to function" relationships - this is especially important in relation to the point about the Novelty measure above.
4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page "AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation." And there are papers writing about this eg Pak et al 2021

Review Point: 1. While the evaluation is conceptually laudable (performance, novelty, diversity), the metrics do not seem to capture the concepts well. Performance is evaluated irrespective of the distance from training. Since ‘Novelty” is evaluated as binary- was in the training set or not. (I think - see equation in 3.1), the metric seems to have no measure of distance from training beyond a single mutation. (It is now well appreciated that predicting the effect of single mutations is relatively successful with even baseline methods such as conservation, Potts models or VAEs with alignments and transformers without alignments.) Therefore - for this piece of work to be evaluated I suggest it’s important to show sequence generation as a function of the distance from training data. A fundamental challenge in protein design is being able to generate sequences with a given function that have sequences different from natural or training examples. As one moves away from known sequences (in eg Hamming distance) - the harder it gets. For sequences that are only one mutation away is relatively easy. ( many papers have shown this). The performance results shown in Tables 1 indicate that their method is only 1% better than a random single mutation for eg GFP, Table1, suggesting the metrics and/or the model is poor. Although the authors note this point , they do not follow up by addressing the reasons.
Review Point: 2. The use of he Oracle twice is circular - therefore invalidates the claims of performance; there are some ways around this that the authors could try More minor weaknesses:
Review Point: 3. the reference used to justify the evaluation metrics is Hoffman et al 2022 - but this paper is about optimising small molecules - which are v different in "seq distance to function" relationships - this is especially important in relation to the point about the Novelty measure above.
Review Point: 4. AlphaFold is not at all appropriate to support the claim of functional sequence optimisation - there are may mutations that will cause a protein to unfold that Alpha fold will predict as having almost exactly the same structure as it will align etc - therefore it proves nothing ( From their own FAQ page "AlphaFold has not been validated for predicting the effect of mutations. In particular, AlphaFold is not expected to produce an unfolded protein structure given a sequence containing a destabilising point mutation." And there are papers writing about this eg Pak et al 2021
==================================================

Focused review:

The novelty of the method seems to be limited, the author should compare to other similar works. 1. The worst case optimization framework is similar to DRO [1*]. The difference is that the author tries to optimize towards the worst case exposure strategy rather than group performance in DRO. 2. [2*] also proposes a dual learning algorithm to learn simultaneously the unbiased exposure distribution and the user preference. Some more experiments may be conducted against this type of work. In the experiments part, the paper's method shows minor improvement over POP as propensity weighting function. And I would like to see some explanations why in Table 1 g cannot be oracle in ACL, and why MLP/oracle is worse than MLP/Pop. [1*] Fairness Without Demographics in Repeated Loss Minimization, https://arxiv.org/abs/1806.08010 [2*] Unbiased Learning to Rank with Unbiased Propensity Estimation, sigir 2018

Review Point: 1. The worst case optimization framework is similar to DRO [1*]. The difference is that the author tries to optimize towards the worst case exposure strategy rather than group performance in DRO.
Review Point: 2. [2*] also proposes a dual learning algorithm to learn simultaneously the unbiased exposure distribution and the user preference. Some more experiments may be conducted against this type of work. In the experiments part, the paper's method shows minor improvement over POP as propensity weighting function. And I would like to see some explanations why in Table 1 g cannot be oracle in ACL, and why MLP/oracle is worse than MLP/Pop. [1*] Fairness Without Demographics in Repeated Loss Minimization, https://arxiv.org/abs/1806.08010 [2*] Unbiased Learning to Rank with Unbiased Propensity Estimation, sigir 2018
==================================================

Focused review:

I’m in general positive about this paper: contributions are clear, and the motivations/logics/experiments all appear to be thoughtful and convincing. A few nitpicks/suggestions: - I’m intrigued by your very good semi-supervsed results at 1% low label rate. Could you possibly include more comparison methods, and could you elaborate more why your method is particularly successful compared to others? - It would be nice to include more backbones besides ResNet18: currently that is the only one used. - Another great thing to add would be ImageNet-scale experiments, although I understand it very hard/impossible for rebuttal window. But please do consider it, as your method shall likely show benefits on it too.

Review Point: - I’m intrigued by your very good semi-supervsed results at 1% low label rate. Could you possibly include more comparison methods, and could you elaborate more why your method is particularly successful compared to others?
Review Point: - It would be nice to include more backbones besides ResNet18: currently that is the only one used.
Review Point: - Another great thing to add would be ImageNet-scale experiments, although I understand it very hard/impossible for rebuttal window. But please do consider it, as your method shall likely show benefits on it too.
==================================================

Focused review:

weaknesses I observe in this paper are: - I am concerned on the extensibility of the proposed method to a large number of tasks, which I would think would pose a challenge in the proposed setting similar to how it does in multitask learning. What would be the impact of a large number of tasks, specifically on the set of shared parameters? The experiments provide results for a limited number of tasks (up to 20).
Questions for authors:
Please address the question regarding the effect of number of tasks on the proposed approach.

Review Point: - I am concerned on the extensibility of the proposed method to a large number of tasks, which I would think would pose a challenge in the proposed setting similar to how it does in multitask learning. What would be the impact of a large number of tasks, specifically on the set of shared parameters? The experiments provide results for a limited number of tasks (up to 20). Questions for authors: Please address the question regarding the effect of number of tasks on the proposed approach.
==================================================

Focused review:

Weaknesses:
1: The theoretical analysis in Theorem 1 is unclear and weak. It is unclear that what the error bound in Theorem 1 means. The authors need to analyze and compare the theoretical results to other comparable methods.
2: The title is ambiguous and may lead to inappropriate reviewers.
3: I see no code attached to this submission, which makes me a bit concerned about reproducibility.

Review Point: 1: The theoretical analysis in Theorem 1 is unclear and weak. It is unclear that what the error bound in Theorem 1 means. The authors need to analyze and compare the theoretical results to other comparable methods.
Review Point: 2: The title is ambiguous and may lead to inappropriate reviewers.
Review Point: 3: I see no code attached to this submission, which makes me a bit concerned about reproducibility.
==================================================

Focused review:

1) Given that the targets are novel, the authors have no choice but to rely on an unvalidated folded structure, or use docking methods that only take their sequence into account. 2) The diverse set of methods used means that there is no underlying theoretical framework for the approach. 3) I would have liked to see more prioritization among the 3500 compounds that are generated by the approach; which are most likely to work? Easiest to synthesize? Most specific? These are all questions that I would have liked to see addressed more in detail.

Review Point: 1) Given that the targets are novel, the authors have no choice but to rely on an unvalidated folded structure, or use docking methods that only take their sequence into account.
Review Point: 2) The diverse set of methods used means that there is no underlying theoretical framework for the approach.
Review Point: 3) I would have liked to see more prioritization among the 3500 compounds that are generated by the approach; which are most likely to work? Easiest to synthesize? Most specific? These are all questions that I would have liked to see addressed more in detail.
==================================================

Focused review:

1. The write-up has many typos and some formulas/explanations are confusing. 
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. 
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. 
4. More strong baselines should be included/discussed in the experiments. 
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. 
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. 
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? 
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.” 
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? 
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? 
12. Line 450: L_{pc} or L_{cp}| in Eq. 7? 
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”? 
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? 
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? 

Review Point: 2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes.
Review Point: 3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data.
Review Point: 4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing.
Review Point: 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling?
Review Point: 11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code?
Review Point: 13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”?
Review Point: 14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?
Review Point: 15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here?
==================================================

Focused review:

Weaknesses:
Clarity of results.
A drawback (which I think can be resolved to some extent) is that although the overall idea is clean, the presentation is way too overwhelming. For example, it will help the readers if important steps in the algorithms can be explained / highlighed / commented, or given more wording in explaining their roles. Also, Theorem 1 poses a huge challenge to the whole flow as the conditions are too complicated. I will suggest replacing it with an informal theorem, and put all these conditions to the appendix.
Relation to the literature.
Although the authors generally did a good job in relating to the literature, more discussion on the novelty of this idea can further improve this work and posit this paper appropriately in the literature. For instance, 1) how does the construction of uncertainty quantification differ from that in the online setting (e.g., Jia et al. 2022)? Is there special tricks for dealing with distribution shift? 2) how does the analysis of 2-layer neural networks rely on existing ones, and which parts are specific to the new offline setting? etc. Questions: Since Q ∗
is used for fitting the networks, I am curious about the definition of Q ∗
. Is it equivalent to the class of 2-layer neural networks with ReLU activation, or is it a superset of it?
What is the relationship between H n t k
and the class of 2-layer NN with ReLU activation? Please clarify this to help the readers.
The conditions in Theorem 1 are too lengthy. Is it possible to reduce it to a cleaner version (potentially with some harmless relaxation)?
It will help if there is a sketch or overview of the key theoretical techniques in Theorem 1. For example, why do the randomized rewards lead to a valid uncertainty quantifier.
Minor issues:
The last sentence in Section 4 is difficult to read. What does it mean by `` with a provable implicit uncertainty quantifier and O ~ ( 1 / K ) ''?
Remark 1 is a bit difficult to read. What does it mean by ``data-dependent quantity that measures the number principle dimensions over which the.."?
What does σ ′
mean in Definition 1? Please provide a formal definition.

Review Point: 2) how does the analysis of 2-layer neural networks rely on existing ones, and which parts are specific to the new offline setting? etc. Questions: Since Q ∗ is used for fitting the networks, I am curious about the definition of Q ∗ . Is it equivalent to the class of 2-layer neural networks with ReLU activation, or is it a superset of it? What is the relationship between H n t k and the class of 2-layer NN with ReLU activation? Please clarify this to help the readers. The conditions in Theorem 1 are too lengthy. Is it possible to reduce it to a cleaner version (potentially with some harmless relaxation)? It will help if there is a sketch or overview of the key theoretical techniques in Theorem 1. For example, why do the randomized rewards lead to a valid uncertainty quantifier. Minor issues: The last sentence in Section 4 is difficult to read. What does it mean by `` with a provable implicit uncertainty quantifier and O ~ ( 1 / K ) ''? Remark 1 is a bit difficult to read. What does it mean by ``data-dependent quantity that measures the number principle dimensions over which the.."? What does σ ′ mean in Definition 1? Please provide a formal definition.
==================================================

Focused review:

- While the algorithm achieves good performance, the baselines are not convincing. The authors borrow sparsification algorithms designed for image classification networks, and report the performance of these algorithms when used for training GANs. As GANs are trained in an adversarial fashion, it seems natural that the proposed adversarial learning framework will produce better results than algorithms meant for image classification networks. - The algorithm is a simple modification of the StarGAN algorithm, where a sparse/compressed generative network is trained jointly with the dense network. - Some of the discussions about distributional entropies and the difficulties of using baseline algorithms do not have valid arguments to back them up. Right now they sound more like wild conjectures than intuition.

Review Point: - While the algorithm achieves good performance, the baselines are not convincing. The authors borrow sparsification algorithms designed for image classification networks, and report the performance of these algorithms when used for training GANs. As GANs are trained in an adversarial fashion, it seems natural that the proposed adversarial learning framework will produce better results than algorithms meant for image classification networks.
Review Point: - The algorithm is a simple modification of the StarGAN algorithm, where a sparse/compressed generative network is trained jointly with the dense network.
Review Point: - Some of the discussions about distributional entropies and the difficulties of using baseline algorithms do not have valid arguments to back them up. Right now they sound more like wild conjectures than intuition.
==================================================

Focused review:

Despite being well written with adequate insights, the manuscript would make for a stronger submission by addressing some of the concerns below, 1. The work demonstrates high quality results however, the presented framework is an incremental extension of [5]. Particularly, the approach augments the loss in [5] with an adversarial component and feeds the conditioning at each layer similar to StyleGan. 2. The experimental evaluation presented in significantly rigorous as compared to the most prominent prior art. Particularly, the authors provide results only on 2 texture classes. 3. Lines 174: The authors make a design choice to split the frequencies into different sized bins and feed it to different layers. The authors claim that this is beneficial since this reduces the number of parameters. Is this beneficial in terms of performance? or training speed? . Where exactly are the benefits of this choice manifested? An experiment to support this design choice would be instructive. Particularly, demonstrating an ablation with feeding the full vector at each layer with A being (m x n) instead of (m x n/4). 4. The authors provide quantitative evaluation highlighting that the generated patches have adequate similarity to input exemplar. However, as in the claims of lines 51-55, the authors present no evaluation regarding the diversity of the synthesized patches. Particularly, how do the authors make sure that the quality is not improved at the cost of diversity? 5. Comparison is provided only to [5] and no other previous methods. Although [5] demonstrates state of the art performance over several previous method already, it is important to highlight the performance of the presented framework in the context of other similar texture synthesis methods like [a,b,c]. 6. Lines 193: A more detailed treatment of "sufficiently similar" texture would provide better insights regarding the generalizability of the framework. Particularly, how does one measure the similarity of the textures. A simple study would be, to demonstrate the generalization performance as a function of Gram Matrix distance of a test texture to the nearest neighbor training texture. This will also lend credence to the fact that the proposed framework beats prior art in terms of quality. [a] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, 2017 [b] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML,2016 [c] Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukac. Texture Mixer: A network for controllable synthesis and interpolation of texture. In CVPR, 2019

Review Point: 2. The experimental evaluation presented in significantly rigorous as compared to the most prominent prior art. Particularly, the authors provide results only on 2 texture classes.
Review Point: 3. Lines 174: The authors make a design choice to split the frequencies into different sized bins and feed it to different layers. The authors claim that this is beneficial since this reduces the number of parameters. Is this beneficial in terms of performance? or training speed?
Review Point: . Where exactly are the benefits of this choice manifested? An experiment to support this design choice would be instructive. Particularly, demonstrating an ablation with feeding the full vector at each layer with A being (m x n) instead of (m x n/4).
Review Point: 4. The authors provide quantitative evaluation highlighting that the generated patches have adequate similarity to input exemplar. However, as in the claims of lines 51-55, the authors present no evaluation regarding the diversity of the synthesized patches. Particularly, how do the authors make sure that the quality is not improved at the cost of diversity?
Review Point: 5. Comparison is provided only to [5] and no other previous methods. Although [5] demonstrates state of the art performance over several previous method already, it is important to highlight the performance of the presented framework in the context of other similar texture synthesis methods like [a,b,c].
Review Point: 6. Lines 193: A more detailed treatment of "sufficiently similar" texture would provide better insights regarding the generalizability of the framework. Particularly, how does one measure the similarity of the textures. A simple study would be, to demonstrate the generalization performance as a function of Gram Matrix distance of a test texture to the nearest neighbor training texture. This will also lend credence to the fact that the proposed framework beats prior art in terms of quality. [a] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, 2017 [b] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML,2016 [c] Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukac. Texture Mixer: A network for controllable synthesis and interpolation of texture. In CVPR, 2019
==================================================

Focused review:

weaknesses: 1. Most of the statements in the paper are conjectures without any theoretical proof. Moreover, even if we just look at empirical results, statements like “Conjecture 4: The limitation of performance in each pruning regime can be predicted by a functional form” are completely unsupported and must be removed from the paper. 2. The main concern I have is on the claim that mCB=1.0 is optimal. The authors make the claim that mCB=1.0 can indicate the winning ticket without any additional proof/analysis. They support it by showing some results in Fig. 5. However, Fig. 5 seems to be plotting data only from the winning subnetworks. If we want to look at the raw accuracy drop data (for all sampled networks) vs. mCB, we should see the plots in Fig. 2 (row 5) and Fig. 4 (row 4). Clearly, mCB=1.0 has nothing to do with winning subnetworks (especially on Fig. 4: models with mCB=1.0 exhibit very high accuracy drop for c_params = 0.02). Specifically, for each value of mCB, there are several subnetworks that achieve high accuracy drop while several others achieve low accuracy drop (e.g., see blue and orange points going high up above the green points in both Figures for almost all the cases). Therefore, mCB=1.0 does not seem to indicate winning subnetworks (in Fig. 2 and Fig. 4). In fact, the optimal value seems to be changing from case to case (e.g., 1.1, 1.2, 1.3, etc., and that is just for one dataset) and, like mentioned before, even at those values, we are not guaranteed to consistently see subnetworks with the least accuracy drops). 3. I tried to see if there are other patterns like in Fig. 2, mCB is positively correlated with accuracy drop (at least for high pruning constraint). This would indicate that the lower the mCB, the lower the accuracy drop and thus it can be used to indicate winning tickets. Unfortunately, in Fig. 4, the same metric is negatively correlated with accuracy drop (particularly for high pruning constraint), and thus, the higher the mCB, the lower the accuracy drop. This again suggests that mCB cannot be used to indicate winning subnetworks. Note that, this change in behavior is for a single dataset. If the behavior of the proposed metric changes for the same dataset (when changing only the experiment setting from “constrain FLOPS” to “constrain parameter count”), it is highly likely that the metric won’t be useful when other, more complex datasets are involved. 4. For the ResNet result, the authors have gone only up to 25% FLOPS and 50% FLOPS. In Fig. 2, there was no correlation between CIFAR-10 accuracy drop and mCB values for 25% FLOPS constraint, so it is not convincing how mCB can be reasonably used for ImageNet. The results are good; however, it is unclear if mCB is the correct explanation for those results.

Review Point: 1. Most of the statements in the paper are conjectures without any theoretical proof. Moreover, even if we just look at empirical results, statements like “Conjecture 4: The limitation of performance in each pruning regime can be predicted by a functional form” are completely unsupported and must be removed from the paper.
Review Point: 2. The main concern I have is on the claim that mCB=1.0 is optimal. The authors make the claim that mCB=1.0 can indicate the winning ticket without any additional proof/analysis. They support it by showing some results in Fig.
Review Point: 5. However, Fig. 5 seems to be plotting data only from the winning subnetworks. If we want to look at the raw accuracy drop data (for all sampled networks) vs. mCB, we should see the plots in Fig. 2 (row 5) and Fig. 4 (row 4). Clearly, mCB=1.0 has nothing to do with winning subnetworks (especially on Fig. 4: models with mCB=1.0 exhibit very high accuracy drop for c_params = 0.02). Specifically, for each value of mCB, there are several subnetworks that achieve high accuracy drop while several others achieve low accuracy drop (e.g., see blue and orange points going high up above the green points in both Figures for almost all the cases). Therefore, mCB=1.0 does not seem to indicate winning subnetworks (in Fig. 2 and Fig. 4). In fact, the optimal value seems to be changing from case to case (e.g., 1.1, 1.2, 1.3, etc., and that is just for one dataset) and, like mentioned before, even at those values, we are not guaranteed to consistently see subnetworks with the least accuracy drops). 3. I tried to see if there are other patterns like in Fig. 2, mCB is positively correlated with accuracy drop (at least for high pruning constraint). This would indicate that the lower the mCB, the lower the accuracy drop and thus it can be used to indicate winning tickets. Unfortunately, in Fig. 4, the same metric is negatively correlated with accuracy drop (particularly for high pruning constraint), and thus, the higher the mCB, the lower the accuracy drop. This again suggests that mCB cannot be used to indicate winning subnetworks. Note that, this change in behavior is for a single dataset. If the behavior of the proposed metric changes for the same dataset (when changing only the experiment setting from “constrain FLOPS” to “constrain parameter count”), it is highly likely that the metric won’t be useful when other, more complex datasets are involved. 4. For the ResNet result, the authors have gone only up to 25% FLOPS and 50% FLOPS. In Fig. 2, there was no correlation between CIFAR-10 accuracy drop and mCB values for 25% FLOPS constraint, so it is not convincing how mCB can be reasonably used for ImageNet. The results are good; however, it is unclear if mCB is the correct explanation for those results.
==================================================

Focused review:

Weaknesses
This paper has marginal novelty. The proposed method FLBoost is incremental compared with FedGen [1] which also uses the data-free distillation. FLBoost uses the reweighting schemes for image generation or model aggregation, which are common operations and not novel. The remaining novelty seems to be the adversarial training scheme.
The experimental comparisons are problematic. First, experiments are conducted only on two small CIFAR datasets, which is far from extensive as claimed in the introduction. Results on other common benchmarks like Tiny-ImageNet or CINIC-10 are required. I also encourage the authors to provide results on large-scaled realistic datasets in [2]. Second, the experimental comparisons with other baselines are messy and unfair. FLBoost uses the state-of-the-art method SCAFFOLD as the default client-level method, while FedGen and FedDF [3] use the old method FedAvg. However, the authors did not remind the readers, and directly compared them in Table1, which is quite misleading. I think Table 1 and Table 3 should be changed to include results of the complete combinations between server-level methods (the # is 3) and client-level methods (the # is 4) on all datasets. Even based on the current results, if the client-level method is SCAFFOLD for all three methods in Table 3, I find the accuracy gaps between FLBoost and the other two server-level methods are significantly decreased. Third, I think using figures rather than tables to show the convergence is more straightforward. Figure 6 can be put in the appendix to give space for more results.
The proposed method explicitly violated the privacy protection regulation in FL: 1. Uploading classes statistics on the clients. 2. Recovering images of clients on the server. It is not a good clarification using the violation of other works to defend the proposed method. Comments
Missing related works: client-level method MOON at CVPR'21 [4].
Symbols not clarified or formulations missing: Symbols inTable 4 are not consistent with those in Section 3.1. The diversity loss is missing both in this submission and FedGen. References
[1] Data-Free Knowledge Distillation for Heterogeneous Federated Learning, ICML'21
[2] Federated Visual Classification with Real-World Data Distribution, ECCV'20
[3] Ensemble Distillation for Robust Model Fusion in Federated Learning, NeurIPS'20
[4] Model-Contrastive Federated Learning, CVPR'21

Review Point: 2. Recovering images of clients on the server. It is not a good clarification using the violation of other works to defend the proposed method. Comments Missing related works: client-level method MOON at CVPR'21 [4]. Symbols not clarified or formulations missing: Symbols inTable 4 are not consistent with those in Section 3.1. The diversity loss is missing both in this submission and FedGen. References [1] Data-Free Knowledge Distillation for Heterogeneous Federated Learning, ICML'21 [2] Federated Visual Classification with Real-World Data Distribution, ECCV'20 [3] Ensemble Distillation for Robust Model Fusion in Federated Learning, NeurIPS'20 [4] Model-Contrastive Federated Learning, CVPR'21
==================================================

Focused review:

Weaknesses
The results of this paper might be hard really to extend to multi-layer network (which is not surprising)
...and also hard/tedious to extend to other distributions, as it would require a replica calculation (although some ground work for that is done in the appendix)
These are not crucial problems, however. Summary
I think it's an interesting paper with a solid theoretical results, good experiments and an interesting connection to real neural data; 8 (strong accept).
The authors adequately addressed the limitations and potential negative societal impact.

Review Point: 8 (strong accept). The authors adequately addressed the limitations and potential negative societal impact.
==================================================

Focused review:

1) The author uses meta learning to learn the memory. However, there are some other works [1][2] uses a memory consisting of raw features. So What is the advantages of the meta-learning strategy to learn memory? 2) The ablation study lacks the experiments to verify the effectiveness of instance-wise FiLM models. 3) The Meta-Neighborhoods is semi-parametric, and is a generalizatio of k-nearest-neighbors. So what is parameters number and the time complexity of the proposed method compared to the baseline and k-nearest-neighbors, [1] Meta-learning with memory-augmented neural networks. ICML 2016. [2] Memory matching networks for one-shot image recognition. CVPR 2018. *******************After Rebuttal************************* I have carefully looked into all the reviews and rebuttals. The rebuttal addresses my concerns. I think the authors have performed significant experiments. It produces a good performance for large-scale tasks, 200-class classification on Tiny-Imagenet. It further reports the results on 1000-class ImageNet classification and also achieves significant gains. I agree with the other reviewers that the idea of combining single parameter initialization with non-parametric approaches for classification is interesting. So I will keep my score.

Review Point: 1) The author uses meta learning to learn the memory. However, there are some other works [1][2] uses a memory consisting of raw features. So What is the advantages of the meta-learning strategy to learn memory?
Review Point: 2) The ablation study lacks the experiments to verify the effectiveness of instance-wise FiLM models.
Review Point: 3) The Meta-Neighborhoods is semi-parametric, and is a generalizatio of k-nearest-neighbors. So what is parameters number and the time complexity of the proposed method compared to the baseline and k-nearest-neighbors, [1] Meta-learning with memory-augmented neural networks. ICML 2016. [2] Memory matching networks for one-shot image recognition. CVPR 2018. *******************After Rebuttal************************* I have carefully looked into all the reviews and rebuttals. The rebuttal addresses my concerns. I think the authors have performed significant experiments. It produces a good performance for large-scale tasks, 200-class classification on Tiny-Imagenet. It further reports the results on 1000-class ImageNet classification and also achieves significant gains. I agree with the other reviewers that the idea of combining single parameter initialization with non-parametric approaches for classification is interesting. So I will keep my score.
==================================================

Focused review:

Weakness: 1. The generator G needs pretraining, increasing extra computation. 2. Transfer performances on other downstream tasks are absent (e.g., detection, segmentation) 3. The idea of “hard individual sample” generated by G is confusing. It’s roughly defined in paragraph “Generating individually hard samples” in page 6 that the main model cannot generate similar representations for this positive pair originating from x_{2i-1} (the hard individual sample). This claim seems to be incorrect, as the representations of the positive pair are not only determined by the input sample, but also the transformations. One can make the two representations identical by simply applying the same transformation twice. Thus, the idea of hard individual sample is hard to understand.

Review Point: 2. Transfer performances on other downstream tasks are absent (e.g., detection, segmentation) 3. The idea of “hard individual sample” generated by G is confusing. It’s roughly defined in paragraph “Generating individually hard samples” in page 6 that the main model cannot generate similar representations for this positive pair originating from x_{2i-1} (the hard individual sample). This claim seems to be incorrect, as the representations of the positive pair are not only determined by the input sample, but also the transformations. One can make the two representations identical by simply applying the same transformation twice. Thus, the idea of hard individual sample is hard to understand.
==================================================

Focused review:

- The algorithmic ideas are not very novel and not of significant interest. The analysis follows the basic analysis that the greedy algorithm is a 2-approximation for k-center. - Why do you get a 5-approximation when the potential center locations are disjoint? There the greedy algorithm is a 3-approximation so you should get that all points in an optimal cluster is within distance 4 from a point in the greedy solution, right? - Also don't your ideas apply for other clustering problems such as k-median and k-means? EDIT after author feedback: I agree with the authors that in general when F neq C, the argument gives a 5-approximation. I suppose you need F subseteq C for a 4-approximation. Thanks for the nice explanation! I also read the other reviews and my score was perhaps too harsh so I increased it slightly.

Review Point: - The algorithmic ideas are not very novel and not of significant interest. The analysis follows the basic analysis that the greedy algorithm is a 2-approximation for k-center.
Review Point: - Why do you get a 5-approximation when the potential center locations are disjoint? There the greedy algorithm is a 3-approximation so you should get that all points in an optimal cluster is within distance 4 from a point in the greedy solution, right?
Review Point: - Also don't your ideas apply for other clustering problems such as k-median and k-means? EDIT after author feedback: I agree with the authors that in general when F neq C, the argument gives a 5-approximation. I suppose you need F subseteq C for a 4-approximation. Thanks for the nice explanation! I also read the other reviews and my score was perhaps too harsh so I increased it slightly.
==================================================

Focused review:

1. The proposed matrix estimation requires a crucial selection of anchor states and actions. Anchor states and actions contain sufficiently information to recover the low-rank Q function. In the matrix estimation algorithm, the authors assume that these anchor states and anchor actions are given, and in the experiments they pick a few states and actions that are far from each other in their respective metric spaces as the anchor states and actions. It would be more convincing if the authors can rigorously state the anchor selection algorithm and prove this algorithm indeed obtains anchor states and action. Otherwise, the challenge of low-rank matrix estimation just transfers to the finding of anchor states and actions. 2. In the main theory, the discounting factor $\gamma$ needs to be very small. For example, in Theorem 2, $\gamma < 1/(2 c_{me})$ and in Proposition 3 (rank 1 case) $c_{me} = 7 R_{max} / R_{min} > 7$. This implies that $\gamma < 1/14$, which seems to be unreasonably small. I notice that the authors used $\gamma = 0.9$ in all the experiments. More discussions on such inconsistency are needed. 3. As shown in all the five experiments, the nuclear norm matrix estimation works very well in practice. It is comparable or better than the proposed method. Given that nuclear norm is a common routine for low-rank matrix estimation, it would be helpful to provide more justifications on why the proposed new matrix estimation approach is needed. For example, some additional experiments might be added. ######### I have read the rebuttal. My concerns have been addressed.

Review Point: 1. The proposed matrix estimation requires a crucial selection of anchor states and actions. Anchor states and actions contain sufficiently information to recover the low-rank Q function. In the matrix estimation algorithm, the authors assume that these anchor states and anchor actions are given, and in the experiments they pick a few states and actions that are far from each other in their respective metric spaces as the anchor states and actions. It would be more convincing if the authors can rigorously state the anchor selection algorithm and prove this algorithm indeed obtains anchor states and action. Otherwise, the challenge of low-rank matrix estimation just transfers to the finding of anchor states and actions.
Review Point: 2. In the main theory, the discounting factor $\gamma$ needs to be very small. For example, in Theorem 2, $\gamma < 1/(2 c_{me})$ and in Proposition 3 (rank 1 case) $c_{me} = 7 R_{max} / R_{min} > 7$. This implies that $\gamma < 1/14$, which seems to be unreasonably small. I notice that the authors used $\gamma = 0.9$ in all the experiments. More discussions on such inconsistency are needed.
Review Point: 3. As shown in all the five experiments, the nuclear norm matrix estimation works very well in practice. It is comparable or better than the proposed method. Given that nuclear norm is a common routine for low-rank matrix estimation, it would be helpful to provide more justifications on why the proposed new matrix estimation approach is needed. For example, some additional experiments might be added. ######### I have read the rebuttal. My concerns have been addressed.
==================================================

Focused review:

Weaknesses: The outcome of the experiments is very predictable. The methods that are employed are very simple and ad-hoc. I found hardly any new idea in that paper. Neither are there any significant lessons that the reader learns about embeddings or sentiment analysis. The main idea (i.e. focusing on more task-specific data for training more accurate embeddings) was already published in the context of named-entity recognition by Joshi et al. (2015). The additions made in this paper are very incremental in nature.
I find some of the experiments inconclusive as (apparently) no statistical signficance testing between different classifiers has been carried out. In Tables 2, 3 and 6, various classifier configurations produce very similar scores. In such cases, only statistical signficance testing can really give a proper indication whether these difference are meaningful. For instance, in Table 3 on the left half reporting results on RT, one may wonder whether there is a significant difference between "Wikipedia Baseline" and any of the combinations. Furthermore, one doubts whether there is any signficant difference between the different combinations (i.e. either using "subj-Wiki", "subj-Multiun" or "subj-Europarl") in that table. 
The improvement by focusing on subjective subsets is plausible in general. 
However, I wonder whether in real life, in particular, a situation in which resources are sparse this is very helpful. Doing a pre-selection with OpinionFinder is some pre-processing step which will not be possible in most languages other than English. There are no equivalent tools or fine-grained datasets on which such functionality could be learnt. The fact that in the experiments for Catalan, this information is not considered proves that. Minor details: - lines 329-334: The discussion of this dataset is confusing. I thought the task is plain polarity classification but the authors here also refer to "opinion holder" and "opinion targets". If these information are not relevant to the experiments carried out in this paper, then they should not be mentioned here.
- lines 431-437: The variation of "splicing" that the authors explain is not very well motivated. First, why do we need this? In how far should this be more effective than simple "appending"?
- lines 521-522: How is the subjective information isolated for these configurations? I assume the authors here again employ OpinionFinder? However, there is no explicit mention of this here.
- lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.
- lines 689-695: Similar to lines 329-334 it is unclear what precise task is carried out. Do the authors take opinion holders and targets in consideration?
***AFTER AUTHORS' RESPONSE*** Thank you very much for these clarifying remarks. 
I do not follow your explanations regarding the incorporation of opinion holders and targets, though.
Overall, I will not change my scores since I think that this work lacks sufficient novelty (the things the authors raised in their response are just insufficient to me). This submission is too incremental in nature. 

Review Point: - lines 329-334: The discussion of this dataset is confusing. I thought the task is plain polarity classification but the authors here also refer to "opinion holder" and "opinion targets". If these information are not relevant to the experiments carried out in this paper, then they should not be mentioned here.
Review Point: - lines 431-437: The variation of "splicing" that the authors explain is not very well motivated. First, why do we need this? In how far should this be more effective than simple "appending"?
Review Point: - lines 521-522: How is the subjective information isolated for these configurations? I assume the authors here again employ OpinionFinder? However, there is no explicit mention of this here.
Review Point: - lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.
Review Point: - lines 689-695: Similar to lines 329-334 it is unclear what precise task is carried out. Do the authors take opinion holders and targets in consideration? ***AFTER AUTHORS' RESPONSE*** Thank you very much for these clarifying remarks. I do not follow your explanations regarding the incorporation of opinion holders and targets, though. Overall, I will not change my scores since I think that this work lacks sufficient novelty (the things the authors raised in their response are just insufficient to me). This submission is too incremental in nature.
==================================================

Focused review:

Weakness - The paper does not construct any new PaI approach, only observing how PaIs are correlated with their graph properties. It would be great if the authors could briefly share future insights of improvements. - Only CIFAR-10 dataset is used in experiments. I’d suggest the authors to report on 1-2 more datasets in order to make their empirical evidences more consistently convincing.

Review Point: - The paper does not construct any new PaI approach, only observing how PaIs are correlated with their graph properties. It would be great if the authors could briefly share future insights of improvements.
Review Point: - Only CIFAR-10 dataset is used in experiments. I’d suggest the authors to report on 1-2 more datasets in order to make their empirical evidences more consistently convincing.
==================================================

Focused review:

Minor ------- - There are some issues with the small-/big-Oh notation used in the paper. The statements between line 121 and 125 highlight this confusion. Indeed, k = o(d) Should normally mean lim_{d -> infty} k / d = 0. However, the author seems to be using it in the sense that the exists c > 0 such that k \le c*d for sufficiently large d. The should have been written d' = Omega(d) instead. To be on the safe side, the authors should clearly state what they mean by big-Oh, small-Oh, Omega, etc. This should would only take a 2 lines of the manuscript. - The paper is missing empirical insight. It shouldn't be difficult to run a few experiments on say an MLP neural net on MNIST data and report some curves which consolidate the theoretical claims. There is space in the manuscript to include such material. Insufficient "Related works" ---------------------------- My main issue with this paper is the incompleteness of the outlined "related works". We now have pretty mature picture on why adversarial examples exist (under different geometric conditions on the classifier and / or the distribution of the data). The related works section doesn't do justice to this rich literature (except 1 or 2 relevant references mentioned in the manuscript...). If every new paper starts with a punch-line like "Adversarial examples are mysterious. In this paper we will proof a powerful theorem which explains why they exist." (without mentioning existing literature in this direction), the field will not go very far. Here is a small sample of the existing literature. * Gilmer et al. "The Relationship Between High-Dimensional Geometry and Adversarial Examples" (2018) * Dohmatob, "Generalized No Free Lunch theorem for adversarial robustness" (ICML 2019) * Mahloujifar et al., "The curse of concentration in robust learning" (AAAI 2019) In the above papers, adversarial examples exist as a consequence of ordinary test-error in high-dimensional problems with concentrated class-conditional distributions. Concerning universal lower bounds on adversarial robustness error (i.e of any classifier), one should also mention the recent work: * Bhagoji et al. "Lower bounds on adversarial robustness from optimal transport" (NeurIPS 2019).

Review Point: - The paper is missing empirical insight. It shouldn't be difficult to run a few experiments on say an MLP neural net on MNIST data and report some curves which consolidate the theoretical claims. There is space in the manuscript to include such material. Insufficient "Related works" ---------------------------- My main issue with this paper is the incompleteness of the outlined "related works". We now have pretty mature picture on why adversarial examples exist (under different geometric conditions on the classifier and / or the distribution of the data). The related works section doesn't do justice to this rich literature (except 1 or 2 relevant references mentioned in the manuscript...). If every new paper starts with a punch-line like "Adversarial examples are mysterious. In this paper we will proof a powerful theorem which explains why they exist." (without mentioning existing literature in this direction), the field will not go very far. Here is a small sample of the existing literature.
Review Point: * Gilmer et al. "The Relationship Between High-Dimensional Geometry and Adversarial Examples" (2018) * Dohmatob, "Generalized No Free Lunch theorem for adversarial robustness" (ICML 2019) * Mahloujifar et al., "The curse of concentration in robust learning" (AAAI 2019) In the above papers, adversarial examples exist as a consequence of ordinary test-error in high-dimensional problems with concentrated class-conditional distributions. Concerning universal lower bounds on adversarial robustness error (i.e of any classifier), one should also mention the recent work:
Review Point: * Bhagoji et al. "Lower bounds on adversarial robustness from optimal transport" (NeurIPS 2019).
==================================================

Focused review:

Overall, I really liked the experiments in the paper, but some of the analysis could be made more complete. In particular, I had the following questions about the experiments: 1. In the experiments of Section 4.1, a possible explanation for the model's performance could be that it's not due to implicit reasoning but due to the distractor subject leaking the answer: For example, given A mammal has a belly button and A whale eats fish, the model can infer that a whale has a belly button simply by combining these facts instead of doing implicit reasoning. 2. What happens if you do not do "context" dropout at training time (removing certain parts of the context 50% of the times etc.)? 3. One possible explanation for why the hypothesis-only results are poor is because the hypothesis-only conditions are only seen in 20% of the examples. What happens if the model is re-trained with only hypothesis information and labels, and then evaluated in the hypothesis only mode? 4. To isolate the effect of pre-trained representations, why do the authors choose to use a different architecture (ESIM) instead of using RoBERTa with randomly initialized weights? 5. In the counting experiments, is the model really counting? A possible explanation is that the model doesn't really count but just looks at vector space similaries? For e.g. *Mick Jagger is a member of Beatles* is predicted as False, simply because it is far away in representation space from Ringo Starr and John Lennon, and not because the model counts. This could be tested by dropping the quantity fact and assessing performance. (To prevent train/test mismatch the model would need to be re-trained without the quantity fact).

Review Point: 1. In the experiments of Section 4.1, a possible explanation for the model's performance could be that it's not due to implicit reasoning but due to the distractor subject leaking the answer: For example, given A mammal has a belly button and A whale eats fish, the model can infer that a whale has a belly button simply by combining these facts instead of doing implicit reasoning.
Review Point: 2. What happens if you do not do "context" dropout at training time (removing certain parts of the context 50% of the times etc.)?
Review Point: 3. One possible explanation for why the hypothesis-only results are poor is because the hypothesis-only conditions are only seen in 20% of the examples. What happens if the model is re-trained with only hypothesis information and labels, and then evaluated in the hypothesis only mode?
Review Point: 4. To isolate the effect of pre-trained representations, why do the authors choose to use a different architecture (ESIM) instead of using RoBERTa with randomly initialized weights?
Review Point: 5. In the counting experiments, is the model really counting? A possible explanation is that the model doesn't really count but just looks at vector space similaries? For e.g. *Mick Jagger is a member of Beatles* is predicted as False, simply because it is far away in representation space from Ringo Starr and John Lennon, and not because the model counts. This could be tested by dropping the quantity fact and assessing performance. (To prevent train/test mismatch the model would need to be re-trained without the quantity fact).
==================================================

Focused review:

QUESTIONS FOR THE AUTHORS - recently, [1] criticized the usage of standard similarity measures such as cosine and Euclidean distance with contextualized embeddings, given that such metrics might be dominated by a small number of outlier dimensions. Do you think this could be affecting your results? Have you thought about using standardized metrics or a rank-based metric (e.g. Spearman correlation between vectors)?
- Section 5.1: "... in cases where a verb is split into multiple subwords, we take the embedding of the first subword token as the verb embedding". I am not sure I understand the motivation of this step. Could you please elaborate more? Why not taking e.g. the average of the subword embeddings?
TYPOS AND MINOR ISSUES: l. 264 LSTMS --> LSTMs l. 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP. 

Review Point: 301 is most similar --> is the most similar EXTRA REFERENCES [1] Timkey and Van Schjindel, 2021. All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. Proceedings of EMNLP.
==================================================

Focused review:

weakness or error in the paper. I think the contribution is solid, and the writing and logic are quite clear. 2. The framework does not postulate a priori assumption on the functional structure, overcoming some problems in the literature. 3. Using the RKHS framework makes the approach widely applicable in many data situations. Minor comments: 1. I think the most relevant literature is [11]. Itâs better to compare with the work more in the method part, or compare with the work in more details. 2. As the most relevant work, itâs better to put [11] in the benchmarks. 3. The detailed description of Figure 1 is lacking, making it confused at first view.

Review Point: 2. The framework does not postulate a priori assumption on the functional structure, overcoming some problems in the literature.
Review Point: 3. Using the RKHS framework makes the approach widely applicable in many data situations. Minor comments:
Review Point: 1. I think the most relevant literature is [11]. Itâs better to compare with the work more in the method part, or compare with the work in more details.
Review Point: 2. As the most relevant work, itâs better to put [11] in the benchmarks.
Review Point: 3. The detailed description of Figure 1 is lacking, making it confused at first view.
==================================================

Focused review:

Weaknesses
Explanation of past work. The authors should explain how prior work relates to their work more clearly. This first becomes problematic in the introduction, where the authors talk about robust feature extraction. This idea comes up several times throughout the paper. However, the authors never explain what they mean by "robust features." (Ilyas et al., 2019) proposed definitions that seem to be related, but the authors never discuss these definitions, meaning that the reader cannot fully understand this paper without reading the paper by Ilyas et al. As this idea is so central to the motivation for the present work and the only other paper that tackles a similar problem, the authors should more thoroughly how their ideas differ from this past work.
The paper would also be improved if the authors could describe what the main difference is between their work and dataset distillation. The related work section mentions some of the relevant literature, but based on their description, it seems as if it would make sense for the authors to compare to this kind of work. If the two paradigms are fundamentally different, the authors should (a) describe in detail what dataset distillation is so that (b) they can explain in a rigorous way why these two problems are different.
Writing. The standard of writing in this paper is not up to the level that would be acceptable for publication. There are numerous typos and instances of poor grammar. This work could be considerably improved if the authors rigorous copy-edited this work before publication.
Confusing notation. Section 3 was difficult to read. Notation is often discussed after it is used, which makes it challenging to understand the contribution/development of ideas. Sometimes notation is not defined at all, e.g. B ( x i , ϵ )
and the Clip function.
There is also a lack of preliminaries and development of the key ideas that led to this formulation. One way to rectify this would be as follows. First, define the learning setting, e.g., the data distribution, loss function, etc., and what it means to train "naturally" on a given dataset. Next, the authors could define what it means to learn "robustly." Then, the authors could introduce robust dataset learning.
However, in its current form, the authors simply skip right to introducing their problem without offering any of the necessary preliminaries. I believe that this will confuse future readers, as only someone who is already quite familiar with the related literature would be able to have any understanding of the problem without any preliminaries.
Looseness of the algorithm derivation. The derivation of the algorithm is imprecise. The authors make no comment of how tight the quadratic approximation of their problem is to the original problem. They also seem to implicitly assume that the Hessian of their loss is proportional to the identity matrix in the equation after (1). This assumption is never discussed or verified. Furthermore, they introduce a variable t
in using Taylor's theorem, yet they make no comment on how this value could be estimated or chosen. The lack of technical rigor here is a weakness of the manuscript.
This lack of rigor is also present in Section 4. First of all, they consider an entirely different problem in (5) than was previously discussed, because the maximum is moved from the inside to the outside of the expectation. This constitutes a different problem from the adversarial robustness problem described previously. There is no discussion regarding why the authors changed the formulation.
Furthermore, the authors make statements such as "we cannot represent w D ′ with D ′
explicitly." What does it mean to "represent" in this case? Does it mean to satisfy the constraint in (5)? If so, (6) should be written with the constraint on D ′
, because without it is't unclear how D ′
enters into this problem. Without this, it's hard to interpret Theorem 2, because it's unclear exactly which optimization problem the authors are referring to. Somewhat confusingly, Theorem 1 does not deal with the problem discussed in the paper (see more discussion on this below); it merely concerns the adversarial training problem. Is this also true of Thm. 2 or 3?
Baselines. One clear weakness is the lack of baseline comparisons. The authors mention that the only other paper to study their setting is (Ilyas et al., 2019). However, they only compare to this paper on CIFAR-10, and not on any of the other datasets they consider. This is a hole in the paper -- it is reasonable to expect the authors to include the relevant baseline comparisons on all datasets. Furthermore, there are actually different baselines for each of the three datasets, i.e. no two datasets consider the same set of baselines. This is confusing, and should be addressed in a future version of the paper.
Lack of description of experimental details. There should be an expanded discussion of the relevant details concerning the experiments. In particular, it's not clear how the authors perform this "pre-training" step or why this step is necessary at all. There is also no discussion of how the hyperparameters were chosen.
Not giving proper credit to prior work. I believe that there is a section of this paper wherein the authors do not properly give credit to past work. The authors cite the work of (Tsipras et al., 2019; Ilyas et al., 2019) throughout their work. And indeed, in Section 4, they adopt the same theoretical setting to study their problem as the setting proposed in these prior works. Up until this point in the paper, the authors give appropriate credit for the ideas proposed in those papers.
However, in Section 4, the authors state a result -- Theorem 1 -- which appears to be nearly a word-for-word copy of Theorem 2.2 in (Tsipras et al., 2019). The authors of the paper under review do not acknowledge that this theorem was proved in prior work. Indeed, Theorem 1 is discussed as if it was a new contribution. However, given how heavily the authors cite (Tsipras et al., 2019; Ilyas et al., 2019) in the remainder of the paper, it seems impossible that the authors were not aware of this theorem, which is one of if not the defining contribution of these past papers.
Elements of the proof -- in particular Lemma 2 in the appendix -- appear to be exactly the same as in (Tsipras et al., 2019) (c.f. Lemma D.1 in that paper). While the authors offer a more verbose proof than did the original paper of Tsipras et al., it does not change the fact that this is result is not a new contribution. I believe that this aspect of the paper needs further discussion and should be the subject of an ethics review.

Review Point: 2 or 3? Baselines. One clear weakness is the lack of baseline comparisons. The authors mention that the only other paper to study their setting is (Ilyas et al., 2019). However, they only compare to this paper on CIFAR-10, and not on any of the other datasets they consider. This is a hole in the paper -- it is reasonable to expect the authors to include the relevant baseline comparisons on all datasets. Furthermore, there are actually different baselines for each of the three datasets, i.e. no two datasets consider the same set of baselines. This is confusing, and should be addressed in a future version of the paper. Lack of description of experimental details. There should be an expanded discussion of the relevant details concerning the experiments. In particular, it's not clear how the authors perform this "pre-training" step or why this step is necessary at all. There is also no discussion of how the hyperparameters were chosen. Not giving proper credit to prior work. I believe that there is a section of this paper wherein the authors do not properly give credit to past work. The authors cite the work of (Tsipras et al., 2019; Ilyas et al., 2019) throughout their work. And indeed, in Section 4, they adopt the same theoretical setting to study their problem as the setting proposed in these prior works. Up until this point in the paper, the authors give appropriate credit for the ideas proposed in those papers. However, in Section 4, the authors state a result -- Theorem 1 -- which appears to be nearly a word-for-word copy of Theorem 2.2 in (Tsipras et al., 2019). The authors of the paper under review do not acknowledge that this theorem was proved in prior work. Indeed, Theorem 1 is discussed as if it was a new contribution. However, given how heavily the authors cite (Tsipras et al., 2019; Ilyas et al., 2019) in the remainder of the paper, it seems impossible that the authors were not aware of this theorem, which is one of if not the defining contribution of these past papers. Elements of the proof -- in particular Lemma 2 in the appendix -- appear to be exactly the same as in (Tsipras et al., 2019) (c.f. Lemma D.1 in that paper). While the authors offer a more verbose proof than did the original paper of Tsipras et al., it does not change the fact that this is result is not a new contribution. I believe that this aspect of the paper needs further discussion and should be the subject of an ethics review.
==================================================

Focused review:

* While I appreciate the probabilistic approach in the paper, the algorithm seems to be a quite straight-forward extension of previous state-space models (HMMs). So I feel the algorithm by itself does not carry too much novelty. * The results from this algorithm is not compared to alternative methods. Would simple methods, such as PCA, also yield similar results shown in Fig 4B,C,D? It would be helpful to justify the necessity of using such more complicated method to analyze these data. The applicability of the proposed method seems to be quite restrictive. The paper would be stronger if the authors could demonstrate or propose some other potentially applications. Overall, I feel that the scientific results presented upon further substantiation (based on larger sample size) could result in a solid scientific paper in a scientific journal. It’s unclear to me how appealing the current manuscript would be for the NeurIPS audience partly due to the incremental technical contributions. Another technical issue: the Gamma model in Eq (6) is likely a poor model for the deconvolve calcium response. It would be useful to show how well the model can fit the data. ***************modified after rebuttal I thank the authors for the feedback. After seeing everything, I remain slight negative about this manuscript. But I also feel this is a borderline paper- perhaps the arguments could be made either way. I am still slightly negative, because I think i) the algorithmic contribution is too incremental; ii) I am not convinced the benefit/gain of modeling the transition probability, and whether one could already obtains most of the results at the population level by running simple procedure such as PCA etc. The authors' rebuttal did not fully address these issues.

Review Point: * While I appreciate the probabilistic approach in the paper, the algorithm seems to be a quite straight-forward extension of previous state-space models (HMMs). So I feel the algorithm by itself does not carry too much novelty.
Review Point: * The results from this algorithm is not compared to alternative methods. Would simple methods, such as PCA, also yield similar results shown in Fig 4B,C,D? It would be helpful to justify the necessity of using such more complicated method to analyze these data. The applicability of the proposed method seems to be quite restrictive. The paper would be stronger if the authors could demonstrate or propose some other potentially applications. Overall, I feel that the scientific results presented upon further substantiation (based on larger sample size) could result in a solid scientific paper in a scientific journal. It’s unclear to me how appealing the current manuscript would be for the NeurIPS audience partly due to the incremental technical contributions. Another technical issue: the Gamma model in Eq (6) is likely a poor model for the deconvolve calcium response. It would be useful to show how well the model can fit the data. ***************modified after rebuttal I thank the authors for the feedback. After seeing everything, I remain slight negative about this manuscript. But I also feel this is a borderline paper- perhaps the arguments could be made either way. I am still slightly negative, because I think i) the algorithmic contribution is too incremental; ii) I am not convinced the benefit/gain of modeling the transition probability, and whether one could already obtains most of the results at the population level by running simple procedure such as PCA etc. The authors' rebuttal did not fully address these issues.
==================================================

Focused review:

Weakness -
The results in the paper are really difficult to parse, making it very difficult to match the figures with the claims in the paper. I do believe that the paper requires some work in its presentation to make it readable to a general audience. Section 3’s Figures which forms the foundation of the claims in this paper are not well explained. Below are some instances
Section 3 introduces alpha_tot as a ratio of total training examples (P) and number of parameters in the model (N). alpha_pruning is basically f*alpha_tot where f represents the fraction of the examples selected. Using this knowledge (as established in lines 159-165) seems incomplete to parse Figure 1. For example, in Figure 1A x axis represents alpha_prune as an increasing variable. That means, f is decreasing. However, the figure also has datapoints associated with different f for the same alpha_prune. Given that alpha_tot is constant, how is that possible?
There are other similar issues in readability (See questions 1,2,3) that make it really difficult to understand the claims. This makes parsing the next section that shows these results on CIFAR-10 and SVHN equally hard.
The technique described in the paper is post-hoc - Given a dataset, how do you prune it to a few high quality samples to reduce the number of samples to train on. This can potentially reduce the training time of the model (line 336). But this aspect was not discussed in detail in the paper. Given that this paper does not discuss how to build a high quality dataset, its impact can be limited.
Have the authors adequately addressed the limitations and potential negative societal impact of their work? - Yes

Review Point: - The results in the paper are really difficult to parse, making it very difficult to match the figures with the claims in the paper. I do believe that the paper requires some work in its presentation to make it readable to a general audience. Section 3’s Figures which forms the foundation of the claims in this paper are not well explained. Below are some instances Section 3 introduces alpha_tot as a ratio of total training examples (P) and number of parameters in the model (N). alpha_pruning is basically f*alpha_tot where f represents the fraction of the examples selected. Using this knowledge (as established in lines 159-165) seems incomplete to parse Figure 1. For example, in Figure 1A x axis represents alpha_prune as an increasing variable. That means, f is decreasing. However, the figure also has datapoints associated with different f for the same alpha_prune. Given that alpha_tot is constant, how is that possible? There are other similar issues in readability (See questions 1,2,3) that make it really difficult to understand the claims. This makes parsing the next section that shows these results on CIFAR-10 and SVHN equally hard. The technique described in the paper is post-hoc - Given a dataset, how do you prune it to a few high quality samples to reduce the number of samples to train on. This can potentially reduce the training time of the model (line 336). But this aspect was not discussed in detail in the paper. Given that this paper does not discuss how to build a high quality dataset, its impact can be limited. Have the authors adequately addressed the limitations and potential negative societal impact of their work?
==================================================

Focused review:

weakness of this work is that it may be of more limited significance. The prior work of Bello demonstrated that RL with pointer networks were capable of outperform OR Tools on the TSP problem. This work is a generalisation of that work (with a minor architectural changes). There are two questions I have that would merit discussion in the paper. - Bello et al. use active search (further RL training on a specific problem instance) in order to iteratively search for a better solution. Was this approach tried here / why was it not used here? - How generalizable are the solutions learned for problems outside the training distribution. For example VRP100 on tasks of size 99 or 101? This seems like an important question for practical applications. Minor issues: - Although possible rewards mentioned are the negative total vehicle distance or average service time, the reward function actually used is, as far as I could find, never explicitly stated. - The policy is denoted as both $\pi$ (standard RL notation) and, to emphasize the sequential structure $P(Y|X_0)$. This change seems distracting. -- The author's response demonstrated some level of generalisation in problem size and explained why they are not using active search (although it would be nice to see this as a baseline). Given the generalisation results I've increased my rating.

Review Point: - Bello et al. use active search (further RL training on a specific problem instance) in order to iteratively search for a better solution. Was this approach tried here / why was it not used here?
Review Point: - How generalizable are the solutions learned for problems outside the training distribution. For example VRP100 on tasks of size 99 or 101? This seems like an important question for practical applications. Minor issues:
Review Point: - Although possible rewards mentioned are the negative total vehicle distance or average service time, the reward function actually used is, as far as I could find, never explicitly stated.
Review Point: - The policy is denoted as both $\pi$ (standard RL notation) and, to emphasize the sequential structure $P(Y|X_0)$. This change seems distracting. -- The author's response demonstrated some level of generalisation in problem size and explained why they are not using active search (although it would be nice to see this as a baseline). Given the generalisation results I've increased my rating.
==================================================

Focused review:

Weaknesses:
There is only one algorithmic problem considered. In related work, more than one algorithm is used to show performance. This is a major weakness of the paper since the main claims are not entirely supported. The abstract states, "We demonstrate that simultaneously learning the dual definition of these optimisation problems in algorithmic learning allows for better learning and qualitatively better solutions." and In the conclusion, the authors write: "We showed that learning together the primal-dual problem can substantially improve the quality of the predictions, as testified by the quantitative and qualitative evaluations of the models. Furthermore, dual algorithmic reasoners have demonstrated to generalise better, showing positive knowledge transfer across different families of graph distributions and extracting informative representations for large-scale graphs while only being trained on toy-synthetic graphs." But I find the claims much more general than the empirical findings which relate to only one problem on graph data and only two different families of graphs.
Minor issues not affecting score: - The citations and figure references are not linked.
After reading the author response, I have changed my score. The authors have addressed my concerns.

Review Point: - The citations and figure references are not linked. After reading the author response, I have changed my score. The authors have addressed my concerns.
==================================================

Focused review:

Weaknesses
1. The novelty is limited. The main claim of the authors is that they propose text-query alignment to address the issue of taxonomy inconsistency. However, introducing language embeddings to build a unified label space for image segmentation has already been proven to be effective [1, 2, 3]. The Category-guided decoding (CGD) here is just another variant of LSeg [1] that exploits the text-pixel affinity to bridge the domain gap across different label spaces. Dataset-aware augmentation (DAA) here is a non-learnable strategy for data augmentation, which seems to be better mentioned in the section of implementation details instead of methodology.
2. The improvement is not strong enough. The numbers shown in Table 2 are not impressive from my point of view. For example, the gain for the average mIOU across four datasets can only be shrunk to a negligible 0.2% for 640K. I focus more on the results trained with a longer schedule since MaskFormer is hard to optimize and originally it needs 300 epochs to train on COCO panoptic. The results look to be prettier when trained under a short schedule, however, this might be due to a better initialization as the encoder of the proposed method is first pre-trained by CLIP (as discussed in Table 4). Reference
[1] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. ICLR 2022
[2] Xu, Mengde, et al. "A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model." arXiv preprint arXiv:2112.14757 (2021).
[3] Rao, Yongming, et al. "Denseclip: Language-guided dense prediction with context-aware prompting." CVPR 2022.

Review Point: 1. The novelty is limited. The main claim of the authors is that they propose text-query alignment to address the issue of taxonomy inconsistency. However, introducing language embeddings to build a unified label space for image segmentation has already been proven to be effective [1, 2, 3]. The Category-guided decoding (CGD) here is just another variant of LSeg [1] that exploits the text-pixel affinity to bridge the domain gap across different label spaces. Dataset-aware augmentation (DAA) here is a non-learnable strategy for data augmentation, which seems to be better mentioned in the section of implementation details instead of methodology.
Review Point: 2. The improvement is not strong enough. The numbers shown in Table 2 are not impressive from my point of view. For example, the gain for the average mIOU across four datasets can only be shrunk to a negligible 0.2% for 640K. I focus more on the results trained with a longer schedule since MaskFormer is hard to optimize and originally it needs 300 epochs to train on COCO panoptic. The results look to be prettier when trained under a short schedule, however, this might be due to a better initialization as the encoder of the proposed method is first pre-trained by CLIP (as discussed in Table 4). Reference [1] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. ICLR 2022 [2] Xu, Mengde, et al. "A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model." arXiv preprint arXiv:2112.14757 (2021). [3] Rao, Yongming, et al. "Denseclip: Language-guided dense prediction with context-aware prompting." CVPR 2022.
==================================================

Focused review:

1. The idea of grouping contextual information using a VLAD/NetVLAD like technics is not novel enough, although the channel-wise practice may have some trivial difference. How does the author compare the proposed methods with counterparts like PointNetVLAD? In the reviewer’s opinion, the only key difference between the proposed method and the PointNetVLAD is the channel-wise grouping practice, of which the contribution is not enough to be presented in NIPs. 2. There are many other methods performed much better that the proposed method in this paper on ScanNetv2 dataset. Why the author only compared with two methods in this paper (i.e. 3DSIS and VoteNet)? In the reviewer’s opinion, if the effectiveness of this method can only be proved by comparing with the “contextual grouping” based counterparts, instead of achieving the SOTA performance, then the contribution of this paper is limited. Moreover, the reviewer would like to know if this method can be used in the current SOTA method on ScanNet leaderboard to further improved their performance? 3. The author claims that the proposed group contextual problem can solve the “data sparsity”. However, this paper does not provide enough experimental analysis on how this technic will take effect to solve this problem. Since this is one of the most important issues claimed to be solved (as described in abstract) by group contextual encoding method, a comprehensive experiment is need to prove its effectiveness.

Review Point: 1. The idea of grouping contextual information using a VLAD/NetVLAD like technics is not novel enough, although the channel-wise practice may have some trivial difference. How does the author compare the proposed methods with counterparts like PointNetVLAD? In the reviewer’s opinion, the only key difference between the proposed method and the PointNetVLAD is the channel-wise grouping practice, of which the contribution is not enough to be presented in NIPs.
Review Point: 2. There are many other methods performed much better that the proposed method in this paper on ScanNetv2 dataset. Why the author only compared with two methods in this paper (i.e. 3DSIS and VoteNet)? In the reviewer’s opinion, if the effectiveness of this method can only be proved by comparing with the “contextual grouping” based counterparts, instead of achieving the SOTA performance, then the contribution of this paper is limited. Moreover, the reviewer would like to know if this method can be used in the current SOTA method on ScanNet leaderboard to further improved their performance?
Review Point: 3. The author claims that the proposed group contextual problem can solve the “data sparsity”. However, this paper does not provide enough experimental analysis on how this technic will take effect to solve this problem. Since this is one of the most important issues claimed to be solved (as described in abstract) by group contextual encoding method, a comprehensive experiment is need to prove its effectiveness.
==================================================

Focused review:

1. Some existing NAS methods can also search the width and depth of network, such as single path one shot, DARTs, more discussions of these methods should be added. 2. The results are under FLOPs constraints, I wonder if the proposed AutoBSS can be used under latency constraints.

Review Point: 1. Some existing NAS methods can also search the width and depth of network, such as single path one shot, DARTs, more discussions of these methods should be added.
Review Point: 2. The results are under FLOPs constraints, I wonder if the proposed AutoBSS can be used under latency constraints.
==================================================

Focused review:

Weaknesses:
1). In experiments, the comparisons with the previous methods are not enough. The following papers are not compared, but they seem to outperform the proposed method:
[1] Few-shot class incremental learning by sampling multi-phase tasks. (TPAMI)
[2] Subspace regularizers for few-shot class incremental learning. (ICLR ‘22)
[3] Metafscil: A meta-learning approach for few-shot class incremental learning. (CVPR ‘22)
[4] Constrained few-shot class-incremental learning. (CVPR ‘22)
[5] Few-shot class incremental learning via entropy-regularized data-free replay. (ECCV ‘22)
[6] Few-shot class-incremental learning from an open-set perspective. (ECCV ‘22)
I notice that the authors give references for [2] (introduction) and [1] (related work), but I do not find a reasons to ignore them in the comparison in experiments.
2). Fixing part of the network, though seems to be novel in FSCIL, but has been studied in the very similar task CIL (Adaptive Aggregation Networks for Class-Incremental Learning, CVPR ‘21). The motivations are very similar. It would be better to have a discussion instead of claiming “first work that makes changes to the parameter space itself”.

Review Point: 1). In experiments, the comparisons with the previous methods are not enough. The following papers are not compared, but they seem to outperform the proposed method: [1] Few-shot class incremental learning by sampling multi-phase tasks. (TPAMI) [2] Subspace regularizers for few-shot class incremental learning. (ICLR ‘22) [3] Metafscil: A meta-learning approach for few-shot class incremental learning. (CVPR ‘22) [4] Constrained few-shot class-incremental learning. (CVPR ‘22) [5] Few-shot class incremental learning via entropy-regularized data-free replay. (ECCV ‘22) [6] Few-shot class-incremental learning from an open-set perspective. (ECCV ‘22) I notice that the authors give references for [2] (introduction) and [1] (related work), but I do not find a reasons to ignore them in the comparison in experiments.
Review Point: 2). Fixing part of the network, though seems to be novel in FSCIL, but has been studied in the very similar task CIL (Adaptive Aggregation Networks for Class-Incremental Learning, CVPR ‘21). The motivations are very similar. It would be better to have a discussion instead of claiming “first work that makes changes to the parameter space itself”.
==================================================

Focused review:

- Some technical details deserves more elaborations, especially on the multi-task learning. The main idea of this paper is to train an RL agents simultaneously with an affordance map segmentation network, though these are essentially treated as two orthogonal objectives, the learning procedure in a whole is still unclear to me. The authors are suggested to provide more details on how the training is proceeded, such as whether these two tasks are trained concurrently or alternatively, and if their learning processes are not synchronous, how they choose the ratio of the learning iteration of each task, and how these extra parameters can affect the performances (in an extra ablation study). A comprehensive loss function and pseudo code will be preferred. - There is still some gap needed to be filled in the evaluation to further improve the sufficiency. To name a few: a) The selected metrics are only evaluated in a limited range. There are only curves over time(training steps) for the converge, while the success rates should also be evaluated in this way but not just the final quantities as it could be insightful to see the analysis on how the proposed method could improve the interaction skills. I think if it does perform as expected (also the counterparts), there should be low success rate at the beginning (as it tend to interact more with the object) but can improve faster than the other methods. b) The authors demonstrate how some downstream RL tasks could benefit from their proposed method and compare with the seemingly strongest baseline (obj coverage). Given the overall quality of their contribution, more evaluation efforts should be included here. I would like to see some endeavor on extending this part towards this direction: * To combine the proposed method with other exploration strategy. I do feel most of the considered baselines focus less on interaction but navigation, which seems sort of opposite to what this paper specifically works with, thus it may be more interesting to see some results on how the proposed method can really mitigate their drawbacks than simply contrasting on some interaction-oriented tasks. This can also further verify the main motivation of this paper---an efficient solution of exploration for interactions. Nevertheless, I do feel the selected downstream tasks could also be more challenging, say there is a significant need for both navigation and interaction.

Review Point: - Some technical details deserves more elaborations, especially on the multi-task learning. The main idea of this paper is to train an RL agents simultaneously with an affordance map segmentation network, though these are essentially treated as two orthogonal objectives, the learning procedure in a whole is still unclear to me. The authors are suggested to provide more details on how the training is proceeded, such as whether these two tasks are trained concurrently or alternatively, and if their learning processes are not synchronous, how they choose the ratio of the learning iteration of each task, and how these extra parameters can affect the performances (in an extra ablation study). A comprehensive loss function and pseudo code will be preferred.
Review Point: - There is still some gap needed to be filled in the evaluation to further improve the sufficiency. To name a few: a) The selected metrics are only evaluated in a limited range. There are only curves over time(training steps) for the converge, while the success rates should also be evaluated in this way but not just the final quantities as it could be insightful to see the analysis on how the proposed method could improve the interaction skills. I think if it does perform as expected (also the counterparts), there should be low success rate at the beginning (as it tend to interact more with the object) but can improve faster than the other methods. b) The authors demonstrate how some downstream RL tasks could benefit from their proposed method and compare with the seemingly strongest baseline (obj coverage). Given the overall quality of their contribution, more evaluation efforts should be included here. I would like to see some endeavor on extending this part towards this direction: * To combine the proposed method with other exploration strategy. I do feel most of the considered baselines focus less on interaction but navigation, which seems sort of opposite to what this paper specifically works with, thus it may be more interesting to see some results on how the proposed method can really mitigate their drawbacks than simply contrasting on some interaction-oriented tasks. This can also further verify the main motivation of this paper---an efficient solution of exploration for interactions. Nevertheless, I do feel the selected downstream tasks could also be more challenging, say there is a significant need for both navigation and interaction.
==================================================

Focused review:

Weaknesses:
1: The paper only presents a few key formulas about the VC theory, but provides no theoretical derivation except a few references of prior works. As the VC-theory is the key of the paper, the authors should include a full analysis of all the details of the theory. The theory presented in the paper should be self-contained, such that readers with some background can understand the analysis/theory with minor or no reference to literature.
Without giving details of the VC-theory, I cannot judge the correctness of the paper.
2: Explaining double descent includes two parts: 1, theoretically connecting test error with weight norm square, or VC-dimension; and 2, theoretically explaining the behavior of weight norm square and VC-dimension. The paper does not theoretically explain the behavior of the weight norm square. Even assuming all the formulas in the paper are correct, one still cannot claim the double descent is explained.
3: The formula of VC-dimension, in Eq.(3), only applies to linear models. However, the most interesting part of double descent is for non-linear models, e.g., over-parameterized neural networks. For the non-linear models, there is still no explanation.
4: The values of a 1 and a 2
are not from theoretical analysis, but from empirical guesses. Then, I don’t think the key equation, Eq.(2), is theoretical.

Review Point: 1: The paper only presents a few key formulas about the VC theory, but provides no theoretical derivation except a few references of prior works. As the VC-theory is the key of the paper, the authors should include a full analysis of all the details of the theory. The theory presented in the paper should be self-contained, such that readers with some background can understand the analysis/theory with minor or no reference to literature. Without giving details of the VC-theory, I cannot judge the correctness of the paper.
Review Point: 2: Explaining double descent includes two parts: 1, theoretically connecting test error with weight norm square, or VC-dimension; and 2, theoretically explaining the behavior of weight norm square and VC-dimension. The paper does not theoretically explain the behavior of the weight norm square. Even assuming all the formulas in the paper are correct, one still cannot claim the double descent is explained.
Review Point: 3: The formula of VC-dimension, in Eq.(3), only applies to linear models. However, the most interesting part of double descent is for non-linear models, e.g., over-parameterized neural networks. For the non-linear models, there is still no explanation.
Review Point: 4: The values of a 1 and a 2 are not from theoretical analysis, but from empirical guesses. Then, I don’t think the key equation, Eq.(2), is theoretical.
==================================================

Focused review:

Weaknesses: - What is the time comparison of VCL relative to BatchNorm and having no normalization? - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently. - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed. - Line 177 typo: "batchsized" - It would aid a reader if the authors summarized the loss and how it is computed at the end of Section 2.2. - How sensitive is the framework to the choice of n? - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions: - Will the authors be open-sourcing the code for the experiments? - Have you experimented with a constant \beta? - Have you experimented with having _both_ BN and VCL?  Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal. 

Review Point: - What is the time comparison of VCL relative to BatchNorm and having no normalization?
Review Point: - The argument tying modes, BatchNorm, and VCL could be better explained. It seems that the observations about modes and normalization outcome is new but the authors don't describe it sufficiently.
Review Point: - I recommend that the authors format their mathematical equations better. For instance, Equations (4), (14), (18), and others, would be easier to parse if the bracketing and indexing were fixed.
Review Point: - How sensitive is the framework to the choice of n?
Review Point: - How does \beta vary over time? Could the authors include a graph for this in the Appendix? Questions:
Review Point: - Will the authors be open-sourcing the code for the experiments?
Review Point: - Have you experimented with having _both_ BN and VCL? Post-rebuttal I will stick to my rating. This is good work and I thank the authors for clarifying my questions in the rebuttal.
==================================================

Focused review:

1. I think the present work lacks sufficient analysis though there are many empirical validation results. It would be better if authors could reason more behind extensive empirical evaluations. 2. It's better to evaluate on a large-scale dataset like ImageNet.

Review Point: 1. I think the present work lacks sufficient analysis though there are many empirical validation results. It would be better if authors could reason more behind extensive empirical evaluations.
Review Point: 2. It's better to evaluate on a large-scale dataset like ImageNet.
==================================================

Focused review:

Weaknesses: -For me, the weakest link of the paper is Section 4.1 (sequentially interactive SCO). The results are suboptimal as they stand, so one wonders why they are useful. The authors do mention that in practice, sometimes only one query per user is possible, which provides some motivation for sequentially interactive algorithms. Can the authors provide references and practical examples to support this claim? I would also like to see other arguments motivating the importance of sequentially interactive protocols. In addition, the paper would be greatly strengthened by either improving the bounds in Section 4.1 (to match the existing central DP lower bounds) or adding nearly tight lower bounds for sequentially interactive algorithms to show a separation between sequential and full interactivity for this problem and establishing near optimality of their bounds. Joseph et al. (2019) seems like a relevant work for this.
-Missed related work: 1. There is a very relevant contemporaneous work that should be cited/discussed in the final version of the paper by Lowy & Razaviyayn (2021) https://arxiv.org/abs/2106.09779. They cover a more general problem of federated learning, but setting n = 1 and M = N
in their Theorem 4.1 recovers your Theorem 4.9. They use different protocols and only have fully interactive algorithms. They also use acceleration and smoothing for some of their results. I don't hold this one against the authors since it is concurrent, but a discussion of this work should be added to the Related Work section of the final version of the paper. 2. Erlingsson et al. (2020) ("ESA Revisited..") should also be referenced with Girgis et al (2021) in the context of shuffle DP ERM. 3. Smith et al. (2017) ("Is interaction necessary...") uses acceleration (Theorem 20), as does Lowy & Razaviyayn (2021)--these should be discussed, and the sentence "we are not aware of previous private SCO results relying on acceleration" in Contribution 3 should be removed. -Intro: O ( d / ϵ n )
is optimal in the local model is true, but I think Duchi et al (2013) only proves it for sequentially interactive pure DP algorithms, right? However, Lowy & Razaviyayn (2021) show that the same optimal loss bound holds for approximate DP fully interactive algorithms.
-Definition 2.7 seems unnecessarily abstract; would be good to explain intuitively
-"allow the analyzer to adaptively choose which subset of users to query.." is not the main/important difference between full and sequential interactivity; sequential algorithms also choose which subset of users to query in each round (but not adaptively). I would either delete this part of the sentence or emphasize "adaptively"; the main distinction is in the second part of the sentence (repeated querying).
-Lemma 3.1: y is never explicitly defined; you should define it and say it lives in [ 0 , 1 ] g + b
; " P 1 D ( S ( y ) )
is an unbiased estimate..." is not technically correct (as it is a tuple)--I think you mean " A 1 D ( S ( y ) )
is an unbiased estimate..."?
-Algorithms 3 and 5: it looks like you are doing gradient ascent instead of descent
-Section 4.1 initial paragraph: include ϵ
in these loss bounds
-Restriction on epsilon should be included in statements of all theorems in Section 4. You require ϵ ≤ 1
for privacy of your vector summation protocol, right? Also, for advanced composition in Theorem 4.9, you need some restriction (even if very mild and possibly subsumed by the restriction needed for vector summation) to bound e ϵ ′ ≤ 2 ϵ ′
. Also, I think you may need to shrink the inputs to your vector summation protocol by constant factor (something like 2 2
) to get exactly ( ϵ , δ )
-DP in Theorem 4.9.
-Clarify wording in "combining it with acceleration to attain a better guarantee": the guarantee is not better than that attained in Bassily et al. (2019).
-"optimization trajectory": what does this mean?
-Theorem 4.9: I think you are missing a l a m b d a
in the denominator of the second term? (Or the units are wrong.); also, you should include runtime and/or communication complexity guarantees (and do the same for results in Section 4.1.)
-Proof of Lemma 3.1. Clarify/highlight where/how shuffling is used to provide privacy (if at all). It wasn't clear to me. Would same privacy guarantee hold without shuffling? Also, explain why V a r ( η 1 ) ≤ 1 / 4.

Review Point: -For me, the weakest link of the paper is Section 4.1 (sequentially interactive SCO). The results are suboptimal as they stand, so one wonders why they are useful. The authors do mention that in practice, sometimes only one query per user is possible, which provides some motivation for sequentially interactive algorithms. Can the authors provide references and practical examples to support this claim? I would also like to see other arguments motivating the importance of sequentially interactive protocols. In addition, the paper would be greatly strengthened by either improving the bounds in Section 4.1 (to match the existing central DP lower bounds) or adding nearly tight lower bounds for sequentially interactive algorithms to show a separation between sequential and full interactivity for this problem and establishing near optimality of their bounds. Joseph et al. (2019) seems like a relevant work for this. -Missed related work:
Review Point: 1. There is a very relevant contemporaneous work that should be cited/discussed in the final version of the paper by Lowy & Razaviyayn (2021) https://arxiv.org/abs/2106.09779. They cover a more general problem of federated learning, but setting n = 1 and M = N in their Theorem 4.1 recovers your Theorem 4.9. They use different protocols and only have fully interactive algorithms. They also use acceleration and smoothing for some of their results. I don't hold this one against the authors since it is concurrent, but a discussion of this work should be added to the Related Work section of the final version of the paper.
Review Point: 2. Erlingsson et al. (2020) ("ESA Revisited..") should also be referenced with Girgis et al (2021) in the context of shuffle DP ERM.
Review Point: 3. Smith et al. (2017) ("Is interaction necessary...") uses acceleration (Theorem 20), as does Lowy & Razaviyayn (2021)--these should be discussed, and the sentence "we are not aware of previous private SCO results relying on acceleration" in Contribution 3 should be removed. -Intro: O ( d / ϵ n ) is optimal in the local model is true, but I think Duchi et al (2013) only proves it for sequentially interactive pure DP algorithms, right? However, Lowy & Razaviyayn (2021) show that the same optimal loss bound holds for approximate DP fully interactive algorithms. -Definition 2.7 seems unnecessarily abstract; would be good to explain intuitively -"allow the analyzer to adaptively choose which subset of users to query.." is not the main/important difference between full and sequential interactivity; sequential algorithms also choose which subset of users to query in each round (but not adaptively). I would either delete this part of the sentence or emphasize "adaptively"; the main distinction is in the second part of the sentence (repeated querying). -Lemma 3.1: y is never explicitly defined; you should define it and say it lives in [ 0 , 1 ] g + b ; " P 1 D ( S ( y ) ) is an unbiased estimate..." is not technically correct (as it is a tuple)--I think you mean " A 1 D ( S ( y ) ) is an unbiased estimate..."? -Algorithms 3 and 5: it looks like you are doing gradient ascent instead of descent -Section 4.1 initial paragraph: include ϵ in these loss bounds -Restriction on epsilon should be included in statements of all theorems in Section 4. You require ϵ ≤ 1 for privacy of your vector summation protocol, right? Also, for advanced composition in Theorem 4.9, you need some restriction (even if very mild and possibly subsumed by the restriction needed for vector summation) to bound e ϵ ′ ≤ 2 ϵ ′ . Also, I think you may need to shrink the inputs to your vector summation protocol by constant factor (something like 2 2 ) to get exactly ( ϵ , δ ) -DP in Theorem 4.9. -Clarify wording in "combining it with acceleration to attain a better guarantee": the guarantee is not better than that attained in Bassily et al. (2019). -"optimization trajectory": what does this mean? -Theorem 4.9: I think you are missing a l a m b d a in the denominator of the second term? (Or the units are wrong.); also, you should include runtime and/or communication complexity guarantees (and do the same for results in Section 4.1.) -Proof of Lemma 3.1. Clarify/highlight where/how shuffling is used to provide privacy (if at all). It wasn't clear to me. Would same privacy guarantee hold without shuffling? Also, explain why V a r ( η 1 ) ≤ 1 / 4.
==================================================

Focused review:

- Although the proposed approach overall is reasonable. It is not quite clear whether it is enough to rely on the diversity led by imitation error. As indicated by the author in lines 98 and 99, the novel states are generated when the trained policy does not exactly follow the demonstrations. Does it mean the diversity comes from this error? If so and if the assumption is that the trained policy performs well on what it is trained on, the error will be small and the novel states will be less different from the original state in the demonstration trajectories. If so, what the method will do to prevent the bias led by the original demonstration trajectories? Is this the reason why the approach shows pretty different performance under certain random seed? Please clarify this point. - As pointed out in the strength of the paper, it is nice to see the authors willing to demonstrate the robustness and generalization of the algorithm. However, I also feel that too many things are squeezed together in the experiment section. This leads to less extensive and clear explanations. I understand that the authors are bounded by the total number of pages, but from a good presentation point of view, it will be better if the experiment sections can outline more about the important messages and provide thorough descriptions. - Table 1 is in the wrong scale. You can adjust the scale of the entire table so that it fits the page. Same thing with Equation (1). - The layout of Figure 3 also looks odd and hard to read. Please adjust them and the inline space: the caption is too close to the sub-figures. - In the baseline for Montezuma's Revenge, the authors mention the DTRA+EXP performs poorly. However, the results are not shown in Table 1. What is the performance of the DTRA+EXP?

Review Point: - Although the proposed approach overall is reasonable. It is not quite clear whether it is enough to rely on the diversity led by imitation error. As indicated by the author in lines 98 and 99, the novel states are generated when the trained policy does not exactly follow the demonstrations. Does it mean the diversity comes from this error? If so and if the assumption is that the trained policy performs well on what it is trained on, the error will be small and the novel states will be less different from the original state in the demonstration trajectories. If so, what the method will do to prevent the bias led by the original demonstration trajectories? Is this the reason why the approach shows pretty different performance under certain random seed? Please clarify this point.
Review Point: - As pointed out in the strength of the paper, it is nice to see the authors willing to demonstrate the robustness and generalization of the algorithm. However, I also feel that too many things are squeezed together in the experiment section. This leads to less extensive and clear explanations. I understand that the authors are bounded by the total number of pages, but from a good presentation point of view, it will be better if the experiment sections can outline more about the important messages and provide thorough descriptions.
Review Point: - Table 1 is in the wrong scale. You can adjust the scale of the entire table so that it fits the page. Same thing with Equation (1).
Review Point: - The layout of Figure 3 also looks odd and hard to read. Please adjust them and the inline space: the caption is too close to the sub-figures.
Review Point: - In the baseline for Montezuma's Revenge, the authors mention the DTRA+EXP performs poorly. However, the results are not shown in Table 1. What is the performance of the DTRA+EXP?
==================================================

Focused review:

The main weaknesses I see in this paper are: - Although technically sound and interesting, the assumption on p(y | x’) not changing is a very strong assumption that seems to contradict the purpose of the framework which is to encourage better decisions. As these decisions are better and better, the target y’ will change in a positive manner, and therefore likely p(y | x’) will change. Not considering may pose the risk of making the problem of feedback loops even worse, as the effect of p(x) that changes to p’(x) is being considered, but not p(y | x’). In the context of predictive policing for example, the actions may imply more arrests for particular neighborhoods (Ensign et al., 2018), i.e. p’(x), while p(y | x’) remains fixed but may be actually changing because of these actions. I think more concrete effect of this assumption should be discussed more thoroughly on the paper, by quantifying, theoretically or experimentally, the effect of these assumptions. - In the experiments, although the experiments are tested with different values of the step-size parameter in the real-world datasets, it is not explained how the choice of this parameter can be made in practice. This is important for the application of this method to other datasets/problems. - There is no mention to the computational complexity of the proposed method. What is the cost of the extra-regularization term and the alternating mechanism proposed to solve this problem? Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C., & Venkatasubramanian, S. (2018, January). Runaway feedback loops in predictive policing. In Conference on Fairness, Accountability and Transparency (pp. 160-171).

Review Point: - Although technically sound and interesting, the assumption on p(y | x’) not changing is a very strong assumption that seems to contradict the purpose of the framework which is to encourage better decisions. As these decisions are better and better, the target y’ will change in a positive manner, and therefore likely p(y | x’) will change. Not considering may pose the risk of making the problem of feedback loops even worse, as the effect of p(x) that changes to p’(x) is being considered, but not p(y | x’). In the context of predictive policing for example, the actions may imply more arrests for particular neighborhoods (Ensign et al., 2018), i.e. p’(x), while p(y | x’) remains fixed but may be actually changing because of these actions. I think more concrete effect of this assumption should be discussed more thoroughly on the paper, by quantifying, theoretically or experimentally, the effect of these assumptions.
Review Point: - In the experiments, although the experiments are tested with different values of the step-size parameter in the real-world datasets, it is not explained how the choice of this parameter can be made in practice. This is important for the application of this method to other datasets/problems.
Review Point: - There is no mention to the computational complexity of the proposed method. What is the cost of the extra-regularization term and the alternating mechanism proposed to solve this problem? Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C., & Venkatasubramanian, S. (2018, January). Runaway feedback loops in predictive policing. In Conference on Fairness, Accountability and Transparency (pp. 160-171).
==================================================

Focused review:

Weaknesses: 1. Some experimental settings are not very reasonable. For example, in Table 4, the authors adopt ResNet-50, Inception v3, MobileNet, and MNASNet altogether as substitute architectures of baselines while adopting ResNet-50 and MobileNet as substitute architectures when combining their method with the ensemble methods. Why not experiment with the same surrogate model? This comparison may be unfair when most of the black box models use the ResNet structure. Moreover, it will be more convincing if the author can add some experiments on advanced defense models, such as adversarial training models, Randomized Smoothing (RS)[1], and Neural Representation Purifier (NRP)[2]. 2. There are some severe spelling mistakes in the article that the author should check carefully, such as “Aidmix” should be “Admix”. 3. The authors could try to add some analysis on why this method works. [1]Cohen J, Rosenfeld E, Kolter Z. Certified adversarial robustness via randomized smoothing[C]//International Conference on Machine Learning. PMLR, 2019: 1310-1320.Naseer M, Khan S, Hayat M, et al. [2] A self-supervised approach for adversarial robustness[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 262-271.

Review Point: 1. Some experimental settings are not very reasonable. For example, in Table 4, the authors adopt ResNet-50, Inception v3, MobileNet, and MNASNet altogether as substitute architectures of baselines while adopting ResNet-50 and MobileNet as substitute architectures when combining their method with the ensemble methods. Why not experiment with the same surrogate model? This comparison may be unfair when most of the black box models use the ResNet structure. Moreover, it will be more convincing if the author can add some experiments on advanced defense models, such as adversarial training models, Randomized Smoothing (RS)[1], and Neural Representation Purifier (NRP)[2].
Review Point: 2. There are some severe spelling mistakes in the article that the author should check carefully, such as “Aidmix” should be “Admix”.
Review Point: 3. The authors could try to add some analysis on why this method works. [1]Cohen J, Rosenfeld E, Kolter Z. Certified adversarial robustness via randomized smoothing[C]//International Conference on Machine Learning. PMLR, 2019: 1310-1320.Naseer M, Khan S, Hayat M, et al. [2] A self-supervised approach for adversarial robustness[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 262-271.
==================================================

Focused review:

The comparison with previous work is apple-to-orange: - Most of previous methods from Meta-Dataset use ResNet-18 operating on resolution 128, while CTX here uses ResNet-34 and works with resolution of 224. Definitely more computational footprint. - While you are showing that this (ResNet-34 + 224 + N.SGD) does not improve ProtoNet, it doesn't indicate it can not improve others. Especially the ProtoNet present in your paper is a bit suspicious, i.e., in 230-231, you claim that data-augmentation harms, which is counter-intuitive and you don't perform enough analysis but just blame that the augmentation is self-supervised specific. To me, this is not really true, because SimCLR aug only adds Gaussian Blur and Color jittering on top of very standard augmentation. - Using Auto-augment is problematic. The policy in Auto-augment is developed by seeing the labels of all 1k classes within ImageNet. While it's fine to say that you only use 712 classes for training, such claim is not that "clean". Important details are missing in the main text, for example, the authors never mention that they use the standard classification training in the main text. However recent studies [a,b] found this part is actually crucial (and the authors should definitely discuss these two papers as they are very relevant). They authors neither discuss the effects of pre-training, nor mention you use two-stage training in the main text (only exists in supp.) [a] A New Meta-Baseline for Few-Shot Learning [b] Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? The results present in [a] and [b], which only use pre-trained features, are much better than CTX-SimCLR eps +Aug here, which uses both pre-training and episode learning. Not to mention that this paper uses a larger network as well as 224 resolution. I also wonder if the improvement comes from longer training schedule because of additional SimCLR episode? There is no such ablation. You should compare: - ProtoNet with X episodes - Your method with 0.5*X normal episode + 0.5*X SimCLR episode.

Review Point: - Most of previous methods from Meta-Dataset use ResNet-18 operating on resolution 128, while CTX here uses ResNet-34 and works with resolution of 224. Definitely more computational footprint.
Review Point: - While you are showing that this (ResNet-34 + 224 + N.SGD) does not improve ProtoNet, it doesn't indicate it can not improve others. Especially the ProtoNet present in your paper is a bit suspicious, i.e., in 230-231, you claim that data-augmentation harms, which is counter-intuitive and you don't perform enough analysis but just blame that the augmentation is self-supervised specific. To me, this is not really true, because SimCLR aug only adds Gaussian Blur and Color jittering on top of very standard augmentation.
Review Point: - Using Auto-augment is problematic. The policy in Auto-augment is developed by seeing the labels of all 1k classes within ImageNet. While it's fine to say that you only use 712 classes for training, such claim is not that "clean". Important details are missing in the main text, for example, the authors never mention that they use the standard classification training in the main text. However recent studies [a,b] found this part is actually crucial (and the authors should definitely discuss these two papers as they are very relevant). They authors neither discuss the effects of pre-training, nor mention you use two-stage training in the main text (only exists in supp.) [a] A New Meta-Baseline for Few-Shot Learning [b] Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? The results present in [a] and [b], which only use pre-trained features, are much better than CTX-SimCLR eps +Aug here, which uses both pre-training and episode learning. Not to mention that this paper uses a larger network as well as 224 resolution. I also wonder if the improvement comes from longer training schedule because of additional SimCLR episode? There is no such ablation. You should compare:
Review Point: - ProtoNet with X episodes - Your method with 0.5*X normal episode + 0.5*X SimCLR episode.
==================================================

Focused review:

1. I am still not clear about the contribution of this paper. The final goal of this paper is to estimate the subspace S_{Y*|X}, and many existing papers already compute this subspace S_{Y*|X}. The result in this paper only provides an alternative to compute this subspace using ideas from optimal transport. It is unclear to me how different methods to compute the same object can lead to better results on the downstream task after being properly adjusted for numerical errors. Unfortunately, throughout the whole paper, the authors do not provide any explanation/intuition on why their method can systematically perform better than the competitors. I thus feel skeptical about the scientific contribution of this paper, and I still wonder how POTD can outperform in Table 1. 2. There is a huge gap between Section 3 and Section 4. The main idea of Section 3 is for binary output, however, Section 4 generalizes to k-ary output. This generalization is not justified and not well explained: - what is the role of the matrix Lambda? - what are the properties of Lambda (dimensions, symmetry, etc.)? - why should the r singular vectors of Lambda will span S_{Y*|X}?

Review Point: 1. I am still not clear about the contribution of this paper. The final goal of this paper is to estimate the subspace S_{Y*|X}, and many existing papers already compute this subspace S_{Y*|X}. The result in this paper only provides an alternative to compute this subspace using ideas from optimal transport. It is unclear to me how different methods to compute the same object can lead to better results on the downstream task after being properly adjusted for numerical errors. Unfortunately, throughout the whole paper, the authors do not provide any explanation/intuition on why their method can systematically perform better than the competitors. I thus feel skeptical about the scientific contribution of this paper, and I still wonder how POTD can outperform in Table 1.
Review Point: 2. There is a huge gap between Section 3 and Section 4. The main idea of Section 3 is for binary output, however, Section 4 generalizes to k-ary output. This generalization is not justified and not well explained:
Review Point: - what are the properties of Lambda (dimensions, symmetry, etc.)?
Review Point: - why should the r singular vectors of Lambda will span S_{Y*|X}?
==================================================

Focused review:

The only important thing I am really missing in the paper is an evaluation of the runtime of the method. Especially given that all pairs of detected corners are considered for the generation of open curve proposals. A clear limitation, pointed out by the authors, is that the presented version of the method uses a limited set of geometric primitives, e.g., ellipses are not included. However, the reasons for this are explained and reasonable. It would be useful if authors could add a sentence or two about how this could be patched and if they would expect some difficulties. Another downside is the method involves relatively many ad-hoc/heuristic bits. Specific points: - I find it unfortunate that important parts of the paper are in the supplementary. The closed curve proposal generation is at least referred to from the main text, however, the proposal selection is just omitted without mention. There should be at least a sentence pointing to the supplement. - An explanation of the clustering (L129) could be improved, as of now I would reimplement it using Euclidean distance and a maximum linkage threshold of \delta. Please check this is the intended way and explain better otherwise.

Review Point: - I find it unfortunate that important parts of the paper are in the supplementary. The closed curve proposal generation is at least referred to from the main text, however, the proposal selection is just omitted without mention. There should be at least a sentence pointing to the supplement.
Review Point: - An explanation of the clustering (L129) could be improved, as of now I would reimplement it using Euclidean distance and a maximum linkage threshold of \delta. Please check this is the intended way and explain better otherwise.
==================================================

Focused review:

- The main weakness of the paper is the fact that efficiency is not discussed. Although several efforts were made to limit the size of the search space (reduced to 16^2=256 in the experiments), it is still unclear how costly it is to train the child network with each of these 256 policies. Covering the question of runtime (in particular for the policy search phase) is needed to assess the practicality of the approach. - I did not see any mention of the fact that the source code would be publicly released, should the paper be accepted. Yet, although such open-sourcing effort would be appreciated, this is not too detrimental to the work as the data augmentation scheme seems simple enough to be easily re-implemented.

Review Point: - The main weakness of the paper is the fact that efficiency is not discussed. Although several efforts were made to limit the size of the search space (reduced to 16^2=256 in the experiments), it is still unclear how costly it is to train the child network with each of these 256 policies. Covering the question of runtime (in particular for the policy search phase) is needed to assess the practicality of the approach.
Review Point: - I did not see any mention of the fact that the source code would be publicly released, should the paper be accepted. Yet, although such open-sourcing effort would be appreciated, this is not too detrimental to the work as the data augmentation scheme seems simple enough to be easily re-implemented.
==================================================

Focused review:

Weaknesses. • The primary disadvantage is the small incremental benefit for some of the steps of the method. For example, the ablation study in Table 6 shows only a 0.07 dB improvement on Set5 for ACLA compared to CLA. Given that ACLA requires neural architecture search, dynamic key selection and more it may be a lot of work for limited benefit to squeeze out less than 0.1 dB improvement.
Small corrections and suggestions:
• The first sentence in the introduction describes degradation as “irreversible”. But the idea of the image restoration is to restore the degradation. If the image is restored, isn’t the degradation reversed? Perhaps this qualifying statement could be removed or clarified.
• Page 1 argues that most CNN-based image restoration methods tend to produce smooth results, potentially due to only referring to keys in the same layer in their attention modules. This seems highly speculative – it would be better to show this is the case or possibly remove this. It’s well known there is a perception-distoration tradeoff in image restoration; e.g. Blau et al., “The Perception-Distortion Tradeoff,” CVPR 2018 and blurring often results in minimizing distortion using standard losses and can be addressed through generative methods that hallucinate detail at the expense of PSNR.
• Page 4, please change “To find for keys” to “To search for keys”
• Page 5, please change “ALDA” to “ACLA”
• Page 5, please change “top op CLA” to “top of CLA”

Review Point: • The primary disadvantage is the small incremental benefit for some of the steps of the method. For example, the ablation study in Table 6 shows only a 0.07 dB improvement on Set5 for ACLA compared to CLA. Given that ACLA requires neural architecture search, dynamic key selection and more it may be a lot of work for limited benefit to squeeze out less than 0.1 dB improvement. Small corrections and suggestions:
Review Point: • The first sentence in the introduction describes degradation as “irreversible”. But the idea of the image restoration is to restore the degradation. If the image is restored, isn’t the degradation reversed? Perhaps this qualifying statement could be removed or clarified.
Review Point: • Page 1 argues that most CNN-based image restoration methods tend to produce smooth results, potentially due to only referring to keys in the same layer in their attention modules. This seems highly speculative – it would be better to show this is the case or possibly remove this. It’s well known there is a perception-distoration tradeoff in image restoration; e.g. Blau et al., “The Perception-Distortion Tradeoff,” CVPR 2018 and blurring often results in minimizing distortion using standard losses and can be addressed through generative methods that hallucinate detail at the expense of PSNR.
Review Point: • Page 4, please change “To find for keys” to “To search for keys” • Page 5, please change “ALDA” to “ACLA” • Page 5, please change “top op CLA” to “top of CLA”
==================================================

Focused review:

- I understand the paper is not about ensembles but for ensembles for adversarial robustness, but I would like to see some papers that discuss ensembles and variants being cited in related works, since ensembles follow a very rich and established line of research. Namely [1,2,3,4,5]. - Eqn 3 and 4 are similar (to an extent) to the motivation of the proposed objective in [4]. I would like to see that being addressed, and how they are different. - NIT: in alg 1. i prefer \alpha being used instead of "lr" to denote learning rate - Resnet-20 is not a standard resnet variant to my knowledge. Is this similar to resnet-18? - Ensembles are very closely related to Bayesian NN, I would like to see a comment about - Whitebox accuracy performance is very similar to the baselines, but still good. - Fig 5. is also interesting, but is it possible to see other baselines with adv training as well on the same plot? - No error bars in any of the plots is very concerning to me. I much rather prefer seeing error bars to see beyond just the mean performance. [1] https://www.researchgate.net/publication/3191841_Neural_Network_Ensembles [2] https://arxiv.org/abs/1612.01474 [3] https://dl.acm.org/doi/abs/10.1145/1015330.1015385?casa_token=4-SXLz4nfOMAAAAA%3Av-L3CG89mjfUS4bqlPP8jq5evYx0i3uB99un8kwEocktvRSSkKcf4hRFA8shpqR_cHzt2tqbxRFN9A [4] https://arxiv.org/abs/2003.04514 [5] https://arxiv.org/abs/2001.10995

Review Point: - I understand the paper is not about ensembles but for ensembles for adversarial robustness, but I would like to see some papers that discuss ensembles and variants being cited in related works, since ensembles follow a very rich and established line of research. Namely [1,2,3,4,5].
Review Point: - Eqn 3 and 4 are similar (to an extent) to the motivation of the proposed objective in [4]. I would like to see that being addressed, and how they are different.
Review Point: - NIT: in alg 1. i prefer \alpha being used instead of "lr" to denote learning rate - Resnet-20 is not a standard resnet variant to my knowledge. Is this similar to resnet-18?
Review Point: - Ensembles are very closely related to Bayesian NN, I would like to see a comment about - Whitebox accuracy performance is very similar to the baselines, but still good.
Review Point: - Fig 5. is also interesting, but is it possible to see other baselines with adv training as well on the same plot?
Review Point: - No error bars in any of the plots is very concerning to me. I much rather prefer seeing error bars to see beyond just the mean performance. [1] https://www.researchgate.net/publication/3191841_Neural_Network_Ensembles [2] https://arxiv.org/abs/1612.01474 [3] https://dl.acm.org/doi/abs/10.1145/1015330.1015385?casa_token=4-SXLz4nfOMAAAAA%3Av-L3CG89mjfUS4bqlPP8jq5evYx0i3uB99un8kwEocktvRSSkKcf4hRFA8shpqR_cHzt2tqbxRFN9A [4] https://arxiv.org/abs/2003.04514 [5] https://arxiv.org/abs/2001.10995
==================================================

Focused review:

- The paper could benefit from a better practical motivation, in its current form it will be quite hard for someone who is not at home in this field to understand why they should care about this work. What are specific practical examples in which the proposed algorithm would be beneficial? - The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function. - More generally, the discussion of the simulation study results could be strenghtened. It is not really clear what the reader should take away from the results, and some discussion could help a lot with interpreting them properly.

Review Point: - The paper could benefit from a better practical motivation, in its current form it will be quite hard for someone who is not at home in this field to understand why they should care about this work. What are specific practical examples in which the proposed algorithm would be beneficial?
Review Point: - The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function.
Review Point: - More generally, the discussion of the simulation study results could be strenghtened. It is not really clear what the reader should take away from the results, and some discussion could help a lot with interpreting them properly.
==================================================

Focused review:

- The algorithm assumes knowledge of the entire training dataset, which seems unrealistic. It would be nice to see how the attack performance degrades if the adversary only has partial knowledge of the training data. - The attack works for a fixed target image known at training time. While this may suffice for some applications (e.g., the watermarking idea at the end of the introduction), this seems insufficient for others (e.g., evading a facial recognition software, which will take a "live" picture of you at some point in the future).

Review Point: - The algorithm assumes knowledge of the entire training dataset, which seems unrealistic. It would be nice to see how the attack performance degrades if the adversary only has partial knowledge of the training data.
Review Point: - The attack works for a fixed target image known at training time. While this may suffice for some applications (e.g., the watermarking idea at the end of the introduction), this seems insufficient for others (e.g., evading a facial recognition software, which will take a "live" picture of you at some point in the future).
==================================================

Focused review:

- This paper missed some previous related works. For example, the work "Multi-objective neural architecture search via predictive network performance optimization" also proposed to use GCN for architecture performance prediction (see https://openreview.net/forum?id=rJgffkSFPS). Therefore, the idea of using GCN based prediction is not novel. Pls discuss the key differences. - Besides the Benchmarks, pls add experiments on real search space, such as DARTS space or MobileNetV3 space. The benchmark is very small, and easy to overfit. For example, if you direct select the large FLOPs models in Benchmarks, the performance is very competitive, surpassing many SOTAs. Add experiments on ImageNet. - Move the details of latency prediction model and binary relative accuracy predictor from supplementary material to main manuscript, because these two parts are important for the work. - No explanation how to address the problem when the cycle exists in binary relative accuracy predictor. For example, acc(m1) > acc(m2), acc(m2) > acc(m3), acc(m3) > acc(m1)

Review Point: - This paper missed some previous related works. For example, the work "Multi-objective neural architecture search via predictive network performance optimization" also proposed to use GCN for architecture performance prediction (see https://openreview.net/forum?id=rJgffkSFPS). Therefore, the idea of using GCN based prediction is not novel. Pls discuss the key differences.
Review Point: - Besides the Benchmarks, pls add experiments on real search space, such as DARTS space or MobileNetV3 space. The benchmark is very small, and easy to overfit. For example, if you direct select the large FLOPs models in Benchmarks, the performance is very competitive, surpassing many SOTAs. Add experiments on ImageNet.
Review Point: - Move the details of latency prediction model and binary relative accuracy predictor from supplementary material to main manuscript, because these two parts are important for the work.
Review Point: - No explanation how to address the problem when the cycle exists in binary relative accuracy predictor. For example, acc(m1) > acc(m2), acc(m2) > acc(m3), acc(m3) > acc(m1)
==================================================

Focused review:

* Empirical evaluation of this paper is relatively weak and more discussions of the results are needed to support the claims the authors made. For example, most of the improvements are marginal on Goodreads and LastFM (the number scale in Table 1 is not consistent). Please include statistical tests for these results. Also, why ACL-MLP with a MLP exposure model outperform MLP with an Oracle exposure model, what does this indicate? * PS is a rather basic baseline for comparison. I would be curious to see how ExpoMF [1] and follow up works such as [2] compare to the purposed adversarial learning method * The organization of the paper could be improved. Some of the theoretical results could be referred to in the appendix and some results should be further discussed (e.g. Theorem 1) while more explanations on the empirical results could be discussed in the main paper. [1] Liang, D., Charlin, L., McInerney, J., & Blei, D. M. (2016 ). Modeling user exposure in recommendation. In Proceedings of the 25th international conference on World Wide Web [2] Wang, M., Gong, M., Zheng, X., & Zhang, K. (2018). Modeling dynamic missingness of implicit feedback for recommendation. In Advances in neural information processing systems

Review Point: * Empirical evaluation of this paper is relatively weak and more discussions of the results are needed to support the claims the authors made. For example, most of the improvements are marginal on Goodreads and LastFM (the number scale in Table 1 is not consistent). Please include statistical tests for these results. Also, why ACL-MLP with a MLP exposure model outperform MLP with an Oracle exposure model, what does this indicate?
Review Point: * PS is a rather basic baseline for comparison. I would be curious to see how ExpoMF [1] and follow up works such as [2] compare to the purposed adversarial learning method * The organization of the paper could be improved. Some of the theoretical results could be referred to in the appendix and some results should be further discussed (e.g. Theorem 1) while more explanations on the empirical results could be discussed in the main paper. [1] Liang, D., Charlin, L., McInerney, J., & Blei, D. M. (2016 ). Modeling user exposure in recommendation. In Proceedings of the 25th international conference on World Wide Web [2] Wang, M., Gong, M., Zheng, X., & Zhang, K. (2018). Modeling dynamic missingness of implicit feedback for recommendation. In Advances in neural information processing systems
==================================================

Focused review:

weakness of this paper: 1 .
There lacks novel proposal of network architecture, which might limit the novelty of this paper. Here, it is worthwhile noting that the novel definition of loss function does qualify as a good contribution. However, the architecture of neural network is not novel. 2 .
Author discussed the applicability of the propose methods. However, the limitation of applicability seems to be obvious as the authors stated. But, it is rather an unfamiliar area for me.

Review Point: 2 . Author discussed the applicability of the propose methods. However, the limitation of applicability seems to be obvious as the authors stated. But, it is rather an unfamiliar area for me.
==================================================

Focused review:

- There are several design choices that could have had a game-changer effect on the human study. This primarily includes the choice of the way the explanation results are illustrated for various methods on different datasets/tasks. Questions like the following can be asked: would Shap receive a better rating if the superimposition was of higher resolution? Would attribution techniques receive a better rating if the explanations were demonstrated using different/less colors? A suggestion here could be to design and use various options in the human study and include the research question of “what is the best visualization of an explanation type” as part of the human study since it seems to be hard to disentangle the two. - The reviewer is not sure about the pertinence of the use of non-expert end-users in the human study, especially for expert decisions such as arrhythmia diagnosis using ECGs. Wouldn’t it be better to conduct such studies with the users that are actually going to be interested and are able to judge explanations appropriately? - The number of datasets and number of comparisons seem too limited for a human study. The reviewer expected orders of magnitude more comparisons (Than ~5000) especially when using non-expert M-Turk users.

Review Point: - There are several design choices that could have had a game-changer effect on the human study. This primarily includes the choice of the way the explanation results are illustrated for various methods on different datasets/tasks. Questions like the following can be asked: would Shap receive a better rating if the superimposition was of higher resolution? Would attribution techniques receive a better rating if the explanations were demonstrated using different/less colors? A suggestion here could be to design and use various options in the human study and include the research question of “what is the best visualization of an explanation type” as part of the human study since it seems to be hard to disentangle the two.
Review Point: - The reviewer is not sure about the pertinence of the use of non-expert end-users in the human study, especially for expert decisions such as arrhythmia diagnosis using ECGs. Wouldn’t it be better to conduct such studies with the users that are actually going to be interested and are able to judge explanations appropriately?
Review Point: - The number of datasets and number of comparisons seem too limited for a human study. The reviewer expected orders of magnitude more comparisons (Than ~5000) especially when using non-expert M-Turk users.
==================================================

Focused review:

- The curves shown in Figure 4 does show an improved convergence rate (28.6% as mentioned in the caption) w.r.t the single reset policy baseline of prior work, but just the number does not convey whether this is a significant improvement for practical purposes. Given that a reset-free task does not require human-assisted reset for say, a real robotic experiment, is this really a significant boost given that the more complicated (and potentially more difficult to learn) objective i.e. the added diversity term? I would be more convinced with experiments that show the stability of the proposed objective, given that it is adversarial in nature while also enforcing the learning of a set of unsupervised skills. - The supplementary material mentions that 2 skills were learnt with DClaw and 10 skills were learnt with the Ant env. The importance of this hyperparameter is not mentioned in the paper, nor is the process of selecting it. Also, are all 10 skills learnt in the Ant environment distinct? Is there a curriculum in number of skills learnt over training? - L205 mentions that hyperparameter \lambda being set to 0 reduces to a prior skill learning method (e.g. DIAYN). It seems like this setting can also be a valid candidate for a reset-free learning method, and should have been compared with in Fig 4.

Review Point: - The curves shown in Figure 4 does show an improved convergence rate (28.6% as mentioned in the caption) w.r.t the single reset policy baseline of prior work, but just the number does not convey whether this is a significant improvement for practical purposes. Given that a reset-free task does not require human-assisted reset for say, a real robotic experiment, is this really a significant boost given that the more complicated (and potentially more difficult to learn) objective i.e. the added diversity term? I would be more convinced with experiments that show the stability of the proposed objective, given that it is adversarial in nature while also enforcing the learning of a set of unsupervised skills.
Review Point: - The supplementary material mentions that 2 skills were learnt with DClaw and 10 skills were learnt with the Ant env. The importance of this hyperparameter is not mentioned in the paper, nor is the process of selecting it. Also, are all 10 skills learnt in the Ant environment distinct? Is there a curriculum in number of skills learnt over training?
Review Point: - L205 mentions that hyperparameter \lambda being set to 0 reduces to a prior skill learning method (e.g. DIAYN). It seems like this setting can also be a valid candidate for a reset-free learning method, and should have been compared with in Fig 4.
==================================================

Focused review:

Weaknesses: 1. The description of the method is not very clear. For example, Eq.(3) uses index i
for both visual prompt h ^
and images I i
. However it seems that i
in the LHS denotes the index of prompts (i.e. i=1,2,3,…,K), while the item in the RHS denotes the index of batch sample (i.e. i=1,2,3,…,B). I am not sure if the notations are abused in this equation as well as in Eq.(4). Besides, the structure of the PGN is not clear. The authors mention that they use Res10 as the backbone network of PGN, but it is not explained that how can the output of Res10 be transformed into multiple probability distributions that indicate the composition of each prompt. Are multiple Res10 used in this process, or there is an extra mapping function? 2. The authors mention in the introduction that compared with direct finetuning, using prompts can help solve catastrophic forgetting. However there is no experiments in this paper supporting this claim. 3. Efficiency is one of the most highlighted characteristic of the proposed method, as illustrated in the title. I can barely agree with this point since the size of parameters introduced by the proposed method is about 13 times by the visual prompt, and also much larger than linear fintuning. The improvement as shown in Tab.5 is not surprising when considering the huge amount of parameters. 4. It seems that in Tab.1 and Tab.5 VP and the proposed PGN share the same number of prompts (K). I am not sure if this is fair enough for comparison. In fact, the proposed method can be seen as an input-dependent masking mechanism on N prompts if the distribution in Eq.(4) is a sharp one. Therefore there is always N alternative prompts that can be used in this model. Using K prompts instead of N in VP definitely has lower capacity, which can possibly lead to worse performance. 5. It would be better if the authors could reorganize the space between figures and captions in Fig.3 and Fig.4.

Review Point: 1. The description of the method is not very clear. For example, Eq.(3) uses index i for both visual prompt h ^ and images I i . However it seems that i in the LHS denotes the index of prompts (i.e. i=1,2,3,…,K), while the item in the RHS denotes the index of batch sample (i.e. i=1,2,3,…,B). I am not sure if the notations are abused in this equation as well as in Eq.(4). Besides, the structure of the PGN is not clear. The authors mention that they use Res10 as the backbone network of PGN, but it is not explained that how can the output of Res10 be transformed into multiple probability distributions that indicate the composition of each prompt. Are multiple Res10 used in this process, or there is an extra mapping function?
Review Point: 2. The authors mention in the introduction that compared with direct finetuning, using prompts can help solve catastrophic forgetting. However there is no experiments in this paper supporting this claim.
Review Point: 3. Efficiency is one of the most highlighted characteristic of the proposed method, as illustrated in the title. I can barely agree with this point since the size of parameters introduced by the proposed method is about 13 times by the visual prompt, and also much larger than linear fintuning. The improvement as shown in Tab.5 is not surprising when considering the huge amount of parameters.
Review Point: 4. It seems that in Tab.1 and Tab.5 VP and the proposed PGN share the same number of prompts (K). I am not sure if this is fair enough for comparison. In fact, the proposed method can be seen as an input-dependent masking mechanism on N prompts if the distribution in Eq.(4) is a sharp one. Therefore there is always N alternative prompts that can be used in this model. Using K prompts instead of N in VP definitely has lower capacity, which can possibly lead to worse performance.
Review Point: 5. It would be better if the authors could reorganize the space between figures and captions in Fig.3 and Fig.4.
==================================================

Focused review:

This paper seems to ignore two important approaches for faster credit assignment in reinforcement learning. As discussed by Ostrovski et al. (2017), "it is well known that better performance, both in terms of learning efficiency and approximation error, is attained by multi-step methods (Sutton, 1996; Tsitsiklis & van Roy, 1997). These methods interpolate between one-step methods (Q-Learning) and the Monte-Carlo update." A simple instantiation of this method that is used throughout is the mixed monte-carlo return (see Ostrovski et al., 2017 for a careful discussion). Another technique equally relevant is the n-step returns so common in deep reinforcement learning. Hessel et al. (2018) recently discussed it and showed its benefits. This reference is just one of the several that use n-step returns. All in all, I don't understand why these two methods are not discussed and they are not used as baselines. My score is mostly tied to this limitation. Specifically, some of the questions this paper doesn't answer are: 1. How is IRCR different from Mixed Monte-Carlo returns and n-step returns? 2. What are its benefits? 3. How does it compare to these other approaches empirically? References: Georg Ostrovski, Marc G. Bellemare, Aäron van den Oord, Rémi Munos: Count-Based Exploration with Neural Density Models. ICML 2017: 2721-2730 Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, David Silver: Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI 2018: 3215-3222

Review Point: 1. How is IRCR different from Mixed Monte-Carlo returns and n-step returns?
Review Point: 3. How does it compare to these other approaches empirically? References: Georg Ostrovski, Marc G. Bellemare, Aäron van den Oord, Rémi Munos: Count-Based Exploration with Neural Density Models. ICML 2017: 2721-2730 Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, David Silver: Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI 2018: 3215-3222
==================================================

Focused review:

1) I am not sure I would call the approach black box in the strict sense, as there is still a fair amount of tailor made analysis for each setting (and the algorithm modification, although intuitive, still requires a deep understanding of the problem). 2) Covering problems are generally very well understood in the online literature. Are there any interesting covering problems left for others to apply this technique?

Review Point: 1) I am not sure I would call the approach black box in the strict sense, as there is still a fair amount of tailor made analysis for each setting (and the algorithm modification, although intuitive, still requires a deep understanding of the problem).
Review Point: 2) Covering problems are generally very well understood in the online literature. Are there any interesting covering problems left for others to apply this technique?
==================================================

Focused review:

Weaknesses:
At the outset, and given 2, Neurips might not be the best fit for this submission. I would expect COLT, theoretical CS conferences STOC/FOCS and of course, a number of journals, on the other hand to be quite well-suited.
The interesting and generalizable part of the paper seem to be some of the proof techniques, and these are relegated to the appendix. I did not have time to read the full appendix (65 pages) but the first 15-20 were useful.
Separately from 1, the paper is made inaccessible by a somewhat loose use of notation that is widespread. As a small example, after using 'd' for dimension, the authors also use it for 'degree' of the SOS program in the beginning of Sec. 2. For accessibility to a stats/ml audience, this paper would require significant polishing.
Related to 3, I trust the results despite point 2, but some of the main results in the text (Theorems 3.1 and 3.2) likely should be rephrased.

Review Point: 2. For accessibility to a stats/ml audience, this paper would require significant polishing. Related to 3, I trust the results despite point 2, but some of the main results in the text (Theorems 3.1 and 3.2) likely should be rephrased.
==================================================

Focused review:

Weaknesses
1). My first concern is about the unrealistic assumptions. For example, Eq (5) “channel condition” requires g1 * g2 = C2, which doesn’t make sense to me: there is no intuition, and most existing convs doesn’t satisfy this assumption: (1) regular conv g1=g2=1 != C2 doesn’t satisfy this; (2) spatial separable conv g1=g2=1 != C2. This assumption is critical to arrive equation (7) and (8), but is unclear where this assumption comes from. Due to these unrealistic assumptions, the term “optimal” is also questionable.
2). Second, the CIFAR results show the new layers are not much better than others. As shown in Figure 3, the largest gain is <1%, and sometimes the o-ResNet (~88%) is slightly worse than d-ResNet (which indicates the propose layers might be not "optimal"?) The improvements on ImageNet in Table 4 seem to be promising, but as discussed in DARTS+ and other recent works, the search process of DARTS is often unstable and could potentially have high variance.
3). My another main concern is about the weak baseline. As this paper is study separable convs, it should compare to separable conv based models like MobileNet/FBNet/EfficientNet, rather than the full conv based ResNet. For example, by leveraging depthwise and seprable convs, MobileNetV3 achieves 75.2% ImageNet top-1 accuracy with 219 FLOPs, which is a much stronger baseline than the on in Table4. I highly recommend the authors to conduct their experiments on these baselines.
======== Suggestions
1). Instead of formulating it as a mathematically optimal solution based on unrealistic assumptions, I recommend the authors to conduct more empirical studies on these design choices. For example, the paper only shows the performance results of “optimal” (g1, g2) computed by equation (7), but it would be helpful to show the performance for different (g1, g2) values, and compare them with the “optimal” (g1, g2).
2). I recommend the authors to use the latest MobileNet or EfficientNet (or other separable conv based models) as baselines, and replace their separable convs with the proposed “optimal separable convs”, and compare the performance gains.

Review Point: 1). My first concern is about the unrealistic assumptions. For example, Eq (5) “channel condition” requires g1 * g2 = C2, which doesn’t make sense to me: there is no intuition, and most existing convs doesn’t satisfy this assumption: (1) regular conv g1=g2=1 != C2 doesn’t satisfy this; (2) spatial separable conv g1=g2=1 != C2. This assumption is critical to arrive equation (7) and (8), but is unclear where this assumption comes from. Due to these unrealistic assumptions, the term “optimal” is also questionable.
Review Point: 2). Second, the CIFAR results show the new layers are not much better than others. As shown in Figure 3, the largest gain is <1%, and sometimes the o-ResNet (~88%) is slightly worse than d-ResNet (which indicates the propose layers might be not "optimal"?) The improvements on ImageNet in Table 4 seem to be promising, but as discussed in DARTS+ and other recent works, the search process of DARTS is often unstable and could potentially have high variance.
Review Point: 3). My another main concern is about the weak baseline. As this paper is study separable convs, it should compare to separable conv based models like MobileNet/FBNet/EfficientNet, rather than the full conv based ResNet. For example, by leveraging depthwise and seprable convs, MobileNetV3 achieves 75.2% ImageNet top-1 accuracy with 219 FLOPs, which is a much stronger baseline than the on in Table4. I highly recommend the authors to conduct their experiments on these baselines. ======== Suggestions 1). Instead of formulating it as a mathematically optimal solution based on unrealistic assumptions, I recommend the authors to conduct more empirical studies on these design choices. For example, the paper only shows the performance results of “optimal” (g1, g2) computed by equation (7), but it would be helpful to show the performance for different (g1, g2) values, and compare them with the “optimal” (g1, g2).
Review Point: 2). I recommend the authors to use the latest MobileNet or EfficientNet (or other separable conv based models) as baselines, and replace their separable convs with the proposed “optimal separable convs”, and compare the performance gains.
==================================================

Focused review:

- The main weakness is that the explanation of the methodology is confusing (Section 2), which I believe has to do with the order of how things are explained (e.g., algorithm before notation), the lack of top level diagram(s) which incorporate notation from the writing, and the sheer quantity of symbols and notation used is overwhelming. Specifically, without diagram(s), the interplay between g_t, m, \varrho, p, R is unnecessarily confusing. The method seems more complex than it actually is as a result of the writing. - Inadequate description of related works - It is unclear if the results change significantly when different pretrained networks to attack are considered? Results for only 1 network per dataset are shown, and although there is a citation for it, the NIN network seems somewhat atypical to use as the one and only model to evaluate CIFAR10 results on. What about very common networks like RN50, VGG16, DenseNet?

Review Point: - The main weakness is that the explanation of the methodology is confusing (Section 2), which I believe has to do with the order of how things are explained (e.g., algorithm before notation), the lack of top level diagram(s) which incorporate notation from the writing, and the sheer quantity of symbols and notation used is overwhelming. Specifically, without diagram(s), the interplay between g_t, m, \varrho, p, R is unnecessarily confusing. The method seems more complex than it actually is as a result of the writing.
Review Point: - Inadequate description of related works - It is unclear if the results change significantly when different pretrained networks to attack are considered? Results for only 1 network per dataset are shown, and although there is a citation for it, the NIN network seems somewhat atypical to use as the one and only model to evaluate CIFAR10 results on. What about very common networks like RN50, VGG16, DenseNet?
==================================================

Focused review:

Weaknesses:
It is not clear if this framework only applies to unweighted or to both unweighted and weighted graphs. This is an important distinction because while in unweighted graphs it is not possible to discern whether high-degree nodes stem from high sampling density or large neighborhood radius, in weighted graphs the sampling density is more related to the number of edges incident to each node, and the neighborhood radius to their edge weights. Related to this, the paper does not include any numerical results on weighted graphs.
The "geometric graph with hubs" model is not motivated by real-world networks, but instead, by the ability to decouple the contribution of the sampling density and the neighborhood radius. While the model works well empirically on the synthetic/citation networks considered in the experiments, there is no discussion on whether its assumptions, in particular the slowly varying density assumption, are realistic. For example, it would be important to understand whether canonical graph models typically used to model real-world graphs---such as small-world, preferential attachment, household, etc.---can be modeled as geometric graphs with hubs.
The numerical experiments are lacking:
The link prediction experiments are arguably the most important experiments, as they demonstrate the validity of the model. However, they are only performed on synthetic graphs and on relatively simple (low rank) citation networks, which are embedded in R 2
. It would be interesting to see link prediction experiments on more complex networks (perhaps the AIDS dataset?), and with larger embedding dimension.
The assumption motivating the self-supervised learning approach---that the task depends mostly on the underlying continuous model---is not rigorously justified, and may be too strong, e.g., in heterophilous graphs.
By definition, geometric graphs with hubs have more degrees of freedom than the correlation (inner product) and conventional geometric graphs with which they are compared in Fig. 2. If possible, the authors should include comparisons with other graph models with more degrees of freedom.
The presentation is lacking and certain concepts are not well-defined, see below.

Review Point: 2. If possible, the authors should include comparisons with other graph models with more degrees of freedom. The presentation is lacking and certain concepts are not well-defined, see below.
==================================================

Focused review:

1. The motivation of this work is very unclear. The author assume both labelling and collecting data are not trivial. I can understand labeling a large image for segmentation task is not easy. But there are millions of pixels in one image. But I don't see any reason that collecting images is hard as well. The author does not give any explanation about it. As the author works on a new problem, it is quite crucial to me that the motivation to do it makes sense, otherwise why we consider this problem. 2. It is not quite clear to me how the single target image is used. After I read the paper especially Figure 2 and 3, I don't find any clear description about how to use the target sample. The author mentioned that they use the target sample as an anchor image to generate similar stylized images. But it is not clear to me how to achieve it. It seems to me that the author just use Gaussian Random noise to generate new images and feedback from the loss is used to guide how to generate noise.

Review Point: 1. The motivation of this work is very unclear. The author assume both labelling and collecting data are not trivial. I can understand labeling a large image for segmentation task is not easy. But there are millions of pixels in one image. But I don't see any reason that collecting images is hard as well. The author does not give any explanation about it. As the author works on a new problem, it is quite crucial to me that the motivation to do it makes sense, otherwise why we consider this problem.
Review Point: 2. It is not quite clear to me how the single target image is used. After I read the paper especially Figure 2 and 3, I don't find any clear description about how to use the target sample. The author mentioned that they use the target sample as an anchor image to generate similar stylized images. But it is not clear to me how to achieve it. It seems to me that the author just use Gaussian Random noise to generate new images and feedback from the loss is used to guide how to generate noise.
==================================================

Focused review:

Weaknesses:
The main issue with this work is that the evaluation setup is not realistic at all. For an experimental paper like this, verifying its applicability on real-world datasets is important. Yet, 2 datasets are synthetically generated and only 1 is of real birds. This birds dataset, too, is very simple, in that the feature is very easily identifiable (the beak), and it is not clear if this method scales to more realistic distributions where the features are not as simple.
Another huge issue is that experiments are only conducted at the extremely small-sample regime, up to 500 samples on the synthetic datasets of shapes and up to 60 examples on the bird dataset. No one is deploying machine learning trained on 60 samples. If the method was to train on all labeled data, and only incorporate some additional explanations, then that would be much more reasonable. But that is not what is happening here. Advice:
The idea of leveraging a few human annotations to increase performance is interesting, but the rest of the paper needs to be completely reworked. Here's what a great version of this paper would look like:
Consider a suite of real-world datasets, such as those in the WILDS benchmark. Do not include any synthetic data experiments (they add no value) and report performance on the specific metric for each dataset. Another benefit of this is that experiments are run on non-binary tasks as well.
Train on all available labeled data. The WILDS dataset contains training data splits. You should compare two main methods primarily: 1) the baseline of training on the labeled data, and 2) the new method of training on the labeled data, plus incorporating input mask explanation annotations for a few (say, 60) examples.
Use modern backbone baselines (say, Resnet50 or DenseNet121) for the feature extraction layer - 3 conv layers is definitely too small for anything non-synthetic.
I have to say that even given this version of the idea, I am skeptical this would work (lots of such robustness/domain invariance interventions have been proposed and have failed). But this is just my opinion, my advice, and the rest of this review is independent of this viewpoint.

Review Point: 1) the baseline of training on the labeled data, and 2) the new method of training on the labeled data, plus incorporating input mask explanation annotations for a few (say, 60) examples. Use modern backbone baselines (say, Resnet50 or DenseNet121) for the feature extraction layer - 3 conv layers is definitely too small for anything non-synthetic. I have to say that even given this version of the idea, I am skeptical this would work (lots of such robustness/domain invariance interventions have been proposed and have failed). But this is just my opinion, my advice, and the rest of this review is independent of this viewpoint.
==================================================

Focused review:

Weakness: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world. 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling.  Detailed Comments: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.  

Review Point: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world.
Review Point: 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments:
Review Point: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.
==================================================

Focused review:

weakness of the paper to me is that I found the flow of the mathematical argument at times hard to follow. For ex.:
- the role and usefulness of Lemma 1 and Corollary 1.1 appears much later than where they are introduced.
- It is not shown that Eq 8 corresponds to a lower bound on the log-likelihood (as Eq 1 is known to be); is it or is it not? The argumentation of l 144 to 148 feels a little convoluted and indirect. I agree that Eq 8 is maximized when p(x,z) and q(x,z) match, but it is unclear to me, and from the paper, what the tradeoff between its two terms achieves when q does not have the capacity to match p. This should be discussed.
l 183: Proposition 1. "equilibrium â¦ is achieved"
About GAN objectives you mentioned earlier in l 119 that "This objective mismatch may lead to the well-known instability issues associated with GAN training". Now your objective in Eq 12 uses a similar min-max objective. Couldn't there be similar instability or collapse issues? 
l 238: experimental evaluation of NLL, "estimated via the variational lower bound"; is this with the traditional VAE variational lower bound of Eq 1 ? Or with some later equation?
In several places, you specify + const. It would be useful to briefly afterwards state what that const is and that it is constant w.r.t. what, as this is not always clear. 
There are a number of errors in some of the equations:
l 145: "E_p(z) log p_\theta(z) = " should be "E_p(z) log q_\phi(z) = "
Eq 10: I think $log q_\phi(x|z)$ should be $log q_\phi(z|x)$ 
Eq 11: has the term ordering (hence the sign) reversed in comparison to Eq 10. It should be changed to the same terms ordering as Eq 10 (to be consistent with Eq 10 and Eq. 12).
l 150,152: "is not possible, as it requires an explicit form for", not only that, log p_\theta(x) is intractable.

Review Point: - the role and usefulness of Lemma 1 and Corollary 1.1 appears much later than where they are introduced.
Review Point: - It is not shown that Eq 8 corresponds to a lower bound on the log-likelihood (as Eq 1 is known to be); is it or is it not? The argumentation of l 144 to 148 feels a little convoluted and indirect. I agree that Eq 8 is maximized when p(x,z) and q(x,z) match, but it is unclear to me, and from the paper, what the tradeoff between its two terms achieves when q does not have the capacity to match p. This should be discussed. l 183: Proposition 1. "equilibrium â¦ is achieved" About GAN objectives you mentioned earlier in l 119 that "This objective mismatch may lead to the well-known instability issues associated with GAN training". Now your objective in Eq 12 uses a similar min-max objective. Couldn't there be similar instability or collapse issues? l 238: experimental evaluation of NLL, "estimated via the variational lower bound"; is this with the traditional VAE variational lower bound of Eq 1 ? Or with some later equation? In several places, you specify + const. It would be useful to briefly afterwards state what that const is and that it is constant w.r.t. what, as this is not always clear. There are a number of errors in some of the equations: l 145: "E_p(z) log p_\theta(z) = " should be "E_p(z) log q_\phi(z) = " Eq 10: I think $log q_\phi(x|z)$ should be $log q_\phi(z|x)$ Eq 11: has the term ordering (hence the sign) reversed in comparison to Eq 10. It should be changed to the same terms ordering as Eq 10 (to be consistent with Eq 10 and Eq. 12). l 150,152: "is not possible, as it requires an explicit form for", not only that, log p_\theta(x) is intractable.
==================================================

Focused review:

weaknesses:
* It is not clear if the ability of the model to detect fall height is because of the absolute timing of the simulations. Falling from a greater height leads to a longer delay before the first impact. This is obvious to an algorithm analyzing fixed-sized wav files, but not to a human listening to sound files with somewhat unknown silent beginnings. A fairer comparison would be to add a random amount of delay before starting the sounds for both listeners.
* The comparison method is changed between the synthetic and real tasks, which seems unfair. If it is necessary to use a more complex comparison method for the real task, then also use it for the synthetic one.
* Line 226 reports several analysis parameters in samples, but never states the sample rate. Please describe these quantities in seconds or ms or provide the sample rate so the reader can perform the conversion themselves.
Overall, this is a strong paper that has gotten a relatively old and appealing idea to work much better than in the past.

Review Point: * It is not clear if the ability of the model to detect fall height is because of the absolute timing of the simulations. Falling from a greater height leads to a longer delay before the first impact. This is obvious to an algorithm analyzing fixed-sized wav files, but not to a human listening to sound files with somewhat unknown silent beginnings. A fairer comparison would be to add a random amount of delay before starting the sounds for both listeners.
Review Point: * The comparison method is changed between the synthetic and real tasks, which seems unfair. If it is necessary to use a more complex comparison method for the real task, then also use it for the synthetic one.
Review Point: * Line 226 reports several analysis parameters in samples, but never states the sample rate. Please describe these quantities in seconds or ms or provide the sample rate so the reader can perform the conversion themselves. Overall, this is a strong paper that has gotten a relatively old and appealing idea to work much better than in the past.
==================================================

Focused review:

The authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. 
- L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557). 

Review Point: - L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557).
==================================================

Focused review:

1) Given my current understanding of the paper and the authors' claims, I'm concerned that the use of ENAS may be superfluous, and that equally good results could be obtained by skipping ENAS (Line 11 of Algorithm 1) and always selecting the largest possible architecture in the search space. This is because (a) larger models, properly regularized, are often more accurate than smaller ones, and (b) based on the description of ReLU pruning in Section 3.1, I don't think that increasing the number skip connections in the network should lead to an increase in ReLUs. Nor should increasing the convolutional kernel sizes. The best way to address this comment would be for the authors to run a control experiment where they always select the largest possible architecture in the search space instead of running ENAS. If accuracies are on par with the paper's current results, getting rid of ENAS would allow them to drastically simplify their method (but would likely require a major revision). 2) Additional baselines would further strengthen the paper. For example: the authors currently compare the relative accuracies of network architectures obtained with ReLU pruning vs. ReLU shuffling (Section 4.2 and Table 1). To justify these optimizations, it would be helpful to include ReLU count/accuracy numbers for a baseline which includes neither shuffling nor pruning.

Review Point: 1) Given my current understanding of the paper and the authors' claims, I'm concerned that the use of ENAS may be superfluous, and that equally good results could be obtained by skipping ENAS (Line 11 of Algorithm 1) and always selecting the largest possible architecture in the search space. This is because (a) larger models, properly regularized, are often more accurate than smaller ones, and (b) based on the description of ReLU pruning in Section 3.1, I don't think that increasing the number skip connections in the network should lead to an increase in ReLUs. Nor should increasing the convolutional kernel sizes. The best way to address this comment would be for the authors to run a control experiment where they always select the largest possible architecture in the search space instead of running ENAS. If accuracies are on par with the paper's current results, getting rid of ENAS would allow them to drastically simplify their method (but would likely require a major revision).
Review Point: 2) Additional baselines would further strengthen the paper. For example: the authors currently compare the relative accuracies of network architectures obtained with ReLU pruning vs. ReLU shuffling (Section 4.2 and Table 1). To justify these optimizations, it would be helpful to include ReLU count/accuracy numbers for a baseline which includes neither shuffling nor pruning.
==================================================

Focused review:

1. I think the "intriguing" transition between n^2/3 and n^1/2 is an overstatement. After all, if d grows with the sample size n, then it has to be factored in. d should not be viewed as a fixed constant. Similarly, the signal level s could also be a function of d, and hence, a function of n. 2. It is interesting that in the data rich regime the author can have a regret lower bound sqrt(dn) that is sqrt(s) smaller than the standard sqrt(sdn) lower bound. Note that s may also grow with n and should not be ignored. I am not fully convinced by the tightness of this lower bound. 3. It is not clear how the optimization (4.1) can be done numerically, if x follows absolutely continuous distribution. Some discussion will be helpful. 4. Much of the proofs can be moved to the supplementary materials to make rooms for some numerical studies.

Review Point: 1. I think the "intriguing" transition between n^2/3 and n^1/2 is an overstatement. After all, if d grows with the sample size n, then it has to be factored in. d should not be viewed as a fixed constant. Similarly, the signal level s could also be a function of d, and hence, a function of n.
Review Point: 2. It is interesting that in the data rich regime the author can have a regret lower bound sqrt(dn) that is sqrt(s) smaller than the standard sqrt(sdn) lower bound. Note that s may also grow with n and should not be ignored. I am not fully convinced by the tightness of this lower bound.
Review Point: 3. It is not clear how the optimization (4.1) can be done numerically, if x follows absolutely continuous distribution. Some discussion will be helpful.
Review Point: 4. Much of the proofs can be moved to the supplementary materials to make rooms for some numerical studies.
==================================================

Focused review:

1, It considers the asymptotic setting, but this is fine for this line of research. 2. The theory is limited to linear diagonal network. Whether the conclusion can be generalized to nonlinear network remains unknown. 3.In Theorem 4, Lyu and Li [2020])'s result actually holds for every finite initialization. Why we need to make it go to infinity in equation 4? Moreover, in the main results of this paper, it still needs the initialization to be infinity. A finite initialization results may be more interested here in my opinion. 4. I suggest the author to highlight the technical difficulty of the paper compared to [Woodworth 2019] and [Chizat et al, 2020].

Review Point: 1, It considers the asymptotic setting, but this is fine for this line of research.
Review Point: 2. The theory is limited to linear diagonal network. Whether the conclusion can be generalized to nonlinear network remains unknown.
Review Point: 3.In Theorem 4, Lyu and Li [2020])'s result actually holds for every finite initialization. Why we need to make it go to infinity in equation 4? Moreover, in the main results of this paper, it still needs the initialization to be infinity. A finite initialization results may be more interested here in my opinion. 4. I suggest the author to highlight the technical difficulty of the paper compared to [Woodworth 2019] and [Chizat et al, 2020].
==================================================

Focused review:

Weakness: - The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP). - As the authors also implied, MC is not enforced in the âright wayâ (columns not normalized). I would like the authors to report their MC performance with column normalization for completeness. 

Review Point: - The experiments are insufficient to validate the claim. Only CIFAR10/100 are used, but many of studied techniques that were effective on CIFAR10/100 and MNIST turned out ineffective on other larger datasets/tasks. I would be happy to raise my score if the authors could provide ImageNet improvement (at least for SO and SRIP).
Review Point: - As the authors also implied, MC is not enforced in the âright wayâ (columns not normalized). I would like the authors to report their MC performance with column normalization for completeness.
==================================================

Focused review:

Weaknesses:
-- Only the performances on ImageNet 1K are reported. It is necessary to report the performances on more downstream tasks like object detection or semantic segmentation.
-- Several recent works of vision transformer are missing in Table 2 for performance comparison:
[A] Crossvit: Cross-attention multi-scale vision transformer for image classification, ICCV. 2021. [B] Contextual transformer networks for visual recognition, IEEE TPAMI, 2022. [C] Towards robust vision transformer, CVPR. 2022. [D] Regionvit: Regional-to-local attention for vision transformers, ICLR. 2022 [E] Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning. ECCV, 2022.
-- Only Params and FLOPs are reported. However, it is important to add more metrics of computation cost (like memory consumption and latency) to better understand the trade-offs.
-- Following DeiT, it is necessary to perform evaluation on ImageNet-v2 [F] to measure the level of overfitting.
[F] Do ImageNet Classifiers Generalize to ImageNet? ICML, 2019.

Review Point: -- Only the performances on ImageNet 1K are reported. It is necessary to report the performances on more downstream tasks like object detection or semantic segmentation. -- Several recent works of vision transformer are missing in Table 2 for performance comparison: [A] Crossvit: Cross-attention multi-scale vision transformer for image classification, ICCV. 2021. [B] Contextual transformer networks for visual recognition, IEEE TPAMI, 2022. [C] Towards robust vision transformer, CVPR. 2022. [D] Regionvit: Regional-to-local attention for vision transformers, ICLR.
Review Point: 2022 [E] Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning. ECCV, 2022. -- Only Params and FLOPs are reported. However, it is important to add more metrics of computation cost (like memory consumption and latency) to better understand the trade-offs. -- Following DeiT, it is necessary to perform evaluation on ImageNet-v2 [F] to measure the level of overfitting. [F] Do ImageNet Classifiers Generalize to ImageNet? ICML, 2019.
==================================================

Focused review:

- Persistent homology on brain topology has been studied before, e.g., Chung et al, Persistent homological approach to detecting white matter abnormality in maltreated children: MRI and DTI multimodal study, and many other papers from his group. What is the difference between Chung's work and this one and what novelty does this paper provide other than going from network to 4D? - What is the dimension of the real data? - The paper lacks detailed explanation in the neuroscience background, which supposed to be a very important piece in this paper.

Review Point: - Persistent homology on brain topology has been studied before, e.g., Chung et al, Persistent homological approach to detecting white matter abnormality in maltreated children: MRI and DTI multimodal study, and many other papers from his group. What is the difference between Chung's work and this one and what novelty does this paper provide other than going from network to 4D?
Review Point: - The paper lacks detailed explanation in the neuroscience background, which supposed to be a very important piece in this paper.
==================================================

Focused review:

1) If I understand correctly there is a need to know the word and phoneme segment boundaries for this task. This is a pretty strong assumption and can be unreliable for many languages. The experimentation done by the authors use both ground truth and provided segmentation which I think is good to show that the technique works even with a segmental model. But the authors should rephrase term "mild assumption". 
2) Details about the model training and dataset is missing. Which will make this work accessible to a smaller set of research community. It would be great if the authors can provide code or additional details about the model. 
1) Regarding the related works -- "there is a long line of work that use supervised, multilingual systems" -- it would be good to acknowledge some of the older works too. 
2) Following up on that, there are works that recognize articulatory features, or directly predict phones -- mentioning some of those works would also be useful. 
3) For the results in 5b, it would be good to add some models from the above work for comparison. As different communities would be interested in different aspects of this paper. 
4) There is a recent work on unsupervised speech recognition at Neurips 2021 (https://arxiv.org/pdf/2105.11084.pdf) which does something similar but without the need for segmental acoustic models. It would be good to make a contrast or have a discussion about that for the readers to have a better understanding. 

Review Point: 1) If I understand correctly there is a need to know the word and phoneme segment boundaries for this task. This is a pretty strong assumption and can be unreliable for many languages. The experimentation done by the authors use both ground truth and provided segmentation which I think is good to show that the technique works even with a segmental model. But the authors should rephrase term "mild assumption".
Review Point: 2) Details about the model training and dataset is missing. Which will make this work accessible to a smaller set of research community. It would be great if the authors can provide code or additional details about the model.
Review Point: 1) Regarding the related works -- "there is a long line of work that use supervised, multilingual systems" -- it would be good to acknowledge some of the older works too.
Review Point: 2) Following up on that, there are works that recognize articulatory features, or directly predict phones -- mentioning some of those works would also be useful.
Review Point: 3) For the results in 5b, it would be good to add some models from the above work for comparison. As different communities would be interested in different aspects of this paper.
Review Point: 4) There is a recent work on unsupervised speech recognition at Neurips 2021 (https://arxiv.org/pdf/2105.11084.pdf) which does something similar but without the need for segmental acoustic models. It would be good to make a contrast or have a discussion about that for the readers to have a better understanding.
==================================================

Focused review:

Weakness: - The motivation to introduce permutation invariance to structured prediction is not clear. I understand this is a particular inductive bias that the authors would like to impose, but how is this property related to structured prediction? in terms of either learning theory or empirical observations? Or it may be more interesting to know how do the authors come up with this idea. - Error in Theorem 1: The direction (1) => graph-permutation invariant (GPI) is easy to understand. However, I don't think GPI => (1) holds true in general (any GPI function can be expressed as in (1) sounds like a strong statement). The proof for this direction is really hard to follow. Even if this proof is correct, a major revision is needed. As a counter example, let's construct a graph-labeling function with each element of its output the same = \sum_i \sum_{j \neq i} \phi_ij(z_ij). This function is GPI by definition. E.g., let \phi_ij = \phi^{i*j}. Note that in (1), \phi is shared by all ij, and \alpha only takes \sum \phi(z_ij) as its 2nd argument, so we cannot introduce \phi_ij by \alpha. If I'm not mistaken, there is no way to express this counter example in the form of (1).  After rebuttal: The rebuttal addressed some of my concerns, especially the counter example as pointed out doesn't hold true. I tend to believe that graph-permutation invariance could be a case of inductive bias for structured prediction, which deserves to be examined by more audiences. The major concern now is the writing. It still sounds a strong statement to me that any graph-permutation invariant function can be expressed as in (1). Rather than presenting the idea by a compact proof. A major rewriting for showing the intuition of (1) would be very helpful. For example, to show that message-passing algorithms can be considered as special cases. 

Review Point: - The motivation to introduce permutation invariance to structured prediction is not clear. I understand this is a particular inductive bias that the authors would like to impose, but how is this property related to structured prediction? in terms of either learning theory or empirical observations? Or it may be more interesting to know how do the authors come up with this idea.
Review Point: - Error in Theorem 1: The direction (1) => graph-permutation invariant (GPI) is easy to understand. However, I don't think GPI => (1) holds true in general (any GPI function can be expressed as in (1) sounds like a strong statement). The proof for this direction is really hard to follow. Even if this proof is correct, a major revision is needed. As a counter example, let's construct a graph-labeling function with each element of its output the same = \sum_i \sum_{j \neq i} \phi_ij(z_ij). This function is GPI by definition. E.g., let \phi_ij = \phi^{i*j}. Note that in (1), \phi is shared by all ij, and \alpha only takes \sum \phi(z_ij) as its 2nd argument, so we cannot introduce \phi_ij by \alpha. If I'm not mistaken, there is no way to express this counter example in the form of (1). After rebuttal: The rebuttal addressed some of my concerns, especially the counter example as pointed out doesn't hold true. I tend to believe that graph-permutation invariance could be a case of inductive bias for structured prediction, which deserves to be examined by more audiences. The major concern now is the writing. It still sounds a strong statement to me that any graph-permutation invariant function can be expressed as in (1). Rather than presenting the idea by a compact proof. A major rewriting for showing the intuition of (1) would be very helpful. For example, to show that message-passing algorithms can be considered as special cases.
==================================================

Focused review:

- The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for "realistic" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated. - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.

Review Point: - The theorem may slightly overstate its result in the following way: it seems that in order for this correspondence between adversarial training and the proposed regularization scheme to hold, \epsilon must be quite small. That is, we are assuming here that all of the points in an \epsilon ball around some data point x are mapped by the model to the same activation pattern \phi_x (i.e. that B_\epsilon^p(x) \subset X(\phi_x)). I would imagine that this may not hold for "realistic" values of \epsilon (e.g. 8/255) all the time. Indeed, my concern is that while this theorem is certainly compelling, it may be the case that it only holds for \epsilon so small that it may not hold in practice. Perhaps the authors can clarify here. I see there is an experiment to this effect in Section 7.16, but this seems to be for only one data point. [EDIT: post-rebuttal] Based on the authors response and a closer look at Section 5.4, I'm satisfied that the authors looked into this potential weakness and were able to add explanation as to its implications.] - Figure 1 is too small to really be useful. It's not really clear what the arrows represent. A more detailed and larger figure here would be appreciated.
Review Point: - The notation when describing the power iteration is a bit strange. This is a small thing, but I think that it would make more sense just to rearrange the steps. For example, in (6) it would be more clear to write \tilde{u} \gets ..., then u_k \gets ..., then \tilde{v}\gets ..., and finally v_k\gets ... so that you have these steps written in the order that you apply them.
==================================================

Focused review:

Weakness:
The method extends the CCVAE which models the dependency between label and data. The information in labels is definitely not comparable to the information in a data modality. I agree that mutual supervision can mitigate this issue as it establishes a symmetric information flow. However, in qualitative results (Fig.12 - Fig.17) it is quite obvious that the generated samples of SVHN (left) are limited in certain colors (grayish) and shapes (thin and centered), while MNIST generations (right) contain much more variations. Can the authors provide more details on the architecture and parameters related to each modality? Can more powerful decoders for SVHN help generate more realistic SVHN samples?
I'm a bit concerned about the scalability of the method. Even though the authors provide some potential ways to scale the current bi-modal setting to a multi-modal setting. Unlike other multi-modal VAEs that simply concatenates a new modality, MEME is based on a bi-modal setting. Thus three modalities will already need 6 information flows (e.g. for ( a , b , c
) modalities, a → b , b → a , a → c , c → a , b → c , c → b
). Could the authors elaborate more on how to scale MEME to modalities > 2? The authors can also add some results on 3-modal datasets like MNIST-SVHN-Text used in [1] to compare the performance and training time of MEME and other baselines.
Minor comments:
The authors need to double-check the manuscripts --- some typos I can find may confuse the audience: 1) In Fig.6 MEME_Caption should be MEME_Image? 2) in Fig.9 caption the middle is MMVAE, not MVAE?
I'm curious about the comb-like distribution of Wassertein distance in Fig.8 SVHN-MNIST. Do authors have any intuition or comments on it?
[1] Thomas M. Sutter, Imant Daunhawer, Julia E. Vogt, Multimodal Generative Learning Utilizing, Jensen-Shannon-Divergence, NeurIPS 2021

Review Point: 2) in Fig.9 caption the middle is MMVAE, not MVAE? I'm curious about the comb-like distribution of Wassertein distance in Fig.8 SVHN-MNIST. Do authors have any intuition or comments on it? [1] Thomas M. Sutter, Imant Daunhawer, Julia E. Vogt, Multimodal Generative Learning Utilizing, Jensen-Shannon-Divergence, NeurIPS 2021
==================================================

Focused review:

I also have some confusions on current manuscript. 1. In Sec.3.2, after SDF changes a little bit, the new surface sample v' is defined as the sample on the new SDF that are closest to the original sample v. I wonder whether this is a principled definition or sort of heuristics? As far as my understanding, there is no constraint on the sample v that can be applied to sampling on both old and new SDF --- such constraint could establish the correspondence between v and v'. Thus, one could not find the v' corresponding to v unless some constraints are imposed, e.g. the closest point here. 2. Following the above definition of v' as the closet point, when we update z via backpropagation and run MC in the next iterations, would the new v' actually sampled by MC really the v' defined above? This is an important question to ask, because this is to make sure that the actually updating on v is guided by the computed differentiation. 3. Regarding minimizing equation (7), how to make the loss differentiable to the discrete values? Judging from line 201, DR_silhouette(M(z)) is a binary variable. I can imagine that when z is continuously changed, the DR_silhouette(M(z)) switches between 0 and 1 as a binary value and is thus a discrete function.

Review Point: 1. In Sec.3.2, after SDF changes a little bit, the new surface sample v' is defined as the sample on the new SDF that are closest to the original sample v. I wonder whether this is a principled definition or sort of heuristics? As far as my understanding, there is no constraint on the sample v that can be applied to sampling on both old and new SDF --- such constraint could establish the correspondence between v and v'. Thus, one could not find the v' corresponding to v unless some constraints are imposed, e.g. the closest point here.
Review Point: 2. Following the above definition of v' as the closet point, when we update z via backpropagation and run MC in the next iterations, would the new v' actually sampled by MC really the v' defined above? This is an important question to ask, because this is to make sure that the actually updating on v is guided by the computed differentiation.
Review Point: 3. Regarding minimizing equation (7), how to make the loss differentiable to the discrete values? Judging from line 201, DR_silhouette(M(z)) is a binary variable. I can imagine that when z is continuously changed, the DR_silhouette(M(z)) switches between 0 and 1 as a binary value and is thus a discrete function.
==================================================

Focused review:

1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model. 
2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B. 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail. 
5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever.
Reference: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 
the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. 
Line 300-304: q and d are confusing 

Review Point: 1. The selling point of this paper is unsupervised pretrained dense retriever(LaPraDoR) can per- form on par with supervised dense retriever, but actually, LaPraDoR is a hybrid retriever rather than a pure dense retriever. In a way, it’s unfair to compare hybrid method to dense/sparse method as shown in table 1, because it’s known that the dense retriever and sparse retriever are complementary. The comparable supervised models should also be hybrid retrievers. Besides, in table 3, it seems that without lexicon enhancement, the performance of proposed unsupervised model is not competitive either on in-domain MS-MARCO or cross domain BEIR benchmark compared with supervised model.
Review Point: 2. In table 4, the combination of self-supervised tasks ICT and DaPI doesn’t seem to be com- plementary, the effectiveness of DaPI task, which will double the GPU memory usage, is not significant (0.434 -> 0.438) 3. ICoL is proposed to mitigate the insufficient memory on a single GPU and allow more neg- ative instances for better performance, but there are no corresponding experiments to show the influence of the number of negatives. As far as I know, the quality of negatives is more important than the quantity of negatives as shown in TAS-B.
Review Point: 4. It sounds unreasonable that increasing the model size can hurt the performance, as recent paper Ni et al. shows that the scaling law is also apply to dense retrieval model, so the preliminary experimental results on Wikipedia about model size should be provided in detail.
Review Point: 5. Thepaperarguethattheproposedapproachistocomplementlexicalmatchingwithsemantic matching, while the training procedure of proposed model is totally independent with lexical matching. Therefore, the argument ”LEDR helps filter out such noise and allows the dense retriever to focus on fine-grained semantic matching” is confusing, because there is no suc- cession relationship between LEDR and dense retriever. Reference:
Review Point: * Ni et al. 2021. https://arxiv.org/abs/2112.07899 the proposed LaPraDoR achieves relative low performance on MS-MARCO while relative high per- formance on BEIR, the inductive bias of the proposed pretrain method is worth exploring. Line 300-304: q and d are confusing
==================================================

Focused review:

1. The proposed method is only applicable to binary classification problems, and thus the application is limited; 2. There are some inappropriate statements, e.g., it is not appropriate to claim that “training time” is an algorithmic parameter. 3. The proposed method is designed for non-convex metric learning, and thus some recently proposed deep metric learning algorithms should be included for comparison.

Review Point: 1. The proposed method is only applicable to binary classification problems, and thus the application is limited;
Review Point: 2. There are some inappropriate statements, e.g., it is not appropriate to claim that “training time” is an algorithmic parameter.
Review Point: 3. The proposed method is designed for non-convex metric learning, and thus some recently proposed deep metric learning algorithms should be included for comparison.
==================================================

Focused review:

Weaknesses: 1.The dependence of the number of measurements on the sparsity level is quadratic instead of linear. 2. Also, I don't really understand Section 4.2. Why does the Theorem follow form inequality (4)? I also don't understand why the inequality on page 8 is true. I am also not sure whether I understand how ( ) \imax
is defined. 3. In general, the presentation of the material can be improved; see some of the comments below.
Further comments:
page 3: It might be good to explain how the expression log ⁡ det ( I + B T B σ 2 )
relates to the SNR!
page 3: "let n
approach infinity". Consider replacing this statement with something more rigorous. Is this an asymptotic or non-asymptotic statement?
page 4: It seems to me that B p , m
also depends on k
. It would be good to make this dependence explicit, for example, by adding k
as a subscript.
page 5, Theorem 3 and Corollary: Consider replacing "Consider the large-system limit..." by something more rigorous. (It is not clear whether this is an asymptotic or non-asymptotic statement!)
page 5, Corollary 1: "where n and p
are sufficiently large. Consider replacing this by a more rigorous statement.
page 5, Theorem 3: Has h
been already properly introduced? If yes, it might be a good idea to recall its definition!
page 7: Why does ‖ B ♯ ‖ F
affect the reconstruction error? I do not understand this statement.
page 8, Discussion w.r.t. sparsity number k
: In contrast to your experiments, Theorem 2 does not say that one needs to increase the SNR when one increases the sparsity level k
. Please add an explanation why this is not a contradiction. Typos:
page 5 sin(c)e all these works

Review Point: 1.The dependence of the number of measurements on the sparsity level is quadratic instead of linear.
Review Point: 2. Also, I don't really understand Section 4.2. Why does the Theorem follow form inequality (4)? I also don't understand why the inequality on page 8 is true. I am also not sure whether I understand how ( ) \imax is defined.
==================================================

Focused review:

Weaknesses]
[One feasible solution..., above section 3]. Authors say that it is suboptimal to divide the proposed task into two subtasks : 1) the nearest frame retrieval and 2) local registration. However, authors still follow this pipeline by first performing KNN submap retrieval and then do local registration. The difference could be authors use an attention-based context aggregation module to extract better global features. According to the above analysis, I would expect the following three experiments:
Substitute the proposed submap retrieval [Sec.3.3] with a state-of-the-art nearest frame retrieval method [e.g., https://github.com/jac99/MinkLoc3Dv2], and keep the remaining modules to test the proposed method;
Use a state-of-the-art nearest frame retrieval method for state-of-the-art methods to find a local submap for a query, and then test the performance of state-of-the-art methods;
Use the proposed submap retrieval [Sec.3.3] for baseline methods to find a local submap for a query, and then test the performance of state-of-the-art methods;
The above three experiments (ablations) can help readers get a clear picture of the proposed method and identifying what is actually working.
It is blurry how the proposed method is tested on the standard 3DMatch and original KITTI datasets. Are you using K=1 and skip the submap matching module?
For comparison with respect to state-of-the-art methods (Table 1), please report the original figures of these methods. For example, [Predator, RR] 90.6 for 3DMatch and 62.4 for 3DLoMatch. Furthermore, please also compare the state-of-the-art method [Geometric Transformer for Fast and Robust Point Cloud Registration], it got 92.5 for 3DMatch and 74.2 for 3DLoMatch.

Review Point: 1) the nearest frame retrieval and 2) local registration. However, authors still follow this pipeline by first performing KNN submap retrieval and then do local registration. The difference could be authors use an attention-based context aggregation module to extract better global features. According to the above analysis, I would expect the following three experiments: Substitute the proposed submap retrieval [Sec.3.3] with a state-of-the-art nearest frame retrieval method [e.g., https://github.com/jac99/MinkLoc3Dv2], and keep the remaining modules to test the proposed method; Use a state-of-the-art nearest frame retrieval method for state-of-the-art methods to find a local submap for a query, and then test the performance of state-of-the-art methods; Use the proposed submap retrieval [Sec.3.3] for baseline methods to find a local submap for a query, and then test the performance of state-of-the-art methods; The above three experiments (ablations) can help readers get a clear picture of the proposed method and identifying what is actually working. It is blurry how the proposed method is tested on the standard 3DMatch and original KITTI datasets. Are you using K=1 and skip the submap matching module? For comparison with respect to state-of-the-art methods (Table 1), please report the original figures of these methods. For example, [Predator, RR] 90.6 for 3DMatch and 62.4 for 3DLoMatch. Furthermore, please also compare the state-of-the-art method [Geometric Transformer for Fast and Robust Point Cloud Registration], it got 92.5 for 3DMatch and 74.2 for 3DLoMatch.
==================================================

Focused review:

- one of the motivations for stochastic NFs is that they overcome topological constraints. In the remainder of the paper, such constraints were not carefully formalized nor a reasoning provided for what makes NFs fail in such scenarios and how SNFs are particularly suited to fix these issues. Put differently, I wonder if the shortcomings of RNVPs in Figure 3 can be overcome with other architectures for normalizing flows, such as invertible resnets, iaf, maf etc. - the empirical evaluation in the context of existing works is largely restricted. For example, the schemes in [21, 35] could very well be applied here as well. Similarly, in the setup for MNIST/fashion datasets Table 3, I would have expected the default use implementation of flows for approximating intractable posteriors in a VAE is an inverse autoregressive flow (IAF) but instead the authors choose it to be a Real-NVP. - the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations. - related work such as A-NICE-MC [35] take special care to ensure that detailed balance is satisfied while using flows as proposals for MCMC. In the current work, it is unclear if such conditions are being satisfied in practice for SNFs.

Review Point: - one of the motivations for stochastic NFs is that they overcome topological constraints. In the remainder of the paper, such constraints were not carefully formalized nor a reasoning provided for what makes NFs fail in such scenarios and how SNFs are particularly suited to fix these issues. Put differently, I wonder if the shortcomings of RNVPs in Figure 3 can be overcome with other architectures for normalizing flows, such as invertible resnets, iaf, maf etc.
Review Point: - the empirical evaluation in the context of existing works is largely restricted. For example, the schemes in [21, 35] could very well be applied here as well. Similarly, in the setup for MNIST/fashion datasets Table 3, I would have expected the default use implementation of flows for approximating intractable posteriors in a VAE is an inverse autoregressive flow (IAF) but instead the authors choose it to be a Real-NVP.
Review Point: - the unbiased guarantees for estimating expectations are only in the asymptotic limit. I would have been curious to see an analysis of the bias-variance tradeoff in the empirical evaluations.
Review Point: - related work such as A-NICE-MC [35] take special care to ensure that detailed balance is satisfied while using flows as proposals for MCMC. In the current work, it is unclear if such conditions are being satisfied in practice for SNFs.
==================================================

Focused review:

Weaknesses:
One weakness is the clarity of the notations. Although the author defined the notations at the beginning of section 2. I found I still got lost while reading the paper, especially when trying to distinguish between an index set and a single index. This could be due to the redefinition of the bold type and the normal type.
About the clarity of the problem definition: I’m not familiar with this task, but when people refer to “online,” they usually will indicate the data distribution changes or is stationary. I guess here the problem assumes a stationary distribution, and “online” indicates the sequential nature of the selection, right? If so, there is no adaptive part of the problem, and the method sounds like a sequential version of the offline counterpart.
For the theoretical results, I feel Eq. 8 and thm 1,2 may require more explanations or assumptions. (But these impressions could also be due to my misunderstanding.) I had the following questions while I was reading the paper
Does p(s) depend on ϕ or θ ?
In the proof of Thm 1, what are the mild conditions mentioned in the main paper? I didn’t follow how Eq. 8 simplifies to Proposition 1.
It seems the practical objective function of the experiments has a gap with Eq. 8. I.e., I didn’t see how to sample from p ( s )
from Alg. 1. It would be great to state the objective function in experiments (with monte carlo estimates of the expectations).
Should Eq. 7 have a reverse inequality sign because v
is the expected loss? This is the same for the gap definition below.
I felt some parts of section 3 (mainly sec. 3.2) are pretty technical but didn’t help me understand the main method in section 4. Perhaps leaving more space in section 4 to explain the training procedure is more rewarding to the audience. (Maybe it’s only a personal taste.)

Review Point: 8. I.e., I didn’t see how to sample from p ( s ) from Alg.
Review Point: 1. It would be great to state the objective function in experiments (with monte carlo estimates of the expectations). Should Eq. 7 have a reverse inequality sign because v is the expected loss? This is the same for the gap definition below. I felt some parts of section 3 (mainly sec. 3.2) are pretty technical but didn’t help me understand the main method in section 4. Perhaps leaving more space in section 4 to explain the training procedure is more rewarding to the audience. (Maybe it’s only a personal taste.)
==================================================

Focused review:

Weaknesses: One major drawback is the tightness of the bound. From Figures 1 and 2, it seems that the proposed bounds are loose in most cases and are only informative when the empirical risks are high, which is not the current operating regime for deep learning.
A lot of important references are missing. This paper completely ignores a recent line of work on information-theoretic generalization bounds. Though the original forms of these bounds are in expectation [1,2], more recent results [3] can provide high-probability generalization bounds and should be mentioned in the discussion.
[1] Xu, Aolin, and Maxim Raginsky. "Information-theoretic analysis of generalization capability of learning algorithms." Advances in Neural Information Processing Systems 30 (2017).
[2] Bu, Yuheng, Shaofeng Zou, and Venugopal V. Veeravalli. "Tightening mutual information-based bounds on generalization error." IEEE Journal on Selected Areas in Information Theory 1, no. 1 (2020): 121-130.
[3] Hellström, Fredrik, and Giuseppe Durisi. "Generalization bounds via information density and conditional information density." IEEE Journal on Selected Areas in Information Theory 1, no. 3 (2020): 824-839.
More specifically, the following two papers on the generalization error for the Gibbs algorithm that provides 1/m in expectation bounds are not discussed.
[4] Kuzborskij, Ilja, Nicolò Cesa-Bianchi, and Csaba Szepesvári. "Distribution-dependent analysis of Gibbs-ERM principle." In Conference on Learning Theory, pp. 2028-2054. PMLR, 2019.
[5] Aminian, Gholamali, Yuheng Bu, Laura Toni, Miguel Rodrigues, and Gregory Wornell. "An exact characterization of the generalization error for the Gibbs algorithm." Advances in Neural Information Processing Systems 34 (2021): 8106-8118.

Review Point: 1 (2020): 121-130. [3] Hellström, Fredrik, and Giuseppe Durisi. "Generalization bounds via information density and conditional information density." IEEE Journal on Selected Areas in Information Theory 1, no.
Review Point: 3 (2020): 824-839. More specifically, the following two papers on the generalization error for the Gibbs algorithm that provides 1/m in expectation bounds are not discussed. [4] Kuzborskij, Ilja, Nicolò Cesa-Bianchi, and Csaba Szepesvári. "Distribution-dependent analysis of Gibbs-ERM principle." In Conference on Learning Theory, pp. 2028-2054. PMLR, 2019. [5] Aminian, Gholamali, Yuheng Bu, Laura Toni, Miguel Rodrigues, and Gregory Wornell. "An exact characterization of the generalization error for the Gibbs algorithm." Advances in Neural Information Processing Systems 34 (2021): 8106-8118.
==================================================

Focused review:

1. Some important references are missing. "Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights", ECCV 2018. This paper studies a pruning based approach where multiple tasks share the same pre-trained network but with a different pruning pattern. Only the pruning pattern is stored to avoid catastrophic forgetting. Conceptually, this approach is quite similar to the proposed method. 2. The paper claims in line 78-79 that "the calibration parameters of the previous task also serve as good initial weights for learning the calibration parameters of the new task". This claim is quite strong but not further justified in the later sections.

Review Point: 1. Some important references are missing. "Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights", ECCV 2018. This paper studies a pruning based approach where multiple tasks share the same pre-trained network but with a different pruning pattern. Only the pruning pattern is stored to avoid catastrophic forgetting. Conceptually, this approach is quite similar to the proposed method.
Review Point: 2. The paper claims in line 78-79 that "the calibration parameters of the previous task also serve as good initial weights for learning the calibration parameters of the new task". This claim is quite strong but not further justified in the later sections.
==================================================

Focused review:

1) The presentation of the paper can be substantially improved. For example, frequent switching between LSCV and LWCV can confuse readers. 2) In the empirical evaluation, it lacks quantitative results, which makes it hard to assess the performance of the proposed algorithms.

Review Point: 1) The presentation of the paper can be substantially improved. For example, frequent switching between LSCV and LWCV can confuse readers.
Review Point: 2) In the empirical evaluation, it lacks quantitative results, which makes it hard to assess the performance of the proposed algorithms.
==================================================

Focused review:

1. The applicability of the proposed BPI is restricted. Although I agree infering 3D scene geometry is an important task for 3D image understanding, I'm wondering how does the proposed method generalize to real scene, where the images are not essentially generated from either inside of a "3d box", nor outside of a "3d box" (e.g. Fig. 9 in NeurVPS[45]). More importantly, the fitting score (which is used for ranking different programs) is defined based on the repeated patterns in the plane image, but even in many images that are formed from a 3d box does not necessarily have repeated patterns (such as indoor bedroom), then how does the proposed method work on this case (where the proposed fitting score may not be meaningful)? 2. The author assumed the pattern should be appearing in lattice, can it handle arbitrary placed repeated patterns (only assuming they did not overlap)? 3. Searching for the program would not only require iterate different wireframe combination, but also, for the same wireframe combination, it requires to iterate all different number of columns and number of rows for the repeated patterns in each plane, could the author provide how big this could be and how fast it would run?

Review Point: 1. The applicability of the proposed BPI is restricted. Although I agree infering 3D scene geometry is an important task for 3D image understanding, I'm wondering how does the proposed method generalize to real scene, where the images are not essentially generated from either inside of a "3d box", nor outside of a "3d box" (e.g. Fig. 9 in NeurVPS[45]). More importantly, the fitting score (which is used for ranking different programs) is defined based on the repeated patterns in the plane image, but even in many images that are formed from a 3d box does not necessarily have repeated patterns (such as indoor bedroom), then how does the proposed method work on this case (where the proposed fitting score may not be meaningful)?
Review Point: 2. The author assumed the pattern should be appearing in lattice, can it handle arbitrary placed repeated patterns (only assuming they did not overlap)?
Review Point: 3. Searching for the program would not only require iterate different wireframe combination, but also, for the same wireframe combination, it requires to iterate all different number of columns and number of rows for the repeated patterns in each plane, could the author provide how big this could be and how fast it would run?
==================================================

Focused review:

The paper introduces two components in the paper. The stochastic Gumbel Annealing (SGA) tends to approximate the discretization with an annealing scheme. The main concern lies in how the model performance is influenced by the choice of annealing scheme. It would be better to add more details and sensitive analysis to show the effect of temperature annealing. The lossy bits back coding part may also lack some details: 1. How the entropy coding is implemented, i.e., which exact coding scheme is used? 2. The bits back coding saves the bitlength by getting the uniform bits during sampling of latent variable back at decoder part which could be used to transmit the so-called "side information". My question is how the side information is designed? If the side information refers to the encoded results of the previous image, then the benefit is related to the sequence length. Then how the sequence length is selected. If not, then what is the exact side information encoded and how the comparison between the proposed methods and other baseline is conducted. Besides, the benefit of lossy bits back coding seems to be marginal as shown in Figure 3. Another question is whether the proposed procedure is only compatible with the hierarchical vae? Maybe the lossy bits back coding is not easy to apply in the vanilla vae, but it will be a benefit to show the performance of SGA in a vanilla vae setting.

Review Point: 1. How the entropy coding is implemented, i.e., which exact coding scheme is used?
Review Point: 2. The bits back coding saves the bitlength by getting the uniform bits during sampling of latent variable back at decoder part which could be used to transmit the so-called "side information". My question is how the side information is designed? If the side information refers to the encoded results of the previous image, then the benefit is related to the sequence length. Then how the sequence length is selected. If not, then what is the exact side information encoded and how the comparison between the proposed methods and other baseline is conducted. Besides, the benefit of lossy bits back coding seems to be marginal as shown in Figure 3. Another question is whether the proposed procedure is only compatible with the hierarchical vae? Maybe the lossy bits back coding is not easy to apply in the vanilla vae, but it will be a benefit to show the performance of SGA in a vanilla vae setting.
==================================================

Focused review:

1. Currently, all the videos are short videos. However, it is unlikely that all the source videos are already well-trimmed. If they are not, then how are the temporal boundary are determined and will these annotations be released? I would like to invite the authors to provide more clarification on this. 2. Is there a human verification or voting mechanism to make sure human annotations accurate?

Review Point: 1. Currently, all the videos are short videos. However, it is unlikely that all the source videos are already well-trimmed. If they are not, then how are the temporal boundary are determined and will these annotations be released? I would like to invite the authors to provide more clarification on this.
Review Point: 2. Is there a human verification or voting mechanism to make sure human annotations accurate?
==================================================

Focused review:

Weaknesses: The primary weakness of the paper is that the model is not too novel. It is essentially a tweak to skip-gram. Furthermore, the full model presented by the paper doesn't seem to be the best one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is substantially better. A similar trend seems to dominate Table 6 too. On the larger UMNSRS data, the proposed model is at best competitive with previous simpler models (Chiu).
- General Discussion: The paper says that it is uses known phrases as distant supervision to train embeddings. However, it is not clear what the "supervision" here is. If I understand the paper correctly, every occurrence of a phrase associated with a concept provides the context to train word embeddings. But this is not supervision in the traditional sense (say for identifying the concept in the text or other such predictive tasks). So the terminology is a bit confusing.
 The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of the paper.
The use of \beta to control for compositionality of phrases by words is quite surprising. Essentially, this is equivalent to saying that there is a single global constant that decides "how compositional" any phrase should be. The surprising part here is that the actual values of \beta chosen by cross validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which basically argues against compositionality. The experimental setup for table 4 needs some explanation. The paper says that the data labels similarity/relatedness of concepts (or entities). However, if the concepts-phrases mapping is really many-to-many, then how are the phrase/word vectors used to compute the similarities? It seems that we can only use the concept vectors.
In table 5, the approximate phr method (which approximate concepts with the average of the phrases in them) is best performing. So it is not clear why we need the concept ontology. Instead, we could have just started with a seed set of phrases to get the same results. 

Review Point: - General Discussion: The paper says that it is uses known phrases as distant supervision to train embeddings. However, it is not clear what the "supervision" here is. If I understand the paper correctly, every occurrence of a phrase associated with a concept provides the context to train word embeddings. But this is not supervision in the traditional sense (say for identifying the concept in the text or other such predictive tasks). So the terminology is a bit confusing. The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of the paper. The use of \beta to control for compositionality of phrases by words is quite surprising. Essentially, this is equivalent to saying that there is a single global constant that decides "how compositional" any phrase should be. The surprising part here is that the actual values of \beta chosen by cross validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which basically argues against compositionality. The experimental setup for table 4 needs some explanation. The paper says that the data labels similarity/relatedness of concepts (or entities). However, if the concepts-phrases mapping is really many-to-many, then how are the phrase/word vectors used to compute the similarities? It seems that we can only use the concept vectors. In table 5, the approximate phr method (which approximate concepts with the average of the phrases in them) is best performing. So it is not clear why we need the concept ontology. Instead, we could have just started with a seed set of phrases to get the same results.
==================================================

Focused review:

1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets. 
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system. 
3. ( minor) It is unclear how the authors arrived at the different components of the "scoring function," nor is it clear how they arrived at the different threshold values/ranges. 
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content. 
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.  2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)? 
3. It will be nice to see some examples of the system on actual texts (vs. other components & models). 
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well. 

Review Point: 1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
Review Point: 2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
Review Point: 3. ( minor) It is unclear how the authors arrived at the different components of the "scoring function," nor is it clear how they arrived at the different threshold values/ranges.
Review Point: 4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
Review Point: 1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.
Review Point: 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
Review Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
Review Point: 4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.
==================================================

Focused review:

The major weakness of the paper is that the authors do not relate their work with recent work on the same topic. I think a section relating the proposed work to prior work will make the paper much better. In addition, the authors should answer the following questions in the rebuttal/re-submission: 1. The authors have mentioned that con-SI does not improve the representation for standard classification. However, it is not clear whether the proposed approach harms standard classification. The authors should clarify whether the performance for standard classification changes a lot with con-SI. If the performance degrades, by how much? The authors should also discuss if there are ways to avoid such degradations in the performance. 2. Did the authors employ a principled way of selecting the set of transformations S? Currently, it's not clear how did the authors select the number of transformations to use and also which transformations to include in S. They have conducted ablation studies to understand the effectiveness of each transformation separately. However, can the authors discuss more principled ways of searching for elements of S? I think there might be some adversarial methods for selecting the best transformations.

Review Point: 1. The authors have mentioned that con-SI does not improve the representation for standard classification. However, it is not clear whether the proposed approach harms standard classification. The authors should clarify whether the performance for standard classification changes a lot with con-SI. If the performance degrades, by how much? The authors should also discuss if there are ways to avoid such degradations in the performance.
Review Point: 2. Did the authors employ a principled way of selecting the set of transformations S? Currently, it's not clear how did the authors select the number of transformations to use and also which transformations to include in S. They have conducted ablation studies to understand the effectiveness of each transformation separately. However, can the authors discuss more principled ways of searching for elements of S? I think there might be some adversarial methods for selecting the best transformations.
==================================================

Focused review:

Weakness: - This paper's approach proposes multi-layer representation learning via gradient boosted trees, and on the top of that, linear regression/softmax regression are placed for supervised learning. But, we can do the opposite representation learning via neural nets, and on the top of that, decision trees can be used (Deep Neural Decision Forests by Kontschieder et al, ICCV 2015).  Comment: - Basically I liked overall ideas. It can demonstrate the deep learning is not the only option for multi-layer representation learning. It's also nice that the input-output recovery like autoencoder would be also in a gradient descent manner similar to gradient boosting itself. Seemingly, it is simpler than the stacked trees such as Deep Forests (Zhou and Feng, IJCAI 2017). - Something is wrong in the descriptions of Algorithm 1. If we initialize G_{2:M}^0 <- null, then the first run of "G_j^t <- G_j^{t-1}" for the j=M to 2 loop would become G_M^1 <- G_M^0 = null, and thus the computation of L_j^inv (as well as the residuals r_k) can be problematic. - This is just a comment, and does not need to reflect the paper this time, but there would be several interesting points to be investigated in future. First, the presented multi-layer representation is probably way weaker than the ones possible with the current neural networks. The presented representation corresponds to the fully connected (FC) architectures of neural networks, and more flexible architectures such as CNN and RNN would not be directly possible. Given this point, it would be unclear whether we should use trees for multi-layered representation learning. At least, we could use boosted model trees having linear regression at leaves, for example. As mentioned in the weakness section, 'differentiable forests' such as Deep Neural Decision Forests was already proposed, and we can use this type of approaches with the modern neural nets for 'representation learning / feature learning' parts. What situation fits what choice would be one interesting open questions. Comments after author response: Thank you for the response. It'll be nice to have the revisions on the notations for initialization, and some discussion or mentions about the different approach to integrate tree-based and multi-layer representation learning.

Review Point: - This paper's approach proposes multi-layer representation learning via gradient boosted trees, and on the top of that, linear regression/softmax regression are placed for supervised learning. But, we can do the opposite representation learning via neural nets, and on the top of that, decision trees can be used (Deep Neural Decision Forests by Kontschieder et al, ICCV 2015). Comment:
Review Point: - Basically I liked overall ideas. It can demonstrate the deep learning is not the only option for multi-layer representation learning. It's also nice that the input-output recovery like autoencoder would be also in a gradient descent manner similar to gradient boosting itself. Seemingly, it is simpler than the stacked trees such as Deep Forests (Zhou and Feng, IJCAI 2017).
Review Point: - Something is wrong in the descriptions of Algorithm 1. If we initialize G_{2:M}^0 <- null, then the first run of "G_j^t <- G_j^{t-1}" for the j=M to 2 loop would become G_M^1 <- G_M^0 = null, and thus the computation of L_j^inv (as well as the residuals r_k) can be problematic.
Review Point: - This is just a comment, and does not need to reflect the paper this time, but there would be several interesting points to be investigated in future. First, the presented multi-layer representation is probably way weaker than the ones possible with the current neural networks. The presented representation corresponds to the fully connected (FC) architectures of neural networks, and more flexible architectures such as CNN and RNN would not be directly possible. Given this point, it would be unclear whether we should use trees for multi-layered representation learning. At least, we could use boosted model trees having linear regression at leaves, for example. As mentioned in the weakness section, 'differentiable forests' such as Deep Neural Decision Forests was already proposed, and we can use this type of approaches with the modern neural nets for 'representation learning / feature learning' parts. What situation fits what choice would be one interesting open questions. Comments after author response: Thank you for the response. It'll be nice to have the revisions on the notations for initialization, and some discussion or mentions about the different approach to integrate tree-based and multi-layer representation learning.
==================================================

Focused review:

- The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE. - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1. - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.

Review Point: - The experimental results are not significant. Two straightforward error rate indicators: the dimension of the space and LR error is chosen as baselines and the authors claim that the proposed bound is superior to these two baselines. However, from the experimental results, Pearson’s r correlation in Fig.4 shows that the proposed improvement is marginal. The results on image dataset from the supplementary material also shows that the Pearson’s r correlation is very close between the LR Error and MSE.
Review Point: - The authors only conduct experiments with k=1. It is unclear whether the empirical conclusion remains the same in Fig.4 if k becomes larger than 1.
Review Point: - The figures are not clear enough, e.g. what does each point stand for in Fig.1 and Fig.2. The authors could add the necessary information to make this paper more self-contained.
==================================================

Focused review:

The paper is well-written, and the presented results are discussed clearly with relevant experimentation. While the amount of contribution with respect to (Ghorbani et al. 2019) may be put into question, overall, I enjoyed reading this paper and found it useful. Some comments and questions for the authors: 1. The proposed results are obtained for a two-layer fully connected network architecture. I understand that this simplified model makes the theoretical analysis more tractable, but could the authors comment on to what extent they expect their results to be valid for deeper network topologies with more than two layers? 2. The proposed study assumes that the function to be learnt is of the form f_*(x) = phi(U^T x). Then, in this setting the NN models are shown to outperform, RKHS models and linear NN approximations in terms of representation power. I am wondering whether the particular function model “phi(U^T x)” assumed in the paper would tend to trivially favor NN models (in comparison to e.g. RKHS models), as it consists of a “linear projection followed by a nonlinear scalar function”, just like in neural network hypothesis classes. This being said, I also understand that the form f_*(x) = phi(U^T x) is the way that authors characterize the “low-dimensionality” of the function class to be learnt, to which I have no objection. Some further comments on these issues would be useful. 3. I guess an important future direction of this study would be the incorporation of finite sample effects into the bounds. I would encourage the authors to pursue their efforts in this direction towards a more mature extension of their work. 4. The current experimental results only compare the NN methods. How would the performance of RKHS methods be in these experiments? Minor comments: 1. The difference between the problem formulations (1) and (2) is not clear. Please explain their difference clearly, by modifying the notation if necessary. Are the coefficients (a, b) and the network weights W variable in both problems? Are some of them fixed in (2)? 2. In Theorem 1, please define what the notation “omega_d(.)” means.

Review Point: 1. The proposed results are obtained for a two-layer fully connected network architecture. I understand that this simplified model makes the theoretical analysis more tractable, but could the authors comment on to what extent they expect their results to be valid for deeper network topologies with more than two layers?
Review Point: 2. The proposed study assumes that the function to be learnt is of the form f_*(x) = phi(U^T x). Then, in this setting the NN models are shown to outperform, RKHS models and linear NN approximations in terms of representation power. I am wondering whether the particular function model “phi(U^T x)” assumed in the paper would tend to trivially favor NN models (in comparison to e.g. RKHS models), as it consists of a “linear projection followed by a nonlinear scalar function”, just like in neural network hypothesis classes. This being said, I also understand that the form f_*(x) = phi(U^T x) is the way that authors characterize the “low-dimensionality” of the function class to be learnt, to which I have no objection. Some further comments on these issues would be useful.
Review Point: 3. I guess an important future direction of this study would be the incorporation of finite sample effects into the bounds. I would encourage the authors to pursue their efforts in this direction towards a more mature extension of their work.
Review Point: 4. The current experimental results only compare the NN methods. How would the performance of RKHS methods be in these experiments? Minor comments:
Review Point: 1. The difference between the problem formulations (1) and (2) is not clear. Please explain their difference clearly, by modifying the notation if necessary. Are the coefficients (a, b) and the network weights W variable in both problems? Are some of them fixed in (2)?
Review Point: 2. In Theorem 1, please define what the notation “omega_d(.)” means.
==================================================

Focused review:

Weakness:
The unbalanced data scenario has not been properly explored by experiments. Under what circumstances can it be counted as an unbalanced data scenario, and what is the data ratio? Therefore, the experiments should not pay more attention to one given setting like TED, WMT, etc., but should construct unbalanced scenarios of different ratios by sampling data in one setting like WMT to verify this important issue.
There is a lack of a reasonable ablation study on the upsampling parameter T, so we cannot confirm whether the oversampling overfit phenomenon will occur, and to what extent will the upsampling reach.
Some baselines are missing in the experimental comparison, such as 1) giving different weights to the loss of unbalanced translation pairs so that in the later stages of training, there will be no situation where rich-resource pairs dominate the training loss; 2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model.
In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense.
Missing References:
Aghajanyan, Armen, et al. "Better Fine-Tuning by Reducing Representational Collapse." International Conference on Learning Representations. 2020.

Review Point: 2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model. In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense. Missing References: Aghajanyan, Armen, et al. "Better Fine-Tuning by Reducing Representational Collapse." International Conference on Learning Representations. 2020.
==================================================

Focused review:

* Missing comparison against persistent sampling: This paper proposes to use short-run MCMC to sample from both the prior and true posterior. In practice, since we have only one prior distribution, sampling from the prior can be also done using persistent sampling which often improves the performance of EBMs by a large margin. It's not clear why the proposed method uses short-run MCMC that can potentially mix slowly and can introduce sampling error. Moreover, Eq. 13 shows that sampling error turns the objective into an upper bound on the log-likelihood. This can be dangerous as the model may start increasing the gap between the distribution of approximate samples and the EBM prior by making the distribution harder to sample from. * Practical limitations: While we have a single prior distribution, we have true posterior distributions as many as training data points. This means that in every parameter update, we require sampling from the true posterior distribution per data point. The computational complexity of sampling from the true posterior is much more than the prior as the former requires evaluation of both prior and decoder networks. This paper limits the decoder to small models with a few layers, but in practice when decoders are deep, running 20 MCMC steps requires evaluating a very expensive network 20 times in each training iteration. It is not clear why the proposed model abandons amortized inference for approximating the true posterior. A variational distribution can be easily used to infer the latent variables in an amortized fashion (as done in VAEs and DVAEs). * Claims on the stability of the algorithm: In line 48, it is claimed that training EBMs unlike GANs doesn't suffer from instability. However, as observed by Du & Mordatch NeurIPS 2019 training energy-based models can be unstable when the sampling procedure in the negative phase cannot catch up with sharp energy functions. In my experience with EBMs, this problem can be a big barrier to training EBMs.

Review Point: * Missing comparison against persistent sampling: This paper proposes to use short-run MCMC to sample from both the prior and true posterior. In practice, since we have only one prior distribution, sampling from the prior can be also done using persistent sampling which often improves the performance of EBMs by a large margin. It's not clear why the proposed method uses short-run MCMC that can potentially mix slowly and can introduce sampling error. Moreover, Eq. 13 shows that sampling error turns the objective into an upper bound on the log-likelihood. This can be dangerous as the model may start increasing the gap between the distribution of approximate samples and the EBM prior by making the distribution harder to sample from.
Review Point: * Practical limitations: While we have a single prior distribution, we have true posterior distributions as many as training data points. This means that in every parameter update, we require sampling from the true posterior distribution per data point. The computational complexity of sampling from the true posterior is much more than the prior as the former requires evaluation of both prior and decoder networks. This paper limits the decoder to small models with a few layers, but in practice when decoders are deep, running 20 MCMC steps requires evaluating a very expensive network 20 times in each training iteration. It is not clear why the proposed model abandons amortized inference for approximating the true posterior. A variational distribution can be easily used to infer the latent variables in an amortized fashion (as done in VAEs and DVAEs).
Review Point: * Claims on the stability of the algorithm: In line 48, it is claimed that training EBMs unlike GANs doesn't suffer from instability. However, as observed by Du & Mordatch NeurIPS 2019 training energy-based models can be unstable when the sampling procedure in the negative phase cannot catch up with sharp energy functions. In my experience with EBMs, this problem can be a big barrier to training EBMs.
==================================================

Focused review:

1) The work is incremental, nothing really new is proposed (expecially in term of solutions). 2) I am not fully convinced that the argument of the paper holds for GRACLUS for two reasons: - It is well known that some problems in a graph have a dual problem in the complement graph (e.g. clique vs. independent set). Thus, it might be that the fact the model learned something about the dual problem in the complement graph is sufficient to explain the similar performances. - The authors do not provide strikingly convincing evidence that pooling in GRACLUS learns homogeneous representations (as opposed to the other methods where the evidence is clear) However, I am willing to increase my judgement if the authors are able to convince me in the rebuttal phase.

Review Point: 1) The work is incremental, nothing really new is proposed (expecially in term of solutions).
Review Point: 2) I am not fully convinced that the argument of the paper holds for GRACLUS for two reasons:
Review Point: - It is well known that some problems in a graph have a dual problem in the complement graph (e.g. clique vs. independent set). Thus, it might be that the fact the model learned something about the dual problem in the complement graph is sufficient to explain the similar performances.
Review Point: - The authors do not provide strikingly convincing evidence that pooling in GRACLUS learns homogeneous representations (as opposed to the other methods where the evidence is clear) However, I am willing to increase my judgement if the authors are able to convince me in the rebuttal phase.
==================================================

Focused review:

- The fine-tuning process takes long time to finish. The cross-domain experiments on news show that in-domain tuning is still necessary. - Importantly, as the authors notice, the reward functions can pick up minor details, which means it can be sensitive to the change of summaries, which can be about content or grammar. Given that the reward function is key for model training, it is thus necessary to give more analysis on how the quality and diversity of the training pairs would affect reward function learning.

Review Point: - The fine-tuning process takes long time to finish. The cross-domain experiments on news show that in-domain tuning is still necessary.
Review Point: - Importantly, as the authors notice, the reward functions can pick up minor details, which means it can be sensitive to the change of summaries, which can be about content or grammar. Given that the reward function is key for model training, it is thus necessary to give more analysis on how the quality and diversity of the training pairs would affect reward function learning.
==================================================

Focused review:

Weaknesses: 1). Some related work is not mentioned and some potential baselines are missed. Actually person-job fit is quite a mature topic in data mining and information retrieval. The first two baselines (Markov regression and bag-of-jobs) seem to come from econometrics, and the third baseline (NEMO) is quite old in data mining. The authors should investigate more and discover more strong baselines to verify the effectiveness.
2). Depside the effectiveness of pretrain-finetune paradigm of transformer architecture, it has been well studied in other tasks such as NLP and CV. Therefore, the technical novelty is inadequate as the paper seems to be an application of transformer in job prediction task.

Review Point: 1). Some related work is not mentioned and some potential baselines are missed. Actually person-job fit is quite a mature topic in data mining and information retrieval. The first two baselines (Markov regression and bag-of-jobs) seem to come from econometrics, and the third baseline (NEMO) is quite old in data mining. The authors should investigate more and discover more strong baselines to verify the effectiveness.
Review Point: 2). Depside the effectiveness of pretrain-finetune paradigm of transformer architecture, it has been well studied in other tasks such as NLP and CV. Therefore, the technical novelty is inadequate as the paper seems to be an application of transformer in job prediction task.
==================================================

Focused review:

Weaknesses
At least in my opinion, Atari 100k is not particularly well benchmarked. I would say that the only "good" algorithm that has been benchmarked by experimenters incentivized to tune the algorithm to maximize performance is EfficientZero. Thus, the extent to which IRIS is good is a bit unclear to me. It would be interesting to see how IRIS performs on the standard Atari benchmark (or, inversely, how something like Dreamer compares to IRIS on Atari 100k).
The statistical precipice paper suggests that results on Atari 100k are reliable (under appropriate metrics) with as few as 10 runs; the submission only uses 5 runs. My gut feeling is that the margin of improvement under various metrics seems substantial enough that it would probably continue to hold under a more reliable number of runs, but it would be good to actually show this.
Addendum: I also concur with reviewer t9Kq: 1) the submission would benefit from additional attention to related work (such as [1],[2],[3]) and 2) additional ablations.
Comments on decision-time planning:
The submission argues: "Moreover, IRIS could be combined with MCTS, both in imagination and in the real environment. Therefore, methods involving lookahead search should not be seen as direct competitors but rather as potential extensions to learning-only methods." In principle, this is of course true. However, as a matter of practice, I am not sure it is as clear. I am not aware of any examples of a non-MuZero-like architecture successfully utilizing decision-time planning. It is plausible to me that algorithms like Dreamer and IRIS, which perform well in a background planning regime, may not enjoy much benefit from decision-time MCTS.
Comments on superhuman performance:
Most notably, human experts were surpassed by deep RL algorithms in a multitude of arcade (Mnih et al., 2015; Schrittwieser et al., 2020; Hafner et al., 2021), real-time strategy (Vinyals et al., 2019; Berner et al., 2019), board (Silver et al., 2016; 2018; Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021; Brown et al., 2020a) games.
I find this sentence is misleading. Deep RL algorithms have not surpassed human experts in most Atari games, as is clearly evidenced by the human world record metric. They have also not surpassed human experts in StarCraft (AlphaStar is only grandmaster level) or DOTA (OpenAI5 played a restricted version of the game and was found to be reliably exploitable by humans).

Review Point: 1) the submission would benefit from additional attention to related work (such as [1],[2],[3]) and 2) additional ablations. Comments on decision-time planning: The submission argues: "Moreover, IRIS could be combined with MCTS, both in imagination and in the real environment. Therefore, methods involving lookahead search should not be seen as direct competitors but rather as potential extensions to learning-only methods." In principle, this is of course true. However, as a matter of practice, I am not sure it is as clear. I am not aware of any examples of a non-MuZero-like architecture successfully utilizing decision-time planning. It is plausible to me that algorithms like Dreamer and IRIS, which perform well in a background planning regime, may not enjoy much benefit from decision-time MCTS. Comments on superhuman performance: Most notably, human experts were surpassed by deep RL algorithms in a multitude of arcade (Mnih et al., 2015; Schrittwieser et al., 2020; Hafner et al., 2021), real-time strategy (Vinyals et al., 2019; Berner et al., 2019), board (Silver et al., 2016; 2018; Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021; Brown et al., 2020a) games. I find this sentence is misleading. Deep RL algorithms have not surpassed human experts in most Atari games, as is clearly evidenced by the human world record metric. They have also not surpassed human experts in StarCraft (AlphaStar is only grandmaster level) or DOTA (OpenAI5 played a restricted version of the game and was found to be reliably exploitable by humans).
==================================================

Focused review:

* The model leverages multiple assumptions that seems to be unfeasible in practice 1. line 99 -- It is assumed that scene consists of K objects. However, when training on unlabelled real images the number of objects in the scene is unknown. The BlockGAN baseline performs training on a single instance real images and then generalises to multi-instance images. How the K parameter is selected during the training? If the manual labelling of the average number of objects is required, then the method does require labeled data (as opposed to unlabelled data in line 3). If the K is chosen randomly, the discriminator will notice the difference between real and synthetic images. 2. line 109 -- For simplicity, we assume \theta_k in R^2 to be a 2D translation. The BlockGAN baseline uses the 3D poses to model 3D scenes. If the RELATE model operates only with 2D poses, it should fail to generate true perspective scenes (the example with the car traffic in the Supplementary is nearly 2D). A field of application of a RELATE that can model only 2D dynamics (e.g., top view) seems to be limited. * Some details regarding the preparation of the training dataset in terms of the number of objects (K) are required to reproduce the reported results.

Review Point: * The model leverages multiple assumptions that seems to be unfeasible in practice 1. line 99 -- It is assumed that scene consists of K objects. However, when training on unlabelled real images the number of objects in the scene is unknown. The BlockGAN baseline performs training on a single instance real images and then generalises to multi-instance images. How the K parameter is selected during the training? If the manual labelling of the average number of objects is required, then the method does require labeled data (as opposed to unlabelled data in line 3). If the K is chosen randomly, the discriminator will notice the difference between real and synthetic images.
Review Point: 2. line 109 -- For simplicity, we assume \theta_k in R^2 to be a 2D translation. The BlockGAN baseline uses the 3D poses to model 3D scenes. If the RELATE model operates only with 2D poses, it should fail to generate true perspective scenes (the example with the car traffic in the Supplementary is nearly 2D). A field of application of a RELATE that can model only 2D dynamics (e.g., top view) seems to be limited.
Review Point: * Some details regarding the preparation of the training dataset in terms of the number of objects (K) are required to reproduce the reported results.
==================================================

Focused review:

weakness of the paper is the results on the full navigation task, which are weak in comparison to past work. As the paper points out, this past work has used a variety of training and inference conditions which improve performance, and are likely orthogonal to the contributions here. However, much of this past work has also reported results without these augmentations, and those results are comparable or better than the navigation performance here. It would be clearer if the paper presented these results (for example the "co-grounding" and "greedy decoding" ablation of Ma et al. which obtains 42 SR and 28 SPL on the val-unseen environments, and the Behavioral Cloning (IL) ablation of Tan et al, which obtains 43.6 SR and 40 SPL on val-unseen) rather than the augmented settings, or explained why they are not comparable. In particular, since this paper uses the panoramic state representation of Fried et al, and an action space similar to theirs, it seems that their "panoramic space" ablation model might be a more appropriate baseline than the non-panoramic Seq2Seq model compared to here.  However, all these differences seem at least partly explainable due to the use of different ResNet visual features than these past works. In addition, the results on the goal prediction task show a substantial improvement over the strong LingUNet model. *Clarity* I found the paper overall extremely clear about the model details and the intuition for each part, and the motivation for the work. There were a few minor details about the training procedure that were underspecified: - Is the true state sequence in 245 always the human trajectory, or does it include the exploration that is done by the model during training? - When training the policy with cross-entropy loss, are the parameters of the rest of the network (e.g. the filter and semantic map) also updated (or are they just updated by the filter supervision)? - Is the mapper-filter in the experiments 5.2 produced by training without the policy, or does this take a trained full model and remove the policy component? (it seems likely to be the first one, but the text is a bit unclear) *Significance* While the results on the full navigation task don't show an improvement over past work, I think that the model class is still likely to be built upon by researchers in this area. Past work has seen two high level problems in this task, which models like this one may be able to address: (1) Substantial improvements from exploration of the environment during inference time. Having a model with an explicit simulated planning component makes it possible to see how much the simulated planning using the learned environment representation could reduce the need for actual exploration. (2) A generalization gap in novel environments. It seems promising that this model has no gap in performance between seen and unseen environments, although the reason for this is not explained in this work. *Minor comments* - 238: The policy does seem to indirectly have access to a representation of the instruction and semantic map through the belief network; this could be clarified by saying that it has no direct access. - I found it surprising that incorporating the agent's heading into the belief state has such a large impact on performance, given the panoramic visual representation and action space. Some discussion of this would be helpful.

Review Point: - Is the true state sequence in 245 always the human trajectory, or does it include the exploration that is done by the model during training?
Review Point: - When training the policy with cross-entropy loss, are the parameters of the rest of the network (e.g. the filter and semantic map) also updated (or are they just updated by the filter supervision)?
Review Point: - Is the mapper-filter in the experiments 5.2 produced by training without the policy, or does this take a trained full model and remove the policy component? (it seems likely to be the first one, but the text is a bit unclear) *Significance* While the results on the full navigation task don't show an improvement over past work, I think that the model class is still likely to be built upon by researchers in this area. Past work has seen two high level problems in this task, which models like this one may be able to address: (1) Substantial improvements from exploration of the environment during inference time. Having a model with an explicit simulated planning component makes it possible to see how much the simulated planning using the learned environment representation could reduce the need for actual exploration. (2) A generalization gap in novel environments. It seems promising that this model has no gap in performance between seen and unseen environments, although the reason for this is not explained in this work. *Minor comments* - 238: The policy does seem to indirectly have access to a representation of the instruction and semantic map through the belief network; this could be clarified by saying that it has no direct access.
Review Point: - I found it surprising that incorporating the agent's heading into the belief state has such a large impact on performance, given the panoramic visual representation and action space. Some discussion of this would be helpful.
==================================================

Focused review:

Weaknesses:
Lacking of discussions or motivations for the importance of the proposed idea
Empirical results: Can be on toy tasks
The paper pursues an interesting research direction, which tries to unify existing POMDP formalisms. The approach looks very promising. The proposed design of the critic is very interesting. It would become very interesting if the paper can provides some basic empirical results on toy tasks to show all important claim in practice. - As the unified framework can now obtain provably efficient learning for most POMDP formalisms. Is there any limitations of its, e.g. can it do the same for any general POMDP formulations (continuous or infinite spaces)? - How can one understand agnostic learning? In Algorithm, is z just defined as historical observations? Or is it in the form of belief?

Review Point: - As the unified framework can now obtain provably efficient learning for most POMDP formalisms. Is there any limitations of its, e.g. can it do the same for any general POMDP formulations (continuous or infinite spaces)?
Review Point: - How can one understand agnostic learning? In Algorithm, is z just defined as historical observations? Or is it in the form of belief?
==================================================

Focused review:

1. What is the runtime of MNCE-RL compared to existing approaches based on graph CNNs? 2. PPO is an on-policy approach, which requires that data used to update the policy are collected by the same policy. Can MNCE-RL use initial (graph, reward) pairs that were collected be a different policy? Labeled data are available in many practical optimization settings but it is unclear if MNCE-RL can make use of these data. 3. l222: 'We report the top 3 property scores, the 50th best score, and the average score of the top 50 molecules'. These metrics can be optimized by generating repeatedly the same molecule with a high score or very similar molecules. However, desired is the ability to find many diverse molecules with a high score. Do you take a minimum distance between generated molecules into account when computing these metrics? 4. Section 'Property optimization with limited property evaluations'. Instead of reporting the best scores after a fixed number (500) of function evaluations, I suggest plotting the maximum of f depending on the number of function evaluations. This will show more clearly how fast methods optimize f and does not require choosing a certain number of problem evaluations. 5. Section 'Generation of novel molecules with antibacterial property'. It is unclear if the classifier that is optimized by MNCE-RL is a the ground truth f(m), or a surrogate f'(m) for the unknown and costly to evaluate objective function f(m) (antimicrobial activity quantified experimentally). In. the first case, it is necessary to report the number of function evaluation performed and comparing MNCE-RL for making the claim 'illustrates the ability of MNCE-RL to generate antibacterial candidate molecules with only limited labeled samples'. In the latter case, the number of function evaluation is negligible since the function is inexpensive to evaluate (it is a model) and can be also optimized by other methods (e.g. JT-VAE). 6. How and which hyper-parameters of MNCE-RL and baseline methods were optimized?

Review Point: 1. What is the runtime of MNCE-RL compared to existing approaches based on graph CNNs?
Review Point: 2. PPO is an on-policy approach, which requires that data used to update the policy are collected by the same policy. Can MNCE-RL use initial (graph, reward) pairs that were collected be a different policy? Labeled data are available in many practical optimization settings but it is unclear if MNCE-RL can make use of these data.
Review Point: 3.l222: 'We report the top 3 property scores, the 50th best score, and the average score of the top 50 molecules'. These metrics can be optimized by generating repeatedly the same molecule with a high score or very similar molecules. However, desired is the ability to find many diverse molecules with a high score. Do you take a minimum distance between generated molecules into account when computing these metrics?
Review Point: 4. Section 'Property optimization with limited property evaluations'. Instead of reporting the best scores after a fixed number (500) of function evaluations, I suggest plotting the maximum of f depending on the number of function evaluations. This will show more clearly how fast methods optimize f and does not require choosing a certain number of problem evaluations.
Review Point: 5. Section 'Generation of novel molecules with antibacterial property'. It is unclear if the classifier that is optimized by MNCE-RL is a the ground truth f(m), or a surrogate f'(m) for the unknown and costly to evaluate objective function f(m) (antimicrobial activity quantified experimentally). In. the first case, it is necessary to report the number of function evaluation performed and comparing MNCE-RL for making the claim 'illustrates the ability of MNCE-RL to generate antibacterial candidate molecules with only limited labeled samples'. In the latter case, the number of function evaluation is negligible since the function is inexpensive to evaluate (it is a model) and can be also optimized by other methods (e.g. JT-VAE).
Review Point: 6. How and which hyper-parameters of MNCE-RL and baseline methods were optimized?
==================================================

Focused review:

- The technical contribution seems limited. Using a temperature parameter is not novel and is widely used in a lot of settings. Similarly, computing distances between images and labels was already done but this combination seems novel. - The authors should give more information about the k-NN. For example, they should explain what feature (normalization or other preprocessings) is used. They should explain how they choose k and how sensible is the model to this hyperparameter. - For d_L distances, I wonder what happens when P(x) or N(x) is empty? - How is the category label \psi(c) computed?

Review Point: - The technical contribution seems limited. Using a temperature parameter is not novel and is widely used in a lot of settings. Similarly, computing distances between images and labels was already done but this combination seems novel.
Review Point: - The authors should give more information about the k-NN. For example, they should explain what feature (normalization or other preprocessings) is used. They should explain how they choose k and how sensible is the model to this hyperparameter.
Review Point: - For d_L distances, I wonder what happens when P(x) or N(x) is empty?
==================================================

Focused review:

Weakness: 1. Contribution is not convincing. They argue that the traditional adaptive filterbank uses a scalar weight shared by all nodes, and their proposed method learns different weights for different nodes. However, in my opinion, FAGCN can do the same thing. 2. There is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric. 3. The novelty of the idea is not enough. In addition to the limitations pointed out above, both new metric and method are relatively straightforward. 4. The improvement in Table 4 does not seem statistically significant because of high variance. 5. There is a problem with the typesetting of the paper.
In addition to the limitations mentioned in the paper, the intrinsic relationship between the proposed metric and method should be taken into consideration. No potential negative societal impact.

Review Point: 1. Contribution is not convincing. They argue that the traditional adaptive filterbank uses a scalar weight shared by all nodes, and their proposed method learns different weights for different nodes. However, in my opinion, FAGCN can do the same thing.
Review Point: 2. There is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.
Review Point: 3. The novelty of the idea is not enough. In addition to the limitations pointed out above, both new metric and method are relatively straightforward.
Review Point: 4. The improvement in Table 4 does not seem statistically significant because of high variance.
Review Point: 5. There is a problem with the typesetting of the paper. In addition to the limitations mentioned in the paper, the intrinsic relationship between the proposed metric and method should be taken into consideration. No potential negative societal impact.
==================================================

Focused review:

- The expected excess risk bound in Theorem 2 is a lot to unpack. The Assumptions 1, 2, especially the former, should be made easier to understand. - What prevents the authors from applying this result to a non-residual deep network? It is important to discuss this.

Review Point: - The expected excess risk bound in Theorem 2 is a lot to unpack. The Assumptions 1, 2, especially the former, should be made easier to understand.
Review Point: - What prevents the authors from applying this result to a non-residual deep network? It is important to discuss this.
==================================================

Focused review:

Again, I am not an expert, so my questions are conceptual, and my score should be interpreted as "undecided", but if I'm convinced by your answers or the other reviewers that my concerns are invalid, which is very probable, I'll recommend accepting. - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the "raw" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening? - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have "training the neural representation" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization? - More a comment: I found the "Algorithm 1" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.

Review Point: - in 4.1, you whiten the intermediate neural representation. Since your paper explicitly wants to investigate the effect of having the neural representation, the additional whitening seems like it adds a second effect into the mix. If you need to do this because of the proofs, then at least it seems like you should also whiten the "raw" representations in section 3. Would the bounds for these change if you did? How much of the proofs rely on this whitening?
Review Point: - You keep the neural representation at its random initialization. How much is it really still a neural representation and not a simple random up-project of the data? When we think of neural intermediate representations, they arise because all the layers are learned, which here is not the case. I understand that you have "training the neural representation" in your future work section, but my criticism isn't that you haven't done it, my criticism is as to in what respect your results are still telling us anything about neural representations. What properties of your random representations actually make the difference for the sample complexity here? The nonlinearities? The fact that you have higher dimension? The whitening you do after? The randomness in initialization?
Review Point: - More a comment: I found the "Algorithm 1" box to be a bit superfluous. The paper is written very well, such that the content of the box is entirely obvious and just repetition at the point where the box appears. But if you have the space, I guess it can't hurt also.
==================================================

Focused review:

Weaknesses:
- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
- The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution. Suggestions:
- It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
- It would be great if authors can address some of the above weaknesses in the revision to make this a good paper.
Review Summary:
- Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community. ------------------
After the Rebuttal: ------------------
My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work. 
With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance.

Review Point: - There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
Review Point: - From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
Review Point: - The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution. Suggestions:
Review Point: - It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
Review Point: - It would be great if authors can address some of the above weaknesses in the revision to make this a good paper. Review Summary:
Review Point: - Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community. ------------------ After the Rebuttal: ------------------ My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work. With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance.
==================================================

Focused review:

Weaknesses Motivation.
Use of synthetic data. I'm having hard time wrapping my head around how the model's behavior on images synthetized from a Gaussian mixture (i.e probably very far away in the input space from both the source and target distributions) could inform the model's behavior on real-world images. Could authors provide additional motivation ?
*Adversarial perspective: * (See below for a more detailed comment) I'd suggest authors provide more motivation on why they chose to address the general problem of "representation quality" through the lens of adversarial robustness, as the link between adversarial robustness and generalization is not trivial (although I concede it is not unreasonable either).
Lack of comparisons. Authors do not really compare to any other line of work. Although I could concede that other works have never tried to address this exact setting (i.e completly task-agnostic evaluation of representations), authors do not seem to have put any effort into adapting any technique to their setting, which makes the relevance of their score (on top of my next comment) hard to evaluate. For instance, cited lines of work that use unsupervised criterions (e.g. mutual information or Minimum Description Length) could be adaptated to the synthetic binary classification task that authors propose (e.g. comparing the mutual information between representations and labels in the input space vs in the representation space), and provide at least some point of comparison.
Hardly falsifiable hypothesis. The contribution section claims that the score evaluates the quality of the representations. On the other hand, authors do not find any strong positive correlation between their score and actual tradeoff performances of models in Table 3. (e.g. standard probing for ViT-L, corresponding to ϵ = 0
, has smaller score but significantly higher OOD generalization). Authors comment on that part that "although SyncBench-score may share trends with empricial real-life tasks, it is meant to characterize a general behavior of the pretrained representations." This feel a bit hand-wavy to me, in the sense that the definition of "general behavior" can mean everything and anything. I believe the paper would benefit from clearly defining what the score is meant to act as a proxy of, and explictly show positive correlation between their score and this metric.
Lack of experiments. Echoing to my previous point, even if strongly positive correlation had actually been found, more experiments would be needed to support the initial goal/claims of producing a score that applies to a wide range of pretrained models and informs downstream accuracy. In particular, more architectures should be tried (at least 1 ConvNet), more datasets (CIFAR is a relatively outdated choice considering the myriad of more recent vision datasets), and at least one additional task (e.g. segmentation/detection ? or even anything in another modality?) to support the task-agnosticity part.

Review Point: * (See below for a more detailed comment) I'd suggest authors provide more motivation on why they chose to address the general problem of "representation quality" through the lens of adversarial robustness, as the link between adversarial robustness and generalization is not trivial (although I concede it is not unreasonable either). Lack of comparisons. Authors do not really compare to any other line of work. Although I could concede that other works have never tried to address this exact setting (i.e completly task-agnostic evaluation of representations), authors do not seem to have put any effort into adapting any technique to their setting, which makes the relevance of their score (on top of my next comment) hard to evaluate. For instance, cited lines of work that use unsupervised criterions (e.g. mutual information or Minimum Description Length) could be adaptated to the synthetic binary classification task that authors propose (e.g. comparing the mutual information between representations and labels in the input space vs in the representation space), and provide at least some point of comparison. Hardly falsifiable hypothesis. The contribution section claims that the score evaluates the quality of the representations. On the other hand, authors do not find any strong positive correlation between their score and actual tradeoff performances of models in Table 3. (e.g. standard probing for ViT-L, corresponding to ϵ = 0 , has smaller score but significantly higher OOD generalization). Authors comment on that part that "although SyncBench-score may share trends with empricial real-life tasks, it is meant to characterize a general behavior of the pretrained representations." This feel a bit hand-wavy to me, in the sense that the definition of "general behavior" can mean everything and anything. I believe the paper would benefit from clearly defining what the score is meant to act as a proxy of, and explictly show positive correlation between their score and this metric. Lack of experiments. Echoing to my previous point, even if strongly positive correlation had actually been found, more experiments would be needed to support the initial goal/claims of producing a score that applies to a wide range of pretrained models and informs downstream accuracy. In particular, more architectures should be tried (at least 1 ConvNet), more datasets (CIFAR is a relatively outdated choice considering the myriad of more recent vision datasets), and at least one additional task (e.g. segmentation/detection ? or even anything in another modality?) to support the task-agnosticity part.
==================================================

Focused review:

Weakness] 1 Theoretical analysis in Sec. 3.1 is quite standard and does not relate to adversarial examples.
2 On page 5 (below eq 8), there is an argument that "the above method with LS focuses more on the smoothness of CE loss surface, which may ignore the attack of target adversarial example." Could I interpret it in this way that LS can lead to significant issue gradient obfuscation that degrades the true robustness? If so, the main argument in this paper is that "robustness via adaptive LS" is not supported well.
3 In Figure 2 (b). It is really hard to infer the stated messages that illustrate the superiority of the proposal.
[Questions] 1 In Figure 6, why PGD is up-trend, but CW is down-trend over LS degree?
2 In Lemma 1, I get confused about why constraining the margin between any logits could induce better robustness. Usually, constraining the margin between any logits could obfuscate gradients, which leads to an illusion of higher robustness.
3 Besides, I doubt the message from Theorem 1. Let us think in an extreme way: the loss surface is entirely flat. The flat loss will also harm the generalization, which puts no use of robustness.

Review Point: 1 Theoretical analysis in Sec. 3.1 is quite standard and does not relate to adversarial examples.
Review Point: 2 On page 5 (below eq 8), there is an argument that "the above method with LS focuses more on the smoothness of CE loss surface, which may ignore the attack of target adversarial example." Could I interpret it in this way that LS can lead to significant issue gradient obfuscation that degrades the true robustness? If so, the main argument in this paper is that "robustness via adaptive LS" is not supported well.
Review Point: 3 In Figure 2 (b). It is really hard to infer the stated messages that illustrate the superiority of the proposal. [Questions] 1 In Figure 6, why PGD is up-trend, but CW is down-trend over LS degree?
Review Point: 2 In Lemma 1, I get confused about why constraining the margin between any logits could induce better robustness. Usually, constraining the margin between any logits could obfuscate gradients, which leads to an illusion of higher robustness.
Review Point: 3 Besides, I doubt the message from Theorem 1. Let us think in an extreme way: the loss surface is entirely flat. The flat loss will also harm the generalization, which puts no use of robustness.
==================================================

Focused review:

## Summary of Weaknesses The weaknesses of the paper are as follows: 	1. It is not clear how Relative slot accuracy correlates to the precision and recall of the model. 
	2. The paper lacks theoretical analysis and justification, which is required while defining a new metric. 
## Comments Suggestions and Typos Theoretical analysis and evidences should be added in the paper in addition to experimental evidences. 

Review Point: 1. It is not clear how Relative slot accuracy correlates to the precision and recall of the model.
==================================================

Focused review:

Weakness:
1. The literature review is inaccurate, and connections to prior works are not sufficiently discussed. To be more specific, there are three connections, (i) the connection of (1) to prior works on multivariate unlabeled sensing (MUS), (ii) the connection of (1) to prior works in unlabeled sensing (US), and (iii) the connection of the paper to (Yao et al., 2021). 
  (i) In the paper, the authors discussed this connection (i). However, the experiments shown in Figure 2 do not actually use the MUS algorithm of (Zhang & Li, 2020) to solve (1); instead the algorithm is used to solve the missing entries case. This seems to be an unfair comparison as MUS algorithms are not designed to handle missing entries. Did the authors run matrix completion prior to applying the algorithm of (Zhang & Li, 2020)? Also, the algorithm of (Zhang & Li, 2020) is expected to fail in the case of dense permutation.
  (ii) Similar to (i), the methods for unlabeled sensing (US) can also be applied to solve (1), using one column of B_0 at a time. There is an obvious advantage because some of the US methods can handle arbitrary permutations (sparse or dense), and they are immune to initialization. In fact, these methods were used in (Yao et al., 2021) for solving more general versions of (1) where each column of B has undergone arbitrary and usually different permutations; moreover, this can be applied to the d-correspondence problem of the paper. I kindly wish the authors consider incoporating discussions and reviews on those methods.
  (iii) Finally, the review on (Yao et al., 2021) is not very accurate. The framework of (Yao et al., 2021), when applied to (1), means that the subspace that contains the columns of A and B is given (when generating synthetic data the authors assume that A and B come from the same subspace). Thus the first subspace-estimation step in the pipeline of (Yao et al., 2021) is automatically done; the subspace is just the column space of A. As a result, the method of (Yao et al., 2021) can handle the situation where the rows of B are densely shuffled, as discussed above in (ii). Also, (Yao et al., 2021) did not consider only "a single unknown correspondence". In fact, (Yao et al., 2021) does not utilize the prior knowledge that each column of B is permuted by the same permutation (which is the case of (1)), instead it assumes every column of B is arbitrarily shuffled. Thus it is a more general situation of (1) and of the d-correspondence problem. Finally, (Yao et al., 2021) discusses theoretical aspects of (1) with missing entries, while an algorithm for this is missing until the present work. 
2. In several places the claims of the paper are not very rigorous. For example,
  (i) Problem (15) can be solved via linear assignment algorithms to global optimality, why do the authors claim that "it is likely to fall into an undesirable local solution"? Also I did not find a comparison of the proposed approach with linear assignment algorithms.
  (ii) Problem (16) seems to be "strictly convex", not "strongly convex". Its Hessian has positive eigenvalues everywhere but the minimum eigenvalue is not lower bounded by some positive constant. This is my feeling though, as in the situation of logistic regression, please verify this.
  (iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Experiments show that the algorithm needs > 1000 iterations to converge. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). Also I did not see any report on running times. Thus I feel uncomfortable to see the author claim in Section 5 that "we propose a highly efficient algorithm".
3. Even though an error bound is derived in Theorem 1 for the nuclear norm minimization problem, there is no guarantee of success on the alternating minimization proposal. Moreover, the algorithm requires several parameters to tune, and is sensitive to initialization. As a result, the algorithm has very lage variance, as shown in Figure 3 and Table 1.  Questions:
1. In (3) the last term r+H(pi_P) and C(pi_P) is very interesting. Could you provide some intuition how it shows up, and in particular give an example?
2. I find Assumption 1 not very intuitive; and it is unclear to me why "otherwise the influence of the permutation will be less significant". Is it that the unknown permutation is less harmful if the magnitudes of A and B are close?
3. Solving the nuclear norm minimization program seems to be NP-hard as it involves optimization over permutation matrices and a complicated objective. Is there any hardness result for this problem?
Suggestions: The following experiments might be useful.
1. Sensitivity to permutation sparsity: As shown in the literature of unlabeled sensing, the alternating minimization of (Abid et al., 2017) works well if the data are sparsely permuted. This might also apply to the proposed alternating minimization algorithm here.
2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that  M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.
3. Sensitivity to other hyper-parameters.
Minor Comments on language usage: (for example)
 1. "we typically considers" in the above of (7)
 2. "two permutation" in the above of Theorem 1
 3. "until converge" in the above of (14)
 4. ......
Please proofread the paper and fix all language problems.

Review Point: 1. The literature review is inaccurate, and connections to prior works are not sufficiently discussed. To be more specific, there are three connections, (i) the connection of (1) to prior works on multivariate unlabeled sensing (MUS), (ii) the connection of (1) to prior works in unlabeled sensing (US), and (iii) the connection of the paper to (Yao et al., 2021). (i) In the paper, the authors discussed this connection (i). However, the experiments shown in Figure 2 do not actually use the MUS algorithm of (Zhang & Li, 2020) to solve (1); instead the algorithm is used to solve the missing entries case. This seems to be an unfair comparison as MUS algorithms are not designed to handle missing entries. Did the authors run matrix completion prior to applying the algorithm of (Zhang & Li, 2020)? Also, the algorithm of (Zhang & Li, 2020) is expected to fail in the case of dense permutation. (ii) Similar to (i), the methods for unlabeled sensing (US) can also be applied to solve (1), using one column of B_0 at a time. There is an obvious advantage because some of the US methods can handle arbitrary permutations (sparse or dense), and they are immune to initialization. In fact, these methods were used in (Yao et al., 2021) for solving more general versions of (1) where each column of B has undergone arbitrary and usually different permutations; moreover, this can be applied to the d-correspondence problem of the paper. I kindly wish the authors consider incoporating discussions and reviews on those methods. (iii) Finally, the review on (Yao et al., 2021) is not very accurate. The framework of (Yao et al., 2021), when applied to (1), means that the subspace that contains the columns of A and B is given (when generating synthetic data the authors assume that A and B come from the same subspace). Thus the first subspace-estimation step in the pipeline of (Yao et al., 2021) is automatically done; the subspace is just the column space of A. As a result, the method of (Yao et al., 2021) can handle the situation where the rows of B are densely shuffled, as discussed above in (ii). Also, (Yao et al., 2021) did not consider only "a single unknown correspondence". In fact, (Yao et al., 2021) does not utilize the prior knowledge that each column of B is permuted by the same permutation (which is the case of (1)), instead it assumes every column of B is arbitrarily shuffled. Thus it is a more general situation of (1) and of the d-correspondence problem. Finally, (Yao et al., 2021) discusses theoretical aspects of (1) with missing entries, while an algorithm for this is missing until the present work.
Review Point: 2. In several places the claims of the paper are not very rigorous. For example, (i) Problem (15) can be solved via linear assignment algorithms to global optimality, why do the authors claim that "it is likely to fall into an undesirable local solution"? Also I did not find a comparison of the proposed approach with linear assignment algorithms. (ii) Problem (16) seems to be "strictly convex", not "strongly convex". Its Hessian has positive eigenvalues everywhere but the minimum eigenvalue is not lower bounded by some positive constant. This is my feeling though, as in the situation of logistic regression, please verify this. (iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Experiments show that the algorithm needs > 1000 iterations to converge. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). Also I did not see any report on running times. Thus I feel uncomfortable to see the author claim in Section 5 that "we propose a highly efficient algorithm".
Review Point: 3. Even though an error bound is derived in Theorem 1 for the nuclear norm minimization problem, there is no guarantee of success on the alternating minimization proposal. Moreover, the algorithm requires several parameters to tune, and is sensitive to initialization. As a result, the algorithm has very lage variance, as shown in Figure 3 and Table 1. Questions:
Review Point: 1. In (3) the last term r+H(pi_P) and C(pi_P) is very interesting. Could you provide some intuition how it shows up, and in particular give an example?
Review Point: 2. I find Assumption 1 not very intuitive; and it is unclear to me why "otherwise the influence of the permutation will be less significant". Is it that the unknown permutation is less harmful if the magnitudes of A and B are close?
Review Point: 3. Solving the nuclear norm minimization program seems to be NP-hard as it involves optimization over permutation matrices and a complicated objective. Is there any hardness result for this problem? Suggestions: The following experiments might be useful.
Review Point: 1. Sensitivity to permutation sparsity: As shown in the literature of unlabeled sensing, the alternating minimization of (Abid et al., 2017) works well if the data are sparsely permuted. This might also apply to the proposed alternating minimization algorithm here.
Review Point: 2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.
Review Point: 3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. "we typically considers" in the above of (7) 2. "two permutation" in the above of Theorem 1 3. "until converge" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.
==================================================

Focused review:

1. Under the identification assumption, if other causal functionals exist, such as regression adjustment and IPTW, what is the advantage of the WERM-ID? 2. In the empirical simulation, there proposed method is only compared with plug-in estimands. What about other weighting methods? 3. The paper contains many technical details but lacks a detailed explanation. If the page limit is a major concern, the author may need to decide what are the essential messages to express and explain them very clearly, instead of thrusting and skipping over many materials. See the Clarity section for more details. ###### post rebuttal comment: The author's feedback addresses the major concerns of the reviewer. I modified the evaluation accordingly to marginal above acceptance.

Review Point: 1. Under the identification assumption, if other causal functionals exist, such as regression adjustment and IPTW, what is the advantage of the WERM-ID?
Review Point: 2. In the empirical simulation, there proposed method is only compared with plug-in estimands. What about other weighting methods?
Review Point: 3. The paper contains many technical details but lacks a detailed explanation. If the page limit is a major concern, the author may need to decide what are the essential messages to express and explain them very clearly, instead of thrusting and skipping over many materials. See the Clarity section for more details. ###### post rebuttal comment: The author's feedback addresses the major concerns of the reviewer. I modified the evaluation accordingly to marginal above acceptance.
==================================================

Focused review:

Weakness: There are some minor problems / questions: Questions:
In Figure 1, the author compares the computational time across different estimators, is this comparison based on the numbers from different implementations, or are all the estimators implemented in the same code base? Instead of shown the comparison of time, it would be great to include a discussion about complexity analysis on each algorithms, which is not subject to the difference in the implementation and helps the readers / reviewers to better understand how the estimator scales with increasing number of categories. E.g. ARSM requires O ( C 2 )
evaluations, where C
is number of categories.
The author(s) discussed the multi-sample version, ZGR s = 2, in experiment section. How does it formulated? Taking average of two estimations from each samples?
Is there any analysis for scaling the number of categories? Would the relative performances of different estimators hold for larger / smaller number of categories?
MNIST is a standard yet simple task. It would be good to extend the analysis on other benchmarks as well, e.g. FashionMNIST and Omniglot.
The author(s) claimed that the bias is very small comparing to the variance, based on the bias and variance estimations at "an evaluation point" "after 100 epochs of training with ARSM". It would be nice if this analysis could be done at an early stage of the training.
A side question: instead of setting the temperature to zero or a finite number, how performant is the model if one makes the temperature learnable?
Missing baselines:
Binary case: as the paper considers the binary case in 3.1. In this case, it's worth to compare against the latest unbiased gradient estimators for binary variables, ARM (Yin et al. 2019) and DisARM (Dong et al. 2020). Both are using antithetical sampling to improve REINFORCE, and DisARM is Rao-Blackwellized ARM by analytically integrating out the randomness.
Categorical case: ARSM is a categorical version of ARM. However, previous research (Dong et al. 2021) found ARSM underperform simpler REINFORCE Leave-one-out (RLOO, Kool et al. 2019) baselines by significant amount with comparable number of samples. Also, Dong et al. 2021 proposed Rao-Blackwellized ARSM with binary reparameterization, further improves RLOO.
Missing References and things to be fixed:
line below Eq (1) "It has proven efficient in practice to ..." Please add reference for this. Also the sentence should be "It has been proven ..."
Eq (2), in general, especially for VAE, as the paper considered in experiments, "x" the latent variable, therefore it is usually sampled from posterior distribution. In this case, L ( x )
will also depends on η
. Correct derivation should include two terms, e.g. Dong el at. 2021 Eq (2) and relevant discussions, but the term with gradients of L ( x )
can typically be estimated with a single Monte Carlo sample and neglected from discussion.
Broken sentence: line below Theorem 2: "By quadratic functions we understand quadratic functions .. with arbitraty coefficients"
paragraph above 4.2, "GR-MC with K = 10 or K=100 MC samples", it's better to use "M" instead of "K", as the paper used "K" for number of categories.
Please try to make the main text self-contained. E.g. Eq (10) mentioned p Z ( η )
, and it is the logistic density. Writing an expression for it, would help the reader to easily reproduce the results. Reference:
Yin, M. and Zhou, M. (2019). ARM: Augment-REINFORCE-merge gradient for stochastic binary networks. ICLR 2029
Dong, Z., Mnih, A., and Tucker, G. (2020). DisARM: An antithetic gradient estimator for binary latent variables. NeurIPS 2020
Kool, W., van Hoof, H., and Welling, M. (2019). Buy 4 reinforce samples, get a baseline for free! Deep RL Meets Structured Prediction ICLR Workshop.
Dong, Z., Mnih, A., and Tucker, G. (2021). Coupled gradient estimators for discrete latent variables. NeurIPS 2021

Review Point: 2021 Eq (2) and relevant discussions, but the term with gradients of L ( x ) can typically be estimated with a single Monte Carlo sample and neglected from discussion. Broken sentence: line below Theorem 2: "By quadratic functions we understand quadratic functions .. with arbitraty coefficients" paragraph above 4.2, "GR-MC with K = 10 or K=100 MC samples", it's better to use "M" instead of "K", as the paper used "K" for number of categories. Please try to make the main text self-contained. E.g. Eq (10) mentioned p Z ( η ) , and it is the logistic density. Writing an expression for it, would help the reader to easily reproduce the results. Reference: Yin, M. and Zhou, M. (2019). ARM: Augment-REINFORCE-merge gradient for stochastic binary networks. ICLR 2029 Dong, Z., Mnih, A., and Tucker, G. (2020). DisARM: An antithetic gradient estimator for binary latent variables. NeurIPS 2020 Kool, W., van Hoof, H., and Welling, M. (2019). Buy 4 reinforce samples, get a baseline for free! Deep RL Meets Structured Prediction ICLR Workshop. Dong, Z., Mnih, A., and Tucker, G. (2021). Coupled gradient estimators for discrete latent variables. NeurIPS 2021
==================================================

Focused review:

1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead. 
2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5. 
1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing. 
2. How would this approach scale to batch size > 1? 
3. What is the required memory overhead to keep 4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc. 

Review Point: 1.While the approach proves to be better than entropy or random routing mechanism its not clear how a simple FFN network with a confidence threshold would perform instead.
Review Point: 2. There is no clear breakdown on how this may work with a variety of architectures such as a swift BART and a Scale T5.
Review Point: 1. What is the impact of using two different swift models in this architecture. Models with different random seeds could easily be ensembled and combined with this approach for routing.
Review Point: 2. How would this approach scale to batch size > 1?
Review Point: 3. What is the required memory overhead to keep 4. How is the performance impacted by changing the various scale models? Medium, another seed for swift model size, etc.
==================================================

Focused review:

Weaknesses and suggestions: 1. The paper is very difficult to follow. While the theory section (Section 4) is reasonably well-written, the rest of the paper needs a substantial rewrite to improve clarity and accessibility. Unfortunately the writing quality makes it difficult to make a strong case for the paper. 2. The experiments only touch upon one aspect of theory discussed in the paper -- the impact of N and M on test performance. A more thorough comparison with SL based few-shot learning and the impact of other factors like number of classes and class imbalance on test performance would make the paper stronger.

Review Point: 2. The experiments only touch upon one aspect of theory discussed in the paper -- the impact of N and M on test performance. A more thorough comparison with SL based few-shot learning and the impact of other factors like number of classes and class imbalance on test performance would make the paper stronger.
==================================================

Focused review:

I have mainly two concerns. 1) The paper's per-sample weight tuning method relies on influence function. However, the explanation of the influence function holds only when the upweight (i.e. parameter weights in this paper's context) is very close to 0. Otherwise, the theoretical approximation of each sample's contribution from Taylor's first-order expansion is not valid any more. If that's the case (e.g. weights are far away from 0 during training iteration), the influence function might not reflect each sample's influence correctly and the weights produced might be unreliable. 2) I think in the experiment, some interesting baselines are missing. For example, supervise learning methods with per-sample weight tuning, and SSL with other existing per-sample weight tuning methods from supervised learning studies if possible. Further, in the ablation studies section, as the proposed algorithm can be extended to all training samples rather than limited to only unlabeled sample, it could shed more insights if comparing to applying influence function to all training sample, and comparing to weight tuning only on labeled data while the weights fixed on unlabeled samples. These comparisons can help us understand better the roles played by per-sample weight tuning.

Review Point: 1) The paper's per-sample weight tuning method relies on influence function. However, the explanation of the influence function holds only when the upweight (i.e. parameter weights in this paper's context) is very close to 0. Otherwise, the theoretical approximation of each sample's contribution from Taylor's first-order expansion is not valid any more. If that's the case (e.g. weights are far away from 0 during training iteration), the influence function might not reflect each sample's influence correctly and the weights produced might be unreliable.
Review Point: 2) I think in the experiment, some interesting baselines are missing. For example, supervise learning methods with per-sample weight tuning, and SSL with other existing per-sample weight tuning methods from supervised learning studies if possible. Further, in the ablation studies section, as the proposed algorithm can be extended to all training samples rather than limited to only unlabeled sample, it could shed more insights if comparing to applying influence function to all training sample, and comparing to weight tuning only on labeled data while the weights fixed on unlabeled samples. These comparisons can help us understand better the roles played by per-sample weight tuning.
==================================================

Focused review:

Weaknesses: - The theoretical results don't have immediate practical implications, although this is certainly understandable given the novelty of the work. As someone who is more of an applied researcher who occasionally dabbles in theory, it would be ideal to see more take-away points for practitioners. The main take-away point that I observed is to query a cluster proportionally to the square root of its size, but it's unclear if this is a novel finding in this paper. - The proposed model produces only 1 node changing cluster per time step on average because the reassignment probability is 1/n. This allows for only very slow dynamics. Furthermore, the proposed evolution model is very simplistic in that no other edges are changed aside from edges with the (on average) 1 node changing cluster. - Motivation by the rate limits of social media APIs is a bit weak. The motivation would suggest that it examines the error given constraints on the number of queries. The paper actually examines the number of probes/queries necessary to achieve a near-optimal error, which is a related problem but not necessarily applicable to the social media API motivation. The resource-constrained sampling motivation is more general and a better fit to the problem actually considered in this paper, in my opinion. Suggestions: Please comment on optimality in the general case. From the discussion in the last paragraph in Section 4.3, it appears that the proposed queue algorithm would is a multiplicative factor of 1/beta from optimality. Is this indeed the case? Why not also show experiment results for just using the algorithm of Theorem 4 in addition to the random baselines? This would allow the reader to see how much practical benefit the queue algorithm provides. Line 308: You state that you show the average and standard deviation, but standard deviation is not visible in Figure 1. Are error bars present but just too small to be visible? If so, state that it is the case. Line 93: "asymptoticall" -> "asymptotically" Line 109: "the some relevant features" -> Remove "the" or "some" Line 182: "queries per steps" -> "queries per step" Line 196-197: "every neighbor of neighbor of v" -> "neighbor of" repeated Line 263: Reference to Appendix in supplementary material shows ?? Line 269: In the equation for \epsilon, perhaps it would help to put parentheses around log n, i.e. (log n)/n rather than log n/n. Line 276: "issues query" -> I believe this should be "issues 1 query" Line 278: "loosing" -> "losing" I have read the author rebuttal and other reviews and have decided not to change my scores. 

Review Point: - The theoretical results don't have immediate practical implications, although this is certainly understandable given the novelty of the work. As someone who is more of an applied researcher who occasionally dabbles in theory, it would be ideal to see more take-away points for practitioners. The main take-away point that I observed is to query a cluster proportionally to the square root of its size, but it's unclear if this is a novel finding in this paper.
Review Point: - The proposed model produces only 1 node changing cluster per time step on average because the reassignment probability is 1/n. This allows for only very slow dynamics. Furthermore, the proposed evolution model is very simplistic in that no other edges are changed aside from edges with the (on average) 1 node changing cluster.
==================================================

Focused review:

- Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.
- [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought. 
- I might have missed it but the results section stated "When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference."; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder? - Would the code and experiments be open sourced like Susanto et al? - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?
- Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline. - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one 

Review Point: - Nothing much, a very well-written and thought out papers and experiments; although it's incremental improvements to the LevT, it's an extensive study with solid empirical evidence for the conjectures stated in the paper.
Review Point: - [Possibly out of scope] The question still remains why can't a NAT learn the alignment mapping with its cross attention to different token positions? It's possibly out of scope of the paper but a good food for thought.
Review Point: - I might have missed it but the results section stated "When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference."; does that mean all the +ACT results only have alignments constraints and prompting during training and no alignments prompting at inference? Actually, it'll be kind of tough to do alignment prompting at inference anyways because should the prompting at every iteration or the first iteration of the decoder?
Review Point: - Would the code and experiments be open sourced like Susanto et al?
Review Point: - Maybe I'm reading too much into Table 4 results, the improvements clearly comes from the constraint training, esp. the soft constraint setup. Thinking out loud, is GIZA++ or any external aligner necessary for the alignment? Can some mechanism/layer be proposed to replace that alignment from GIZA?
Review Point: - Section 6.3 is honest limitation but the improved from 94.25 -> 96.90 is also quite a feat since it's inching at a strong baseline.
Review Point: - Figure 2 also shows where the BLEU is lost in the 10-30% bin, would it be possible to identify all terms in that bin and list them in the appendix with the (source, term) -> LevT vs LevT + ACT outputs? A few possible reason that the model is finding it hard to learn that 10-30% bin, (i) term is mapped to too many multiple targets in training data or mapped to targets that's not in the constraint list of terms in a skewed manner, (ii) the translated outputs though not in the reference are valid translations though not the preferred one
==================================================

Focused review:

weaknesses of previous methods. It could handle a large number of images per probe (N) while considering Intra-set relation. It is batch order invariant, and guarantees set order invariance by employing a Set Permutation Consistency loss.
CAFace provides the best performance boost compared with other feature fusion techniques on IJB-A, IJB-B, IJB-C, and IJB-S benchmarks.
CAFace seems to group low-quality images into a cluster, which has the fusion weith near zero. Weaknesses
L18-19: This definition is only valid for Face Identification, one of two subproblems in Face Recognition. In Face verification, we do not have Gallery or Probe.
It is unclear to me when the cluster centers are trained. Are they trained along with the fusion network and be universal to all probes? Or are they optimized to fit each probe? In the case of universal cluster centers, some analyses of facial images near the cluster centers are recommended to add.
The proposed method seems over-complicated. The authors should add more ablation studies to show the need for the proposed network components. For example, what if we do not use the style embeddings but learn the cluster centers for identity features f_i themselves? What if we do not use the Aggregation Network and simply average the clustered features in F'?
In Section 3.2, the definition of f^{(p)}_{GT} is confusing. Is not all training data labeled? What is the difference between f^{(p)} and f^{(p)}_{GT}?
The authors should improve the description for Fig. 6:
What are C0, C1, and C2? Are they cluster centers? C3 is not visualized?
In CAFace plot, there are many blue points with high cosine similarity to the Gallery. The authors should visualize and analyze the differences between the images corresponding to these points and the red/yellow points that make them have different weights.
Which network layer was used to extract the intermediate feature maps M_i in the experiments, and what is the feature map size?
In Table 1, an empty cell can be either in a merged cell or means "No". It may cause confusion. The authors should improve this, e.g., use 'x' to denote 'No' instead of using an empty cell.
The norm embedding definition should be moved to the main text.

Review Point: 6: What are C0, C1, and C2? Are they cluster centers? C3 is not visualized? In CAFace plot, there are many blue points with high cosine similarity to the Gallery. The authors should visualize and analyze the differences between the images corresponding to these points and the red/yellow points that make them have different weights. Which network layer was used to extract the intermediate feature maps M_i in the experiments, and what is the feature map size? In Table 1, an empty cell can be either in a merged cell or means "No". It may cause confusion. The authors should improve this, e.g., use 'x' to denote 'No' instead of using an empty cell. The norm embedding definition should be moved to the main text.
==================================================

Focused review:

weakness of the proposed method is that its effectiveness still depends critically on the availability of good features \phi. The paper could be strengthened if ideas of how interpolation can be applied to other value function approximators were provided. The experimental evaluation is insufficient as only simple problems are considered and not nearly enough experimentation is provided to show empirically the error scaling, the effect of the anchor point selection, the advantage over averagers, or the dependence on the choice of \phi. More results should have been provided on the dependence between the anchor number and location optimization and the choice of features \phi. The theoretical derivation of the error bounds is illuminating in terms of the structure of error scaling but the bounds themselves do not seem practically useful.   ## Major Comments + The introduction and preliminaries sections are very well written, giving the right context and a formal problem definition.  + The specification of the necessary number of samples to minimize the error between the interpolant and the best linear Q-function approximation in Lemma 2 is nice. The greedy approach to adding anchor points to meet a specified amplification target is a nice contribution.  - The greedy heuristic to construct a good set of anchor points before the learning process is a good addition to the proposed algorithm but is not explored in sufficient detail. A discussion on how complex the optimization problem max_{s,a} ||\theta^{\phi(s,a)}||_1 is should be added. Examples of the numbers, positions of anchor points, and the resulting extrapolation coefficient for common choices of \phi would have been nice to see.   - The experiments are similar and somewhat simple. It would have been interesting to evaluate LAVIER on a real scale problem in addition to the toy problems vs LS-AVI. For example, it would have been great to see how LAVIER performs on a reinforcement learning problem where a good choice of features is not known but polynomial, RBF, or kitchen sink features are used. A comparison versus averagers and/or OPPQ-Learning [15] would have been interesting as well. It would be have been good to provide more variations on the Chain MDP evaluation, in terms of different feature choices and different planning horizon choices.   ## Minor Comments: - Typos: "Whenever the ineherent Bellman error is unbounded," - It would be cleaner to explicitly write the dependence on s_i and a_i in eq. (1) - The meaning of \delta should be introduced more clearly in Lemma 2 - In Fig. 3 middle, what value of C is achieved by the shown anchor points? - The notation is sloppy at places, switching from arguments in parentheses to subscripts for functions or from having two to one subscript (e.g., Proof of Proposition 3, around eq. (2)) 

Review Point: + The specification of the necessary number of samples to minimize the error between the interpolant and the best linear Q-function approximation in Lemma 2 is nice. The greedy approach to adding anchor points to meet a specified amplification target is a nice contribution.
Review Point: - The greedy heuristic to construct a good set of anchor points before the learning process is a good addition to the proposed algorithm but is not explored in sufficient detail. A discussion on how complex the optimization problem max_{s,a} ||\theta^{\phi(s,a)}||_1 is should be added. Examples of the numbers, positions of anchor points, and the resulting extrapolation coefficient for common choices of \phi would have been nice to see.
Review Point: - The experiments are similar and somewhat simple. It would have been interesting to evaluate LAVIER on a real scale problem in addition to the toy problems vs LS-AVI. For example, it would have been great to see how LAVIER performs on a reinforcement learning problem where a good choice of features is not known but polynomial, RBF, or kitchen sink features are used. A comparison versus averagers and/or OPPQ-Learning [15] would have been interesting as well. It would be have been good to provide more variations on the Chain MDP evaluation, in terms of different feature choices and different planning horizon choices. ## Minor Comments:
Review Point: - The notation is sloppy at places, switching from arguments in parentheses to subscripts for functions or from having two to one subscript (e.g., Proof of Proposition 3, around eq. (2))
==================================================

Focused review:

Weakness:
The design of the model is not well-motivated.
the domain gap between target and source is usually the main point to decrease the generalization of the model. However, both the proposed IPNet and FSAT do not directly address the domain shift or use any domain loss. I feel like fusing two domain features brutally will still suffer from domain shift when using the prototyping. Only the task of the few-shot challenge is addressed here.
The idea of using FSAT for separating foreground and background is not well-motivated. Does this module aim to address the issue of missing detection or learn a better representation for feature encoder? Looks like this module is only to regularize the training of feature extractor like ResNet101 or VGG16 with prototypes from IPNet? It would be also good to explain why RPN is not trained in both of the two modules if this is the case.
FSAT is using adversarial learning for foreground and background instead of source or target. While this learning process may help to address the domain gap without aligning the distribution between source and target, would the inherent domain shift affect the performance of the foreground/background separation?
Since we only have few labeled target images and many source images, the issue of data imbalance will be taking place in this model. Will the imbalance of source and target images affect the model? I see only the focal loss applied for boxes imbalance issue.
Lack of analysis:
The idea of prototyping in IPM is not new and does not have further revision or customized for cross-domain tasks. Obviously, directly fusing two data would suffer from the domain gap. For example, does the improvement come from seeing few labeled data from the target or the model is really addressing the domain gap? 2)How would separating foreground and background be effective for domain shift is missing.
Missing context
The setting of few-shot cross-domain object detection is not strictly defined well. For example, CDOD assumes the labels of target domains are unavailable. Here FSCDOD can access the labels from the target domain.
Many recent DAOD related works in CVPR are missing. For example:
[a] He, Mengzhe, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. "Cross Domain Object Detection by Target-Perceived Dual Branch Distillation." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 9570-9580. 2022.
[b]Zhao, Liang, and Limin Wang. "Task-specific Inconsistency Alignment for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14217-14226. 2022
[c] Li, Yu-Jhe, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter Vajda. "Cross-Domain Adaptive Teacher for Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 7581-7590. 2022
[d] Wu, Jiaxi, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, Bingqi Ma, Weihao Gan, Wei Wu, Yali Wang, and Di Huang. "Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 5301-5310. 2022.
[e] Li, Wuyang, Xinyu Liu, and Yixuan Yuan. "SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022.
[f] Xu, Yunqiu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. "H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., 2022.
Minor comments
Figure 2 is not clear for IPM. Does the class-wise average only apply to the support set? If so, what is the process for the query set (red features). This is missing in the figure.
In figure 2, since global average pooling is also performed aligned with class, is it the same as class-wise average? Why use two different names?

Review Point: 2)How would separating foreground and background be effective for domain shift is missing. Missing context The setting of few-shot cross-domain object detection is not strictly defined well. For example, CDOD assumes the labels of target domains are unavailable. Here FSCDOD can access the labels from the target domain. Many recent DAOD related works in CVPR are missing. For example: [a] He, Mengzhe, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. "Cross Domain Object Detection by Target-Perceived Dual Branch Distillation." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 9570-9580. 2022. [b]Zhao, Liang, and Limin Wang. "Task-specific Inconsistency Alignment for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14217-14226.
Review Point: 2022 [c] Li, Yu-Jhe, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, Kan Chen, Bichen Wu, Zijian He, Kris Kitani, and Peter Vajda. "Cross-Domain Adaptive Teacher for Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 7581-7590.
Review Point: 2022 [d] Wu, Jiaxi, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, Bingqi Ma, Weihao Gan, Wei Wu, Yali Wang, and Di Huang. "Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., pp. 5301-5310. 2022. [e] Li, Wuyang, Xinyu Liu, and Yixuan Yuan. "SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. [f] Xu, Yunqiu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. "H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)., 2022. Minor comments Figure 2 is not clear for IPM. Does the class-wise average only apply to the support set? If so, what is the process for the query set (red features). This is missing in the figure. In figure 2, since global average pooling is also performed aligned with class, is it the same as class-wise average? Why use two different names?
==================================================

Focused review:

Weaknesses / questions - First of all, I would like to know why is the assumption about asymptotic setting with infinite width needed. Where in the proof is it used? In terms of the parametrization proposed here it means that we work with n->infty. Eventually, in the experiments, the networks have rather small width compared to the depth. Following-up on that, could it be somehow empirically validated? Is this width assumption important? - What is the exact setup in the experiment presented in FIg. 2 (right). The text of the paper fails to exactly describe the experimental setup. What is the depth of the network considered here? - I find the experiments with resNets contestable. First of all the range of depths considered here is very surprising. Indeed, a resnet with 10000 blocks per stage is rather uncommon. Second, this experiment is ran for "one epoch of training" (l. 215) which in my opinion fails to show anything else than "PyTorch default init fails completely with such deep WN resnets". - The formulation on l. 243 is rather surprising: "Unlike previous works which use the test set for hyperparameter tuning". I do agree that proper hyper parameter tuning is essential in ML, but the wording is a bit hard in my opinion. Moreover, the "smaller training set" (l. 245) could be avoided by training the network with optimal parameters on the complete train+val. - In Table 1, I would expect a baseline WN model to be presented for other architectures than Resnet-110. Overall this is a good paper, coping with an interesting problem and is executed properly. I would like the authors to react to my negative comments / questions, and await the discussion period to make my final decision. # Rebuttal After reading the other reviews and the author's response, I decide to stick to my accept rating of 7. The author's response answered most of my concerns and doubts.

Review Point: - What is the exact setup in the experiment presented in FIg. 2 (right). The text of the paper fails to exactly describe the experimental setup. What is the depth of the network considered here?
Review Point: - I find the experiments with resNets contestable. First of all the range of depths considered here is very surprising. Indeed, a resnet with 10000 blocks per stage is rather uncommon. Second, this experiment is ran for "one epoch of training" (l. 215) which in my opinion fails to show anything else than "PyTorch default init fails completely with such deep WN resnets".
Review Point: 243 is rather surprising: "Unlike previous works which use the test set for hyperparameter tuning". I do agree that proper hyper parameter tuning is essential in ML, but the wording is a bit hard in my opinion. Moreover, the "smaller training set" (l. 245) could be avoided by training the network with optimal parameters on the complete train+val.
Review Point: - In Table 1, I would expect a baseline WN model to be presented for other architectures than Resnet-110. Overall this is a good paper, coping with an interesting problem and is executed properly. I would like the authors to react to my negative comments / questions, and await the discussion period to make my final decision. # Rebuttal After reading the other reviews and the author's response, I decide to stick to my accept rating of 7. The author's response answered most of my concerns and doubts.
==================================================

Focused review:

1) How valid / necessary is the power-law fit? Looking at the values in Figure 1, the range of negative log likelihoods in the leftmost panel is 0.8 -1.8. The middle panel show the linear scale and beyond the n = 1 point, the curves look pretty straight to me. That is reflected in the right panel where you show the index of the powerlaw is just a bit above -1.0, so essentially 1/n and a bit, but a bit slower decay. Many functions would locally look like this. What are the main reasons for using the powerlaw fit in particular? 2) Derivation in equation 2. What would be the conditions on the derivation in equation 2 to work? You characterize the distribution by its first two moments, which seems fine, but how sensitive is your conclusions that the resulting NLL depends on 1/n (=> the powerlaw) on the your assumption about the distribution of p*? In particular since you work with the log of those, will there be any other additional complications caused by that? 3) Different architectures, different power-laws? I wonder how to use this in practice given that different architectures seem to produce different powerlaws. Would I have to do an exploration of the (n,s) plane first, make a fit, and only then be able to predict the optimal memory split? This could defeat the practical purpose if that exploration took a lot of resources.

Review Point: 1) How valid / necessary is the power-law fit? Looking at the values in Figure 1, the range of negative log likelihoods in the leftmost panel is 0.8 -1.8. The middle panel show the linear scale and beyond the n = 1 point, the curves look pretty straight to me. That is reflected in the right panel where you show the index of the powerlaw is just a bit above -1.0, so essentially 1/n and a bit, but a bit slower decay. Many functions would locally look like this. What are the main reasons for using the powerlaw fit in particular?
Review Point: 2) Derivation in equation 2. What would be the conditions on the derivation in equation 2 to work? You characterize the distribution by its first two moments, which seems fine, but how sensitive is your conclusions that the resulting NLL depends on 1/n (=> the powerlaw) on the your assumption about the distribution of p*? In particular since you work with the log of those, will there be any other additional complications caused by that?
Review Point: 3) Different architectures, different power-laws? I wonder how to use this in practice given that different architectures seem to produce different powerlaws. Would I have to do an exploration of the (n,s) plane first, make a fit, and only then be able to predict the optimal memory split? This could defeat the practical purpose if that exploration took a lot of resources.
==================================================

Focused review:

- Unlike Tandem Model [4,5] and cVAE based methods the proposed method uses gradient updates and therefore is slow. The authors acknowledge this in the manuscript and demonstrate study the method as a function of inference budget. - The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.

Review Point: - Unlike Tandem Model [4,5] and cVAE based methods the proposed method uses gradient updates and therefore is slow. The authors acknowledge this in the manuscript and demonstrate study the method as a function of inference budget.
Review Point: - The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.
==================================================

Focused review:

- Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:   - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction   - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. 
In line "544", I still quite understand why using entity representations can reduce language bias. 

Review Point: - Several works have shown that entity embeddings provide effective features in cross-lingual tasks, and the contribution of this paper is incremental. For example:
Review Point: - GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction - Cross-lingual Structure Transfer for Relation and Event Extraction - no comparison with methods that incorporate entity information through an auxiliary loss function. In line "544", I still quite understand why using entity representations can reduce language bias.
==================================================

Focused review:

Weaknesses:
One of the major issues is that it is not clear what’s the unique challenges for continual graph learning and how the proposed benchmark contribute to advancing the direction.
Another major concern is that for link prediction/classification tasks, the existing related works are neither discussed nor compared. For instance, traditional degree or triad-based features have shown good performance for domain adaptation/transfer learning. In addition, many network embedding works or GNN-based models have explored this direction, but none of them are mentioned.
Similar problem exists for dynamic graphs, and related works (spatial-temporal modeling) in this direction are not discussed or compared.
Based on my understanding, the proposed domain incremental setting can be treated as a special case of class incremental setting.
The paper provides two more metrics to evaluate continual graph learning, however, the authors fail to explain why those two metrics are necessary and what extra info they can bring.
As a benchmark, in the code, the readme file does not provide any useful information (only dependencies are useful) for utilizing the benchmark. I roughly check the code but no comment is provided…
Lines of code are not an effective criterion to compare different benchmarks. The implementation/running efficiency actually matters.
As a benchmark, the datasets used in the paper are all relatively small. 4 graphs used in the experiment only have thousands of nodes and the largest dataset only have around 160K nodes. In addition, the number of tasks is also relatively small and the detailed partition is not discussed or provided.

Review Point: 4 graphs used in the experiment only have thousands of nodes and the largest dataset only have around 160K nodes. In addition, the number of tasks is also relatively small and the detailed partition is not discussed or provided.
==================================================

Focused review:

weakness 1. How to set the \alpha at the bottom of page five? More parameter analyses are expected. 2. As introduced in the introduction and abstract, many related works about this topic have been proposed, e.g., “Rubi: Reducing unimodal biases in visual question answering”, “Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies”. What are the improvements to the proposed methods? Why not compare these methods? The reviewer believes that the experiments are very lacking in comparison with the SOTA methods.

Review Point: 1. How to set the \alpha at the bottom of page five? More parameter analyses are expected.
Review Point: 2. As introduced in the introduction and abstract, many related works about this topic have been proposed, e.g., “Rubi: Reducing unimodal biases in visual question answering”, “Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies”. What are the improvements to the proposed methods? Why not compare these methods? The reviewer believes that the experiments are very lacking in comparison with the SOTA methods.
==================================================

Focused review:

Weaknesses:
Presentation: Unfortunately, the writing quality of this paper does not match ICLR standards. My main question lies in Eq (2). How X N
is precisely learned? What is a function that generates X N
, what are its inputs, and what is the objective function that it is optimizing? The paper just says that X N
is learned in a DETR-style, which is very wage. Here is a list of minor points in this regard. It would be better to include a figure that explains the method. Figure 1. and referring to it in the introduction are not informative. 2) I did not find the writing accurate in general. For instance, Section 3 starts with a paragraph of two lines. The second paragraph starts with a preposition and is not well organized. In the third paragraph, the authors mention that "These few tokens put great pressure on the cross-attention later". What does "pressure" mean precisely? 3) Eq(1) and the corresponding paragraph are obvious and unnecessary.
Contribution : I could not understand the contribution of this paper precisely. Is it about learning X N
? If so, how is it done precisely?
Performance: The paper underperforms some recent approaches. More specifically, BLIP[1] tackles the same problem and archives higher performance.
[1] Li etal, BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

Review Point: 2) I did not find the writing accurate in general. For instance, Section 3 starts with a paragraph of two lines. The second paragraph starts with a preposition and is not well organized. In the third paragraph, the authors mention that "These few tokens put great pressure on the cross-attention later". What does "pressure" mean precisely?
Review Point: 3) Eq(1) and the corresponding paragraph are obvious and unnecessary. Contribution : I could not understand the contribution of this paper precisely. Is it about learning X N ? If so, how is it done precisely? Performance: The paper underperforms some recent approaches. More specifically, BLIP[1] tackles the same problem and archives higher performance. [1] Li etal, BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
==================================================

Focused review:

Broadly, the paper makes a number of claims while motivating the work that do not seem to be backed up. At the same time, its experimental improvement is relatively minor, and it is not clear whether even these improvements come from its graph formulation. - While the main contribution of the paper is arguably in its graph-based "UFE-layer", it appears from the ablation study in Table 3 that most of its benefits over previous 'all pixel' method [7] comes from its NR-net architecture. This largely undercuts not only the specific graph-based design of the UFE-layer, but also the more general claim that 'all-pixel' methods do not make full use of 'inter-image' information. - In defining the UFE-layer as graph-based aggregation, the paper presents the motivation that an 'observation map' type approach as used in older single-pixel methods (like [4]) is sub-optimal, because it requires a resolution trade-off. This is not experimentally demonstrated—and should be, because it is central to the whole 'graph-based' premise. An important ablation would be to use an observation map-based layers (like [4]) as an alternative to the UFE-layers, which generates feature maps that are then pooled and fed to NR-net. If such an approach works just as well or better, it would significantly diminish the contribution and narrative of this paper. - It is worth noting that the single pixel method [4] actually does better than the proposed method with larger numbers of inputs (Table 1: for 64, 96). This would suggest that with more inputs, when the 'intra-image' information is stronger, the proposed method isn't able to exploit it as well as [4]. - Beyond the way that the UFE-layer generates its per-image feature vectors (by considering all other images), another difference to standard all-pixel methods is the nature of pooling. Other all-pixel methods just use max-pooling, while the proposed method seems to use both average and max-pooling (equation 5). It is not clear to what extent this contributes to the advantage of the proposed method over previous all-pixel approaches, and should be verified by running an ablation with only max-pooling in equation 5.

Review Point: - While the main contribution of the paper is arguably in its graph-based "UFE-layer", it appears from the ablation study in Table 3 that most of its benefits over previous 'all pixel' method [7] comes from its NR-net architecture. This largely undercuts not only the specific graph-based design of the UFE-layer, but also the more general claim that 'all-pixel' methods do not make full use of 'inter-image' information.
Review Point: - In defining the UFE-layer as graph-based aggregation, the paper presents the motivation that an 'observation map' type approach as used in older single-pixel methods (like [4]) is sub-optimal, because it requires a resolution trade-off. This is not experimentally demonstrated—and should be, because it is central to the whole 'graph-based' premise. An important ablation would be to use an observation map-based layers (like [4]) as an alternative to the UFE-layers, which generates feature maps that are then pooled and fed to NR-net. If such an approach works just as well or better, it would significantly diminish the contribution and narrative of this paper.
Review Point: - It is worth noting that the single pixel method [4] actually does better than the proposed method with larger numbers of inputs (Table 1: for 64, 96). This would suggest that with more inputs, when the 'intra-image' information is stronger, the proposed method isn't able to exploit it as well as [4].
Review Point: - Beyond the way that the UFE-layer generates its per-image feature vectors (by considering all other images), another difference to standard all-pixel methods is the nature of pooling. Other all-pixel methods just use max-pooling, while the proposed method seems to use both average and max-pooling (equation 5). It is not clear to what extent this contributes to the advantage of the proposed method over previous all-pixel approaches, and should be verified by running an ablation with only max-pooling in equation 5.
==================================================

Focused review:

Weaknesses: 1. Some intuitions can be further explained, e.g., in section 2.2 the situation that breaks the factorized distribution can have a factorized support. It will be more convincing to give an example which does not have a factorized support will fail to disentangle, more intuitively show the relationship between factorized support and disentanglement. 2. Since this paper claims to aim at the realistic scenario of disentangled representation learning, it is better to conduct experiments on real world datasets instead of the synthetic datasets(at least for the out-of-distribution setting.). 3. The compared disentangled baselines seem to be out-of-date, it is better to incorporate the more recent disentangling methods.

Review Point: 1. Some intuitions can be further explained, e.g., in section 2.2 the situation that breaks the factorized distribution can have a factorized support. It will be more convincing to give an example which does not have a factorized support will fail to disentangle, more intuitively show the relationship between factorized support and disentanglement.
Review Point: 2. Since this paper claims to aim at the realistic scenario of disentangled representation learning, it is better to conduct experiments on real world datasets instead of the synthetic datasets(at least for the out-of-distribution setting.).
Review Point: 3. The compared disentangled baselines seem to be out-of-date, it is better to incorporate the more recent disentangling methods.
==================================================

Focused review:

- My major concern is that this paper misses important comparisons with the uniform channel scaling baseline. As shown in [1] that the final architecture plays a huge role in the effectiveness of channel/filter pruning, I think pruning methods should compare to a naive way of model scaling (width-multiplier). More specifically, width-multiplier uniformly increases or decreases the channel counts across all layers with the same proportion (e.g., 50% of the original channel counts). Such a method is very simple (near zero computational cost) and can match any desired FLOPs requirement. If after all the pruning efforts, one still fail to compare with uniform channel scaling, what is the point? To perform a fair comparison, I'd suggest training such a baseline using the epoch number of training+fine-tuning, which is 248 for ResNet on ImageNet, 512 for MobileNetv2 on ImageNet, and 400 for CIFAR according to this paper's setup. - I think [7] is also very relevant to this work that is not cited. [7] proposed a regularizer that improves upon L1 to avoid shrinkage for all parameters, which is of the same motivation for this work. I think it is necessary to compare with this work. - Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility. - The empirical results only focus on pruning less than ~50% FLOPs away. It seems to me that going lower might not favor the proposed method due to the huge regularization imposed in the training loss. This seems to be a problem for L1 regularizer as well. However, there are many methods for filter pruning and it is not clear why one wants to go for regularization-based methods. It would be great if the paper provides a wider spectrum of FLOPs (this at least can be done by comparing the proposed method to uniform pruning and L1 pruning). - Since I can find better pruning results (available prior to the submission deadline) [2-6], I do not think it is appropriate to claim state-of-the-art. I agree the results of this paper look good, but without proper comparisons with these work (taking into account the differences in training settings), it seems to me the presented results are "comparable" to the current state-of-the-art. Also, it is informative to discuss these competitive methods [2-6] in the paper. [1] Liu, Zhuang, et al. "Rethinking the value of network pruning." ICLR 2019. [2] Guo, Shaopeng, et al. "DMCP: Differentiable Markov Channel Pruning for Neural Networks." arXiv preprint arXiv:2005.03354 (2020). [3] Chin, Ting-Wu, et al. "Towards Efficient Model Compression via Learned Global Ranking." arXiv preprint arXiv:1904.12368 (2019). [4] You, Zhonghui, et al. "Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks." Advances in Neural Information Processing Systems. 2019. [5] Dong, Xuanyi, and Yi Yang. "Network pruning via transformable architecture search." Advances in Neural Information Processing Systems. 2019. [6] Yu, Jiahui, and Thomas Huang. "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers." arXiv preprint arXiv:1903.11728 (2019). [7] Yang, Huanrui, Wei Wen, and Hai Li. "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures." ICLR 2020.

Review Point: - My major concern is that this paper misses important comparisons with the uniform channel scaling baseline. As shown in [1] that the final architecture plays a huge role in the effectiveness of channel/filter pruning, I think pruning methods should compare to a naive way of model scaling (width-multiplier). More specifically, width-multiplier uniformly increases or decreases the channel counts across all layers with the same proportion (e.g., 50% of the original channel counts). Such a method is very simple (near zero computational cost) and can match any desired FLOPs requirement. If after all the pruning efforts, one still fail to compare with uniform channel scaling, what is the point? To perform a fair comparison, I'd suggest training such a baseline using the epoch number of training+fine-tuning, which is 248 for ResNet on ImageNet, 512 for MobileNetv2 on ImageNet, and 400 for CIFAR according to this paper's setup.
Review Point: - I think [7] is also very relevant to this work that is not cited. [7] proposed a regularizer that improves upon L1 to avoid shrinkage for all parameters, which is of the same motivation for this work. I think it is necessary to compare with this work.
Review Point: - Another concern is regarding hyperparameter tuning. While this paper makes a good argument regarding the property of polarization regularizer compared to the L1 regularizer, it is has two hyperparameters and make pruning laborious. Most notably, since both hyperparameters affect FLOPs and accuracy, there should be multiple solutions that hit the same FLOPs, which makes the tuning process hard as one cannot navigate on the simple objective (FLOPs) to decide. According to current presentation, it is not clear how the grid search is done and I'd appreciate detailed discussions regarding this issue for reproducibility.
Review Point: - The empirical results only focus on pruning less than ~50% FLOPs away. It seems to me that going lower might not favor the proposed method due to the huge regularization imposed in the training loss. This seems to be a problem for L1 regularizer as well. However, there are many methods for filter pruning and it is not clear why one wants to go for regularization-based methods. It would be great if the paper provides a wider spectrum of FLOPs (this at least can be done by comparing the proposed method to uniform pruning and L1 pruning).
Review Point: - Since I can find better pruning results (available prior to the submission deadline) [2-6], I do not think it is appropriate to claim state-of-the-art. I agree the results of this paper look good, but without proper comparisons with these work (taking into account the differences in training settings), it seems to me the presented results are "comparable" to the current state-of-the-art. Also, it is informative to discuss these competitive methods [2-6] in the paper. [1] Liu, Zhuang, et al. "Rethinking the value of network pruning." ICLR 2019. [2] Guo, Shaopeng, et al. "DMCP: Differentiable Markov Channel Pruning for Neural Networks." arXiv preprint arXiv:2005.03354 (2020). [3] Chin, Ting-Wu, et al. "Towards Efficient Model Compression via Learned Global Ranking." arXiv preprint arXiv:1904.12368 (2019). [4] You, Zhonghui, et al. "Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks." Advances in Neural Information Processing Systems. 2019. [5] Dong, Xuanyi, and Yi Yang. "Network pruning via transformable architecture search." Advances in Neural Information Processing Systems. 2019. [6] Yu, Jiahui, and Thomas Huang. "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers." arXiv preprint arXiv:1903.11728 (2019). [7] Yang, Huanrui, Wei Wen, and Hai Li. "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures." ICLR 2020.
==================================================

Focused review:

Weaknesses:
I think the paper is too premature in terms of both, writing and method.
Sec. 3.4 claims to deliver a theoretical analysis, but it stays pretty vague. Eq 3 seems central, but the derivation to get there from Eq. 2 is just informally described instead of adding at least 1-2 formal lines, especially since there are some many different x
version etc. where one can get lost in the expression.
Overall, intuitively I cannot follow the method's idea.
I feel such a representation is not robust to distributional changes etc. I think it often happens that in one instance (say, for one initialization), a state-action pair is good (taken by the expert), while in another instance it is bad (taken by non-expert). I.e., whether a state is good or bad can heavily depend on initial state, random fluctuations, etc. E.g., from one starting position in a maze, the expert may never want to visit a certain state, while from another one, the state would lie on the shortest path. It's hard for me to make this intuitive counterargument specific, and maybe GAIL suffers from similar problems, but it does not seem to be a robust approach.
It seems it does something similar as GAIL, but in a less principled way. But then why do we need to add it in the first place?
Also I'm confused by relating the tuning of the representation, to tuning the policy. I think these are different things and I have a hard time seeing that Sec. 3.4 justifies this work from IRL/AL principles.
Some (minor) points re. writing:
The overall method, i.e., GAIL + the new representation mechanism, is described too little; I feel it is just Fig. 2.
"leanring" p3
"markov" p3
x is only informally introduced as state-action-pair. I strongly recommend to introduce all important notation in a formal, central way.
what does "anchoring on policy" mean (p4)?

Review Point: 2. "leanring" p3 "markov" p3 x is only informally introduced as state-action-pair. I strongly recommend to introduce all important notation in a formal, central way. what does "anchoring on policy" mean (p4)?
==================================================

Focused review:

weaknesses:
1, Some math notations used in this paper miss definitions. For example, the definition of E p 1 , . . . , p K [ V a r ( Q q ( p ) ) ]
is missing.
2, The quantization Q q ( p )
used in this paper based on section 3.3 QUANTIZATION WITH FEDERATED QSGD is " Q q ( p )
then returns the sign of p and jpj rounded to one of the endpoints". This quantization is not an unbiased estimator of p
, which is conflict with the claim on page 14.
3, The paper introduces the expected variance of an accumulation of quantized parameter E [ V a r ( ∑ Q ( p ) ) ]
as a measure of the performance of quantized FL algorithm and tries to minimize it in Thereom 1. The authors should explain more why it is a good measure. Why not give the theoretical analysis in the following logic line: convergence guarantee -> get rounds T, given e p s i l o n
-> get computational complexity and communication cost?
4, In algorithm 1, RunClient( c k
) is missing between line 8 and line 9.

Review Point: 1, Some math notations used in this paper miss definitions. For example, the definition of E p 1 , .
Review Point: .. , p K [ V a r ( Q q ( p ) ) ] is missing. 2, The quantization Q q ( p ) used in this paper based on section 3.3 QUANTIZATION WITH FEDERATED QSGD is " Q q ( p ) then returns the sign of p and jpj rounded to one of the endpoints". This quantization is not an unbiased estimator of p , which is conflict with the claim on page 14. 3, The paper introduces the expected variance of an accumulation of quantized parameter E [ V a r ( ∑ Q ( p ) ) ] as a measure of the performance of quantized FL algorithm and tries to minimize it in Thereom 1. The authors should explain more why it is a good measure. Why not give the theoretical analysis in the following logic line: convergence guarantee -> get rounds T, given e p s i l o n -> get computational complexity and communication cost? 4, In algorithm 1, RunClient( c k ) is missing between line 8 and line 9.
==================================================

Focused review:

Weakness
Motivation is weak. This paper claims that the prior frameworks like DAgger have two issues: 1) applying rule-based policies is hard; 2) optimization of such policies is tricky (because of the convex realization). These issues depend on specific applications. It seems that they are not fundamental issues.
The BC's guarantee is weaker, and the improvements over BC are questionable. The direct reduction of BC to supervised learning yields a loose bound, as shown in [1]. The expert annotation complexity of BC should be \widetilde{O}(H^2/\epsilon) rather than \widetilde{O}(H^4/\epsilon^2).
[1] Rajaraman, Nived, et al. "Toward the fundamental limits of imitation learning." Advances in Neural Information Processing Systems 33 (2020): 2914-2924.
This paper studies classification-based online imitation learning (COIL). By the powerful techniques from the online optimization literature, new theoretical results are obtained in the context of imitation learning. Some theoretical results can corroborate some discussions in [3]. Though this paper has fruitful results, I am worried about the implications of these results.
What are new messages? (compared with the old paper like DAGGER and the new paper [2]) How can the theoretical results of this paper guide practice? This question is related to the remark in the conclusion part. What kind of experiments do you want to conduct to empirically evaluate algorithms?
This paper is well written. A minor issue is a claim in the introduction (Line 30 and 31). In general, interactive IL can not be better than its offline counterpart; see [1]. Only under suitable assumptions, interactive IL can have superior performance; see [2]. A side message is that for tasks similar to the lower bound in [1], online algorithms cannot help. This message is important for practitioners. Please consider clarifying this misleading claim.
I am willing to recommend accepting this paper if the above concerns can be well-addressed.
[3] Spencer, Jonathan, et al. "Feedback in imitation learning: The three regimes of covariate shift." arXiv preprint arXiv:2102.02872 (2021).

Review Point: 2) optimization of such policies is tricky (because of the convex realization). These issues depend on specific applications. It seems that they are not fundamental issues. The BC's guarantee is weaker, and the improvements over BC are questionable. The direct reduction of BC to supervised learning yields a loose bound, as shown in [1]. The expert annotation complexity of BC should be \widetilde{O}(H^2/\epsilon) rather than \widetilde{O}(H^4/\epsilon^2). [1] Rajaraman, Nived, et al. "Toward the fundamental limits of imitation learning." Advances in Neural Information Processing Systems 33 (2020): 2914-2924. This paper studies classification-based online imitation learning (COIL). By the powerful techniques from the online optimization literature, new theoretical results are obtained in the context of imitation learning. Some theoretical results can corroborate some discussions in [3]. Though this paper has fruitful results, I am worried about the implications of these results. What are new messages? (compared with the old paper like DAGGER and the new paper [2]) How can the theoretical results of this paper guide practice? This question is related to the remark in the conclusion part. What kind of experiments do you want to conduct to empirically evaluate algorithms? This paper is well written. A minor issue is a claim in the introduction (Line 30 and 31). In general, interactive IL can not be better than its offline counterpart; see [1]. Only under suitable assumptions, interactive IL can have superior performance; see [2]. A side message is that for tasks similar to the lower bound in [1], online algorithms cannot help. This message is important for practitioners. Please consider clarifying this misleading claim. I am willing to recommend accepting this paper if the above concerns can be well-addressed. [3] Spencer, Jonathan, et al. "Feedback in imitation learning: The three regimes of covariate shift." arXiv preprint arXiv:2102.02872 (2021).
==================================================

Focused review:

Weaknesses: -- The two-step clipping method was introduced to accommodate for 8-bit batch normalization. However, it doesn't seem to be necessary for VGG-style block. According to Eq. (2), all we need to compute is sign of the batch norm's output, which can be achieved by comparing the input of batch norm with the threshold tau. Of course, tau can be represented using 8 bits. Then, the input of batch norm doesn't need to be represented using 32 bits anymore; its 8-bit representation suffices. I am also assuming the scaling factor of binarization is merged with BN, is it a right assumption? For the ResNet-style block, the batch norm is mixed-precision not 8-bit according to Fig. 3. Then, what's the point of representing the input using 8 bits?
-- 8-bit batch norm has been proposed before in literature (e.g., https://arxiv.org/pdf/1805.11046.pdf), which undermines the novelty of this work.

Review Point: -- The two-step clipping method was introduced to accommodate for 8-bit batch normalization. However, it doesn't seem to be necessary for VGG-style block. According to Eq. (2), all we need to compute is sign of the batch norm's output, which can be achieved by comparing the input of batch norm with the threshold tau. Of course, tau can be represented using 8 bits. Then, the input of batch norm doesn't need to be represented using 32 bits anymore; its 8-bit representation suffices. I am also assuming the scaling factor of binarization is merged with BN, is it a right assumption? For the ResNet-style block, the batch norm is mixed-precision not 8-bit according to Fig.
Review Point: 3. Then, what's the point of representing the input using 8 bits? -- 8-bit batch norm has been proposed before in literature (e.g., https://arxiv.org/pdf/1805.11046.pdf), which undermines the novelty of this work.
==================================================

Focused review:

- The objective function (1): I have two concerns about the definition of this objective: 1. If the intuitive goal consists of finding a set of policies that contains an optimal policy for every test MDP in S_{test}, I would rather evaluate the quality of \overline{\Pi} with the performance in the worst MDP. In other words, I would have employed the \min over S_{test} rather than the summation. With the summation we might select a subset of policies that are very good for the majority of the MDPs in S_{test} but very bad of the remaining ones and this phenomenon would be hidden by the summation but highlighted by the \min. 2. If no conditions on the complexity of \overline{\Pi} are enforced the optimum of (1) would be exactly \Pi, or, at least, the largest subset allowed. - Latent-Conditioned Policies: How is this different from considering a hyperpolicy that is used to sample the parameters of the policy, Like in Parameter-based Exploration Policy Gradients (PGPE)? Sehnke, Frank, et al. "Policy gradients with parameter-based exploration for control." International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2008. - Choice of the latent variable distribution: at line 139 the authors say that p(Z) is chosen to be the uniform distribution, while at line 150 p(Z) is a categorical distribution. Which one is actually used in the algorithm? Is there a justification for choosing one distribution rather than another? Can the authors motivate? **Minor*** - line 64: remove comma after a_i - line 66: missing expectation around the summation - line 138: what is H(Z)? - line 322: don’t -> do not - Equation (2): at this point the policy becomes a parametric function of \theta. Moreover, the dependence on s when using the policy as an argument for R_{\mathcal{M}} should be removed - Figure 3: the labels on the axis are way too small - Font size of the captions should be the same as the text

Review Point: - The objective function (1): I have two concerns about the definition of this objective:
Review Point: 1. If the intuitive goal consists of finding a set of policies that contains an optimal policy for every test MDP in S_{test}, I would rather evaluate the quality of \overline{\Pi} with the performance in the worst MDP. In other words, I would have employed the \min over S_{test} rather than the summation. With the summation we might select a subset of policies that are very good for the majority of the MDPs in S_{test} but very bad of the remaining ones and this phenomenon would be hidden by the summation but highlighted by the \min.
Review Point: 2. If no conditions on the complexity of \overline{\Pi} are enforced the optimum of (1) would be exactly \Pi, or, at least, the largest subset allowed.
Review Point: - Latent-Conditioned Policies: How is this different from considering a hyperpolicy that is used to sample the parameters of the policy, Like in Parameter-based Exploration Policy Gradients (PGPE)? Sehnke, Frank, et al. "Policy gradients with parameter-based exploration for control." International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2008.
Review Point: - Choice of the latent variable distribution: at line 139 the authors say that p(Z) is chosen to be the uniform distribution, while at line 150 p(Z) is a categorical distribution. Which one is actually used in the algorithm? Is there a justification for choosing one distribution rather than another? Can the authors motivate? **Minor*** - line 64: remove comma after a_i - line 66: missing expectation around the summation - line 138: what is H(Z)?
Review Point: - line 322: don’t -> do not - Equation (2): at this point the policy becomes a parametric function of \theta. Moreover, the dependence on s when using the policy as an argument for R_{\mathcal{M}} should be removed - Figure 3: the labels on the axis are way too small - Font size of the captions should be the same as the text
==================================================

Focused review:

My major concern is on the empirical results. Maybe I missed it, but it seems not quite clear to me why the comparing methods are selected under different conditions. For example, why GS-ADMM results are not shown for l_1 and l_2-norm optimization, i.e., Table 2 and 3? It would be helpful to show the objective function curves of the comparing method in Fig. 1. How the parameters c, κ, and \epsilon determined in the experiments?

Review Point: 1. How the parameters c, κ, and \epsilon determined in the experiments?
==================================================

Focused review:

- The method requires additional information to what is typically provided in a semantic segmentation dataset in that it needs to know whether masks are complete or incomplete for training. - While the ablation study shows the importance of the unpaired data for finetuning but also highlights the fact that the invisible component of the objects are typically small (i.e., far fewer pixels than the visible part). It would be interesting to see a plot of accuracy versus percentage occlusion. - The multiple plausible completions is never really tested given that the experiments are only run on rigid objects (I'm considering pedestrians here as rigid given their size and viewpoint in the chosen datasets). It would have been interesting to see results on articulated objects (such as animals) or objects where there is a greater variation in possible object shape.

Review Point: - The method requires additional information to what is typically provided in a semantic segmentation dataset in that it needs to know whether masks are complete or incomplete for training.
Review Point: - While the ablation study shows the importance of the unpaired data for finetuning but also highlights the fact that the invisible component of the objects are typically small (i.e., far fewer pixels than the visible part). It would be interesting to see a plot of accuracy versus percentage occlusion.
Review Point: - The multiple plausible completions is never really tested given that the experiments are only run on rigid objects (I'm considering pedestrians here as rigid given their size and viewpoint in the chosen datasets). It would have been interesting to see results on articulated objects (such as animals) or objects where there is a greater variation in possible object shape.
==================================================

Focused review:

Weaknesses: The overall result is not very useful for ML practioners in this field, because it merely confirms what has been known or suspected, i.e. it depends on the task at hand, the labeled data set size, the type of the model, etc. So, the result in this paper is not very actionable. The reviewer noted that this comprehensive analysis deepens the understanding of this topic.
- General Discussion: The paper's presentation can be improved. Specifically: 1) The order of the figures/tables in the paper should match the order they are mentioned in the papers. Right now their order seems quite random.
2) Several typos (L250, 579, etc). Please use a spell checker.
3) Equation 1 is not very useful, and its exposition looks strange. It can be removed, and leave just the text explanations.
4) L164 mentions the "Appendix", but it is not available in the paper.
5) Missing citation for the public skip-gram data set in L425.
6) The claim in L591-593 is too strong. It must be explained more clearly, i.e. when it is useful and when it is not.
7) The observation in L642-645 is very interesting and important. It will be good to follow up on this and provide concrete evidence or example from some embedding. Some visualization may help too.
8) In L672 should provide examples of such "specialized word embeddings" and how they are different than the general purpose embedding.
9) Figuer 3 is too small to read. 

Review Point: - General Discussion: The paper's presentation can be improved. Specifically:
Review Point: 1) The order of the figures/tables in the paper should match the order they are mentioned in the papers. Right now their order seems quite random.
Review Point: 3) Equation 1 is not very useful, and its exposition looks strange. It can be removed, and leave just the text explanations.
Review Point: 4) L164 mentions the "Appendix", but it is not available in the paper.
Review Point: 5) Missing citation for the public skip-gram data set in L425.
Review Point: 6) The claim in L591-593 is too strong. It must be explained more clearly, i.e. when it is useful and when it is not.
Review Point: 7) The observation in L642-645 is very interesting and important. It will be good to follow up on this and provide concrete evidence or example from some embedding. Some visualization may help too.
Review Point: 8) In L672 should provide examples of such "specialized word embeddings" and how they are different than the general purpose embedding.
==================================================

Focused review:

1. Insufficient analysis of the results, for example the claim on line 274 should be further substantiated by looking at the data more closely, or by conducting further experiments. 2. Unclear as to the definition of NSI. Why is this obvious equivalent to the social dilemma conditions? It is not clear what the introduction of the term NSI is adding here. 3. There are some missing baselines, most obviously the RIAL algorithm. 4. There are some minor mistakes, for instance, the equilbrium in figure 4D seems to be incorrectly identified. 5. The literature on learning in network games could be more extensively cited. For example this paper seems relevant: https://perso.amse-aixmarseille.fr/bervoets/publis/WP_BervoetsBravoFaure.pdf.

Review Point: 1. Insufficient analysis of the results, for example the claim on line 274 should be further substantiated by looking at the data more closely, or by conducting further experiments.
Review Point: 2. Unclear as to the definition of NSI. Why is this obvious equivalent to the social dilemma conditions? It is not clear what the introduction of the term NSI is adding here.
Review Point: 3. There are some missing baselines, most obviously the RIAL algorithm.
Review Point: 4. There are some minor mistakes, for instance, the equilbrium in figure 4D seems to be incorrectly identified.
Review Point: 5. The literature on learning in network games could be more extensively cited. For example this paper seems relevant: https://perso.amse-aixmarseille.fr/bervoets/publis/WP_BervoetsBravoFaure.pdf.
==================================================

Focused review:

1. As the paper’s main proposal is to use wavelet transforms to obtain a natural hierarchy of normalizing flows, it is surprising to me that the authors have not shown an exploration on the effect of the different possible choices for the wavelets. The influence of different wavelet choices on the sample quality and quantitative performance seems like an important topic to investigate, as wavelet transforms for normalizing flows form such a central part of the novelty of this paper. 2. One of the main claims of the authors is that the proposed method can be trained up to 15 times faster than other flow methods like Glow. Although it is likely that the parallel training of the flow components can decrease the training time, the comparison between training times presented in the paper has some issues. The authors only report the total number of GPU hours required to train their method to convergence on their hardware, and compare this to *estimates* of the total training time (in GPU hours) of Glow by looking at log files and through discussions on github. This seems problematic for several reasons. If the number of epochs used for training in the methods is not the same, then even if the estimates are correct, the numbers are hard to compare. A more informative comparison would be training time per epoch, or the time required for a single forward pass (with and without a parameter update). Since Glow’s code is publicly available, and the authors have used part of Glow in their own code, running such a comparison *on the same hardware* should not pose a problem, especially for the lower resolution images. Furthermore, the authors state that for the LSUN bedroom dataset, wavelet flows are 7.5 times faster than Glow, but wavelet flows also perform worse than Glow in terms of bpd. If you would train Glow to the same performance level as Wavelet flows, what would the difference in training time then be? Finally, the authors claim that Wavelet flows have a 15x speedup in training time for CelebA at a 256x256 resolution, but for this resolution of CelebA no results are shown in terms of bits-per-dimension for wavelet flows, so it is not clear if there is a trade off between training time and quantitative performance on this dataset. 3. A significant portion of the paper is devoted to sampling from the unnormalized annealed density with MCMC, with the authors arguing that samples from flow models are better at lower temperatures. As stated in the experiment section, the authors pick a temperature T=0.97 for the annealing temperature, which is very close to T=1 for which no MCMC is required and one can sample exactly from the flow model. This makes me wonder if running the MCMC procedure, which can also misbehave in high-D, is worth it when one can sample exactly at T=1.

Review Point: 1. As the paper’s main proposal is to use wavelet transforms to obtain a natural hierarchy of normalizing flows, it is surprising to me that the authors have not shown an exploration on the effect of the different possible choices for the wavelets. The influence of different wavelet choices on the sample quality and quantitative performance seems like an important topic to investigate, as wavelet transforms for normalizing flows form such a central part of the novelty of this paper.
Review Point: 2. One of the main claims of the authors is that the proposed method can be trained up to 15 times faster than other flow methods like Glow. Although it is likely that the parallel training of the flow components can decrease the training time, the comparison between training times presented in the paper has some issues. The authors only report the total number of GPU hours required to train their method to convergence on their hardware, and compare this to *estimates* of the total training time (in GPU hours) of Glow by looking at log files and through discussions on github. This seems problematic for several reasons. If the number of epochs used for training in the methods is not the same, then even if the estimates are correct, the numbers are hard to compare. A more informative comparison would be training time per epoch, or the time required for a single forward pass (with and without a parameter update). Since Glow’s code is publicly available, and the authors have used part of Glow in their own code, running such a comparison *on the same hardware* should not pose a problem, especially for the lower resolution images. Furthermore, the authors state that for the LSUN bedroom dataset, wavelet flows are 7.5 times faster than Glow, but wavelet flows also perform worse than Glow in terms of bpd. If you would train Glow to the same performance level as Wavelet flows, what would the difference in training time then be? Finally, the authors claim that Wavelet flows have a 15x speedup in training time for CelebA at a 256x256 resolution, but for this resolution of CelebA no results are shown in terms of bits-per-dimension for wavelet flows, so it is not clear if there is a trade off between training time and quantitative performance on this dataset.
Review Point: 3. A significant portion of the paper is devoted to sampling from the unnormalized annealed density with MCMC, with the authors arguing that samples from flow models are better at lower temperatures. As stated in the experiment section, the authors pick a temperature T=0.97 for the annealing temperature, which is very close to T=1 for which no MCMC is required and one can sample exactly from the flow model. This makes me wonder if running the MCMC procedure, which can also misbehave in high-D, is worth it when one can sample exactly at T=1.
==================================================

Focused review:

In general, my biggest concern is that the efficacy of the proposed method is not very significant. - For example, it only improves over MoCo-v2 on ImageNet-100 with 0.8% and on ImageNet-1k with 0.1 (67.9% v.s. 68.0). - And the improvement on PASCAL VOC detection is also not very significant. - I also wonder whether the proposed approach is compatible with more advanced data augmentation that sets a higher baseline. For example, can the proposed module directly be dropped in to improve [25] and [28]? In line 233-234, the explanation here is a bit counter-intuitive. I understand this is possible if inter-class distance increases more than intra-class variance, but can you plot it to justify it better? Otherwise, increasing intra-class variance should lead to lower accuracy. As shown in Figure 3(b), why are s' and s not comparable. For example, the last column shows that increase s' actually harms? It would be great to have deeper understanding on how s and s' are really manipulating the latent space (or logits). Why in section 4, authors always argues for light weight computation, I wonder how big is the difference of running time (e.g. relatively x% slower)? Because ranking the logits and mix features from noncontinuous GPU ram (as the top-N negatives are not continuous in the queue) should take some time. The line 141-150 need to be revised. Indeed, the pre-text task accuracy drop between MoCo and MoCo v2, as shown in Figure 2(b), mostly results from non-linear projection head rather than data augmentation, though augmentation also contributes a bit. Correct me if I am wrong. How can p goes beyond 1 in Figure 2(a)?

Review Point: - For example, it only improves over MoCo-v2 on ImageNet-100 with 0.8% and on ImageNet-1k with 0.1 (67.9% v.s. 68.0).
Review Point: - And the improvement on PASCAL VOC detection is also not very significant.
Review Point: - I also wonder whether the proposed approach is compatible with more advanced data augmentation that sets a higher baseline. For example, can the proposed module directly be dropped in to improve [25] and [28]? In line 233-234, the explanation here is a bit counter-intuitive. I understand this is possible if inter-class distance increases more than intra-class variance, but can you plot it to justify it better? Otherwise, increasing intra-class variance should lead to lower accuracy. As shown in Figure 3(b), why are s' and s not comparable. For example, the last column shows that increase s' actually harms? It would be great to have deeper understanding on how s and s' are really manipulating the latent space (or logits). Why in section 4, authors always argues for light weight computation, I wonder how big is the difference of running time (e.g. relatively x% slower)? Because ranking the logits and mix features from noncontinuous GPU ram (as the top-N negatives are not continuous in the queue) should take some time. The line 141-150 need to be revised. Indeed, the pre-text task accuracy drop between MoCo and MoCo v2, as shown in Figure 2(b), mostly results from non-linear projection head rather than data augmentation, though augmentation also contributes a bit. Correct me if I am wrong. How can p goes beyond 1 in Figure 2(a)?
==================================================

Focused review:

1. Authors have introduced a human model without connecting to other models employed in the literature (discrete choice models, quantal response models etc.). There needs to be a clear connection and justification for why this model should be employed. For any realistic problem, there are millions of states, so it is not a reasonable model which can be specified (as they expect in Figure 1 right side) 2. A similar issue exists with robot model. How does this connect to Reinforcement learning model, bandit problems etc. 3. The way these models are specified, it would seem obvious that if utility is specified on some of the attributes only, then it is easy to create examples where user will lose a lot. Adversarial AI has showcased this under much stricter conditions, where examples are only slightly different, the loss can be significant. 4. Preference elicitation (work by Craig Boutilier et al.) has talked about iterative methods for obtaining utilities of agents through specific questions. That work would need to be cited. 5. There are some unjustified assumptions: “U is continuous and strictly increasing in each attribute”. While there are interesting bits, it could have been made specific to existing models, so that the underlying assumptions are justified.

Review Point: 1. Authors have introduced a human model without connecting to other models employed in the literature (discrete choice models, quantal response models etc.). There needs to be a clear connection and justification for why this model should be employed. For any realistic problem, there are millions of states, so it is not a reasonable model which can be specified (as they expect in Figure 1 right side) 2. A similar issue exists with robot model. How does this connect to Reinforcement learning model, bandit problems etc.
Review Point: 3. The way these models are specified, it would seem obvious that if utility is specified on some of the attributes only, then it is easy to create examples where user will lose a lot. Adversarial AI has showcased this under much stricter conditions, where examples are only slightly different, the loss can be significant.
Review Point: 4. Preference elicitation (work by Craig Boutilier et al.) has talked about iterative methods for obtaining utilities of agents through specific questions. That work would need to be cited.
Review Point: 5. There are some unjustified assumptions: “U is continuous and strictly increasing in each attribute”. While there are interesting bits, it could have been made specific to existing models, so that the underlying assumptions are justified.
==================================================

Focused review:

Weaknesses 1: The writing could be further improved, e.g., “via being matched to” should be “via matching to” in Abstract.
2: The “Def-adv” needs to be clarified.
3: The accuracies of the target model using different defenses against the FGSM attack are not shown in Figure 1. Hence, it is unclear the difference between the known attacks and the unknown attacks.
4: Even though authors compare their framework with an advanced defense APE-GAN, they can further compare the proposed framework with a method that is designed to defend against multiple attacks (maybe the research on defense against multiple attacks is relatively rare). The results would be more meaningful if the authors could present this comparison in their paper.
Overall the paper presents an interesting study that would be useful for defending the threat of increasing malicious perturbations.

Review Point: 1: The writing could be further improved, e.g., “via being matched to” should be “via matching to” in Abstract.
Review Point: 3: The accuracies of the target model using different defenses against the FGSM attack are not shown in Figure 1. Hence, it is unclear the difference between the known attacks and the unknown attacks.
Review Point: 4: Even though authors compare their framework with an advanced defense APE-GAN, they can further compare the proposed framework with a method that is designed to defend against multiple attacks (maybe the research on defense against multiple attacks is relatively rare). The results would be more meaningful if the authors could present this comparison in their paper. Overall the paper presents an interesting study that would be useful for defending the threat of increasing malicious perturbations.
==================================================

Focused review:

Weaknesses: 1. A interesting baseline to compare with (both in terms of running time and iteration complexity) would be some optimizer that optimizes eq (2) directly, which would provide more intuition about the effect of using this upper bound. Response to Authors' Feedback: I totally agree with making an approximation on Eq (2) and I didn't mean to optimize Eq (2) directly since it would be impractical. What I suggested is to take a small problem and compare direct optimizer on Eq (2) and the proposed optimizer, which shows the quality of approximation on Eq (2). 2. Although some baselines wouldn't work on larger datasets, it is still helpful to consider datasets with larger number of clusters, e.g. imagenet with reduced number of samples and feature dimensions. This makes the story more complete.

Review Point: 1. A interesting baseline to compare with (both in terms of running time and iteration complexity) would be some optimizer that optimizes eq (2) directly, which would provide more intuition about the effect of using this upper bound. Response to Authors' Feedback: I totally agree with making an approximation on Eq (2) and I didn't mean to optimize Eq (2) directly since it would be impractical. What I suggested is to take a small problem and compare direct optimizer on Eq (2) and the proposed optimizer, which shows the quality of approximation on Eq (2).
Review Point: 2. Although some baselines wouldn't work on larger datasets, it is still helpful to consider datasets with larger number of clusters, e.g. imagenet with reduced number of samples and feature dimensions. This makes the story more complete.
==================================================

Focused review:

Weakness:
One of the advantages of a deep network is that it can be applied to a wide arrange of images. In contrast, conventional methods often need to optimize energy functions on each specific image. In this paper, an image-specific NAS technique is proposed which aims to provide optimal network architectures for each specific image. It means the network architecture is highly specific and has nearly zero robustness. Although the search for architecture is training-free, we still need a certain amount of time to optimize the searched network. The time and computation complexities should also be considered. Also since the setting is close to conventional methods setting, conventional methods should also be included in the comparison, in terms of accuracy and processing time.
The assumption/ observation made by this paper is dependent on the complexity of images. However, there is no definition or quantitative measurement of this property, i.e., why Lena has lower complexity than Baboon? Also, how to define an image as a fine-grained image or a coarse-grained image? Without a proper definition, the assumptions/findings of the paper cannot be held.
The experiments are not convincing. 1). The evaluation is only performed on 3 small datasets with less than 100 images in total. Image denoising is a common low-level vision task and can be easily extended to thousands or millions of images. Whether the findings still hold in general images is yet to be proven. It can be seen that there are some ambiguities in the BSD68 dataset and it only contains 68 images. 2). The paper only evaluates the denoising performance of hand-crafted Gaussian white noise. Since this noise is almost distinct from nature images in frequency, the performance of real noise images is yet to be proven. 3). What are the PSNR and the SSIM of the noised images? 4). As the noise level increases, the performance of the proposed method degenerates faster than competitors. Is that mean the network may have trouble handling large noise? 5). What are the ground truth widths? How to guarantee they are the optimal widths?

Review Point: 1). The evaluation is only performed on 3 small datasets with less than 100 images in total. Image denoising is a common low-level vision task and can be easily extended to thousands or millions of images. Whether the findings still hold in general images is yet to be proven. It can be seen that there are some ambiguities in the BSD68 dataset and it only contains 68 images.
Review Point: 2). The paper only evaluates the denoising performance of hand-crafted Gaussian white noise. Since this noise is almost distinct from nature images in frequency, the performance of real noise images is yet to be proven.
Review Point: 3). What are the PSNR and the SSIM of the noised images?
Review Point: 4). As the noise level increases, the performance of the proposed method degenerates faster than competitors. Is that mean the network may have trouble handling large noise?
Review Point: 5). What are the ground truth widths? How to guarantee they are the optimal widths?
==================================================

Focused review:

- One particular weakness of the propose method is the use of the LSTM predictor. this sequential predictor can slow the training and inference of the model. Especially during the inference, a beam search run first to decode the set of links between nodes. The training time and inference time should be reported in the paper to highlight the trade off between performance and latency. For language modeling task, where \alpha is large (256) and L is large (50K), beam search might be very slow. The use of LSTM predictor might be problematic for the proposed approach when the input sequence is getting longer (i.e modeling document level). - While the authors compare SAC to previous work, I wonder what is the advantage of the LSTM link predictor compared to manual design links between nodes. For language modeling and machine translation tasks, local n-gram are strong features for the task (this is also why CNN models in fair-seq do well in NLP). A baseline that uses \alpha links per node could be the model where each node links to \alpha nearest nodes (or \alpha previous nodes). Such a baseline is important in my opinion to highlight advantage of SAC where the prediction of links might be very slow.

Review Point: - One particular weakness of the propose method is the use of the LSTM predictor. this sequential predictor can slow the training and inference of the model. Especially during the inference, a beam search run first to decode the set of links between nodes. The training time and inference time should be reported in the paper to highlight the trade off between performance and latency. For language modeling task, where \alpha is large (256) and L is large (50K), beam search might be very slow. The use of LSTM predictor might be problematic for the proposed approach when the input sequence is getting longer (i.e modeling document level).
Review Point: - While the authors compare SAC to previous work, I wonder what is the advantage of the LSTM link predictor compared to manual design links between nodes. For language modeling and machine translation tasks, local n-gram are strong features for the task (this is also why CNN models in fair-seq do well in NLP). A baseline that uses \alpha links per node could be the model where each node links to \alpha nearest nodes (or \alpha previous nodes). Such a baseline is important in my opinion to highlight advantage of SAC where the prediction of links might be very slow.
==================================================

Focused review:

Weaknesses: The only problem is that the paper sounds too similar with Ref [Kim et al., 2016] which will be officially published in the coming IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017. 
Kim at al., 2016, proposes joint CTC-attention using MTL for English ASR task, and this paper proposes joint CTC-attention using MTL+joint decoding for Japanese and Chinese ASR tasks. I guess the difference is on joint decoding and the application to Japanese/Chinese ASR tasks. However, the difference is not clearly explained by the authors. So it took sometimes to figure out the original contribution of this paper.
(a) Title: The title in Ref [Kim et al., 2016] is “Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning”, while the title of this paper is “Joint CTC-attention End-to-end Speech Recognition”. I think the title is too general. If this is the first paper about "Joint CTC-attention" than it is absolutely OK. Or if Ref [Kim et al., 2016] will remain only as pre-published arXiv, then it might be still acceptable. But since [Kim et al., 2016] will officially publish in IEEE conference, much earlier than this paper, then a more specified title that represents the main contribution of this paper in contrast with the existing publication would be necessary. (b) Introduction: The author claims that “We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by [Kim at al., 2016].“ Taking advantage of the constrained CTC alignment in a hybrid CTC-attention is the original idea from [Kim at al., 2016]. So the whole argument about attention-based end-to-end ASR versus CTC-based ASR, and the necessary of CTC-attention combination is not novel. 
Furthermore, the statement “we propose … as proposed by [Kim et al, 2016]” is somewhat weird. We can build upon someone proposal with additional extensions, but not just re-propose other people's proposal. Therefore, what would be important here is to state clearly the original contribution of this paper and the position of the proposed method with respect to existing literature (c) Experimental Results: Kim at al., 2016 applied the proposed method on English task, while this paper applied the proposed method on Japanese and Mandarin Chinese tasks. I think it would be interesting if the paper could explain in more details about the specific problems in Japanese and Mandarin Chinese tasks that may not appear in English task. For example, how the system could address multiple possible outputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input without using any linguistic resources. This could be one of the important contributions from this paper.
- General Discussion: I think it would be better to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference, rather than pre-published arXiv: Kim, S., Hori, T., Watanabe, S., "Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear. 

Review Point: - General Discussion: I think it would be better to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference, rather than pre-published arXiv: Kim, S., Hori, T., Watanabe, S., "Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear.
==================================================

Focused review:

My main concern about the paper is whether this proposed algorithm is actually implementable due to the specific expression of the (constant) learning rate. I have two concerns: 1. The learning rate depends on t_{mix} in Theorem 1 and on the universal constants c_1 in both Theorem 1 and Theorem 2. How can we compute/approximate t_{mix} in advance? If we cannot, is it sufficient to employ a lower-bound on t_{mix}? Concerning the constant c_1. Looking at the proofs c_1 is a function of constant c (Equation 55) that in turn derives from Bernstein's inequality (Equation 81) and subsequently \tilde{c} (Equation 84), but its value is never explicitly computed. I am aware that also in [33] the learning rate schedule (that is not constant) depends on \mu_{min} and t_{mix}, but I think the authors should elaborate more on this and explain how to deal with it in practice, if possible. Furthermore, in [19] this dependence on t_{mix} is not present as the learning rate is chosen as a power of 1/n(s,a), where n(s,a) is the number of visits to state-action pair (s,a). In [19] the theoretical guarantees are weaker. I wonder if using a learning rate independent from t_{mix} prevents from achieving these stronger convergence guarantees. 2. Provided that we can deal with the issues of the previous point, a deeper problem is that the learning rate depends on the accuracy threshold \epsilon and on the number of iterations T. This suggests that a hypothetical algorithm should decide in advance \epsilon and T (maybe a user input) and then compute the learning rate. If we let the algorithm proceed to learn beyond T what will happen? Will it converge asymptotically? Intuitively, looking at the expression of the learning rate, to have asymptotic convergence I should set T->\infty and \epsilon->0 leading to a learning rate of value zero. It seems to me that this issue is specific to this work and not present for instance in [33]. If I look at Theorem 7 of [33], the learning rate schedule does not depend on T and the bound can be instanced for any T. Thus, for T->infty we will have asymptotic convergence. If I am correct on this reasoning, I think that the algorithm has a non-negligible limitation that prevents it from being applied in practice. Can the authors clarify this point? [19] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of machine learning Research, 5(Dec):1–25, 2003. ***Minor*** - There are some broken references in Appendix B - There are some equations in Appendix that go beyond the margin - In Appendix C, the definition of t_{cover} (Equation 34a) is different from the definition of t_{cover} presented in the main paper (Equation 5). I suggest employing different symbols to avoid confusion.

Review Point: 1. The learning rate depends on t_{mix} in Theorem 1 and on the universal constants c_1 in both Theorem 1 and Theorem 2. How can we compute/approximate t_{mix} in advance? If we cannot, is it sufficient to employ a lower-bound on t_{mix}? Concerning the constant c_1. Looking at the proofs c_1 is a function of constant c (Equation 55) that in turn derives from Bernstein's inequality (Equation 81) and subsequently \tilde{c} (Equation 84), but its value is never explicitly computed. I am aware that also in [33] the learning rate schedule (that is not constant) depends on \mu_{min} and t_{mix}, but I think the authors should elaborate more on this and explain how to deal with it in practice, if possible. Furthermore, in [19] this dependence on t_{mix} is not present as the learning rate is chosen as a power of 1/n(s,a), where n(s,a) is the number of visits to state-action pair (s,a). In [19] the theoretical guarantees are weaker. I wonder if using a learning rate independent from t_{mix} prevents from achieving these stronger convergence guarantees.
Review Point: 2. Provided that we can deal with the issues of the previous point, a deeper problem is that the learning rate depends on the accuracy threshold \epsilon and on the number of iterations T. This suggests that a hypothetical algorithm should decide in advance \epsilon and T (maybe a user input) and then compute the learning rate. If we let the algorithm proceed to learn beyond T what will happen? Will it converge asymptotically? Intuitively, looking at the expression of the learning rate, to have asymptotic convergence I should set T->\infty and \epsilon->0 leading to a learning rate of value zero. It seems to me that this issue is specific to this work and not present for instance in [33]. If I look at Theorem 7 of [33], the learning rate schedule does not depend on T and the bound can be instanced for any T. Thus, for T->infty we will have asymptotic convergence. If I am correct on this reasoning, I think that the algorithm has a non-negligible limitation that prevents it from being applied in practice. Can the authors clarify this point? [19] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of machine learning Research, 5(Dec):1–25, 2003. ***Minor*** - There are some broken references in Appendix B - There are some equations in Appendix that go beyond the margin - In Appendix C, the definition of t_{cover} (Equation 34a) is different from the definition of t_{cover} presented in the main paper (Equation 5). I suggest employing different symbols to avoid confusion.
==================================================

Focused review:

Weaknesses: 1) The proposed framework has only been tested on one language. It is not clear whether the framework is portable to other languages. For example, the proposed framework relies on a dependency parser which may not be available in some languages or in poor performance in some other languages.
2) The number of sentence pairs edited by leader judges is not reported so the correctness and efficiency of the automatic expansion framework can not be evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs need further post-editing is worrying. 3) There are quite a number of grammatical mistakes. Here are some examples but not the complete and exhaustive list: line 210, 212, 213: "on a displayed image/picture" -> "in a displayed image/picture" line 428: "Similarly as in" -> "Similar to" A proofread pass on the paper is needed.
- General Discussion: 

Review Point: 1) The proposed framework has only been tested on one language. It is not clear whether the framework is portable to other languages. For example, the proposed framework relies on a dependency parser which may not be available in some languages or in poor performance in some other languages.
Review Point: 2) The number of sentence pairs edited by leader judges is not reported so the correctness and efficiency of the automatic expansion framework can not be evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs need further post-editing is worrying.
==================================================

Focused review:

I feel that the paper is not accessible enough for the NeurIPS community. Many notions are properly introduced in the middle of the paper, which makes the paper rather obscure even to audience familiar with sampling. Specifically, I refer to: - what is a negative update ? - heavy hitters are only defined in 2.3. It seems that for a specific epsilon, any element can be a heavy hitter, so referring to a l_2 heavy hitter refers to any element, - what is p ? Without context (e.g. in title, in "the case for p's) p refers to a probability but not here, \ell_p is already clearer. - in the definition of composable sketches, the expression "data structures that are such that the desired output can be produced from the sketch" is not a clear definition. --- After rebuttal --- After reading the rebuttal and the other reviews, I still believe that: while the paper might be well-considered in a data mining conference, it is not accessible enough to the general ML community. For this reason, I still believe it should be rejected. I don't believe the claim that distributed sketching appears significantly at all generalist ML conference, but in doubt I'll raise by score to weak reject.

Review Point: - heavy hitters are only defined in 2.3. It seems that for a specific epsilon, any element can be a heavy hitter, so referring to a l_2 heavy hitter refers to any element, - what is p ? Without context (e.g. in title, in "the case for p's) p refers to a probability but not here, \ell_p is already clearer.
Review Point: - in the definition of composable sketches, the expression "data structures that are such that the desired output can be produced from the sketch" is not a clear definition. --- After rebuttal --- After reading the rebuttal and the other reviews, I still believe that: while the paper might be well-considered in a data mining conference, it is not accessible enough to the general ML community. For this reason, I still believe it should be rejected. I don't believe the claim that distributed sketching appears significantly at all generalist ML conference, but in doubt I'll raise by score to weak reject.
==================================================

Focused review:

1. The overall novelty of the proposed model is limited to some extend. 1) The structural module is generally a traditional graph embedding approaches; 2) the meta-learning module utilizes the prototypical model to calculate the centers of the positive and negative nodes respectively, and the transformation generally leverages an self-attention mechanism to achieve the adapted node embeddings for different labels. I think this module is very similar to meta-GNN. Both of them conduct adaptation on the support set, then do evaluation on the query set, though they employ prototype and MAML respectively. 3) the optimization module applies a simple iteration strategy to optimize the two loss functions. In my view, the overall model stands on the shoulder on some traditional approaches, and seems a bit incremental. 2. For the motivation, why to use meta-learning? Could some other approaches, such as fine-tune (which is often utilized as the comparison with meta-learning), solve this novel label problem? The authors should give more explanations, and verify them by experiments. 3. Some concerns about the first contribution "To the best of our knowledge, this is the first work that only uses the graph structure and some known labels to study the problem of NCFNL". I think this contribution is over claimed. Actually, Meta-GNN [33] also utilizes the graph structure (GNN) and some known labels (the labeled nodes) to study the problem of NCFNL (to predict the node labels for novel classes). We cannot regard it as overlooking the graph structure just because it utilizes the GNN models but not graph embedding models. 4. I agree with the usage of transformation, which actually does adaptation to transform the task agnostic embeddings to some task-specific ones for each task. My concern is that, why to use self-attention as the transformation, and what is the insight? The authors should give more explanations. 5. The baselines are not quite sufficient. 1) For GNN models, the authors could apply the graph embeddings learned by some traditional approaches (e.g., deepwalk) as the node features, which would be better than identity matrix, since the graph embeddings could preserve the graph structure. 2) Some more baselines should be taken into consideration, such as the fine-tuning approaches. For example, we can first train a GCN model on the training data for label prediction, then in test we can fine tune the GNN parameters on the novel labels for label prediction. I think fine-tuning approaches are important baselines for meta-learning models, and this could verify the performance comparison between fine-tuning methods and meta-learning based approaches.

Review Point: 1. The overall novelty of the proposed model is limited to some extend.
Review Point: 1) The structural module is generally a traditional graph embedding approaches;
Review Point: 2) the meta-learning module utilizes the prototypical model to calculate the centers of the positive and negative nodes respectively, and the transformation generally leverages an self-attention mechanism to achieve the adapted node embeddings for different labels. I think this module is very similar to meta-GNN. Both of them conduct adaptation on the support set, then do evaluation on the query set, though they employ prototype and MAML respectively.
Review Point: 3) the optimization module applies a simple iteration strategy to optimize the two loss functions. In my view, the overall model stands on the shoulder on some traditional approaches, and seems a bit incremental.
Review Point: 2. For the motivation, why to use meta-learning? Could some other approaches, such as fine-tune (which is often utilized as the comparison with meta-learning), solve this novel label problem? The authors should give more explanations, and verify them by experiments.
Review Point: 3. Some concerns about the first contribution "To the best of our knowledge, this is the first work that only uses the graph structure and some known labels to study the problem of NCFNL". I think this contribution is over claimed. Actually, Meta-GNN [33] also utilizes the graph structure (GNN) and some known labels (the labeled nodes) to study the problem of NCFNL (to predict the node labels for novel classes). We cannot regard it as overlooking the graph structure just because it utilizes the GNN models but not graph embedding models.
Review Point: 4. I agree with the usage of transformation, which actually does adaptation to transform the task agnostic embeddings to some task-specific ones for each task. My concern is that, why to use self-attention as the transformation, and what is the insight? The authors should give more explanations.
Review Point: 1) For GNN models, the authors could apply the graph embeddings learned by some traditional approaches (e.g., deepwalk) as the node features, which would be better than identity matrix, since the graph embeddings could preserve the graph structure.
Review Point: 2) Some more baselines should be taken into consideration, such as the fine-tuning approaches. For example, we can first train a GCN model on the training data for label prediction, then in test we can fine tune the GNN parameters on the novel labels for label prediction. I think fine-tuning approaches are important baselines for meta-learning models, and this could verify the performance comparison between fine-tuning methods and meta-learning based approaches.
==================================================

Focused review:

Weaknesses:
The support samples of "difficult task" is selected by fixing the query samples which means the difficulty of each task is highly correlated with the fixed query data. The hypothesis is somewhat unreasonable, since the FSL aims to adapt to the whole new class not the fixed query samples. The reviewer supposes that the tasks in the HARD-META-DATASET++ may not be a reliable estimator in the test-time in FSL.
The proposed method can be seen as an extension of the paper[1], since the paper[1] have already proposed that FSL are extremely sensitive to the data used for adaptation and used a greedy algorithm to find those difficult tasks. Although the proposed method is more efficient, the novelty is somewhat limited.
3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000.
As for questions, I would like to ask:
1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?

Review Point: 3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000. As for questions, I would like to ask:
Review Point: 1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD a better criterion for evaluation?
Review Point: 2 How to use the "difficult task" in the training phase in FSL? Can the "difficult task" in base classes help the model improve the generality to novel classes?
==================================================

Focused review:

1. The writing felt a bit rushed, with a few typos and confusing notations (see charity below). 2. I'm not entirely sure if I understand the motivation of certain parts of the analysis. In particular, the CI is only established for the linear part of the target (which kind of makes sense since in this asymptotic limit, it is the only thing that RF model can learn); therefore, I do not know the value of including a nonlinear component in addition. See the following sections for additional comments and questions. 3. While I am not an expert in the area, my feeling is that a considerable part of the analysis is known if not rather standard. For instance the setup for CLT resembles Bellec and Zhang 2019, the risk of the RF model is provided in Mei and Montanari 2019, and the utilized linearization in high dimensions is relatively well-known (e.g. previous analysis of the kernel Gram matrix).

Review Point: 2. I'm not entirely sure if I understand the motivation of certain parts of the analysis. In particular, the CI is only established for the linear part of the target (which kind of makes sense since in this asymptotic limit, it is the only thing that RF model can learn); therefore, I do not know the value of including a nonlinear component in addition. See the following sections for additional comments and questions.
Review Point: 3. While I am not an expert in the area, my feeling is that a considerable part of the analysis is known if not rather standard. For instance the setup for CLT resembles Bellec and Zhang 2019, the risk of the RF model is provided in Mei and Montanari 2019, and the utilized linearization in high dimensions is relatively well-known (e.g. previous analysis of the kernel Gram matrix).
==================================================

Focused review:

Weaknesses: - the approach inherits the disadvantages of wake-sleep. Thus, there is no clear objective optimized here and the method might even diverge. Furthermore, there is an additional approximation step which was not present in the original wake-sleep algorithm, namely the prediction of the required expectations/gradients. While the authors mention that the original wake-sleep algorithm has these drawbacks, they don't discuss them at all in their approach. - the motivation to fix VAEs' problem to learn deep models is addressed rather briefly in the experiments, by learning a model over binarized MNIST and 3 hidden sigmoid layers. This is not too convincing. Quality: The paper is technically sound, in that way that the choices made are clear and very convincing. However, as the approach is based in wake-sleep, it is clear that the approach is heuristic, in particular as it includes a further approximation (expectations/gradients used for learning). The consequences of this fact and weaknesses of the model should be discussed in a more open way and ideally be addressed in the experimental evaluation (e.g. showing fail cases, or discussing why there are none).  Clarity: The paper is very well written and easy to follow. Originality: The approach presented is, to the best of my knowledge, novel, natural and elegant.  Significance: Although the proposed is heuristic, it has a high potential to trigger further research in this direction. Summary: while the paper has its weaknesses (heuristic, which should be better discussed), its originality and potential inspiration for further work in this direction are the main reasons why I recommend an accept.  *** EDIT *** I read the authors' reply and I'm rather happy with it. While their point is convincing that the approximation in DDC-HM can be made arbitrarily close, I'm still not sure if we can guarantee stable training. The authors claim that it can be shown that the algorithm approaches the vicinity of a stationary point of the likelihood, if the gradient approximation error is small. I think this might be a crucial point, but I don't fully understand if this implies convergences (rather than oscillatory behavior) and how small the error needs to get. In any case, the authors should include this result in their paper. And again, nice and refreshing work, I stick with my original rating. 

Review Point: - the approach inherits the disadvantages of wake-sleep. Thus, there is no clear objective optimized here and the method might even diverge. Furthermore, there is an additional approximation step which was not present in the original wake-sleep algorithm, namely the prediction of the required expectations/gradients. While the authors mention that the original wake-sleep algorithm has these drawbacks, they don't discuss them at all in their approach.
Review Point: - the motivation to fix VAEs' problem to learn deep models is addressed rather briefly in the experiments, by learning a model over binarized MNIST and 3 hidden sigmoid layers. This is not too convincing. Quality: The paper is technically sound, in that way that the choices made are clear and very convincing. However, as the approach is based in wake-sleep, it is clear that the approach is heuristic, in particular as it includes a further approximation (expectations/gradients used for learning). The consequences of this fact and weaknesses of the model should be discussed in a more open way and ideally be addressed in the experimental evaluation (e.g. showing fail cases, or discussing why there are none). Clarity: The paper is very well written and easy to follow. Originality: The approach presented is, to the best of my knowledge, novel, natural and elegant. Significance: Although the proposed is heuristic, it has a high potential to trigger further research in this direction. Summary: while the paper has its weaknesses (heuristic, which should be better discussed), its originality and potential inspiration for further work in this direction are the main reasons why I recommend an accept. *** EDIT *** I read the authors' reply and I'm rather happy with it. While their point is convincing that the approximation in DDC-HM can be made arbitrarily close, I'm still not sure if we can guarantee stable training. The authors claim that it can be shown that the algorithm approaches the vicinity of a stationary point of the likelihood, if the gradient approximation error is small. I think this might be a crucial point, but I don't fully understand if this implies convergences (rather than oscillatory behavior) and how small the error needs to get. In any case, the authors should include this result in their paper. And again, nice and refreshing work, I stick with my original rating.
==================================================

Focused review:

1. Some experimental results do not support the authors’ claim on the effectiveness of the proposed method, or require further explanation. 2. The proposed linear evaluation of RoCL is a very standard adversarial learning technique and the significance of the proposed transformation smoothed inference is limited.

Review Point: 1. Some experimental results do not support the authors’ claim on the effectiveness of the proposed method, or require further explanation.
Review Point: 2. The proposed linear evaluation of RoCL is a very standard adversarial learning technique and the significance of the proposed transformation smoothed inference is limited.
==================================================

Focused review:

1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. 
2) Lacking strong baselines both on entity-level factuality evaluation and summarization. 
1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? 
2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? 
3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? 
4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). 

Review Point: 1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity.
Review Point: 2) Lacking strong baselines both on entity-level factuality evaluation and summarization.
Review Point: 1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced?
Review Point: 2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated?
Review Point: 3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement?
Review Point: 4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7).
==================================================

Focused review:

While the paper proposes an interesting solution, I believe it falls short on a range of aspects which greatly affected my score. Theoretical. 1. It is not clear what measures of uncertainty are used for OOD detection. Previous work on Prior Networks and ensemble methods consistently make use of mutual information to obtain a separable set of estimates of total, aleatoric and epistemic uncertainty. However, this work does neither mentions this nor uses these *established* and *theoretically meaningful* measures. Rather perplexingly, this work seems to make use of max alpha_c^{I} scores for Prior Network and variance of probability for ensembles. While this may have an interpretation, it needs to be developed in far greater detail, rather than hidden away in the discussion of the experimental setup. Furthermore, It is interesting how the proposed method affects estimates of aleatoric/epistemic uncertainty separately. Finally, using the established decomposition based on mutual information also allows comparing ensembles and Prior Networks on equal terms. 2. The Uncertainty aware loss function - the author's fail to point out that this is exactly equivalent to the RKL loss defined by Malinin et al (2019) with a target beta = 1, and also equivalent to an ELBO loss. Thus, what the authors are in fact doing is maximising the ELBO with a uniform prior over the Dirichlet latent variable. This connection should really be highlighted to make a proper connect to prior work on VAEs and other latent variable models. 3. The author's claim that placing a Flow-based model over a latent representation does not suffer from the same problems as flow-based models over the image space needs to be further developed. 4. Is there a limit on scaling for the model? If a flow needs to be trained with very class, does this limit deployment to tasks with >100 classes? Empirical 1. The primary concern regarding the experiments is that the models achieve very poor classification accuracy on CIFAR10 using the VGG architecture. While by no means SOTA, VGG16 can nevertheless achieve over 92-93% accuracy on CIFAR10. However, the models presented here have, at best, an accuracy of ~84%. The creates two concerns. Firstly, the models may be either under-trained, and mis-specified in some way, which clearly will impact the quality of uncertainty estimates. More generally, is a method provides good uncertainty, but poor quality, it is unlikely to ever be used. However, considering that RKL-DPNs can achieve a far higher CIFAR-10 accuracy than presented in this paper, I believe it is more likely that the training procedure was mis-specified in someway. 2. The author's make a strange distinction between aleatoric OOD and epistemic OOD. Are different uncertainty measures being used? Also, why is AUC-PR used, rather tan AUC-ROC, as is standard for OOD detection? This makes it difficult to compare these number to prior work. 3. Thorough comparisons of PostNet and Ensembles on CIFAR-10 are missing. More generally, the method should be validated on a larger dataset (CIFAR-100, TinyImageNet, ImageNet). It is interesting how the current method scales with the number of classes. 4. A greater and more diverse range of OOD datasets needs to be considered. Distinguishing CIFAR-10 from SVHN is not particularly difficult. Until a proper comparison is made with results from prior work, it is difficult to accept the claim of SOTA OOD detection results.

Review Point: 1. It is not clear what measures of uncertainty are used for OOD detection. Previous work on Prior Networks and ensemble methods consistently make use of mutual information to obtain a separable set of estimates of total, aleatoric and epistemic uncertainty. However, this work does neither mentions this nor uses these *established* and *theoretically meaningful* measures. Rather perplexingly, this work seems to make use of max alpha_c^{I} scores for Prior Network and variance of probability for ensembles. While this may have an interpretation, it needs to be developed in far greater detail, rather than hidden away in the discussion of the experimental setup. Furthermore, It is interesting how the proposed method affects estimates of aleatoric/epistemic uncertainty separately. Finally, using the established decomposition based on mutual information also allows comparing ensembles and Prior Networks on equal terms.
Review Point: 2. The Uncertainty aware loss function - the author's fail to point out that this is exactly equivalent to the RKL loss defined by Malinin et al (2019) with a target beta = 1, and also equivalent to an ELBO loss. Thus, what the authors are in fact doing is maximising the ELBO with a uniform prior over the Dirichlet latent variable. This connection should really be highlighted to make a proper connect to prior work on VAEs and other latent variable models.
Review Point: 3. The author's claim that placing a Flow-based model over a latent representation does not suffer from the same problems as flow-based models over the image space needs to be further developed.
Review Point: 4. Is there a limit on scaling for the model? If a flow needs to be trained with very class, does this limit deployment to tasks with >100 classes? Empirical 1. The primary concern regarding the experiments is that the models achieve very poor classification accuracy on CIFAR10 using the VGG architecture. While by no means SOTA, VGG16 can nevertheless achieve over 92-93% accuracy on CIFAR10. However, the models presented here have, at best, an accuracy of ~84%. The creates two concerns. Firstly, the models may be either under-trained, and mis-specified in some way, which clearly will impact the quality of uncertainty estimates. More generally, is a method provides good uncertainty, but poor quality, it is unlikely to ever be used. However, considering that RKL-DPNs can achieve a far higher CIFAR-10 accuracy than presented in this paper, I believe it is more likely that the training procedure was mis-specified in someway.
Review Point: 2. The author's make a strange distinction between aleatoric OOD and epistemic OOD. Are different uncertainty measures being used? Also, why is AUC-PR used, rather tan AUC-ROC, as is standard for OOD detection? This makes it difficult to compare these number to prior work.
Review Point: 3. Thorough comparisons of PostNet and Ensembles on CIFAR-10 are missing. More generally, the method should be validated on a larger dataset (CIFAR-100, TinyImageNet, ImageNet). It is interesting how the current method scales with the number of classes.
Review Point: 4. A greater and more diverse range of OOD datasets needs to be considered. Distinguishing CIFAR-10 from SVHN is not particularly difficult. Until a proper comparison is made with results from prior work, it is difficult to accept the claim of SOTA OOD detection results.
==================================================

Focused review:

weaknesses. First of all, the experimental results are quite interesting, especially that the algorithm outperforms DQN on Atari. The results on the synthetic experiment are also interesting. I have three main concerns about the paper.  1. There is significant difficulty in reconstructing what is precisely going on. For example, in Figure 1, what exactly is a head? How many layers would it have? What is the "Frame"? I wish the paper would spend a lot more space explaining how exactly bootstrapped DQN operates (Appendix B cleared up a lot of my queries and I suggest this be moved into the main body). 2. The general approach involves partitioning (with some duplication) the samples between the heads with the idea that some heads will be optimistic and encouraging exploration. I think that's an interesting idea, but the setting where it is used is complicated. It would be useful if this was reduced to (say) a bandit setting without the neural network. The resulting algorithm will partition the data for each arm into K (possibly overlapping) sub-samples and use the empirical estimate from each partition at random in each step. This seems like it could be interesting, but I am worried that the partitioning will mean that a lot of data is essentially discarded when it comes to eliminating arms. Any thoughts on how much data efficiency is lost in simple settings? Can you prove regret guarantees in this setting? 3. The paper does an OK job at describing the experimental setup, but still it is complicated with a lot of engineering going on in the background. This presents two issues. First, it would take months to re-produce these experiments (besides the hardware requirements). Second, with such complicated algorithms it's hard to know what exactly is leading to the improvement. For this reason I find this kind of paper a little unscientific, but maybe this is how things have to be. I wonder, do the authors plan to release their code? Overall I think this is an interesting idea, but the authors have not convinced me that this is a principled approach. The experimental results do look promising, however, and I'm sure there would be interest in this paper at NIPS. I wish the paper was more concrete, and also that code/data/network initialisation can be released. For me it is borderline.  Minor comments: * L156-166: I can barely understand this paragraph, although I think I know what you want to say. First of all, there /are/ bandit algorithms that plan to explore. Notably the Gittins strategy, which treats the evolution of the posterior for each arm as a Markov chain. Besides this, the figure is hard to understand. "Dashed lines indicate that the agent can plan ahead..." is too vague to be understood concretely. * L176: What is $x$? * L37: Might want to mention that these algorithms follow the sampled policy for awhile. * L81: Please give more details. The state-space is finite? Continuous? What about the actions? In what space does theta lie? I can guess the answers to all these questions, but why not be precise? * Can you say something about the computation required to implement the experiments? How long did the experiments take and on what kind of hardware?  * Just before Appendix D.2. "For training we used an epsilon-greedy ..." What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy? 

Review Point: 1. There is significant difficulty in reconstructing what is precisely going on. For example, in Figure 1, what exactly is a head? How many layers would it have? What is the "Frame"? I wish the paper would spend a lot more space explaining how exactly bootstrapped DQN operates (Appendix B cleared up a lot of my queries and I suggest this be moved into the main body).
Review Point: 2. The general approach involves partitioning (with some duplication) the samples between the heads with the idea that some heads will be optimistic and encouraging exploration. I think that's an interesting idea, but the setting where it is used is complicated. It would be useful if this was reduced to (say) a bandit setting without the neural network. The resulting algorithm will partition the data for each arm into K (possibly overlapping) sub-samples and use the empirical estimate from each partition at random in each step. This seems like it could be interesting, but I am worried that the partitioning will mean that a lot of data is essentially discarded when it comes to eliminating arms. Any thoughts on how much data efficiency is lost in simple settings? Can you prove regret guarantees in this setting?
Review Point: 3. The paper does an OK job at describing the experimental setup, but still it is complicated with a lot of engineering going on in the background. This presents two issues. First, it would take months to re-produce these experiments (besides the hardware requirements). Second, with such complicated algorithms it's hard to know what exactly is leading to the improvement. For this reason I find this kind of paper a little unscientific, but maybe this is how things have to be. I wonder, do the authors plan to release their code? Overall I think this is an interesting idea, but the authors have not convinced me that this is a principled approach. The experimental results do look promising, however, and I'm sure there would be interest in this paper at NIPS. I wish the paper was more concrete, and also that code/data/network initialisation can be released. For me it is borderline. Minor comments:
Review Point: * L156-166: I can barely understand this paragraph, although I think I know what you want to say. First of all, there /are/ bandit algorithms that plan to explore. Notably the Gittins strategy, which treats the evolution of the posterior for each arm as a Markov chain. Besides this, the figure is hard to understand. "Dashed lines indicate that the agent can plan ahead..." is too vague to be understood concretely.
Review Point: * L37: Might want to mention that these algorithms follow the sampled policy for awhile.
Review Point: * L81: Please give more details. The state-space is finite? Continuous? What about the actions? In what space does theta lie? I can guess the answers to all these questions, but why not be precise?
Review Point: * Can you say something about the computation required to implement the experiments? How long did the experiments take and on what kind of hardware?
Review Point: * Just before Appendix D.2. "For training we used an epsilon-greedy ..." What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy?
==================================================

Focused review:

A human evaluation of the "gaps" filled by the model would have been a plus for the paper. 
A better evaluation of the output produced by the proposed method would be beneficial for the reader. For instance focusing on: - How often invalid constraints are finally hypothesized by the model?
- How grammatical are the "filled gaps" proposed by the model given that in principle the model does not modifies human constraints?
- Which is the impact of using different ratios of raw/augmented sentences for training?
Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs?
The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints: - Boosting Neural Machine Translation with Similar Translations. Jitao Xu, Josep Crego, Jean Senellart - Training Neural Machine Translation to Apply Terminology Constraints. Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan - Neural Machine Translation Decoding with Terminology Constraints. Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, Bill Byrne - Levenshtein Transformer. Jiatao Gu, Changhan Wang, Jake Zhao Typos: 261: in the abve equation "are" used to guarantee that all 

Review Point: - How often invalid constraints are finally hypothesized by the model?
Review Point: - How grammatical are the "filled gaps" proposed by the model given that in principle the model does not modifies human constraints?
Review Point: - Which is the impact of using different ratios of raw/augmented sentences for training? Table 5 shows a performance boost of 0.68 BLEU when comparing #Raw and #Augmented. Where does the gain come from? Did you find the same (or similar) gains on the other language pairs? The paper would benefit from a comparison to (or at least mentioning) related work on integrating lexical constraints:
==================================================

Focused review:

Weaknesses ---
[1 - Evaluation (major concern)] In few-shot image synthesis, image quality metrics (FID, KID) can be often improved at the cost of the synthesis diversity [1] (figure 4). For example, the model that perfectly reproduces the training set can achieve near-to-zero FID and KID. The diversity aspect is not evaluated in this work at all. This makes the whole evaluation of the paper unconvincing. Does FreGAN improve FID of FastGAN at the cost of diversity? Is FreGAN memorizing training images more than FastGAN? Are the interpolations in the latent space of FreGAN smooth? It is not possible to answer with the provided figures and tables.
In addition to FID or KID, I suggest adding a metric that explicitly measures the diversity of generated images, as done in some previous works. One can choose pairwise LPIPS as in [2] (tables G,H), or intra-cluster LPIPS as in [3] (table 2). Moreover, it would be good to measure the average LPIPS from generated images to the nearest example from the training set, which would help to assess the degree of memorization of the training set ([4], table 8). Lack of diversity is also partially supported by low recall across different datasets reported in the supplementary materials. On the qualitative side, it would be good to add examples of latent space interpolations as in [4] (figure 5) and the nearest real neighbors to some of generated images ([4], figure 12-13). Overall, without this analysis, given the size of the used datasets, only improving FID and KID is not a convincing result to me.
[2 - Claim (major concern)] The paper claims that the proposed method alleviates the unhealthy competition between G and D (lines 11, 47, 54) in the low data regime. The caption of figure 5 says that the FreGAN generator can better deceive the discriminator. The justification for the claim is claimed in the loss curves plot in figure 5(a).
In fact, what is seen in figure 5(a) seems to say the opposite. The discriminator loss of FreGAN is lower than the one of FastGAN (even though it has an additional term L^HF). This indicates that the discriminator can dissect real and fake images more confidently. It would be interesting to see the curves of the discriminator outputs during training (e.g., as in [5], figure 1(b-c)), where this effect should be more visible. Consequently, based on the plot 5(a), the FreGAN generator struggles to fool the discriminator more than FastGAN. This is confirmed by the larger G loss for FreGAN. It is thus not clear how the better GAN equilibrium is quantified by the authors, so one of the main claims is not supported.
[3.1 – Clarity (major concern)] Generally, it is not clear why the technical proposals of the paper are introduced as techniques for training GANs in low data regimes. The frequency bias for GANs exists not only when the training data size is limited [6]. In principle, any GAN model should benefit from generating images with better spectral properties (e.g., the motivation in lines 33-39 also fits to training GANs on large datasets). The explanation on mitigating unhealthy G and D competition is, in my opinion, unsupported (see Weaknesses-2).
Why are the introduced modifications tested only in low-data regimes? Do the proposed techniques help to improve the quality of images or spectral properties of GANs trained on large datasets?
[3.2 – Related work (major concern)] The proposed method is by design aimed to improve the quality of images in the frequency domain. There are other works with similar motivation (e.g., [6] as an overview). These methods can be potentially placed instead of the techniques introduced in section 3.2-3.4. It is not clear whether the effect of the proposed techniques could not be achieved with already existing techniques mitigating spectral biases of GANs. A proper comparison to previous works is expected.
[3.3 – Evaluation (major concern)] The proposed method is aimed to improve the quality of images in the frequency domain, but this aspect is not evaluated quantitatively. The only provided comparisons of high/low frequencies are visual, which makes it subjective, especially since ground truth spectrums to compare to are not available. Quantitative assessment of GAN images spectrums was approached before, for example by measuring the accuracy of a binary classifier trained on spectrum features of real and fake images [7]. A proper quantitative comparison of spectral properties of images for different models is expected.
[4 – Clarity (moderate concern)] The motivation of the frequency alignment loss (lines 151-161) is not clear. Firstly, what is meant by “G can only synthesize arbitrary frequency signals”? The synthesis of G is guided with the discriminator loss, which provides supervision on the image realism, which includes the intensity of details of different frequencies. So they should not be arbitrary. Secondly, why is HDA called a “regularizer for D”, while it is included only to the objective of G?
Finally, eq. (4) is a form of a reconstruction loss, but it is computed between D’s features of a real image and G’s features of a randomly sampled fake image. These images are not necessarily aligned spatially, so it is not clear why imposing a pixel-wise L1 reconstruction loss is meaningful. Please explain.
--- References ---
[1] Few-Shot Adaptation of Generative Adversarial Networks. Robb et al. Arxiv, 2020.
[2] Generating Novel Scene Compositions from Single Images and Videos. Sushko et al. Arxiv, 2021.
[3] Few-shot Image Generation via Cross-domain Correspondence. Ojha et al. CVPR 2021.
[4] Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis. Liu et al. ICLR 2021.
[5] Training Generative Adversarial Networks with Limited Data. Karras et al. NeurIPS 2020.
[6] On the Frequency Bias of Generative Models. Schwarz et al. NeurIPS 2021.
[7] Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions. Durall et al. CVPR 2020.
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Review Point: --- [1 - Evaluation (major concern)] In few-shot image synthesis, image quality metrics (FID, KID) can be often improved at the cost of the synthesis diversity [1] (figure 4). For example, the model that perfectly reproduces the training set can achieve near-to-zero FID and KID. The diversity aspect is not evaluated in this work at all. This makes the whole evaluation of the paper unconvincing. Does FreGAN improve FID of FastGAN at the cost of diversity? Is FreGAN memorizing training images more than FastGAN? Are the interpolations in the latent space of FreGAN smooth? It is not possible to answer with the provided figures and tables. In addition to FID or KID, I suggest adding a metric that explicitly measures the diversity of generated images, as done in some previous works. One can choose pairwise LPIPS as in [2] (tables G,H), or intra-cluster LPIPS as in [3] (table 2). Moreover, it would be good to measure the average LPIPS from generated images to the nearest example from the training set, which would help to assess the degree of memorization of the training set ([4], table 8). Lack of diversity is also partially supported by low recall across different datasets reported in the supplementary materials. On the qualitative side, it would be good to add examples of latent space interpolations as in [4] (figure 5) and the nearest real neighbors to some of generated images ([4], figure 12-13). Overall, without this analysis, given the size of the used datasets, only improving FID and KID is not a convincing result to me. [2 - Claim (major concern)] The paper claims that the proposed method alleviates the unhealthy competition between G and D (lines 11, 47, 54) in the low data regime. The caption of figure 5 says that the FreGAN generator can better deceive the discriminator. The justification for the claim is claimed in the loss curves plot in figure 5(a). In fact, what is seen in figure 5(a) seems to say the opposite. The discriminator loss of FreGAN is lower than the one of FastGAN (even though it has an additional term L^HF). This indicates that the discriminator can dissect real and fake images more confidently. It would be interesting to see the curves of the discriminator outputs during training (e.g., as in [5], figure 1(b-c)), where this effect should be more visible. Consequently, based on the plot 5(a), the FreGAN generator struggles to fool the discriminator more than FastGAN. This is confirmed by the larger G loss for FreGAN. It is thus not clear how the better GAN equilibrium is quantified by the authors, so one of the main claims is not supported.
==================================================

Focused review:

- "In addition, we force \hat{\phi} to only be learned through L_model, so that vm uses it as an additional input" (189) seems unclear / unmotivated. - Atari normalized score looks standard, and not much insight gained into the strengths of the hindsight value functions. - What is Y_t in figure 2, maybe this figure could be better represented?

Review Point: - "In addition, we force \hat{\phi} to only be learned through L_model, so that vm uses it as an additional input" (189) seems unclear / unmotivated.
Review Point: - Atari normalized score looks standard, and not much insight gained into the strengths of the hindsight value functions.
Review Point: - What is Y_t in figure 2, maybe this figure could be better represented?
==================================================

Focused review:

Weaknesses: 1. The representation could be further improved. For example, there are both “unseen classes” and “unseen-classes” in the paper, this should be unified. 2. It would be better to study the impact of the ratio of unseen classes. For example, how the performance varies with different ratios of unseen classes unlabeled examples.

Review Point: 1. The representation could be further improved. For example, there are both “unseen classes” and “unseen-classes” in the paper, this should be unified.
Review Point: 2. It would be better to study the impact of the ratio of unseen classes. For example, how the performance varies with different ratios of unseen classes unlabeled examples.
==================================================

Focused review:

- While the baselines are strong, the way they are reported may be a bit misleading. In particular, models are compared based on the sparsity percentage, which puts models with fewer parameters (e.g., MiniBERT) at a disadvantage. - As with most work on pruning, it is not yet possible to realize efficiency gains on GPU.

Review Point: - While the baselines are strong, the way they are reported may be a bit misleading. In particular, models are compared based on the sparsity percentage, which puts models with fewer parameters (e.g., MiniBERT) at a disadvantage.
Review Point: - As with most work on pruning, it is not yet possible to realize efficiency gains on GPU.
==================================================

Focused review:

Weaknesses: - A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important. - There are no ablation studies on network architectures and also no ablation experiments on how the representative points are selected. - The runtime of the proposed network seems slow compared to several recent techniques. Even for just 1K-2K points, the network seem to be taking 0.2-0.3 seconds. How does the runtime scales with more points (say 100K to 1M points)? It would be good if authors also report relative runtime comparisons with existing techniques.  Minor corrections: - Line 88: "lose" -> "loss". - line 135: "where K" -> "and K".  Minor suggestions: - "PointCNN" is a very short non-informative title. It would be good to have a more informative title that represents the proposed technique. - In several places: "firstly" -> "first". - "D" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion.  Review summary: - The proposed technique is sensible and the performance on different benchmarks is impressive. Missing comparisons to established STN technique (with both local and global transformations) makes this short of being a very good paper.  After rebuttal and reviewer discussion: - I have the following minor concerns and reviewers only partially addressed them. 1. Explicit comparison with STN: Authors didn't explicitly compare their technique with STN. They compared with PointNet which uses STN. 2. No ablation studies on network architecture. 3. Runtimes are only reported for small point clouds (1024 points) but with bigger batch sizes. How does runtime scale with bigger point clouds? Authors did not provide new experiments to address the above concerns. They promised that a more comprehensive runtime comparison will be provided in the revision. Overall, the author response is not that satisfactory, but the positive aspects of this work make me recommend acceptance assuming that authors would update the paper with the changes promised in the rebuttal. Authors also agreed to change the tile to better reflect this work.

Review Point: - A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important.
Review Point: - There are no ablation studies on network architectures and also no ablation experiments on how the representative points are selected.
Review Point: - The runtime of the proposed network seems slow compared to several recent techniques. Even for just 1K-2K points, the network seem to be taking 0.2-0.3 seconds. How does the runtime scales with more points (say 100K to 1M points)? It would be good if authors also report relative runtime comparisons with existing techniques. Minor corrections:
Review Point: - line 135: "where K" -> "and K". Minor suggestions:
Review Point: - "PointCNN" is a very short non-informative title. It would be good to have a more informative title that represents the proposed technique.
Review Point: - "D" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary:
Review Point: - The proposed technique is sensible and the performance on different benchmarks is impressive. Missing comparisons to established STN technique (with both local and global transformations) makes this short of being a very good paper. After rebuttal and reviewer discussion:
Review Point: - I have the following minor concerns and reviewers only partially addressed them.
Review Point: 1. Explicit comparison with STN: Authors didn't explicitly compare their technique with STN. They compared with PointNet which uses STN.
Review Point: 3. Runtimes are only reported for small point clouds (1024 points) but with bigger batch sizes. How does runtime scale with bigger point clouds? Authors did not provide new experiments to address the above concerns. They promised that a more comprehensive runtime comparison will be provided in the revision. Overall, the author response is not that satisfactory, but the positive aspects of this work make me recommend acceptance assuming that authors would update the paper with the changes promised in the rebuttal. Authors also agreed to change the tile to better reflect this work.
==================================================

Focused review:

1. The proposed Bayesian surrogate model contains two parts: 1) a GP model that captures the ``noise-free'' component of the immediate feedback, and 2) a noise distribution used to absorb all the stochasticity. It's not clear what are the noises? How do they affect the feedback? It would help better understand the model if authors can provide some examples. 2. For the uncertainty, the authors should explain what kind of uncertainty could be extracted from the noises. 3. The synthetic experiments look good, but it's not convincing if there is no real world experiments in production systems.

Review Point: 1) a GP model that captures the ``noise-free'' component of the immediate feedback, and 2) a noise distribution used to absorb all the stochasticity. It's not clear what are the noises? How do they affect the feedback? It would help better understand the model if authors can provide some examples.
Review Point: 2. For the uncertainty, the authors should explain what kind of uncertainty could be extracted from the noises.
Review Point: 3. The synthetic experiments look good, but it's not convincing if there is no real world experiments in production systems.
==================================================

Focused review:

Weaknesses
The writing should be improved overall. Issues throughout include grammar, misuse of commas, capitalization (as in reference to algorithm 1 on page 6), technical details left out, poor formatting.
There seems to be a lack of novelty in the modeling approach: the same training schemes, DNN model, etc have been developed before, in papers that are cited by this one.
The results are rather limited and do not seem to show a clear advantage over standard techniques.
Some sentences have unclear meaning, e.g.: Page 1 - ‘a manual adaptation and business knowledge are needed…” - ‘its dynamic dimension reflects directly the demand change over the learning steps…” Page 2 - ‘Before we deep dive in the model architecture and present its main components, we should briefly highlight some problem-related concepts’ - the narration is too casual Page 5 - Equation 13 label is cut off - means that violation is given -infinity in what sense? A large floating point number? Page 6 - Training: Sample rollout and greedy rollout - are these ever defined? What is the baseline function used in the REINFORCE algorithm? Page 7 - Google OR-Tools baseline - what are the details of the implementation? Is it the CP-Sat solver? A specialized solver? - What is the meaning of the numbers in Table 1? Is lower better? - Why does RNN-RL appear twice in Table 2 with different results? - Is OR-tools called with a solver timeout? Or it is allowed to run to completion? - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method?
General questions:
Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case?
Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper?
What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.

Review Point: - What is the meaning of the numbers in Table 1? Is lower better?
Review Point: - Why does RNN-RL appear twice in Table 2 with different results?
Review Point: - Is OR-tools called with a solver timeout? Or it is allowed to run to completion?
Review Point: - The results for RNN-RL are very similar to OR-tools. Can you highlight what is thew advantage of your method? General questions: Because the solution is built incrementally, is it possible to take an action that leads to no further feasible actions? (Assuming that feasible solutions require every demand to be met - this isn’t made clear in the VRPTW description). This case is different from those in the remark on page 5 - what is done in this case? Generally, does the masking scheme guarantee solutions to be feasible? Is this discussed in the paper? What is the novelty of the approach? The network architecture, training scheme, masking, input representation, etc have all been studied before.
==================================================

Focused review:

Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks. - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary? Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary. More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD? Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. 
Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).
Minor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. 

Review Point: - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary? Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary. More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD? Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract). Minor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate.
==================================================

Focused review:

Weaknesses
- The novelty of the proposed random masked hindsight control is a bit limited. Although the authors claimed the difference to BERT-like models, the general idea of mask-and-predict is the same and the only difference is the items to mask. This idea is also similar to those visual pre-training models like ViT etc.
- It is unclear why only masking the actions while leaving out the observations. The authors claimed the reason to be "force the model to learn global temporal relations", but there is no clear evidence to support this. What if including the observations as well, or partially? or what if similar to the random scheme as proposed, randomly predict the observations as well?
- From the description, the overall objective used all three mentioned terms as mentioned in Sec. 5.2 / Fig. 2. The description also seems to suggest the effectiveness of the 3rd term of random masked hindsight control, but it is unclear how would the model performs if only using this term.
- For the experiment of ‘versatility’ as shown in Fig. 3, there are some cases where the CT-single performs on par with or even better than the proposed SMART, i.e. single-task performs better. It would be better if some explanation or insights could have been provided.

Review Point: - The novelty of the proposed random masked hindsight control is a bit limited. Although the authors claimed the difference to BERT-like models, the general idea of mask-and-predict is the same and the only difference is the items to mask. This idea is also similar to those visual pre-training models like ViT etc.
Review Point: - It is unclear why only masking the actions while leaving out the observations. The authors claimed the reason to be "force the model to learn global temporal relations", but there is no clear evidence to support this. What if including the observations as well, or partially? or what if similar to the random scheme as proposed, randomly predict the observations as well?
Review Point: - From the description, the overall objective used all three mentioned terms as mentioned in Sec. 5.2 / Fig.
Review Point: 2. The description also seems to suggest the effectiveness of the 3rd term of random masked hindsight control, but it is unclear how would the model performs if only using this term.
Review Point: - For the experiment of ‘versatility’ as shown in Fig. 3, there are some cases where the CT-single performs on par with or even better than the proposed SMART, i.e. single-task performs better. It would be better if some explanation or insights could have been provided.
==================================================

Focused review:

Weakness: 1. At Page 4, right after Eq.3, it is claimed that the sample size should be necessarily large. However, I fail to see clear connection between the sample size and the proposed RNN-based approach. 2. Though RNN can be used as an unfolding of an iterative algorithm, it still requires the assumption that different columns in single patches are formulated as a sequence with clear order. However, this may not be true. At least a bi-directional RNN considering the order of columns along two complementary directions should be considered. 3. The experiments are not comprehensive. In Table 1, the performance difference between RNN & RNN-RFN is not significant. A more detailed explanation should be provided. Also, the PSNR and SSIM are not clearly labeled (I assume only the bottom approaches have SSIM scores). 4. What are the compared approaches? The author should at least give a reference for approaches listed in Table 1 and explain why the performance underperforms the compared approaches by a large margin. 5. As mentioned above, the tasks used for evaluation are too simple. In addition to the simple cases, evaluation on a more challenging task (e.g., a more challenging noise situation added in image deblurring) should be performed.

Review Point: 1. At Page 4, right after Eq.3, it is claimed that the sample size should be necessarily large. However, I fail to see clear connection between the sample size and the proposed RNN-based approach.
Review Point: 2. Though RNN can be used as an unfolding of an iterative algorithm, it still requires the assumption that different columns in single patches are formulated as a sequence with clear order. However, this may not be true. At least a bi-directional RNN considering the order of columns along two complementary directions should be considered.
Review Point: 3. The experiments are not comprehensive. In Table 1, the performance difference between RNN & RNN-RFN is not significant. A more detailed explanation should be provided. Also, the PSNR and SSIM are not clearly labeled (I assume only the bottom approaches have SSIM scores).
Review Point: 4. What are the compared approaches? The author should at least give a reference for approaches listed in Table 1 and explain why the performance underperforms the compared approaches by a large margin.
Review Point: 5. As mentioned above, the tasks used for evaluation are too simple. In addition to the simple cases, evaluation on a more challenging task (e.g., a more challenging noise situation added in image deblurring) should be performed.
==================================================

Focused review:

WEAKNESSES
** missing references **
-First, the literature review should be extended to comment on recent approaches relying on part discovery/recognition. All provided references are 10+ years old, and recent works on compositional and part based representations/models for classification should be discussed.
Few examples could be:
CoupleNet, Zhu et al, ICCV 2017
Object-Part Attention Model for Fine-GrainedImage Classification, IEEE TIP, 2018
Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up, CVPR 2019
Exploiting spatial relation for fine-grained image classification, Pattern Recognition, 2019
Learning Compositional Representations for Few-Shot Recognition, ICCV 2019
While these works are not necessarily closely related to the proposed work and do not reevaluate novelty, they are contemporary part based works that explore part based modelling.
More importantly, authors should mention and discuss
‘Neural activation models’ Simon et al 2015, ICCV
Which aims to integrate a constellation model within a neural network model and is therefore strongly related work.
** clarity/justifications **
-While the approach appears to yield good performance increase, it is difficult to comprehend intuitively why that is the case. Justification for certain model decisions is not clearly provided, and certain claims are not supported by evidence. For example, the claim that the clustering procedure explicitly identifies object parts is not obvious not clearly shown in experiments. Indeed, clusters and their centers are assumed to represent object parts. However, experiments demonstrate that larger numbers of clusters yield stronger performance, suggesting that a coarse superpixel type clustering might be more promising than hoping to identify object parts across images. Similarly, the results in Fig 3 suggest that the discovered clusters are more appearance oriented than identifying parts (cf brown vs white on dog instead of recognising parts e.g. ears).
The clustering process itself lacks clarity. Are cluster centers fixed after training? Is the optimal number of cluster affected by the number of classes/similarities between classes? Are all cluster centers receiving assignments for each batches? Are batches constructed so as to optimise cluster learning? Can authors expand on the role of count parameter s? Are different clusters relevant to e.g. dog classes different from clusters for bird classes? As intuitively these clusters represent object parts, providing more attention to these, both in terms of explanation and experiments would be preferable.
-The motivation behind the use of a distance map is unclear. Could authors elaborate on why performing self attention on the distance map provides relevant information vs e.g. self attention on the clustered feature maps?
-The paper would benefit from a clearer depiction of the constellation model and how the proposed approach relates to them, intuitively. With the current writing, the concept of constellation models is quickly brushed over, and the motivation behind the use of a distance map + attention is not clearly stated but only proposed as an alternative to the probabilistic modelling used in old school constellation models. The paper would strongly benefit from providing justification on why this is a valid alternative, and what we hope to learn using this strategy; and more precisely, why this strategy can be viewed as a constellation type model.
-Several aspects of the few-shot learning formulation should be more clearly explained. While the paper is perfectly understandable for a reader accustomed to FSL methods and settings, the lack of explanations regarding episode training, meta-training/testing, and the protonet model itself would make it very difficult to follow for a non expert. RECOMMENDATION
The proposed work aims to introduce part representation in few-shot models, which is an appealing strategy. Adapting popular traditional models to contemporary settings is a promising idea, and the proposed method reaches SOTA performance on standard benchmark.
The paper in its current state needs a little more attention, and I will be happy to increase my rating if my main concerns are addressed
1- Relating the model and its components more closely to constellation models, and justification as to why the proposed strategy is a better implementation of constellations in deep learning framework than Simon et al. 2015
2- Providing clarifications regarding design decisions, experimental setting, and more intuition. In particular regarding the distance map and cluster centers. If possible, according more attention to interpretability/observed behaviour of cluster centers in the experimental section.
ADDITIONAL COMMENTS - Experiments are missing important details. For example, it is not specified for experiments in Figure 2 and 3 which dataset and parameter configurations are used. In particular for figure 3, is the number of clusters set to 64? Are all cluster centers relevant to a given class? - Examples from the same class provided in this figure look very similar in appearance. What happens when examples of the same class look different? Are same cluster patterns observed? - The multi-branch training strategy is not new and was suggested in TADAM, Oreshkin et al., NeurIPS, 2018. - Regarding ablation experiments, it would be interesting to see the influence of having a single module on the last layer (where levels of abstraction would be higher) vs modules at every layer. - claims regarding ‘explicit modelling of parts’ should be revised. There is no explicit part discovery (nor a guarantee that object parts are indeed discovered), nor a clear, explicit modelling of their interactions. Maybe a more accurate characterisation would be that the approach integrate spatial information between image regions of similar appearance/texture. Similarly, it is not obvious that CNNs extract object parts.

Review Point: - Examples from the same class provided in this figure look very similar in appearance. What happens when examples of the same class look different? Are same cluster patterns observed?
Review Point: - The multi-branch training strategy is not new and was suggested in TADAM, Oreshkin et al., NeurIPS, 2018.
Review Point: - Regarding ablation experiments, it would be interesting to see the influence of having a single module on the last layer (where levels of abstraction would be higher) vs modules at every layer.
Review Point: - claims regarding ‘explicit modelling of parts’ should be revised. There is no explicit part discovery (nor a guarantee that object parts are indeed discovered), nor a clear, explicit modelling of their interactions. Maybe a more accurate characterisation would be that the approach integrate spatial information between image regions of similar appearance/texture. Similarly, it is not obvious that CNNs extract object parts.
==================================================

Focused review:

- The grouping of bias into malignant vs benign seems not very convincing. This is because training with those biases can vary under different settings, for example, the number of training samples, the number of training epochs, learning rates, weight decay and model architectures. The biased attribute can be malignant under one setting, while is benign under a different setting. What would happen to a benign bias attribute when the total number of input dimensions are reduced? For example, just leave the center area of the Color MNIST images? - I cannot agree with the claim “easier” bias attributes are likely malignant. To me, it is more like “the texture biases (like color) are likely malignant, while the shape biases are likely benign”. It is hard to say “easier to learn”, but more like “learn more thoroughly”, as the color information will contribute the most to the neurons’ activations in the early stage. - The use of GCE loss is not well motivated, as there are many loss functions that differentiate between low and high confidence samples including the CE loss. GCE was initially designed to mitigate overfitting to noisy labels, however, it seems that the baised model needs more overfitting (to the bias attribute)? Suppose the upweight to CE gradients is indeed necessary, then why not use Focal Loss and its variant? Another question is, since the biased model learns the bias in the early stage, then why not stop training it at the later stages, considering that the later stage will learn more hard (not easy) examples, as pointed out in Arpit et al. [1]. Isn’t the later training will forget some easy examples, or equivalently ‘easier’ and malignant attributes? - Why need two networks? If you look at the two equations at lines 162 and 168, the confidence output of the network can also be used to do the weighting? In other words, since ‘easier’ (e.g. high confidence) examples can carry malignant bias, so not just use the confidence to obtain the relative difficulty score? Or why not modify the GCE loss to avoid training a biased model? Why it has to train an extremely biased model using GCE, then use it to train a second model. The proposed two-network architecture looks similar to co-teaching [2] and related structures, but not discussed.

Review Point: - The grouping of bias into malignant vs benign seems not very convincing. This is because training with those biases can vary under different settings, for example, the number of training samples, the number of training epochs, learning rates, weight decay and model architectures. The biased attribute can be malignant under one setting, while is benign under a different setting. What would happen to a benign bias attribute when the total number of input dimensions are reduced? For example, just leave the center area of the Color MNIST images?
Review Point: - I cannot agree with the claim “easier” bias attributes are likely malignant. To me, it is more like “the texture biases (like color) are likely malignant, while the shape biases are likely benign”. It is hard to say “easier to learn”, but more like “learn more thoroughly”, as the color information will contribute the most to the neurons’ activations in the early stage.
Review Point: - The use of GCE loss is not well motivated, as there are many loss functions that differentiate between low and high confidence samples including the CE loss. GCE was initially designed to mitigate overfitting to noisy labels, however, it seems that the baised model needs more overfitting (to the bias attribute)? Suppose the upweight to CE gradients is indeed necessary, then why not use Focal Loss and its variant? Another question is, since the biased model learns the bias in the early stage, then why not stop training it at the later stages, considering that the later stage will learn more hard (not easy) examples, as pointed out in Arpit et al. [1]. Isn’t the later training will forget some easy examples, or equivalently ‘easier’ and malignant attributes?
Review Point: - Why need two networks? If you look at the two equations at lines 162 and 168, the confidence output of the network can also be used to do the weighting? In other words, since ‘easier’ (e.g. high confidence) examples can carry malignant bias, so not just use the confidence to obtain the relative difficulty score? Or why not modify the GCE loss to avoid training a biased model? Why it has to train an extremely biased model using GCE, then use it to train a second model. The proposed two-network architecture looks similar to co-teaching [2] and related structures, but not discussed.
==================================================

Focused review:

- Some definitions and statements are not clear or well justified.
- Lack of clarity in the definition of the input/outputs for each subtask 
063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
092 Missing reference for wordnet definition 112-114 "The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1." - > It doesn't seem clear.
115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention "mango" and "nutrient" and do not seem to be concepts according to your definition.
188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?
Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity. 

Review Point: - Some definitions and statements are not clear or well justified.
Review Point: - Lack of clarity in the definition of the input/outputs for each subtask 063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
Review Point: 092 Missing reference for wordnet definition 112-114 "The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1." - > It doesn't seem clear.
Review Point: 115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention "mango" and "nutrient" and do not seem to be concepts according to your definition.
Review Point: 188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
Review Point: 214 and the synset is represented by the pre-trained embedding of the synonym word itself.
Review Point: - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ...
Review Point: 245 The GAT is trained with the whole model? Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity.
==================================================

Focused review:

Weaknesses:
I have several concerns regarding this paper.
• Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).
• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).
• Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments.
• Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this.
• Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3].
• Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling.
• Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable.
• Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting.
[1] https://arxiv.org/pdf/2003.00982.pdf [2] https://ogb.stanford.edu/ [3] https://paperswithcode.com/task/node-classification ==========
After reading the authors comments.
I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score.
I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.

Review Point: • Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).
Review Point: • Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).
Review Point: • Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments.
Review Point: • Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this. • Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3]. • Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling. • Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable. • Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting. [1] https://arxiv.org/pdf/2003.00982.pdf [2] https://ogb.stanford.edu/ [3] https://paperswithcode.com/task/node-classification ========== After reading the authors comments. I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score. I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.
==================================================

Focused review:

Weaknesses: - The evaluation is incomplete. The previous papers by Mirhoseini et al. unfortunately didn't compare against simple/trivial baseline algorithms like random search (try random placements and keep track of the best), hill climbing, genetic algorithms, etc. to show whether a learning approach outperforms them given the same computational budget and placement evaluation budget. This paper also doesn't address the shortcoming. So it's not at all clear whether placement optimization really requires any sophisticated optimization in the first place. Comments/Questions: - Please include in the Appendix the type of graphs shown in Figure 2 for all the entries in Table 1. This way it will be clear that the same kind of behavior seen in the three graphs in Figure 2 hold for all the settings. - Are the results in Table 1 strictly controlled for the same amount of computation and the same number of placement evaluations allowed to Policy Gradient and Post? The text does not say this. If they are not, then the results may not be meaningful. - For many TensorFlow graphs for which placement optimization is difficult, a key challenge is to stay within the memory limits of the devices. So it may be difficult to find placements, especially early on in the optimization, that satisfy memory constraints. How is this problem handled by the proposed algorithm? - TensorFlow graph running times can show high variance. What are the error bars for Figure 2 and Table 1? - Is the algorithm that is labelled as Policy Gradient a re-implementation of the Mirhoseini et al.'s approach? If so, their work relied on grouping the ops and making group-level device placements, and how were the grouping decisions made?

Review Point: - The evaluation is incomplete. The previous papers by Mirhoseini et al. unfortunately didn't compare against simple/trivial baseline algorithms like random search (try random placements and keep track of the best), hill climbing, genetic algorithms, etc. to show whether a learning approach outperforms them given the same computational budget and placement evaluation budget. This paper also doesn't address the shortcoming. So it's not at all clear whether placement optimization really requires any sophisticated optimization in the first place. Comments/Questions:
Review Point: - Please include in the Appendix the type of graphs shown in Figure 2 for all the entries in Table 1. This way it will be clear that the same kind of behavior seen in the three graphs in Figure 2 hold for all the settings.
Review Point: - Are the results in Table 1 strictly controlled for the same amount of computation and the same number of placement evaluations allowed to Policy Gradient and Post? The text does not say this. If they are not, then the results may not be meaningful.
Review Point: - For many TensorFlow graphs for which placement optimization is difficult, a key challenge is to stay within the memory limits of the devices. So it may be difficult to find placements, especially early on in the optimization, that satisfy memory constraints. How is this problem handled by the proposed algorithm?
Review Point: - TensorFlow graph running times can show high variance. What are the error bars for Figure 2 and Table 1?
Review Point: - Is the algorithm that is labelled as Policy Gradient a re-implementation of the Mirhoseini et al.'s approach? If so, their work relied on grouping the ops and making group-level device placements, and how were the grouping decisions made?
==================================================

Focused review:

- The paper is fairly incremental, developing a single heuristic local search method (namely NOTEARS that enjoys no non-trivial performance guarantees). - I find the claim that NOTEARS and FGS outperform earlier methods (Line 276) questionable. E.g., for moderate numbers of nodes d, as studied in the present paper, Ref. 2 (Table 1, p. 2306) shows that MMHC and PC perform much better than GES (here FGS). This is critical for the overall positioning of the empirical results in the context of the state of the art. (The same weakness concern also the original NOTEARS work and many follow-up works.)

Review Point: - The paper is fairly incremental, developing a single heuristic local search method (namely NOTEARS that enjoys no non-trivial performance guarantees).
Review Point: - I find the claim that NOTEARS and FGS outperform earlier methods (Line 276) questionable. E.g., for moderate numbers of nodes d, as studied in the present paper, Ref.
Review Point: 2 (Table 1, p. 2306) shows that MMHC and PC perform much better than GES (here FGS). This is critical for the overall positioning of the empirical results in the context of the state of the art. (The same weakness concern also the original NOTEARS work and many follow-up works.)
==================================================

Focused review:

Weakness:
Could the proposed formulation be applied to other 3D shape representations, e.g. point cloud, implicit? If so, an experiment should be added. If not, then better to mention this is a mesh-specific technic.
The algorithm seems to be computational costy, e.g. 1 min per image. Would this heavily affect the training efficiency? While comparing to other methods, e.g. SoftRas, the run-time efficiency and hyper-parameters, e.g. number of iteration, training epoch, should be mentioned. Otherwise the comparison may not be on a fair basis.
Keeping the boundary differentiable is one of the main issues in existing differentiable rendering technics, which is also argued mainly in this paper. However it is not clear how this is improved by the proposed method. From Fig 2 and 4, it seems the boundary area has obvious artifacts. Also some experiments should be performed to highlight the improvements over occlusion boundary.
The performance on 3D shape prediction does not outperform existing approaches.
Overall, despite a novel formulation, the improvements over existing approaches are not clear. As shown in the experiment, the propose method does not necessarily produce more accurate rendering or support better for vision tasks. Considering that the proposed tech is also quite slow, the overall advantage over existing approaches is not very clear.

Review Point: 1 min per image. Would this heavily affect the training efficiency? While comparing to other methods, e.g. SoftRas, the run-time efficiency and hyper-parameters, e.g. number of iteration, training epoch, should be mentioned. Otherwise the comparison may not be on a fair basis. Keeping the boundary differentiable is one of the main issues in existing differentiable rendering technics, which is also argued mainly in this paper. However it is not clear how this is improved by the proposed method. From Fig 2 and 4, it seems the boundary area has obvious artifacts. Also some experiments should be performed to highlight the improvements over occlusion boundary. The performance on 3D shape prediction does not outperform existing approaches. Overall, despite a novel formulation, the improvements over existing approaches are not clear. As shown in the experiment, the propose method does not necessarily produce more accurate rendering or support better for vision tasks. Considering that the proposed tech is also quite slow, the overall advantage over existing approaches is not very clear.
==================================================

Focused review:

1. I was surprised not to see the discussion of the convergence results for SCAFFOLD (Karimireddy et al., 2019) in the paper. To the best of my knowledge, SCAFFOLD is the only known federated method that is able to converge with a linear rate in the strongly convex case when workers compute full gradients of local functions. Therefore, in order to provide a complete picture of the best-known results to this moment, it is needed to add SCAFFOLD into consideration. 2. Although the paper is mainly theoretical, the experimental part of the paper seems to be underdeveloped and does not support all the new insights suggested by the theory. For example, it would be interesting to see the experiments showing that Accelerated MB-SGD outperforms MB-SGD and Local-SGD when \zeta_*^2 is big. Next, for large K (K = 100) it is not easy to see from the presented plots how Local-SGD and MB-SGD differ in terms of achieved error when \zeta_*^2 is small.

Review Point: 1. I was surprised not to see the discussion of the convergence results for SCAFFOLD (Karimireddy et al., 2019) in the paper. To the best of my knowledge, SCAFFOLD is the only known federated method that is able to converge with a linear rate in the strongly convex case when workers compute full gradients of local functions. Therefore, in order to provide a complete picture of the best-known results to this moment, it is needed to add SCAFFOLD into consideration.
Review Point: 2. Although the paper is mainly theoretical, the experimental part of the paper seems to be underdeveloped and does not support all the new insights suggested by the theory. For example, it would be interesting to see the experiments showing that Accelerated MB-SGD outperforms MB-SGD and Local-SGD when \zeta_*^2 is big. Next, for large K (K = 100) it is not easy to see from the presented plots how Local-SGD and MB-SGD differ in terms of achieved error when \zeta_*^2 is small.
==================================================

Focused review:

Weakness: The main idea of learning detectors with carefully generated chips is good and reasonable, but is implemented by a set of simple practical techniques. 3) Weakness: This is an extension of SNIP [24], and focuses mostly on speed-up. Thus its novelty is significantly limited.

Review Point: 3) Weakness: This is an extension of SNIP [24], and focuses mostly on speed-up. Thus its novelty is significantly limited.
==================================================

Focused review:

Although the improvement is interesting, it has some drawbacks in analysis: 1. It is unfair to compare with Ghadimi and Lan 2013, because they assume weaker assumptions about stochastic gradient, which is very important in the context of gradient clipping. The reason that gradient clipping is useful is that stochastic gradient might have large error, it could be heavy-tail or unbounded. However, this work simply assume that the difference between stochastic gradient and true gradient is bounded, which implies that stochastic gradient is bounded if we assume the true gradient is bounded. This is very problematic. Even one can relax the assumption to sub-Gaussian light tail assumption, it is still not valid in practice. 2. The theoretical version of the proposed algorithm is very impractical. In particular, the clipping parameter is very small in the level of \epsilon^2, and the step size \eta is also very small. This makes a huge gap between theory and experiments, where \gamma is set to be a constant. 3. The theoretical results seems indicate the setting \beta=0 gives the fastest convergence. This seems problematic. There should be some tradeoff in the iteration complexity involving \beta.

Review Point: 1. It is unfair to compare with Ghadimi and Lan 2013, because they assume weaker assumptions about stochastic gradient, which is very important in the context of gradient clipping. The reason that gradient clipping is useful is that stochastic gradient might have large error, it could be heavy-tail or unbounded. However, this work simply assume that the difference between stochastic gradient and true gradient is bounded, which implies that stochastic gradient is bounded if we assume the true gradient is bounded. This is very problematic. Even one can relax the assumption to sub-Gaussian light tail assumption, it is still not valid in practice.
Review Point: 2. The theoretical version of the proposed algorithm is very impractical. In particular, the clipping parameter is very small in the level of \epsilon^2, and the step size \eta is also very small. This makes a huge gap between theory and experiments, where \gamma is set to be a constant.
Review Point: 3. The theoretical results seems indicate the setting \beta=0 gives the fastest convergence. This seems problematic. There should be some tradeoff in the iteration complexity involving \beta.
==================================================

Focused review:

* Practicality: the algorithmic improvements seem trivial. It is just an MOSS algorithm with some micro improvements based on law of iterated logarithm. While there is an EMP version to fine-tune the parameters, I am not seeing how it introduces practical changes if we similarly fine-tune MOSS. * Self-containment: The paper omitted the explanation of MOSS (through reference, I guess this method is equivalent to EXP4, which is better known).

Review Point: * Practicality: the algorithmic improvements seem trivial. It is just an MOSS algorithm with some micro improvements based on law of iterated logarithm. While there is an EMP version to fine-tune the parameters, I am not seeing how it introduces practical changes if we similarly fine-tune MOSS.
Review Point: * Self-containment: The paper omitted the explanation of MOSS (through reference, I guess this method is equivalent to EXP4, which is better known).
==================================================

Focused review:

My concerns are listed as follows. 1. The assumptions are still quite strong: the width needs to be infinite. There still exists a gap between this paper and the practice. It is desired to give an "error analyse" for the “distance” between networks in such an overparameterized regime and those used in practice. For example, how significant would the results change if the width of the network is bounded? 2. The authors suggest that its theoretical results (“infinite-depth networks are equivalent to one-layer networks”) supports ResNets’ good generalizability. A direct measure for generalizability would be a generalization bound (or hypothesis complexity here). Would the current result help deliver a smaller generalization bound? Or, does the current form of equivalence (in the view of NTK) straightforwardly lead to the equivalence in the view of complexity or generalizability? ====== Thanks for the response. My concerns are cleared. It would be great to add the explanation regarding generalization in the next version.

Review Point: 1. The assumptions are still quite strong: the width needs to be infinite. There still exists a gap between this paper and the practice. It is desired to give an "error analyse" for the “distance” between networks in such an overparameterized regime and those used in practice. For example, how significant would the results change if the width of the network is bounded?
Review Point: 2. The authors suggest that its theoretical results (“infinite-depth networks are equivalent to one-layer networks”) supports ResNets’ good generalizability. A direct measure for generalizability would be a generalization bound (or hypothesis complexity here). Would the current result help deliver a smaller generalization bound? Or, does the current form of equivalence (in the view of NTK) straightforwardly lead to the equivalence in the view of complexity or generalizability? ====== Thanks for the response. My concerns are cleared. It would be great to add the explanation regarding generalization in the next version.
==================================================

Focused review:

* This work is motivated by exposure bias due to the mismatch of the maximum-likelihood training objective with inference sampling strategy. However, in the context of stellar results from GPT2/3, this motivation feels less strong. * Space is an issue, but I would suggest including samples in the main text. * Why did the authors not also run ColdGAN without pretraining? How does that perform relative to ScratchGAN?

Review Point: * This work is motivated by exposure bias due to the mismatch of the maximum-likelihood training objective with inference sampling strategy. However, in the context of stellar results from GPT2/3, this motivation feels less strong.
Review Point: * Space is an issue, but I would suggest including samples in the main text.
Review Point: * Why did the authors not also run ColdGAN without pretraining? How does that perform relative to ScratchGAN?
==================================================

Focused review:

Weaknesses:
The experiment part is kind of weak. Only experiments of CIFAR-100 are conducted and only DeiT-small/-base are compared. 1. For comparison with DeiT, can you add DeiT variants with the same number of layers, heads, hidden dimension as CMHSA to do a fair comparison? DeiT with 12 layers performs worse than CMHSA with 6 layers on CIFAR-100 is as expected, thus not a convincing comparison.
2. If possible, can you add CNN models to show if CMHSA would actually make ViT performs better/on-par with CNN models, which should be the ultimate goal of training ViT in low-data regime, otherwise one would pretrain ViT on large scale dataset or just use CNN models.
3. If possible, results on ImageNet can be more convincing of the proposed method

Review Point: 1. For comparison with DeiT, can you add DeiT variants with the same number of layers, heads, hidden dimension as CMHSA to do a fair comparison? DeiT with 12 layers performs worse than CMHSA with 6 layers on CIFAR-100 is as expected, thus not a convincing comparison.
Review Point: 2. If possible, can you add CNN models to show if CMHSA would actually make ViT performs better/on-par with CNN models, which should be the ultimate goal of training ViT in low-data regime, otherwise one would pretrain ViT on large scale dataset or just use CNN models.
Review Point: 3. If possible, results on ImageNet can be more convincing of the proposed method
==================================================

Focused review:

- The paper doesn't discuss the comparison of the new objective compared to standard language modeling. - The new model is a bit incremental but seems effective based on empirical evidence.

Review Point: - The paper doesn't discuss the comparison of the new objective compared to standard language modeling.
Review Point: - The new model is a bit incremental but seems effective based on empirical evidence.
==================================================

Focused review:

Weaknesses
1.This paper is an extended paper to Safaryan's, which reduces the significance.
2.Though authors have discussed the technical contribution, the main difficulty of extending to the arbitrary unbiased compression operators seems to be easy to handle.

Review Point: 1.This paper is an extended paper to Safaryan's, which reduces the significance.
Review Point: 2.Though authors have discussed the technical contribution, the main difficulty of extending to the arbitrary unbiased compression operators seems to be easy to handle.
==================================================

Focused review:

Weaknesses: The experiment results on morphological inflection generation is somewhat mixed. The proposed model is effective if the amount of training data is small (such as CELEX). It is also effective if the alignment is mostly monotonic and less context sensitive (such as Russian, German and Spanish).
- General Discussion: The authors proposed a novel neural model for morphological inflection generation which uses "hard attention", character alignments separately obtained by using a Bayesian method for transliteration. It is substantially different from the previous state of the art neural model for the task which uses "soft attention", where character alignment and conversion are solved jointly in the probabilistic model.
The idea is novel and sound. The paper is clearly written. The experiment is comprehensive. The only concern is that the proposed method is not necessarily the state of the art in all conditions. It is suitable for the task with mostly monotonic alignment and with less context sensitive phenomena. The paper would be more convincing if it describe the practical merits of the proposed method, such as the ease of implementation and computational cost. 

Review Point: - General Discussion: The authors proposed a novel neural model for morphological inflection generation which uses "hard attention", character alignments separately obtained by using a Bayesian method for transliteration. It is substantially different from the previous state of the art neural model for the task which uses "soft attention", where character alignment and conversion are solved jointly in the probabilistic model. The idea is novel and sound. The paper is clearly written. The experiment is comprehensive. The only concern is that the proposed method is not necessarily the state of the art in all conditions. It is suitable for the task with mostly monotonic alignment and with less context sensitive phenomena. The paper would be more convincing if it describe the practical merits of the proposed method, such as the ease of implementation and computational cost.
==================================================

Focused review:

Weaknesses:
It is unfortunate, but I have to say that after reading the paper, I believe a reasonable user of MAE will still use the original MAE recipe. So the primary goal of the paper is not achieved. There are many reasons for this, for example: 1) the final performance is still not beating MAE significantly. There could be some reproducibility issues, but to claim "robust and effective", just with improvements that close the "reproducibility" gap is definitely not enough. 2) Right now the experimental section is not complete, e.g., I could not find the influence of final ImageNet accuracy with different ways or normalization, so this means it is not conclusive. 3) In order to show the recipe is indeed robust, I would very much like to see its effect on even larger models (ViT-H). 4) How robust is the technique in terms of compositionally?
I can tell that the paper is done in a rush. The experiments are not yet complete, there are quite a few broken (and lengthy) sentences, and lots of space is devoted to less important things (I believe for a report like this, an introduction should just be half a page long -- 1.5 pages are too long and way more space can be devoted to experiments if they are done.

Review Point: 1) the final performance is still not beating MAE significantly. There could be some reproducibility issues, but to claim "robust and effective", just with improvements that close the "reproducibility" gap is definitely not enough.
Review Point: 2) Right now the experimental section is not complete, e.g., I could not find the influence of final ImageNet accuracy with different ways or normalization, so this means it is not conclusive.
Review Point: 3) In order to show the recipe is indeed robust, I would very much like to see its effect on even larger models (ViT-H).
Review Point: 4) How robust is the technique in terms of compositionally? I can tell that the paper is done in a rush. The experiments are not yet complete, there are quite a few broken (and lengthy) sentences, and lots of space is devoted to less important things (I believe for a report like this, an introduction should just be half a page long -- 1.5 pages are too long and way more space can be devoted to experiments if they are done.
==================================================

Focused review:

Weaknesses Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training? 
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al "Using Blur to Affect Perceived Distance and Size", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.

Review Point: - the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
Review Point: - related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
Review Point: - According to Held et al "Using Blur to Affect Perceived Distance and Size", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
Review Point: - the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
Review Point: - in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
Review Point: - the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
Review Point: - fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
Review Point: - what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels? Minor suggestions - figure text should be rasterized, and figures should maintain its aspect ratio.
Review Point: - figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
Review Point: - the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
Review Point: - the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
Review Point: - calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.
==================================================

Focused review:

Weaknesses:
The second guarantee of the proposed post-processing algorithm in Theorem 2 requires a strong assumption that the input classifier is Bayes-optimal. Is there a way to bound the performance of the proposed algorithm with any classifier f
using the approximation error between f and f ∗ ?
In addition, if the classifier f
is first trained using some dataset S t r a i n
, all the theoretical results only hold when the dataset S
used in the post-processing is independent of S t r a i n
. Is it correct? Thus, a train/validation/test split is still needed in this setting, and the sample complexity results in table 2 should be interpreted as the validation sample complexity since it does not include the sample needed for training f .
Minor comments:
Figure 1 is really difficult for readers to digest at the beginning of the paper. Readers need to understand α
and DEOO to appreciate the results, so maybe deferring it to later sections will be better. Also, figures are more readable than tables, why not include more figures in the main body and move those tables to the appendix?
Typo: 2 line above Paper Organization, should be “group” instead of “grup”

Review Point: 2 line above Paper Organization, should be “group” instead of “grup”
==================================================

Focused review:

Weakness: 1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions). 2. This paper is not very easy to follow. First, many definitions are used before they are defined. For example, on line 59 in Theorem 1, the authors used the definition of "conditionally positive definite function of order 1", which is defined on line 126 in Definition 2. Also, the author used the definition "\epsilon-negative definite" on line 162, which is defined on line 177 in Definition 3. It will be better if the authors could define those important concepts before using them. Second, the introduction is a little bit too long (more than 2.5 pages) and many parts of that are repeated in Section 2 and 3. It might be better to restructure the first 3 sections for a little bit. Third, it will be good if more captions could be added to the figures in the experiment section so that the readers could understand the results more easily. 3. For the description of Theorem 3, from the proof it seems that we need to use Equation 12 as a condition. It will be better if this information is included in the theorem description to avoid confusions (though I know this is mentioned right before the theorem, it is still better to have it in the theorem description). Also, for the constant c_1, it is better to give it an explicit formula in the theorem.  Reference: [1] Burkard, R. E., Cela, E., Pardalos, P. M., and Pitsoulis, L. S. (1998). The quadratic assignment problem. In Handbook of combinatorial optimization, pages 1713â1809. Springer.  

Review Point: 1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions).
Review Point: 2. This paper is not very easy to follow. First, many definitions are used before they are defined. For example, on line 59 in Theorem 1, the authors used the definition of "conditionally positive definite function of order 1", which is defined on line 126 in Definition 2. Also, the author used the definition "\epsilon-negative definite" on line 162, which is defined on line 177 in Definition 3. It will be better if the authors could define those important concepts before using them. Second, the introduction is a little bit too long (more than 2.5 pages) and many parts of that are repeated in Section 2 and 3. It might be better to restructure the first 3 sections for a little bit. Third, it will be good if more captions could be added to the figures in the experiment section so that the readers could understand the results more easily.
Review Point: 3. For the description of Theorem 3, from the proof it seems that we need to use Equation 12 as a condition. It will be better if this information is included in the theorem description to avoid confusions (though I know this is mentioned right before the theorem, it is still better to have it in the theorem description). Also, for the constant c_1, it is better to give it an explicit formula in the theorem. Reference: [1] Burkard, R. E., Cela, E., Pardalos, P. M., and Pitsoulis, L. S. (1998). The quadratic assignment problem. In Handbook of combinatorial optimization, pages 1713â1809. Springer.
==================================================

Focused review:

Weakness: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body. 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).

Review Point: 1. The paper is a little hard to understand, especially for Section 3.1. The meaning of the symbol \xi is not clearly explained, and this symbol seems to disappear in pseudo-code although it appears in the main body.
Review Point: 2. Figure 3 is the most important picture in the article ,but it is confusing. Although the style of the figure is nice, it does not help me understand PRG. There is no need to place a black box on (1,1).
==================================================

Focused review:

Two comments are listed as below: 1) As the title named High-Fidelity Generative Image Compression, I can not understand why it is called high-fidelity. With the GAN loss and perceptual loss added, the compressed image can be more perceptually preferred as proved before. However, texture generated by GAN is usually fake and it may not be called high-fidelity. I think an additional section is needed to discuss texture generated by the proposed method is much closer to input than others apart from perceptual evaluation. 2) Comparison with the prior work which utilized GAN is necessary to include in section 4 to illustrate the advantage of the proposed method.

Review Point: 1) As the title named High-Fidelity Generative Image Compression, I can not understand why it is called high-fidelity. With the GAN loss and perceptual loss added, the compressed image can be more perceptually preferred as proved before. However, texture generated by GAN is usually fake and it may not be called high-fidelity. I think an additional section is needed to discuss texture generated by the proposed method is much closer to input than others apart from perceptual evaluation.
Review Point: 2) Comparison with the prior work which utilized GAN is necessary to include in section 4 to illustrate the advantage of the proposed method.
==================================================

Focused review:

Weaknesses My main concerns are the presentation and theoretical results.
On the presentation, it is not clear what are the causal associations of the underlying system under investigation and what are the functional relationships assumed for the model that are ultimately learned, e.g. Def. 3 is presented as the underlying data generating mechanism which it isn't (I think). Moreover, it would be more standard to talk about causal graphs when referring to the underlying system, vaguely shown in Fig. 2, rather than for the label inference process. And in addition, definition 4 is hard to read because we don’t know what graph should be considered to determine independencies.
The proof of Thm. 1 is straightforward once Def. 3 and its assumptions are given. Is there any particular insight in this result? There is no discussion of Thm. 2.
Questions and suggestions
Can the authors better describe the reason for such a large performance gap between the proposed approach and existing algorithms. What does “min” and “avg” on table 1 refer to? By the description of the CMNIST dataset it seems agree with that used in the IRM but the performance results are very different.
I don’t understand the paragraph starting “Why is our causal graph general”. It seems to omit unobserved confounding, I don’t think it is accurate that all possible world structures can be reduced to the simple model that is considered.
Please write the optimization problem is Sec. 4.1 in a separate line.

Review Point: 3 is presented as the underlying data generating mechanism which it isn't (I think). Moreover, it would be more standard to talk about causal graphs when referring to the underlying system, vaguely shown in Fig. 2, rather than for the label inference process. And in addition, definition 4 is hard to read because we don’t know what graph should be considered to determine independencies. The proof of Thm.
Review Point: 3 and its assumptions are given. Is there any particular insight in this result? There is no discussion of Thm.
Review Point: 2. Questions and suggestions Can the authors better describe the reason for such a large performance gap between the proposed approach and existing algorithms. What does “min” and “avg” on table 1 refer to? By the description of the CMNIST dataset it seems agree with that used in the IRM but the performance results are very different. I don’t understand the paragraph starting “Why is our causal graph general”. It seems to omit unobserved confounding, I don’t think it is accurate that all possible world structures can be reduced to the simple model that is considered. Please write the optimization problem is Sec. 4.1 in a separate line.
==================================================

Focused review:

1. Some related work seems to have been overlooked: Playing Repeated Security Games with No Prior Knowledge (Xu et al, AAMA 2016). As far as I can see, the paper also makes no assumption about the opponent's utilities, so it seems to be an important benchmark to compare with. (In fact, they didn't even make the assumption that the opponent is behaving according to some best-response function, so perhaps more legitimate to say that in their model the opponent model is unknown.) Following the above comment, I wonder how effective the proposed approach is in regret minimization, compared with an approach similar to that of Xu et al. Understandably, the regret is defined slightly differently here as the best-response function is also incorporated in the regret, so the goal is not completely the same, but in any case, it seems necessary to have some comparison, at least empirically, to highlight the advantage of the proposed approach. 2. I'm not sure how efficient and practical the approach is in dealing with repeated Stackelberg games. The discretization of the leader's mixed strategy space seems to be very costly, so the scalability of the approach is questionable.

Review Point: 1. Some related work seems to have been overlooked: Playing Repeated Security Games with No Prior Knowledge (Xu et al, AAMA 2016). As far as I can see, the paper also makes no assumption about the opponent's utilities, so it seems to be an important benchmark to compare with. (In fact, they didn't even make the assumption that the opponent is behaving according to some best-response function, so perhaps more legitimate to say that in their model the opponent model is unknown.) Following the above comment, I wonder how effective the proposed approach is in regret minimization, compared with an approach similar to that of Xu et al. Understandably, the regret is defined slightly differently here as the best-response function is also incorporated in the regret, so the goal is not completely the same, but in any case, it seems necessary to have some comparison, at least empirically, to highlight the advantage of the proposed approach.
Review Point: 2. I'm not sure how efficient and practical the approach is in dealing with repeated Stackelberg games. The discretization of the leader's mixed strategy space seems to be very costly, so the scalability of the approach is questionable.
==================================================

Focused review:

I have only two minor comments here: - The authors contrast their approach with RNN modeling of neural responses and state that “training a RNN does not clarify why a particular solution is a good solution, or, indeed, if it is a good solution at all”. I appreciate that the two approaches have different advantages but see them as complimentary, and I am not sure that framing RNNs in opposition to normative theories draws a useful distinction. As the authors point out, the parameterisation of their nonparametric approach corresponds to a linear dynamical system which has close connections to RNNs. - In Figure 3, its not clear to me that these neural predictions are uniquely predicted by their normative account. Would an alternative agent, similarly defined in the RL setting but without history compression predict the same neural responses illustrated in Fig 3? It seems as though Figure 3 shows the same diagram for the FDT task whether or not there is policy compression.

Review Point: - The authors contrast their approach with RNN modeling of neural responses and state that “training a RNN does not clarify why a particular solution is a good solution, or, indeed, if it is a good solution at all”. I appreciate that the two approaches have different advantages but see them as complimentary, and I am not sure that framing RNNs in opposition to normative theories draws a useful distinction. As the authors point out, the parameterisation of their nonparametric approach corresponds to a linear dynamical system which has close connections to RNNs.
Review Point: - In Figure 3, its not clear to me that these neural predictions are uniquely predicted by their normative account. Would an alternative agent, similarly defined in the RL setting but without history compression predict the same neural responses illustrated in Fig 3? It seems as though Figure 3 shows the same diagram for the FDT task whether or not there is policy compression.
==================================================

Focused review:

weaknesses:
1.) Lack of computational cost analysis. According to the implementational details, the overall training process has two steps, which is likely to increase the computational burden. To this end, the authors are suggested to conduct some analysis on this issue.
2.) The comparison experiments with some recent and important methods are missing. For example, the following three papers also focus on Auto-ML based semantic segmentation:
[1] FasterSeg: Searching for Faster Real-time Semantic Segmentation, ICLR 2020;
[2] Graph-guided Architecture Search for Real-time Semantic Segmentation, CVPR 2020;
[3] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation, CVPR 2019.
As the Auto Seg-Loss is particularly designed for Auto-ML based semantic segmentation, the comparison with the related methods is required.
In addition, there are some other recent semantic segmentation methods (proposed in 2019 and 2020) focusing on the PASCAL VOC and Cityscapes datasets, although they are not related to Auto-ML. As an effective semantic segmentation method, the overall segmentation performance of the model is supposed to be competitive. To this end, the authors are also suggested to make comparisons with these SOTA semantic segmentation methods.
3.) The writing of the abstract needs to be improved. In the current manuscript, the abstract is too brief. To better attract the interests of the readers, the authors can first introduce the background of the problem studied in this paper. Next, some detailed motivations can also be included.

Review Point: 1.) Lack of computational cost analysis. According to the implementational details, the overall training process has two steps, which is likely to increase the computational burden. To this end, the authors are suggested to conduct some analysis on this issue.
Review Point: 2.) The comparison experiments with some recent and important methods are missing. For example, the following three papers also focus on Auto-ML based semantic segmentation: [1] FasterSeg: Searching for Faster Real-time Semantic Segmentation, ICLR 2020; [2] Graph-guided Architecture Search for Real-time Semantic Segmentation, CVPR 2020; [3] Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation, CVPR 2019. As the Auto Seg-Loss is particularly designed for Auto-ML based semantic segmentation, the comparison with the related methods is required. In addition, there are some other recent semantic segmentation methods (proposed in 2019 and 2020) focusing on the PASCAL VOC and Cityscapes datasets, although they are not related to Auto-ML. As an effective semantic segmentation method, the overall segmentation performance of the model is supposed to be competitive. To this end, the authors are also suggested to make comparisons with these SOTA semantic segmentation methods.
Review Point: 3.) The writing of the abstract needs to be improved. In the current manuscript, the abstract is too brief. To better attract the interests of the readers, the authors can first introduce the background of the problem studied in this paper. Next, some detailed motivations can also be included.
==================================================

Focused review:

- The paper could have done a better job of motivating the problem. It is true that the lower order bits in a floating point representation might leak information, but it's unclear how many of these bits are actually revealed to any practical adversary. Even so: why doesn't a simple randomized rounding at the end of a group of computations work? Is it because the difference accumulates? For an iterative method, how does the difference between a perfect and an imperfect implementation accumulate with rounds? The paper would be a lot stronger if these points were clarified. - The proofs while highly technical and nice do not involve a lot of novel ideas. But overall this is a minor weakness I think.

Review Point: - The paper could have done a better job of motivating the problem. It is true that the lower order bits in a floating point representation might leak information, but it's unclear how many of these bits are actually revealed to any practical adversary. Even so: why doesn't a simple randomized rounding at the end of a group of computations work? Is it because the difference accumulates? For an iterative method, how does the difference between a perfect and an imperfect implementation accumulate with rounds? The paper would be a lot stronger if these points were clarified.
Review Point: - The proofs while highly technical and nice do not involve a lot of novel ideas. But overall this is a minor weakness I think.
==================================================

Focused review:

1. It appears that the non-stationarity considered in this paper is governed by a underlying linear model, and seems like the phi(i)'s are known ahead of time (please correct me if I am wrong). 2. Also, seems like [8] is a very closely related setting, and the solutions are pretty related. It would be helpful if the authors can explain the key differences in techniques.

Review Point: 1. It appears that the non-stationarity considered in this paper is governed by a underlying linear model, and seems like the phi(i)'s are known ahead of time (please correct me if I am wrong).
Review Point: 2. Also, seems like [8] is a very closely related setting, and the solutions are pretty related. It would be helpful if the authors can explain the key differences in techniques.
==================================================

Focused review:

My main concern is that they haven't quite demonstrated enough to validate the claim that these are demonstrating a causal role for syntactic knowledge. Two crticisms in particular: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion. 2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to "cheat" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure.
Finally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information.
Really, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. 
I got garden pathed reading line 331 "One could define other Z1 and Z2..." The author seems to really like the bigram "prior art." 

Review Point: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion.
Review Point: 2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to "cheat" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure. Finally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information. Really, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. I got garden pathed reading line 331 "One could define other Z1 and Z2..." The author seems to really like the bigram "prior art."
==================================================

Focused review:

Weaknesses: - As mentioned in section 3.2, MetaAnchor does not show significant improvement for two-stage anchor-based object detection. - The experiment evaluation is only done for one method and on one dataset. It is not very convincing that MetaAnchor is able to work with most of the anchor-based object detection system. Rebuttal Response: Overall, I think this approach has value on one-stage frameworks. The experiment is only on COCO, but performed in an extensive way. It would be great to have more consistently good results on another benchmark dataset such as OpenImage. But for this submission now, I am willing the upscale my score from 4 to 6. 

Review Point: - As mentioned in section 3.2, MetaAnchor does not show significant improvement for two-stage anchor-based object detection.
Review Point: - The experiment evaluation is only done for one method and on one dataset. It is not very convincing that MetaAnchor is able to work with most of the anchor-based object detection system. Rebuttal Response: Overall, I think this approach has value on one-stage frameworks. The experiment is only on COCO, but performed in an extensive way. It would be great to have more consistently good results on another benchmark dataset such as OpenImage. But for this submission now, I am willing the upscale my score from 4 to 6.
==================================================

Focused review:

1. It is prefer to empirically compare proposed algorithms with existing second-order methods with line-search. 2. Line 51 states aggregating Newton Method has global convergence of O(1/k^(1+\nu)), but Theorem 4 shows the rate is O(1/k^(2+\nu)).

Review Point: 1. It is prefer to empirically compare proposed algorithms with existing second-order methods with line-search.
Review Point: 2. Line 51 states aggregating Newton Method has global convergence of O(1/k^(1+\nu)), but Theorem 4 shows the rate is O(1/k^(2+\nu)).
==================================================

Focused review:

1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
 2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. 
1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points. 
 2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts.
In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work. 

Review Point: 1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points.
Review Point: 2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
Review Point: 3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts. In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work.
Review Point: 1. It seems that the hypothesis needs to consider a compact space for continuous prompts in order to build an approximate mapping to the discrete prompt space, as there is only a finite number of discrete prompts. Any point in an unbounded space seems hard to be mapped with a certain accuracy into a discrete space with a finite number of points.
Review Point: 2. The authors design an objective to build up a trade-off between the task accuracy and the prompt F1. According to the experimental results, it is possible to achieve ≥ 94% prompt F1 when projecting the soft prompt to task-unrelated instructions with under 2% drop in accuracy. The authors want to use these results to prove that there is little correspondence between continuous prompts and their interpretation. However, methods based on soft prompt were designed with these factors in mind when it was first introduced. The soft prompt has nothing to do with the natural language that humans are accustomed to and can exist in any form. The soft prompt is only a sequence of ‘abstract pseudo tokens’ that can be understood by machines to assist the training of language models.
Review Point: 3. The authors find that project to “true” target prompts is no more effective at solving the tasks. This is not a new discovery either. OPTIPROMPT(Zhong et al. 2021) proposed a method that initializes virtual tokens based on discovered discrete prompts, then fine-tunes the embeddings to increase task accuracy. The resulting accuracy greatly surpasses the that based on so-called “true” discrete prompts, this previous result already shows that the so-called “true” target prompt is not effective compared with the soft prompts. In a summary, the authors have done a lot of exploration and experimentation, as well as using experiments to prove many of their conjectures, yet many of these conjectures can be directly summarized by previous work.
==================================================

Focused review:

Weaknesses
1. The paper is not well organized and hard to follow.
For example, the abstract is too concise to include necessary information regarding the background and motivation of the problem that the paper is going to solve and the introduction of the proposed method. The introduction lacks a clear and coherent line to follow. I am interested in seeing 1) the typical task of theorem proving and proof search; 2) the limitations of existing methods in theorem proving; 3) the introduction of expert iteration with an illustration of the target task; 4) the design of the proposed method in this paper; 5) the designs of the experiments and the main results. I am not very comfortable with the organization of the related work section as most of the content is put in the appendix. In the methodology section, it would be better to give an example of the miniF2F benchmark and an illustration of the Lean environment. I am struggling to understand the task, the dataset, and the environment that the work is working on.
2. The experiments are not extensive enough.
Most of the experiments are conducted on the miniF2F dataset, which consists of 244 validation and 244 test formalized statements of mathematical problems. However, miniF2F is limited to a small data scale, making the results not solid enough. Also, the paper fails to compare more baselines or search strategies in the experiments.
3. The writing could be improved.
It would be nice to provide a reference when mentioning some work for the first time. For example, the paper misses the reference when mentioning Go on page 2 and misses the reference for Lean in the related work section. There are some typos in the paper. For instance, "Proof datasets extraction". The statement "These two differences make a naive application of reinforcement learning to formal mathematics unlikely to succeed." lacks the necessary supporting facts in the paper.

Review Point: 1. The paper is not well organized and hard to follow. For example, the abstract is too concise to include necessary information regarding the background and motivation of the problem that the paper is going to solve and the introduction of the proposed method. The introduction lacks a clear and coherent line to follow. I am interested in seeing 1) the typical task of theorem proving and proof search;
Review Point: 3) the introduction of expert iteration with an illustration of the target task;
Review Point: 4) the design of the proposed method in this paper;
Review Point: 5) the designs of the experiments and the main results. I am not very comfortable with the organization of the related work section as most of the content is put in the appendix. In the methodology section, it would be better to give an example of the miniF2F benchmark and an illustration of the Lean environment. I am struggling to understand the task, the dataset, and the environment that the work is working on.
Review Point: 2. The experiments are not extensive enough. Most of the experiments are conducted on the miniF2F dataset, which consists of 244 validation and 244 test formalized statements of mathematical problems. However, miniF2F is limited to a small data scale, making the results not solid enough. Also, the paper fails to compare more baselines or search strategies in the experiments.
==================================================

Focused review:

1. The novelty is somewhat limited. The motivation and the model structure share similar spirits with Hierarchically Structured Meta-learning (HSML), which is published on ICML’19. More thorough discussions are needed to demonstrate how the contribution of this paper differs from HSML. 2. The baselines are not complete. The proposed method is related to both online meta-learning and structured meta-learning, but the authors only provide online meta-learning baselines, ignoring structured meta-learning models (i.e., Hierarchically Structured Meta-learning). 3. Though this paper provided some analyses about the constructed meta-hierarchical tree, more detailed illustrations are recommended for better understanding and explanation.

Review Point: 1. The novelty is somewhat limited. The motivation and the model structure share similar spirits with Hierarchically Structured Meta-learning (HSML), which is published on ICML’19. More thorough discussions are needed to demonstrate how the contribution of this paper differs from HSML.
Review Point: 2. The baselines are not complete. The proposed method is related to both online meta-learning and structured meta-learning, but the authors only provide online meta-learning baselines, ignoring structured meta-learning models (i.e., Hierarchically Structured Meta-learning).
Review Point: 3. Though this paper provided some analyses about the constructed meta-hierarchical tree, more detailed illustrations are recommended for better understanding and explanation.
==================================================

Focused review:

UPDATE: I appreciate the author's clarification on the comparison with local DP, the notations \phi, \mu, and the additional experiments on examining the effect of loss balancing factor. As an additional suggestion, it would be better if authors can include more related work on the topic of learning robust representation, such as "Zhu, Sicheng, Xiao Zhang, and David Evans. Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization", "Garg, Shivam, et al. A spectral view of adversarially robust features.", "Pensia, Ankit, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a robust information bottleneck" and so on. While I believe these studies are different from this paper, I think this topic is highly relevant to the topic of learning fair representation. 1. The performance highly depends on the choice of classifiers/parameters (e.g., loss balancing factors) and datasets, neither theoretical guarantee nor the guidance on the selection of classifiers/parameters is provided, which is one of the main reasons that limit my score. The performance is only compared with the BASE case. I wonder if authors can compare (at least empirically) with other fair representation learning methods. 2. The individual fairness studied in the paper — treating similar people similarly — is highly related to differential privacy and robustness ML. In essence, all are trying to guarantee that any small perturbation of training data cannot change the output significantly. Their relations have been studied extensively in the literature. It is thus NOT surprising to see the use of robust learning in providing certificates for individual fairness. Moreover, most of the methods in the paper are adopted from the existing work (e.g., the use of logical constraints, the formulation of min-max optimization, etc.). I am concerned that the work doesn’t have significant novelty and contribution. It would be helpful if authors can explain how their framework differs fundamentally from the existing work, especially those in the domain of robust and (local) differential privacy. 3. When the fairness constraint is imposed in machine learning, typically there should be a tradeoff between accuracy and fairness. I believe there is also such a tradeoff in the proposed framework, which is an important criterion but has not been discussed in the paper. However, as shown in the experiments, both accuracy and fairness can be improved simultaneously in many cases (e.g., HEALTH dataset). It seems that fairness can be attained "for free" without losing accuracy. I wonder if authors can explain why this can happen. I suggest authors conducting more experiments to examine the impact of loss balancing factor on the performance, which I believe plays a critical role in balancing fairness-accuracy tradeoff.

Review Point: 1. The performance highly depends on the choice of classifiers/parameters (e.g., loss balancing factors) and datasets, neither theoretical guarantee nor the guidance on the selection of classifiers/parameters is provided, which is one of the main reasons that limit my score. The performance is only compared with the BASE case. I wonder if authors can compare (at least empirically) with other fair representation learning methods.
Review Point: 2. The individual fairness studied in the paper — treating similar people similarly — is highly related to differential privacy and robustness ML. In essence, all are trying to guarantee that any small perturbation of training data cannot change the output significantly. Their relations have been studied extensively in the literature. It is thus NOT surprising to see the use of robust learning in providing certificates for individual fairness. Moreover, most of the methods in the paper are adopted from the existing work (e.g., the use of logical constraints, the formulation of min-max optimization, etc.). I am concerned that the work doesn’t have significant novelty and contribution. It would be helpful if authors can explain how their framework differs fundamentally from the existing work, especially those in the domain of robust and (local) differential privacy.
Review Point: 3. When the fairness constraint is imposed in machine learning, typically there should be a tradeoff between accuracy and fairness. I believe there is also such a tradeoff in the proposed framework, which is an important criterion but has not been discussed in the paper. However, as shown in the experiments, both accuracy and fairness can be improved simultaneously in many cases (e.g., HEALTH dataset). It seems that fairness can be attained "for free" without losing accuracy. I wonder if authors can explain why this can happen. I suggest authors conducting more experiments to examine the impact of loss balancing factor on the performance, which I believe plays a critical role in balancing fairness-accuracy tradeoff.
==================================================

Focused review:

Despite the strengths mentioned above the derivation of the Bayesian Filtering framework is not rigorous and is based off of a number unjustified steps. Starting from the setttings of stochastic optimization and Bayesian filtering, multiple reduction steps which include un-realistic assumptions, weaken the connection between the initial Bayesian filtering framework and the derived AdaBayes optimizer. 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense? 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion. 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this. 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the "desired" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues: - "philosophical note" paragraph seems a bit digressive. - Line 109 failed to use superscript? - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.

Review Point: 1.The few sentences in 77-81 are non-rigorous and not well justified. Why should the factorized model of the parameters make sense?
Review Point: 2. The argument that the mini-batch gradients noise follows a normal distribution is a topic of recent research and discussion.
Review Point: 3. In equation 12, the updates on the weights are confusing, why would the parameters of the network be updated according to a constant multiple of their current value? This does not seem to reflect of gradient optimization. Even if sigma is time-varying, I am having a hard time wrapping my head around this.
Review Point: 3. The simplification replacing the Hessian by the squared gradient is non-trivial, and seems to be the casue for the "desired" RMS style optimizer. Finally the introduction of lambda replacing eta/2sigma^2 additionally extends the gap between the resultant optimizer and what we would expect from the Bayesian filtering model. Minor issues:
Review Point: - Line 223 Needs proper definition of OU acronym. ___ After reviewing the rebuttal, the authors were able to address some of my concerns, At the same time I find some of the approximations to still not be well justified. I am maintaining my current score for now.
==================================================

Focused review:

- Lack of illustrative examples regarding the model outputs.
- In the evaluation, I'm missing a detailed error analysis. It would be very enlightening to add a manual error analysis to figure out the most prevalent mistakes of the different approaches. 
- l. 25/26: "... can benefit many downstream examples" List/Cite some examples.
- l. 40: "... previous studies have shown the benefit..." Citation?
- l. 419: stmodel udy -> model study - l. 567: We -> we 

Review Point: - In the evaluation, I'm missing a detailed error analysis. It would be very enlightening to add a manual error analysis to figure out the most prevalent mistakes of the different approaches.
Review Point: - l. 25/26: "... can benefit many downstream examples" List/Cite some examples.
Review Point: - l.40: "... previous studies have shown the benefit..." Citation?
Review Point: - l. 419: stmodel udy -> model study - l. 567: We -> we
==================================================

Focused review:

weakness (insight and contribution), my initial rating is borderline. Strengths:
+ The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.
+ The ablation studies and analysis in Section 4.4 is well organized and clearly written. It is easy to follow the analysis and figure our the contribution of each component. Also, Figure 2 is well designed and clear to illustrate the pipeline.
+ The experimental analysis is comprehensive. The analysis on computation time and inference speed is also provided. Weakness:
- (major concern) The contribution is somehow limited. The main contribution is applying optimal transport for few-shot adaptation of CLIP. After reading the paper, it is not clear enough to me why Optimal Transport is better than other distance. Especially, the insight behind the application of Optimal Transport is not clear. I would like to see more analysis and explanation on why Optimal Transport works well. Otherwise, it seems that this work is just an application work on a specific model and a specific task, which limits the contribution.
- The recent related work CoCoOp [1] is not compared in the experiments. Although it is a CVPR'22 work that is officially published after the NeurIPS deadline, as the extended version of CoOp, it is necessary to compare with CoCoOp in the experiments.
- In the approach method, there lacks a separate part or subsection to introduce the inference strategy, i.e., how to use the multiple prompts in the test stage.
- Table 2 mixed different ablation studies (number of prompts, visual feature map, constraint). It would be great if the table can be split into several tables according to the analyzed component.
- The visualization in Figure 4 is not clear. It is not easy to see the attention as it is transparent. References
[1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.
After reading the authors' response and the revised version, my concerns (especially the contribution of introducing the optimal transport distance for fine-tuning vision-language models) are well addressed and I am happy to increase my rating.

Review Point: + The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.
Review Point: + The ablation studies and analysis in Section 4.4 is well organized and clearly written. It is easy to follow the analysis and figure our the contribution of each component. Also, Figure 2 is well designed and clear to illustrate the pipeline.
Review Point: + The experimental analysis is comprehensive. The analysis on computation time and inference speed is also provided. Weakness:
Review Point: - (major concern) The contribution is somehow limited. The main contribution is applying optimal transport for few-shot adaptation of CLIP. After reading the paper, it is not clear enough to me why Optimal Transport is better than other distance. Especially, the insight behind the application of Optimal Transport is not clear. I would like to see more analysis and explanation on why Optimal Transport works well. Otherwise, it seems that this work is just an application work on a specific model and a specific task, which limits the contribution.
Review Point: - The recent related work CoCoOp [1] is not compared in the experiments. Although it is a CVPR'22 work that is officially published after the NeurIPS deadline, as the extended version of CoOp, it is necessary to compare with CoCoOp in the experiments.
Review Point: - In the approach method, there lacks a separate part or subsection to introduce the inference strategy, i.e., how to use the multiple prompts in the test stage.
Review Point: - Table 2 mixed different ablation studies (number of prompts, visual feature map, constraint). It would be great if the table can be split into several tables according to the analyzed component.
Review Point: - The visualization in Figure 4 is not clear. It is not easy to see the attention as it is transparent. References [1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022. After reading the authors' response and the revised version, my concerns (especially the contribution of introducing the optimal transport distance for fine-tuning vision-language models) are well addressed and I am happy to increase my rating.
==================================================

Focused review:

1. From a probability calibration view, this paper focuses only on the simplest setting of confidence calibration, which is known to be less suitable for the general multi-class setting. 2. The proposed approach is only based on the temperature scaling case and leaves many closely related approaches (e.g. vector scaling/matrix scaling ) untouched in both the method and experiments. 3. A minor point is this paper also only considers covariate shifts, but, understandably, it is probably the first step to solve issues related to domain adaption.

Review Point: 1. From a probability calibration view, this paper focuses only on the simplest setting of confidence calibration, which is known to be less suitable for the general multi-class setting.
Review Point: 2. The proposed approach is only based on the temperature scaling case and leaves many closely related approaches (e.g. vector scaling/matrix scaling ) untouched in both the method and experiments.
Review Point: 3. A minor point is this paper also only considers covariate shifts, but, understandably, it is probably the first step to solve issues related to domain adaption.
==================================================

Focused review:

Weaknesses:
Related work section is confusing in a few places and can be streamlined further. Examples:
The Camera detectors section contains a discussion of PointPillars, which is a purely Lidar method.
Range images are not really Euclidean space (see line 88)
89: "Recently, people start to exploit these two feature modalities to increase the representation power" --> There is earlier work to do this, if I understand correctly the statement. E.g. [5] from the paper, or End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds, by Yin Zhou et al, CoRL 2019.
90: " Another line of work is to exploit the benefit of the bird’s eye view plane similar to the camera perception" --> a lot of this work came before camera started exploiting the BEV view.
Intuitive explanations are lacking in a couple of instances:
Work does not explain the intuition why the model needs to be trained in two stages. What happens if it's trained in a single stage?
14: "Note that we do not conduct data augmentation when multi-view image input is involved, while data augmentation plays a critical part in other cutting edge methods." Is this a limitation of camera fusion methods in general or something specifically lacking in your case? Can you please clarify?
It is unclear whether the approach is SOTA on nuScenes or not. Can you please explicitly contrast your performance relative to the nuScenes leaderboard (at least for the published approaches). When exploring that leaderboard myself, I see mentions of a method called BEVFusion that is SOTA but seems to be a different method? Assuming that method is different and already on the leaderboard, your naming may be confusing / too generic.
nuScenes is a dataset with particularly poor lidar (compared to other public datasets, such as Waymo Open Dataset, Argoverse2.0 etc). Results on at least one more dataset with high quality and longer-range lidar are highly desirable. The core issue of missing lidar points may be a lot less pertinent for more modern lidars. Also, as range increases beyond ~40m to 70-200m, the approach here may actually underperform lidar-painting approaches, since BEV view can start containing errors > 10m in the camera case making fusion in BEV space difficult. To this effect, analysis of the method performance as a function of object distance, relative to SOTA fusion methods for long distances will help.
Language: There are minor language issues and typos in the paper, it would benefit from another proofreading pass.
See comment on weaknesses. Some core potential limitations of the existing method have not been fully explored.
My current rating is predicated on the assumption that a similar idea has not been published yet (not completely certain) and that I will receive reasonable responses to my questions.

Review Point: 90: " Another line of work is to exploit the benefit of the bird’s eye view plane similar to the camera perception" --> a lot of this work came before camera started exploiting the BEV view. Intuitive explanations are lacking in a couple of instances: Work does not explain the intuition why the model needs to be trained in two stages. What happens if it's trained in a single stage?
==================================================

Focused review:

Weaknesses:
The major weakness is clarity. The writing of the paper is confusing and probably prevents the reader from understanding the underlying technical merit. Here are some examples:
The paper needs to be more self-contained. While the approach is built upon TrajGRU, it is difficult for the reader (at least for me) to understand what exactly is ‘the neural unit of the top layer’, ‘horizontal’, ‘vertical’, ‘zigzag connection’, etc. without a reference. Simply from the language description, it is difficult to infer the intuition behind the design choice. A proper background section is required, at least, for people to fully appreciate the contribution of the paper.
The description of TrajGRU-Attention and TrajGRU-Attention-ODE suffers from the same issue. Some terms borrowed from previous works are confusing, e.g. left-right approach.
Certain expressions are also confusing, for example,
‘these latent continuous-time states are incorporated with the hidden states generated by the extended TrajGRU layer to get the work done more effectively', what does that mean by ‘get the work done’ here?
‘the learnable ODE function of the ODE solver module is not sufficient to capture practical features and reconstruct the resulting outcomes with high sharpness simultaneously’ What does practical feature mean here?
Minor points:
It is better to use consistent punctuation, for example, the full stop sign at the end of captions. 2 ‘Therefore, the TrajGRU-ODE block (see Figure 1) can simultaneously learn the sequence’s continuous-time and spatiotemporal dynamics simultaneously.’, ‘simultaneously’ is duplicated.

Review Point: 2 ‘Therefore, the TrajGRU-ODE block (see Figure 1) can simultaneously learn the sequence’s continuous-time and spatiotemporal dynamics simultaneously.’, ‘simultaneously’ is duplicated.
==================================================

Focused review:

1. The authors presented a fine-grained evaluation set in this paper. However, the anti-stereotype that appears in previous datasets is missing in the constructed dataset. In addition, details of annotations are missing in this paper. Since stereotype detection is quite challenging, it would be important to discuss how to guarantee the annotation quality and whether annotators can reach an agreement on collected corpus.
2. Missing related baselines. Only PLMs are considered in this paper and other task-related baselines are missing.
3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set? 
1. More discussions about dataset construction should be provided, e.g., the time range of data collection, preprocessing strategies and quality control.
2. All "table" and "figure" in the context should be capitalized. 

Review Point: 1. The authors presented a fine-grained evaluation set in this paper. However, the anti-stereotype that appears in previous datasets is missing in the constructed dataset. In addition, details of annotations are missing in this paper. Since stereotype detection is quite challenging, it would be important to discuss how to guarantee the annotation quality and whether annotators can reach an agreement on collected corpus.
Review Point: 2. Missing related baselines. Only PLMs are considered in this paper and other task-related baselines are missing.
Review Point: 3. Missing in-depth analysis on experimental results. For example, why the improvements of models are limited on offense detection dataset and are significant on coarse stereotype set?
Review Point: 1. More discussions about dataset construction should be provided, e.g., the time range of data collection, preprocessing strategies and quality control.
Review Point: 2. All "table" and "figure" in the context should be capitalized.
==================================================

Focused review:

Weaknesses:
As to Theorem 1, there is 1 − f ∗ ( x b a d + v ) − f ∗ ( x b a d ) < ϵ
. For a practical distribution μ ( x )
, if ϵ
is small (satisfying the definition of Trojan Twin), then x b a d
is usually located at low-probability regions. So consequently, μ ( B ( x b a d , ϵ / ( 4 β ) ) )
, i.e., the ratio of successful trojan ρ
would be quite small. Besides, practical model family F
usually has a large value of Lipschitz constant β
, especially for deep networks. 1 − f ( x + v ) − f ( x ) < ϵ
is not a good criterion for poisoning, for example, f ( x + v ) = f ( x ) = 0.5
corresponds to ϵ = 0
, but this case is not considered as a successful poisoning.
In Section 3.4, actually, I do not see any close connection between the concept of Trojan Twin and the proposed Algorithm 1. For me, Algorithm 1 is just constructing universal poisoning perturbation on an ensemble of adversarially trained models, which is nothing new beyond [1]+[2].
Why the poisoning trigger in Algorithm 1 is preferred to be universal, rather than input-dependent? Besides, do the baselines in experiments exploit the same knowledge from adversarially trained models as done in the proposed method? Refs:
[1] Universal Adversarial Perturbations. CVPR 2017
[2] Ensemble Adversarial Training: Attacks and Defenses. ICLR 2018

Review Point: 1 − f ( x + v ) − f ( x ) < ϵ is not a good criterion for poisoning, for example, f ( x + v ) = f ( x ) = 0.5 corresponds to ϵ = 0 , but this case is not considered as a successful poisoning. In Section 3.4, actually, I do not see any close connection between the concept of Trojan Twin and the proposed Algorithm 1. For me, Algorithm 1 is just constructing universal poisoning perturbation on an ensemble of adversarially trained models, which is nothing new beyond [1]+[2]. Why the poisoning trigger in Algorithm 1 is preferred to be universal, rather than input-dependent? Besides, do the baselines in experiments exploit the same knowledge from adversarially trained models as done in the proposed method? Refs: [1] Universal Adversarial Perturbations. CVPR 2017 [2] Ensemble Adversarial Training: Attacks and Defenses. ICLR 2018
==================================================

Focused review:

1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.
2. Figure 2 does not show the time complexity of SimCSE_{CLS} method.
3. I am confused about the definition of "\vec \mathbf{1}" in Equation(1). 
Missing citation: BERTScore: Evaluating Text Generation with BERT (Zhang et al. 2020) For other suggestions, please refer to the weakness section. 

Review Point: 1. The method in this paper is quite similar to BERTScore, but the authors have not cited that paper.
Review Point: 2. Figure 2 does not show the time complexity of SimCSE_{CLS} method.
Review Point: 3. I am confused about the definition of "\vec \mathbf{1}" in Equation(1). Missing citation: BERTScore: Evaluating Text Generation with BERT (Zhang et al. 2020) For other suggestions, please refer to the weakness section.
==================================================

Focused review:

Weakness:
The gradient descent in this paper is actually a modified version. While the authors highlight this and mention "Without our modification, the statistical rate will become sub-optimal in some cases", it will be great if the results with the original gradient descent could be provided to facilitate the understanding of the gap.
There are some typos and mismatches of symbols (part of them listed in "limitations"). This paper will benefit from checking the mathematical presentations throughout.
Overall, I am in a positive position for this paper. The paper can be further improved by taking the above suggestions.
Please refer to the "weakness" part. Here I list the typos:
Line 100: e j → e i
Line 106: k → K , K x → K z
Line 117: k → K
Line 120: σ → Σ
Line 195-197: All u
in the inner product should be t h e t a
or all θ
in the inner product should be u
The first case in Theorem 3.2: it should be β < . . .
Line 661-663: do we need the transpose symbol ⊤ here?
Line 671: I guess the q ^ λ
here is the same as the g ^ λ
in Line 718?
Line 712: Σ I d , A 1
is not defined.

Review Point: .. Line 661-663: do we need the transpose symbol ⊤ here? Line 671: I guess the q ^ λ here is the same as the g ^ λ in Line 718? Line 712: Σ I d , A 1 is not defined.
==================================================

Focused review:

Weaknesses
The proposed method was shown effective to only a limited class of segmentation noises, i.e., over/under-coloring. Although such noises may ubiquitously exist, they may not comprise the most critical kind of noises. For instance, instance-level missing or mis-categorization could be much more harmful than over/under-coloring. The proposed method cannot properly handle such noises (because it wasn't designed to do so) as can be seen in Fig. 7.
The design of the SegTHOR experiment with synthetic noise is unrealistic and inadequate.
The synthetic noises were injected only to the training set, not to the validation set. In practice, it is highly likely that annotation noises exist in both training and validation sets. In that case, adaptive correction of annotations causes the training and validation annotations to diverge, becoming a source of confusion in practice. How is the proposed method supposed to handle it?
The synthetic noises were injected to all categories. What if there exists a systemtatic bias that some categories are noisy and others are noise-free? How does the proposed method affect the performance of noise-free categories?
In the experiments, what kind of data augmentation was used? Extensive data augmentation may resist the noise memorization, so the efficacy of the proposed method should be measured against the different levels of data augmentation.
The proposed method is simple and general, but it is unclear how it can be further advanced beyond simply picking low-hanging fruits. Please discuss it more specifically.
Minor comments
It is unclear from the text how exactly the annotation correction was done. Was the model's output binarized or not when replacing the annotation?
When applying the multi-scale consistency regularization term L Multiscale ( x )
, it is said "the term is only applied to the input x
where the maximum entropy of q ( x )
is above a threshold ρ
." Please define and/or explain what the "maximum entropy" is.
"ADELE-M" appears several times without any definition nor explanation.

Review Point: 7. The design of the SegTHOR experiment with synthetic noise is unrealistic and inadequate. The synthetic noises were injected only to the training set, not to the validation set. In practice, it is highly likely that annotation noises exist in both training and validation sets. In that case, adaptive correction of annotations causes the training and validation annotations to diverge, becoming a source of confusion in practice. How is the proposed method supposed to handle it? The synthetic noises were injected to all categories. What if there exists a systemtatic bias that some categories are noisy and others are noise-free? How does the proposed method affect the performance of noise-free categories? In the experiments, what kind of data augmentation was used? Extensive data augmentation may resist the noise memorization, so the efficacy of the proposed method should be measured against the different levels of data augmentation. The proposed method is simple and general, but it is unclear how it can be further advanced beyond simply picking low-hanging fruits. Please discuss it more specifically. Minor comments It is unclear from the text how exactly the annotation correction was done. Was the model's output binarized or not when replacing the annotation? When applying the multi-scale consistency regularization term L Multiscale ( x ) , it is said "the term is only applied to the input x where the maximum entropy of q ( x ) is above a threshold ρ ." Please define and/or explain what the "maximum entropy" is. "ADELE-M" appears several times without any definition nor explanation.
==================================================

Focused review:

- I think comparisons to prior work on generative lossy image compression (not only to prior work on hierarchical stacks of VQ-VAEs) are missing. In particular, a comparison to ref [39] (for which online code is available) could be interesting. - This paper builds on prior work which proposed similar concepts. The authors indeed discuss these works and the added contribution well, yet this somewhat limits the novelty.

Review Point: - I think comparisons to prior work on generative lossy image compression (not only to prior work on hierarchical stacks of VQ-VAEs) are missing. In particular, a comparison to ref [39] (for which online code is available) could be interesting.
Review Point: - This paper builds on prior work which proposed similar concepts. The authors indeed discuss these works and the added contribution well, yet this somewhat limits the novelty.
==================================================

Focused review:

Weaknesses:
Conceptually, the adopted high-level computer vision pretext tasks seem in the opposite positions w.r.t. the BVQA task. More specifically, for a particular high-level computer vision algorithm, we want it to work robustly under various kinds of visual distortions. That is, the learned feature representations should be distortion-insensitive. This is not the case for BVQA, in which we would like to learn distortion-sensitive features for quality assessment. However, this is not observed in Fig. 2. Is it to say the current BVQA method leverages the imperfections of the adopted computer vision methods?
The intra-consistency constraint is a little counterintuitive. Are we supposed to use a diverse set of feature representations to achieve robust BVQA? If one feature representation is harmful, the method will learn to ignore it with a close-to-zero w i
. In addition, the degree of intra-consistency relies heavily on the adopted transformation for channel alignment. If the transformation has enough model capacity, feature collapse may occur.
The inter-divisibility constraint also seems a little ad-hoc, as it requires quantizing the quality variable into a set of discrete bins.
No validation set is found in the experimental setup, which may have a risk of overfitting.
The authors may consider training and testing their method on LSVQ and LSVQ 1080p. The authors are also encouraged to provide cross-dataset performance as a form of generalization test.
The authors may want to conduct experiments on the inference time of the proposed method, and discuss ways to reduce it.

Review Point: 2. Is it to say the current BVQA method leverages the imperfections of the adopted computer vision methods? The intra-consistency constraint is a little counterintuitive. Are we supposed to use a diverse set of feature representations to achieve robust BVQA? If one feature representation is harmful, the method will learn to ignore it with a close-to-zero w i . In addition, the degree of intra-consistency relies heavily on the adopted transformation for channel alignment. If the transformation has enough model capacity, feature collapse may occur. The inter-divisibility constraint also seems a little ad-hoc, as it requires quantizing the quality variable into a set of discrete bins. No validation set is found in the experimental setup, which may have a risk of overfitting. The authors may consider training and testing their method on LSVQ and LSVQ 1080p. The authors are also encouraged to provide cross-dataset performance as a form of generalization test. The authors may want to conduct experiments on the inference time of the proposed method, and discuss ways to reduce it.
==================================================

Focused review:

The main weakness of this paper is the motivation, which is extremely  imprecise. The pedagogical literature cited in lines 36-46 is opaquely  summarized and does not lend sufficient grounding for introducing the  problem: "mathematics is about patterns and not merely about numbers";  "...students explore patterns, not just memorize procedures". The authors  distill these insights into their hypothesis, where they posit that  "semantics affects problem-solving". The word "semantics" is used  extremely liberally here, and it is was unclear to me what the authors  actually meant here (vs. prototype equations). The analysis and discussion  offered by Section 3 is difficult to follow because of this, and it is not  clear how instances "similar semantics" are tracked or computed (see, e.g.  Figure 3). Upon several re-readings, it became apparent that what the authors  actually refer to is lexical overlap (at least as evidenced by Probs C and D  in Figure 1). Due to this, the representation clusters for each prototype  equation tend to be expanded by the problems with high lexical overlap across  classes. This is more or less straightforward, but more care needs to be  devoted to making the language precise and less lofty. 
Comments:  * When investigating cosine similarity, authors should consider the work that  investigates MLM embedding spaces, namely  https://aclanthology.org/D19-1006.pdf and  https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the  anisotropy phenomenon.   * It would be interesting to see how the embedding spaces look like for BERT   out of the box.
  * It is important to make distinction between `procedures' and 'patterns'  Questions:  * What layer are the plots in the top row in Figure 2 from? Conversely, what   epoch are the plots in the bottom from?
  * Relevant to above point: how are "similar semantics" calculated? What is   meant by `semantics'? This sort of language not useful. ( lines 216-20)  * Lines 250-52: what does this mean?   Style/typos:  * Line 46 - hypothesize instead of `think'  * Line 55 - citation for encoder/decoder  * Line 155 - representations 

Review Point: * When investigating cosine similarity, authors should consider the work that investigates MLM embedding spaces, namely https://aclanthology.org/D19-1006.pdf and https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the anisotropy phenomenon.
Review Point: * It would be interesting to see how the embedding spaces look like for BERT out of the box.
Review Point: * It is important to make distinction between `procedures' and 'patterns' Questions:
Review Point: * What layer are the plots in the top row in Figure 2 from? Conversely, what epoch are the plots in the bottom from?
Review Point: * Line 46 - hypothesize instead of `think' * Line 55 - citation for encoder/decoder * Line 155 - representations
==================================================

Focused review:

Weaknesses:
The motivation is not well-supported. This paper focus on the lexicographic multi-objective problems that can not be solved by Lexicographic Q-Learning (TLQ), and two types of maze example are analyzed. Each of them will be discussed below.
Problems with Reachability Constraint (Fig.1). The problem can be solved by TQL, but the problem itself needs to be redefined. The problem should contain a third objective time cost. The primary objective (Reach G) does not consider the time spent, i.e. no discounting situation, so the algorithm converges to a random policy (described in the paper as "all actions would have the value τ 1
"). If the time factor is not taken into account, the random policy is right, but it's not an ideal policy. The discount factor is a mathematical trick to make an infinite sum finite, and it also chooses a suitable planning horizon. So discount factor can be viewed as a trade-off of task rewards and time costs, which is also necessary in reality, because no one can do something without considering the time. Then, the secondary objective (avoid bad tiles) will lead to another stochastic policy that does not step on tiles but may move back and forth. The third objective (time cost) can be formalized as a time penalty or discount factor, leading to the deterministic ideal policy.
Problems with Non-reachability Constraint (Fig.3). This problem (or just the example in Fig.3.) can be solved by TQL. The primary objective (minimizing the cost of tiles) can converge to the unique path (including (0-3,0), (3,1), (0-3,2), (0,3), (0-1,3) ), but there may be backtracking (like Left in (1,0) with previous step Right in (0,0)). And the secondary objective (minimizing the time) then allows the agent to reach the goal without backtracking, i.e., in 11 steps.
If the number of objectives is greater than or equal to 3, which means that "return None" of Algorithm 2 may occur, no theoretical proof of the convergence or experimental verification is given in this paper.
Here are some minor typographical errors and suggestions.
Spelling error on page 4, line 9. "iff" → "if"
The full name of LMDP should be given before using LMDP in Sec. 1.
The description of Fig. 1 uses H
s and h
s indicate tiles, while H H
s and h h
s are in the description of Fig. 3.

Review Point: 1. The description of Fig. 1 uses H s and h s indicate tiles, while H H s and h h s are in the description of Fig.
==================================================

Focused review:

weakness of the paper are the empirical evaluation which lacks some rigor, and the presentation thereof: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what "error"?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as "sub-standard".  - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics.  - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results.  Another remark: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly.   Minor: *  

Review Point: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what "error"?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as "sub-standard".
Review Point: - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics.
Review Point: - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark:
Review Point: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor:
==================================================

Focused review:

Significance: I am not convinced that the considered setting is relevant: * in the "natural" setting (as in the example of a face detector trained primarily based on light-skinned faces P and not on dark-skinned faces Q), providing unlabeled samples from Q requires (a) the developer being aware that P is not representative (b) being able to collect samples from Q and (c) not label these samples from Q. This seems implausible to me. Particularly, it would be unethical to apply selective classification for reducing labeling cost by skipping step c (as discussed by the authors also in the broader impact section). * in the presence of an adversary, if one assumes that the adversary has access to the classifier h (white-box), then it would also make sense to assume the adversary having access to the set S of non-abstained inputs and select Q_adv from within this set. This would result in the typical arms race, where S would need to be adapted to an adversary that itself chooses Q_adv depending on S. Assuming a fixed adversary unaware of S seems of little relevance. Empirical evaluation: * The empirical evaluation is conducted on a very much simplified version of the proposed algorithms, which consists of "training a predictor h^{Dis} to distinguish between examples from P and Q, and train a classifier h on P". I would consider this rather as a baseline (because it is the naive thing to do in a setting where fixed test examples from distribtion Q are given). I would expect the authors to shows in the empirical evaluation how the proposed algorithms perform relative to this baseline, that is Urejectron with T>1 and actually using a threshold \Lambda. * The way Q_adv is constructed is not a challenging setting because it consists of just very many (3,000) imperceptible modifications of a few examples. These can of course very easily be identified and be abstained. Evaluation on more challengingsettings would be desirable, for instance: create a set of misclassified examples and create Q_adv such that it consists of convex combinations of pairs of these misclassified examples.

Review Point: * in the "natural" setting (as in the example of a face detector trained primarily based on light-skinned faces P and not on dark-skinned faces Q), providing unlabeled samples from Q requires (a) the developer being aware that P is not representative (b) being able to collect samples from Q and (c) not label these samples from Q. This seems implausible to me. Particularly, it would be unethical to apply selective classification for reducing labeling cost by skipping step c (as discussed by the authors also in the broader impact section).
Review Point: * in the presence of an adversary, if one assumes that the adversary has access to the classifier h (white-box), then it would also make sense to assume the adversary having access to the set S of non-abstained inputs and select Q_adv from within this set. This would result in the typical arms race, where S would need to be adapted to an adversary that itself chooses Q_adv depending on S. Assuming a fixed adversary unaware of S seems of little relevance. Empirical evaluation:
Review Point: * The empirical evaluation is conducted on a very much simplified version of the proposed algorithms, which consists of "training a predictor h^{Dis} to distinguish between examples from P and Q, and train a classifier h on P". I would consider this rather as a baseline (because it is the naive thing to do in a setting where fixed test examples from distribtion Q are given). I would expect the authors to shows in the empirical evaluation how the proposed algorithms perform relative to this baseline, that is Urejectron with T>1 and actually using a threshold \Lambda.
Review Point: * The way Q_adv is constructed is not a challenging setting because it consists of just very many (3,000) imperceptible modifications of a few examples. These can of course very easily be identified and be abstained. Evaluation on more challengingsettings would be desirable, for instance: create a set of misclassified examples and create Q_adv such that it consists of convex combinations of pairs of these misclassified examples.
==================================================

Focused review:

1.It seems that they only evaluate their algorithm on a single map of ViZDoom and SafetyGym. Lacking of abundant experimental evaluations makes their method less convincing. 2.When memory graphs or tasks gets more complex, it's impossible to use an optimal search method to plan on the graph. The proposed algorithm may have its limitation when extended to more challenging tasks.

Review Point: 1.It seems that they only evaluate their algorithm on a single map of ViZDoom and SafetyGym. Lacking of abundant experimental evaluations makes their method less convincing.
Review Point: 2.When memory graphs or tasks gets more complex, it's impossible to use an optimal search method to plan on the graph. The proposed algorithm may have its limitation when extended to more challenging tasks.
==================================================

Focused review:

Weaknesses
1. The complexity analysis is insufficient. In the draft, the author only provide the rough overall complexity. A better way is to show the comparison between the proposed method and some other methods, including the number of model parameter and network forwarding time.
2. In the converting of point cloud to concentric spherical signal, the Gaussian radial basis function is adopted to summarize the contribution of points. Is there any other function that can accomplish this job? The reviewer would like to the discussion about this.
3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.
4. There are some typos in the draft. The first is the wrong use of "intra-sphere" and "inter-sphere". The second is the use of two consecutive "stacking" in the Spherical Discretization subsection. Please check the full text carefully.
5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.

Review Point: 1. The complexity analysis is insufficient. In the draft, the author only provide the rough overall complexity. A better way is to show the comparison between the proposed method and some other methods, including the number of model parameter and network forwarding time.
Review Point: 2. In the converting of point cloud to concentric spherical signal, the Gaussian radial basis function is adopted to summarize the contribution of points. Is there any other function that can accomplish this job? The reviewer would like to the discussion about this.
Review Point: 3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.
Review Point: 5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.
==================================================

Focused review:

1. While in the "Strengths" I remarked that regret bounds essentially match those for the scalar reward setting, I feel that there is still some gap in the bound. In Theorem 4.1, the consumption regret upper bound of "Ld · CONSREG" has a rather loose dependence on d, as in it does not match the dependence of \| 1_d \| in (Agrawal and Devanur 2011). Here, I denote 1_d as the d-dimensional all 1 vector, and g is L Lipschitz continuous with respect to the norm \|\cdot\|. 2. In the main results, the authors show that the constraint regrets are bounded in expectation. Do these bound translate to a high probability bound, in the same way as (Badanidiyuru 2013, Agrawal and Devanur 2014, Cheung 2019)? 3. Another place that needs substantial improvement is the numerical experiments. While the authors have provided additional details for the numerical experiments in Appendix E, there are quite a few places that require clarification: - How does the plots in Figure 1 corroborate with the regret bounds? More precisely, it is not clear how a trajectory means in the online model. For example, if I look at the top left plot for RCPO, it reports that at No. of trajectories = 500, the reward is \approx 1.65. What does it mean? Does it mean that if I run the RCPO with 500 episodes than the empirical average reward realizes as 1.65, or does it mean something else? - Can the authors provide plots about the cumulative regret of the algorithms (at least for the proposed algorithms, in case they don't make sense for existing algorithms like RCPO for some reason)? This provide a more direct way to empirically evaluate the proposed algorithms. - In relation to the previous point, in footnote 7, it remarks that the bottom row corresponds to "the aggregate actual constraint incurred during training". However, in an online model, how is the notion of "during training" defined? - It is not clear why the authors include A2C, which requires the access to the latent model on p as shown in the Appendix (so it seems to violate the model assumption of not knowing p for example?) - I am in fact quite confused by column C. Is there any reason why TFW-UCRL2 is run with significantly more trajectories than the other two algorithms ConRL-A2C and ConRL-Value Iteration? - When I tried to dig deeper in Appendix E, it is stated that “TFW-UCRL2 gives fixed weights to reward and constraint violation and maximizes a scalar function. Therefore we tuned TFW-UCRL2 for different weights and the reported result is for the best weight.” Nevertheless, to my knowledge, the TFW-UCRL2 in fact assign dynamic weights () to the penalty function for the constraints, and those weights do not need tuning in the sense that the dynamic update of the weights Can the authors provide a high level sketch on how they implemented the TFW-UCRL2 in the online episodic setting?

Review Point: 1. While in the "Strengths" I remarked that regret bounds essentially match those for the scalar reward setting, I feel that there is still some gap in the bound. In Theorem 4.1, the consumption regret upper bound of "Ld · CONSREG" has a rather loose dependence on d, as in it does not match the dependence of \| 1_d \| in (Agrawal and Devanur 2011). Here, I denote 1_d as the d-dimensional all 1 vector, and g is L Lipschitz continuous with respect to the norm \|\cdot\|.
Review Point: 2. In the main results, the authors show that the constraint regrets are bounded in expectation. Do these bound translate to a high probability bound, in the same way as (Badanidiyuru 2013, Agrawal and Devanur 2014, Cheung 2019)?
Review Point: 3. Another place that needs substantial improvement is the numerical experiments. While the authors have provided additional details for the numerical experiments in Appendix E, there are quite a few places that require clarification:
Review Point: - How does the plots in Figure 1 corroborate with the regret bounds? More precisely, it is not clear how a trajectory means in the online model. For example, if I look at the top left plot for RCPO, it reports that at No. of trajectories = 500, the reward is \approx 1.65. What does it mean? Does it mean that if I run the RCPO with 500 episodes than the empirical average reward realizes as 1.65, or does it mean something else?
Review Point: - Can the authors provide plots about the cumulative regret of the algorithms (at least for the proposed algorithms, in case they don't make sense for existing algorithms like RCPO for some reason)? This provide a more direct way to empirically evaluate the proposed algorithms.
Review Point: - In relation to the previous point, in footnote 7, it remarks that the bottom row corresponds to "the aggregate actual constraint incurred during training". However, in an online model, how is the notion of "during training" defined?
Review Point: - It is not clear why the authors include A2C, which requires the access to the latent model on p as shown in the Appendix (so it seems to violate the model assumption of not knowing p for example?) - I am in fact quite confused by column C. Is there any reason why TFW-UCRL2 is run with significantly more trajectories than the other two algorithms ConRL-A2C and ConRL-Value Iteration?
Review Point: - When I tried to dig deeper in Appendix E, it is stated that “TFW-UCRL2 gives fixed weights to reward and constraint violation and maximizes a scalar function. Therefore we tuned TFW-UCRL2 for different weights and the reported result is for the best weight.” Nevertheless, to my knowledge, the TFW-UCRL2 in fact assign dynamic weights () to the penalty function for the constraints, and those weights do not need tuning in the sense that the dynamic update of the weights Can the authors provide a high level sketch on how they implemented the TFW-UCRL2 in the online episodic setting?
==================================================

Focused review:

1. In algorithm 3, you assume the dominating set is known to the agent. I am not sure this is true is most reinforcement learning applications. In the robot moving example, the transition model is unknown which means the agent does not know which states are in the same line. Then you don’t know the dominating set in this case. 2. In algorithm 3, authors use the extended MDP to simultaneously learn policies to reach the goal and the points in the dominating set. However, the sample complexity analysis does not show the benefit of the ‘multi-task’ learning process.

Review Point: 1. In algorithm 3, you assume the dominating set is known to the agent. I am not sure this is true is most reinforcement learning applications. In the robot moving example, the transition model is unknown which means the agent does not know which states are in the same line. Then you don’t know the dominating set in this case.
Review Point: 2. In algorithm 3, authors use the extended MDP to simultaneously learn policies to reach the goal and the points in the dominating set. However, the sample complexity analysis does not show the benefit of the ‘multi-task’ learning process.
==================================================

Focused review:

- Even though the experiments are performed on top of Dreamer, the paper only presents results for a subset of the tasks considered in the Dreamer paper. The results for the higher-dimensional tasks such as cheetah and quadruped are not presented. - Regularization of model-based RL so that the imagined and real trajectories are similar have also been identified and considered in several other papers. For example ensembling [1], DAE regularization [2], energy-based models [3], and Section 3.6 in [4]. The paper does not compare to any of them.

Review Point: - Even though the experiments are performed on top of Dreamer, the paper only presents results for a subset of the tasks considered in the Dreamer paper. The results for the higher-dimensional tasks such as cheetah and quadruped are not presented.
Review Point: - Regularization of model-based RL so that the imagined and real trajectories are similar have also been identified and considered in several other papers. For example ensembling [1], DAE regularization [2], energy-based models [3], and Section 3.6 in [4]. The paper does not compare to any of them.
==================================================

Focused review:

Weaknesses: 1. It is claimed that these three tasks require understanding of both structure and content of the web-page. While it is easy to see that textual content plays a key role in each of the three tasks, the role played by the structure of the web-page is not clear. It can be argued that no significant HTML structure analysis or understanding is needed for these tasks. For example, in Semantic Classification, what is most important for classifying HTML element 'input' into, say, 'username' is the value of its two attributes, 'type' and 'id'. As these attributes are in the close neighbourhood of 'input', parsing of HTML is not strictly necessary. Therefore, it might a good idea to do some experiments that demonstrate unequivocally the need for HTML structure analysis or understanding in these tasks. One such experiment could be to map all HTML tags in the web-page except the salient tags to the same token (say, ***) so that the input is now a sequence of salient tags, and ***. 2. There is not much novelty in the methodological aspects of the work.

Review Point: 1. It is claimed that these three tasks require understanding of both structure and content of the web-page. While it is easy to see that textual content plays a key role in each of the three tasks, the role played by the structure of the web-page is not clear. It can be argued that no significant HTML structure analysis or understanding is needed for these tasks. For example, in Semantic Classification, what is most important for classifying HTML element 'input' into, say, 'username' is the value of its two attributes, 'type' and 'id'. As these attributes are in the close neighbourhood of 'input', parsing of HTML is not strictly necessary. Therefore, it might a good idea to do some experiments that demonstrate unequivocally the need for HTML structure analysis or understanding in these tasks. One such experiment could be to map all HTML tags in the web-page except the salient tags to the same token (say, ***) so that the input is now a sequence of salient tags, and ***.
Review Point: 2. There is not much novelty in the methodological aspects of the work.
==================================================

Focused review:

Weaknesses: 1. When introducing the theoretical results, we should make a detailed comparison with the existing cross-entropy loss results. The current writing method cannot reflect the advantages of square loss. 2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model? 3. This paper presents that the loss functions like hinge loss don’t provide reliable information on the prediction confidence. In this regard, there is a lack of references to some relevant literature. [Gao, 2013] has given a detailed analysis of the advantages and disadvantages between the entire margin distribution and the minimum margin. Based on this, [Lyu, 2018] designed a square-type margin distribution loss to improve the generalization ability of DNN.
[Gao, 2013] W. Gao and Z.-H. Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence 203:1-18 2013.
[Lyu, 2018] Shen-Huan Lyu, Lu Wang, and Zhi-Hua Zhou. Improving Generalization of Neural Networks by Leveraging Margin Distribution. http://arxiv.org/abs/1812.10761

Review Point: 1. When introducing the theoretical results, we should make a detailed comparison with the existing cross-entropy loss results. The current writing method cannot reflect the advantages of square loss.
Review Point: 2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model?
Review Point: 3. This paper presents that the loss functions like hinge loss don’t provide reliable information on the prediction confidence. In this regard, there is a lack of references to some relevant literature. [Gao, 2013] has given a detailed analysis of the advantages and disadvantages between the entire margin distribution and the minimum margin. Based on this, [Lyu, 2018] designed a square-type margin distribution loss to improve the generalization ability of DNN. [Gao, 2013] W. Gao and Z.-H. Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence 203:1-18 2013. [Lyu, 2018] Shen-Huan Lyu, Lu Wang, and Zhi-Hua Zhou. Improving Generalization of Neural Networks by Leveraging Margin Distribution. http://arxiv.org/abs/1812.10761
==================================================

Focused review:

- Though the LoCA regret is presented as a way to measure model-based behavior, there wasn't much justification that adaptation to local change is the right/only measure, other than its use in studies of biological organisms. I can imagine clearly model-based agents that might take a long time to adapt simply because of low step-sizes (e.g. to account for highly stochastic dynamics). I can also imagine model-free agents that might adapt very quickly (e.g. "The Brute" from Machado et al. JAIR 2018, which simply memorizes action sequences and their returns). The paper would be strengthened by a discussion of the limitations of this approach and exploration of distinctions it might not be suitable to make. - Though the LoCA regret is contrasted conceptually with single task sample complexity as an indicator of model use, the claims about the differences between the two is never quite evaluated. The paper would be strengthened by a clear example where LoCA is able to distinguish between methods where sample complexity improvement is caused by confounding factors and methods where sample complexity improvement is caused by effective planning.

Review Point: - Though the LoCA regret is presented as a way to measure model-based behavior, there wasn't much justification that adaptation to local change is the right/only measure, other than its use in studies of biological organisms. I can imagine clearly model-based agents that might take a long time to adapt simply because of low step-sizes (e.g. to account for highly stochastic dynamics). I can also imagine model-free agents that might adapt very quickly (e.g. "The Brute" from Machado et al. JAIR 2018, which simply memorizes action sequences and their returns). The paper would be strengthened by a discussion of the limitations of this approach and exploration of distinctions it might not be suitable to make.
Review Point: - Though the LoCA regret is contrasted conceptually with single task sample complexity as an indicator of model use, the claims about the differences between the two is never quite evaluated. The paper would be strengthened by a clear example where LoCA is able to distinguish between methods where sample complexity improvement is caused by confounding factors and methods where sample complexity improvement is caused by effective planning.
==================================================

Focused review:

Weaknesses
In Sec5.2.1, testing data are generated using the data augmentation pipeline proposed in this work and this could give Scene2Vec unfair advantages in my opinion as the augmentation strategy is applied directly during training (I assume that results in Sec5.2.2 are also obtained in the same way. But please correct me if I am wrong). Though there is still insight from Table3, I am wondering if it is possible to evaluate on raw spatial data instead of augmented ones to better reflect the overall performance of Scene2Vec.
It is still not clear to me how point sets of polylines or polygons are properly augmented. In addition, what is the criteria of choosing point sets for these structures.
Some details seems missing such as the final lengths after padding after augmentation module, training and testing profile.
Lack of ablation of key design modules. Related to last point, though several tasks are evaluated, there lacks of necessary ablation and discussions about the key contributions such as the benefits of different components in augmentation.
In definition 1, shoud c i be c j ? d j = [ x j 1 − c i , . . . ] or d j = [ x j 1 − c j , . . . ]
Writing quality needs to be improved. There are several typos, missing spaces and incomplete sentences in the main text. For example,
footnote of page3, a tree instead of A tree
beginning of sec 5.2.1, Considering that xxx, Since points are xxx ...

Review Point: .. ] or d j = [ x j 1 − c j , .
==================================================

Focused review:

1. This work is not well-motivated. It seems estimating the distribution is not necessary since the variance of the learned parameters for each item (teams, fund managers, etc.) already indicates whether the parameters of all items closely center somewhere or spread uniformly. So I don't see the point of a sophisticated second step in the algorithm. This paper does not provide a comparison in the experiments between their approach and the naive approach mentioned above either. 2. Most of this paper seems very technical but the theorems do not qualify as theorems. "sufficiently large constants" and "sufficiently large n" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.

Review Point: 1. This work is not well-motivated. It seems estimating the distribution is not necessary since the variance of the learned parameters for each item (teams, fund managers, etc.) already indicates whether the parameters of all items closely center somewhere or spread uniformly. So I don't see the point of a sophisticated second step in the algorithm. This paper does not provide a comparison in the experiments between their approach and the naive approach mentioned above either.
Review Point: 2. Most of this paper seems very technical but the theorems do not qualify as theorems. "sufficiently large constants" and "sufficiently large n" appear in every theorem statement but the authors fail to specify how large is sufficient. These are necessary for theorems.
==================================================

Focused review:

weakness I see is that I'm not sure there is a lot of information gain from this paper. The results are more or less what I think most readers will expect. The findings seem in line with past work on generalization and with conventional wisdom -- ID works, OOD works worse, inductive biases can help on OOD, etc. That's not to say the present work is not valuable -- there are open questions here and the present paper is one of the most extensive studies I have seen on them. Just that the paper doesn't provide entirely unexpected answers. Because of this, I think the most valuable contribution of the paper may be the benchmark it provides, on top of which future studies may find something really new.
To elaborate further, the related work covers numerous papers that have come to roughly similar conclusions. Two more papers come to mind that also have similar conclusion but were not discussed:
Packer et al., “Assessing Generalization in Deep Reinforcement Learning”, 2019 -- This paper also studied ID vs OOD generalization but in the context of reinforcement learning. Figure 1 from Packer et al. shows their train/test setup, which is quite related to the current paper's settings in Figure 2. The conclusions are similar to the current paper: 1) extrapolation is harder than interpolation, 2) SOTA algorithms that are supposed to "solve" this problem fail.
Jahanian et al., "On the 'Steerability' of Generative Adversarial Networks", 2020 -- This paper studied the ability of GANs to extrapolate in their latent space. The finding is that they struggle to generate transformations that extend beyond the distribution seen during training. These results are similar to the findings in the current paper on the failure of VAE latent representations to extrapolate beyond the training data.
This is all to say I think there is ample prior literature that make the present conclusions unsurprising. But thorough work on this topic, and new benchmarks, is still valuable and that's what the current paper provides. I should also note that the finding about modularity is something I hadn't seen before, and I think that's a valuable contribution as well.
Aside from this, I think the paper is very solid. A few minor comments follow:
Using CelebGlow feels a bit awkward since it is a generative model fit to data, and then you are again fitting samples from this model with another generative model. I wonder if there could be some bias where the samples are easier to model with a VAE since they were generated with a related model (Glow)... It's probably all fine but some commentary on this could be useful.
Repeated reference to Hendrycks and Diettrich 2019.
I would say inductive biases 1 and 3 overlap: inductive bias 1, as it is implemented in the paper, could be considered a special case of transfer learning where the pretraining is done with VAEs. This could be clarified to avoid implying that these are independent inductive biases.
“if factors are located in a particular edge of the FoV hyper cube given by all FoVs” — “edge” —> “corner”?
“Here, we further see that, on average, the performances seem to increase as we increase the supervision signal.” — This is a bit vague I don’t see it fully reflected in the figure. This point could be made more precise. What does “increase the supervision signal” refer to?
“We find that the degree of downstream performance correlates weakly but positively with the degree of disentanglement (Pearson ρ = 0.63, Spearman ρ = 0.67)” — I’m not sure I would call these weak correlations. In many fields I believe this would be considered a strong correlation.
“Existing notions of disentanglement models with a readout MLP do not help to facilitate the learning of the underlying mechanisms in the tested datasets.” — I don’t understand this conclusion. The correlations are substantial. My understanding of the results is that greater disentanglement does correlate with better ability to identify the underlying mechanisms. Appendix Fig 7 seems to support this as well, with all but one of the correlations being positive.

Review Point: 1) extrapolation is harder than interpolation, 2) SOTA algorithms that are supposed to "solve" this problem fail. Jahanian et al., "On the 'Steerability' of Generative Adversarial Networks", 2020 -- This paper studied the ability of GANs to extrapolate in their latent space. The finding is that they struggle to generate transformations that extend beyond the distribution seen during training. These results are similar to the findings in the current paper on the failure of VAE latent representations to extrapolate beyond the training data. This is all to say I think there is ample prior literature that make the present conclusions unsurprising. But thorough work on this topic, and new benchmarks, is still valuable and that's what the current paper provides. I should also note that the finding about modularity is something I hadn't seen before, and I think that's a valuable contribution as well. Aside from this, I think the paper is very solid. A few minor comments follow: Using CelebGlow feels a bit awkward since it is a generative model fit to data, and then you are again fitting samples from this model with another generative model. I wonder if there could be some bias where the samples are easier to model with a VAE since they were generated with a related model (Glow)... It's probably all fine but some commentary on this could be useful. Repeated reference to Hendrycks and Diettrich 2019. I would say inductive biases 1 and 3 overlap: inductive bias 1, as it is implemented in the paper, could be considered a special case of transfer learning where the pretraining is done with VAEs. This could be clarified to avoid implying that these are independent inductive biases. “if factors are located in a particular edge of the FoV hyper cube given by all FoVs” — “edge” —> “corner”? “Here, we further see that, on average, the performances seem to increase as we increase the supervision signal.” — This is a bit vague I don’t see it fully reflected in the figure. This point could be made more precise. What does “increase the supervision signal” refer to? “We find that the degree of downstream performance correlates weakly but positively with the degree of disentanglement (Pearson ρ = 0.63, Spearman ρ = 0.67)” — I’m not sure I would call these weak correlations. In many fields I believe this would be considered a strong correlation. “Existing notions of disentanglement models with a readout MLP do not help to facilitate the learning of the underlying mechanisms in the tested datasets.” — I don’t understand this conclusion. The correlations are substantial. My understanding of the results is that greater disentanglement does correlate with better ability to identify the underlying mechanisms. Appendix Fig 7 seems to support this as well, with all but one of the correlations being positive.
==================================================

Focused review:

Weaknesses: 1. While the performance on FSD-1000 and ADE-20k are impressive, it is compared against just 1-shot. Also, the gap is larger in Pascal-5i and COCO-20i with less mIoU and FB-IoU for LSeg. 2. The strongest baseline uses ResNet-101 which has much fewer parameters (~45M) than the backbone used by LSeg (307M). So, it does not seem like a fair comparison.

Review Point: 1. While the performance on FSD-1000 and ADE-20k are impressive, it is compared against just 1-shot. Also, the gap is larger in Pascal-5i and COCO-20i with less mIoU and FB-IoU for LSeg.
Review Point: 2. The strongest baseline uses ResNet-101 which has much fewer parameters (~45M) than the backbone used by LSeg (307M). So, it does not seem like a fair comparison.
==================================================

Focused review:

Weakness:
The overall novelty seems limited since the instance-adaptive method is from existing work with no primary changes. Here are some main questions and concerns:
1). How many optimization steps are used to produce the final reported performance in Figure.1 as well as in some other figs and tables?
2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison?
Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison:
Guo Lu, et al., "Content Adaptive and Error Propagation Aware Deep Video Compression." ECCV 2020.

Review Point: 1). How many optimization steps are used to produce the final reported performance in Figure.1 as well as in some other figs and tables?
Review Point: 2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison? Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison: Guo Lu, et al., "Content Adaptive and Error Propagation Aware Deep Video Compression." ECCV 2020.
==================================================

Focused review:

Weaknesses: The baseline methods seem too naïve and lacks more ablation study experiments. 1. The authors claim their dataset could be applied into other ML task, e.g. data studying, anomaly detection. Wish to conduct related experiments to verify this statement. 2. I wonder whether there exist additional data from other zones, which could be implement the cross evaluation of their baseline models. 3. There seems lack more details related works about this topic. 4. The evaluation metric just uses MSE in Table 2, which cannot reflect the accuracy of nowcasting. 5. The results of the previous methods on the proposed dataset also should be listed.

Review Point: 1. The authors claim their dataset could be applied into other ML task, e.g. data studying, anomaly detection. Wish to conduct related experiments to verify this statement.
Review Point: 2. I wonder whether there exist additional data from other zones, which could be implement the cross evaluation of their baseline models.
Review Point: 3. There seems lack more details related works about this topic.
Review Point: 4. The evaluation metric just uses MSE in Table 2, which cannot reflect the accuracy of nowcasting.
Review Point: 5. The results of the previous methods on the proposed dataset also should be listed.
==================================================

Focused review:

Weaknesses
The clarity of the writing could be improved substantially. Descriptions are often vague, which makes the technical details harder to understand. I think it's fine to give high-level intuitions separate from low-level details, but the current writing invites confusion. For example, at the start of Section 3, the references to buffers and clusters are vague. The text refers readers to where these concepts are described, but the high-level description doesn't really give a clear picture, making the text that follows harder to understand.
Ideas are not always presented clearly. For example:
may only exploit a small part of it, making most of the goals pointless.```
- Along the same lines, at the start of the Experiments section, when reading ```the ability of DisTop to select skills to learn``` I am left to wonder what this "ability" and "selection" refers to. This is not a criticism of word choice. The issue is that the previous section did not set up these ideas.
- Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.
- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.
- The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better.
### Questions/Comments:
- The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.
- The same applies to 2.2. The technical details are hard to follow.
- You claim "In consequence, we avoid using a hand engineered environment-specific scheduling" on page 4. Does this suggest that the $\beta$ parameter and the $\omega'$ update rate are environment independent?
- Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?
- It is somewhat strange phrasing to describe Skew-Fit as having "favorite" environments (page 6).

Review Point: - Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.
Review Point: - Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.
Review Point: - The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better. ### Questions/Comments:
Review Point: - The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.
Review Point: - The same applies to 2.2. The technical details are hard to follow.
Review Point: - You claim "In consequence, we avoid using a hand engineered environment-specific scheduling" on page 4. Does this suggest that the $\beta$ parameter and the $\omega'$ update rate are environment independent?
Review Point: - Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?
Review Point: - It is somewhat strange phrasing to describe Skew-Fit as having "favorite" environments (page 6).
==================================================

Focused review:

Weaknesses: . It is claimed that the generated OOD samples cover larger diversity ranges. If the generated OOD samples all belong to the same classes as those in the training set, how to quantify such diversity?
. The experiments show that with more synthesized data the OOD generalization improves. Then the question is what if we use only synthesized data for training classifiers? What's the performance? Is there any tradeoff between using real data and synthesized data?
. The authors emphasized the superior performance of the proposed algorithm. However, with more synthetic data the training time would be significantly increased. It's better to add discussion on both the advantage and limitations of the proposed algorithm for a fair comparison with benchmarks.
. In Algorithm 1 line 3: how to pick \theta_i to update \theta_j? To achieve a better performance, do you have to carefully pick a network for fine-tuning?
. How to set the interpolation coefficients (Equ. 1) in your experiments? How do these parameters affect the performance of OOD generalization?
. In Table 1, what's the difference between in distribution check mark and cross mark? What's the OOD data here? Are the results averaged over different OOD datasets or for some particular OOD dataset?
. Results on colored fashion MNIST: the described numbers are incorrect. They are from Table 1, not Table 2.

Review Point: . It is claimed that the generated OOD samples cover larger diversity ranges. If the generated OOD samples all belong to the same classes as those in the training set, how to quantify such diversity?
Review Point: . The experiments show that with more synthesized data the OOD generalization improves. Then the question is what if we use only synthesized data for training classifiers? What's the performance? Is there any tradeoff between using real data and synthesized data?
Review Point: . The authors emphasized the superior performance of the proposed algorithm. However, with more synthetic data the training time would be significantly increased. It's better to add discussion on both the advantage and limitations of the proposed algorithm for a fair comparison with benchmarks.
Review Point: . In Algorithm 1 line 3: how to pick \theta_i to update \theta_j? To achieve a better performance, do you have to carefully pick a network for fine-tuning?
Review Point: . How to set the interpolation coefficients (Equ. 1) in your experiments? How do these parameters affect the performance of OOD generalization?
Review Point: . In Table 1, what's the difference between in distribution check mark and cross mark? What's the OOD data here? Are the results averaged over different OOD datasets or for some particular OOD dataset?
Review Point: . Results on colored fashion MNIST: the described numbers are incorrect. They are from Table 1, not Table 2.
==================================================

Focused review:

- Perhaps most importantly, empirical advantages of the method are not demonstrated. Together with the lack of theoretical motivation, this is the largest shortcoming of the work. I am left still not knowing what the primary motivation and reason for pursuing QP over EP is. - Lack of theoretical motivation. The main theory of the work relates to some variance bounds for QP versus EP, although these are just bounds. No gap between these methods is given. - The bounds only apply to when the same cavity distribution is used. No analysis of fixed points is given. - The algorithm is not so clearly explained in the main text, and is given in the appendix. - No convergence properties of the algorithm are discussed. - In cases where one does not have access to the CDF of the cavity distribution, the authors do not discuss performance. - The look-up tables depend on the tilted distribution and so change from problem to problem. - Showing convergence is still a major issue for the algorithm. Are there any modifications for future work for which you could prove convergence and analyze the fixed points? - Is there any guidance for constructing the look-up tables?

Review Point: - Perhaps most importantly, empirical advantages of the method are not demonstrated. Together with the lack of theoretical motivation, this is the largest shortcoming of the work. I am left still not knowing what the primary motivation and reason for pursuing QP over EP is.
Review Point: - Lack of theoretical motivation. The main theory of the work relates to some variance bounds for QP versus EP, although these are just bounds. No gap between these methods is given.
Review Point: - The bounds only apply to when the same cavity distribution is used. No analysis of fixed points is given.
Review Point: - The algorithm is not so clearly explained in the main text, and is given in the appendix.
Review Point: - In cases where one does not have access to the CDF of the cavity distribution, the authors do not discuss performance.
Review Point: - The look-up tables depend on the tilted distribution and so change from problem to problem.
Review Point: - Showing convergence is still a major issue for the algorithm. Are there any modifications for future work for which you could prove convergence and analyze the fixed points?
Review Point: - Is there any guidance for constructing the look-up tables?
==================================================

Focused review:

Weakness:
May need some interpretations for some weird cases in the Table and Figures.
For example,
1). In Table 4, it looks more context does not help for Medium tasks?
2). Similarly, in Figure 3, it looks MLP is quite a strong baseline on the Hard tasks?

Review Point: 2). Similarly, in Figure 3, it looks MLP is quite a strong baseline on the Hard tasks?
==================================================

Focused review:

- Examine a very specialized setting in which the network is first passed through a randomly initialized network and then into a trainable network. This is non-standard for many practical neural network problems. Moreover the networks is that of a Quadratic Taylor model, which is not commonly used in practice. - Demonstrate that adding an additional layer outperforms a low-rank polynomial, which does not seem like the strongest baseline.

Review Point: - Examine a very specialized setting in which the network is first passed through a randomly initialized network and then into a trainable network. This is non-standard for many practical neural network problems. Moreover the networks is that of a Quadratic Taylor model, which is not commonly used in practice.
Review Point: - Demonstrate that adding an additional layer outperforms a low-rank polynomial, which does not seem like the strongest baseline.
==================================================

Focused review:

Weaknesses:
The idea presented in this work (to the best of my understanding) can be summarized as follows: shared information should not pollute modality-specific latent information. To ensure this, a variant of the mixture of expert ELBO is derived, by introducing learnable pseudo-priors that induce the reconstruction of a given modality to rely solely on its modality-specific latent space. A natural question is as follows: since you assume independence between shared and modality-specific latent spaces, and further you constrain modality-specific information to be solely responsible for reconstruction, then why not training individual VAEs, one per modality, then a multimodal VAE to learn a shared representation, and then “stitch” together all latent spaces? In other words, what I miss is a naive baseline to double check that the idea presented in this paper is a valid approach when compared to a simpler method.
I find Expression (5) in the main paper not very clear. In a previous workshop paper that presents the very same idea and the very same method I find the use of a tilde on the “other-modalities” private latent variables easier to understand and more clear. On the good side, I think the textual explanations of the ELBO are much more useful in this version of the paper.
In the experiments, unconditional coherence seem to be a more difficult objective to attain, as was also shown in prior works. In this case, we usually sample from the prior on the latent space, and only rely on decoders to generate each modality. Then we check the coherence of each generated modality. 1) It would be useful to clarify how does this work when you have a latent space that is segregated. I assume you take the prior of each modality-specific latent, and the shared prior, but it would be good to state this clearly (in the appendix). Why do you only have results on PolyMNIST for this challenging metric, and not for CUB? Moreover, when we look at table 3 in the appendix (for PolyMNIST), we can see that the proposed method really struggles to achieve high generation quality (there, the MVAE approach based on PoE works better). It is also interesting to note that additional “tricks” to improve base models (importance sampling, double reparametrization), seem to hurt generation quality, which is not expected.
In the experiments, when we look at “competitors”, we learn that if the capacity is sufficient, then the modality-specific encoder can learn all the information (both shared and private) such that an adverse effect of a shortcut appears. But if all information is stored in the modality-specific latent space, and this is true for each modality, then wouldn’t this be enough to achieve coherence? Aren’t we simply looking at the “fallacy” of the MoE approach that, by construction, samples only one of the components for the generation?
Fig 5 raise several questions. ** Fig 5b indicates latent classification accuracy, obtained by training a simple linear model on “freezed” encoders that produce latent features. It seems counter intuitive to state that lower accuracy is better. This is because the goal is to show that if we only use modality-specific encodings we should not be able to classify correctly, if the hypothesis of no shared information being present in those latent spaces. Beside the fact that I am not convinced that this metric (first introduced in Shi et al ‘19) is useful to state anything about the latent space natural segregation, I think that comments to this figure should be more informative. ** Fig 5 c: I assume this figure is for the conditional generation right? Since there are works that criticize the use of FID score to assess the generative quality [1], I wonder why we do not have a table with likelihood values, or Bit per Dimension, which can also be useful to assess the quality of the model. ** Overall, from the three sub-figures in fig 5, we can remark that mmJSD is a close competitor to MMVAE+. I think this should be acknowledged and commented.
The last paragraph before section 4.3 claims for an optimal balance between coherence and generation quality. How can you claim optimality?
In section 4.3, qualitative results for the CUB dataset, in terms of coherence, use an original metric discussed in the appendix. This is an important detail that I think should be discussed at least a minimum in the main paper. Indeed, the proposed metric is somehow questionable and should be put upfront for the reader to properly understand. In my opinion, it is not clear why you could have not used the same technique used in Shi et al ‘19, by taking the learned representation of your ResNet, in a similar way as if you used a pre-trained network. The introduction of artificial captions and the count of correctly colored pixels suffers from the problems stated by the authors, e.g. counting only the background color. Using the top-2 most frequent pixel colors might work in some cases an not others, so I am not sure how much can we rely on this coherence metric. Also, as commented above, this is conditional coherence, which seems an easier task to achieve than unconditional coherence. Do you have any comments why, and why didn’t you show unconditional coherence metric fo CUB?
[1] @misc{FID_critics, url = {https://arxiv.org/abs/2203.06026}, author = {Kynkäänniemi, Tuomas and Karras, Tero and Aittala, Miika and Aila, Timo and Lehtinen, Jaakko}, title = {The Role of ImageNet Classes in Fréchet Inception Distance}, publisher = {arXiv}, year = {2022}, }

Review Point: 1) It would be useful to clarify how does this work when you have a latent space that is segregated. I assume you take the prior of each modality-specific latent, and the shared prior, but it would be good to state this clearly (in the appendix). Why do you only have results on PolyMNIST for this challenging metric, and not for CUB? Moreover, when we look at table 3 in the appendix (for PolyMNIST), we can see that the proposed method really struggles to achieve high generation quality (there, the MVAE approach based on PoE works better). It is also interesting to note that additional “tricks” to improve base models (importance sampling, double reparametrization), seem to hurt generation quality, which is not expected. In the experiments, when we look at “competitors”, we learn that if the capacity is sufficient, then the modality-specific encoder can learn all the information (both shared and private) such that an adverse effect of a shortcut appears. But if all information is stored in the modality-specific latent space, and this is true for each modality, then wouldn’t this be enough to achieve coherence? Aren’t we simply looking at the “fallacy” of the MoE approach that, by construction, samples only one of the components for the generation? Fig 5 raise several questions. ** Fig 5b indicates latent classification accuracy, obtained by training a simple linear model on “freezed” encoders that produce latent features. It seems counter intuitive to state that lower accuracy is better. This is because the goal is to show that if we only use modality-specific encodings we should not be able to classify correctly, if the hypothesis of no shared information being present in those latent spaces. Beside the fact that I am not convinced that this metric (first introduced in Shi et al ‘19) is useful to state anything about the latent space natural segregation, I think that comments to this figure should be more informative. ** Fig 5 c: I assume this figure is for the conditional generation right? Since there are works that criticize the use of FID score to assess the generative quality [1], I wonder why we do not have a table with likelihood values, or Bit per Dimension, which can also be useful to assess the quality of the model. ** Overall, from the three sub-figures in fig 5, we can remark that mmJSD is a close competitor to MMVAE+. I think this should be acknowledged and commented. The last paragraph before section 4.3 claims for an optimal balance between coherence and generation quality. How can you claim optimality? In section 4.3, qualitative results for the CUB dataset, in terms of coherence, use an original metric discussed in the appendix. This is an important detail that I think should be discussed at least a minimum in the main paper. Indeed, the proposed metric is somehow questionable and should be put upfront for the reader to properly understand. In my opinion, it is not clear why you could have not used the same technique used in Shi et al ‘19, by taking the learned representation of your ResNet, in a similar way as if you used a pre-trained network. The introduction of artificial captions and the count of correctly colored pixels suffers from the problems stated by the authors, e.g. counting only the background color. Using the top-2 most frequent pixel colors might work in some cases an not others, so I am not sure how much can we rely on this coherence metric. Also, as commented above, this is conditional coherence, which seems an easier task to achieve than unconditional coherence. Do you have any comments why, and why didn’t you show unconditional coherence metric fo CUB? [1] @misc{FID_critics, url = {https://arxiv.org/abs/2203.06026}, author = {Kynkäänniemi, Tuomas and Karras, Tero and Aittala, Miika and Aila, Timo and Lehtinen, Jaakko}, title = {The Role of ImageNet Classes in Fréchet Inception Distance}, publisher = {arXiv}, year = {2022}, }
==================================================

Focused review:

weaknesses
The documentation of experiments and method explanation need to be improved. For example:
Section 1 Paragraph 2: "Fig. 1a, the standard variance of the scale of instances in MS-COCO is 188.4, while that of ImageNet is 56.7" what is the unit of measure? Also, the images were resized differently therefore the statistics are incomparable.
Section 3.1 "Towards handling the large scale variation, the s is selected from {1, 2, 3, 4}, which also matches the sizes of feature maps in FPN and the label assignment rules" . It is not clear if the method uniformly samples from the set {1,2,3,4} or that a single number is used. Later, in sec 4.3 it says that s=2 is the optimal scale. It is not clear which value was used for the experiments in table 1.
Section 4.2 "Fig. 1c shows that the discordance between different sizes is alleviated" the setup is not clear, was it measured during the training phase? or did you augment the validation images to measure the discrepancy.
Section 4.3 "Fig. 5 shows that the fraction of valid instances..." the term "valid instances" was not defined. It seems like an important and critical quantity that was not defined explained or further discussed.

Review Point: 5 shows that the fraction of valid instances..." the term "valid instances" was not defined. It seems like an important and critical quantity that was not defined explained or further discussed.
==================================================

Focused review:

weaknesses are very obvious in this paper.
On the strength side:
The paper itself is very well written. The notations are well defined and the methods are very clearly explained. The presentation is fluent.
The proposed method seems novel and interesting. The derivations are technically correct.
Experiments show performance improvement over baselines, especially on CUB200/Dogs/Cars.
On the weakness side:
I think experiment presents the most significant weakness of this paper: 1) The comparison is rather weak without any reference to existing prior arts such as [1]. A simple search with respect to the 5 experiment datasets also show significant performance gaps between the proposed method and latest methods. 2) I remain skeptical about the solidness of the baseline performance as they show considerable gaps to standard baseline training without bells and whistles (https://github.com/weiaicunzai/pytorch-cifar100). 3) The performance gain diminishes very quickly on bigger dataset such as Tiny ImageNet. What about the results on ImageNet?
The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work. In certain cases, semantic hierarchy may not always be a reasonable choice to guide the learning of visual embedding.
I have some concern about the selection of initial radius R 0
and its decay policy. I think this parameter should be dataset dependent due to different numbers of categories and the densities of class distributions. As a result, how such parameter and policy can be optimally determined becomes a question.
Finally, forcing a fixed radius does not sound as reasonable as allowing a learnable radius with soft regularization.
[1] Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018
========================== Post Rebuttal ==============================
The authors did a good job in addressing some of my concerns in the rebuttal. Thus I am increasing the score in response to the clarifications. However, I feel there is still some improvement space for the experiment part of this section, and I encourage the authors to incorporate the changes, including ImageNet experiment and following stronger baselines to make the results more solid and convincing.

Review Point: 1) The comparison is rather weak without any reference to existing prior arts such as [1]. A simple search with respect to the 5 experiment datasets also show significant performance gaps between the proposed method and latest methods.
Review Point: 2) I remain skeptical about the solidness of the baseline performance as they show considerable gaps to standard baseline training without bells and whistles (https://github.com/weiaicunzai/pytorch-cifar100).
Review Point: 3) The performance gain diminishes very quickly on bigger dataset such as Tiny ImageNet. What about the results on ImageNet? The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work. In certain cases, semantic hierarchy may not always be a reasonable choice to guide the learning of visual embedding. I have some concern about the selection of initial radius R 0 and its decay policy. I think this parameter should be dataset dependent due to different numbers of categories and the densities of class distributions. As a result, how such parameter and policy can be optimally determined becomes a question. Finally, forcing a fixed radius does not sound as reasonable as allowing a learnable radius with soft regularization. [1] Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018 ========================== Post Rebuttal ============================== The authors did a good job in addressing some of my concerns in the rebuttal. Thus I am increasing the score in response to the clarifications. However, I feel there is still some improvement space for the experiment part of this section, and I encourage the authors to incorporate the changes, including ImageNet experiment and following stronger baselines to make the results more solid and convincing.
==================================================

Focused review:

Weaknesses: Proposed method has a number of technical issues.
minor: 1. Eq (3) and (8) seems to be incorrect as formula and is not what the authors actually used. The maximum of a polynomial is +inf if the leading coefficient is positive.
What the authors did was to take the maximum of the fit values on the normal dataset - see figure 5.
minor: 2. Eq(6) has two issues. Firstly, optimizing degree using it does not depend on the cardinalities of \mathcal{N} and \mathcal{A}. They are unnecessary added complexities. Secondly, it has nothing to do with accuracy. The usage of accuracy there is misleading. Compactness is undefined what it should be.
minor: 3. Fitting a polynomial to the large flat low scoring part is not meaningful when one is interested only in the largest values.
major: 4. If the authors just take the highest polynomial fit value, then this method is questionable. The result for proper fitting with degree of at least 3 should be very close to the maximum of the sorted scores which were used to fit it.
But then one can simply take the maximum over the raw scores , or threshold along quantile scores for high values (or take the maximum as extreme value), which does the same in a simpler way.
major: 5. Figure 5 and figure 4 suggests that the results were tuned by early stopping of the polynomial fitting optimization.
This is because after convergence in figure 5 it returns simply the maximum of the scores of the normal dataset, which can be achieved without any polynomial fitting. This is also supported by figure 4, where a simple linear normalization improves the fitting outcome. With proper optimization this should not occur and one should obtain the same fit.
But if that is the essential gain using this method, then , firstly, it uses a hidden parameter in an non-transparent way to achieve good scores (the amount of optimization steps), and secondly, one could achieve this much simpler by thresholding along high quantile values (this also would use sorting but skips fitting a parametric model).
major: 6. Using the max of fitted scores does not make use of the L-trend described in section 4, thus the title is misleading. Reason is that the maximum merely looks at the very top of the normal data, if the polynomial is fitted well.

Review Point: 1. Eq (3) and (8) seems to be incorrect as formula and is not what the authors actually used. The maximum of a polynomial is +inf if the leading coefficient is positive. What the authors did was to take the maximum of the fit values on the normal dataset - see figure 5. minor:
Review Point: 2. Eq(6) has two issues. Firstly, optimizing degree using it does not depend on the cardinalities of \mathcal{N} and \mathcal{A}. They are unnecessary added complexities. Secondly, it has nothing to do with accuracy. The usage of accuracy there is misleading. Compactness is undefined what it should be. minor:
Review Point: 3. Fitting a polynomial to the large flat low scoring part is not meaningful when one is interested only in the largest values. major:
Review Point: 4. If the authors just take the highest polynomial fit value, then this method is questionable. The result for proper fitting with degree of at least 3 should be very close to the maximum of the sorted scores which were used to fit it. But then one can simply take the maximum over the raw scores , or threshold along quantile scores for high values (or take the maximum as extreme value), which does the same in a simpler way. major:
Review Point: 5. Figure 5 and figure 4 suggests that the results were tuned by early stopping of the polynomial fitting optimization. This is because after convergence in figure 5 it returns simply the maximum of the scores of the normal dataset, which can be achieved without any polynomial fitting. This is also supported by figure 4, where a simple linear normalization improves the fitting outcome. With proper optimization this should not occur and one should obtain the same fit. But if that is the essential gain using this method, then , firstly, it uses a hidden parameter in an non-transparent way to achieve good scores (the amount of optimization steps), and secondly, one could achieve this much simpler by thresholding along high quantile values (this also would use sorting but skips fitting a parametric model). major:
Review Point: 6. Using the max of fitted scores does not make use of the L-trend described in section 4, thus the title is misleading. Reason is that the maximum merely looks at the very top of the normal data, if the polynomial is fitted well.
==================================================

Focused review:

Overall I like this paper and think it made good contributions to both ViT and efficient computation fields. Some weaknesses and comments are:
1.Why the identity of the attention map can improve the expressive power of attention module? Is Identity mapping the best choice? More discussion should be conducted. 2.How about this adder attention applied on natural language processing tasks?

Review Point: 1.Why the identity of the attention map can improve the expressive power of attention module? Is Identity mapping the best choice? More discussion should be conducted.
Review Point: 2.How about this adder attention applied on natural language processing tasks?
==================================================

Focused review:

weakness and issues which should be clarified: 1) The novelty of this paper is incremental. The proposed method is developed based on the MDNet framework. It seems that the only difference is that the proposed method further incorporate the attention regularization for backward propagation. 2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value. 3) The author claims that the proposed method can enable the classifier attend to temporal invariant motion patterns. It seems that no explanation is provided about what motion patterns mean in this paper. Although some figures show the evolvement of attention during training, no motion pattern is illustrated. In addition, some large variations may happen during the tracking process, such as out-plane-rotation, how can the proposed method ensure that the temporal motion invariant pattern can be found and the classifiers can attend to them? [POST-REBUTTAL COMMENTS] I have read the rebuttal and still have the concerns on the theoretical support for the regularization term. I keep my rating. 

Review Point: 1) The novelty of this paper is incremental. The proposed method is developed based on the MDNet framework. It seems that the only difference is that the proposed method further incorporate the attention regularization for backward propagation.
Review Point: 2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value.
Review Point: 3) The author claims that the proposed method can enable the classifier attend to temporal invariant motion patterns. It seems that no explanation is provided about what motion patterns mean in this paper. Although some figures show the evolvement of attention during training, no motion pattern is illustrated. In addition, some large variations may happen during the tracking process, such as out-plane-rotation, how can the proposed method ensure that the temporal motion invariant pattern can be found and the classifiers can attend to them? [POST-REBUTTAL COMMENTS] I have read the rebuttal and still have the concerns on the theoretical support for the regularization term. I keep my rating.
==================================================

Focused review:

Since I found the exposition of this paper is not very clear in some parts, I would ask questions on the settings / designs, and then raise concerns. Please confirm/explain if anything. 1. Are the transferring functions between categories (e.g. human->cat) precomputed on two specific template mesh of human and cat, or two sets of mesh instances? If it’s from two specific template meshes, then I doubt the transformation estimated from functional maps would be general enough for transforming different types of cats. Further, I recall the LB basis doesn’t stay static (in corresponding location on surface) if the object mesh is deforming. Therefore the proposed method to get transformation would also not fit for cats in different poses (rigid and non-rigid). In this sense, the design of the algorithm in this part is already not suitable for the problem that aims to cover deformable objects in various appearances and poses. 2. The transfer function from “universal” descriptor to target descriptor is designed to be linear. Do you think the linear transfer is sufficient? Since the appearance of different animals are quite different, I doubt so. (regardless, this would be an interesting experiment) 3. Would you show the solved correspondence transfer in visual results? The difficult parts would be those parts for which humans are not significant (tails, ear, nose->elephant trunk). Those parts might be definable “anatomically” (humans have a tail bone corresponding to long tails of many animals), but the mapping will result in quite extreme deformation in correspondence (not isometric). 4. On results: I found the improvement from baseline (e.g. in table 2) is quite marginal. Table 2: the improvement of best cases of CSE to corresponding setting of baseline are mostly <1.0 in all metrics. The visual results also presents potential mismatches: Figure 4: Row 1 example 1, the feet of cats are mistaken. Similar issue in Row 1 example 6, Row 2 example 5, giraffe feet in Row 3 example 3 The elephant trunk looks very similar to the color mapping of feet. I see artifacts that look “blocky”, such as row 2 example 3, row 4 example 4. 5. Improve visualization: might be good add the visualization that you would randomly sample some point on source image / scan and draw a line to the matched point by the proposed method. One more (slightly ambitious) visualization is to use the predicted descriptors to drive the template mesh (similar to the one in DensePose video results). This is actually a good application case, in that you could track animal motion from a single view image / video. 6. Improve writing: To me I feel the explanation on Laplacian-Beltrami operators and functional maps can be more concise or moved to supplement materials. Would be good to reorganize the procedure of compute the transfer and add more results.

Review Point: 1. Are the transferring functions between categories (e.g. human->cat) precomputed on two specific template mesh of human and cat, or two sets of mesh instances? If it’s from two specific template meshes, then I doubt the transformation estimated from functional maps would be general enough for transforming different types of cats. Further, I recall the LB basis doesn’t stay static (in corresponding location on surface) if the object mesh is deforming. Therefore the proposed method to get transformation would also not fit for cats in different poses (rigid and non-rigid). In this sense, the design of the algorithm in this part is already not suitable for the problem that aims to cover deformable objects in various appearances and poses.
Review Point: 2. The transfer function from “universal” descriptor to target descriptor is designed to be linear. Do you think the linear transfer is sufficient? Since the appearance of different animals are quite different, I doubt so. (regardless, this would be an interesting experiment) 3. Would you show the solved correspondence transfer in visual results? The difficult parts would be those parts for which humans are not significant (tails, ear, nose->elephant trunk). Those parts might be definable “anatomically” (humans have a tail bone corresponding to long tails of many animals), but the mapping will result in quite extreme deformation in correspondence (not isometric).
Review Point: 4. On results: I found the improvement from baseline (e.g. in table 2) is quite marginal. Table 2: the improvement of best cases of CSE to corresponding setting of baseline are mostly <1.0 in all metrics. The visual results also presents potential mismatches: Figure 4: Row 1 example 1, the feet of cats are mistaken. Similar issue in Row 1 example 6, Row 2 example 5, giraffe feet in Row 3 example 3 The elephant trunk looks very similar to the color mapping of feet. I see artifacts that look “blocky”, such as row 2 example 3, row 4 example 4.
Review Point: 5. Improve visualization: might be good add the visualization that you would randomly sample some point on source image / scan and draw a line to the matched point by the proposed method. One more (slightly ambitious) visualization is to use the predicted descriptors to drive the template mesh (similar to the one in DensePose video results). This is actually a good application case, in that you could track animal motion from a single view image / video.
Review Point: 6. Improve writing: To me I feel the explanation on Laplacian-Beltrami operators and functional maps can be more concise or moved to supplement materials. Would be good to reorganize the procedure of compute the transfer and add more results.
==================================================

Focused review:

Weaknesses
Figure 2 can be further improved by simply annotating everything in Eq 4-7 in the figure. Now I'm very confused by the arrow pointing from 'Current image' to 'Value' and the one pointing from 'Key' to 'Current image'. What are they referring to?
While I understand this paper is just trying to illustrate a case of extending the idea of fast weight to more domains, more experiments certainly help to show how generic the proposed method is. Currently, there are only results with image + GAN training, while a quick experiment with other data domains like audio or training methods like VAE or diffusion can be a plus.
Minor points:
1. In the Sequence Processor paragraph, we did not manage to train any models successfully when the standard Transformer was used instead. Does it mean transformers completely fail in this case? I don't see a clear reason why transformers cannot work at all here.
Based on the 2nd point in Weaknesses, I am very interested to see a synergy between the proposed method and diffusion, as both are iterative.

Review Point: 1. In the Sequence Processor paragraph, we did not manage to train any models successfully when the standard Transformer was used instead. Does it mean transformers completely fail in this case? I don't see a clear reason why transformers cannot work at all here. Based on the 2nd point in Weaknesses, I am very interested to see a synergy between the proposed method and diffusion, as both are iterative.
==================================================

Focused review:

- The results of the BERT experiments are not strong enough. Most gains come from the RTE that is a very small dataset. However, on the most important task MNLI, the performance degrades significantly. In terms of performance, it seems that knowledge distillation outperforms the proposed method, with the same reduction of memory size. - The speedup in terms of detailed GPU hours shouls also be reported in the tables. - The previous attention clustering methods are not throughly compared with in the experiments. - It's unclear how GPU friendly the method is. - More analysis would help readers to understand the limitation of the proposed method. - The method is evaluated for fine-tuned BERT models. It's more insightful to show that the proposed method can also work well in the pre-training setting.

Review Point: - The results of the BERT experiments are not strong enough. Most gains come from the RTE that is a very small dataset. However, on the most important task MNLI, the performance degrades significantly. In terms of performance, it seems that knowledge distillation outperforms the proposed method, with the same reduction of memory size.
Review Point: - The speedup in terms of detailed GPU hours shouls also be reported in the tables.
Review Point: - The previous attention clustering methods are not throughly compared with in the experiments.
Review Point: - More analysis would help readers to understand the limitation of the proposed method.
Review Point: - The method is evaluated for fine-tuned BERT models. It's more insightful to show that the proposed method can also work well in the pre-training setting.
==================================================

Focused review:

Weakness: 1) My main concern is that the Gaussian mixture data assumption may be too strong. As shown in Eq. (7), this paper assumed the inputs are under multivariable Gaussian distribution. Many real-world datasets break this condition, such as long-tailed distribution. Moreover, the classic random features, for example random Fourier features, have no restriction on data distribution. Besides the Gaussian data assumption, Assumption 1 (iii) also seems to be strict. The authors are expected to provide more examples to illustrate the applicability of these assumptions. 2) The presented asymptotic theory requires the dimension of input space to approach infinity p → ∞
is unfamiliar in practical. Because the dimension of input space is fixed, I wonder that is there still a good approximation between K and K ~ if p
is small? For a given task with a fixed p
, is there a natural gap between K and K ~
? 3) It seems both Theorem 1 and Corollary 1 are independent from the required number of random features m
. In the existing random features literature, m
is crucial to the approximation ability and generalization ability. In general case, m = O ( n )
random features can guarantee the similar generalization ability (Rudi and Rosasco, 2017). The authors may illustrate how the number of ternary random features influence the approximation or generalization. 4) The kernel hyperparameters usually determine the performance of kernel methods, but the proposed random features approach seems to be independent from kernel hyperparameters and only depend on the kernel type. Can TRF approximate any kernel with different hyperparameters? How does TRF remove the influence of kernel hyperparameters?
Rudi A, Rosasco L. Generalization Properties of Learning with Random Features[C]//NIPS. 2017: 3215-3225.

Review Point: 1) My main concern is that the Gaussian mixture data assumption may be too strong. As shown in Eq. (7), this paper assumed the inputs are under multivariable Gaussian distribution. Many real-world datasets break this condition, such as long-tailed distribution. Moreover, the classic random features, for example random Fourier features, have no restriction on data distribution. Besides the Gaussian data assumption, Assumption 1 (iii) also seems to be strict. The authors are expected to provide more examples to illustrate the applicability of these assumptions.
Review Point: 2) The presented asymptotic theory requires the dimension of input space to approach infinity p → ∞ is unfamiliar in practical. Because the dimension of input space is fixed, I wonder that is there still a good approximation between K and K ~ if p is small? For a given task with a fixed p , is there a natural gap between K and K ~ ?
Review Point: 3) It seems both Theorem 1 and Corollary 1 are independent from the required number of random features m . In the existing random features literature, m is crucial to the approximation ability and generalization ability. In general case, m = O ( n ) random features can guarantee the similar generalization ability (Rudi and Rosasco, 2017). The authors may illustrate how the number of ternary random features influence the approximation or generalization.
Review Point: 4) The kernel hyperparameters usually determine the performance of kernel methods, but the proposed random features approach seems to be independent from kernel hyperparameters and only depend on the kernel type. Can TRF approximate any kernel with different hyperparameters? How does TRF remove the influence of kernel hyperparameters? Rudi A, Rosasco L. Generalization Properties of Learning with Random Features[C]//NIPS. 2017: 3215-3225.
==================================================

Focused review:

weaknesses:
(1) The technical novelty is weak, as the contribution is more on the experiments and results rather than the method.
(2) For the threat models:
From Figure 2, Point addition (PA) seems to be just adding random points near the surface, so the attack looks weak.
Line 176 says that a larger number of modifications would be easy to detect. This argument sounds strange, cause this work is to explore how AT (adversarial training) can help to improve the robustness of the trained network, so dropping just 10% points does not sound sufficient. It is better to perform the experiment by trying different (and larger) amount of point drops.
The point drop is only 10%; doing so does not really affect global tasks, particularly classification. See Fig.5 in this paper: "SampleNet: Differentiable Point Cloud Sampling". Dropping large proportion of points (>10%) still can't affect the classification accuracy. Therefore, the setting for PD looks too weak, particularly for the classification experiment.
Also, from Figure 2, it seems that all the salient points were on the bottom of the chair. Does it make sense? Why not consider existing methods for 3D key point detection?
(3) About the third architecture, i.e., PCT:
The description on PCT [17] is too short and not informative in the paper. From supp., it seems that this PCT is developed based on EdgeConv (DGCNN) with k=32 (?). Since this paper [17] is not published yet, should motivate why this is a good point transformer to pick, since there are other point transformers on arxiv, e.g., https://github.com/Strawberry-Eat-Mango/PCT_Pytorch and https://github.com/lucidrains/point-transformer-pytorch
But in fact, why not also try other point transformers at the same time and see if you have consistent results? The result in Section 3.1 seems to suggest that DGCNN is better than PCT [17], but I wonder if it is also true for other transformers (see above).
(4) I can see that the authors have tried their best to discuss the "insight" in each experiment but it is still very hard to understand the connections between the experimental results and the underlying reasons.
Overall, my rating is more borderline. ======================================================
Other comments:
Line 69: please use the full word for AT here. It was defined too early in line 33.
Line 177: what is the meaning of 14,5? 14 or 5?
Missing reference:
Adversarial robustness: From self-supervised pre-training to fine-tuning. CVPR 2020.
Even though this is an exploratory paper that focuses on the experiments, the authors better discuss the limitations of the experiments.

Review Point: 14 or 5? Missing reference: Adversarial robustness: From self-supervised pre-training to fine-tuning. CVPR 2020. Even though this is an exploratory paper that focuses on the experiments, the authors better discuss the limitations of the experiments.
==================================================

Focused review:

- There needs to be a discussion of how this work relates to earlier work by Chang et al. on crowdsourcing synthetic 3D scenes to investigate language grounding (including spatial relation grounding) -- see the detailed comments on related work. - A few details are missing from the exposition and should be clarified: 1) how were specific object instances selected and were they pre-specified for a given crowdworker? 2) Rationale for subsampling only 1/4 of all relations and what relations are excluded due to that? 3) To my understanding, a crowdworker's judgment that a relation is not valid in a presented image is deemed to imply that the relation is not using an intrinsic frame of reference -- is this a reasonable assumption (i.e. are there other cases when a crowdworker might say the relation does not hold)?

Review Point: - There needs to be a discussion of how this work relates to earlier work by Chang et al. on crowdsourcing synthetic 3D scenes to investigate language grounding (including spatial relation grounding) -- see the detailed comments on related work.
Review Point: - A few details are missing from the exposition and should be clarified:
Review Point: 1) how were specific object instances selected and were they pre-specified for a given crowdworker?
Review Point: 2) Rationale for subsampling only 1/4 of all relations and what relations are excluded due to that?
Review Point: 3) To my understanding, a crowdworker's judgment that a relation is not valid in a presented image is deemed to imply that the relation is not using an intrinsic frame of reference -- is this a reasonable assumption (i.e. are there other cases when a crowdworker might say the relation does not hold)?
==================================================

Focused review:

* I found the contributions of the paper to be difficult to follow. It seems the authors’ claimed contribution is (1) the use of an additional weight to adjust for covariate shift, (2) the use of direct density ratio estimation and, (3) asymptotic results showing double robustness. However, the paper as it stands makes it difficult to clearly read these. * The overlap conditions seem to be especially difficult to satisfy in this setting especially for high dimensional covariate sets where overlap violations become very likely (D’Amour, et al. 2017). * The contribution of adding an additional weight to account for covariate shift is an interesting idea but should be put in better context. The idea is similar to the transportability estimator put forth by Barenboim & Pearl (PNAS) & collaborators. Further, the use of direct density estimation for policy evaluation was also proposed recently (Sondhi, et al. (AISTATS 2020)). * It is not clear why two separate weights are required when using direct density ratio estimation. What prevents the ratio being learned for both weights simultaneously? * Typical off-policy evaluation with discrete actions use a rejection step in the estimator (c.f., Dudik, et al. (2011)). However, this does not appear to be mentioned anywhere in the paper or used in the experiments. In practice this can have a dramatic effect on the performance of estimators. It is difficult to parse and understand performance of the proposed method w.r.t. the other methods in light of this. * What is the motivation behind using Nadarya-Watson estimators for the methods compared to? We should expect this estimator to perform very poorly, even under an otherwise competitive alternative approach. Why not also allow those methods the use of direct density ratio estimation? * In the supplement results are included that use self-normalization (i.e., Hajek estimators). However, it appears that the unnormalized version of the proposed method greatly outperform the normalized version. This is incredibly counter intuitive to me, as normalization almost uniformly will improve estimators in practice on most datasets. What is the reasoning / intuition behind this. * The experiments all use 800 samples, this is (a) a _very_ small number and (b) does not give a sense of how any of the estimators will perform as sample size is increased. In practice, we would expect to see at least an order of magnitude more samples than this.

Review Point: * I found the contributions of the paper to be difficult to follow. It seems the authors’ claimed contribution is (1) the use of an additional weight to adjust for covariate shift, (2) the use of direct density ratio estimation and, (3) asymptotic results showing double robustness. However, the paper as it stands makes it difficult to clearly read these.
Review Point: * The overlap conditions seem to be especially difficult to satisfy in this setting especially for high dimensional covariate sets where overlap violations become very likely (D’Amour, et al. 2017).
Review Point: * The contribution of adding an additional weight to account for covariate shift is an interesting idea but should be put in better context. The idea is similar to the transportability estimator put forth by Barenboim & Pearl (PNAS) & collaborators. Further, the use of direct density estimation for policy evaluation was also proposed recently (Sondhi, et al. (AISTATS 2020)).
Review Point: * It is not clear why two separate weights are required when using direct density ratio estimation. What prevents the ratio being learned for both weights simultaneously?
Review Point: * Typical off-policy evaluation with discrete actions use a rejection step in the estimator (c.f., Dudik, et al. (2011)). However, this does not appear to be mentioned anywhere in the paper or used in the experiments. In practice this can have a dramatic effect on the performance of estimators. It is difficult to parse and understand performance of the proposed method w.r.t. the other methods in light of this.
Review Point: * What is the motivation behind using Nadarya-Watson estimators for the methods compared to? We should expect this estimator to perform very poorly, even under an otherwise competitive alternative approach. Why not also allow those methods the use of direct density ratio estimation?
Review Point: * In the supplement results are included that use self-normalization (i.e., Hajek estimators). However, it appears that the unnormalized version of the proposed method greatly outperform the normalized version. This is incredibly counter intuitive to me, as normalization almost uniformly will improve estimators in practice on most datasets. What is the reasoning / intuition behind this.
Review Point: * The experiments all use 800 samples, this is (a) a _very_ small number and (b) does not give a sense of how any of the estimators will perform as sample size is increased. In practice, we would expect to see at least an order of magnitude more samples than this.
==================================================

Focused review:

Weaknesses:
Lack of some critical comparisons: 1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN? 2) What about the inference time (or complexity) of NSB-GAN compared to BigGAN?
If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer.
Overall, the suggested work effectively decreases the training time of the BigGAN using a simple idea. However, there are missing comparisons and analyses such as 1) comparison with pre-trained BigGAN -> ESRGAN 2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.

Review Point: 1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN?
Review Point: 2) What about the inference time (or complexity) of NSB-GAN compared to BigGAN? If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer. Overall, the suggested work effectively decreases the training time of the BigGAN using a simple idea. However, there are missing comparisons and analyses such as 1) comparison with pre-trained BigGAN -> ESRGAN 2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.
==================================================

Focused review:

The paper is not very clearly writtem. Too much information is moved to Appendix. 
Some important details of data collection and data generation are omitted. 
The main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. 
I did not notice many typos. However, the paper organization can be improved. 
For example, data generation and data collection can be described in the same section. 
Also, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. 
Below are some detailed comments/questions: 1. What "in-the-wild" term means? 
2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). 
3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. 
4. Conclusions from Table 4 are missing. 
5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. 

Review Point: 2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided).
Review Point: 3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3.
Review Point: 5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear.
==================================================

Focused review:

Weaknesses
I encourage the authors to put forward the general notion of sparsity that is assumed across the paper (as defined in S ( w )
) early on in the introduction so that reader can follow the ideas put forward in Fig. 1.
One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent.
From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal)
By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?

Review Point: 1. One of the major issues in the context of pruning literatures' results is the use of MNIST, FashionMNIST and CIFAR10 to evaluate the performance of the proposed model. I encourage the authors to further expand the set of dataset-DNN pairs they experiment on in order to incorporate more real-world data and ensure their observations remain consistent. From a more practical perspective, could the authors discuss the absolute limit up to which they can push the sparsity limit of various networks? (Since that is the ultimate goal) By extension, could the authors discuss difference in performance values and PQI at the extreme end of sparsity (highlight in existing results)?
==================================================

Focused review:

Weaknesses:
The authors should provide more details about the derivation of Eq. 3.
ML FF methods mentioned in the related work were not compared with the proposed method.

Review Point: 3. ML FF methods mentioned in the related work were not compared with the proposed method.
==================================================

Focused review:

Weaknesses
While the authors have demonstrated strong results on the ALFRED benchmark with their approach, I feel that the overall idea of using modular components to tackle long-range, target-driven navigation is not new and has been explored sufficiently in prior work (https://arxiv.org/abs/1907.02022, https://arxiv.org/abs/2110.02207, https://arxiv.org/abs/2007.00643, https://arxiv.org/abs/2007.00643, https://openreview.net/pdf?id=HklXn1BKDH, https://arxiv.org/abs/1809.00786). Having said that, I didn't find any particularly novel insight from the perspective of using waypoint-guided navigation via modular methods (the fact that semantic/spatial memories help long range navigation is not very interesting at this point).
While I do agree with the evidence for the claim that FILM generalizes well, however, there seems to be a huge gap between the performance of FILM and other competing approaches under the setting where low-level instructions are used in Tests-Seen (with FILM turning out to be inferior than the rest). This suggests that the proposed approach is not able to make use of the low-level instructions optimally. This is also evidenced from the trends seen in Tab. 2. Can the authors share their thoughts about this?
Within the Language Processing module, the authors use the oracle sub-task decomposition template for each question type and fill in the blanks with the output from the argument predictor. This seems to be privileged information that would greatly help modular methods. the authors must clarify whether all competing methods make use of this information and if not, then does it still remain a valid comparison?
The text (although overall well written), seems to be missing some critical details that might make a full comprehension of their approach problematic. Please see below for specific examples on missing information.
Some questions/clarifications/suggestions
Although not a critical issue at all (doesn't factor into the review), but I felt that the name of their approach FILM might get confused / overloaded with the visual reasoning model with the same name (https://arxiv.org/abs/1709.07871). Just thought that the authors should be aware of this as an FYI, if they are not already.
While going through the Introductory sections of the paper, I felt that a concrete definition (in terms of inputs, outputs and assumptions) of the Embodied Instruction Following (EIF) task as defined on the ALFRED benchmark is missing. Without that, it feels like the authors are talking about EIF in the generic sense of the term. Within that context, it becomes a little confusing when a comparison with VLN is made (because, again, speaking in general terms, VLN is also an instance of EIF). The text later clarifies the exact task definition in the subsequent sections, but I felt that the definition should have come sooner.
The text refers to ALFRED as an environment at one place. I'd consider it as a benchmark (or, a dataset) rather than an environment.
The description of the argument prediction sub-module of the LP module talks about 4 kinds of arguments: “obj”, “recep”, “mrecep” and “sliced”. Whereas, in the figures and in the later sections, there is mention of “parent” being one of the argument types in place of “recep”. The authors should clarify the list of possible argument types and fix the inconsistencies.
The paper doesn't talk in detail about how the low-level instructions are used by their proposed modular approach. What parts of the existing pipeline are modified and how?
What do the authors mean when they say that the task is deemed successful within a tolerance of "10 failed actions”? Do these refer to the low-level actions (forward, left, right, …)? Or, are these high-level goals such as "slice bread” or both? Might be useful for readers who are not aware of the specifics of the ALFRED benchmark evaluation.
Is the aggregation over time steps in the Semantic Map Module done using something like simple averaging? Or, via some sequence models?
It seems like the number of semantic categories whose information is being saved at each of the MxM metric locations on the top-down map is different for different episodes (C+num of sub-goal objects). How is this handled?
Is the agent working with an egocentric or an allocentric map? The Figures seem to suggest an allocentric map. But the description of the semantic search policy states that the GT location for small objects is computed every 25 steps, which seems to suggest an egocentric map. The authors should consider making this explicit in the text.
How is the overall framework keeping track of completed sub-tasks and deciding to advance to the next one?

Review Point: 2. Can the authors share their thoughts about this? Within the Language Processing module, the authors use the oracle sub-task decomposition template for each question type and fill in the blanks with the output from the argument predictor. This seems to be privileged information that would greatly help modular methods. the authors must clarify whether all competing methods make use of this information and if not, then does it still remain a valid comparison? The text (although overall well written), seems to be missing some critical details that might make a full comprehension of their approach problematic. Please see below for specific examples on missing information. Some questions/clarifications/suggestions Although not a critical issue at all (doesn't factor into the review), but I felt that the name of their approach FILM might get confused / overloaded with the visual reasoning model with the same name (https://arxiv.org/abs/1709.07871). Just thought that the authors should be aware of this as an FYI, if they are not already. While going through the Introductory sections of the paper, I felt that a concrete definition (in terms of inputs, outputs and assumptions) of the Embodied Instruction Following (EIF) task as defined on the ALFRED benchmark is missing. Without that, it feels like the authors are talking about EIF in the generic sense of the term. Within that context, it becomes a little confusing when a comparison with VLN is made (because, again, speaking in general terms, VLN is also an instance of EIF). The text later clarifies the exact task definition in the subsequent sections, but I felt that the definition should have come sooner. The text refers to ALFRED as an environment at one place. I'd consider it as a benchmark (or, a dataset) rather than an environment. The description of the argument prediction sub-module of the LP module talks about 4 kinds of arguments: “obj”, “recep”, “mrecep” and “sliced”. Whereas, in the figures and in the later sections, there is mention of “parent” being one of the argument types in place of “recep”. The authors should clarify the list of possible argument types and fix the inconsistencies. The paper doesn't talk in detail about how the low-level instructions are used by their proposed modular approach. What parts of the existing pipeline are modified and how? What do the authors mean when they say that the task is deemed successful within a tolerance of "10 failed actions”? Do these refer to the low-level actions (forward, left, right, …)? Or, are these high-level goals such as "slice bread” or both? Might be useful for readers who are not aware of the specifics of the ALFRED benchmark evaluation. Is the aggregation over time steps in the Semantic Map Module done using something like simple averaging? Or, via some sequence models? It seems like the number of semantic categories whose information is being saved at each of the MxM metric locations on the top-down map is different for different episodes (C+num of sub-goal objects). How is this handled? Is the agent working with an egocentric or an allocentric map? The Figures seem to suggest an allocentric map. But the description of the semantic search policy states that the GT location for small objects is computed every 25 steps, which seems to suggest an egocentric map. The authors should consider making this explicit in the text. How is the overall framework keeping track of completed sub-tasks and deciding to advance to the next one?
==================================================

Focused review:

Weaknesses: - It is obvious that there is a strong connection between PCA and linear regression (see for example [41]), thus the originality of the general idea is limited. - The spiked covariance model for PCA is quite restrictive (for example, Gaussian white noise) and unlikely to be met in practical applications. In fact Gaussianity is crucial for this approach, which makes the results less significant. - The assumption that the number of nonzero components of u are known is strong (though the authors argue that there are adaptive methods to adjust this). - It is not clear how easy it is to check / verify for a particular problem and SLR method that Condition 2.4 holds. It looks like a strong assumptions which is hard to verify. Ideally, the authors should have demonstrated verifying this condition on an example. - The fact that the underlying SLR method is treated as a black-box might hide the problem of selecting the appropriate SLR method. Which one should be used for various problems? Minor comments: - The authors argue that though random design matrices for linear regression can arise, it makes no difference to assume that it is deterministic (by conditioning). This argument is a bit misleading, as it is only true if the design matrix (\mathbb{X}) and the noise vector (w) affecting the observations are independent. This is not the case, for example, if the linear regression problem arises from a time-series problem, such as estimating the parameters of an autoregressive model (in which case the design matrix cannot be assumed to be deterministic). - It is not a good practice to cite works that are only available on arXiv (as they did not go through any review process, they could contain unsupported claims, etc.). Post-rebuttal comments: Thank you for your replies. It is a nice feature that the presented approach can turn a black-box SLR to a SPCA solver. Nevertheless, it requires strong assumptions, for example, the spiked covariance model, knowledge of sparsity, and RE, which limit its theoretical and practical relevance. I understand that these assumptions could be relaxed a bit, for example, the method could be extended to sub-Gaussian variables and unknown sparsity could be handled by binary search (for hypothesis testing). It would be good to discuss these issues in more detail in the revised paper. 

Review Point: - It is obvious that there is a strong connection between PCA and linear regression (see for example [41]), thus the originality of the general idea is limited.
Review Point: - The spiked covariance model for PCA is quite restrictive (for example, Gaussian white noise) and unlikely to be met in practical applications. In fact Gaussianity is crucial for this approach, which makes the results less significant.
Review Point: - The assumption that the number of nonzero components of u are known is strong (though the authors argue that there are adaptive methods to adjust this).
Review Point: - It is not clear how easy it is to check / verify for a particular problem and SLR method that Condition 2.4 holds. It looks like a strong assumptions which is hard to verify. Ideally, the authors should have demonstrated verifying this condition on an example.
Review Point: - The fact that the underlying SLR method is treated as a black-box might hide the problem of selecting the appropriate SLR method. Which one should be used for various problems? Minor comments:
Review Point: - The authors argue that though random design matrices for linear regression can arise, it makes no difference to assume that it is deterministic (by conditioning). This argument is a bit misleading, as it is only true if the design matrix (\mathbb{X}) and the noise vector (w) affecting the observations are independent. This is not the case, for example, if the linear regression problem arises from a time-series problem, such as estimating the parameters of an autoregressive model (in which case the design matrix cannot be assumed to be deterministic).
Review Point: - It is not a good practice to cite works that are only available on arXiv (as they did not go through any review process, they could contain unsupported claims, etc.). Post-rebuttal comments: Thank you for your replies. It is a nice feature that the presented approach can turn a black-box SLR to a SPCA solver. Nevertheless, it requires strong assumptions, for example, the spiked covariance model, knowledge of sparsity, and RE, which limit its theoretical and practical relevance. I understand that these assumptions could be relaxed a bit, for example, the method could be extended to sub-Gaussian variables and unknown sparsity could be handled by binary search (for hypothesis testing). It would be good to discuss these issues in more detail in the revised paper.
==================================================

Focused review:

From an imitation learning perspective, the notions of imitability and p-imitability are excessively strong. In practice, one is typically not interested in ensuring that P(y | do(pi)) = P(y), but rather in maximising E[y | pi]. The relatively strong conditions required for Algorithm 1 (existence of an i-instrument, availability of the causal graph) will still rule out many situations where "imitation" (in the colloquial sense of successfully solving roughly the same task as the demonstrator) is still possible. Relatedly, the requirement for a causal graph that _includes the latent reward variable_ is also very strong. Imitation learning is useful in settings where it's hard to define an appropriate reward function by hand, and therefore hard to do planning/RL directly. However, this method additionally requires that it's easy to identify a small subset of state variables that the reward function could depend on, which seems like a setting where it would also be easy to define the reward function by hand. - The specific motivating example used in the introduction is dubious—see comments on related work. - The writing is difficult to follow at times—see comments on clarity.

Review Point: - The specific motivating example used in the introduction is dubious—see comments on related work.
Review Point: - The writing is difficult to follow at times—see comments on clarity.
==================================================

Focused review:

- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. 
If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. 
- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? 

Review Point: - The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.
Review Point: - The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ?
==================================================

Focused review:

1. The assumption that the testing distribution were available seems unrealistic. In machine learning, test set is used to provide an unbiased evaluation of a final model fit on the training dataset, especially the generalization ability. If we assume that the testing distribution were available, then the model will be oriented to fit the test set. In this situation, the test set cannot give unbiased evaluation. The authors should give more explanation for the assumption and show that the assumption can be used in practice. 2. The novelty of this paper is limited. The construction of the proposed unbiased risk estimator is a common technology widely used in lots of learning settings, for example in the Positive Unlabeled Learning. 3. In Theorem 4, ‘…with high probability…”is not clear. The probability should be quantitatively. 4. In the experiment, comparison on deep models, the paper proposes that ‘The unlabeled data for EULAC are sampled from the testing data and the rest are used for evaluation’. What are the testing data used in other method? All the testing data or same with EULAC? Overall, this paper shows some math skills, but does not slove any real problems in practice and does not conduct strong theoretical analysis. ------------ I read the rebuttal. In the rebuttal, the author cliams that " We do NOT use any testing sample in training time. Instead, we try to approximate the testing distribution by only labeled and unlabeled training data". Obviously, it is a big erroneous. The paper study the problem of learning with augmented classes (LAC), where new classes that do not appear in the training data might emerge in the testing phase. The basic assumption is that the testing data have new classes which is not appear in the training data. Based on this setting. The distribution of training data and testing data should be totally different. But the author use training data to approximate the testing distribution. It is contradictory and not correct. In the rebuttal, the author cliams that "we are actually studying the same setting as previous works [3,15]". I carefully checked reference [15]. However, I find the setting of [15] is totally different from this paper. [15] only requires unlabeled samples drawn from a mixture of nominal data distribution and unknown alien distribution, but the mixture proportion $\theta$ of the unlabeled samples is not required to be same with the test data. It means the distribution of test data can be different with the unlabeled data. However, in this paper, the mixture proportion $\theta$ is necessary, and the distribution of test data must be same with the distribution. In paper [15], it is presented that “At training time, we assume that $D_m$ is a mixture distribution, with probability α of generating an alien data point from Da and probability of $1 − \alpha$ of generating a nominal point. Our results hold even if the test queries come from a mixture with a different value of α as long as the alien test points are drawn from $D_a$.” In addition, my concern about Theorem 4 is that ‘…with high probability…”is not clear. The probability should be quantitatively defined. The author did not address my concern about the Theorem 4.

Review Point: 1. The assumption that the testing distribution were available seems unrealistic. In machine learning, test set is used to provide an unbiased evaluation of a final model fit on the training dataset, especially the generalization ability. If we assume that the testing distribution were available, then the model will be oriented to fit the test set. In this situation, the test set cannot give unbiased evaluation. The authors should give more explanation for the assumption and show that the assumption can be used in practice.
Review Point: 2. The novelty of this paper is limited. The construction of the proposed unbiased risk estimator is a common technology widely used in lots of learning settings, for example in the Positive Unlabeled Learning.
Review Point: 3. In Theorem 4, ‘…with high probability…”is not clear. The probability should be quantitatively.
Review Point: 4. In the experiment, comparison on deep models, the paper proposes that ‘The unlabeled data for EULAC are sampled from the testing data and the rest are used for evaluation’. What are the testing data used in other method? All the testing data or same with EULAC? Overall, this paper shows some math skills, but does not slove any real problems in practice and does not conduct strong theoretical analysis. ------------ I read the rebuttal. In the rebuttal, the author cliams that " We do NOT use any testing sample in training time. Instead, we try to approximate the testing distribution by only labeled and unlabeled training data". Obviously, it is a big erroneous. The paper study the problem of learning with augmented classes (LAC), where new classes that do not appear in the training data might emerge in the testing phase. The basic assumption is that the testing data have new classes which is not appear in the training data. Based on this setting. The distribution of training data and testing data should be totally different. But the author use training data to approximate the testing distribution. It is contradictory and not correct. In the rebuttal, the author cliams that "we are actually studying the same setting as previous works [3,15]". I carefully checked reference [15]. However, I find the setting of [15] is totally different from this paper. [15] only requires unlabeled samples drawn from a mixture of nominal data distribution and unknown alien distribution, but the mixture proportion $\theta$ of the unlabeled samples is not required to be same with the test data. It means the distribution of test data can be different with the unlabeled data. However, in this paper, the mixture proportion $\theta$ is necessary, and the distribution of test data must be same with the distribution. In paper [15], it is presented that “At training time, we assume that $D_m$ is a mixture distribution, with probability α of generating an alien data point from Da and probability of $1 − \alpha$ of generating a nominal point. Our results hold even if the test queries come from a mixture with a different value of α as long as the alien test points are drawn from $D_a$.” In addition, my concern about Theorem 4 is that ‘…with high probability…”is not clear. The probability should be quantitatively defined. The author did not address my concern about the Theorem 4.
==================================================

Focused review:

Weaknesses：
The abstract and the introduction section does not state the background of the task, the motivation of the method and where the innovation lies. Instead, the authors define the hypergraph, knowledge hypergraph, and hypergraph prediction tasks in this section, which I think should be explained in the model section.
The captions for Figures 2 and 3 are too long. The description of the model should be placed in the body of the text.
The composition of the pipeline should be further rethought to allow the reader to better understand the model details.
What are equations 1 and 2 for? They are not used in the subsequent objective function Eq. 6.
The logic of the article is not clear, making it difficult for the reader to catch the point. The authors should rethink the organization of their paper to better explain their work.

Review Point: 6. The logic of the article is not clear, making it difficult for the reader to catch the point. The authors should rethink the organization of their paper to better explain their work.
==================================================

Focused review:

1) Lack of interpretability: There could be more of a discussion of why "semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. 
2) It will be interesting to how this method scales with respect to more complex mathematical questions. 
3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. 
The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior. 

Review Point: 1) Lack of interpretability: There could be more of a discussion of why "semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers". This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others.
Review Point: 2) It will be interesting to how this method scales with respect to more complex mathematical questions.
Review Point: 3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's behavior.
==================================================

Focused review:

Weaknesses - Since it is difficult to evaluate image generation tasks quantitatively, a human study could be performed where users need to select their preferred generated image. This evaluation could help showing the increased visual quality with respect to other methods. - Since Equation 5 is applied multiple times it should contain some notion of iterations (e.q. M_w^{(i)}. Otherwise M_w appears on both sides of the equation.  - An interesting evaluation could be correlating reconstruction quality and RS measure. This could show that the loss actually helps finding the right correspondences between patches. Minor Comments - L. 82: {1, 2, â¦, n} - The type of upsampling for the feature maps before concatenation is not mentioned.  - Since the ID-MRF is only used in training it would be interesting to report training times or even time per image during training compared to inference.  - Potential typo in Equation (4). L_M(conv4_2) appears twice.  - The intuitive example for L_M from the supplementary material could be included in the paper. I found it easier to follow than the explanations in lines 117-128. - L. 224: our method is still difficult to deal with large-scale -> still has difficulties dealing with [â¦]  Rating & Evaluation Given the above strengths and weaknesses I am in favor of accepting this submission. A user study and a more in-depth analysis of the proposed MRF loss could make this paper even stronger.  -- After reading the other reviews and the rebuttal I find most of my concerns addressed. The authors provide several additional quantitative results including a user study with convincing outcome. I vote in favor of accepting the paper.

Review Point: - Since it is difficult to evaluate image generation tasks quantitatively, a human study could be performed where users need to select their preferred generated image. This evaluation could help showing the increased visual quality with respect to other methods.
==================================================

Focused review:

* The formal analysis focuses only on the first layer. (This is not a major weakness.) In section 2.5, experiments with deeper layers are discussed. This discussion assumes that what's going on at the first layer is representative of deeper layers as well. However, the paper also discusses the task specification that occurs at deeper levels. It seems there will be a trade off between the bottom-up eigenspace alignment and the top-down task-specificity. The theoretical relationship between these two forces is left for future work. * The authors propose no broader societal or ethical impacts of their work. I think an argument can be made that work on understanding deep learning can contribute to interpretable and explainable AI, which has potential to help identify sources of bias, for example. I encourage the authors to try to write something more meaningful in that section.

Review Point: * The formal analysis focuses only on the first layer. (This is not a major weakness.) In section 2.5, experiments with deeper layers are discussed. This discussion assumes that what's going on at the first layer is representative of deeper layers as well. However, the paper also discusses the task specification that occurs at deeper levels. It seems there will be a trade off between the bottom-up eigenspace alignment and the top-down task-specificity. The theoretical relationship between these two forces is left for future work.
Review Point: * The authors propose no broader societal or ethical impacts of their work. I think an argument can be made that work on understanding deep learning can contribute to interpretable and explainable AI, which has potential to help identify sources of bias, for example. I encourage the authors to try to write something more meaningful in that section.
==================================================

Focused review:

- The key claim made by this paper is that history-dependent BC policy (BC-MO) suffers from the copycat problem because the history contains enough information about the past action taken by the policy. However, the motivating example presented in Table 1 and line 132-145 is not really a convincing evidence for the copycat problem. An alternative explanation for the result is that the BC policy is known to suffer from the mode collapse problem, where if there are contradicting supervisions given the same state due to noises, BC learner tend to "average out" the noise, making the action more predictable. A more convincing example would be to show that the actions from a history-independent BC policy (BC-SO) is more difficult to predict than that of the BC-MO policy. - Another implicit assumption made by the paper is that learning to infer past actions (a_{t-N}, N > 0) from observation histories are easier than learning to predict the action directly from the observations. This claim might be true for most environments, but would there be cases where this is not true? I'd like to see a conceptual analysis of this assumption in the paper. - The Information Bottleneck (IB) seem ad-hoc and orthogonal to the main claim of the paper. The intuition behind IB provided by the paper "Intuitively, the IB implements a penalty for every bit that E transmits to F, encouraging it to transmit only the most essential information" is a bit hand-wavy and it'd be great to see some theoretical justifications to this claim.

Review Point: - The key claim made by this paper is that history-dependent BC policy (BC-MO) suffers from the copycat problem because the history contains enough information about the past action taken by the policy. However, the motivating example presented in Table 1 and line 132-145 is not really a convincing evidence for the copycat problem. An alternative explanation for the result is that the BC policy is known to suffer from the mode collapse problem, where if there are contradicting supervisions given the same state due to noises, BC learner tend to "average out" the noise, making the action more predictable. A more convincing example would be to show that the actions from a history-independent BC policy (BC-SO) is more difficult to predict than that of the BC-MO policy.
Review Point: - Another implicit assumption made by the paper is that learning to infer past actions (a_{t-N}, N > 0) from observation histories are easier than learning to predict the action directly from the observations. This claim might be true for most environments, but would there be cases where this is not true? I'd like to see a conceptual analysis of this assumption in the paper.
Review Point: - The Information Bottleneck (IB) seem ad-hoc and orthogonal to the main claim of the paper. The intuition behind IB provided by the paper "Intuitively, the IB implements a penalty for every bit that E transmits to F, encouraging it to transmit only the most essential information" is a bit hand-wavy and it'd be great to see some theoretical justifications to this claim.
==================================================

Focused review:

Weakness: The paper appears to be technically sound, but the experiments are limited without much explanation. Since the paper is partially inspired by [16], a comparison may be included to further illustrate the effectiveness of the proposed method. Some related references are missing: Li et al., Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via Thresholded Wirtinger Flow, 2016 Yang et al. Misspecified Nonconvex Statistical Optimization for Sparse Phase Retrieval, 2017 Li et al. Symmetry, Saddle Points and Global Optimization Landscape of Nonconvex Matrix Factorizaiton, 2016 Ge et al. No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis, 2017 There are also some minor issues (tyÂ¬Â¬pos): 1. The letter $m$ stands for two quantities simultaneously: the dimension of the observed signal y and the number of samples; 2. The words font does not follow the standard NIPS template; 3. Figures may be enlarged, since some content and axes are hardly readable.

Review Point: 1. The letter $m$ stands for two quantities simultaneously: the dimension of the observed signal y and the number of samples;
Review Point: 2. The words font does not follow the standard NIPS template;
Review Point: 3. Figures may be enlarged, since some content and axes are hardly readable.
==================================================

Focused review:

（1） The paper assumes a full-information reward feedback, which can be hardly thought as a realistic assumption. Instead, it would be much appreciated to consider the bandit feedback as what [1] does. （2） Moreover, the paper assumes the access of upper bounds of P_T and D_T. This is undesired in practice. There are some recent efforts in removing such dependency [2,3]. The basic idea is to run another meta bandits algorithm for selecting the optimal parameter. The techniques, in my opinion, are very possible to remove these factors. （3） The authors claim that one of the advantages of proposed algorithm comparing to previous non-stationary RL algorithms is that the algorithm is model-free and hence is more efficient in both time and space complexity. I am not aware whether the argument holds in general. I believe authors should ellaborate more on this issue. Additionally, empirical evaluations should be carried out to support the claim. -------------------- Other comment: The idea of restarting the algorithm for fighting non-stationarity is not novel. I appreciate the discussion below Algorithm 1. I'd like to add two more papers that also use restart method for handling non-stationary bandits: [4] use restart for MAB with abrupt changes; [5] use restart for linear bandits with changing regression parameter. Some minor issues: - In appendix line 541. this is a compiling error "defined in Line ?? of " Given above concerns, I can only recommend for weak acceptance. ------------------------------------ Reference [1] Efroni, Y., Shani, L., Rosenberg, A., & Mannor, S. (2020). Optimistic Policy Optimization with Bandit Feedback. arXiv preprint arXiv:2002.08243. [2] Cheung, W. C., Simchi-Levi, D., & Zhu, R. (2020). Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism. arXiv preprint arXiv:2006.14389. [3] Cheung, W. C., Simchi-Levi, D., & Zhu, R. (2019, April). Learning to optimize under non-stationarity. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 1079-1087). [4] Allesiardo, R., Féraud, R., & Maillard, O. A. (2017). The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4), 267-283. [5] Zhao, P., Zhang, L., Jiang, Y., & Zhou, Z. H. (2020). A simple approach for non-stationary linear bandits. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, AISTATS (Vol. 2020).

Review Point: - In appendix line 541. this is a compiling error "defined in Line ?? of " Given above concerns, I can only recommend for weak acceptance. ------------------------------------ Reference [1] Efroni, Y., Shani, L., Rosenberg, A., & Mannor, S. (2020). Optimistic Policy Optimization with Bandit Feedback. arXiv preprint arXiv:2002.08243. [2] Cheung, W. C., Simchi-Levi, D., & Zhu, R. (2020). Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism. arXiv preprint arXiv:2006.14389. [3] Cheung, W. C., Simchi-Levi, D., & Zhu, R. (2019, April). Learning to optimize under non-stationarity. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 1079-1087). [4] Allesiardo, R., Féraud, R., & Maillard, O. A. (2017). The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4), 267-283. [5] Zhao, P., Zhang, L., Jiang, Y., & Zhou, Z. H. (2020). A simple approach for non-stationary linear bandits. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, AISTATS (Vol. 2020).
==================================================

Focused review:

1) Why symmetry is very important? 2) In deep learning, especially in practical tasks, e.g., classification etc., does symmetry affect the performance of neural networks？ 3) The training mechanism is quite common. In my opinion, this work is a slightly-modified version of GAN+VAE framework. Please illustrate more insightful contents or the major differences

Review Point: 2) In deep learning, especially in practical tasks, e.g., classification etc., does symmetry affect the performance of neural networks？ 3) The training mechanism is quite common. In my opinion, this work is a slightly-modified version of GAN+VAE framework. Please illustrate more insightful contents or the major differences
==================================================

Focused review:

Weaknesses: I'd like to see more analysis of certain dependency structures. I'm particularly interested in how coordination and relative clauses are handled when the predicate argument structure of CCG is at odds with the dependency structures normally used by other dependency parsers.
- General Discussion: I'm very happy with this work and feel it's a very nice contribution to the literature. The only thing missing for me is a more in-depth analysis of the types of constructions which saw the most improvement (English and Japanese) and a discussion (mentioned above) reconciling Pred-Arg dependencies with those of other parsers. 

Review Point: - General Discussion: I'm very happy with this work and feel it's a very nice contribution to the literature. The only thing missing for me is a more in-depth analysis of the types of constructions which saw the most improvement (English and Japanese) and a discussion (mentioned above) reconciling Pred-Arg dependencies with those of other parsers.
==================================================

Focused review:

Weaknesses
- More evaluation would have been welcome, especially on CIFAR-10 in the full label and lower label scenarios.
- The CIFAR-10 results are a little disappointing with respect to temporal ensembles (although the results are comparable and the proposed approach has other advantages)
- An evaluation on the more challenging STL-10 dataset would have been welcome. Comments
- The SVNH evaluation suggests that the model is better than pi an temporal ensembling especially in the low-label scenario. With this in mind, it would have been nice to see if you can confirm this on CIFAR-10 too (i.e. show results on CIFAR-10 with less labels)
- I would would have like to have seen what the CIFAR-10 performance looks like with all labels included.
- It would be good to include in the left graph in fig 3 the learning curve for a model without any mean teacher or pi regularization for comparison, to see if mean teacher accelerates learning or slows it down.
- I'd be interested to see if the exponential moving average of the weights provides any benefit on it's own, without the additional consistency cost.

Review Point: - More evaluation would have been welcome, especially on CIFAR-10 in the full label and lower label scenarios.
Review Point: - The CIFAR-10 results are a little disappointing with respect to temporal ensembles (although the results are comparable and the proposed approach has other advantages) - An evaluation on the more challenging STL-10 dataset would have been welcome. Comments - The SVNH evaluation suggests that the model is better than pi an temporal ensembling especially in the low-label scenario. With this in mind, it would have been nice to see if you can confirm this on CIFAR-10 too (i.e. show results on CIFAR-10 with less labels) - I would would have like to have seen what the CIFAR-10 performance looks like with all labels included.
Review Point: - It would be good to include in the left graph in fig 3 the learning curve for a model without any mean teacher or pi regularization for comparison, to see if mean teacher accelerates learning or slows it down.
Review Point: - I'd be interested to see if the exponential moving average of the weights provides any benefit on it's own, without the additional consistency cost.
==================================================

Focused review:

The empirical evaluation is one of the weakest aspects of the paper. The fact that this is done on only one, seemingly arbitrarily chosen, dataset diminishes the significance of the results. I would have liked to see evaluation on more standard datasets. There are some aspects of the biological mapping that may not be biologically plausible: - Only linear model are considered. In biology, pyramidal cells are known to have many non-linear effects. - Inhibition is considered with tied input and output weights. Need more details on which type of inhibitory neurons ave this property. - Distal plasticity ignores known data from (Milstein et al. 2020; Magee and Grienberger 2020 [1]) the first of which is cited in the paper for other reasons. - The authors don't consider dendritic spikes and calcium plateau potentials as events. - V_y updates use activity of soma and input to distal compartments, which is quite unusual for biological rules. - Ca^{2+} plateau potential are generally know to affect distal dendrites, which is not considered here. - They don't consider binary or spiking data which could be more biologically plausible. An analysis of what happens when moving from full dataset to single samples would have been useful. The authors consider algorithms that are used for dimensionality reduction (as mentioned in their discussion), but need to specify which pyramidal cells in the brain are known to have this property. A discussion of how representations learnt using RRMSE or CCA can be used by further processing layers can strenghten the paper. [1] Magee, J.C., and Grienberger, C. (2020). Synaptic Plasticity Forms and Functions. Annual Review of Neuroscience 43.

Review Point: - Only linear model are considered. In biology, pyramidal cells are known to have many non-linear effects.
Review Point: - Inhibition is considered with tied input and output weights. Need more details on which type of inhibitory neurons ave this property.
Review Point: - Distal plasticity ignores known data from (Milstein et al. 2020; Magee and Grienberger 2020 [1]) the first of which is cited in the paper for other reasons.
Review Point: - The authors don't consider dendritic spikes and calcium plateau potentials as events.
Review Point: - V_y updates use activity of soma and input to distal compartments, which is quite unusual for biological rules.
Review Point: - Ca^{2+} plateau potential are generally know to affect distal dendrites, which is not considered here.
Review Point: - They don't consider binary or spiking data which could be more biologically plausible. An analysis of what happens when moving from full dataset to single samples would have been useful. The authors consider algorithms that are used for dimensionality reduction (as mentioned in their discussion), but need to specify which pyramidal cells in the brain are known to have this property. A discussion of how representations learnt using RRMSE or CCA can be used by further processing layers can strenghten the paper. [1] Magee, J.C., and Grienberger, C. (2020). Synaptic Plasticity Forms and Functions. Annual Review of Neuroscience 43.
==================================================

Focused review:

1. The authors did not clearly define 'relevance score'. 2. The proposed theorems, do not address the 'relevance score' effect for either semi-supervised or self-supervised learning scenarios. 3. I guess section 3 is largely used to motivate section 4, but I am not sure if you have experiments showing that self-supervised learning is more resistant to 'non-relevant unlabeled data'. I guess self-supervised learning would still be affected if the unlabeled data is totally non-relevant. Like using sounds to improve speech recognition. 4. Title seems to be a little bit misleading, actually, in the SSL scenarios, pseudo labels (and also relevance score) matters; While in the self-supervised learning scenario, the amount of unlabeled data matters. 5. Though very motivating, the theorems need to be further developed to explain more complex models.

Review Point: 2. The proposed theorems, do not address the 'relevance score' effect for either semi-supervised or self-supervised learning scenarios.
Review Point: 3. I guess section 3 is largely used to motivate section 4, but I am not sure if you have experiments showing that self-supervised learning is more resistant to 'non-relevant unlabeled data'. I guess self-supervised learning would still be affected if the unlabeled data is totally non-relevant. Like using sounds to improve speech recognition.
Review Point: 4. Title seems to be a little bit misleading, actually, in the SSL scenarios, pseudo labels (and also relevance score) matters; While in the self-supervised learning scenario, the amount of unlabeled data matters.
Review Point: 5. Though very motivating, the theorems need to be further developed to explain more complex models.
==================================================

Focused review:

I have a concern about the paper's novelty. From my understanding present work mostly combines two existed ideas: 1) training sparser networks via approximating an intractable l0-regularization penalty and 2) finding lottery ticket hypothesis under a framework similar to IMP. For the first part, the authors didn't clearly differentiate the present l0-approximation method from previous works. For the second part, the authors did mention that the rewinding for weights is related (to early-stage instead of original initialization) but unfortunately I think it is just a minor change. I believe the novelty of this work depends on the part of l0-approximation and it would be great if the authors can clarify it further. EDIT: Thanks for the authors' response. It clearly alleviates my concern of novelty of l0 approximation. Moreover, other reviews give me a better understanding of this paper. I would like to increase my score and champion the paper for acceptance.

Review Point: 1) training sparser networks via approximating an intractable l0-regularization penalty and 2) finding lottery ticket hypothesis under a framework similar to IMP. For the first part, the authors didn't clearly differentiate the present l0-approximation method from previous works. For the second part, the authors did mention that the rewinding for weights is related (to early-stage instead of original initialization) but unfortunately I think it is just a minor change. I believe the novelty of this work depends on the part of l0-approximation and it would be great if the authors can clarify it further. EDIT: Thanks for the authors' response. It clearly alleviates my concern of novelty of l0 approximation. Moreover, other reviews give me a better understanding of this paper. I would like to increase my score and champion the paper for acceptance.
==================================================

Focused review:

1. I believe this paper could provide even more insight if it includes a brief discussion on single task models, especially because in the experiment section DAG-GP is compared against them. 2. Is it possible to provide more details on how the synthetic data are generated, potentially in supplementary?

Review Point: 1. I believe this paper could provide even more insight if it includes a brief discussion on single task models, especially because in the experiment section DAG-GP is compared against them.
Review Point: 2. Is it possible to provide more details on how the synthetic data are generated, potentially in supplementary?
==================================================

Focused review:

Weaknesses ---
Other than the SIG module, the overall novelty of this paper is limited, in comparison to SAT. However, the proposed LAR model has to explicitly employ the SIG module both at the training and testing stages, it thus requires more computational budgets than the previous 2D-3D combined methods, such as SAT. The computational cost of the LAR model should be discussed.
The SIG module may require accurate object proposals for the rendering of 2D images, which is okay for Nr3D and Sr3D, but for actual scenarios such as ScanRefer, how to maintain the reliability of SIG-generated 2D data is not guaranteed. How will the performance degrade if the proposals are predicted?
Even with a bunch of sophisticated fusion techniques and extra computational cost introduced by the 2D visual stream, the proposed method just achieve marginal gains than SAT, which harms the significance of the claimed contributions.
The presentation of this paper needs significant improvement. For example,
Tab. 2: What do referit3D and non-SAT mean in this table?
Tab. 3: is SAT †
means the re-implemented SAT?
How the camera and image augmentations are applied in the experiments other than Tab. 4.
A number of typos and grammar errors throughout the whole paper.
Yes, the authors have adequately addressed the limitations and potential negative societal impact of their work.

Review Point: --- Other than the SIG module, the overall novelty of this paper is limited, in comparison to SAT. However, the proposed LAR model has to explicitly employ the SIG module both at the training and testing stages, it thus requires more computational budgets than the previous 2D-3D combined methods, such as SAT. The computational cost of the LAR model should be discussed. The SIG module may require accurate object proposals for the rendering of 2D images, which is okay for Nr3D and Sr3D, but for actual scenarios such as ScanRefer, how to maintain the reliability of SIG-generated 2D data is not guaranteed. How will the performance degrade if the proposals are predicted? Even with a bunch of sophisticated fusion techniques and extra computational cost introduced by the 2D visual stream, the proposed method just achieve marginal gains than SAT, which harms the significance of the claimed contributions. The presentation of this paper needs significant improvement. For example, Tab.
Review Point: 2: What do referit3D and non-SAT mean in this table? Tab.
Review Point: 3: is SAT † means the re-implemented SAT? How the camera and image augmentations are applied in the experiments other than Tab.
==================================================

Focused review:

Weakness
How is sampling achieved for generating diverse molecules for a specific pocket? In the paper, only part mentioning it refers to Nucleus [68], does it imply that sampling is only involved after generating molecule from the shape?
Efficiency is neither compared nor discussed. The authors make a fair argument in the introduction about the drawback of GEKO, which performs equally powerful as the proposed method. Given the proposed method also takes tons of training time, it would be good to see the comparison and discussion.
Many discussions or relations to the literature are not discussed. e.g. how tokenization and lineraization are related to the literature of graph generation, framgment/scaffold-based molecule generation, etc.
Experiments are only done over 12 protein targets, I would suggest adding more experiments, e.g. 100 targets in 3D SBDD paper.
Ablation study is not complete, the comparison method, especially deep learning-based SBDD, 3D SBDD and liGAN are based on atomic reconstructions, it would be interesting to see whether the proposed model benefit from fragment-based method or the new training framework.
Many details are missing or less mentioned, For example, how the post-processing is done. How many bins are cut for the rotation and translation operations, where the origin is, how the tranformation is done, etc. In training & decoding section 2.4, only ligands data are mentioned while no protein data are mentioned. Figure 10 is not clear, not sure what shape, rotation, translation and category mean.
Codes are not uploaded, which hinders the reproducibility of this work. It would be beneficial to the community if authors consider publishing the codes if the paper is accepted.
Overall, I think this paper makes a fair point and it is a good attempt in the direction of pocket-conditioned ligand design. I am willing to raise my score if the questions get answered.
The limitations are not discussed in the paper but may come from two sides: (1) whether this method can generate diverse ligands that bind to protein targets, (2) the efficiency of the method. The first one concerns whether the formulation makes sense in the biological context, or if not, is still a good way to leverage unlabeled data. The second one needs a bit more experiments to support.

Review Point: 100 targets in 3D SBDD paper. Ablation study is not complete, the comparison method, especially deep learning-based SBDD, 3D SBDD and liGAN are based on atomic reconstructions, it would be interesting to see whether the proposed model benefit from fragment-based method or the new training framework. Many details are missing or less mentioned, For example, how the post-processing is done. How many bins are cut for the rotation and translation operations, where the origin is, how the tranformation is done, etc. In training & decoding section 2.4, only ligands data are mentioned while no protein data are mentioned. Figure 10 is not clear, not sure what shape, rotation, translation and category mean. Codes are not uploaded, which hinders the reproducibility of this work. It would be beneficial to the community if authors consider publishing the codes if the paper is accepted. Overall, I think this paper makes a fair point and it is a good attempt in the direction of pocket-conditioned ligand design. I am willing to raise my score if the questions get answered. The limitations are not discussed in the paper but may come from two sides: (1) whether this method can generate diverse ligands that bind to protein targets, (2) the efficiency of the method. The first one concerns whether the formulation makes sense in the biological context, or if not, is still a good way to leverage unlabeled data. The second one needs a bit more experiments to support.
==================================================

Focused review:

- Although combining the static and dynamic pruning method is interesting, the proposed method looks like a incremental work on previous dynamic pruning method, which indeed limits the contribution of this work. - DRL methods usually introduce considerable extra cost. There is no analysis on the cost of the pruning process. - DRL-based methods usually are harder to implement. Since code is not provided, I have concerns on the reproducibility of this paper.

Review Point: - Although combining the static and dynamic pruning method is interesting, the proposed method looks like a incremental work on previous dynamic pruning method, which indeed limits the contribution of this work.
Review Point: - DRL methods usually introduce considerable extra cost. There is no analysis on the cost of the pruning process.
Review Point: - DRL-based methods usually are harder to implement. Since code is not provided, I have concerns on the reproducibility of this paper.
==================================================

Focused review:

Suggestions for improvement: - A discussion of computational costs compared to MAML would be interesting. How much slower is meta-training with ALPHA? - More emphasis could be put on the ALPHA+Random algorithm. Could you perhaps look at per task final adaptive parameter norms and compare ALPHA+Random, pure MAML and the main result? I suspect that all MAML variants substantially modify parameters based on the very biased training samples, while ALPHA relies heavily on model "prior" inductive bias. - Perhaps another set of boxplots in the appendix could be plotted with deviations from the initialization, if informative. Both learning rates and regularization coefficient matter in relation to the scale of gradients at each step, and aren't directly comparable if gradient scale falls dramatically. - I am curious how the authors rationalize and interpret negative learning rates, especially for variants which include MAML. - Should some L2 regularization be added, or be meta-learned, for the parameter initialization as well? Negative learning rates tell me that some amount of meta-overfitting may be due to the initialization being updated via MAML. It may turn out that optimal results are somewhere in between Random and MAML initializations, e.g. MAML with some (meta-learned) weight decay. - A discussion of possible limitations of proposed method would be interesting. Can the authors imagine conditions in which ALPHA would be detrimental or lead to divergence? Some failure cases mentioned in code comments could be brought into the main paper.

Review Point: - More emphasis could be put on the ALPHA+Random algorithm. Could you perhaps look at per task final adaptive parameter norms and compare ALPHA+Random, pure MAML and the main result? I suspect that all MAML variants substantially modify parameters based on the very biased training samples, while ALPHA relies heavily on model "prior" inductive bias.
Review Point: - Perhaps another set of boxplots in the appendix could be plotted with deviations from the initialization, if informative. Both learning rates and regularization coefficient matter in relation to the scale of gradients at each step, and aren't directly comparable if gradient scale falls dramatically.
Review Point: - I am curious how the authors rationalize and interpret negative learning rates, especially for variants which include MAML.
Review Point: - Should some L2 regularization be added, or be meta-learned, for the parameter initialization as well? Negative learning rates tell me that some amount of meta-overfitting may be due to the initialization being updated via MAML. It may turn out that optimal results are somewhere in between Random and MAML initializations, e.g. MAML with some (meta-learned) weight decay.
Review Point: - A discussion of possible limitations of proposed method would be interesting. Can the authors imagine conditions in which ALPHA would be detrimental or lead to divergence? Some failure cases mentioned in code comments could be brought into the main paper.
==================================================

Focused review:

Relevance: Nothing significant. Novelty: Nothing major, it's a clever idea to train an ensemble of neural networks that has the asymptotic width distribution as an analytic NTK GP. In many ways however, it is practically a straightforward extension of [22, 23] by just adding the Jacobian term into the network. Theoretical grounding: I am a little concerned that in the infinite limit you are just training a NTK GP, so why not just use the NTK GP kernel method in most of the small-medium sized data situations in the paper. Obviously, scalability is a concern (the airlines experiment is certainly not easy for an exact kernel method) and not all architectures are analytically tractable. However, it feels like the model will ultimately inherit both the good and bad performance of an analytic NTK GP, which isn't that difficult to use in Jax. So, when possible, why not just use a NTK GP by computing the kernel in Jax as you get the predictive uncertainties for free from the gp representation, rather than having to train many neural networks. Experimental Evaluation: Beyond just comparing to [23] when possible, why not also compare to the analytic NTK GP on the MNIST experiment? There's no kernel learning involved here, so you just need to form and solve the kernel inverse problem, which is possible on a single GPU using the neural tangents library with a little bit of engineering effort [31]. It'd be interesting to see how well the approximation method performs in comparison to the true GP here. I find the lack of a real detailed comparison to the analytic NTK GP to be the weakest part of the experiment section and would certainly like to see the comparison in the rebuttal. {{Thank you for the comparison on the two moons problem; it's interesting to see that the sin activation function strongly outperforms the ReLU GPs here. However, I do think it's entirely possible to compute the analytic NTK gp on the MNIST experiment with a single GPU in a couple of hours -- you'd probably have to do a little bit of wrapping the kernel using `nt.utils.batch.__serial` if I remember correctly.}} Overall, it seems like the results are somewhat of a wash between the parameter and function space regularization techniques.. It seems like NTKGP-fn outperforms NTKGP-param in both panels of Figure 2 (lower NLL on all but the smallest training set and lower RMSE at most confidence thresholds) on the right panel of Figure 3 (Figure 2 is a wash for all methods it seems like). A couple of questions result: - Why not just present NTKGP-fn as the primary method instead of NTKGP-param? Is it simply an interpretation difference? - Show NTKGP-fn in Figure 1 if possible - I'm curious to know if it performs well in this sanity check. {{These would have been nice to see.}} Given that the model decomposes into the forwards pass and the Jacobian term, would it be possible to show how the prediction decomposes into these two terms, as well as the predictive uncertainty? {{Thanks for the clarification; however, seeing those terms in a plot would have been nice to disentangle what is giving the good predicitive uncertainties.}} Significance: While the method proposed seems to work reasonably well, I'm not entirely sure how it's more than a little bit better in practice than deep ensembles. Is the benefit of Bayesian-ness the only real benefit here? If so, then why not show an application (active learning?) where the Bayesian-ness is actually really helpful... As it stands, this feels like an easy trick to make an ensemble Bayesian, much like [22,23], but performance wise doesn't go much beyond [23] on the regression problem. It's also slower on the forwards pass than Bayesian style methods like [22,23] due to the forced inclusion of the Jacobian term. {{Thanks for the updates on how this method would be used in practice. Again, I find the discussion of its usefulness in practice quite fair and balanced overall.}}

Review Point: - Why not just present NTKGP-fn as the primary method instead of NTKGP-param? Is it simply an interpretation difference?
Review Point: - Show NTKGP-fn in Figure 1 if possible - I'm curious to know if it performs well in this sanity check. {{These would have been nice to see.}} Given that the model decomposes into the forwards pass and the Jacobian term, would it be possible to show how the prediction decomposes into these two terms, as well as the predictive uncertainty? {{Thanks for the clarification; however, seeing those terms in a plot would have been nice to disentangle what is giving the good predicitive uncertainties.}} Significance: While the method proposed seems to work reasonably well, I'm not entirely sure how it's more than a little bit better in practice than deep ensembles. Is the benefit of Bayesian-ness the only real benefit here? If so, then why not show an application (active learning?) where the Bayesian-ness is actually really helpful... As it stands, this feels like an easy trick to make an ensemble Bayesian, much like [22,23], but performance wise doesn't go much beyond [23] on the regression problem. It's also slower on the forwards pass than Bayesian style methods like [22,23] due to the forced inclusion of the Jacobian term. {{Thanks for the updates on how this method would be used in practice. Again, I find the discussion of its usefulness in practice quite fair and balanced overall.}}
==================================================

Focused review:

Weaknesses
Only evaluated with accuracy and ECE, so the results do not fully reflect the uncertainty quantification aspect of the models.
Somewhat unfair comparison environment for DM (see below).
No experiments on larger scale datasets such as TinyImageNet or ImageNet.
As far as I see from the paper, DM and Hydra use only 8 heads but still distill from 120 samples; this might seriously harm the performance of the baselines, especially because these methods are designed to distill each teacher member to each subnetwork in a one-to-one fashion. If possible, it would be good to see the comparison in the setting where the teachers come with a small number of ensemble members so that one can directly distill DM and Hydra in a one-to-one fashion.
As mentioned above, no metrics other than ACCs and ECEs.
No experiments on large-scale image classification benchmarks, such as ImageNet. I find the distillation method often suffers when it comes to the scale of ImageNet, so would be good to see the scalability of the proposed methods on such datasets.
The main contribution can be understood as a combination of two ingredients; 1) design of the student networks that introduce Gaussian perturbations for inputs and intermediate layers, and 2) learning with MMD. Without an ablation study, it is hard to see the net effect of each component. For instance, we can try learning the proposed model with typical knowledge distillation loss, or try distilling a Hydra architecture with MMD loss.

Review Point: 1) design of the student networks that introduce Gaussian perturbations for inputs and intermediate layers, and 2) learning with MMD. Without an ablation study, it is hard to see the net effect of each component. For instance, we can try learning the proposed model with typical knowledge distillation loss, or try distilling a Hydra architecture with MMD loss.
==================================================

Focused review:

* Only sublinear convergence can be guaranteed for the algorithm proposed in Section 4.1 It would have been nice if the authors discussed this in more detail, in particular also reporting the run times of the numerical experiments and discussing the current limits imposed by the convergence rates. Note that in Remark 4.3 and Section B.4 faster (local) convergence rates are discussed. However, as noted by the authors acceleration with Gauss-Newton method cannot be done in a model-free manner. Since this paper is aimed at the reinforcement learning community this seems to be a major weakness. This aspect makes the discussion of the slow guaranteed convergence mentioned above even more important. * The discussion of the stability problems in Section 3.2 is very vague. In particular, it is not clear how Example 3.5 is related to the (very short) discussion preceding it. Reading Section C.1.2 makes things a bit clearer, but Section 3.2 should be improved. Furthermore, it would be nice if Section 3 contains a summary like in lines 718-720 in the supplementary.

Review Point: * Only sublinear convergence can be guaranteed for the algorithm proposed in Section 4.1 It would have been nice if the authors discussed this in more detail, in particular also reporting the run times of the numerical experiments and discussing the current limits imposed by the convergence rates. Note that in Remark 4.3 and Section B.4 faster (local) convergence rates are discussed. However, as noted by the authors acceleration with Gauss-Newton method cannot be done in a model-free manner. Since this paper is aimed at the reinforcement learning community this seems to be a major weakness. This aspect makes the discussion of the slow guaranteed convergence mentioned above even more important.
Review Point: * The discussion of the stability problems in Section 3.2 is very vague. In particular, it is not clear how Example 3.5 is related to the (very short) discussion preceding it. Reading Section C.1.2 makes things a bit clearer, but Section 3.2 should be improved. Furthermore, it would be nice if Section 3 contains a summary like in lines 718-720 in the supplementary.
==================================================

Focused review:

Weaknesses:
Novelty is weak, the top-n operator was previously proposed for deep active learning [1] and also being trained within an RL-style framework
The mathematical notation is inconsistent
Several central claims are not supported by the experiments: (i) policy learning diversity sampling (I doubt this, as I cannot see this in the introduced methodology), (ii) that the learnt acquisition function can be transferred between datasets, (iii) that only little computational resources are required in relation to current approaches.
The experimental study has several weaknesses (see more details below). Methodology
It is not clear how you minimize the Euclidean distance, as you say in Section 4.4 (last paragraph). Isnt it just a part of the action
The paper introduces the MDP using a four-tuple ( s t , a t , r t + 1 , s t + 1 )
– usually that is not correct as an MDP is defined over a set and not over concrete time-step samples, i.e., { S , A , R , [ P ] , γ
}, where S
is a set of states, A
a set of possible actions, R
a reward function, P
a probability matrix that describes the system dynamics, and γ
the discount factor. Elements such as s t or a t
are concrete elements from those sets at particular time-steps but they do not define the MDP.
“5. Train f_t one iteration on the new labelled dataset” – what do you mean with one iteration? One epoch? If not: then I doubt that we see a full RL setting here. Does it make a difference if we label datapoint A first and then B versus labeling B first and then A?
Equ. 1 is incorrect: γ
should be to the power of i
. How is i
bounded? (I guess over the length of the episode, but there is a lot of notation being inconsistent here…) Q π ⋆ → Q π ⋆
“The optimal value is the Q π ⋆ ( s , a ) = m a x π Q π ( s , a )
– please refer to standard RL literature to clear abuse of notation
State representation: I have two questions on that: (1) Don’t you include any classifier-model-specific parameters into the state? If not, I doubt that the model explicitly learns diversity-based strategies…, (2) The state representation seems to be a bit misaligned and too minimal. I guess you formulate a POMDP here.
Sec. 3.1.2 was unclear to me for quite some time and only became clear after iterating together with Section 3.2. The authors should rewrite this part to make it more accessible and more intuitive.
Equation 4 is unclear. The argmax-operator as defined (although I am not sure if this is common-sense to do it like this) returns a set of actions but it is used as a scalar for computation. This is undefined. Or do you use the set over A as a proxy for actions? But then it is inconsistent with the description in 3.1.2
Experimental results
Claims of superior performance should be better supported with suitable stronger baseline methods in experiments section (Coreset [Sener 2018], Ensemble [Beluch 2018], MC-Dropout [Gal 2017], IALE [Loeffler 2022], BADGE [Ash 2020 (please see the final reference paper instead of your cited preprint], BatchBALD [Kirsch2019, please see the final reference paper instead of your cited preprint], ...).
No ablation study of the differences between Konyushkova et al. (2018) and the proposed method. That would be interesting to see.
Why do you show only the 68% confidence interval for your results?
Please define what N is in the main experiments (I guess it is 4)
Figure 1ff: the accuracy at point 0 (annotions = 0) seem to be wrong. Is it the performance of the classifier after the warm-up? (then, I guess it has already seen 64 samples (16 episodes à 4 samples). Shouldn’t the graphs start at 64? At x=10 the classifier has seen 74 samples, right? (64 random + 10 selected by heuristic)
To make your method more comparable, how does your method perform compared to the baselines on commonly used image classification datasets such as CIFAR10/100, SVHN, and MNIST variants, with commonly used classifier architectures (ResNet18 etc)?
The presentation of results in Figs. 1-3 is unintuitive and there is little to no benefit to active learning visible. Why is that? I would also expect to run AL at least for 1K annotations to see how things evolve…
How do the warm-start episodes exactly work? This seems important, can you perform a study on the effect and cost of it?
The experiments on batch-sizes between 1 and 10 have several issues:
Is each datapoint in Figure 4 a separate run of the policy from 0 to 50 samples? If yes, why is the graph in Fig. 4 connecting the separate runs as if they are continuous? I’d like to see each run from 0 to 50 over each other in one figure.
What is the warmup?
Training time measurements seem noisy between 1222 and 3374, the N=10 datapoint seems like an outlier from this, how does it perform for N=20, 50, 100, 1000? -Why is 1 worse than the others?
To me runtime or computation time seems to be a real issue. In fact the authors also mention that throughout the paper but there is no results or analysis on that.
Section 4.2.2: to me this section is a bit misleading. I do not see why this aspect seems to be important here.
What about model transfer, i.e, learning the policy on one dataset and the applying it to another?
Some minor points to the experiments
How do you differentiate from MedSelect?
The results in Fig. 1,2,3 do not present the results well, due to the point at 0 labeled samples that can be removed. Also, no sub-figs used there.
Table 1: it would help to highlight the best-performing method
Missing references and prior work & updated citations
[Loeffler 2022] Christoffer Loeffler and Christopher Mutschler: “IALE: Imitating Active Learner Ensembles”. In: Journal of Machine Learning Research 23 (2022) 1-29
[Ash 2020] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning Representations (ICLR), 2020.
[Kirsch 2019] Andreas Kirsch, Joost van Amersfoort, Yarin Gal. BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. NeurIPS, 2019.

Review Point: 1 is incorrect: γ should be to the power of i . How is i bounded? (I guess over the length of the episode, but there is a lot of notation being inconsistent here…) Q π ⋆ → Q π ⋆ “The optimal value is the Q π ⋆ ( s , a ) = m a x π Q π ( s , a ) – please refer to standard RL literature to clear abuse of notation State representation: I have two questions on that: (1) Don’t you include any classifier-model-specific parameters into the state? If not, I doubt that the model explicitly learns diversity-based strategies…, (2) The state representation seems to be a bit misaligned and too minimal. I guess you formulate a POMDP here. Sec. 3.1.2 was unclear to me for quite some time and only became clear after iterating together with Section 3.2. The authors should rewrite this part to make it more accessible and more intuitive. Equation 4 is unclear. The argmax-operator as defined (although I am not sure if this is common-sense to do it like this) returns a set of actions but it is used as a scalar for computation. This is undefined. Or do you use the set over A as a proxy for actions? But then it is inconsistent with the description in 3.1.2 Experimental results Claims of superior performance should be better supported with suitable stronger baseline methods in experiments section (Coreset [Sener 2018], Ensemble [Beluch 2018], MC-Dropout [Gal 2017], IALE [Loeffler 2022], BADGE [Ash 2020 (please see the final reference paper instead of your cited preprint], BatchBALD [Kirsch2019, please see the final reference paper instead of your cited preprint], ...). No ablation study of the differences between Konyushkova et al. (2018) and the proposed method. That would be interesting to see. Why do you show only the 68% confidence interval for your results? Please define what N is in the main experiments (I guess it is 4) Figure 1ff: the accuracy at point 0 (annotions = 0) seem to be wrong. Is it the performance of the classifier after the warm-up? (then, I guess it has already seen 64 samples (16 episodes à 4 samples). Shouldn’t the graphs start at 64? At x=10 the classifier has seen 74 samples, right? (64 random + 10 selected by heuristic) To make your method more comparable, how does your method perform compared to the baselines on commonly used image classification datasets such as CIFAR10/100, SVHN, and MNIST variants, with commonly used classifier architectures (ResNet18 etc)? The presentation of results in Figs. 1-3 is unintuitive and there is little to no benefit to active learning visible. Why is that? I would also expect to run AL at least for 1K annotations to see how things evolve… How do the warm-start episodes exactly work? This seems important, can you perform a study on the effect and cost of it? The experiments on batch-sizes between 1 and 10 have several issues: Is each datapoint in Figure 4 a separate run of the policy from 0 to 50 samples? If yes, why is the graph in Fig. 4 connecting the separate runs as if they are continuous? I’d like to see each run from 0 to 50 over each other in one figure. What is the warmup? Training time measurements seem noisy between 1222 and 3374, the N=10 datapoint seems like an outlier from this, how does it perform for N=20, 50, 100, 1000? -Why is 1 worse than the others? To me runtime or computation time seems to be a real issue. In fact the authors also mention that throughout the paper but there is no results or analysis on that. Section 4.2.2: to me this section is a bit misleading. I do not see why this aspect seems to be important here. What about model transfer, i.e, learning the policy on one dataset and the applying it to another? Some minor points to the experiments How do you differentiate from MedSelect? The results in Fig. 1,2,3 do not present the results well, due to the point at 0 labeled samples that can be removed. Also, no sub-figs used there. Table 1: it would help to highlight the best-performing method Missing references and prior work & updated citations [Loeffler 2022] Christoffer Loeffler and Christopher Mutschler: “IALE: Imitating Active Learner Ensembles”. In: Journal of Machine Learning Research 23 (2022) 1-29 [Ash 2020] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning Representations (ICLR), 2020. [Kirsch 2019] Andreas Kirsch, Joost van Amersfoort, Yarin Gal. BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. NeurIPS, 2019.
==================================================

Focused review:

Weaknesses: - This is not actually an OMP algorithm as the authors claim, because their algorithm selects for multiple orthogonal bases, instead of just one. This is worth mentioning since it means the intuition that we have from the analysis of OMP, that it will converge to a minimum, does not hold here - The authors analyze their algorithm assuming that the basis functions are orthonormal, but it is not clear that the experiments conformed to this assumption: are the basis functions obtained from the Taylor Series expansion of the gaussian kernel orthonormal? The authors should make it clear either way. - The theory is provided for hard constraints on the mixture coefficients, while the experimental results are given for a lagrangian relaxation. It is possible to obtain theory that applies directly to the lagrangian relaxation - I would like to have seen a comparison with a fast randomized feature map such as FastFood: the authors compare to the basic random Fourier Feature map, which is much slower because it uses more iid random variables. I believe the practice is to use FastFood. - The graphs are illegible when the paper is printed in black and white. It would be preferable to use markers and different line styles to help avoid this issue.

Review Point: - This is not actually an OMP algorithm as the authors claim, because their algorithm selects for multiple orthogonal bases, instead of just one. This is worth mentioning since it means the intuition that we have from the analysis of OMP, that it will converge to a minimum, does not hold here - The authors analyze their algorithm assuming that the basis functions are orthonormal, but it is not clear that the experiments conformed to this assumption: are the basis functions obtained from the Taylor Series expansion of the gaussian kernel orthonormal? The authors should make it clear either way.
Review Point: - The theory is provided for hard constraints on the mixture coefficients, while the experimental results are given for a lagrangian relaxation. It is possible to obtain theory that applies directly to the lagrangian relaxation - I would like to have seen a comparison with a fast randomized feature map such as FastFood: the authors compare to the basic random Fourier Feature map, which is much slower because it uses more iid random variables. I believe the practice is to use FastFood.
Review Point: - The graphs are illegible when the paper is printed in black and white. It would be preferable to use markers and different line styles to help avoid this issue.
==================================================

Focused review:

1. The paper doesn't show how tertiary claim classes and visual claim classes can help in fake news detection. So it leads to the question, why do we care the proposed label classes in claim detection? 
2. The annotated dataset size is limited for the use of largely used heavy multi-modal models. And while claims are related to 3 topics, they are still limited. 
3. Experiments include fine-tuning on ALBEF but not fine-tuning on CLIP. 
4. The dataset has conflicts in annotated data. Why not adding more annotators? 
1. Try to show the value of the dataset -- to show the use of tertiary claim classes and visual claim classes can actually help in fake news detection performance. Also, it will be interesting to see whether using the classification results on visual claim classes can help the classification performance on tertiary claim classes. 
2. Increase the number of annotators each data to avoid conflicts. If possible, annotate more data. 
3. In experiment, it can also do fine-tuning on CLIP. 

Review Point: 1. The paper doesn't show how tertiary claim classes and visual claim classes can help in fake news detection. So it leads to the question, why do we care the proposed label classes in claim detection?
Review Point: 2. The annotated dataset size is limited for the use of largely used heavy multi-modal models. And while claims are related to 3 topics, they are still limited.
Review Point: 3. Experiments include fine-tuning on ALBEF but not fine-tuning on CLIP.
Review Point: 4. The dataset has conflicts in annotated data. Why not adding more annotators?
Review Point: 1. Try to show the value of the dataset -- to show the use of tertiary claim classes and visual claim classes can actually help in fake news detection performance. Also, it will be interesting to see whether using the classification results on visual claim classes can help the classification performance on tertiary claim classes.
Review Point: 2. Increase the number of annotators each data to avoid conflicts. If possible, annotate more data.
Review Point: 3. In experiment, it can also do fine-tuning on CLIP.
==================================================

Focused review:

Weaknesses
Using PSNR to assess the strength of the attacks seems a bit problematic since it is not a metric the attacks optimize: for example, using an ℓ ∞
-bounded attack and fixed a level of PSNR, there might be adversarial perturbations in the ℓ ∞
-ball with the desired PSNR which are not found by PGD since they have lower loss values (the objective of the attack) than other points (similar arguments hold for other attacks). Then, it is not clear how to interpret a comparison based on PSNR.
The paper compares the results of FGSM and PGD, but these operate in the same threat model, and the latter is just a stronger optimizer. Since robustness is evaluated in the worst-case scenario, it is unclear which insights the results of FGSM could add.
It is observed that "gradient-based attack methods tend to perturb high frequency features in images...", but I think this depends on the definition of the threat model (and maybe optimizer), e.g. using ℓ 2
- instead of ℓ ∞
-constraints should yield lower frequency perturbations.
It is hard to see the differences across threat models in Fig. 2.
PGD with bound ϵ
is used with T
iterations and step size ϵ / T
: this might be suboptimal (assuming that, as commonly done, the sign of the gradient is used) since with random start not every point in the feasible set can be reached.

Review Point: 2. PGD with bound ϵ is used with T iterations and step size ϵ / T : this might be suboptimal (assuming that, as commonly done, the sign of the gradient is used) since with random start not every point in the feasible set can be reached.
==================================================

Focused review:

Weaknesses
Practical significance of the proposal is questionable, as only a toy example of compressing MNIST images is provided as a real-world problem.
As for the image compression experiment in Section 5.2:
Although the proposal is compared against the deterministic encoder and the dithered quantizer, there is no comparison with any other approach to RDP coding.
It seems that how the authors evaluated F X
and its inverse is not described.
According to Appendix E.2, "encoding with uniform thresholds" was employed in the experiment, so that we cannot see how efficient the threshold optimization methods proposed in Section 4.2 are.
I wonder whether it is possible to extend the proposed framework from scalar quantization to vector quantization of a few (not many) samples. There seems no argument provided on such possible extension.
Minor points:
Page 1, line 6: one (at) a time
Page 2, line 7: will utilize(s)
Page 2, line 19: two extreme cases (of) with
Page 2, line 42: referred to (as)
Page 4, Figure 1 (c): 7 out of 9 figures seem incorrect. From (4), the quantizer outputs 1 when θ ( x ) ∈ [ − Z , π − Z ) mod 2 π
, but these figures seem to assume, incorrectly, that it outputs 1 when θ ( x ) ∈ [ Z − π , Z ) mod 2 π
. The locations of the dots representing the reconstruction also seem to be imprecise.
Page 4, line 21: Extra space after "Fig. 1 (c)".
Page 5, line 10: which uniformly partition(s)
Page 5, line 11: by offset(t)ing
Page 5, line 33: Theorem(s) 3.1-3.3
Page 7, line 7: Is " f ( x ) and n " f n ( x ) ?
Page 7, line 28: that update(s) the thresholds
Page 8, Figure 5: In both cases one can see that in a certain range of bitrate "private randomness only" seems to outperform "dithered quantization". I would appreciate a possible explanation to this.
Page 8, line 16: Figure 7 → 5
Page 8, line 28: We follow(s)
Page 8, line 32: Mu(tl → lt)iQuan
Page 9, line 11: Kan(o)torovich's

Review Point: 7 out of 9 figures seem incorrect. From (4), the quantizer outputs 1 when θ ( x ) ∈ [ − Z , π − Z ) mod 2 π , but these figures seem to assume, incorrectly, that it outputs 1 when θ ( x ) ∈ [ Z − π , Z ) mod 2 π . The locations of the dots representing the reconstruction also seem to be imprecise. Page 4, line 21: Extra space after "Fig. 1 (c)". Page 5, line 10: which uniformly partition(s) Page 5, line 11: by offset(t)ing Page 5, line 33: Theorem(s) 3.1-3.3 Page 7, line 7: Is " f ( x ) and n " f n ( x ) ? Page 7, line 28: that update(s) the thresholds Page 8, Figure 5: In both cases one can see that in a certain range of bitrate "private randomness only" seems to outperform "dithered quantization". I would appreciate a possible explanation to this. Page 8, line 16: Figure 7 → 5 Page 8, line 28: We follow(s) Page 8, line 32: Mu(tl → lt)iQuan Page 9, line 11: Kan(o)torovich's
==================================================

Focused review:

Weaknesses: They gloss over the details of their character-based encoder. 
There are many different ways to learn character-based representations, and omitting a discussion of how they do this leaves open questions about the generality of their findings. Also, their analysis could've been made more interesting had they chosen languages with richer and more challenging morphology such as Turkish or Finnish, accompanied by finer-grained morphology prediction and analysis.
- General Discussion: This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of "Does String-Based Neural MT Learn Source Syntax?," using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn. 

Review Point: - General Discussion: This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of "Does String-Based Neural MT Learn Source Syntax?," using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn.
==================================================

Focused review:

Weaknesses:
A significant weakness in my opinion is the somewhat incremental methodological contribution. Whereas I can accept the practical value of boundary modifications for the application in mind, the idea of adding virtual nodes to a simulation based on a heuristic ball radius, and the consideration of additional node features (as detailed in Section 3) is modest as a contribution to either a "representation" or the "learning" of it, which is the general requirement of ICLR.
The message passing to update node embeddings seems to be directly based on past work. The specifics of what these messages contain (the new node features, i.e., beyond triangle inclination) are difficult to tease out. Computational considerations, such as how additional coordinate locations are chosen to minimize distance between boundary points and "real" nodes are not addressed. There's an attempt to illustrate the idea in Fig. 2 but it falls short of an algorithm and lacks a complexity analysis.
Much of Section 4, which discusses further details of how the graph is made "dynamic" appears to be a coverage of implementation issues. Here the additional features features are listed: 1) real or virtual particle, 2) normal vectors but how these features are related to the physics of granular flow is not discussed at any length.
I found the title to be a bit misleading. While the paper agues for a dynamic modification of a GNN at its boundary for 3D simulations, the experiments and the particular instantiation appear to be almost entirely for granular flow.

Review Point: 1) real or virtual particle, 2) normal vectors but how these features are related to the physics of granular flow is not discussed at any length. I found the title to be a bit misleading. While the paper agues for a dynamic modification of a GNN at its boundary for 3D simulations, the experiments and the particular instantiation appear to be almost entirely for granular flow.
==================================================

Focused review:

It is not clear if the initialization in Section 3.1 is realistic or not (i.e., is it similar to something that is used in practice). What is a concrete example of an initialization that satisfies the conditions mentioned in this section? Some of the notations and statements are not clear: 1. Lemma 4.1 part 4 does not seem to be used in the proof of Theorem 3.2. Is it used? The convergence to the global minimum follows by the convergence to 0 loss and not using Lemma 4.1 part 4. Is this correct? 2. After line 92, does lambda_F^2 and lambda_F^3 correspond to lambda_F to the power of 2 and 3? If so, then why are both assumptions needed (both give lower bounds on lambda_F)? 3. Can gamma be equal to 0 in equation (2), such that we get the ReLU activation and not Leaky ReLU?

Review Point: 1. Lemma 4.1 part 4 does not seem to be used in the proof of Theorem 3.2. Is it used? The convergence to the global minimum follows by the convergence to 0 loss and not using Lemma 4.1 part 4. Is this correct?
Review Point: 2. After line 92, does lambda_F^2 and lambda_F^3 correspond to lambda_F to the power of 2 and 3? If so, then why are both assumptions needed (both give lower bounds on lambda_F)?
Review Point: 3. Can gamma be equal to 0 in equation (2), such that we get the ReLU activation and not Leaky ReLU?
==================================================

Focused review:

weakness of the paper (but it should be changed): It seems that the paper is using the ICLR 2021 (last year) format because the page headers say “Under review as a conference paper at ICLR 2021”. Strengths
Incorporating inductive biases into flow architectures is a very interesting and important problem to deal with.
The method presented in section 3.1 of converting a distribution into a uniform distribution and the uniform distribution into another one sounds very interesting and applicable in other settings.
The presented model outperforms the baselines in the presented experiments. Weaknesses
The paper claims outperforming the state-of-the-art. I do not think that this conclusion can be drawn from the presented experiments:
The paper compares the suggested architecture with two flow models, Inverse Autoregressive Flows (IAFs) and Masked Autoregressive Flows (MAFs), which were published in 2016 and 2017, respectively. Both baseline models are dated and do not constitute the current state of the art. In a more recent survey on normalizing flows ([1], p. 13), MAFs did not achieve state-of-the-art performance on any of the covered datasets.
The datasets that the paper uses for evaluation are not common for assessing the quality of normalizing flow models. None of the datasets is used in [1]. In particular, neither the IAF paper nor the MAF paper use these datasets for evaluation. I recommend evaluating the presented model on some (simple) image datasets, like MNIST and CIFAR10, which are very commonly used in the normalizing flow literature.
The conclusion states “We showed how, by choosing appropriate inductive biases, EMF can improve over generic normalizing flows on a range of different domains, with only a negligible increase in complexity …” This conclusion should only be drawn if it can be supported by some concrete numbers. To the best of my knowledge, the paper does not mention anywhere how the complexity of the proposed EMF compares to the complexity of the baseline model. Indeed, presenting training and sampling times of the proposed and the baselines models would strengthen the contribution. Clarity
I have really tried to understand the methodology in the main section 3.2, but the section is still not clear to me. I understand the univariate case in section 3.1, but I cannot make sense of the main section 3.2. In Figure 2, I do not understand what expressions like model_generator, gen.send and d.as_bijector mean. Moreover, the function forward_and_log_det_jacobian seems to be called within itself, but with only one argument instead of the 3 used in the definition. (The same applies to inverse_and_log_det_jacobian.) What is the input and the output of the structured layers? What do the epsilon variables in e.g. equation (5) mean? Shouldn’t the epsilons only come from the flow latent space and not be present in intermediate layers? A toy example where each step of the proposed layers is explained for some easy problem would be really helpful. This could either be put into the appendix or replace Figure 1, which I did not find very illuminating.
Minor and typos
x_0 is not present on the left hand side of equation (2), so it should not be on the right hand side either.
On p. 3, in the sentence “The probability integral transform theorem states …”: The upper limit of the integral should be x and not y.
p. 2: “We call these architectures embedded-model flow s (EMF).” -> flows
[1] Normalizing Flows: An Introduction and Review of Current Methods, Kobyzev, Prince, Brubaker.

Review Point: 2: “We call these architectures embedded-model flow s (EMF).” -> flows [1] Normalizing Flows: An Introduction and Review of Current Methods, Kobyzev, Prince, Brubaker.
==================================================

Focused review:

1. The importance of Renyi divergence is not well explained. It is not clear why Renyi divergence is so special for a reader who is not familiar with differential privacy. 2. What is the benefit using the unadjusted Langevin process, instead of the adjusted process? If we use the adjusted process, we may get the exact distribution. 3. Is the obtained rate optimal? How far is it from the optimal rate? At least it is not a good rate comparing with the results from KL divergence. 4. The paper does not provide any new algorithm or new insight about the algorithm. It is just a theoretical analysis on existing algorithm under a difference divergence. 5. Can you hightlight the novelty in proofs? What new techniques are developed and necessary in proofs? 6. Is it possible to show some numerical results to demonstrate that the theoretical results are aligned with the numerical results?

Review Point: 1. The importance of Renyi divergence is not well explained. It is not clear why Renyi divergence is so special for a reader who is not familiar with differential privacy.
Review Point: 2. What is the benefit using the unadjusted Langevin process, instead of the adjusted process? If we use the adjusted process, we may get the exact distribution.
Review Point: 3. Is the obtained rate optimal? How far is it from the optimal rate? At least it is not a good rate comparing with the results from KL divergence.
Review Point: 4. The paper does not provide any new algorithm or new insight about the algorithm. It is just a theoretical analysis on existing algorithm under a difference divergence.
Review Point: 5. Can you hightlight the novelty in proofs? What new techniques are developed and necessary in proofs?
Review Point: 6. Is it possible to show some numerical results to demonstrate that the theoretical results are aligned with the numerical results?
==================================================

Focused review:

Weakness:
This work would be of a stronger stance if its originality is more well-justified: 1) the paper writes that "...three perceptual loss functions for PSR, two of which are new", while the neural network and the loss functions used (reconstruction error, TV, perceptual loss, dice) are not unheard in the field of medical imaging. The authors might elaborate which parts of the loss functions are new. 2) While the proposed evaluation paradigm is, in a sense, new, I would like to see a deeper discussion of why this is new (e.g., is this a novel way of seeing the problem of lacking SR ground truth label?)
The contribution bullet states that "demonstration of the impact of loss choice on performance differences in improving detection power in [population studies] of neurodegenerative disease". The presented results haven't directly shown the power of the proposed method on a population-scale. The authors might elaborate more about the stated population-level effect of the proposed method.

Review Point: 1) the paper writes that "...three perceptual loss functions for PSR, two of which are new", while the neural network and the loss functions used (reconstruction error, TV, perceptual loss, dice) are not unheard in the field of medical imaging. The authors might elaborate which parts of the loss functions are new.
Review Point: 2) While the proposed evaluation paradigm is, in a sense, new, I would like to see a deeper discussion of why this is new (e.g., is this a novel way of seeing the problem of lacking SR ground truth label?) The contribution bullet states that "demonstration of the impact of loss choice on performance differences in improving detection power in [population studies] of neurodegenerative disease". The presented results haven't directly shown the power of the proposed method on a population-scale. The authors might elaborate more about the stated population-level effect of the proposed method.
==================================================

Focused review:

1. While the paper does achieve superior results, large chunks are devoted to highlighting BN and arguing that BN approaches can do better than other approaches. To this end, the novelty seems to be limited to putting the similar idea with [38]. 2. Authors make a point about exchanging channel — but exchanging channel is only achieved using BN. I am not really sure if such a contributionis incremental. In my opinion, the rationale of Theorem 1 are mostly based on [38], and Theorem 1 seems like a simple fact. 3. It would be better if the authors provide a figure of the percentage of exchanged channel for every layer, which will help readers better understand the model. 4. As stated in the paper, all modalities must be homogeneous.

Review Point: 1. While the paper does achieve superior results, large chunks are devoted to highlighting BN and arguing that BN approaches can do better than other approaches. To this end, the novelty seems to be limited to putting the similar idea with [38].
Review Point: 2. Authors make a point about exchanging channel — but exchanging channel is only achieved using BN. I am not really sure if such a contributionis incremental. In my opinion, the rationale of Theorem 1 are mostly based on [38], and Theorem 1 seems like a simple fact.
Review Point: 3. It would be better if the authors provide a figure of the percentage of exchanged channel for every layer, which will help readers better understand the model.
Review Point: 4. As stated in the paper, all modalities must be homogeneous.
==================================================

Focused review:

- I think it is informative to have some explanation in the Algorithm section. Something that is not clear to me is how they sample from auxiliary data during training. Is that correct that during first epochs, the estimation of relatendness is not much accurate since we are in the primary stages of training, but after some epochs, more related auxiliary tasks are selected. - There is no discussion about how efficient is the proposed method (since objective function is the distance between the gradient of two losses).

Review Point: - I think it is informative to have some explanation in the Algorithm section. Something that is not clear to me is how they sample from auxiliary data during training. Is that correct that during first epochs, the estimation of relatendness is not much accurate since we are in the primary stages of training, but after some epochs, more related auxiliary tasks are selected.
Review Point: - There is no discussion about how efficient is the proposed method (since objective function is the distance between the gradient of two losses).
==================================================

Focused review:

weakness)? 
4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)? 
5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data? 
6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss. 

Review Point: 4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)?
Review Point: 5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data?
Review Point: 6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss.
==================================================

Focused review:

- It will be helpful if the authors could further support their assumptions with examples of control systems, or connections to classical control literature. Is this a new control setting? Under what control settings does it make sense to assume that future costs and noises are known? This is also related to my question about the interaction model in OCO with structured memory, see additional feedback. -In Theorem 2, the competitive ratio can be upper bounded in two ways: 1) a competitive ratio with an additive term; 2) a competitive ratio with a factor of (l + \alpha^2). In the first case, the additive term has the sum of \|v_t - \tilde{v}_t\|^2 over t, which can be very large. In the second case, how should I reconcile this factor with the counterexample in Appendix B, where the lipschitz constant also appears?

Review Point: - It will be helpful if the authors could further support their assumptions with examples of control systems, or connections to classical control literature. Is this a new control setting? Under what control settings does it make sense to assume that future costs and noises are known? This is also related to my question about the interaction model in OCO with structured memory, see additional feedback. -In Theorem 2, the competitive ratio can be upper bounded in two ways:
Review Point: 2) a competitive ratio with a factor of (l + \alpha^2). In the first case, the additive term has the sum of \|v_t - \tilde{v}_t\|^2 over t, which can be very large. In the second case, how should I reconcile this factor with the counterexample in Appendix B, where the lipschitz constant also appears?
==================================================

Focused review:

- A major weakness of the paper is the circular way it is evaluated. Imperfect trajectories are generated using a particular model of goal-conditional planning bounded rationality. Then we perform inference on the goal of the agent assuming that the model of the agent is the exact same that generated the data. This is akin to generating random unsupervised data sets by a probabilistic program that generates random mixtures of gaussian, then perform model learning on these datasets, compare it to other baselines, and conclude that mixture of gaussians are exactly what you need for unsupervised learning. I don't object to the particular approach (we probably assume that others use a similar model of planning as we do), but to its evaluation. It seems clear that the generalisability / transfer of the inner model will be key to the usefulness of the overall model. If the mismatch is strong, the conclusions would likely be very different (say if data was generated from a depth-first search agent with bounded rationality, and the inference was done with a breadth-first agent). - For that matter, I am not terribly convinced by the model of bounded rationality adopted by the authors. It is strongly tied to a deterministic environment (a fact the authors acknowledge), and somewhat implies deciding one action at a time is suboptimal. We know from the theory of RL optimal actions can be computed without explicit planning (and even one-step planning can be optimal provided we do adequate bootstrapping). I understand the authors are going for a pure-planning, no heuristic/value functions approach, and so one-step planning is greedy, but this does not really reflect current state of the art planning method, nor does is it stated explicitly/discussed enough. - Finally, all planning is done under the assumption of a perfect model (goal aside), which should be more explicitly stated.

Review Point: - A major weakness of the paper is the circular way it is evaluated. Imperfect trajectories are generated using a particular model of goal-conditional planning bounded rationality. Then we perform inference on the goal of the agent assuming that the model of the agent is the exact same that generated the data. This is akin to generating random unsupervised data sets by a probabilistic program that generates random mixtures of gaussian, then perform model learning on these datasets, compare it to other baselines, and conclude that mixture of gaussians are exactly what you need for unsupervised learning. I don't object to the particular approach (we probably assume that others use a similar model of planning as we do), but to its evaluation. It seems clear that the generalisability / transfer of the inner model will be key to the usefulness of the overall model. If the mismatch is strong, the conclusions would likely be very different (say if data was generated from a depth-first search agent with bounded rationality, and the inference was done with a breadth-first agent).
Review Point: - For that matter, I am not terribly convinced by the model of bounded rationality adopted by the authors. It is strongly tied to a deterministic environment (a fact the authors acknowledge), and somewhat implies deciding one action at a time is suboptimal. We know from the theory of RL optimal actions can be computed without explicit planning (and even one-step planning can be optimal provided we do adequate bootstrapping). I understand the authors are going for a pure-planning, no heuristic/value functions approach, and so one-step planning is greedy, but this does not really reflect current state of the art planning method, nor does is it stated explicitly/discussed enough.
Review Point: - Finally, all planning is done under the assumption of a perfect model (goal aside), which should be more explicitly stated.
==================================================

Focused review:

I have some questions as well as some suggestions, clearing out them could help strengthen the existing work. - The proposed method can be thought of as a two-phase approach involving identifying and learning the augmented causal graph (including special variables such as context variables along with X and Y) and then learning a predictor in the target domain based on the invariant factors across the source and target domain as well as learning the specific distribution changes in the target domain as compared to the source domains. In relation to the first part of identifying the distribution changes across environments, I find that certain comparisons are missing that I'd like to point out. First, the idea is similar to [1] which also aims to encode the changes in the distribution across the domains using a graphical model as well. Secondly, there have been attempts at using observational data to identify the differences across domains using two-sample testing approaches [2]. Such methods can aid in understanding not just where the differences exist across domains but also analyze the effect of these shifts on the posteriors. Discussion on such methods is missing in the current setting. - The motivation for using latent-variable CGAN is not very clear. Since Gaussian Processes also are capable of approximating the posteriors, the merit of CGAN seems blur here. A better motivation for using the said approach can help clarify the said advantages. - Can multi-environment causal discovery approaches be used for learning the causal graph across all domains, which can then be combined with the domain adaptation methods? [1] Subbaswamy, Adarsh, and Suchi Saria. "I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models." arXiv preprint arXiv:2002.08948 (2020). [2] Rabanser, Stephan, Stephan Günnemann, and Zachary Lipton. "Failing loudly: An empirical study of methods for detecting dataset shift." Advances in Neural Information Processing Systems. 2019.

Review Point: - The proposed method can be thought of as a two-phase approach involving identifying and learning the augmented causal graph (including special variables such as context variables along with X and Y) and then learning a predictor in the target domain based on the invariant factors across the source and target domain as well as learning the specific distribution changes in the target domain as compared to the source domains. In relation to the first part of identifying the distribution changes across environments, I find that certain comparisons are missing that I'd like to point out. First, the idea is similar to [1] which also aims to encode the changes in the distribution across the domains using a graphical model as well. Secondly, there have been attempts at using observational data to identify the differences across domains using two-sample testing approaches [2]. Such methods can aid in understanding not just where the differences exist across domains but also analyze the effect of these shifts on the posteriors. Discussion on such methods is missing in the current setting.
Review Point: - The motivation for using latent-variable CGAN is not very clear. Since Gaussian Processes also are capable of approximating the posteriors, the merit of CGAN seems blur here. A better motivation for using the said approach can help clarify the said advantages.
Review Point: - Can multi-environment causal discovery approaches be used for learning the causal graph across all domains, which can then be combined with the domain adaptation methods? [1] Subbaswamy, Adarsh, and Suchi Saria. "I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models." arXiv preprint arXiv:2002.08948 (2020). [2] Rabanser, Stephan, Stephan Günnemann, and Zachary Lipton. "Failing loudly: An empirical study of methods for detecting dataset shift." Advances in Neural Information Processing Systems. 2019.
==================================================

Focused review:

weaknesses are:
the organization of the paper could be improved, and
the paper lacks any tips for practitioners.
1: Section 3 explains the general ideas while Sections 4 and 5 give rigorous discussions about the loss and optimization dynamics, respectively.
I noticed this structure only after reading through the three sections, which was confusing to me. Explaining the structure of the discussion at an appropriate place (at the end of Introduction or at the beginning of Section 3) may improve the clarity.
2: Most practitioners use nonlinear VAEs, but the paper provides a negative result in that the VAE objective can induce a larger intrinsic dimension and larger support of data distribution. Is there any wisdom or tip from the theory discussed in the paper for practitioners? Such information will strengthen the importance of the paper. For example, the impact of the choice of latent dimensionality r
or a way to choose r
can help VAE users.
Question about gradient flow analysis: can we extend to stochastic gradient setup? In the linear case, the expectation over latent variable z
in the VAE objective is analytic. On the other hand, we need a Monte-Carlo approximation for typical nonlinear cases using the reparameterization trick, etc.

Review Point: 1: Section 3 explains the general ideas while Sections 4 and 5 give rigorous discussions about the loss and optimization dynamics, respectively. I noticed this structure only after reading through the three sections, which was confusing to me. Explaining the structure of the discussion at an appropriate place (at the end of Introduction or at the beginning of Section 3) may improve the clarity.
Review Point: 2: Most practitioners use nonlinear VAEs, but the paper provides a negative result in that the VAE objective can induce a larger intrinsic dimension and larger support of data distribution. Is there any wisdom or tip from the theory discussed in the paper for practitioners? Such information will strengthen the importance of the paper. For example, the impact of the choice of latent dimensionality r or a way to choose r can help VAE users. Question about gradient flow analysis: can we extend to stochastic gradient setup? In the linear case, the expectation over latent variable z in the VAE objective is analytic. On the other hand, we need a Monte-Carlo approximation for typical nonlinear cases using the reparameterization trick, etc.
==================================================

Focused review:

Weaknesses:
1.The method proposed is not novel. SwAV is widely utilised in numerous RL and computer vision domains. The further proposed Temporal Consistency appears to be a incremental trick, and it brings only marginal improvements in relation to Figure 5.
2.The overall method is only able to achieve marginal improvements over the simple baseline: Dreamer+Context (see Figure 3). . The results indicate that the proposed method is incremental.
3.Despite the fact that CaDM, TMCL, and RIA were not initially proposed for high-dimensional control tasks, they can be utilised for such tasks. However, this paper lacks these three essential dynamics generalisation baselines, so the baselines used in this method are weak, and the results do not convincingly demonstrate the effectiveness of the proposed method.
4.How would the experimental outcomes change if we used multiple time parts instead of two time parts to improve Temporal Consistency?

Review Point: 1.The method proposed is not novel. SwAV is widely utilised in numerous RL and computer vision domains. The further proposed Temporal Consistency appears to be a incremental trick, and it brings only marginal improvements in relation to Figure 5.
Review Point: 2.The overall method is only able to achieve marginal improvements over the simple baseline: Dreamer+Context (see Figure 3).
Review Point: . The results indicate that the proposed method is incremental.
Review Point: 3.Despite the fact that CaDM, TMCL, and RIA were not initially proposed for high-dimensional control tasks, they can be utilised for such tasks. However, this paper lacks these three essential dynamics generalisation baselines, so the baselines used in this method are weak, and the results do not convincingly demonstrate the effectiveness of the proposed method.
Review Point: 4.How would the experimental outcomes change if we used multiple time parts instead of two time parts to improve Temporal Consistency?
==================================================

Focused review:

- This paper presents two flipping method for generating corrupted labels: Uniform and Pair. However, it is confusing that the method from previous works uses a different flipping method such as 'Symmetric' and 'Asymmetric' from [28], [39], or [43]. And they are all compared with the method of Uniform and Pair in this article, which are not in previous works. It is curious why the two flipping is proposed and used for comparison to previous works. - Though the proposed method is compared to many previous works, the number of the dataset used is limited(two dataset CIFAR 10/100 from similar domain and Clothing1M). Authors need to consider giving performance comparison to other datasets so that the excellence of the method be stood out.

Review Point: - This paper presents two flipping method for generating corrupted labels: Uniform and Pair. However, it is confusing that the method from previous works uses a different flipping method such as 'Symmetric' and 'Asymmetric' from [28], [39], or [43]. And they are all compared with the method of Uniform and Pair in this article, which are not in previous works. It is curious why the two flipping is proposed and used for comparison to previous works.
Review Point: - Though the proposed method is compared to many previous works, the number of the dataset used is limited(two dataset CIFAR 10/100 from similar domain and Clothing1M). Authors need to consider giving performance comparison to other datasets so that the excellence of the method be stood out.
==================================================

Focused review:

The main limitation is that the study only presents the results in an asymptotic regime. It is not directly clear to the referee that how much insight can be shedded into practically important issues, either in a finite data / model, or beyond the linear model. Nonetheless, I am convince that the work is still important enough. A few minor comments that can be improved. 1) Figure 1 is actually not referenced in the main text. 2) page 2, it would be nice to give a reference to the statement on line 41. 3) I find Definition 2 hard to understand. Especially what is "the same order" ? Does it refer to entrywise of the same order or the vector norm of the same order. It would be better to have a more precise definition, preferrably with a forma. 4) line 241, change "conjuncture" to "conjecture". One last point is that some numerical results on more complicated models would greatly strengthen the study, e.g., on simple two-layer neural networks.

Review Point: 1) Figure 1 is actually not referenced in the main text.
Review Point: 2) page 2, it would be nice to give a reference to the statement on line 41.
Review Point: 3) I find Definition 2 hard to understand. Especially what is "the same order" ? Does it refer to entrywise of the same order or the vector norm of the same order. It would be better to have a more precise definition, preferrably with a forma.
Review Point: 4) line 241, change "conjuncture" to "conjecture". One last point is that some numerical results on more complicated models would greatly strengthen the study, e.g., on simple two-layer neural networks.
==================================================

Focused review:

Weaknesses: 1. The motivation for the bi-hypersphere compression is still difficult to understand. The authors claim that anomalous data may appear in the empty inner decision region. But Figure 3 did not show this phenomenon. More importantly, the authors did not explain why bi-hypersphere compression can help. For example, if the normal area becomes more compact, how can we guarantee that anomalous data will not appear in the empty inner decision region? The authors may need to provide some comparisons to show the proposed method can indeed solve this problem. 2. Improvements on some of the datasets seem unreasonable. For example, in Table3, compared with DOHSC, DO2HSC often improves the results by less than 2% on most of the datasets. But on class 1 of ER_MD, DO2HSC has a more than 20% improvement. Analysis needs to be provided. For example, it will be very interesting to show, on ER_MD, more anomalous data will appear in the empty inner decision region so that DO2HSC has an impressive improvement on this dataset.

Review Point: 1. The motivation for the bi-hypersphere compression is still difficult to understand. The authors claim that anomalous data may appear in the empty inner decision region. But Figure 3 did not show this phenomenon. More importantly, the authors did not explain why bi-hypersphere compression can help. For example, if the normal area becomes more compact, how can we guarantee that anomalous data will not appear in the empty inner decision region? The authors may need to provide some comparisons to show the proposed method can indeed solve this problem.
Review Point: 2. Improvements on some of the datasets seem unreasonable. For example, in Table3, compared with DOHSC, DO2HSC often improves the results by less than 2% on most of the datasets. But on class 1 of ER_MD, DO2HSC has a more than 20% improvement. Analysis needs to be provided. For example, it will be very interesting to show, on ER_MD, more anomalous data will appear in the empty inner decision region so that DO2HSC has an impressive improvement on this dataset.
==================================================

Focused review:

1. The title is misleading and the authors might overclaim their contribution. Indeed, the stochastic problem in Eq.(1) is a special instance of nonconvex-concave minimax problems and equivalent to nonconvex compositional optimization problem in Eq.(2). Solving such problem is easier than the general case consider in [23, 34]; see also (Rafique, Arxiv 1810.02060) and (Thekumparampil, NeurIPS'19). In addition, the KKT points and approximate KKT points are also defined based on such special structure. 2. The literature review is not complete. The authors mainly focus on the algorithms for stochastic compositional optimization instead of stochastic nonconvex-concave minimax optimization. 3. The algorithm is not single-loop in general. To be more specific, Algorithm 1 needs to solve Eq.(9) at each loop. This is also a nonsmooth strongly convex problem in general and the solution does not have the closed form. To this end, what is the advantage of Algorithm 1 over prox-linear algorithms in nonsmooth case? 4. Given the current stochastic problem in Eq.(1), I believe that the prox-linear subproblem can be reformulated using the conjugate function and becomes the same as the subproblem in Algorithm 1. That is to say, we can simply improve prox-linear algorithms for solving stochastic problem in Eq.(1). This makes the motivation of Algorithm 1 unclear. 5. The proof techniques heavily depend on the biased hybrid estimators introduced in [29]. The current paper does not convince me that such extension is nontrivial and has sufficient technical novelty.

Review Point: 1. The title is misleading and the authors might overclaim their contribution. Indeed, the stochastic problem in Eq.(1) is a special instance of nonconvex-concave minimax problems and equivalent to nonconvex compositional optimization problem in Eq.(2). Solving such problem is easier than the general case consider in [23, 34]; see also (Rafique, Arxiv 1810.02060) and (Thekumparampil, NeurIPS'19). In addition, the KKT points and approximate KKT points are also defined based on such special structure.
Review Point: 2. The literature review is not complete. The authors mainly focus on the algorithms for stochastic compositional optimization instead of stochastic nonconvex-concave minimax optimization.
Review Point: 3. The algorithm is not single-loop in general. To be more specific, Algorithm 1 needs to solve Eq.(9) at each loop. This is also a nonsmooth strongly convex problem in general and the solution does not have the closed form. To this end, what is the advantage of Algorithm 1 over prox-linear algorithms in nonsmooth case?
Review Point: 4. Given the current stochastic problem in Eq.(1), I believe that the prox-linear subproblem can be reformulated using the conjugate function and becomes the same as the subproblem in Algorithm 1. That is to say, we can simply improve prox-linear algorithms for solving stochastic problem in Eq.(1). This makes the motivation of Algorithm 1 unclear.
Review Point: 5. The proof techniques heavily depend on the biased hybrid estimators introduced in [29]. The current paper does not convince me that such extension is nontrivial and has sufficient technical novelty.
==================================================

Focused review:

Weaknesses: Clarification is needed in several places.
1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures? - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better. 

Review Point: 1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
Review Point: 2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
Review Point: 3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
Review Point: 4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures?
Review Point: - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.
==================================================

Focused review:

I wouldn't call it a weakness, but the relatively small improvement from using C-RBP over RBP and BPTT on MS-COCO makes me wonder whether the authors are being too conservative in their modification of ResNet-FPN into a recurrent network. Only the Mask-RCNN layers have been replaced by an RNN, whereas in principle the entire ResNet backbone could be made into a shallower recurrent model, and interaction between feature levels in the FPN could as well. I don't think any new experiments are necessary for this submission's acceptance, but I'm eager to see what happens when these methods are fully applied to large-scale computer vision models. Edits after seeing author response and other reviews: 1. Adding a future directions section to the main text seems like a good use of the extra space allowed in revision. The authors might also comment on whether there are any additional issues in using more "fully" recurrent architectures, not just the ones here that replace a small part of the ResNet-FPN-Mask-RCNN with their hGRU. 2. Reviewer 4 pointed out that, according to leaderboards, there are much more successful algorithms for doing panoptic segmentation on COCO-stuff test-dev. I think this deserves mention in the text, but I largely agree with the authors' response that many (most?) of these methods are not given enough detail to reimplement and also are not tested rigorously enough in their report to know where improvements are coming from (core architecture, hyperparameters, etc.) I think the authors took the right approach here of making precise changes to a well-known, publicly available baseline architecture (which is widely viewed to be a landmark for segmentation tasks.) Hence my confidence that, even though the performance/efficiency improvement is pretty small, it's due to the authors' proposed techniques.

Review Point: 1. Adding a future directions section to the main text seems like a good use of the extra space allowed in revision. The authors might also comment on whether there are any additional issues in using more "fully" recurrent architectures, not just the ones here that replace a small part of the ResNet-FPN-Mask-RCNN with their hGRU.
Review Point: 2. Reviewer 4 pointed out that, according to leaderboards, there are much more successful algorithms for doing panoptic segmentation on COCO-stuff test-dev. I think this deserves mention in the text, but I largely agree with the authors' response that many (most?) of these methods are not given enough detail to reimplement and also are not tested rigorously enough in their report to know where improvements are coming from (core architecture, hyperparameters, etc.) I think the authors took the right approach here of making precise changes to a well-known, publicly available baseline architecture (which is widely viewed to be a landmark for segmentation tasks.) Hence my confidence that, even though the performance/efficiency improvement is pretty small, it's due to the authors' proposed techniques.
==================================================

Focused review:

Most of the experiments focus on spreadsheet related task including formula prediction (NCP task helps) and cell type classification (NRP tasks helps). For Table QA task, it would be better to conduct experiments on more comprehensive datasets like WikiTableQuestion (TableQA), TabFact (Table-base fact verification) following TaPEx. 
Questions: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability? 
2. In Table 2, what about the result of SpreadsheetCoder under 20% train set? 
3. L174, is there an [SEP] exists between columns, like the delimiter among value cells?
Suggestions: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part. 
2. The order of terms in L288-L290 should be aligned with Figure 2. 
3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example. 
4. While the error analysis is conducted in Appendix, there is no case study to make the improvement more concretely. Typos: 1. Table#1 in Figure 1: (C3-B3)/B3 -> (D3-C3)/C3 

Review Point: 1. For table-text reasoning, why NRP task + formula-based prompt improve this reasoning ability?
Review Point: 2. In Table 2, what about the result of SpreadsheetCoder under 20% train set?
Review Point: 3. L174, is there an [SEP] exists between columns, like the delimiter among value cells? Suggestions:
Review Point: 1. A little introduction of 'Formula', 'Sketch', 'Range', 'table hierarchies' in session2 will lead to better understanding, which are often used in the latter part.
Review Point: 2. The order of terms in L288-L290 should be aligned with Figure 2.
Review Point: 3. In Figure 2, It would be better to replace [ RANDOM …… TOKENS …… from …… VOCAB ] with an specific example.
==================================================

Focused review:

Weaknesses:
1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.
2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.
3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.

Review Point: 1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.
Review Point: 2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.
Review Point: 3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.
==================================================

Focused review:

Relevance: The application is really quite niche which could mean that the equivariant architecture solution could get lost in the swamp of NeurIPS papers. Significance: I would really have hoped that the authors devoted a little bit more time to answering the following two claims: 1) Why is phylogenetic inference interesting to the broader ML community, and more specifically what can we do with better phylogenetic inference tools? Or, is there a larger problem that the development of this tool helps to solve? [Thanks for the clarification. I hope that some of the extra space for the camera ready goes to discuss your response.] 2) Why exactly is the "diagonal Lognormal branch design distribution ... not ... flexible enough"? My understanding is that MCMC methods are essentially the state of the art in this area, so is there evidence for substantial correlation between branches for these runs? [Thanks for the argument, but what I was really hoping for was some sort of traceplot demonstrating that the MCMC (and flow) methods actually pick up correlations over these parameters...] Empirical Evaluation: See point 2) above. You probably need to justify the usage of the importance sampling method in the VI approximation a bit better, although some googling on my end finds that the importance sampling from the VI estimate works pretty well - https://escholarship.org/content/qt77d8v106/qt77d8v106.pdf. The natural choice in the ML community is probably annealed importance sampling. [Thanks for the clarification and promise of experiment.] From a ML perspective, it's well known that normalizing flows produce really high quality likelihood estimates (Section 6.1 https://arxiv.org/pdf/1912.02762.pdf), which could explain why adding new layers just seems to continue decreasing the MLL. Is there a tradeoff between the number of layers and the MLL beyond which adding new layers to the flow just stops improving the likelihood (or it even becomes untrainable)? [Somewhat unaddressed.] Novelty: Although this is one of the first papers to study equivariant normalizing flows, it's not the first, and Section 5.6 of https://arxiv.org/pdf/1912.02762.pdf contains the broad portions of your proofs. However, they broadly just point to two workshop papers (one of which seems to have been extended into the ICML paper cited above).

Review Point: 1) Why is phylogenetic inference interesting to the broader ML community, and more specifically what can we do with better phylogenetic inference tools? Or, is there a larger problem that the development of this tool helps to solve? [Thanks for the clarification. I hope that some of the extra space for the camera ready goes to discuss your response.] 2) Why exactly is the "diagonal Lognormal branch design distribution ... not ... flexible enough"? My understanding is that MCMC methods are essentially the state of the art in this area, so is there evidence for substantial correlation between branches for these runs? [Thanks for the argument, but what I was really hoping for was some sort of traceplot demonstrating that the MCMC (and flow) methods actually pick up correlations over these parameters...] Empirical Evaluation: See point 2) above. You probably need to justify the usage of the importance sampling method in the VI approximation a bit better, although some googling on my end finds that the importance sampling from the VI estimate works pretty well - https://escholarship.org/content/qt77d8v106/qt77d8v106.pdf. The natural choice in the ML community is probably annealed importance sampling. [Thanks for the clarification and promise of experiment.] From a ML perspective, it's well known that normalizing flows produce really high quality likelihood estimates (Section 6.1 https://arxiv.org/pdf/1912.02762.pdf), which could explain why adding new layers just seems to continue decreasing the MLL. Is there a tradeoff between the number of layers and the MLL beyond which adding new layers to the flow just stops improving the likelihood (or it even becomes untrainable)? [Somewhat unaddressed.] Novelty: Although this is one of the first papers to study equivariant normalizing flows, it's not the first, and Section 5.6 of https://arxiv.org/pdf/1912.02762.pdf contains the broad portions of your proofs. However, they broadly just point to two workshop papers (one of which seems to have been extended into the ICML paper cited above).
==================================================

Focused review:

Weaknesses ---
W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.
W2. An important weakness that is not mentioned anywhere is that the factors A ( k )
in Eq (8) must have dimensions that factorize the dimensions of W
. For example, they must satisfy ∏ k = 1 S a j ( k ) = w j
. So what is hailed as greater flexibility of the proposed model in the caption of Fig 1 is in fact a limitation. For example, if the dimensions of W
are prime numbers, then for each mode of W
, only a single tensor A ( k )
can have a non-singleton dimension in that same mode. This may be fixable with appropriate zero padding, but this has to at least be discussed and highlighted in the paper.
W3. The 2nd point in the list of contributions in Sec 1 claims that the paper provides a means of finding the best approximation in the proposed format. In fact, it is easy to see that this claim is likely to be false: The decomposition corresponds to a difficult non-convex optimization problem, and it is therefore unlikely that a simple algorithm with a finite number of steps could solve it optimally.
W4. SeKron is claimed to generalize various other decompositions. But it is not clear that the proposed algorithm could ever reproduce those decompositions. For example, since there is no SVD-based algorithm for CP decomposition, I strongly suspect that the proposed algorithm (which is SVD-based) cannot recreate the decomposition that, say, an alternating least squares based approach for CP decomposition would achieve.
W5. The paper is unclear and poor notation is used in multiple places. For examples:
Subscripts are sometimes used to denote indices (e.g., Eq (5)), sometimes to denote sequences of tensors (e.g., Eqs (7), (8)), and sometimes used to denote both at the same time (e.g., Thm 3, Eq (35))! This is very confusing.
It is unclear how Eq (7) follows from Eq (5). The confusing indices exacerbate this.
In Thm 1, A ( k )
are tensors, so it's unclear what you mean by " R i
are ranks of intermediate matrices".
In Alg 1, you apply SVD to a 3-way tensors. This operation is not defined. If you mean batched SVD, you need to specify that. The W r 1 ⋯ r k − 1 ( k )
tensors in Eq (10) haven't been defined.
The definition of Unfold below Eq (13) is ambiguous. Similarly, you say that Mat reformulates a tensor to a matrix, but list the output space as R d 1 ⋯ d N
, i.e., indicating that the output is a vector.
Below Eq (15) you discuss "projection". This is not an appropriate term to use, since these aren't projections; projection is a term with a specific meaning in linear algebra.
In Eq (16), the r k
indices appear on the right-hand side but not on the left-hand side.

Review Point: --- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.
==================================================

Focused review:

1) The authors claim that the reason for SSL can not work well is that they adopt biased pseudo-labels. However, most deep SSL methods are based on the smooth assumption and encourage the original data and the augmented data have similar predictions. Actually, they don't need to assign a pseudo-label to an unlabeled example explicitly. So I think the claim does not make sense. 2) The proposed method in Section 3 is based on the true class distribution of unlabeled data. This is not feasible in real applications. Although the authors give an estimation method in the experiment section, it is still a problem with the practicability of the proposal. More analysis about the estimation method need to be discussed, for example, the distance between the estimated distribution and the true distribution 3) All experiment results are conducted on CIFAR-10 datasets, experiments on more data sets should be reported to demonstrate the effectiveness of the proposal. **The authors have carefully addressed issues on 2) and 3) in the rebuttal**

Review Point: 1) The authors claim that the reason for SSL can not work well is that they adopt biased pseudo-labels. However, most deep SSL methods are based on the smooth assumption and encourage the original data and the augmented data have similar predictions. Actually, they don't need to assign a pseudo-label to an unlabeled example explicitly. So I think the claim does not make sense.
Review Point: 2) The proposed method in Section 3 is based on the true class distribution of unlabeled data. This is not feasible in real applications. Although the authors give an estimation method in the experiment section, it is still a problem with the practicability of the proposal. More analysis about the estimation method need to be discussed, for example, the distance between the estimated distribution and the true distribution 3) All experiment results are conducted on CIFAR-10 datasets, experiments on more data sets should be reported to demonstrate the effectiveness of the proposal. **The authors have carefully addressed issues on 2) and 3) in the rebuttal**
==================================================

Focused review:

I would like to get clarifications about the following: 1. Based on the intuition from heuristic search the authors chose to subtract the representation of H(x) from F(x). This is one approach. Have you tried other approaches as well, for example, using another NN over concatenated representations with the gradient reversal layer applied only on the fundament network? 2. Line 118 states that each sub-network tends to model local domain-specific property. Why? Can it be shown (even empirically)? Do they model different properties? And if that is indeed correct why a summation over the sub-networks' representations is the right thing to do? 3. The authors published their code and deserve compliments on that. However, the paper lacks in presenting several implementation details. Some I was able to find in the code. For example, what networks were used for the fundament network and heuristic network? How the hyper-parameters were chosen? Was early stopping applied? If so, on which part of the dataset (train, val, test) and on which domain? I would like to get answers to these questions and in general more details. From the code, it seems that early stopping was done based on the target-test set. If that is indeed the case, in my opinion, it is very problematic in an unsupervised setup.

Review Point: 1. Based on the intuition from heuristic search the authors chose to subtract the representation of H(x) from F(x). This is one approach. Have you tried other approaches as well, for example, using another NN over concatenated representations with the gradient reversal layer applied only on the fundament network?
Review Point: 2. Line 118 states that each sub-network tends to model local domain-specific property. Why? Can it be shown (even empirically)? Do they model different properties? And if that is indeed correct why a summation over the sub-networks' representations is the right thing to do?
Review Point: 3. The authors published their code and deserve compliments on that. However, the paper lacks in presenting several implementation details. Some I was able to find in the code. For example, what networks were used for the fundament network and heuristic network? How the hyper-parameters were chosen? Was early stopping applied? If so, on which part of the dataset (train, val, test) and on which domain? I would like to get answers to these questions and in general more details. From the code, it seems that early stopping was done based on the target-test set. If that is indeed the case, in my opinion, it is very problematic in an unsupervised setup.
==================================================

Focused review:

weaknesses:
Unfortunately there are two major issues with this paper. First, the proposed method contains too many, if not all, heuristic constructions that, in my opinion, may not make sense in general. Second, the empirical evaluations are missing important information about the datasets and details of the experiments, and hence, the reported results may be potentially misleading.
In what follows I provide details.
About the proposed methods:
Why is mean aggregation (instead of other possible aggregation methods) used to compute the feature vector of a hyperedge? Some discussions are needed to justify the choice.
When assigning nodes to hyperedges, why is the cosine similarity used? Can we use other similarity measures? My guess is that the authors use the cosine similarity because in their experiments for node classification, the datasets are citation networks and the node feature vectors come from bag-of-words encoding. In these datasets, cosine similarity aligns well with class labels. However, no explanation is ever provided for the choice of cosine similarity in general. Discussions on the choice of similarity measure in the coarsening step should be provided.
When assigning nodes to hyperedges, and when the hypergraph does not have node features, the authors suggest to compute some "importance" measure, e.g., w_j/d_j. But what if w_j = d_j for all j? The authors suggest to break the tie randomly, but in this extreme case when we randomly assign every node to an arbitrary hyperedge in the coarsening step, will the method still provide meaningful embeddings? More explanations and discussions are needed.
How many iterations of refinement (i.e., Laplacian smoothing) is suggested? Table 5 in the appendix (not in Section 4) seems to empirically demonstrate that around 30 iterations is sufficient for the citation networks, but why 30, not 5? Moreover, why does the improvement stops after around 100 iterations? Is it due to the fact that Laplacian smoothing converges reasonably after 100 iterations, or due to the fact that for these citation networks, local network structure (i.e., locally smoothed feature vectors) is already very informative about the class labels, so global network structure (i.e., fully smoothed feature vectors) is not useful? Some discussions should be provided either in the main text or in the appendix.
About the experiments
The paper does not provide a citation for the datasets in Table 1. The authors claim that they "[use] the standard hypergraph datasets from prior works", but I could not find verify the source of the datasets. For example, the Cora dataset that the authors provided in the supplementary material does not match with the Cora dataset used in [Yadati et al. (2019)] or the one used in [1]. The original Cora dataset has 2708 nodes, why is the Cora dataset used in this paper has only 1434 nodes? I tried to keep the largest connected component, but I still get slightly less than 2400 nodes.
Since the source of datasets is not clear to me, I cannot verify the validity of the empirical results. I suggest the authors use the same publicly available datasets used by [Yadati et al. (2019)], and follow the same train/test splits. This will ensure consistency for comparing methods and results reported across papers. Moreover, it will improve reproducibility. Please also include other datasets in the supplementary material, e.g., Citeseer, Pubmed, DBLP, right now the supplementary material only contains the Cora datasets.
The authors should compare their method with [1] and [2]. [2] is particularly relevant because [2] is also an unsupervised node embedding method for hypergraphs. Maybe more importantly, Table 2 in [2] shows that the method in [2] is significantly better than the method proposed in this paper, for all the node classification tasks. But, as I mentioned in my previous point, the empirical settings may be very different. This highlights the importance of using the same benchmark hypergraphs and the same train/test splits. For example, for the Cora dataset, please report a test result when using 20 labelled nodes from each of the 7 classes. [1] applies GCN to the clique expansion of hypergraphs, so [1] is relevant and the authors should compare with [1].
Once node embeddings are obtained, how did you perform node classification? Did you train a multi-class classification model? If so, what loss function did you use? Minor
In high-order network analysis, people usually refer the number of nodes connected by a hyperedge as the order of that hyperedge. It might be helpful to add this as a footnote, as the term "degree" of a hyperedge sounds a bit less common.
The authors claim generality as a contribution in this work. However, without specific experiments on graphs (not hypergraphs), I could not see how generality is particularly important.
In Section 2.1, it might be more clear if graph embedding methods are categorized into supervised and unsupervised approaches.
Please discuss the details about parallel implementation in the appendix.
Additional references
[1] Hypergraph Neural Networks. Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, Yue Gao. 2019
[2] A nonlinear diffusion method for semi-supervised learning on hypergraphs. Francesco Tudisco, Konstantin Prokopchik, Austin R. Benson. 2021.

Review Point: 2019 [2] A nonlinear diffusion method for semi-supervised learning on hypergraphs. Francesco Tudisco, Konstantin Prokopchik, Austin R. Benson. 2021.
==================================================

Focused review:

1. In #141, “The OT loss will approximate well the dense areas of the crowd, but the approximation might be poorer for the low density areas of the crowd”. I wonder why there is a performance gap in the crowd areas and low-density areas for the OT loss. More details should be provided for explanation. 2. In #248, “In all experiments, DM-Count outperforms all other methods except CAN under MSE in NWPU (where they are comparable)”. Why the DM-Count performs bad on RMSE compared with CAN? 3. In Table 3, without using the TV loss, the performance of DM-Count is worse than Bayesian Loss (in Table 2). I think the OT loss may not robust. 4. Is this possible to transfer the DM-Count to other similar tasks, like keypoint regression? I think it is interesting if the DM-Count could solve the crowded problems in the human-pose estimation.

Review Point: 1. In #141, “The OT loss will approximate well the dense areas of the crowd, but the approximation might be poorer for the low density areas of the crowd”. I wonder why there is a performance gap in the crowd areas and low-density areas for the OT loss. More details should be provided for explanation.
Review Point: 2. In #248, “In all experiments, DM-Count outperforms all other methods except CAN under MSE in NWPU (where they are comparable)”. Why the DM-Count performs bad on RMSE compared with CAN?
Review Point: 3. In Table 3, without using the TV loss, the performance of DM-Count is worse than Bayesian Loss (in Table 2). I think the OT loss may not robust.
Review Point: 4. Is this possible to transfer the DM-Count to other similar tasks, like keypoint regression? I think it is interesting if the DM-Count could solve the crowded problems in the human-pose estimation.
==================================================

Focused review:

- It is hard to highlight the limitations of this work for me. Theoretical derivations are sound. However, I could not check the correctness of all proofs. The experimental evaluation is extensive and detailed. - The explanations can be expanded for the readers who are less familiar with GCNs and topological attacks.

Review Point: - It is hard to highlight the limitations of this work for me. Theoretical derivations are sound. However, I could not check the correctness of all proofs. The experimental evaluation is extensive and detailed.
Review Point: - The explanations can be expanded for the readers who are less familiar with GCNs and topological attacks.
==================================================

Focused review:

Weakness:
The paper combines two dynamic thresholds that exhibit positive and negative correlations with the average membrane potential. It would be better to explain the reason and the related mathematical formulation in detail. Plus, the author compares BDETT with four variants of the spiking actor-network (SAN), are there any other recent SNNs to compare with?
The experiments are restricted to the robot and control tasks. It would be better to include more tasks such as tasks in computer vision. 3. The supplementary material has a heavy overlap with the main body part.

Review Point: 3. The supplementary material has a heavy overlap with the main body part.
==================================================

Focused review:

1. The convergence result seems to be more 'qualitative', showing a general effect of different error sources, rather than 'quantitative'. Most of the terms have a complicated dependence on the parameters of the problem class. I think, they can hardly be computed in practice. For example, they did not provide a direct answer to the question: how to choose 's' in practice? It somewhat expected that for 'big enough 's', the stochastic approximation would be reasonably good. However, I still believe that the given theoretical answers may help to advance in developing distributed methods. 2. Numerical experiments. It is not clear what is the 'Accuracy' on the graphs (is it the functional residual?). It looks like all the methods stuck at some level (85-95). I think, it is important to demonstrate that in *some regimes* the method (for example, full Newton method) may achieve near 100 accuracy.

Review Point: 1. The convergence result seems to be more 'qualitative', showing a general effect of different error sources, rather than 'quantitative'. Most of the terms have a complicated dependence on the parameters of the problem class. I think, they can hardly be computed in practice. For example, they did not provide a direct answer to the question: how to choose 's' in practice? It somewhat expected that for 'big enough 's', the stochastic approximation would be reasonably good. However, I still believe that the given theoretical answers may help to advance in developing distributed methods.
Review Point: 2. Numerical experiments. It is not clear what is the 'Accuracy' on the graphs (is it the functional residual?). It looks like all the methods stuck at some level (85-95). I think, it is important to demonstrate that in *some regimes* the method (for example, full Newton method) may achieve near 100 accuracy.
==================================================

Focused review:

- The theoretical foundation builds off a number of assumptions, specifically assumption 5, that may limit its applicability in real world scenarios. - The work claims the attack is limited by the adversaries limited access to the attack set (the vulnerable nodes of the graph). But there is no discussion on how this attack set is generated, its properties, or its effect on the proposed methodology. - The three baseline attacks used in the experimental results are not sufficiently introduced. - Further, it is not clear why the chosen baselines are more sufficient comparisons than recent works like [1] and [3]. - While the results indicate that in many cases the methodology decreases the performance of the model, in many of the scenarios there is only a minor improvement over the baseline comparisons. - Lambda, the magnitude of perturbation on each node, is used as a measure of the strength of the attack, but intuitively it seems like J, the number of node perturbed, would be a more accurate judge of the attack strength.

Review Point: - The theoretical foundation builds off a number of assumptions, specifically assumption 5, that may limit its applicability in real world scenarios.
Review Point: - The work claims the attack is limited by the adversaries limited access to the attack set (the vulnerable nodes of the graph). But there is no discussion on how this attack set is generated, its properties, or its effect on the proposed methodology.
Review Point: - The three baseline attacks used in the experimental results are not sufficiently introduced.
Review Point: - Further, it is not clear why the chosen baselines are more sufficient comparisons than recent works like [1] and [3].
Review Point: - While the results indicate that in many cases the methodology decreases the performance of the model, in many of the scenarios there is only a minor improvement over the baseline comparisons.
Review Point: - Lambda, the magnitude of perturbation on each node, is used as a measure of the strength of the attack, but intuitively it seems like J, the number of node perturbed, would be a more accurate judge of the attack strength.
==================================================

Focused review:

Weaknesses:
No major weaknesses identified. Questions:
It would be interesting to see the variance observed when applying parameter-efficient methods. Previous work found large variance in downstream performance across different seeds when performing full fine-tuning [1,2,3]. This is something you could add, for example, to Table 2. Typos:
In Figure 2, the performance of Adapter is 21.00, not 20.46
The "v" subscript is missing in Eq. 5
[1] Dodge et al. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. 2021 [2] Mosbach et al. On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ICLR 2021 [3] Bugliarello et al. Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs. TACL 2021

Review Point: 2021 [2] Mosbach et al. On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ICLR 2021 [3] Bugliarello et al. Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs. TACL 2021
==================================================

Focused review:

Weakness
the set-up of experiments and results reported in this paper are still questionable. (Datasets, Architectures, FL settings such as participation ratio, data heterogeneity settings with Dirichlet distribution)
The empirical evidence for Theorem 1 may be needed.
------updated after rebuttal---------
Thank you for your detailed response for my raised concerns. still I think this work needs more comparisons with related FL approaches based on bayesian approaches, but most concerns are solved. So, I decided to increase my score from 4 to 5.
Despite their significant empirical results, there remain limitations: 1) More other methods should be conducted to verify its robustness in applications to advanced Personalized FL methods. 2) ablation studies on each factor of FedPop such as warm start strategy, gradient estimator etc... as well as hyperparamter settings of other regularization methods such as weight decay, augmentation...

Review Point: 1) More other methods should be conducted to verify its robustness in applications to advanced Personalized FL methods.
Review Point: 2) ablation studies on each factor of FedPop such as warm start strategy, gradient estimator etc... as well as hyperparamter settings of other regularization methods such as weight decay, augmentation...
==================================================

Focused review:

Weaknesses: - Important pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Schütt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018. - The experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate. - The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank-2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1? - In section 3, the authors speak of “collections of rank-p tensors”. However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2. Except for in sec 3.2.2, in which a p=3 tensor has a=1. - In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it’s the former, I don’t see how the network is equivariant. If it’s the latter, I don’t understand the last paragraph of 3.2.2, which says 1H \in R^{N x f_in}, which looks like a 0-tensor. - Can the authors clarify “To achieve translation equivariance, a constant tensor can be added to the output collection of tensors.”? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance. - Am I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a naïve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations). - The authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices?
Recommendation: In its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I’d like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score.
Suggestions for improvement: - Be clear about what the G object is and what eq 1 means. - Be explicit about types the objects, be more explicit about the indices that refer to the permutation representation, to the indices that refer to the Euclidean representation and the indices that refer to copies of the same representation. I think there is an opportunity to be more clear, more explicit, while reducing notational clutter. - Expand the related work section - Compare to the strong baselines that use the coordinates. - Provide argumentation for the claim to scale to 1M vertices.
Minor points: - Eq 7, \times should be \otimes? - Eq 14, what is j? - The authors write: “A, B and C are X, Y and Z respectively”. Perhaps this could be re-written to the easier to read “A=X, B=Y and C=Z”. This happens each time the word “respectively” is used. - Table 3 typo, gluster -> cluster
Post rebuttal
The authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7.

Review Point: - Important pieces of prior work are missing from the related work section. The paper seems to be strongly related to Tensor Field Networks (TFN) (Thomas et al. 2018), as both define Euclidean and permutation equivariant convolutions on point clouds / graphs. Furthermore, there are several other methods that operate on graph that are embedded in a Euclidean space, such as SchNet (Schütt et al 2017). The graph network methods currently discussed all do not include the point coordinates in their operations. Lastly, the proposed method operates globally linearly on features on a graph, equivariantly to permutations, which is done in prior work, e.g. Maron 2018.
Review Point: - The experimental section only compares to methods that in their convolution are unaware of the point coordinates (except for in the input features). A comparison to coordinate-aware methods, such as TFN or SchNet seems appropriate.
Review Point: - The core object, the isometric adjacency matrix G, is ill-defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank-2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. In section 3, it appears like that the defined objects D take the role of object G in the above, so what is the role of eq 1?
Review Point: - In section 3, the authors speak of “collections of rank-p tensors”. However, these objects seem to actually be tensors of the shape N^a x d^p, where N is the number of nodes, d is the dimensionality of the embedding, and a and p are natural numbers. These objects transform under both permutations and Euclidean transformations in the obvious way. Why not make this fact explicit? That would make section 3 much easier to read. It seems like that when p=0, then a=1, and when p>0, then a=2. Except for in sec 3.2.2, in which a p=3 tensor has a=1.
Review Point: - In Sec 3.2, what are f_in and f_out? Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation? If it’s the former, I don’t see how the network is equivariant. If it’s the latter, I don’t understand the last paragraph of 3.2.2, which says 1H \in R^{N x f_in}, which looks like a 0-tensor.
Review Point: - Can the authors clarify “To achieve translation equivariance, a constant tensor can be added to the output collection of tensors.”? The proposed method seems to only lead to translation invariant features. I do not follow how adding a constant tensor leads to translation equivariance that is not invariance.
Review Point: - Am I correct in understanding that the method scales cubic with the number of vertices (e.g. eqs 4, 6)? Or is there some sparsity used in the implementation, but not mentioned? Should we expect a method of cubic complexity to scale to 1M vertices? In a naïve implementation, a fast modern GPU with 14.2E12 flops would need 20h for a single 1Mx1M matrix-matrix multiplication (1E18 floating point operations).
Review Point: - The authors claim the method scales to 1M vertices, but I cannot find this in the experiments. Table 4 speaks of 155k vertices. How did the authors determine the method scales to 1M vertices? Recommendation: In its current form, I recommend rejection of this paper. Section 3 is insufficiently clear written, the related work lack important references to prior work and the experiments lack a comparison to potentially strong other methods. This is a shame, because I’d like to see this paper succeed, as the core idea is very strong. Significant improvements in the above criticisms can improve my score. Suggestions for improvement:
Review Point: - Be clear about what the G object is and what eq 1 means.
Review Point: - Be explicit about types the objects, be more explicit about the indices that refer to the permutation representation, to the indices that refer to the Euclidean representation and the indices that refer to copies of the same representation. I think there is an opportunity to be more clear, more explicit, while reducing notational clutter.
Review Point: - Expand the related work section - Compare to the strong baselines that use the coordinates.
Review Point: - Provide argumentation for the claim to scale to 1M vertices. Minor points:
Review Point: - The authors write: “A, B and C are X, Y and Z respectively”. Perhaps this could be re-written to the easier to read “A=X, B=Y and C=Z”. This happens each time the word “respectively” is used.
==================================================

Focused review:

- there are several places that have inaccurate descriptions or misleading: e.g. For IBP [29], the method is actually "certification method" because it introduces the interval bounds in the training. It is not based on the "verification" method. They use Interval bounds in the training, and some of their results use MIO verifier to evaluate the best test errors they can get. Also, the computation complexity of [12, 17, 18] in Line 27 are totally different. Some are polynomial time, some are NP-complete. Usually in this field, only the formal verification based method such as [17] will be described as computationally expensive. For line 29, those methods are not used for detecting adversarial examples, because they provide a certified region for consistent classifications as the input example x. They can only be used to detect guaranteed "non"-adversarial example of x given a new input x'. - there are also some places that are not clear: e.g. it's not clear how to train the prototypes w in equation 2. It looks like d is pre-defined and the only parameters are w. Also, what are the relations of eq 3 to eq 2? - The NPC models are not as commonly used as other models such as NN in my understanding, so it's not clear how useful/important the robustness analysis is in this regard.

Review Point: - there are several places that have inaccurate descriptions or misleading: e.g. For IBP [29], the method is actually "certification method" because it introduces the interval bounds in the training. It is not based on the "verification" method. They use Interval bounds in the training, and some of their results use MIO verifier to evaluate the best test errors they can get. Also, the computation complexity of [12, 17, 18] in Line 27 are totally different. Some are polynomial time, some are NP-complete. Usually in this field, only the formal verification based method such as [17] will be described as computationally expensive. For line 29, those methods are not used for detecting adversarial examples, because they provide a certified region for consistent classifications as the input example x. They can only be used to detect guaranteed "non"-adversarial example of x given a new input x'.
Review Point: - there are also some places that are not clear: e.g. it's not clear how to train the prototypes w in equation 2. It looks like d is pre-defined and the only parameters are w. Also, what are the relations of eq 3 to eq 2?
Review Point: - The NPC models are not as commonly used as other models such as NN in my understanding, so it's not clear how useful/important the robustness analysis is in this regard.
==================================================

Focused review:

Weaknesses: Given that the authors argue convincingly that the Schur decomposition should give much larger expressiveness, it is somewhat surprising that the performance gap between nnRNN and LSTM (0.12) is still twice as large as the one between nnRNN and expRNN (0.06). This seems to indicate that nnRNN are not really that much further ahead than the best-performing orthogonal RNN when compared to LSTM. How do the authors explain this, is this just due to the gating mechanism that nnRNN lack? Minor: - Isnât the result in Fig. 1b completely expected, given that an orthogonal RNN lacks the off-diagonal elements (which could boost hidden states) in the Schur form?

Review Point: - Isnât the result in Fig. 1b completely expected, given that an orthogonal RNN lacks the off-diagonal elements (which could boost hidden states) in the Schur form?
==================================================

Focused review:

- The authors compared their method to the baseline approach only. However, there are plenty of curriculum learning methods that could have been used as relevant state-of-the-art competing methods to compare with, e.g. [R1, R2, R3, R4]. Comparison with such competing methods is mandatory, in my opinion. - In Eq. (2.1), I believe that the non-linearity is typically applied before the pooling operation. - In terms of novelty, the idea of adding some Gaussian kernels to the network is quite straightforward and simple. Even so, it is not clear why it works so well. The provided motivation is not enough. I would have like to see some visualizations of low-level, mid-level and high-level filters and how these evolve during training in order to figure out what is happening. All the experiments are performed on images, so I would consider this a vision paper. A vision paper without figures is not a properly written vision paper. - Does the approach apply to data other than images? Until proven otherwise, it should clearly stated in the title that the approach applies to images only, e.g. "Curriculum by Smoothing for Images". - Are the improvements statistically significant? A statistical test should be performed to test the null hypothesis. Missing references: [R1] Saxena, S., Tuzel, O. and DeCoste, D., 2019. Data parameters: A new family of parameters for learning a differentiable curriculum. In Advances in Neural Information Processing Systems (pp. 11093–11103). [R2] Soviany, P., Ardei, C., Ionescu, R.T. and Leordeanu, M., 2020. Image difficulty curriculum for generative adversarial networks (CuGAN). In The IEEE Winter Conference on Applications of Computer Vision (pp. 3463-3472). [R3] Penha, G. and Hauff, C., 2020, April. Curriculum Learning Strategies for IR. In European Conference on Information Retrieval (pp. 699-713). [R4] Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. "Progressive growing of gans for improved quality, stability, and variation." arXiv preprint arXiv:1710.10196 (2017).

Review Point: - The authors compared their method to the baseline approach only. However, there are plenty of curriculum learning methods that could have been used as relevant state-of-the-art competing methods to compare with, e.g. [R1, R2, R3, R4]. Comparison with such competing methods is mandatory, in my opinion.
Review Point: - In Eq. (2.1), I believe that the non-linearity is typically applied before the pooling operation.
Review Point: - In terms of novelty, the idea of adding some Gaussian kernels to the network is quite straightforward and simple. Even so, it is not clear why it works so well. The provided motivation is not enough. I would have like to see some visualizations of low-level, mid-level and high-level filters and how these evolve during training in order to figure out what is happening. All the experiments are performed on images, so I would consider this a vision paper. A vision paper without figures is not a properly written vision paper.
Review Point: - Does the approach apply to data other than images? Until proven otherwise, it should clearly stated in the title that the approach applies to images only, e.g. "Curriculum by Smoothing for Images".
Review Point: - Are the improvements statistically significant? A statistical test should be performed to test the null hypothesis. Missing references: [R1] Saxena, S., Tuzel, O. and DeCoste, D., 2019. Data parameters: A new family of parameters for learning a differentiable curriculum. In Advances in Neural Information Processing Systems (pp. 11093–11103). [R2] Soviany, P., Ardei, C., Ionescu, R.T. and Leordeanu, M., 2020. Image difficulty curriculum for generative adversarial networks (CuGAN). In The IEEE Winter Conference on Applications of Computer Vision (pp. 3463-3472). [R3] Penha, G. and Hauff, C., 2020, April. Curriculum Learning Strategies for IR. In European Conference on Information Retrieval (pp. 699-713). [R4] Karras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen. "Progressive growing of gans for improved quality, stability, and variation." arXiv preprint arXiv:1710.10196 (2017).
==================================================

Focused review:

- Despite the extensive empirical evaluations, the three multimodal variants as proposed by the paper are direct extensions of the DeepCluster algorithm [4]. The main contributions appear to be (1) a working pipeline which demonstrates that variants of DeepCluster works with video and audio encoders; (2) scaling up the training to extremely large datasets. While both contributions are interesting, they appear to me to be less relevant to the audience of NeurIPS. - I would appreciate if the authors could come up with an explanation / conjecture why crossmodal outperforms single-modal; and when XDC outperforms CDC and MDC. It would also be great if such conjectures are accompanied with empirical evaluations on more diverse tasks than the three classification datasets. That would help the audience understand when to apply the XDC variant of DeepCluster (e.g. is it specific to audio and visual in videos, or is it more general?), and have a nice intuition on why it is so effective, thus can further apply the method on other multimodal tasks. - Minor: the claim that "self-supervised video representation learning outperforms large-scale fully-supervised pretraining for action recognition" is a bit misleading to me as the self-supervised approach requires one or two orders of magnitude larger datasets.

Review Point: - Despite the extensive empirical evaluations, the three multimodal variants as proposed by the paper are direct extensions of the DeepCluster algorithm [4]. The main contributions appear to be (1) a working pipeline which demonstrates that variants of DeepCluster works with video and audio encoders; (2) scaling up the training to extremely large datasets. While both contributions are interesting, they appear to me to be less relevant to the audience of NeurIPS.
Review Point: - I would appreciate if the authors could come up with an explanation / conjecture why crossmodal outperforms single-modal; and when XDC outperforms CDC and MDC. It would also be great if such conjectures are accompanied with empirical evaluations on more diverse tasks than the three classification datasets. That would help the audience understand when to apply the XDC variant of DeepCluster (e.g. is it specific to audio and visual in videos, or is it more general?), and have a nice intuition on why it is so effective, thus can further apply the method on other multimodal tasks.
Review Point: - Minor: the claim that "self-supervised video representation learning outperforms large-scale fully-supervised pretraining for action recognition" is a bit misleading to me as the self-supervised approach requires one or two orders of magnitude larger datasets.
==================================================

Focused review:

- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. 
 * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. 
 * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
- The number of questions with multi-span answers in the proposed dataset is small. 
 * The sizes are Train: 6465, Val: 646, Test: 646. 
 * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. 
 * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. 
#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].
[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.
#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world 

Review Point: - How the proposed multiple-span answer setting is essential in real-world applications is unclear to me.
Review Point: * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting.
Review Point: * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.
Review Point: - The number of questions with multi-span answers in the proposed dataset is small.
Review Point: * The sizes are Train: 6465, Val: 646, Test: 646.
Review Point: * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test.
Review Point: * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. #### Minor comments:
Review Point: - L47: consistinga of -> consisting of - L348: real-word -> real-world
==================================================

Focused review:

1. I don't think the experiments (Table 3) in Section 5.2 are comparing with the strongest baseline for certifying l_infty robustness. For example, for large perturbation (e.g. 8/255 or 16/255), IBP based methods (see "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models") are known to significantly outperform random-smoothing based methods (e.g., ~32% certified accuracy on CIFAR-10 for 8/255 radius even though the paper was published 2 years ago). Even for certifying small perturbation radius like 2/255, the paper still does not compare with the strongest baseline: a better certified accuracy for 2/255 is around 63% in the same setup (e.g., [31]), while you report 60%. 2. [30,31,32] have shown that Gaussian noise is optimal to certify l_p robustness for p>2 up to constant factor, while the paper claims that they can improve the certified robustness for p>2 by looking at a better noise distribution than Gaussian. To me, the only explanation for this claim is that the improvement is tiny (i.e., they only improve a small constant factor). 3. The design in Section 3.2 (i.e., the involvement of dual form) and the design of new noise distribution in Section 4 seem to be two totally different elements to improve the robustness. It is unclear to me which element play a more important role. In other words, the ablation study about the two elements is missing.

Review Point: 1. I don't think the experiments (Table 3) in Section 5.2 are comparing with the strongest baseline for certifying l_infty robustness. For example, for large perturbation (e.g. 8/255 or 16/255), IBP based methods (see "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models") are known to significantly outperform random-smoothing based methods (e.g., ~32% certified accuracy on CIFAR-10 for 8/255 radius even though the paper was published 2 years ago). Even for certifying small perturbation radius like 2/255, the paper still does not compare with the strongest baseline: a better certified accuracy for 2/255 is around 63% in the same setup (e.g., [31]), while you report 60%.
Review Point: 2. [30,31,32] have shown that Gaussian noise is optimal to certify l_p robustness for p>2 up to constant factor, while the paper claims that they can improve the certified robustness for p>2 by looking at a better noise distribution than Gaussian. To me, the only explanation for this claim is that the improvement is tiny (i.e., they only improve a small constant factor).
Review Point: 3. The design in Section 3.2 (i.e., the involvement of dual form) and the design of new noise distribution in Section 4 seem to be two totally different elements to improve the robustness. It is unclear to me which element play a more important role. In other words, the ablation study about the two elements is missing.
==================================================

Focused review:

Some claims are not convincing to me. For example, the authors claim in Section 1 that "ARMA networks are complementary to the aforementioned architectures including encoder-decoder structured networks, dilated convolutional networks and non-local attention networks." I agree with the "encoder-decoder structured networks" part but am less convinced that ARMA networks are complementary to dilated convolutional networks and non-local attention networks. In the experimental results in Tables 2&3, using the ARMA layer together with dilated convolutions or the attention mechanism does not bring any improvement. Such results do not suggest the "complementary" relationship. Another example of unconvincing claims lies in Section 7, where the authors claim that the proposed ARMA layer is related to several methods. However, no explanation on how they are related is given. In addition, citations of the mentioned methods are not provided. Besides the above problems, the major weakness of this work is in its experiments, as elaborated below. - Semantic Segmentation on Medical Images: The authors chose the attention u-net [24] as a baseline that uses the attention mechanism. However, [24] actually uses the gate mechanism instead of the attention mechanism in [30,31]. An obvious difference is that the gate mechanism 'filters' each input location by multiplying with a scalar number, while the attention mechanism computes a weighted sum of input locations. In addition, the purpose of using the gate mechanism in [24] is irrelevant to increasing the ERF. In order to compare the ARMA layer with the attention mechanism in the u-net framework, a much more suitable baseline is the AAAI'20 paper "Non-local U-Net for Biomedical Image Segmentation", where non-local attention blocks are used to replace convolutions/deconvolutions in u-net in order to achieve global RF. In addition, since the results in Table 2 are average of 10 runs, std should be reported to make the comparison fair and convincing. - Pixel-level Video Prediction: My major concerns lie in the results in Table 4. Inserting the non-local attention block to Conv-LSTM does not serve as a fair baseline. The functionality of the non-local attention block and the LSTM is overlapping. In [31] and many other studies using the self-attention mechanism, the LSTM is not used at all. Inserting the non-local attention block to Conv-LSTM will not show the true effectiveness of the non-local attention block. The backbone model of this task should be changed to more popular and powerful models, e.g. the backbone model used in [31]. To conclude, 'weak' or inappropriate baselines are used in the experiments, making the results less convincing.

Review Point: - Semantic Segmentation on Medical Images: The authors chose the attention u-net [24] as a baseline that uses the attention mechanism. However, [24] actually uses the gate mechanism instead of the attention mechanism in [30,31]. An obvious difference is that the gate mechanism 'filters' each input location by multiplying with a scalar number, while the attention mechanism computes a weighted sum of input locations. In addition, the purpose of using the gate mechanism in [24] is irrelevant to increasing the ERF. In order to compare the ARMA layer with the attention mechanism in the u-net framework, a much more suitable baseline is the AAAI'20 paper "Non-local U-Net for Biomedical Image Segmentation", where non-local attention blocks are used to replace convolutions/deconvolutions in u-net in order to achieve global RF. In addition, since the results in Table 2 are average of 10 runs, std should be reported to make the comparison fair and convincing.
Review Point: - Pixel-level Video Prediction: My major concerns lie in the results in Table 4. Inserting the non-local attention block to Conv-LSTM does not serve as a fair baseline. The functionality of the non-local attention block and the LSTM is overlapping. In [31] and many other studies using the self-attention mechanism, the LSTM is not used at all. Inserting the non-local attention block to Conv-LSTM will not show the true effectiveness of the non-local attention block. The backbone model of this task should be changed to more popular and powerful models, e.g. the backbone model used in [31]. To conclude, 'weak' or inappropriate baselines are used in the experiments, making the results less convincing.
==================================================

Focused review:

- How useful were the inter-path transitions? An analysis on would help reason about how useful is this fusion. - In Table 5, how does the I3D + FFC compare with I3D + NL? - Analysis on how cross-scale fusion is helping the approach is necessary - The core component and methodology do not seem to be significantly different from [Chi et al.2019]. The approach seems to improve numbers through adding local convolutions and cross-scale fusion to the previous approach.

Review Point: - How useful were the inter-path transitions? An analysis on would help reason about how useful is this fusion.
Review Point: - In Table 5, how does the I3D + FFC compare with I3D + NL?
Review Point: - Analysis on how cross-scale fusion is helping the approach is necessary - The core component and methodology do not seem to be significantly different from [Chi et al.2019]. The approach seems to improve numbers through adding local convolutions and cross-scale fusion to the previous approach.
==================================================

Focused review:

Weaknesses: As a reader of a ACL paper, I usually ask myself what important insight can I take away from the paper, and from a big picture point of view, what does the paper add to the fields of natural language processing and computational linguistics. How does the task of lexical substitutability in general and this paper in particular help either in improving an NLP system or provide insight about language? I can't find a good answer answer to either question after reading this paper.
As a practitioner who wants to improve natural language understanding system, I am more focused on the first question -- does the lexical substitutability task and the improved results compared to prior work presented here help any end application? Given the current state of high performing systems, any discrete clustering of words (or longer utterances) often break down when compared to continuous representations words (see all the papers that utilitize discrete lexical semantics to achieve a task versus words' distributed representations used as an input to the same task; e.g. machine translation, question answering, sentiment analysis, text classification and so forth). How do the authors motivate work on lexical substitutability given that discrete lexical semantic representations often don't work well? The introduction cites a few papers from several years back that are mostly set up in small data scenarios, and given that this word is based on English, I don't see why one would use this method for any task. I would be eager to see the authors' responses to this general question of mine.
As a minor point, to further motivate this, consider the substitutes presented in Table 1. 
1. Tasha snatched it from him to rip away the paper. 
2. Tasha snatched it from him to rip away the sheet.
To me, these two sentences have varying meanings -- what if he was holding on to a paper bag? In that scenario, can the word "paper" be substituted by "sheet"? At least, in my understanding, it cannot. Hence, there is so much subjectivity in this task that lexical substitutes can completely alter the semantics of the original sentence.
Minor point(s): - Citations in Section 3.1.4 are missing.
Addition: I have read the author response and I am sticking to my earlier evaluation of the paper. 

Review Point: 1. Tasha snatched it from him to rip away the paper.
Review Point: 2. Tasha snatched it from him to rip away the sheet. To me, these two sentences have varying meanings -- what if he was holding on to a paper bag? In that scenario, can the word "paper" be substituted by "sheet"? At least, in my understanding, it cannot. Hence, there is so much subjectivity in this task that lexical substitutes can completely alter the semantics of the original sentence. Minor point(s):
Review Point: - Citations in Section 3.1.4 are missing. Addition: I have read the author response and I am sticking to my earlier evaluation of the paper.
==================================================

Focused review:

Weakness:
The claim in Line 175~176 is not validated which it is valuable to see whether the proposed method could prevents potential classes from being incorrectly classified into historical classed.
In Tab. 1, for VOC 2-2 (10 tasks) and VOC 19-1 (2 tasks) MicroSeg gets inferior performance compared with SSUL, the reason should be explained. It also appear in ADE 100-50 (2 tasks) in Tab. 2.
The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods.
The authors adequately addressed the limitations and potential negative societal impact of their work.

Review Point: 2. The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods. The authors adequately addressed the limitations and potential negative societal impact of their work.
==================================================

Focused review:

Weaknesses:
My biggest concern is on the underlying assumption of this paper---the model should not be impacted by the incorrect labels. It's controversial here because when we say in-context "learning", we actually want the model to be truthful to the provided demonstration examples. I am willing to hear the author's thoughts on this.
Despite that this work might be the first to show that the copy of incorrect labels emerges in the later layers, this finding is not that surprising considering the early layers are mostly responsible for fusing information, and the later layers are mainly for generating the final output (see the references).
Regarding the proposed method, although removing the particular components can mitigate the negative impact of incorrect labels, it's also questionable how this will affect the generation capability. It would be helpful if the author can show how much this will change the model's performance when the labels are correct. References:
* Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?
* Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Taeuk Kim. Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations
* Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith. Linguistic Knowledge and Transferability of Contextual Representations
* Jesse Vig, Yonatan Belinkov. Analyzing the Structure of Attention in a Transformer Language Model

Review Point: * Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?
Review Point: * Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Taeuk Kim. Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations * Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith. Linguistic Knowledge and Transferability of Contextual Representations * Jesse Vig, Yonatan Belinkov. Analyzing the Structure of Attention in a Transformer Language Model
==================================================

Focused review:

Weakness: 1. The contribution of multilingual chain-of-thought is incremental compared to the villa chain-of-though. 2. It is interesting to see the large gap between Translate-EN, EN-CoT and Native-CoT in MGSM. While the gaps in other benchmarks are not too much. Is it because MGSM benchmark is originated from translation?

Review Point: 1. The contribution of multilingual chain-of-thought is incremental compared to the villa chain-of-though.
Review Point: 2. It is interesting to see the large gap between Translate-EN, EN-CoT and Native-CoT in MGSM. While the gaps in other benchmarks are not too much. Is it because MGSM benchmark is originated from translation?
==================================================

Focused review:

Weaknesses] *assumption* - I am not sure if it is safe to assume any programmatic policy can be parameterized by a vector \theta and is differentiable in \theta. (for Theorem 4.2) *initial policy* - In all the experiments (TORCS, MountainCar, and Pendulum), the IPPG polices improve upon the PRIOR. It is not clear if IPPG can learn from scratch. Showing the performance of IPPG learning from scratch would be important to verify this. - Can IPPG be initialized with a neural policy? It seems that it is possible based on Algorithm 1. If so, it would be interesting to see how well IPPG work using a neural policy learned with DDPG instead of PRIOR. Can IIPG improve upon DDPG? *experiment setup* - It is mentioned that "both NDPS and VIPER rely on imitating a fixed neural policy oracle" (L244). What is this policy oracle? Is this the policy learned using DDPG shown in the tables? If not, what's the performance of using NDPS and VIPER to distill the DDPG policies? - It would be interesting to see if the proposed framework works with different policy gradient approaches. *experiment results* - How many random seeds are used for learning the policies (DDPO and IPPG)? - What are the standard deviation or confidence intervals for all performance values? Are all the tracks deterministic? Are the DDPG policies deterministic during testing? - It would be better if the authors provided some videos showing different policies controlling cars on different tracks so that we can have a better idea of how different methods work. *reproducibility* - Some implementation details are lacking from the main paper, which makes reproducing the results difficult. It is not clear to me what policy gradient approach is used. - The provided dropbox link leads to an empty folder (I checked it on July 5th). *related work* - I believe it would be better if some prior works [1-5] exploring learning-based program synthesis frameworks were mentioned in the paper. *reference* [1] "Neuro-symbolic program synthesis" in ICLR 2017 [2] "Robustfill: Neural program learning under noisy I/O" in ICML 2017 [3] "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis" in ICLR 2018 [4] "Neural program synthesis from diverse demonstration videos" in ICML 2018 [5] "Execution-Guided Neural Program Synthesis" in ICLR 2019 ----- final review ----- After reading the other reviews and the author response, I have mixed feelings about this paper. On one hand, I do recognize the importance of this problem and appreciate the proposed framework (IPPG). On the other hand, many of my concerns (e.g. the choices of initial policy, experiment setup, and experiment results) are not addressed, which makes me worried about the empirical performance of the proposed framework.  To be more specific, I believe the following questions are important for understanding the performance of IPPG, which remain unanswered: (1) Can IPPG learn from scratch (i.e. where no neural policy could solve the task that we are interested in)? The authors stated that "IPPG can be initialized with a neural policy, learned for example via DDPG, and thus can be made to learn" in the rebuttal, which does not answer my question, but it is probably because my original question was confusing. (2) Can IPPG be initialized with a neural policy? If so, can IPPG be initialized with a policy learned using DDPG and improve it? As DDPG achieves great performance on different tracks, I am just interested in if IPPG can even improve it. (3) How many random seeds are used for learning the policies (DDPO and IPPG)? What are the standard deviation or confidence intervals for all performance values? I believe this is important for understanding the performance of RL algorithms. (4) What is the oracle policy that NDPS and VIPER learn from? If they do not learn from the DDPG policy, what is the performance if they distill the DDPG policy. (5) Can IPPG learn from a TPRO/PPO policy? While the authors mentioned that TRPO and PPO can't solve TORCS tasks, I believe this can be verified using the CartPole or other simpler environment. In sum, I decided to keep my score as 5. I am ok if this paper gets accepted (which is likely to happen given positive reviews from other reviewers) but I do hope this paper gets improved from the above points. Also, it would be good to discuss learning-based program synthesis frameworks as they are highly-related.

Review Point: *assumption* - I am not sure if it is safe to assume any programmatic policy can be parameterized by a vector \theta and is differentiable in \theta. (for Theorem 4.2) *initial policy* - In all the experiments (TORCS, MountainCar, and Pendulum), the IPPG polices improve upon the PRIOR. It is not clear if IPPG can learn from scratch. Showing the performance of IPPG learning from scratch would be important to verify this.
Review Point: - Can IPPG be initialized with a neural policy? It seems that it is possible based on Algorithm 1. If so, it would be interesting to see how well IPPG work using a neural policy learned with DDPG instead of PRIOR. Can IIPG improve upon DDPG? *experiment setup* - It is mentioned that "both NDPS and VIPER rely on imitating a fixed neural policy oracle" (L244). What is this policy oracle? Is this the policy learned using DDPG shown in the tables? If not, what's the performance of using NDPS and VIPER to distill the DDPG policies?
Review Point: - It would be interesting to see if the proposed framework works with different policy gradient approaches. *experiment results* - How many random seeds are used for learning the policies (DDPO and IPPG)?
Review Point: - What are the standard deviation or confidence intervals for all performance values? Are all the tracks deterministic? Are the DDPG policies deterministic during testing?
Review Point: - It would be better if the authors provided some videos showing different policies controlling cars on different tracks so that we can have a better idea of how different methods work. *reproducibility* - Some implementation details are lacking from the main paper, which makes reproducing the results difficult. It is not clear to me what policy gradient approach is used.
Review Point: - The provided dropbox link leads to an empty folder (I checked it on July 5th). *related work* - I believe it would be better if some prior works [1-5] exploring learning-based program synthesis frameworks were mentioned in the paper. *reference* [1] "Neuro-symbolic program synthesis" in ICLR 2017 [2] "Robustfill: Neural program learning under noisy I/O" in ICML 2017 [3] "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis" in ICLR 2018 [4] "Neural program synthesis from diverse demonstration videos" in ICML 2018 [5] "Execution-Guided Neural Program Synthesis" in ICLR 2019 ----- final review ----- After reading the other reviews and the author response, I have mixed feelings about this paper. On one hand, I do recognize the importance of this problem and appreciate the proposed framework (IPPG). On the other hand, many of my concerns (e.g. the choices of initial policy, experiment setup, and experiment results) are not addressed, which makes me worried about the empirical performance of the proposed framework. To be more specific, I believe the following questions are important for understanding the performance of IPPG, which remain unanswered: (1) Can IPPG learn from scratch (i.e. where no neural policy could solve the task that we are interested in)? The authors stated that "IPPG can be initialized with a neural policy, learned for example via DDPG, and thus can be made to learn" in the rebuttal, which does not answer my question, but it is probably because my original question was confusing. (2) Can IPPG be initialized with a neural policy? If so, can IPPG be initialized with a policy learned using DDPG and improve it? As DDPG achieves great performance on different tracks, I am just interested in if IPPG can even improve it. (3) How many random seeds are used for learning the policies (DDPO and IPPG)? What are the standard deviation or confidence intervals for all performance values? I believe this is important for understanding the performance of RL algorithms. (4) What is the oracle policy that NDPS and VIPER learn from? If they do not learn from the DDPG policy, what is the performance if they distill the DDPG policy. (5) Can IPPG learn from a TPRO/PPO policy? While the authors mentioned that TRPO and PPO can't solve TORCS tasks, I believe this can be verified using the CartPole or other simpler environment. In sum, I decided to keep my score as 5. I am ok if this paper gets accepted (which is likely to happen given positive reviews from other reviewers) but I do hope this paper gets improved from the above points. Also, it would be good to discuss learning-based program synthesis frameworks as they are highly-related.
==================================================

Focused review:

Weaknesses: While the paper goes into mathematical technicalities about certain formal details, it does not explain clearly certain high-level, core, aspects of the approach, which make it difficult for the reader to really understand what is actually claimed. My first question below raises this point in more detail. Questions:
It is crucial for the reader to clearly understand how the "pretraining distribution" is actually constructed. Although I could not find it detailed in the paper (and actually found some statements that seemed to contradict each other, see below), my understanding is that, after a concept θ
is drawn, a sequence of examples of the concept is produced, separated by a special symbol. The total number of symbols produced is T
(is this correct?), a parameter which is not detailed. If this interpretation is correct, am I right to assume that it means that the pretraining distribution, for a given θ
, generates essentially the same sequences as the "prompt distribution" for that θ ?
Note: Another interpretation, slightly different from the one I just wrote, would also seem to be supported by the paper, namely that while the pretrained distribution is not exactly the same as the prompt distribution, it does ensure that sequences of examples are generated in almost the same way, but I think the nuance is subtle.
Assuming my interpretation is correct, the formal part of the paper essentially says that, (under some technical conditions), if we let the pretraining distribution generate a long sequence of examples (but without disclosing which concept θ ∗
was employed), and then rerun the (full) pretraining distribution conditionally from that prefix, then the next example generated will tend to be an example "associated" with θ ∗ .
If this is the main observation that the paper makes, it should be stated more openly, even if there is a danger that some readers may dismiss it as a bit "obvious" (which it is not really at the formal level).
Concerning statements that look to go in different directions relative to this point: the caption in Fig. 1 says "... and are thus 'out of distribution' ", but the example in Fig. 2 seems to concatenate several examples. The paragraph after assumption 3 on p. 4 says: "the prompt is still out of distribution", while the next "regularity" assumption seems to be there to avoid such a possibility...
Related to these points, a clearer description of the way the GINC dataset is produced, in terms of whether and how it produces sequences of examples, would be very welcome. Also, the definition of the GINC should be related to the formal assumptions in section 2.1, which it is not.
Other questions:
Please explain why you need a special treatment for the first symbol of each example in the description of the prompt distribution in the first half of p. 3. Is this related with the different color used for "Albert" and "Einstein" in Figure 1 ?
In your simulations, the fact that neural LMs such as LSTMs or Transformers are able to recover the completion of examples should be sufficiently explained by their ability to well approximate the pretraining distribution underlying the GINC dataset, without having to invoke additional ICL abilities. However, your experiments indicate that their ability to do completions goes beyond their ability to approximate the underlying distribution. Do you think your theory could be extended to explain that phenomenon, or is it orthogonal to it?
Comment on the title of the paper: as a last comment, I feel that the title, claiming to provide an Explanation of iCL in terms of Bayesian learning, exaggerates the current contribution. A true explanation would require to go into how the hidden states of a pretrained Transformer model focus more and more onto a characterization of the task being illustrated by the sequence of examples in the prompt (and typically do so based on only a few examples) and are often able to solve that task even if these examples appear to be remotely related to the training distribution. This is not what the paper does, so I think the title could be reformulated in a more cautious way.

Review Point: 4 says: "the prompt is still out of distribution", while the next "regularity" assumption seems to be there to avoid such a possibility... Related to these points, a clearer description of the way the GINC dataset is produced, in terms of whether and how it produces sequences of examples, would be very welcome. Also, the definition of the GINC should be related to the formal assumptions in section 2.1, which it is not. Other questions: Please explain why you need a special treatment for the first symbol of each example in the description of the prompt distribution in the first half of p.
Review Point: 3. Is this related with the different color used for "Albert" and "Einstein" in Figure 1 ? In your simulations, the fact that neural LMs such as LSTMs or Transformers are able to recover the completion of examples should be sufficiently explained by their ability to well approximate the pretraining distribution underlying the GINC dataset, without having to invoke additional ICL abilities. However, your experiments indicate that their ability to do completions goes beyond their ability to approximate the underlying distribution. Do you think your theory could be extended to explain that phenomenon, or is it orthogonal to it? Comment on the title of the paper: as a last comment, I feel that the title, claiming to provide an Explanation of iCL in terms of Bayesian learning, exaggerates the current contribution. A true explanation would require to go into how the hidden states of a pretrained Transformer model focus more and more onto a characterization of the task being illustrated by the sequence of examples in the prompt (and typically do so based on only a few examples) and are often able to solve that task even if these examples appear to be remotely related to the training distribution. This is not what the paper does, so I think the title could be reformulated in a more cautious way.
==================================================

Focused review:

These are not weaknesses but rather questions. 1) Is there a general relation between the strict complementarity, F*, and the pyramidal width? I understand it in the case of the simplex, I wonder if something can be said in general. 2) It would be useful to discuss some practical applications (for example in sparse recovery) and the implication of the analysis to those. In general, I found the paper would be stronger if better positioned wrt particular practical applications. 3) I found the motivation in the introduction with the low-rank factorization unnecessary given that the main result is about polytopes. If the result has implications for low-rank matrix factorization I would like to see them explicitly discussed.

Review Point: 1) Is there a general relation between the strict complementarity, F*, and the pyramidal width? I understand it in the case of the simplex, I wonder if something can be said in general.
Review Point: 2) It would be useful to discuss some practical applications (for example in sparse recovery) and the implication of the analysis to those. In general, I found the paper would be stronger if better positioned wrt particular practical applications.
Review Point: 3) I found the motivation in the introduction with the low-rank factorization unnecessary given that the main result is about polytopes. If the result has implications for low-rank matrix factorization I would like to see them explicitly discussed.
==================================================

Focused review:

- The amount of novelty is quite limited because the main contribution of this paper is just to show that Hessian trace can be a better method compared to top Hessian eigenvalue that has been introduced in HAWQ[7]. - One example that showing why Hessian trace is good is only shown in the initial part of Section 2.1 (100x^2 +y^2 vs 100x^2+99y^2). But what is missing to verify the major claim is to show a few major Hessian eigenvalues in some example models. Suppose that top Hessian eigenvalue is highly dominating in general, then Hessian trace would not show distinguished advantages while computational complexity increases. It is necessary to show empirical results to support major claims. - HAQ and a few other previous works do not rely on top Hessian eigenvalue. Hence, this paper is mainly comparing with HAWQ[7] only. It is necessary to prove that Hessian trace is superior to other methods such as machine learning based ideas, e.g., HAQ[22]. - In Table 1 and 2, comparisons are not fair because many methods assume the fixed number of quantization bits while this paper depends on the relaxed assumption that each layer can have different number quantization bits. It would be necessary to compare experimental results on the same principles and hardware requirements. - It is difficult to understand what authors wanted to claim with Figure 4. Since perturbation is defined as a metric that authors have chosen to minimize, it is natural that AMQ results lie on Pareto frontier lines. What is missing is the relationship between total perturbation and model accuracy.

Review Point: - The amount of novelty is quite limited because the main contribution of this paper is just to show that Hessian trace can be a better method compared to top Hessian eigenvalue that has been introduced in HAWQ[7].
Review Point: - One example that showing why Hessian trace is good is only shown in the initial part of Section 2.1 (100x^2 +y^2 vs 100x^2+99y^2). But what is missing to verify the major claim is to show a few major Hessian eigenvalues in some example models. Suppose that top Hessian eigenvalue is highly dominating in general, then Hessian trace would not show distinguished advantages while computational complexity increases. It is necessary to show empirical results to support major claims.
Review Point: - HAQ and a few other previous works do not rely on top Hessian eigenvalue. Hence, this paper is mainly comparing with HAWQ[7] only. It is necessary to prove that Hessian trace is superior to other methods such as machine learning based ideas, e.g., HAQ[22].
Review Point: - In Table 1 and 2, comparisons are not fair because many methods assume the fixed number of quantization bits while this paper depends on the relaxed assumption that each layer can have different number quantization bits. It would be necessary to compare experimental results on the same principles and hardware requirements.
Review Point: - It is difficult to understand what authors wanted to claim with Figure 4. Since perturbation is defined as a metric that authors have chosen to minimize, it is natural that AMQ results lie on Pareto frontier lines. What is missing is the relationship between total perturbation and model accuracy.
==================================================

Focused review:

1) The experiment is somewhat inadequate. In the paper, the author only compares the proposed SIRI approach to the baseline from original Touchdown dataset paper [2]. In fact, spatial description resolution is a similar task as referring expression or instruction grounding. It is necessary for the author to further compare to approaches (such as Mattnet [18] or other new methods in 2019) in those tasks. For example, although Mattnet is not designed for spatial description resolution, but there is also semantic and position modules to handle spatial relation and object relationship reasoning, which can be served as a substitute of Part I & II of SIRI. 2) Although this is a new task, but the solution is compositional and of limited novelty. The proposed framework seems hard to generalize to other tasks besides spatial description resolution since part II and III plays a very important role in SIRI, and the two parts are heavily rely on spatial words. Moreover, the idea to explore spatial relation and reason among objects proposed in the paper is not new and quite prevalent in other vision-language tasks. 3) The ablation study in Table 3 only shows three combinations besides pure LingUnet -- I, I+II and I+II+III, it will be better if the author could provide the combinations of II + III, III only and II only. Then we can better evaluate the importance of Part II and Part III. 4) About the generalization ability of Stage II & III: The global position embedding (Part III) seems to play a quite important role in SIRI. Are this part only limit to Spatial Description Resolution tasks or panoramic images? And if this part can help the navigation task of Touchdown dataset or other referring expressions tasks? For local spatial-word aware gating (Part II), can this part function well in other tasks? 5) The paper has some typos, such as: [Line 92]: adapt should be adopt; [Line 121]: averaged should be summed; [In figure 2]: VI (in fact 6) should be IV (4). If the author(s) could successfully address my above concerns, I will consider to improve my rating after rebuttal. ------------ After rebuttal ------------- After reading the author response and all other reviewers' comments, I think most of my concerns have been well addressed. I would like to improve my overall rating to 6.

Review Point: 1) The experiment is somewhat inadequate. In the paper, the author only compares the proposed SIRI approach to the baseline from original Touchdown dataset paper [2]. In fact, spatial description resolution is a similar task as referring expression or instruction grounding. It is necessary for the author to further compare to approaches (such as Mattnet [18] or other new methods in 2019) in those tasks. For example, although Mattnet is not designed for spatial description resolution, but there is also semantic and position modules to handle spatial relation and object relationship reasoning, which can be served as a substitute of Part I & II of SIRI.
Review Point: 2) Although this is a new task, but the solution is compositional and of limited novelty. The proposed framework seems hard to generalize to other tasks besides spatial description resolution since part II and III plays a very important role in SIRI, and the two parts are heavily rely on spatial words. Moreover, the idea to explore spatial relation and reason among objects proposed in the paper is not new and quite prevalent in other vision-language tasks.
Review Point: 3) The ablation study in Table 3 only shows three combinations besides pure LingUnet -- I, I+II and I+II+III, it will be better if the author could provide the combinations of II + III, III only and II only. Then we can better evaluate the importance of Part II and Part III.
Review Point: 4) About the generalization ability of Stage II & III: The global position embedding (Part III) seems to play a quite important role in SIRI. Are this part only limit to Spatial Description Resolution tasks or panoramic images? And if this part can help the navigation task of Touchdown dataset or other referring expressions tasks? For local spatial-word aware gating (Part II), can this part function well in other tasks?
==================================================

Focused review:

The paper is too long (important aspects have been moved to the appendix, the conclusion rephrased as the broader impact section put of 9th page; broader impact section itself is missing). The paper is not self-contained. Experimental design: the presentation of the experimental results is not clear in general. More points: - the authors don’t give a std deviation across different training runs. This would be interesting to judge whether the small improvements for some models wrt. performance or robustness are significant. - The adversary bounds chosen are not explained or varied. - The baseline adversarial methods Critic and Random sometimes improve agent performance? - The adversaries implemented are quite weak. (Bit of speculation on my part): As far as I understand it, the regularization essentially constrains the policy to be smooth over the observation. Are the proposed algorithms (and rather heavy optimization methods) the right way to achieve this objective? The authors propose to use expensive second-order optimization for their method. I suppose the proposed method produces a huge computational workload. The authors miss to give an analysis on this in the experimental section.

Review Point: - the authors don’t give a std deviation across different training runs. This would be interesting to judge whether the small improvements for some models wrt. performance or robustness are significant.
Review Point: - The adversary bounds chosen are not explained or varied.
Review Point: - The baseline adversarial methods Critic and Random sometimes improve agent performance?
Review Point: - The adversaries implemented are quite weak. (Bit of speculation on my part): As far as I understand it, the regularization essentially constrains the policy to be smooth over the observation. Are the proposed algorithms (and rather heavy optimization methods) the right way to achieve this objective? The authors propose to use expensive second-order optimization for their method. I suppose the proposed method produces a huge computational workload. The authors miss to give an analysis on this in the experimental section.
==================================================

Focused review:

1. The main contribution of the proposed approach lacks novelty. Since BM25 has strong zero-shot capability, it seems that the combination of the BM25 serves mainly to enhance the zero-shot capability of the model. The proposed ICoL is mainly an integration of some existing methods. 
2. The authors do not illustrate why it is better to keep representations in the same hidden space, it also has not been experimentally verified that they are in the same hidden space. 
3. The link between the ICoL and BM25 weighting is not fully worked out. 
1. In line 300-301 "{ d i,1−, . . . , d i,n−} is a set of randomly sampled query negatives", query->document? 

Review Point: 1. The main contribution of the proposed approach lacks novelty. Since BM25 has strong zero-shot capability, it seems that the combination of the BM25 serves mainly to enhance the zero-shot capability of the model. The proposed ICoL is mainly an integration of some existing methods.
Review Point: 2. The authors do not illustrate why it is better to keep representations in the same hidden space, it also has not been experimentally verified that they are in the same hidden space.
Review Point: 3. The link between the ICoL and BM25 weighting is not fully worked out.
Review Point: .. , d i,n−} is a set of randomly sampled query negatives", query->document?
==================================================

Focused review:

Weaknesses
+ The idea of using an offline dataset to calibrate the simulator is intuitive and interesting. This is an alternative way of using the offline datasets to help the training of decision-making agents.
- The proposed method is relatively heuristic and may have limitations on the novelty. As the authors mentioned, using the policy gradient method to optimize simulator parameters could have a large variance. The proposed solution (picking the best hyperparameter after multiple fittings) in this paper may not be applicable in high-dimensional cases due to the high computational cost.
- If I understand correctly, one assumption of the proposed method is that the model of the simulator is correct and only the simulator parameters are inaccurate. If so, this would limit the application of this method in more complex tasks, where we are unable to access or build a correct model. I suspect that the Offline RL method may outperform the proposed method.
- There are lots of components in multiple stages in the proposed method. However, the analyses of these components are not enough. For example, the authors use a behavior cloning (BC) method as a surrogate model to collect data from the simulator. Why do they select BC methods and why is this a good choice? What if the BC model is not well-trained and makes the learning of the simulator fail? Questions
(1) What if we don’t have an accurate model of the simulator? For example, there could be some friction parameters that are not considered in simple simulators. In this case, the performance of the proposed method will always have a gap. In some worst cases, the learned simulator is too simple to fit the offline data and the online agents overfit the simulator. Could the author provide more experiments on an inaccurate simulator model with fewer parameters than the testing environment?
(2) The joint training of the discriminator and parameter generator could be unstable. For example, a strong discriminator may make the generator fail. How do the authors deal with this problem?
(3) In equation (3), a hyperparameter ϵ
is introduced and the authors say that it can balance exploitation and safety. What does safety mean here? I assume this paper is not related to safety since this term is only mentioned here.
(4) What’s the influence of the seed i
in Section 4.2? How large is i
enough to reduce the high variance? Are there any theoretical or empirical analyses about it?

Review Point: + The idea of using an offline dataset to calibrate the simulator is intuitive and interesting. This is an alternative way of using the offline datasets to help the training of decision-making agents.
Review Point: - The proposed method is relatively heuristic and may have limitations on the novelty. As the authors mentioned, using the policy gradient method to optimize simulator parameters could have a large variance. The proposed solution (picking the best hyperparameter after multiple fittings) in this paper may not be applicable in high-dimensional cases due to the high computational cost.
Review Point: - If I understand correctly, one assumption of the proposed method is that the model of the simulator is correct and only the simulator parameters are inaccurate. If so, this would limit the application of this method in more complex tasks, where we are unable to access or build a correct model. I suspect that the Offline RL method may outperform the proposed method.
Review Point: - There are lots of components in multiple stages in the proposed method. However, the analyses of these components are not enough. For example, the authors use a behavior cloning (BC) method as a surrogate model to collect data from the simulator. Why do they select BC methods and why is this a good choice? What if the BC model is not well-trained and makes the learning of the simulator fail? Questions (1) What if we don’t have an accurate model of the simulator? For example, there could be some friction parameters that are not considered in simple simulators. In this case, the performance of the proposed method will always have a gap. In some worst cases, the learned simulator is too simple to fit the offline data and the online agents overfit the simulator. Could the author provide more experiments on an inaccurate simulator model with fewer parameters than the testing environment? (2) The joint training of the discriminator and parameter generator could be unstable. For example, a strong discriminator may make the generator fail. How do the authors deal with this problem? (3) In equation (3), a hyperparameter ϵ is introduced and the authors say that it can balance exploitation and safety. What does safety mean here? I assume this paper is not related to safety since this term is only mentioned here. (4) What’s the influence of the seed i in Section 4.2? How large is i enough to reduce the high variance? Are there any theoretical or empirical analyses about it?
==================================================

Focused review:

Weakness
The paper didn't explain how to determine the coarse classes. It seems that the datasets used in the paper all have defined hierachical labels, which could be directly used for the proposed method. What if there are no hierarchical labels? Do we have to manually label it? Also, what is the influence of the coarse labels on the final performance. For example, if the dataset can only be divided into very few coarse labels, how much improvement the method could get?
The paper didn't explain which layer to add the prompt and what is the effect of different layers.
The author mentioned multiple layers of hierarchy, it is better to explain this in the method and Fig. 1.
In Fig. 4, the no prompt variant can also improve over the baseline. (1) The author should explain more details about the implementation of this variant. (2) I think this variant should be used as the baseline which the proposed method should compare with in all the experiments. The naive baseline don't use hierarchical labels thus it is not fair.
In Table 1, some results seem not very strong. E.g., with ImageNet pre-training, the baseline ViT can only achieve 84.98% top-1 accuracy. The public available results of ViT-B with Imagenet pretraining on CIFAR-100 is around 94%. I understand the author used a smaller ViT here. Thus I suggest the author to use ViT-B as the baseline which makes the results stronger and more convincing.
In Fig.5, it is better to provide the coarse and fine label of each image.
The method seems to be limited to ViT backbone and image classification.

Review Point: 1. In Fig. 4, the no prompt variant can also improve over the baseline. (1) The author should explain more details about the implementation of this variant. (2) I think this variant should be used as the baseline which the proposed method should compare with in all the experiments. The naive baseline don't use hierarchical labels thus it is not fair. In Table 1, some results seem not very strong. E.g., with ImageNet pre-training, the baseline ViT can only achieve 84.98% top-1 accuracy. The public available results of ViT-B with Imagenet pretraining on CIFAR-100 is around 94%. I understand the author used a smaller ViT here. Thus I suggest the author to use ViT-B as the baseline which makes the results stronger and more convincing. In Fig.5, it is better to provide the coarse and fine label of each image. The method seems to be limited to ViT backbone and image classification.
==================================================

Focused review:

- the writing of the paper can be improved in some sections (Introduction/Related work) to accommodate readers that are not experts on the task - missing details on corpus creation (instructions, agreement) 
- the paragraph from l90 to l116 is very unclear. It is explained in later paragraphs, but on first reading it is somewhat confusing - l121 - ``consists'' -> ``consists of'' or ``contains'' - can you include more details on the selection criteria for candidate examples from reddit (section 3.1). E.g.: what are the topics, what are the stereotype targets, what kind of filtering do you apply?
- can you include agreement statistics for the corpus in section 3.1 or 3.2?
- while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for "selecting relevant examples", in order to justify the use of RL in that setup over a simpler selection method 

Review Point: - can you include agreement statistics for the corpus in section 3.1 or 3.2?
Review Point: - while the contribution of the reinforcement agent is clear, it would be good to include a different baseline for "selecting relevant examples", in order to justify the use of RL in that setup over a simpler selection method
==================================================

Focused review:

I feel the design of NVSB and some experimental results need more explanation (more information in the section below). 
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input? 
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI. 

Review Point: 1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
Review Point: 2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.
==================================================

Focused review:

- Even though the efficiency of the search algorithm is improved. The search cost still takes tens of hours. It seems that the search algorithm is conducted independently on each data set. However, the data statistics indicate some similarities among them, e.g., DBP-WD, DBP-YG. Considering the similarity should be helpful for this problem. - In the setup, the paths are sampled from the graph and then learned by the searched models. It is not sure whether the search problem will be influenced when the paths change.

Review Point: - Even though the efficiency of the search algorithm is improved. The search cost still takes tens of hours. It seems that the search algorithm is conducted independently on each data set. However, the data statistics indicate some similarities among them, e.g., DBP-WD, DBP-YG. Considering the similarity should be helpful for this problem.
Review Point: - In the setup, the paths are sampled from the graph and then learned by the searched models. It is not sure whether the search problem will be influenced when the paths change.
==================================================

Focused review:

Weakness: 1) It is difficult to understand for the readers without the neural architecture search background. It's technical contribution is limited. 2) The hyper-parameter sensitivity analysis is missing in the experiments. 3) The dataset is relatively small, which consists of 853 videos. A larger dataset will be more meaningful.

Review Point: 1) It is difficult to understand for the readers without the neural architecture search background. It's technical contribution is limited.
Review Point: 2) The hyper-parameter sensitivity analysis is missing in the experiments.
Review Point: 3) The dataset is relatively small, which consists of 853 videos. A larger dataset will be more meaningful.
==================================================

Focused review:

Weakness:  1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?

Review Point: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim.
Review Point: 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc.
Review Point: 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods.
Review Point: 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work.
Review Point: 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?
==================================================

Focused review:

The paper does a fair and thorough job of describing some of its main limitations (see Section 6), which includes that 1) doing Bayesian inference in large-scale models (which is required for their results to hold) remains challenging in practice (which I personally don't really consider to be a shortcoming of their paper), 2) their theoretical results only hold for a limiting case that cannot be attained in practice, and 3) they only consider certain types of adversarial attacks, namely those relying on gradient information. Another shortcoming is that the empirical evaluation is somewhat small-scale, only considering shallow fully-connected neural networks and simple benchmark datasets (i.e. MNIST and Fashion MNIST). While the experimental results are fairly conclusive and convincing in those settings, it is not clear if those results extend to larger-scale, practically relevant scenarios, i.e. settings in which we care most about adversarial robustness. It would significantly strenghten the paper if it included empirical results on networks that are deeper, have more complex structure (e.g. convolutional layers), and that are evaluated on more challenging benchmark tasks (e.g. CIFAR10/0, or even ImageNet). While I understand that Bayesian inference remains challenging for such networks, recent advances in Bayesian deep learning, e.g. (Ritter 2018, Maddox 2019, Osawa 2019, Zhang 2020), have succeeded (to some degree) in scaling different types of inference to larger-scale settings. Also, the experiments only consider image classification tasks; while I understand that those are the most commonly-used benchmarks for general-purpose machine learning methods, it would nevertheless be insightful to see results for other application domains in which adversarial robustness is important. Furthermore, in the experiments described in Section 5.2, it would be insightful to compare to an ordinary (i.e. non-Bayesian / point-estimated / deterministic) neural network as a baseline. In particular, it would be good to know how the robustness of a BNN compares to that of a non-Bayesian NN on the considered adversarial attacks. At present, this experiment does not support the conclusion that BNNs are more robust to gradient-based adversarial attacks than deterministic NNs (which is not a claim that the authors seem to make explicitly, although it would be very intersting to know whether this is the case). Questions: 1) in Section 5, you first mention that "it is not obvious (and likely not true) that the posterior distribution is the sole ensemble with the zero averaging property of the gradients", before saying that "while the Bayesian posterior ensemble may not be the only randomization to provide protection, it is clear that some simpler randomizations such as bootstrap will be ineffective"; how do deep ensembles (Lakshminarayanan, 2017) fit into this picture, which also provide an approximation to the Bayesian model average in Equation (1) (Wilson and Izmailov, 2020), and which can be fit using the bootstrap (Lakshminarayanan, 2017), but are in practice more commonly fit by solely relying on the randomness within the weight initialization and mini-batching? given the effectiveness and thus popularity of deep ensembles for Bayesian model averaging, it would be insightful to have a brief discussion on how your results might or might not translate to them; furthermore, I think deep ensembles would be an interesting additional baseline in your empirical evaluation, which you should consider adding 2) in Section 5, you mention that "unless the priors are too informative, we do not expect a major deviation from the idealised case"; do you think of the commonly-used standard-normal prior N(0,1) as an informative or uninformative prior? which conditions exactly does the prior need to fulfil in order for your result to hold? what is the main challenge in extending your result to other types of priors? 3) do you have any intuition for why in deterministic NNs, accuracy and robustness appears to be negatively correlated, why in BNNs, it seems to be positively correlated (as you demonstrate in Section 5.3, and as previous work has also partially shown)? References: Lakshminarayanan et al., "Simple and scalable predictive uncertainty estimation using deep ensembles", NeurIPS 2017. Ritter et al., "A scalable laplace approximation for neural networks", ICLR 2018. Maddox et al., "A simple baseline for bayesian uncertainty in deep learning", NeurIPS 2019. Osawa et al., "Practical deep learning with bayesian principles", NeurIPS 2019. Zhang et al., "Cyclical stochastic gradient MCMC for Bayesian deep learning", ICLR 2020. Wilson and Izmailov, "Bayesian deep learning and a probabilistic perspective of generalization", arXiv 2020.

Review Point: 1) in Section 5, you first mention that "it is not obvious (and likely not true) that the posterior distribution is the sole ensemble with the zero averaging property of the gradients", before saying that "while the Bayesian posterior ensemble may not be the only randomization to provide protection, it is clear that some simpler randomizations such as bootstrap will be ineffective"; how do deep ensembles (Lakshminarayanan, 2017) fit into this picture, which also provide an approximation to the Bayesian model average in Equation (1) (Wilson and Izmailov, 2020), and which can be fit using the bootstrap (Lakshminarayanan, 2017), but are in practice more commonly fit by solely relying on the randomness within the weight initialization and mini-batching? given the effectiveness and thus popularity of deep ensembles for Bayesian model averaging, it would be insightful to have a brief discussion on how your results might or might not translate to them; furthermore, I think deep ensembles would be an interesting additional baseline in your empirical evaluation, which you should consider adding 2) in Section 5, you mention that "unless the priors are too informative, we do not expect a major deviation from the idealised case"; do you think of the commonly-used standard-normal prior N(0,1) as an informative or uninformative prior? which conditions exactly does the prior need to fulfil in order for your result to hold? what is the main challenge in extending your result to other types of priors?
Review Point: 3) do you have any intuition for why in deterministic NNs, accuracy and robustness appears to be negatively correlated, why in BNNs, it seems to be positively correlated (as you demonstrate in Section 5.3, and as previous work has also partially shown)? References: Lakshminarayanan et al., "Simple and scalable predictive uncertainty estimation using deep ensembles", NeurIPS 2017. Ritter et al., "A scalable laplace approximation for neural networks", ICLR 2018. Maddox et al., "A simple baseline for bayesian uncertainty in deep learning", NeurIPS 2019. Osawa et al., "Practical deep learning with bayesian principles", NeurIPS 2019. Zhang et al., "Cyclical stochastic gradient MCMC for Bayesian deep learning", ICLR 2020. Wilson and Izmailov, "Bayesian deep learning and a probabilistic perspective of generalization", arXiv 2020.
==================================================

Focused review:

Weaknesses: -- Although I value the authors' observation regarding why [22] and [23] could potentially be effective for the trajectory prediction task, I feel that the novelty of this paper is limited in a sense that the authors mainly adopt and combine two existing frameworks for their task. -- The claim "While the BiGAN architecture does not help performance much when K = 20, we show in Table 2 that it does help improve generalization at lower settings of K" in lines 243-244 was not supported by Table 2. Table 2 only compares Social-BiGAT against two other baselines. In order to validate the above statement, Table 2 should also include the results from GAT and BiGAN as Table 1 did. -- I have a few questions for clarification  1) In lines 154-156, why is it the case that max or average pooling leads to each pedestrian receiving "an identical joint feature representation that discards some uniqueness". Wouldn't each pedestrian still receive a unique features that encode interactions between the selected pedestrian and other pedestrians?   2) Was Sophie the state-of-the-art approach at the time of submission? I am asking this to make sure that the proposed method was compared against strong baselines. Overall comments: I think that the technical contributions of this paper are on the NeurIPS borderline due to the fact that the authors mainly adopted the existing frameworks for a new task. However, I believe that applying these to a new problem is not trivial. The authors well justified their choice of [22] and [23] for the trajectory prediction task in the paper. Also, the authors provided enough experimental results to validate their claims and show the effectiveness of the proposed method. Thus, I believe that the paper is in good shape and includes enough new findings that an ML audience like NeurIPS would appreciate. Final comment: The authors addressed the major concerns raised by the reviewers successfully. Thus, I will keep my original score (7). 

Review Point: -- Although I value the authors' observation regarding why [22] and [23] could potentially be effective for the trajectory prediction task, I feel that the novelty of this paper is limited in a sense that the authors mainly adopt and combine two existing frameworks for their task. -- The claim "While the BiGAN architecture does not help performance much when K = 20, we show in Table 2 that it does help improve generalization at lower settings of K" in lines 243-244 was not supported by Table 2. Table 2 only compares Social-BiGAT against two other baselines. In order to validate the above statement, Table 2 should also include the results from GAT and BiGAN as Table 1 did. -- I have a few questions for clarification 1) In lines 154-156, why is it the case that max or average pooling leads to each pedestrian receiving "an identical joint feature representation that discards some uniqueness". Wouldn't each pedestrian still receive a unique features that encode interactions between the selected pedestrian and other pedestrians?
Review Point: 2) Was Sophie the state-of-the-art approach at the time of submission? I am asking this to make sure that the proposed method was compared against strong baselines. Overall comments: I think that the technical contributions of this paper are on the NeurIPS borderline due to the fact that the authors mainly adopted the existing frameworks for a new task. However, I believe that applying these to a new problem is not trivial. The authors well justified their choice of [22] and [23] for the trajectory prediction task in the paper. Also, the authors provided enough experimental results to validate their claims and show the effectiveness of the proposed method. Thus, I believe that the paper is in good shape and includes enough new findings that an ML audience like NeurIPS would appreciate. Final comment: The authors addressed the major concerns raised by the reviewers successfully. Thus, I will keep my original score (7).
==================================================

Focused review:

1. Some detaills are missing as I mention below. I undertand this situation becuase of the lots of details for the experiments. Hope it will be more clear in the next version. 2. I am not sure whether the method are called search-based. It seems like it is simulated annealing for approximating the global optimum based on the scores in Equation (1). It is still a sampling-based method. 3. In this framework, the final outputs are from fine-tuned GPT2, not the SA outputs based on the scorer. How is the performance on SA output? In your work, the fine-tuned GPT2 could be treated as the inference network during the decoding. It looks similar to the dirction of these work [1] [2][3]. 4. I think the reader could see the effect of scores. How is the learned scorers? Maybe an ablatioin study could be done? Or compasion bettwen several outputs? [1]. Lifu Tu, Kevin Gimpel. 2018. Learning Approximate Inference Networks for Structured Prediction [2]. LIfu Tu, Kevin Gimpel. 2019. Benchmarking Approximate Inference Methods for Neural Structured Prediction. [3]. Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel. 2020. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation

Review Point: 1. Some detaills are missing as I mention below. I undertand this situation becuase of the lots of details for the experiments. Hope it will be more clear in the next version.
Review Point: 2. I am not sure whether the method are called search-based. It seems like it is simulated annealing for approximating the global optimum based on the scores in Equation (1). It is still a sampling-based method.
Review Point: 3. In this framework, the final outputs are from fine-tuned GPT2, not the SA outputs based on the scorer. How is the performance on SA output? In your work, the fine-tuned GPT2 could be treated as the inference network during the decoding. It looks similar to the dirction of these work [1] [2][3].
Review Point: 4. I think the reader could see the effect of scores. How is the learned scorers? Maybe an ablatioin study could be done? Or compasion bettwen several outputs? [1]. Lifu Tu, Kevin Gimpel. 2018. Learning Approximate Inference Networks for Structured Prediction [2]. LIfu Tu, Kevin Gimpel. 2019. Benchmarking Approximate Inference Methods for Neural Structured Prediction. [3]. Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel. 2020. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
==================================================

Focused review:

In my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.
I had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results  directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these  subsets. 
Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. 

Review Point: - It would be nice in table 5 could present the human results directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these subsets. Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task.
==================================================

Focused review:

- I am not convinced by the motivating examples given in the paper. It is still unclear to me how those problems (e.g., sensor replacement & movie recommendation) can be modeled as robust sequence submodular optimization problems and why we should model it in that way. I am concerning the practical value of this paper. - The proposed methods have not been validated by experiments. In fact, I am curious if there are some benchmarks that can be modeled as robust sequence submodular problems. - Some of the bounds are not very practical. E.g. the bound in Theorem 3 is almost useless for large \tau.

Review Point: - I am not convinced by the motivating examples given in the paper. It is still unclear to me how those problems (e.g., sensor replacement & movie recommendation) can be modeled as robust sequence submodular optimization problems and why we should model it in that way. I am concerning the practical value of this paper.
Review Point: - The proposed methods have not been validated by experiments. In fact, I am curious if there are some benchmarks that can be modeled as robust sequence submodular problems.
Review Point: - Some of the bounds are not very practical. E.g. the bound in Theorem 3 is almost useless for large \tau.
==================================================

Focused review:

Weaknesses
The empirical investigation relies on only one environment (Hopper).
Further suggestions for improvement
The statement "Our algorithm achieves better performance than MOPO in all datasets" should be softened, e.g. "Our algorithm achieves equal or better performance than MOPO in all datasets", because in medium_replay the performance is not significantly better.
In Table 2 and Table 3 standard deviations are given after ±
. Correctly, the ±
sign is used to indicate the uncertainty of the measurement. So a confidence interval or a standard error. This serves to ensure that the statistical significance of differences in the measured values can be easily grasped.
Uncertainties should be stated with one or at most two valid digits.
The number of decimal places in A ±
B must match. E.g. "31.34 ± 0.5" -> "31.3 ± 0.5".
Using only three repetitions (seeds) leads to unreliable results. If it is somehow possible, there should be more, e.g. 10 or 50.
In "Other methods include behavior regularized policy optimization", also (Fujimoto et al., 2019) should be cited.
Similar to the present paper, (Depeweg et al., Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning 2018) estimates uncertainty without ensemble and uses the uncertainty for conservatives in offline RL, it should, therefore, be cited. However, long roll-outs are used for the return estimation and no Q-function is used, so that the approach is structurally clearly different from MOPO.
It is claimed "Conservatism in MBRL is achieved by uncertainty-based penalization of the model predictions." however, this is only one possible way, another possibility is also in model-based the behavior regularized policy optimization, e.g. (Swazinna et al, Overcoming model bias for robust offline deep reinforcement learning, 2021). Presumably other techniques exist, so it is probably better to write, e.g., "Conservatism in MBRL is frequently achieved by uncertainty-based penalization of the model predictions."
It would be interesting to study the behavior in a stochastic environment, because only in stochastic environments the problem exists to separate aleatory and epistemic uncertainty (future work).

Review Point: 10 or 50. In "Other methods include behavior regularized policy optimization", also (Fujimoto et al., 2019) should be cited. Similar to the present paper, (Depeweg et al., Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning 2018) estimates uncertainty without ensemble and uses the uncertainty for conservatives in offline RL, it should, therefore, be cited. However, long roll-outs are used for the return estimation and no Q-function is used, so that the approach is structurally clearly different from MOPO. It is claimed "Conservatism in MBRL is achieved by uncertainty-based penalization of the model predictions." however, this is only one possible way, another possibility is also in model-based the behavior regularized policy optimization, e.g. (Swazinna et al, Overcoming model bias for robust offline deep reinforcement learning, 2021). Presumably other techniques exist, so it is probably better to write, e.g., "Conservatism in MBRL is frequently achieved by uncertainty-based penalization of the model predictions." It would be interesting to study the behavior in a stochastic environment, because only in stochastic environments the problem exists to separate aleatory and epistemic uncertainty (future work).
==================================================

Focused review:

Weaknesses:
General points:
-Measuring privacy loss in terms of privacy loss per iteration is not meaningful. Privacy loss accumulates, so it is very unusual and not informative to measure the loss of each step; instead epsilon should capture the privacy loss of the full algorithm (T rounds) to quantify the risk of inference attacks on the model
-The significance of the VN condition is not adequately explained. How exactly is this condition relevant to BR?
-The convergence results are not very interpretable, seem to have incorrect units
-Writing needs a lot of work
Specifics: 0. Abstract
-"...rendered invalid when workers enforce DP" is not accurate for two reasons: you only show this for a particular DP protocol, namely the Gaussian mechanism; showing it fundamentally for any DP mechanism would be much more interesting. Also, what you show is in terms of the VN condition, which is not explicitly connected to BR. Introduction
-"trusted server" and "honest but curious server" are both used to describe the set up, which is confusing. Usually "trusted server" means honest and not curious (i.e. not a privacy threat).
-"Byzantine" should be informally defined or briefly described early on before it is used many times
-Dwork et al (2014) is a strange choice for citation for a sentence about private ML. There are many other more relevant works from 2014 to present. Also, "especially when considering neural networks" does not seem to add anything and I'm not sure I agree that neural nets are special in this sense.
-Abadi et al (2016) is the wrong/incomplete citation for DP SGD: Bassily et al (2014) and Song et al (2013) considered DP SGD earlier.
-Theorem (Informal): should state conditions on loss; need to define (or informally explain, at the very least) "approximated VN" before using it in a theorem; paragraph following Theorem is too much detail for not having defined VN
-"parameters have very little impact in most settings when considering DP or BR separately" is not clear
-Figure 1. Is the number of iterations fixed?
Problem Settign
-"common dataset" what does this mean? Are the m points divided/partitioned among n workers or all workers have access to D?
-"By far, the most widely used approach....is the differentially private version...": strong claim made without any evidence; also, there are many ways to provide DP besides Gaussian noise, so there is no single "the" DP version of SGD.
-"we are mainly interested in...per-step...privacy": Why?! This is not nearly as meaningful. If T goes to infinity then essentially the algorithm provides no privacy at all but your epsilon might still be small, which is very misleading.
-Def 2: explain intuition; provide an example of a BR GAR
Section 3:
-Algorithm 1: clarify the presentation. When you loop through "honest" and "Byzantine" workers, it seems as if the analyst/curator who is implementing the algorithm knows which workers are honest and not, which is clearly not the case
-Contextualize Theorem 1. the privacy properties of Gaussian mechanism and subsampling are well-known, so this theorem is not at all novel; this should be stated. Also the log term in the denominator can be tightened; can take s 2 ≈ C 2 l o g ( 1 / δ ) / ϵ 2 m 2
-What's the significance of VN condition? How does it relate to BR? -"when ϵ and δ
are non-zero...": strange sentence because s
increases as ϵ and δ decrease.
-Theorem 2: why doesn't κ
appear? Units appear to be wrong. Should provide comparison to Aliastarh, Allen-Zhu, Li (2018) and the references therein. Also should compare to DP optimization rates
-Corollary 1: misleading because ϵ
is not the actual privacy budget of full T-round algorithm, so first term should also scale with T. This remark applies to experiments too.

Review Point: 0. Abstract -"...rendered invalid when workers enforce DP" is not accurate for two reasons: you only show this for a particular DP protocol, namely the Gaussian mechanism; showing it fundamentally for any DP mechanism would be much more interesting. Also, what you show is in terms of the VN condition, which is not explicitly connected to BR. Introduction -"trusted server" and "honest but curious server" are both used to describe the set up, which is confusing. Usually "trusted server" means honest and not curious (i.e. not a privacy threat). -"Byzantine" should be informally defined or briefly described early on before it is used many times -Dwork et al (2014) is a strange choice for citation for a sentence about private ML. There are many other more relevant works from 2014 to present. Also, "especially when considering neural networks" does not seem to add anything and I'm not sure I agree that neural nets are special in this sense. -Abadi et al (2016) is the wrong/incomplete citation for DP SGD: Bassily et al (2014) and Song et al (2013) considered DP SGD earlier. -Theorem (Informal): should state conditions on loss; need to define (or informally explain, at the very least) "approximated VN" before using it in a theorem; paragraph following Theorem is too much detail for not having defined VN -"parameters have very little impact in most settings when considering DP or BR separately" is not clear -Figure 1. Is the number of iterations fixed? Problem Settign -"common dataset" what does this mean? Are the m points divided/partitioned among n workers or all workers have access to D? -"By far, the most widely used approach....is the differentially private version...": strong claim made without any evidence; also, there are many ways to provide DP besides Gaussian noise, so there is no single "the" DP version of SGD. -"we are mainly interested in...per-step...privacy": Why?! This is not nearly as meaningful. If T goes to infinity then essentially the algorithm provides no privacy at all but your epsilon might still be small, which is very misleading. -Def 2: explain intuition; provide an example of a BR GAR Section 3: -Algorithm 1: clarify the presentation. When you loop through "honest" and "Byzantine" workers, it seems as if the analyst/curator who is implementing the algorithm knows which workers are honest and not, which is clearly not the case -Contextualize Theorem 1. the privacy properties of Gaussian mechanism and subsampling are well-known, so this theorem is not at all novel; this should be stated. Also the log term in the denominator can be tightened; can take s 2 ≈ C 2 l o g ( 1 / δ ) / ϵ 2 m 2 -What's the significance of VN condition? How does it relate to BR? -"when ϵ and δ are non-zero...": strange sentence because s increases as ϵ and δ decrease. -Theorem 2: why doesn't κ appear? Units appear to be wrong. Should provide comparison to Aliastarh, Allen-Zhu, Li (2018) and the references therein. Also should compare to DP optimization rates -Corollary 1: misleading because ϵ is not the actual privacy budget of full T-round algorithm, so first term should also scale with T. This remark applies to experiments too.
==================================================

Focused review:

Weaknesses:
Similar to the previous work on Metamath, it is hard to understand what are the possible leaks between the pretraining "WebMath" corpus and the testing set. This seems quite negligent when training large models with almost a billion of parameters with a large capacity for memorization. The evaluation should really include the success rate without the WebMath pretraining.
To expand on this, many ITP corpora are published on the web and in public repos in many different forms, sometimes after various syntactic translations (to latex, etc.), which are however often easy to recover by neural architectures (https://doi.org/10.1007/978-3-319-96812-4_22). It is completely unclear to me how big would be the effect of GPT pretraining on the test dataset translated in various ways. See e.g. the work of Gauthier for relatively simple statistical methods that quite reliably transfer the proof knowledge between syntactically different corpora (https://doi.org/10.1016/j.jsc.2018.04.005 , https://doi.org/10.1007/978-3-662-48899-7_26 ).
It is unclear what resources go into the test evaluation that remotely uses gptf. This makes an honest comparison with non-remote methods using standard hardware difficult. This should be at least discussed.
I do not understand the argument that RL-based data synthesis is more expensive. The RL-style proof data synthesis done in TacticToe, http://arxiv.org/abs/1805.07563 or https://doi.org/10.4230/LIPIcs.ITP.2019.34 is most likely orders of magnitude cheaper than just the pretraining done on WebMath here.
While the contribution is solid and data augmentation methods are certainly useful in DL, the introductory claims about data scarcity being a newly encountered difficult problem in ML-for-TP are uninformed. In particular, all of the larger ITP corpora (Isabelle, Mizar, HOl, Coq) are capable of easily exporting millions of problems and proofs to start with and this has been done many times since long ago. Already the 2003 version of MPTP and the AI/TP experiments based on it (https://doi.org/10.1007/s10817-004-6245-1) allowed and announced a straightforward generation of 630000 related proof tasks from Mizar. Further millions/billions/zillions of proving and training tasks can be created easily by chasing the large derivation graphs of the ITP libraries. This has been to various extent used in works such as [1,2,3], where the benefit of learning from additional proof tasks was also demonstrated. Equally easy and cheap is chasing the graphs of large ATP proofs and generating problems from them, running ATPs to generate terabytes of further data, etc. The theorem proving domain is really the antithesis of data scarcity, has been such for long time, and it is one of its great advantages over NLP domains.
A number of related experiments have been done with argument, witness, conjecture and proof (step) synthesis recently. See e.g. [4-7].
[1] Cezary Kaliszyk, Josef Urban: Learning-assisted theorem proving with millions of lemmas. J. Symb. Comput. 69: 109-128 (2015)
[2] Kaliszyk, C., Urban, J. & Vyskocil, J. Lemmatization for Stronger Reasoning in Large Theories in FroCoS 2015 9322 (Springer, 2015), 341–356.
[3] Bartosz Piotrowski, Josef Urban: Stateful Premise Selection by Recurrent Neural Networks. LPAR 2020: 409-422
[4] Thibault Gauthier: Deep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic. LPAR 2020: 230-248
[5] Thibault Gauthier: Deep Reinforcement Learning in HOL4. CoRR abs/1910.11797 (2019)
[6] Bartosz Piotrowski, Josef Urban: Guiding Inferences in Connection Tableau by Recurrent Neural Networks. CICM 2020: 309-314
[7] Josef Urban, Jan Jakubuv: First Neural Conjecturing Datasets and Experiments. CICM 2020: 315-323 UPDATE:
I do not agree with the idea that the potential test set contamination would be a significant advantage due to its autoformalization potential:
The Wang et all 2018 paper I mentioned and their related 2020 paper show that RNNs and Transformers are very good at such translations. I don't agree this would be surprising today for GPT.
This is really an opposite of a "significant advantage". The test set evaluation is thus made incomparable to any other honest evaluation done on Lean in the future.
The WebMath dataset has not been published and is impossible to check by readers and researchers. I would expect at least its publication as a partial response to such concerns.
Not doing the test set ATP evaluation without the WebMath pretraining is a serious omission.
I also do not see any improvement in explaining the hardware resources used for the evaluation. This again makes the numbers here hard to compare with for other researchers and methods.
There is also hardly any improvement of the overclaims (noted also by other reviews) about the technical novelty, claims about comparable slowness of RL setups, etc. %%%%%%%%%%%%%%%%%%%

Review Point: 69: 109-128 (2015) [2] Kaliszyk, C., Urban, J. & Vyskocil, J. Lemmatization for Stronger Reasoning in Large Theories in FroCoS 2015 9322 (Springer, 2015), 341–356. [3] Bartosz Piotrowski, Josef Urban: Stateful Premise Selection by Recurrent Neural Networks. LPAR 2020: 409-422 [4] Thibault Gauthier: Deep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic. LPAR 2020: 230-248 [5] Thibault Gauthier: Deep Reinforcement Learning in HOL4. CoRR abs/1910.11797 (2019) [6] Bartosz Piotrowski, Josef Urban: Guiding Inferences in Connection Tableau by Recurrent Neural Networks. CICM 2020: 309-314 [7] Josef Urban, Jan Jakubuv: First Neural Conjecturing Datasets and Experiments. CICM 2020: 315-323 UPDATE: I do not agree with the idea that the potential test set contamination would be a significant advantage due to its autoformalization potential: The Wang et all 2018 paper I mentioned and their related 2020 paper show that RNNs and Transformers are very good at such translations. I don't agree this would be surprising today for GPT. This is really an opposite of a "significant advantage". The test set evaluation is thus made incomparable to any other honest evaluation done on Lean in the future. The WebMath dataset has not been published and is impossible to check by readers and researchers. I would expect at least its publication as a partial response to such concerns. Not doing the test set ATP evaluation without the WebMath pretraining is a serious omission. I also do not see any improvement in explaining the hardware resources used for the evaluation. This again makes the numbers here hard to compare with for other researchers and methods. There is also hardly any improvement of the overclaims (noted also by other reviews) about the technical novelty, claims about comparable slowness of RL setups, etc. %%%%%%%%%%%%%%%%%%%
==================================================

Focused review:

Weaknesses:
Essentially, the proposed SMTL is a special case of an existing work (PS-MCNN [1]). PS-MCNN degenerates into SMTL after two steps of modification: 1) remove the interactions between SNet and TSNets in shallow layers, and 2) remove the deep layers of SNet. Therefore, the novelty of this work is marginal. The authors should compare SMTL with the more general PS-MCNN and demonstrate the superiority of SMTL both theoretically and experimentally.
Clearly there is a benefit in the quantitative metrics across various datasets. However, in order to prove the better performance of SMTL is not from an increase in the number of parameters, there should be some discussion about the number of parameters added to the single task networks.
Key parameters such as the initial value of αt should be varied and further analyzed.
The authors should discuss about how much computation or memory can be actually saved by replacing SMTL with L-SMTL on each dataset.
[1] J. Cao, Y. Li and Z. Zhang, "Partially Shared Multi-task Convolutional Neural Network with Local Constraint for Face Attribute Learning," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 4290-4299, doi: 10.1109/CVPR.2018.00451.

Review Point: 1) remove the interactions between SNet and TSNets in shallow layers, and 2) remove the deep layers of SNet. Therefore, the novelty of this work is marginal. The authors should compare SMTL with the more general PS-MCNN and demonstrate the superiority of SMTL both theoretically and experimentally. Clearly there is a benefit in the quantitative metrics across various datasets. However, in order to prove the better performance of SMTL is not from an increase in the number of parameters, there should be some discussion about the number of parameters added to the single task networks. Key parameters such as the initial value of αt should be varied and further analyzed. The authors should discuss about how much computation or memory can be actually saved by replacing SMTL with L-SMTL on each dataset. [1] J. Cao, Y. Li and Z. Zhang, "Partially Shared Multi-task Convolutional Neural Network with Local Constraint for Face Attribute Learning," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 4290-4299, doi: 10.1109/CVPR.2018.00451.
==================================================

Focused review:

1. The training time and memory cost could increase by several times. 2. Since the training is more time consuming, many other solutions such as mimicking / distilling based solutions are suggested to compare with. 3. Only a very simple sub-network sampling strategy is considered, what about randomly choosing a sub-network each time, or keeping the most part fixed and a some portion randomly chosen? 4. Only a very simple data augmentation (different input training size) is considered in the sub-network training. What about other choices? ====== post rebuttal ======= I misunderstood some important points in the paper in my original review comments. 1. The difference between GradAug and GradAug+. Now that I know that GradAug+ (GradAug with CutMix augmentations) does achieve STOA results on several tasks. For example, for ImageNet classification, the result of 79.6% acc of top 1 is STOA as I far as I know. 2. Experiments show that GradAug need less epochs to converge to a good result, and this alleviate my concern about the time cost to some extent. 3. The idea also seems to work good in the setting of stochastic depth.

Review Point: 1. The training time and memory cost could increase by several times.
Review Point: 2. Since the training is more time consuming, many other solutions such as mimicking / distilling based solutions are suggested to compare with.
Review Point: 3. Only a very simple sub-network sampling strategy is considered, what about randomly choosing a sub-network each time, or keeping the most part fixed and a some portion randomly chosen?
Review Point: 4. Only a very simple data augmentation (different input training size) is considered in the sub-network training. What about other choices? ====== post rebuttal ======= I misunderstood some important points in the paper in my original review comments.
Review Point: 1. The difference between GradAug and GradAug+. Now that I know that GradAug+ (GradAug with CutMix augmentations) does achieve STOA results on several tasks. For example, for ImageNet classification, the result of 79.6% acc of top 1 is STOA as I far as I know.
Review Point: 2. Experiments show that GradAug need less epochs to converge to a good result, and this alleviate my concern about the time cost to some extent.
Review Point: 3. The idea also seems to work good in the setting of stochastic depth.
==================================================

Focused review:

The relevance of this paper is entirely unclear, for multiple reasons: 1. The author themselves state "This work does not present any foreseeable societal consequence.", raising the question why we should we care about this work in the first place. 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise. 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.

Review Point: 1. The author themselves state "This work does not present any foreseeable societal consequence.", raising the question why we should we care about this work in the first place.
Review Point: 2. They don't make any detectable effort towards arguing for why their work is relevant in the paper either, rendering it a purely theoretical exercise.
Review Point: 3. No empirical evaluation whatsoever is provided, there is no comparison (except for on an abstract level) with other methods. It is completely unclear what the practical value of the contribution even could be. Even a theoretical paper should at least try to argue for why it matters, this is not the case with this submission. The theoretical contributions may well be significant and valuable, however, in its current form this paper is not suitable for a publication at NeurIPS.
==================================================

Focused review:

Weaknesses:
The novelty is limited. the idea is to have a mixed image with style and content from different images, which is similar to StyleMix.
The proposed approach needs more computational resources to obtain good performances: 2000 epochs on Cifar10/100 (300 epochs in StyleMix). How long does the entire training take? In some cases, it might be a problem. I would suggest to precise more details about the computational resources in the paper. Moreover, as the approach takes longer to train and the inference is the same for all the competitive approaches, it is not convincing that the approach is more efficient in terms of computational complexity.
Table1.b Results are not comparable, some approaches seem to be trained much fewer epochs than the proposed one. For example, StyleMix only trained for 100 epochs, while the proposed approach trained for 300 epochs. It would be better to clarify this difference by also reporting results in Table.5.
Some important training details are missing, for example, image resolution plays an important role in the performance of image classification[1]; Are there other data augmentations used during the training?
During the training, the proposed algorithm needs to sample from 4 (5 for alignMix/AE) cases, I would suggest an analysis on these cases. For example, we could remove one case to see how the performance drops. The goal is to understand where the improvement comes from.
It is interesting to see how the proposed approach improves over state-of-the-art approaches. For example: Deit[2], Swin transformer[3] etc. Clarity:
The approach is in general well-presented, some details are missing:
Table 1.b: it would be better to precise MESC/BATCH also in the caption of the table.
PreActResnet18 seems to be different from the original paper [He et. 2016], if so, please clarify the difference.
Supplementary B. Parameter setting. The following description is confusing: “We also train R-50 on ImageNet for 100 epochs, following the training protocol described in Kim et al. (Kim et al., 2021).” I guess it refers to the results in table 5, but the associated text should be also there.
The description of the architecture is confusing, the dimension of the output A should be different for CIFAR10 and ImageNet.
The ablation study is hard to follow (Appendix C). The notations are confusing: “c ∈ R 512
” but “e is a 128 × 2 ×
2 tensor”. Different sets in the column LAYERS (TABLE 8) are also hard to understand. I would suggest revising this part and making the messages clear.
[1] Touvron, H., Vedaldi, A., Douze, M., & Jégou, H. (2019). Fixing the train-test resolution discrepancy. NeurIPS 2019
[2] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR.
[3] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. ICCV 2021

Review Point: 2000 epochs on Cifar10/100 (300 epochs in StyleMix). How long does the entire training take? In some cases, it might be a problem. I would suggest to precise more details about the computational resources in the paper. Moreover, as the approach takes longer to train and the inference is the same for all the competitive approaches, it is not convincing that the approach is more efficient in terms of computational complexity.
==================================================

Focused review:

1. (Partially addressed by rebuttal) As mentioned above, I am somewhat concerned about the generality of this method. The method was tested on non-standard datasets for unclear reasons, and there is no layer choice hyper-parameter ablation study. 2. (Addressed by rebuttal) The human interpretability experiment set up seems interesting, but a bit strange to me. Why not show the test subject a subset of the method's most concept/cluster relevant images and then ask them to classify some other images as concept/cluster relevant or not relevant (the set-up I'm suggesting is used by contemporary work submitted to NeurIPS 2020)? Importantly, this should be done on the AwA dataset or Imagenet, not just the toy dataset. 3. (Addressed by rebuttal) The NLP experiment augmenting inputs is an interesting idea, but it seems to me it needs a control scenario. For example, why not append 5 random words to check whether the effects of going out of distribution is somehow having a large effect? Overall, I think the motivation and proposed solution are great, but the empirical validation is somewhat problematic. A priori, if I had only read the description of the method without the experiments, I'd suspect that the method would be unlikely to provide human interpretable concepts. Although the experimental design could feasibly deal with this concern, the incomplete benchmarking raises questions for me about how the experiments were selected and why results were not more extensively reported.

Review Point: 1. (Partially addressed by rebuttal) As mentioned above, I am somewhat concerned about the generality of this method. The method was tested on non-standard datasets for unclear reasons, and there is no layer choice hyper-parameter ablation study.
Review Point: 2. (Addressed by rebuttal) The human interpretability experiment set up seems interesting, but a bit strange to me. Why not show the test subject a subset of the method's most concept/cluster relevant images and then ask them to classify some other images as concept/cluster relevant or not relevant (the set-up I'm suggesting is used by contemporary work submitted to NeurIPS 2020)? Importantly, this should be done on the AwA dataset or Imagenet, not just the toy dataset.
Review Point: 3. (Addressed by rebuttal) The NLP experiment augmenting inputs is an interesting idea, but it seems to me it needs a control scenario. For example, why not append 5 random words to check whether the effects of going out of distribution is somehow having a large effect? Overall, I think the motivation and proposed solution are great, but the empirical validation is somewhat problematic. A priori, if I had only read the description of the method without the experiments, I'd suspect that the method would be unlikely to provide human interpretable concepts. Although the experimental design could feasibly deal with this concern, the incomplete benchmarking raises questions for me about how the experiments were selected and why results were not more extensively reported.
==================================================

Focused review:

- In section 5, using JPEG compression without backpropagation seems questionable. As in this way, the encoder will not adjust the encoding methods to protect the information from the compression. In section 4.2, high-frequency information is significant for recovering the secret data. However, JPEG is known to clip the high-frequency information. If the encoder is not adjusted, the conclusion is kind of contradictory. - In section 4.1, the analysis of spatial and channel dimension is not convincing enough. Is there the possibility it depends more on the deep architecture or the training methods.

Review Point: - In section 5, using JPEG compression without backpropagation seems questionable. As in this way, the encoder will not adjust the encoding methods to protect the information from the compression. In section 4.2, high-frequency information is significant for recovering the secret data. However, JPEG is known to clip the high-frequency information. If the encoder is not adjusted, the conclusion is kind of contradictory.
Review Point: - In section 4.1, the analysis of spatial and channel dimension is not convincing enough. Is there the possibility it depends more on the deep architecture or the training methods.
==================================================

Focused review:

Weakness:
1: The proposed method aims to replace NS using a variance reduction sampling and achieves better convergence. The question is whether NS achieves SOTA performance on the standard GNNs datasets?
2: My main concern is on the experiment part:
The paper is about sampling method to speed up GNN training. However, there are no result on wall time for training GNNs.
There is no accuracy reported for all the four datasets tested in the experiment part.
No comparison with latest GNN fast training methods.
missing ablation studies.

Review Point: 1: The proposed method aims to replace NS using a variance reduction sampling and achieves better convergence. The question is whether NS achieves SOTA performance on the standard GNNs datasets?
Review Point: 2: My main concern is on the experiment part: The paper is about sampling method to speed up GNN training. However, there are no result on wall time for training GNNs. There is no accuracy reported for all the four datasets tested in the experiment part. No comparison with latest GNN fast training methods. missing ablation studies.
==================================================

Focused review:

1. The novelty of the method is rather incremental, and heuristic in the selecting k number. 2. The problem of GAN to be addressed in the paper is unclear. 3. No comparison of similar works using critics for rejection sampling, e.g., [2].

Review Point: 1. The novelty of the method is rather incremental, and heuristic in the selecting k number.
Review Point: 2. The problem of GAN to be addressed in the paper is unclear.
Review Point: 3. No comparison of similar works using critics for rejection sampling, e.g., [2].
==================================================

Focused review:

Weaknesses of the paper:
All the experimental results in the paper adopt a very large architecture: 12 enc, 12 decoder layers, with 1024 dim, on relatively small-medium sized data setups. There might be a chance that the overfitting behavior shown in the paper might be exacerbated by the large architecture choice. One aspect missing from the paper is how the study is affected by different architecture sizes.
It has become a common practice nowadays to opt for a deep shared encoder and separate language specific decoders, or introducing language specific parameters in the decoder. I believe that the off-target translation problem in zero-shot could be greatly reduced by introducing target language specific parameters.
In one of the experimental analysis section, the paper shows that the off-target translation issue is resolved more or less completely by just adding 2 language centric data instead of one-language centric. This leads me to believe that adding a small amount of synthetically generated or crawled non-centric data into the mix will greatly reduce the problem. The paper does have some data-augmentation experiments at the end, but it would have been interesting to see the reduction in the off-target translation issue with varying amounts of augmented data. It may be the case that adding very small amount of augmented data already solves the problem.

Review Point: 12 enc, 12 decoder layers, with 1024 dim, on relatively small-medium sized data setups. There might be a chance that the overfitting behavior shown in the paper might be exacerbated by the large architecture choice. One aspect missing from the paper is how the study is affected by different architecture sizes. It has become a common practice nowadays to opt for a deep shared encoder and separate language specific decoders, or introducing language specific parameters in the decoder. I believe that the off-target translation problem in zero-shot could be greatly reduced by introducing target language specific parameters. In one of the experimental analysis section, the paper shows that the off-target translation issue is resolved more or less completely by just adding 2 language centric data instead of one-language centric. This leads me to believe that adding a small amount of synthetically generated or crawled non-centric data into the mix will greatly reduce the problem. The paper does have some data-augmentation experiments at the end, but it would have been interesting to see the reduction in the off-target translation issue with varying amounts of augmented data. It may be the case that adding very small amount of augmented data already solves the problem.
==================================================

Focused review:

In decreasing order of importance: 1. My biggest concern is with the way that $\epsilon_1$ behaves depending on n. Firstly, it seems the choice of the value $-n$ for $\rho$ is arbitrary (with the choice being repercuted in the definition of $\epsilon$), and this should be discussed more clearly in the text. Next, it is not clear to me why the choice $\rho=-n$ is the best. Does it optimize $\epsilon_1$ in some way? Furthermore, as n tends to $\infty$, it seems that $\epsilon_1$ does NOT tend to infinity. This would imply that the bounds in the cases where no good bound on $\epsilon_2$ is available (i.e., the cases which are not covered by the work [8]), the final bound, although not trivial, is not a "high probability" bound in the strict sense of a vanishingly small failure probability. The closest thing to an explanation for this seems to be in the beginning of Section 4, but I could not find the answer to my question there. I assume I misunderstood something here, and I might lower my score in the less likely case that this turns out to be as serious an issue as it superficially appears to me. 2. The idea that fairness improves the bounds is not counter intuitive as claimed in the introduction: it is clear that the fairness assumption which applies to both the ground truth and the function search space reduces the complexity of the problem. (And without a good answer to point one, the improvement would be incremental from the theoretical point of view, although I agree that the experiments section shows improved results in this case). Furthermore, there is no attempt at extending the results to a slightly different setting or applying the results to any well defined machine learning problem, which undermines the relevance of the work to the community. It would also be intresting to try to tackle the case where the ground truth is not fair but the method requires fairness (this is closer to the general paradigm studied in the fairness literature). One could for instance consider the case where the ground is close to satisfying fairness (but does not exactly), as a result of being drawn from a high dimensional distribution whose expectation satisfies the fairness assumption: for instance, suppose the vertices of the graph are people and the attribute is "male/female", with the labels representing failure or success. For a finite graph, it is reasonable to assume that the ground truth labels are drawn from a distribution with each label being independent of gender. This will not necessarily translate to an exactly fair set of ground truth labels such as the ones considered in this work. It is a little underwhelming that such natural situations are not treatable with the results provided in this work. (2. bis. Still no solution for square grids...) And less importantly: 3. Although many of the proofs are impressive and rigorous, I don't feel they are very reader friendly (though it could be explained by a lack of familiarity with the literature on my part). In the Review Section "Additional feedback", I list some aspects which could be explained in greater detail. 4. There are some other very minor issues I list in the Section "Additional feedback".

Review Point: 1. My biggest concern is with the way that $\epsilon_1$ behaves depending on n. Firstly, it seems the choice of the value $-n$ for $\rho$ is arbitrary (with the choice being repercuted in the definition of $\epsilon$), and this should be discussed more clearly in the text. Next, it is not clear to me why the choice $\rho=-n$ is the best. Does it optimize $\epsilon_1$ in some way? Furthermore, as n tends to $\infty$, it seems that $\epsilon_1$ does NOT tend to infinity. This would imply that the bounds in the cases where no good bound on $\epsilon_2$ is available (i.e., the cases which are not covered by the work [8]), the final bound, although not trivial, is not a "high probability" bound in the strict sense of a vanishingly small failure probability. The closest thing to an explanation for this seems to be in the beginning of Section 4, but I could not find the answer to my question there. I assume I misunderstood something here, and I might lower my score in the less likely case that this turns out to be as serious an issue as it superficially appears to me.
Review Point: 2. The idea that fairness improves the bounds is not counter intuitive as claimed in the introduction: it is clear that the fairness assumption which applies to both the ground truth and the function search space reduces the complexity of the problem. (And without a good answer to point one, the improvement would be incremental from the theoretical point of view, although I agree that the experiments section shows improved results in this case). Furthermore, there is no attempt at extending the results to a slightly different setting or applying the results to any well defined machine learning problem, which undermines the relevance of the work to the community. It would also be intresting to try to tackle the case where the ground truth is not fair but the method requires fairness (this is closer to the general paradigm studied in the fairness literature). One could for instance consider the case where the ground is close to satisfying fairness (but does not exactly), as a result of being drawn from a high dimensional distribution whose expectation satisfies the fairness assumption: for instance, suppose the vertices of the graph are people and the attribute is "male/female", with the labels representing failure or success. For a finite graph, it is reasonable to assume that the ground truth labels are drawn from a distribution with each label being independent of gender. This will not necessarily translate to an exactly fair set of ground truth labels such as the ones considered in this work. It is a little underwhelming that such natural situations are not treatable with the results provided in this work. (2. bis. Still no solution for square grids...) And less importantly:
Review Point: 3. Although many of the proofs are impressive and rigorous, I don't feel they are very reader friendly (though it could be explained by a lack of familiarity with the literature on my part). In the Review Section "Additional feedback", I list some aspects which could be explained in greater detail.
Review Point: 4. There are some other very minor issues I list in the Section "Additional feedback".
==================================================

Focused review:

Weaknesses:
+ The perspective of using mutual information to increase the robustness is interesting. The authors provide clear analyses of how to use mutual information to learn task-irrelevant information based on multi-view settings.
+ The proposed method is a plug-in penalty that can be integrated into many modern RL algorithms.
- The representation encoder and the estimator of mutual information (using InfoNCE) are borrowed from existing works, which may limit the contribution of this work.
- The proposed methods show advantages in DMC and ProcGen environments. However, the settings in both environments are very similar: changing the background image in the same task. I am not sure if there are any real-world tasks that can be better solved by the proposed method. It would be great if the author can conduct experiments on other realistic tasks.
General questions
(1) How is the data augmentation conducted to create the multi-view setting? For example, in Figure 1, is the entire image rotated or only the background rotated? If the entire image is rotated, will the state change and the action not be consistent with the state?
(2) How to create the multi-view setting for layouts in the ProcGen suite? In my understanding, the layout of environments is corresponding to the difficulty level. How to conduct image augmentation on such difficulty levels? Or, do the author only change the background image in ProcGen?
(3) Based on (2), I am also wondering if this method can be extended to more general settings to increase the robustness?

Review Point: + The perspective of using mutual information to increase the robustness is interesting. The authors provide clear analyses of how to use mutual information to learn task-irrelevant information based on multi-view settings.
Review Point: + The proposed method is a plug-in penalty that can be integrated into many modern RL algorithms.
Review Point: - The representation encoder and the estimator of mutual information (using InfoNCE) are borrowed from existing works, which may limit the contribution of this work.
Review Point: - The proposed methods show advantages in DMC and ProcGen environments. However, the settings in both environments are very similar: changing the background image in the same task. I am not sure if there are any real-world tasks that can be better solved by the proposed method. It would be great if the author can conduct experiments on other realistic tasks. General questions (1) How is the data augmentation conducted to create the multi-view setting? For example, in Figure 1, is the entire image rotated or only the background rotated? If the entire image is rotated, will the state change and the action not be consistent with the state? (2) How to create the multi-view setting for layouts in the ProcGen suite? In my understanding, the layout of environments is corresponding to the difficulty level. How to conduct image augmentation on such difficulty levels? Or, do the author only change the background image in ProcGen? (3) Based on (2), I am also wondering if this method can be extended to more general settings to increase the robustness?
==================================================

Focused review:

Weakness
1 The way of using GP is kind of straightforward and naive. In the GP community, dynamical modeling has been widely investigated, from the start of Gaussian Process Dynamical Model in NIPs 2005.
2 I do not quite get the modules of LSTM Frame Generation and GP Frame Generation in Eq (4). Where are these modules in Fig.3 ? The D in the Stage 3? Using GP to generate Images? Does it make sense? GP is more suitable to work in the latent space, is it?
3 The datasets are not quite representative, due to the simple and experimental scenarios. Moreover, the proposed method is like a fundamental work. But is it useful for high-level research topics, e.g., large-scale action recognition, video caption, etc?

Review Point: 1 The way of using GP is kind of straightforward and naive. In the GP community, dynamical modeling has been widely investigated, from the start of Gaussian Process Dynamical Model in NIPs 2005.
Review Point: 2 I do not quite get the modules of LSTM Frame Generation and GP Frame Generation in Eq (4). Where are these modules in Fig.3 ? The D in the Stage 3? Using GP to generate Images? Does it make sense? GP is more suitable to work in the latent space, is it?
Review Point: 3 The datasets are not quite representative, due to the simple and experimental scenarios. Moreover, the proposed method is like a fundamental work. But is it useful for high-level research topics, e.g., large-scale action recognition, video caption, etc?
==================================================

Focused review:

Weaknesses:
• Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
• Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed
• Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance
• How are the hyperparameters for the learning chosen?
• Do the results hold with multiple random initializations of the model?

Review Point: • Not clear what the algorithmic contribution is; the methods provided in section 2 and algorithm1 seem to be already existent.
Review Point: • Recently developed EBM approaches have been effectively applied for HEP event detection but no new machine learning approaches have been proposed • Ablation experiments to understand the effect of the effect of various components and especially the transformer network is crucial to understand the model performance • How are the hyperparameters for the learning chosen?
Review Point: • Do the results hold with multiple random initializations of the model?
==================================================

Focused review:

Weaknesses: Not much novelty in method. Not quite clear if data set is general enough for other domains.
- General Discussion: This paper describes a rule-based method for generating additional weakly labeled data for event extraction. The method has three main stages. First, it uses Freebase to find important slot fillers for matching sentences in Wikipedia (using all slot fillers is too stringent resulting in too few matches). Next, it uses FrameNet to to improve reliability of labeling trigger verbs and to find nominal triggers. Lastly, it uses a multi-instance learning to deal with the noisily generated training data.
What I like about this paper is that it improves over the state-of-the-art on a non-trival benchmark. The rules involved don't seem too obfuscated, so I think it might be useful for the practitioner who is interested to improve IE systems for other domains. On the other hand, some some manual effort is still needed, for example for mapping Freebase event types to ACE event types (as written in Section 5.3 line 578). This also makes it difficult for future work to calibrate apple-to-apple against this paper.       Apart from this, the method also doesn't seem too novel.
Other comments: - I'm also concern with the generalizability of this method to other  domains. Section 2 line 262 says that 21 event types are selected  from Freebase. How are they selected? What is the coverage on the 33 event types in the ACE data.
- The paper is generally well-written although I have some  suggestions for improvement.       Section 3.1 line 316 uses "arguments liked time, location...". If you mean roles or arguments, or maybe you want to use actual realizations of time and location as examples. There are minor typos, for e.g. line 357 is missing a "that", but this is not a major concern I have for this paper. 

Review Point: - General Discussion: This paper describes a rule-based method for generating additional weakly labeled data for event extraction. The method has three main stages. First, it uses Freebase to find important slot fillers for matching sentences in Wikipedia (using all slot fillers is too stringent resulting in too few matches). Next, it uses FrameNet to to improve reliability of labeling trigger verbs and to find nominal triggers. Lastly, it uses a multi-instance learning to deal with the noisily generated training data. What I like about this paper is that it improves over the state-of-the-art on a non-trival benchmark. The rules involved don't seem too obfuscated, so I think it might be useful for the practitioner who is interested to improve IE systems for other domains. On the other hand, some some manual effort is still needed, for example for mapping Freebase event types to ACE event types (as written in Section 5.3 line 578). This also makes it difficult for future work to calibrate apple-to-apple against this paper. Apart from this, the method also doesn't seem too novel. Other comments:
Review Point: - I'm also concern with the generalizability of this method to other domains. Section 2 line 262 says that 21 event types are selected from Freebase. How are they selected? What is the coverage on the 33 event types in the ACE data.
==================================================

Focused review:

Weaknesses. It's hard to judge impact in real-world settings when most of the quantitative evaluations are on datasets not representative of complex natural images (e.g. MNIST and NORB). On MNIST, the method shows clear advantages over competing methods. However, even on NORB, where a lot of the deformations can't easily be parameterized, this advantage has turned into being only on par with other leading methods. I think the inclusion of the faces dataset was important for this reason. I was confused for a while what the exact orbit was for each dataset. I kept scanning the text for this. A table of all three datasets and a short note on how orbits were defined and canonical samples selected would make things a lot clearer. Concurrent work. Similar ideas of representation learning through transformation priors have appeared in recent work. I don't think it takes away any novelty from this submission, since judging from the dates these is concurrent works. I just thought I would bring your attention to it: - https://openreview.net/pdf?id=S1v4N2l0- (ICLR 2018) - https://arxiv.org/pdf/1804.01552.pdf (CVPR 2018) Minor comments. - eq. 6: what connects the orbit with this loss? I don't see the connection just yet - eq. 7: "x_q not in Oxq!=Oxi" What is this notation "set1 != set2" that seems to imply it forms another set (and not a true/false value) line 136: Oxq \not= Oxi, again, I'm not sure about this notation. I understand what it means, but it looks odd to me. I have never seen this as part of set notation before. - eq. 8: where is x_p, x_q, x_c coming from? Shouldn't the summand be $(x_i, x_p, x_q) \in \mathcal{T}$? The canonical sample x_c is still unclear where it comes from. If x_c is the canonical instance for each orbit, then it also changes in the summation. This is not clear from the notation. - line 196: max unpooling transfers the argmax knowledge of maxpooling to the decoder. Do you use this behavior too? - Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11? - line 213: are all feature spaces well-suited for 1-NN? If a feature space is not close to a spherical Gaussian, it may perform poorly. If feature dimensions are individually standardized, it would avoid this issue. - It was a bit unclear how canonical samples were constructed on the face dataset ("least yaw displacement from a frontal pose"). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?

Review Point: - https://openreview.net/pdf?id=S1v4N2l0- (ICLR 2018) - https://arxiv.org/pdf/1804.01552.pdf (CVPR 2018) Minor comments.
Review Point: - eq.6: what connects the orbit with this loss? I don't see the connection just yet - eq.
Review Point: 7: "x_q not in Oxq!=Oxi" What is this notation "set1 != set2" that seems to imply it forms another set (and not a true/false value) line 136: Oxq \not= Oxi, again, I'm not sure about this notation. I understand what it means, but it looks odd to me. I have never seen this as part of set notation before.
Review Point: - eq.8: where is x_p, x_q, x_c coming from? Shouldn't the summand be $(x_i, x_p, x_q) \in \mathcal{T}$? The canonical sample x_c is still unclear where it comes from. If x_c is the canonical instance for each orbit, then it also changes in the summation. This is not clear from the notation.
Review Point: - line 196: max unpooling transfers the argmax knowledge of maxpooling to the decoder. Do you use this behavior too?
Review Point: - Table 1: Should EX/NORB really be bold-faced? Is the diff between 0.59+/-0.12 really statistically significant from 0.58+/-0.11?
Review Point: - line 213: are all feature spaces well-suited for 1-NN? If a feature space is not close to a spherical Gaussian, it may perform poorly. If feature dimensions are individually standardized, it would avoid this issue.
Review Point: - It was a bit unclear how canonical samples were constructed on the face dataset ("least yaw displacement from a frontal pose"). This seems to require a lot of priors on faces and does not seem like purely unsupervised learning. Did the other competing methods require canonical examples to be designated?
==================================================

Focused review:

Major Comments: (1) Organization of theoretical results. * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper. * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)

Review Point: * It is not clear to me the significance of Lemma 1 and Theorem 1, specifically, why they are surprising or important for CE (e.g., lemma 1 seem to be a standard application of LLN), and what insight can be derived from them (e.g., why should I care about the asymptotic rate of the stochastically bound O(1/nm)? Does it related to any operating characteristics of the model?). Comparing to these results, Proposition 1 seem to be more central to this paper, and may merit more detailed explanation. If authors agree, it may be beneficial to shift the emphasis in the presentation of the theoretical results, for example, move Lemma 1 to the Appendix. Unless Theorem 1 has crucial implications that is central to the story of the paper, I suggest author move it to the appendix also, or alternatively give a bit more explanation to justify its position in the paper.
Review Point: * As mentioned in the last paragraph, Proposition 1 seem to be more important and deserves its own background section. In particular, author should explain what $\alpha$ in Equation (2) represents, and explain how to compute it in theory and in practice, thereby providing context for Algorithm 1. It will also reader to understand the importance of Proposition 1 by explaining the connection between Var(K) and model's generalization ability, like what you illustrated empirically in Figure 4. (2) (Optionally,) adding baseline comparison to experiments 4.2, and more discussion. While I find the experiments interesting, it might be beneficial to add at least one standard baseline method (e.g., standard neural architecture search) to compare against primal formulation. So author can compare the difference in terms of resulting architecture, estimated kernel variance, testing error and total wall clock time used to better illustrate the benefit of the existing method. Minor:
Review Point: * line 68, "respectively.." (should be only one period) * line 148 cifar10/100 -> CIFAR-10/-100 * There's some relevant recent work on NTK for ensemble models, e.g., [1] and references therein. It appears rather relevant and should be included in Section 5 as well. [1] Bobby He, Balaji Lakshminarayanan, Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. (https://arxiv.org/abs/2007.05864)
==================================================

Focused review:

Although I believe the intrinsic difference of neurons could benefit for information transmission, I have some conceptual questions. I think properly answer these questions in the Discussion or briefly mention some of them in author feedback could improve the impact of this work in general. 1. Whether a network is an integrator of a differentiator is highly determined by the value of \beta. Is it possible with an intermediate value of \beta, the network’s output is proportional to the input, i.e., the network simply relay the input but neither differentiating or integrating. In this case, we probably only need one layer to transmit the input without the cascade of an integrator and a differentiator. Update after rebuttal: I think in principle there might be a particular \beta to achieve same transmission with integrator-differentiator circuit, but some fine-tuned mechanisms are needed. And hence the heterogeneity would be a more robust mechanism to achieve reliable transmission. 2. If we reverse the order of differentiator and integrator, would the input be reliably transmitted as well? Is there some considerations for Drosophila’s olfactory system has a structure of an integrator followed by a differentiator? Update after rebuttal: I agree author’s statement about this and I hope the author could briefly discuss the order of integrator and differentiator in a revised manuscript.

Review Point: 1. Whether a network is an integrator of a differentiator is highly determined by the value of \beta. Is it possible with an intermediate value of \beta, the network’s output is proportional to the input, i.e., the network simply relay the input but neither differentiating or integrating. In this case, we probably only need one layer to transmit the input without the cascade of an integrator and a differentiator. Update after rebuttal: I think in principle there might be a particular \beta to achieve same transmission with integrator-differentiator circuit, but some fine-tuned mechanisms are needed. And hence the heterogeneity would be a more robust mechanism to achieve reliable transmission.
Review Point: 2. If we reverse the order of differentiator and integrator, would the input be reliably transmitted as well? Is there some considerations for Drosophila’s olfactory system has a structure of an integrator followed by a differentiator? Update after rebuttal: I agree author’s statement about this and I hope the author could briefly discuss the order of integrator and differentiator in a revised manuscript.
==================================================

Focused review:

weaknesses. Are other methods such as Barak, Kelner, Steuer 2014 "Rounding sum-of-squares relaxations" relevant? 
6. Sec 4 Experiments. 
When you run BP-SP, you obtain marginals. How do you then compute your approximate MAP solution? Do you use the same CLAP rounding approach or something else? This may be important since in your experiments, BP-SP performs very well.
Since you use triangles as regions for PSOS(4), could you try the same for GBP to make the comparison more similar? Particularly since it appears somewhat odd that the current GBP with 4-sets is not doing better than BP-SP.
Times should be reported for all methods to allow more meaningful comparisons [I recognize this can be tricky with non-optimized code but the pattern as larger models are examined would still be helpful].
If possible, it would be instructive to add experiments for larger planar models with no singleton potentials, where it is feasible to compute the exact MAP score. 
Minor points:
In a few places, claims are perhaps stronger than justified - e.g. in the Abstract, "significantly outperforms BP and GBP" ; l. 101 perhaps remove "extensive"; l. 243 - surely the exact max was obtained only for the small experiments; you don't know for the larger models?
A few capitalizations are missing in the References, e.g. Ising, Burer-Monteiro, SDP =======================
I have read the rebuttal and thank the authors for addressing some of my concerns.

Review Point: 6. Sec 4 Experiments. When you run BP-SP, you obtain marginals. How do you then compute your approximate MAP solution? Do you use the same CLAP rounding approach or something else? This may be important since in your experiments, BP-SP performs very well. Since you use triangles as regions for PSOS(4), could you try the same for GBP to make the comparison more similar? Particularly since it appears somewhat odd that the current GBP with 4-sets is not doing better than BP-SP. Times should be reported for all methods to allow more meaningful comparisons [I recognize this can be tricky with non-optimized code but the pattern as larger models are examined would still be helpful]. If possible, it would be instructive to add experiments for larger planar models with no singleton potentials, where it is feasible to compute the exact MAP score. Minor points: In a few places, claims are perhaps stronger than justified - e.g. in the Abstract, "significantly outperforms BP and GBP" ; l.
Review Point: 243 - surely the exact max was obtained only for the small experiments; you don't know for the larger models? A few capitalizations are missing in the References, e.g. Ising, Burer-Monteiro, SDP ======================= I have read the rebuttal and thank the authors for addressing some of my concerns.
==================================================

Focused review:

- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations. - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings. 
- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better. Missing References: Syntactically controlled paraphrase generation: Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control <-- this is a contemporaneous work, but would be nice to cite in next version. Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization 

Review Point: - Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.
Review Point: - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
Review Point: - Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
Review Point: - The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings.
Review Point: - The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better. Missing References: Syntactically controlled paraphrase generation: Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control <-- this is a contemporaneous work, but would be nice to cite in next version. Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization
==================================================

Focused review:

- Presentation: I think the space that the paper spends on the BP background is more than necessary since the BP algorithm is just the standard one. The paper would be more compelling if the BP background is compressed and a more complete explanation of their algorithm is presented, for example some visual illustration that comes with the explanation of their implementation in Section 3.3. Moreover, since there are not many notations used in the paper, it is better not to use the same notation for different meanings to avoid confusion. For example, $k$ is used for the number of top elements throughout the paper and also index of variable at Line 285; at Line 301 the parameter $H$ is used without definition, and later on at Line 302 it denotes the tree height while at Line 334 a parameter in the Splash algorithm. - Missing references: There are two highly related work that are not included as baselines. [1] proposed a parallel BP algorithm incorporating belief residual scheduling and uniform work Splash operations; [2] proposed a asynchronous distributed framework to perform BP using a prioritized block scheduler. Could the authors provide some conceptual or empirical comparison of them with the proposed one? - Experiments: One part that is missing in the experiments is the comparison of inference accuracy, in order to show how the relaxed scheduler can affect the accuracy of computed beliefs in comparison with other baselines when they converge. For example in similar way to the Section 6.5 in [2]. [1] Joseph Gonzalez, Yucheng Low, Carlos Guestrin, and David R. O’Hallaron. 2009. Distributed Parallel Inference on Large Factor Graphs. In UAI 2009. [2] Jiangtao Yin and Lixin Gao. 2014. Scalable Distributed Belief Propagation withPrioritized Block Updates. InProceedings of the 23rd ACM International Conferenceon Conference on Information and Knowledge Management, CIKM 2014.

Review Point: - Presentation: I think the space that the paper spends on the BP background is more than necessary since the BP algorithm is just the standard one. The paper would be more compelling if the BP background is compressed and a more complete explanation of their algorithm is presented, for example some visual illustration that comes with the explanation of their implementation in Section 3.3. Moreover, since there are not many notations used in the paper, it is better not to use the same notation for different meanings to avoid confusion. For example, $k$ is used for the number of top elements throughout the paper and also index of variable at Line 285; at Line 301 the parameter $H$ is used without definition, and later on at Line 302 it denotes the tree height while at Line 334 a parameter in the Splash algorithm.
Review Point: - Missing references: There are two highly related work that are not included as baselines. [1] proposed a parallel BP algorithm incorporating belief residual scheduling and uniform work Splash operations; [2] proposed a asynchronous distributed framework to perform BP using a prioritized block scheduler. Could the authors provide some conceptual or empirical comparison of them with the proposed one?
Review Point: - Experiments: One part that is missing in the experiments is the comparison of inference accuracy, in order to show how the relaxed scheduler can affect the accuracy of computed beliefs in comparison with other baselines when they converge. For example in similar way to the Section 6.5 in [2]. [1] Joseph Gonzalez, Yucheng Low, Carlos Guestrin, and David R. O’Hallaron. 2009. Distributed Parallel Inference on Large Factor Graphs. In UAI 2009. [2] Jiangtao Yin and Lixin Gao. 2014. Scalable Distributed Belief Propagation withPrioritized Block Updates. InProceedings of the 23rd ACM International Conferenceon Conference on Information and Knowledge Management, CIKM 2014.
==================================================

Focused review:

They are not weaknesses but I have some concerns: 1. I do agree with the authors that the 'attention' should not be calculated based on the previous states, instead, it should use the 'future' states. However, according to the setting of hyper-parameter 'lambda', the model only works well when it equals to 0.1, which is a rather small value. This may suggest that this 'modification' of the attention mechanism is not significant. I can accept the explanation that a large 'lambda' will lead to the attention weight bias towards the last token in the sequence, but whether this suggests the 'nested' regularization mechanism proposed in this paper is not appropriate or even a wrong direction? 2. It is OK to say the proposed model outperforms all the published state-of-the-art models, however, I don't think it is appropriate to say it achieves the 1st place on the leaderboard. It only achieves the best results when C40 references are used. And it is not clear whether other models use ensemble or not. 3. This will be an excellent work if authors can show it also works on other attention-based models, on non-caption generation tasks.

Review Point: 1. I do agree with the authors that the 'attention' should not be calculated based on the previous states, instead, it should use the 'future' states. However, according to the setting of hyper-parameter 'lambda', the model only works well when it equals to 0.1, which is a rather small value. This may suggest that this 'modification' of the attention mechanism is not significant. I can accept the explanation that a large 'lambda' will lead to the attention weight bias towards the last token in the sequence, but whether this suggests the 'nested' regularization mechanism proposed in this paper is not appropriate or even a wrong direction?
Review Point: 2. It is OK to say the proposed model outperforms all the published state-of-the-art models, however, I don't think it is appropriate to say it achieves the 1st place on the leaderboard. It only achieves the best results when C40 references are used. And it is not clear whether other models use ensemble or not.
Review Point: 3. This will be an excellent work if authors can show it also works on other attention-based models, on non-caption generation tasks.
==================================================

Focused review:

Weaknesses A small constant sigma_min > 0 is used throughout the paper as the initial noise applied to the data distribution. That is, t = 1 corresponds to sigma = sigma_min instead of sigma = 0. As far as I can see, the reason for introducing sigma_min is not stated explicitly. Is it there for practical/numerical or theoretical reasons? Why can't we have sigma = 0 at t = 1? This should be explained explicitly.
I think the biggest difficulty I have with the paper is the baseline diffusion model used for the empirical evaluation. A key claim of the paper is that the proposed method effectively gives practitioners a better way to fit the vector field of a probability path than through the diffusion formulation. I would therefore expect the implementation of the diffusion baseline to be treated with due care.
For example, appendix E.1 eq (41) states that the diffusion model regresses the score function directly. In practice, diffusion models often target a quantity related to the score, such as the clean data, or the Gaussian noise added to the clean data, from which the score is derived. The choice of parameterization can significantly impact the quality of the learned model (DDPM, Ho et al. 2020), since the magnitude of the score varies widely over time. Moreover, the ODE in eq. 42 has semi-linear structure (the sum of a linear and non-linear term). Existing work (DDIM, Song et al. 2020, DPM-solver, Lu et al. 2022) has shown that using black-box solvers which don't account for this structure (i.e. solving the linear part exactly) can lead to unnecessary errors in the solution, again impacting performance.
If it is an advantage of flow matching that the parameterization of the regression target and the choice of ODE solver do not need such careful consideration, this should be stated explicitly and taken into account for direct comparison, rather than comparing to a diffusion model implementation which is not as performant as it might be. Nits:
4: 'There is potentially an infinite number of vector fields that generate any particular probability path, but the vast majority of these' -- what is meant by vast majority? Quantify this formally.
4.1: 'we can set them to any reasonable function' -- what is a 'reasonable function'? Again be precise.
6.1: 'we find that we can achieve very reasonable performance' -- what is 'very reasonable performance'?
6.1: The BigGAN citation is for Lučić et al. 2019 but the canonical citation is Brock et al. 2018
7: 'FM can be generalized to manifold data' -- perhaps, but you can't claim this without demonstrating it

Review Point: 4: 'There is potentially an infinite number of vector fields that generate any particular probability path, but the vast majority of these' -- what is meant by vast majority? Quantify this formally. 4.1: 'we can set them to any reasonable function' -- what is a 'reasonable function'? Again be precise. 6.1: 'we find that we can achieve very reasonable performance' -- what is 'very reasonable performance'? 6.1: The BigGAN citation is for Lučić et al. 2019 but the canonical citation is Brock et al. 2018 7: 'FM can be generalized to manifold data' -- perhaps, but you can't claim this without demonstrating it
==================================================

Focused review:

weaknesses of the method. Clarity: The paper has been written in a manner that is straightforward to read and follow. Significance: There are two factors which dent the significance of this work. 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too. 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain. 

Review Point: 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too.
Review Point: 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.
==================================================

Focused review:

While the described approach is simple and very generally applicable, there are some major issues with the evaluation that need to be addressed. If 1. and 2. are addressed I would be willing to update my scores. 1. The BLEU evaluation is not clearly described for the WMT and IWSLT experiments. Given the major variations observed in BLEU scores due to differences in post-processing or the BLEU evaluation script used, it's hard to fairly compare against previous work without clearly describing the post-processing, tokenization and BLEU evaluation tool used for these experiments. [1] 2. When training with synthetic data, BLEU scores are an unreliable measure of translation quality due to the translationese effects present in several standard test sets [2,3 and several follow-up works]. Since the proposed method relies heavily on using backward and forward translated data, these effects are bound to affect the observed BLEU improvements. A careful study of the effect of this approach on the forward and backward translated subsets of the evaluation sets and ideally evaluating translation quality with human raters on the two subsets should address this concern. Other minor concerns: 3. Table 1. suggests that the amount of "actual" training data required for Back-translation and NMT+BERT is much higher. This seems misleading given that those approaches rely on unlabeled data which is readily available. 4. Missing reference and discussion on self-training. [4] 5. Additional analysis on out-of-domain generalization (or out-of-domain test sets) would be nice to have. Is augmentation with smooth synthetic data affecting the generalization ability of the model beyond the test sets being used? References: [1] A Call for Clarity in Reporting BLEU Scores, Post et al. [2] APE at Scale and its Implications on MT Evaluation Biases, Freitag et al. [3] On The Evaluation of Machine Translation Systems Trained With Back-Translation, Edunov et al. [4] REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION, He et al.

Review Point: 1. The BLEU evaluation is not clearly described for the WMT and IWSLT experiments. Given the major variations observed in BLEU scores due to differences in post-processing or the BLEU evaluation script used, it's hard to fairly compare against previous work without clearly describing the post-processing, tokenization and BLEU evaluation tool used for these experiments. [1] 2. When training with synthetic data, BLEU scores are an unreliable measure of translation quality due to the translationese effects present in several standard test sets [2,3 and several follow-up works]. Since the proposed method relies heavily on using backward and forward translated data, these effects are bound to affect the observed BLEU improvements. A careful study of the effect of this approach on the forward and backward translated subsets of the evaluation sets and ideally evaluating translation quality with human raters on the two subsets should address this concern. Other minor concerns:
Review Point: 3. Table 1. suggests that the amount of "actual" training data required for Back-translation and NMT+BERT is much higher. This seems misleading given that those approaches rely on unlabeled data which is readily available.
Review Point: 4. Missing reference and discussion on self-training. [4] 5. Additional analysis on out-of-domain generalization (or out-of-domain test sets) would be nice to have. Is augmentation with smooth synthetic data affecting the generalization ability of the model beyond the test sets being used? References: [1] A Call for Clarity in Reporting BLEU Scores, Post et al. [2] APE at Scale and its Implications on MT Evaluation Biases, Freitag et al. [3] On The Evaluation of Machine Translation Systems Trained With Back-Translation, Edunov et al. [4] REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION, He et al.
==================================================

Focused review:

1. The paper overall lacks clarity, in particular the figures of computational graphs (Fig. 1, 2, 4) are not well explained and lack meaningful captions. 2. The demonstrated advantages are mainly relative to either purely activation or purely timing-based rules, but do not compare to state-of-the-art methods. The accuracies in MNIST and N-MNIST are well below those reached in other papers. The paper admits this and highlights the efficient use of very few spikes. While this is an advantage, I would like to see a recommendation or even better evidence that the accuracy gap can be closed, e.g. with larger networks. 3. Broader impact is not addressed.

Review Point: 1. The paper overall lacks clarity, in particular the figures of computational graphs (Fig. 1, 2, 4) are not well explained and lack meaningful captions.
Review Point: 2. The demonstrated advantages are mainly relative to either purely activation or purely timing-based rules, but do not compare to state-of-the-art methods. The accuracies in MNIST and N-MNIST are well below those reached in other papers. The paper admits this and highlights the efficient use of very few spikes. While this is an advantage, I would like to see a recommendation or even better evidence that the accuracy gap can be closed, e.g. with larger networks.
==================================================

Focused review:

Weakness:
1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
4.(Minor) Line 121, minimized -> minimize.
[1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.

Review Point: 1.The technique contribution is limited. This paper mainly uses two existing techniques, DP-SGD and gradient matching.
Review Point: 2.This work only uses one algorithm from data condensation, i.e., gradient matching. It would be better if the authors can try more algorithms so the community can have a better understanding about data condensation for differentially private data generation. For example, distribution matching [1] that minimizes the distance between the averaged feature of real data and the averaged feature of synthetic data, which is also easy to implement with DP.
Review Point: 3.In Section 6 you show the generator from a previous work can improve the visual quality of your algorithm. How does the visual quality of your algorithm compare with the visual quality of data directly generated from that generator?
Review Point: 4.(Minor) Line 121, minimized -> minimize. [1]: DATASET CONDENSATION WITH DISTRIBUTION MATCHING, https://arxiv.org/pdf/2110.04181v1.pdf.
==================================================

Focused review:

- The method is evaluated only on a single dataset, and therefore it is difficult to say when and how it breaks as compared to other methods. - The chosen architecture is quite complicated (local features from a ResNet, global ones from a UNet, differentiable renderer, UNet for a refinement network), and there are no ablation studies that would examine the significance of different components. - The authors claim that the method is a) unsupervised and b) that it uses 50x less training data than baselines. Both are incorrect, and here is why. a) The method is trained by using two input images taken from known camera poses. It is "unsupervised", because it does not use a "target" image and an associated camera pose to compute its loss, which is in contrast to baselines, which do. However, given that this method uses two images with poses, it might as well use a supervised loss where representation is extracted from a single image, and the second image is used as the target one. This would be enough to train GQN, say. In fact, the cycle-consistency losses require bi-directional transformation between the two images, which requires the known poses, and is very similar to a directly-supervised loss. -- Additionally, using a pre-trained segmentation network, while being clever, is like cheating in this case. Such networks are trained with supervised data, and hence it is incorrect to claim that a method based on a segmentation network is unsupervised. b) The 50x less assumption is based on 108 images per object in the dataset, while this method only uses 2 images per object. However, the method still uses all 108 images in the training set, and it is trivial to show that with stochastic minibatch training this is the same as taking a monte-carlo approximation of using all 108 images at once. The authors do not show what happens when they actually train on 50x less data, which makes this claim incorrect.

Review Point: - The method is evaluated only on a single dataset, and therefore it is difficult to say when and how it breaks as compared to other methods.
Review Point: - The chosen architecture is quite complicated (local features from a ResNet, global ones from a UNet, differentiable renderer, UNet for a refinement network), and there are no ablation studies that would examine the significance of different components.
Review Point: - The authors claim that the method is a) unsupervised and b) that it uses 50x less training data than baselines. Both are incorrect, and here is why. a) The method is trained by using two input images taken from known camera poses. It is "unsupervised", because it does not use a "target" image and an associated camera pose to compute its loss, which is in contrast to baselines, which do. However, given that this method uses two images with poses, it might as well use a supervised loss where representation is extracted from a single image, and the second image is used as the target one. This would be enough to train GQN, say. In fact, the cycle-consistency losses require bi-directional transformation between the two images, which requires the known poses, and is very similar to a directly-supervised loss. -- Additionally, using a pre-trained segmentation network, while being clever, is like cheating in this case. Such networks are trained with supervised data, and hence it is incorrect to claim that a method based on a segmentation network is unsupervised. b) The 50x less assumption is based on 108 images per object in the dataset, while this method only uses 2 images per object. However, the method still uses all 108 images in the training set, and it is trivial to show that with stochastic minibatch training this is the same as taking a monte-carlo approximation of using all 108 images at once. The authors do not show what happens when they actually train on 50x less data, which makes this claim incorrect.
==================================================

Focused review:

Weaknesses ====== The authors are missing some recent related works concerning bilinear saddle point games. In particular, Sherman [3] has established rates for bilinear saddle point games that improve upon the extragradient methods of Nemirovski [1] and Nesterov [2], by combining a special type of regularizer with the dual extrapolation method in [2]. Concerning dependence on the ârangeâ of the regularizers, it would be recommended that the authors provide additional discussion comparing these methods (and perhaps mention subsequent work by Sidford and Tian [4], which focuses on the special case of l_infty regression). Along the same lines of the ârangeâ considerations, and though I may be mistaken, is there a missing diameter/distance term for the runtime of the extragradient methods in eq.(1)? While I know it is the case that the log(m) and log(n) factors are being hidden in \tilde{O} (which is fine for l_1 - l_1 games), for l_1 - l_2 games, shouldnât it be necessary to account for the distance from a center point (w.r.t. the distance generating function), which may introduce an extra poly(m) term? At the very least, it would be helpful to clarify the dependence on the distance generating function(s) (including for mirror-prox), and their relation to the various min/max domains. ============ I am generally inclined to accept this paper, as the work provides an interesting insight into faster convergence rates for matrix games (in certain regimes).  [1] Nemirovski, A. "Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems."Â SIAM Journal on OptimizationÂ 15, no. 1 (2004): 229-251. [2] Nesterov, Y. "Dual extrapolation and its applications to solving variational inequalities and related problems."Â Mathematical ProgrammingÂ 109, no. 2-3 (2007): 319-344. [3] Sherman, J. "Area-convexity, lâ regularization, and undirected multicommodity flow." InÂ Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 452-460. ACM, 2017. [4] Sidford, A., and Tian, K. "Coordinate methods for accelerating ââ regression and faster approximate maximum flow." InÂ 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp. 922-933. IEEE, 2018.  ============================ After reading the authors' response, I appreciate the clarification and acknowledgement of the additional related work, and I have adjusted my score accordingly. ============================

Review Point: 1 (2004): 229-251. [2] Nesterov, Y. "Dual extrapolation and its applications to solving variational inequalities and related problems."Â Mathematical ProgrammingÂ 109, no. 2-3 (2007): 319-344. [3] Sherman, J. "Area-convexity, lâ regularization, and undirected multicommodity flow." InÂ Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 452-460. ACM, 2017. [4] Sidford, A., and Tian, K. "Coordinate methods for accelerating ââ regression and faster approximate maximum flow." InÂ 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp. 922-933. IEEE, 2018. ============================ After reading the authors' response, I appreciate the clarification and acknowledgement of the additional related work, and I have adjusted my score accordingly. ============================
==================================================

Focused review:

Weakness:
The necessity and complexity of using PLMs are not clear, as 1) the total amount of data generated is small, in the range of hundreds. 2) significant human review is still needed up and down the pipeline.
The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions).
Detailed suggestions:
Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics?
Section 3.2 paragraph 2: better explain what is "naturally occuring tables and columns".
Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting?
Section 4. Upper-case of common model names, "Bert-large" -> "BERT-large" etc.

Review Point: 2) significant human review is still needed up and down the pipeline. The size of DR SPIDER (The Reviewer's impression is 1k) is much smaller than SPIDER (10K questions). Detailed suggestions: Section 2 paragraph 1, last sentence: it's better to explain why we need semantic-changing perturbations, e.g. to be used as negative examples? or to test the model's ability to distinguish between closely related but different semantics? Section 3.2 paragraph 2: better explain what is "naturally occuring tables and columns". Last paragraph in Section 3.2: Why split the filtered data into chunks, while ensuring each chuck have one annotators, why not just annotate all without chuck-splitting? Section 4. Upper-case of common model names, "Bert-large" -> "BERT-large" etc.
==================================================

Focused review:

Negatives: 1) Why not posterior over model parameters: It’s not clear to me what is the advantage of this framework over standard variational continual learning type approaches (https://arxiv.org/abs/1710.10628, https://openreview.net/pdf?id=SJxSOJStPr, etc). In this work and in the VCL type approaches the objective is to model the distribution over network parameters. Could the authors point out why using model parameters as training data for VAE (like they did) is better than standard VAE training in a continual setting? It seems like a lot of machinery has been used in this work without properly grounding the study in the literature. The best baselines to study this work would have been VCL and likes. There are no comparisons with them in the experiments section. 2) Task-specific learned priors: Why do the authors choose these? What would happen if you take a standard normal prior or any other sensible prior and finetune the decoder on the exemplars? I can’t seem to find the experiments where this ablation has been done, which, frankly, makes the choice rather ad-hoc and unnecessarily cumbersome. You would want to experimentally show that this choice makes sense. 3) Experiments: There are a few issues with experiments. 3.1) For SplitCIFAR100 and miniImageNet, 10 classes per task would correspond to the 5000 samples per class and not the 2500 samples. The authors of the cited works used 5 samples per task, making 2500 samples per class. 3.2) Network architectures not being the same: It’s the standard practice in continual learning to make the network architectures the same size. More overly parameterized architectures are more prone to prune forgetting. The use of bigger architectures in the baselines may already hurt their performance. Please make a fair comparison using an equal number of parameters. 3.3) Baselines in the task-aware setting: Some of the baselines (GEM etc) are known to work well in the task-aware setting. The authors didn’t report the numbers of these baselines in that setting in the main paper. Please report that. Also, there is no information on the amount of episodic memory used for these baselines. Please provide that information. 3.4) As pointed out earlier, the best baselines to compare this work against are the ones based on Bayesian continual learning (VCL, etc). 4) Efficiency: The authors claim that at inference time their method works in real-time which seems strange given the number of steps they perform at inference time (Alg 3). Could you please provide the timing comparison of train/ test times with the baselines?

Review Point: 2) Task-specific learned priors: Why do the authors choose these? What would happen if you take a standard normal prior or any other sensible prior and finetune the decoder on the exemplars? I can’t seem to find the experiments where this ablation has been done, which, frankly, makes the choice rather ad-hoc and unnecessarily cumbersome. You would want to experimentally show that this choice makes sense.
Review Point: 3) Experiments: There are a few issues with experiments. 3.1) For SplitCIFAR100 and miniImageNet, 10 classes per task would correspond to the 5000 samples per class and not the 2500 samples. The authors of the cited works used 5 samples per task, making 2500 samples per class. 3.2) Network architectures not being the same: It’s the standard practice in continual learning to make the network architectures the same size. More overly parameterized architectures are more prone to prune forgetting. The use of bigger architectures in the baselines may already hurt their performance. Please make a fair comparison using an equal number of parameters. 3.3) Baselines in the task-aware setting: Some of the baselines (GEM etc) are known to work well in the task-aware setting. The authors didn’t report the numbers of these baselines in that setting in the main paper. Please report that. Also, there is no information on the amount of episodic memory used for these baselines. Please provide that information. 3.4) As pointed out earlier, the best baselines to compare this work against are the ones based on Bayesian continual learning (VCL, etc). 4) Efficiency: The authors claim that at inference time their method works in real-time which seems strange given the number of steps they perform at inference time (Alg 3). Could you please provide the timing comparison of train/ test times with the baselines?
==================================================

Focused review:

1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers. 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment. 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully. 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning). 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006. 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison. 7. Line 253: how was the data divided into 30 batches? 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem? 9. Figure 5-right: annotations on the colours used are missing. 10. Minor issues. Typo. Line 179: unchaing

Review Point: 1. While the presented model can be posed as a transfer learning problem. This paper is more about concept drift. Therefore, the title Transfer learning via l1 Regularization is a bit too broad and can be misleading for some readers.
Review Point: 2. In the experiments on concept drift and "transfer learning", only Lasso is the only method studied and compared. However, Lasso is not considered as a state-of-the-art (SOTA) for concept drift and transfer learning. Lasso is designed for neither of these problem. On the other hand, there are many other methods for concept drift and transfer learning, with some discussed in the Related Work section but none is compared against in the experiment.
Review Point: 3. There are many existing works on concept drift (e.g. twitter activities, anomaly detection), as the authors have cited. However, this paper studies only synthetic concept drift problems. It is not clear whether the proposed solution can deal with concept drift in real data successfully.
Review Point: 4. Similar to the above, there are many transfer learning benchmarks and methods. However, this paper studies only a synthetic example without comparing to any transfer learning methods (as said above, Lasso is not designed for transfer learning).
Review Point: 5. The end of Sec. 4.2 states that Transfer Lasso showed the best accuracy in feature screening. However, previous works on Lasso screening are not cited or compared, e.g. Ren et al. "Safe feature screening for generalized LASSO." TPAMI 40.12 (2017): 2992-3006.
Review Point: 6. Section 4.3 follows the experiments in [17]. However, the presented results did not include [17] (and related works on the same data) in comparison.
Review Point: 7. Line 253: how was the data divided into 30 batches?
Review Point: 8. Line 258: What is the cause of such computational instability for binary features? What are the ways to mitigate this problem?
Review Point: 9. Figure 5-right: annotations on the colours used are missing.
==================================================

Focused review:

Weaknesses: I think it may be better to focus on the details of only one situation, and give more experimental results.
A general framework is proposed in this paper, covering cover learning with noisy labels, partial label learning, and semi-supervised learning. I think it may be better to focus on the details of one situation.
As to the supervision and counter-supervision signals, whether additional artificial work is needed or not? 3.Only two data sets (CIFAR-10 and CIFAR-100) are used and the experiment may be inadequate.

Review Point: 3.Only two data sets (CIFAR-10 and CIFAR-100) are used and the experiment may be inadequate.
==================================================

Focused review:

- Contributions that are claimed are somewhat weak. In fact, I think that main contribution is in the gradient analysis (as I mentioned in Strengths) and all of the three claimed contributions can be bundled into one "incremental improvements of previous works". Namely: C1: L2-regularization is interesting, but ablation study shows it has the smallest effect on the performance; C2: hybrid similarity measure is a simple combination of two established similarity measures, and it additionally adds another hyper-parameter (alpha) that seems to be very sensitive to setup (Fig5(a) shows that setting lower alpha reduces performance by 1mAP, and setting higher reduces by 0.5 mAP); C3: novel architecture is actually almost identical architecture as [21,22] with addition of FRN block from [36] (ablation study shows this gives the most increase to performance), so it more of a practical combination of previous work than actual contribution. Minor: - Color code Tab 2 with 1st, 2nd and 3rd best result for each column, to be easier for reader to follow, it is a pretty big and unreadable table

Review Point: - Contributions that are claimed are somewhat weak. In fact, I think that main contribution is in the gradient analysis (as I mentioned in Strengths) and all of the three claimed contributions can be bundled into one "incremental improvements of previous works". Namely:
Review Point: - Color code Tab 2 with 1st, 2nd and 3rd best result for each column, to be easier for reader to follow, it is a pretty big and unreadable table
==================================================

Focused review:

1. In the considered three datasets, DRO does not work well, even worse than the equal-weighting baseline. What about for other datasets (if any) in which the benefits of DRO are apparent? It may be the case in which there are very few noisy outliers. Even a synthetic dataset might be considered if available. If so, this needs to be extensively discussed with enough experimental results and potential analysis. 2. As shown in Table 1 (COMPAS), the proposed algorithm exhibits no or less gain, compared to the baseline, for scenarios in which group information is not reflected in (x,y) samples, as confirmed in Table 3. This reviewer wonders if there is a unified approach that performs best for all possible scenarios. 3. The employed reweighting approach is standard, while it offers great performance in the considered problem. 4. As demonstrated in Fig. 3(b), the proposed approach (together with DRO) is vulnerable to label bias. Wonder if it can be adopted to equip the robustness aspect against such poisoning.

Review Point: 1. In the considered three datasets, DRO does not work well, even worse than the equal-weighting baseline. What about for other datasets (if any) in which the benefits of DRO are apparent? It may be the case in which there are very few noisy outliers. Even a synthetic dataset might be considered if available. If so, this needs to be extensively discussed with enough experimental results and potential analysis.
Review Point: 2. As shown in Table 1 (COMPAS), the proposed algorithm exhibits no or less gain, compared to the baseline, for scenarios in which group information is not reflected in (x,y) samples, as confirmed in Table 3. This reviewer wonders if there is a unified approach that performs best for all possible scenarios.
Review Point: 3. The employed reweighting approach is standard, while it offers great performance in the considered problem.
Review Point: 4. As demonstrated in Fig. 3(b), the proposed approach (together with DRO) is vulnerable to label bias. Wonder if it can be adopted to equip the robustness aspect against such poisoning.
==================================================

Focused review:

Weaknesses
(-) This work relies on the assumption that “the exact distribution of covertext is assumed to be known”. However, in practice, this might not be the case, since the model of language might not be perfectly known. Hence, for most practical scenarios this approach might not provide perfectly secure steganography.
(-) While the results in Figure 2 are close to zero, they are not exactly zero. Does this mean that perfect security is not obtained as claimed?
(-) The authors evaluated the security of their approach empirically, however, it is not clear if this method can withstand modern steganalysis methods [A, B, C, D], or human evaluation.
[A] Modern Text Hiding, Text Steganalysis, and Applications: A Comparative Analysis; Entropy. 2019
[B] Linguistic Steganalysis With Graph Neural Networks; IEEE Signal Processing Letters. 2021
[C] A fast and efficient text steganalysis method; IEEE Signal Processing Letters. 2019
[D] Exploiting language model for efficient linguistic steganalysis; ICASSP 2022

Review Point: 2019 [B] Linguistic Steganalysis With Graph Neural Networks; IEEE Signal Processing Letters.
Review Point: 2021 [C] A fast and efficient text steganalysis method; IEEE Signal Processing Letters.
Review Point: 2019 [D] Exploiting language model for efficient linguistic steganalysis; ICASSP 2022
==================================================

Focused review:

1. My main concern would be the practical usage of the proposed approach. Despite the fact that the theory looks very nice, the benefit of using the multilevel optimization scheme remains somewhat questionable to me. It would be really interesting to see some numerical examples. Probably, this work would be much more suitable for optimization venue, than for NeurIPS conference. 2. Do I understand right, that for mu-strongly convex functions with L-Lipschitz continuous gradients, the constant of Hessian stability 'c' is equal to 'L / mu'? Therefore, for this problem class, Theorem 8 (and the corresponding Algrorithm 7) does not improve upon the classical Gradient Method (and thus loose to the basic Fast Gradient Method). Overall, this is not clear, do we obtain any gain from this framework for solving some optimization problems from the standard problem classes.

Review Point: 1. My main concern would be the practical usage of the proposed approach. Despite the fact that the theory looks very nice, the benefit of using the multilevel optimization scheme remains somewhat questionable to me. It would be really interesting to see some numerical examples. Probably, this work would be much more suitable for optimization venue, than for NeurIPS conference.
Review Point: 2. Do I understand right, that for mu-strongly convex functions with L-Lipschitz continuous gradients, the constant of Hessian stability 'c' is equal to 'L / mu'? Therefore, for this problem class, Theorem 8 (and the corresponding Algrorithm 7) does not improve upon the classical Gradient Method (and thus loose to the basic Fast Gradient Method). Overall, this is not clear, do we obtain any gain from this framework for solving some optimization problems from the standard problem classes.
==================================================

Focused review:

1. Some claims in the paper lack enough groundings. For instance, in lines 246-249, "This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet." This claim will be justified if the authors can provide the specific bias scores and numbers of examples of each bias type, but I didn't find the corresponding part for analyzing this. Also, this paper mentions several times the intuition "occupations and not actions associated with those occupations are related to gender, hence can encode social biases" (lines 595-597). However, I don't really agree. Take "engineer" as an instance, in the Merriam-webster dictionary, the first meaning of "engineer" as a verb (https://www.merriam-webster.com/dictionary/engineer#:~:text=engineered%3B%20engineering%3B%20engineers,craft%20engineer%20a%20business%20deal) is "to lay out, construct, or manage as an engineer". I think it is very much biased towards the male gender as well, according to social conventions. 
2. Some analyses can be more detailed. For example, in "language/nationality", the data includes Japanese, Chinese, English, Arabic, German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them. 
3. The definition of "bias" is debatable. In SSSB, a language that is "difficult to learn/understand/write" is considered to be stereotypical, and "easy to learn/understand/write" is anti-stereotypical. In daily conversations, I think it is not widely considered as "biased". Also, I don't really understand the meaning of "<xxx language is hash>". Do you mean "harsh" by "hash"? If so, I think the conclusions derived from this dataset are less trustworthy. 
4. Writing can be improved. For example, even though I read very carefully, I am not sure I fully follow the method in lines 203-210. It will be nice if you can provide some examples for s(_i) and a(_j) 5. A question: why would you use Equation 7 to derive word embeddings? From your results, I assume the sense embeddings are not normalized. This will bring an issue: the embedding will be dominated by the sense with a larger length. To make the experiments more rigorous, I think it would be nice to also use pre-trained static word embeddings (e.g. skip-gram) and normalize the embedding. 
1. I think it would be more clear if you can introduce sense embeddings a bit before introducing the bias measuring procedures. 
2. A number of typos exist. Mostly, it doesn't influence the reading. However, sometimes it affects my understanding of the paper (e.g., again "occupations and not actions associated with those occupations are related to gender" (lines 595-596, and -> but, if I understand it correctly)). Another round of proofreading may be needed. 

Review Point: 1. Some claims in the paper lack enough groundings. For instance, in lines 246-249, "This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet." This claim will be justified if the authors can provide the specific bias scores and numbers of examples of each bias type, but I didn't find the corresponding part for analyzing this. Also, this paper mentions several times the intuition "occupations and not actions associated with those occupations are related to gender, hence can encode social biases" (lines 595-597). However, I don't really agree. Take "engineer" as an instance, in the Merriam-webster dictionary, the first meaning of "engineer" as a verb (https://www.merriam-webster.com/dictionary/engineer#:~:text=engineered%3B%20engineering%3B%20engineers,craft%20engineer%20a%20business%20deal) is "to lay out, construct, or manage as an engineer". I think it is very much biased towards the male gender as well, according to social conventions.
Review Point: 2. Some analyses can be more detailed. For example, in "language/nationality", the data includes Japanese, Chinese, English, Arabic, German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them.
Review Point: 3. The definition of "bias" is debatable. In SSSB, a language that is "difficult to learn/understand/write" is considered to be stereotypical, and "easy to learn/understand/write" is anti-stereotypical. In daily conversations, I think it is not widely considered as "biased". Also, I don't really understand the meaning of "<xxx language is hash>". Do you mean "harsh" by "hash"? If so, I think the conclusions derived from this dataset are less trustworthy.
Review Point: 4. Writing can be improved. For example, even though I read very carefully, I am not sure I fully follow the method in lines 203-210. It will be nice if you can provide some examples for s(_i) and a(_j) 5. A question: why would you use Equation 7 to derive word embeddings? From your results, I assume the sense embeddings are not normalized. This will bring an issue: the embedding will be dominated by the sense with a larger length. To make the experiments more rigorous, I think it would be nice to also use pre-trained static word embeddings (e.g. skip-gram) and normalize the embedding.
Review Point: 1. I think it would be more clear if you can introduce sense embeddings a bit before introducing the bias measuring procedures.
==================================================

Focused review:

Weaknesses: 1. While the paper has good results, the novelty seems to be limited. It’s not clear why SVRG can boost transferability. The authors claim that previously computed gradient information is not utilized in existing methods, but in fact, prior works usually use a momentum factor to leverage historical gradients [1]. 2. Although the authors conduct an experiment to show the improvements are not caused by more gradient calculations, it’s better to conduct experiments under stronger attacks, especially random-based methods like SI-TI-DIM. MI-FGSM is a deterministic algorithm, so it is easier to converge. However, some random operations are introduced in SVR, which may play a role like data augmentation. Thus, from Figure 4, we can see that the SVR-Ens methods are worse than Ens under a few gradient calculations but better with more gradient calculations. The experiments will be more solid if the authors discuss other random-based attack methods and increase iterations to make all methods (SVR-Ens) converge like MI-FGSM. 3. Besides SI-TI-DIM, it's necessary to compare with Variance Tuning Gradient-based Attack methods [2]. Both methods aim to reduce the gradient variance, and [2] discussed the difference between their methods and your SVR methods. Please see details in Section 3.1 of [2]. It's better to conduct experiments to verify whether SVR-Ens outperform [2] under the same gradient calculations times or compare the performance when both methods converge. 4. [1] proposed to ensemble on logits to achieve higher transferability. This scheme can be naturally integrated with other methods, but I think it is incompatible with SVR-Ens. Is it a limitation of SVR-Ens? [1] Dong Y, Liao F, Pang T, et al. Boosting adversarial attacks with momentum[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 9185-9193. [2] Wang X, He K. Enhancing the Transferability of Adversarial Attacks through Variance Tuning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1924-1933.

Review Point: 1. While the paper has good results, the novelty seems to be limited. It’s not clear why SVRG can boost transferability. The authors claim that previously computed gradient information is not utilized in existing methods, but in fact, prior works usually use a momentum factor to leverage historical gradients [1].
Review Point: 2. Although the authors conduct an experiment to show the improvements are not caused by more gradient calculations, it’s better to conduct experiments under stronger attacks, especially random-based methods like SI-TI-DIM. MI-FGSM is a deterministic algorithm, so it is easier to converge. However, some random operations are introduced in SVR, which may play a role like data augmentation. Thus, from Figure 4, we can see that the SVR-Ens methods are worse than Ens under a few gradient calculations but better with more gradient calculations. The experiments will be more solid if the authors discuss other random-based attack methods and increase iterations to make all methods (SVR-Ens) converge like MI-FGSM.
Review Point: 3. Besides SI-TI-DIM, it's necessary to compare with Variance Tuning Gradient-based Attack methods [2]. Both methods aim to reduce the gradient variance, and [2] discussed the difference between their methods and your SVR methods. Please see details in Section 3.1 of [2]. It's better to conduct experiments to verify whether SVR-Ens outperform [2] under the same gradient calculations times or compare the performance when both methods converge.
Review Point: 4. [1] proposed to ensemble on logits to achieve higher transferability. This scheme can be naturally integrated with other methods, but I think it is incompatible with SVR-Ens. Is it a limitation of SVR-Ens? [1] Dong Y, Liao F, Pang T, et al. Boosting adversarial attacks with momentum[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 9185-9193. [2] Wang X, He K. Enhancing the Transferability of Adversarial Attacks through Variance Tuning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1924-1933.
==================================================

Focused review:

1. There are many technical assumptions and it is hard to identify which play the essential role in the result and which are just for technical issues, and thus possible to relax in the future. 2. The loss functions for the pseudo-label self-training and entropy minimization are not standard. I suggest analyzing log-loss for the pseudo-label self-training and making the equivalence between l_exp and l_ent formal. 3. The authors need to explain why the additional source loss on labeled data in self-training is required in the experiment on CelebA. === Post rebuttal === The author's response answers my questions. My rate remains unchanged.

Review Point: 1. There are many technical assumptions and it is hard to identify which play the essential role in the result and which are just for technical issues, and thus possible to relax in the future.
Review Point: 2. The loss functions for the pseudo-label self-training and entropy minimization are not standard. I suggest analyzing log-loss for the pseudo-label self-training and making the equivalence between l_exp and l_ent formal.
Review Point: 3. The authors need to explain why the additional source loss on labeled data in self-training is required in the experiment on CelebA. === Post rebuttal === The author's response answers my questions. My rate remains unchanged.
==================================================

Focused review:

the paper presents minor weaknesses: 1) it is too rich in content probably given the page limitations: there is a lot (too much) material that is just relegated to the appendix...an appendix of 90 pages is excessive..maybe I would suggest to submit the paper to a journal rather than a conference in order to allow the reviewers to have the time to go through the appendix material and to include more of the material in the actual paper. 2) I find the title not so in line with the main content of the paper. The title should probably be focused on the uniform analysis of distributed SGD-based methods which is the main topic of the paper. 3) the authors are not discussing the following methods which seem to be highly related to their problem framework: https://arxiv.org/abs/1611.02189 https://www.research-collection.ethz.ch/handle/20.500.11850/183454 http://proceedings.mlr.press/v80/duenner18a.html maybe they should point to those and cite those papers. 4) it is a bit disappointing that for the experiments the distributed setting is only 'simulated' on a single machine with a for loop... 5) improve the readability of the plots by increasing their size (they are currently a bit too small) ========== After Authors' Rebuttal ========== I would like to thank the authors for providing their comments/reply to each potential weakness point that I had underlined in this section. Regarding the first point, although there are no-strict limits on the appendix length enforced, I still find the current appendix's length (and content) to be excessive wrt the paper's length (and content) and still think that the content should have been re-organized between paper and appendix and submitted to a journal rather than a conference. At the same time, I agree with the authors regarding the fact that details on derivations for special cases are important and can be exploited in the future by other researchers (indeed I have never said to remove those details/derivations but just pointed out that most of the content is unfortunately and in my opinion wrongly relegated to the appendix....while the appendix's role in my opinion should be different). Regarding the second point, I suggest to switch back to the title of the 'working version' or find a new one that incorporates both these aspects without sacrificing the unified theory aspect. In the light of the authors's reply and taking into account also some of the other reviewers'votes, I confirm my (positive) vote for this paper.

Review Point: 1) it is too rich in content probably given the page limitations: there is a lot (too much) material that is just relegated to the appendix...an appendix of 90 pages is excessive..maybe I would suggest to submit the paper to a journal rather than a conference in order to allow the reviewers to have the time to go through the appendix material and to include more of the material in the actual paper.
Review Point: 2) I find the title not so in line with the main content of the paper. The title should probably be focused on the uniform analysis of distributed SGD-based methods which is the main topic of the paper.
Review Point: 3) the authors are not discussing the following methods which seem to be highly related to their problem framework: https://arxiv.org/abs/1611.02189 https://www.research-collection.ethz.ch/handle/20.500.11850/183454 http://proceedings.mlr.press/v80/duenner18a.html maybe they should point to those and cite those papers.
Review Point: 4) it is a bit disappointing that for the experiments the distributed setting is only 'simulated' on a single machine with a for loop...
Review Point: 5) improve the readability of the plots by increasing their size (they are currently a bit too small) ========== After Authors' Rebuttal ========== I would like to thank the authors for providing their comments/reply to each potential weakness point that I had underlined in this section. Regarding the first point, although there are no-strict limits on the appendix length enforced, I still find the current appendix's length (and content) to be excessive wrt the paper's length (and content) and still think that the content should have been re-organized between paper and appendix and submitted to a journal rather than a conference. At the same time, I agree with the authors regarding the fact that details on derivations for special cases are important and can be exploited in the future by other researchers (indeed I have never said to remove those details/derivations but just pointed out that most of the content is unfortunately and in my opinion wrongly relegated to the appendix....while the appendix's role in my opinion should be different). Regarding the second point, I suggest to switch back to the title of the 'working version' or find a new one that incorporates both these aspects without sacrificing the unified theory aspect. In the light of the authors's reply and taking into account also some of the other reviewers'votes, I confirm my (positive) vote for this paper.
==================================================

Focused review:

My main concern / question is about the interpretation of the resulting skill distributions in Section 4. Bradley-Terry models are typically parametrized in one of two different ways: 1. strictly positive parameters, such that the ratio of the parameters corresponding to two items equals the odds of one item winning over the other (let us call these parameters "skills") 2. real-valued parameters, such that the probability of one item winning over the other is equal to the logistic function of the difference in parameters (let us call these parameters "logits") In this paper, the authors work with the parametrization (1). I would argue that comparing entropies of distributions in that parametrization leads to results that are somewhat difficult to interpret. In particular, it is not clear to me what a uniform skill distribution over [eps, 1] means in terms of expected outcomes. Indeed, the comparison outcome probabilities between teams with skill eps vs. 2*eps are comparable to those between teams with skills 0.5 and 1. But a uniform distribution puts much more mass on the interval (0.5, 1) as it does on the interval (eps, 2*eps). In my opinion, Figures 1c and 1f illustrate this: "World" and "English" seem to have very different skill distributions (in terms of "fan experience"), yet result in a similar differential entropy scores. I suspect it is much easier to interpret distributions over logits. Do the authors have any comments on this? --- EDIT after response Thank you for your feedback. In terms of applications / interpretation, I am still not convinced: if there are n >> 2 teams - a uniform distribution over logits has a fairly intuitive interpretation. In expectation, prob(i wins over j) depends only on the rank difference between teams i and j - Whereas a uniform distribution over skills is hard to make sense of. In expectation, match outcomes will be much "noisier" / uncertain between teams at the top of the ranking, much less so for teams at the bottom of the ranking. As such, I believe the link between skill distribution entropy and sports intuition & fan experience could still be clarified. Nevertheless, since the rest of the results developed in the paper are rigorous, strong & interesting, I continue to be very much in favor of accepting this paper.

Review Point: 1. strictly positive parameters, such that the ratio of the parameters corresponding to two items equals the odds of one item winning over the other (let us call these parameters "skills") 2. real-valued parameters, such that the probability of one item winning over the other is equal to the logistic function of the difference in parameters (let us call these parameters "logits") In this paper, the authors work with the parametrization (1). I would argue that comparing entropies of distributions in that parametrization leads to results that are somewhat difficult to interpret. In particular, it is not clear to me what a uniform skill distribution over [eps, 1] means in terms of expected outcomes. Indeed, the comparison outcome probabilities between teams with skill eps vs. 2*eps are comparable to those between teams with skills 0.5 and 1. But a uniform distribution puts much more mass on the interval (0.5, 1) as it does on the interval (eps, 2*eps). In my opinion, Figures 1c and 1f illustrate this: "World" and "English" seem to have very different skill distributions (in terms of "fan experience"), yet result in a similar differential entropy scores. I suspect it is much easier to interpret distributions over logits. Do the authors have any comments on this? --- EDIT after response Thank you for your feedback. In terms of applications / interpretation, I am still not convinced: if there are n >> 2 teams - a uniform distribution over logits has a fairly intuitive interpretation. In expectation, prob(i wins over j) depends only on the rank difference between teams i and j - Whereas a uniform distribution over skills is hard to make sense of. In expectation, match outcomes will be much "noisier" / uncertain between teams at the top of the ranking, much less so for teams at the bottom of the ranking. As such, I believe the link between skill distribution entropy and sports intuition & fan experience could still be clarified. Nevertheless, since the rest of the results developed in the paper are rigorous, strong & interesting, I continue to be very much in favor of accepting this paper.
==================================================

Focused review:

Weaknesses ---------- The main section of the paper (section 3) seems carelessly written. Some obvious weaknesses: - Algorithm 1 seems more confusing, than clarifying: a) Shouldn't the gradient step be taken in the direction of the gradient of the loss with respect to Theta? b) There is no description of the variables, most importantly X and f. It is better for the reader to define them in the algorithm than later in the text. Otherwise, the algorithm definition seems unnecessary. - Equation (1) is very unclear:  a) Is the purpose to define a loss function or the optimization problem? It seems that it is mixing both.  b) The optimization variable x is defined to be in R^n. Probably it is meant to be in R^k?  c) The constraints notation (s.t. C(A, x)) is rather unusual.  - It is briefly mentioned that an alternating direction method is used to solve the min-min problem. Which method? - The constraints in equation (2) are identical to the ones in equation (3). They can be mentioned as such to gain space. - In section 4.1, line 194, K = 10, presumably refers to the number of atoms in the dictionary, namely it should be a small k? The same holds for section 4.4, line 285. - In section 4.1, why is the regularizer coefficient gamma set to zero? Intuitively, structured sparcity should be particularly helpful in finding keypoint correspondences. What is the effect on the solution when gamma is larger than zero?  The experimental section of the paper seems well written (with a few exceptions, see above). Nevertheless, the experiments in 4.2 and 4.3 each compare to only one existing work. In general, I can see the idea of the paper has merit, but the carelessness in the formulations in the main part and the lack of comparisons to other works make me hesitant to accept it as is at NIPS.

Review Point: ---------- The main section of the paper (section 3) seems carelessly written. Some obvious weaknesses:
Review Point: - Algorithm 1 seems more confusing, than clarifying: a) Shouldn't the gradient step be taken in the direction of the gradient of the loss with respect to Theta? b) There is no description of the variables, most importantly X and f. It is better for the reader to define them in the algorithm than later in the text. Otherwise, the algorithm definition seems unnecessary. - Equation (1) is very unclear: a) Is the purpose to define a loss function or the optimization problem? It seems that it is mixing both. b) The optimization variable x is defined to be in R^n. Probably it is meant to be in R^k? c) The constraints notation (s.t. C(A, x)) is rather unusual. - It is briefly mentioned that an alternating direction method is used to solve the min-min problem. Which method? - The constraints in equation (2) are identical to the ones in equation (3). They can be mentioned as such to gain space. - In section 4.1, line 194, K = 10, presumably refers to the number of atoms in the dictionary, namely it should be a small k? The same holds for section 4.4, line 285. - In section 4.1, why is the regularizer coefficient gamma set to zero? Intuitively, structured sparcity should be particularly helpful in finding keypoint correspondences. What is the effect on the solution when gamma is larger than zero? The experimental section of the paper seems well written (with a few exceptions, see above). Nevertheless, the experiments in 4.2 and 4.3 each compare to only one existing work. In general, I can see the idea of the paper has merit, but the carelessness in the formulations in the main part and the lack of comparisons to other works make me hesitant to accept it as is at NIPS.
==================================================

Focused review:

Weaknesses: except for the qualitative analysis, the paper may belong better to the applications area, since the models are not particularly new but the application itself is most of its novelty - General Discussion: This paper presents a "sequence-to-sequence" model with attention mechanisms and an auxiliary phonetic prediction task to tackle historical text normalization. None of the used models or techniques are new by themselves, but they seem to have never been used in this problem before, showing and improvement over the state-of-the-art. Most of the paper seem like a better fit for the applications track, except for the final analysis where the authors link attention with multi-task learning, claiming that the two produce similar effects. The hypothesis is intriguing, and it's supported with a wealth of evidence, at least for the presented task. 
I do have some questions on this analysis though: 1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two models are aligned? Is it safe to do so?
2) Section 5.2, I don't get what you mean by the errors that each of the models resolve independently of each other. This is like symmetric-difference? That is, if we combine the two models these errors are not resolved anymore?
On a different vein, 3) Why is there no comparison with Azawi's model?
======== After reading the author's response.
I'm feeling more concerned than I was before about your claims of alignment in the hidden space of the two models. If accepted, I would strongly encourage the authors to make clear in the paper the discussion you have shared with us for why you think that alignment holds in practice. 

Review Point: 1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two models are aligned? Is it safe to do so?
Review Point: 2) Section 5.2, I don't get what you mean by the errors that each of the models resolve independently of each other. This is like symmetric-difference? That is, if we combine the two models these errors are not resolved anymore? On a different vein, 3) Why is there no comparison with Azawi's model? ======== After reading the author's response. I'm feeling more concerned than I was before about your claims of alignment in the hidden space of the two models. If accepted, I would strongly encourage the authors to make clear in the paper the discussion you have shared with us for why you think that alignment holds in practice.
==================================================

Focused review:

Weaknesses: 1) One of the key components is the matching metric, namely, the Pearson correlation coefficient (PCC). However, the assumption that PCC is a more relaxed constraint compared with KL divergence because of its invariance to scale and shift is not convincing enough. The constraint strength of a loss function is defined via its gradient distribution. For example, KL divergence and MSE loss have the same optimal solution while MSE loss is stricter than KL because of stricter punishment according to its gradient distribution. From this perspective, it is necessary to provide the gradient comparison between KL and PCC. 2) The experiments are not sufficient enough. 2-1) There are limited types of teacher architectures. 2-2) Most compared methods are proposed before 2019 (see Tab. 5). 2-3) The compared methods are not sufficient in Tab. 3 and 4. 2-4) The overall performance comparisons are only conducted on the small-scale dataset (i.e., CIFAR100). Large datasets (e.g., ImageNet) should also be evaluated. 2-5) The performance improvement compared with SOTAs is marginal (see Tab. 5). Some students only have a 0.06% gain compared with CRD. 3) There are some typos and some improper presentations. The texts of the figure are too small, especially the texts in Fig.2. Some typos, such as “on each classes” in the caption of Fig. 3, should be corrected.
The authors have discussed the limitations and societal impacts of their works. The proposed method cannot fully address the binary classification tasks.

Review Point: 1) One of the key components is the matching metric, namely, the Pearson correlation coefficient (PCC). However, the assumption that PCC is a more relaxed constraint compared with KL divergence because of its invariance to scale and shift is not convincing enough. The constraint strength of a loss function is defined via its gradient distribution. For example, KL divergence and MSE loss have the same optimal solution while MSE loss is stricter than KL because of stricter punishment according to its gradient distribution. From this perspective, it is necessary to provide the gradient comparison between KL and PCC.
==================================================

Focused review:

Weaknesses: 1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: "implies physical relations": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. " Thematic proto-roles and argument selection." Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 "values" ==> "value"?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. "Distributional semantics in technicolor." Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. " Models of Semantic Representation with Visual Attributes." ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what "grounded" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms "pre-condition" and "post-condition", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or "ideal") would help.
Figure 2. I don't really see the "x is slower than y" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level). 
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 "Above definition": determiner missing Section 3 "Action verbs": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are "action frames"? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 "with... PMI": something missing (threshold?)
371 did you do this partitions randomly?
376 "rate *the* general relationship" 378 "knowledge dimension we choose": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph? 
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 "both classes of knowledge": antecedent missing.
421 "object first type" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 "also"?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term "message" and its role in the factor graph.
621 why do you need a "soft 1" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 "more skimp seed knowledge": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, "larger" is not the same as "stronger".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 "latent in verbs": why don't you mention objects here?
781 "both tasks": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details. 

Review Point: 1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
Review Point: 2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline.
Review Point: - General Discussion: The paper needs quite a bit of work before it is ready for publication.
Review Point: - Detailed comments:026 five dimensions, not six Figure 1, caption: "implies physical relations": how do you know which physical relations it implies? Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature? Dowty, David. " Thematic proto-roles and argument selection." Language (1991): 547-619.
Review Point: 135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
Review Point: 146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
Review Point: 152 it is not clear what "grounded" means at this point Section 2.1: why these dimensions, and how did you choose them?
Review Point: 177 explain terms "pre-condition" and "post-condition", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or "ideal") would help. Figure 2. I don't really see the "x is slower than y" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level). I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
Review Point: 248 "Above definition": determiner missing Section 3 "Action verbs": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are "action frames"? How do you pick them?
Review Point: 326 How do you know whether the frame is under- or over-generating? Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
Review Point: 336 "with... PMI": something missing (threshold?) 371 did you do this partitions randomly?
Review Point: 376 "rate *the* general relationship" 378 "knowledge dimension we choose": ? ( how do you choose which dimensions you will annotate for each frame?) Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph? More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
Review Point: 421 "object first type" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
Review Point: 461 "also"?471 where do you get verb-level similarities from? Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
Review Point: 598 define term "message" and its role in the factor graph.
Review Point: 621 why do you need a "soft 1" instead of a hard 1? 647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
Review Point: 659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, "larger" is not the same as "stronger".
Review Point: 681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 "latent in verbs": why don't you mention objects here?
Review Point: 781 "both tasks": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.
==================================================

Focused review:

1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure "unlabelled" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels. 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1. 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples? 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT. 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that "it would be beneficial for the LD to be trained with examples containing events". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training. 
1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043. 
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method. 
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT. 
4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance? 
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern. 

Review Point: 1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure "unlabelled" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels.
Review Point: 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1.
Review Point: 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples?
Review Point: 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT.
Review Point: 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
Review Point: 6. It is claimed in lines 128-132 that "it would be beneficial for the LD to be trained with examples containing events". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
Review Point: 7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training.
Review Point: 2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method.
Review Point: 3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT.
Review Point: 4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance?
Review Point: 5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern.
==================================================

Focused review:

1. I think the major drawback of the paper is lack of detailed discussions and comparisons with MetaPruning [15], which also focuses on the direction of “semi-sharing” for channel search. Rather than semi-orthogonal projections from a shared mete-weight, [15] proposes to share a meta-network among all channel configurations, which sounds more general and elegant to me. Also, [15] suggests it is possible to end-to-end train the meta-network with the task loss without additional optimization (like Eq 4) or learning rate tuning (Line 166). So, I am curious about the result of directly optimizing P and Q with the task loss instead of Eq 4. In addition, though in Table 2 the proposed APS-T outperforms MetaPruning, however the gap is quite minor, and the training method seems not to be aligned. I think a fair ablation study may be required to compare the two sharing strategies. 2. Though in Line 245 the authors claim that the method may benefit from larger search spaces, however, since P and Q seems independent parameters for different configurations (Line 100), I doubt about the efficiency of scaling up; however, existing counterparts like MetaPruning do not suffer from the issue.

Review Point: 1. I think the major drawback of the paper is lack of detailed discussions and comparisons with MetaPruning [15], which also focuses on the direction of “semi-sharing” for channel search. Rather than semi-orthogonal projections from a shared mete-weight, [15] proposes to share a meta-network among all channel configurations, which sounds more general and elegant to me. Also, [15] suggests it is possible to end-to-end train the meta-network with the task loss without additional optimization (like Eq 4) or learning rate tuning (Line 166). So, I am curious about the result of directly optimizing P and Q with the task loss instead of Eq 4. In addition, though in Table 2 the proposed APS-T outperforms MetaPruning, however the gap is quite minor, and the training method seems not to be aligned. I think a fair ablation study may be required to compare the two sharing strategies.
Review Point: 2. Though in Line 245 the authors claim that the method may benefit from larger search spaces, however, since P and Q seems independent parameters for different configurations (Line 100), I doubt about the efficiency of scaling up; however, existing counterparts like MetaPruning do not suffer from the issue.
==================================================

Focused review:

1) the authors do not compare with the model in [15]: "Modeling long-and short-term temporal patterns with deep neural networks.". This restricts the potential impact of the model. 2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code) 3) the paper doesn't propose a conceptual/computational novelty. it combines existing modules to achieves its results.

Review Point: 1) the authors do not compare with the model in [15]: "Modeling long-and short-term temporal patterns with deep neural networks.". This restricts the potential impact of the model.
Review Point: 2) the model has many components whose hyper parameters are not fully provided (someone may have to trace them in the source code) 3) the paper doesn't propose a conceptual/computational novelty. it combines existing modules to achieves its results.
==================================================

Focused review:

Weaknesses:
The main contribution seems to be the new transformer architecture combining context and spatial branches, resulting to a generic bloc that breaks the complexity. This contribution is mainly presented in the context of mobile segmentation application. I wonder if considering in a more generic way this contribution would implies more impact: 1) As you point with the small experiment on classification, this is very generic and can be used for classification, detection, segmentation (semantic, instances, ...); and 2) blocks are designed for mobile applications (MobileNet blocks) and could be extended to GPU systems with more classical operations without changing the O(WH) complexity. Extending experiments in this way would be interesting with may be more impact for the paper.

Review Point: 1) As you point with the small experiment on classification, this is very generic and can be used for classification, detection, segmentation (semantic, instances, ...); and 2) blocks are designed for mobile applications (MobileNet blocks) and could be extended to GPU systems with more classical operations without changing the O(WH) complexity. Extending experiments in this way would be interesting with may be more impact for the paper.
==================================================

Focused review:

Weakness ===============
A strong motivation of the practicality of radial neural networks seems to be lacking. It will be helpful to provide working examples of radial neural networks and indicate that in some important applications, radial neural networks outperform pointwise networks.
The universal approximation theory allows the network size to grow to infinity. It seems unfair to claim that not requiring bounded domain is a contribution. In early asymptotic results, e.g., Cybenko and Barron, neural networks are already shown to be dense in differential function spaces with unbounded domain. Later results established the rate of approximation requires a compact domain. As there is no rate of approximation provided, considering unbounded domain does not add much technical challenges. Moreover, the asymptotic results does not provide understanding of potential advantages of radial neural networks over pointwise neural networks in terms of function approximation.
Synthetic data is relatively toy and the comparison with ReLU network is not fully convincing. On the one hand, synthetic data are low-dimensional and the target function is simple in that the activation function is almost the form of the target function. On the other hand, Step-ReLU radial network outperforms ReLU MLP might correlate to multiple reasons: 1) different activation functions; 2) different network architectures; 3) hyper parameter tuning. By the way, is it possible to simply perform classification on MNIST using radial and pointwise neural networks?

Review Point: 1) different activation functions;2) different network architectures;3) hyper parameter tuning. By the way, is it possible to simply perform classification on MNIST using radial and pointwise neural networks?
==================================================

Focused review:

Weaknesses
- The main contribution of this paper is the "double-contrast" strategy. Compare to the traditional contrastive loss, the novelty is a bit limited. Besides, it is unclear why such a "double-contrast" is necessary, and why have to use the multiplication weight as a selection approach instead of others.
- The proposed method is motivated by the conventional CL's robustness on uncurated datasets, as claimed by the authors. But the experimental analysis does not validate this well. The "wild" uncurated data is not only data-imbalance, it is also related to the quality and noise level of the data. As a result, suggest the authors either tone down the claim or provide more convincing justifications, either empirical or theoretical.
- Although the authors demonstrate the effectiveness of the proposed method in a controlled experiment (Sec. 5.1), when compared to SOTAs (Sec. 5.2), the performance is not significant, just comparable/incremental to prior works.
- It would be better if the authors could present an analysis of the add on computational complexity / number of parameters, by using the proposed CACR strategy.
- The proposed method is also related to (though not exactly the same scenario) the hard-negative and hard-positive sampling used in video-audio representation learning (e.g. [*1, *2, *3]) and it would be better to discuss the relationships.
[*1] "Cooperative learning of audio and video models from self-supervised synchronization", NeurIPS 2018
[*2] "Self-supervised contrastive video-speech representation learning for ultrasound", MICCAI 2020
[*3] "Spoken moments: Learning joint audio-visual representations from video descriptions", CVPR 2021.

Review Point: - The main contribution of this paper is the "double-contrast" strategy. Compare to the traditional contrastive loss, the novelty is a bit limited. Besides, it is unclear why such a "double-contrast" is necessary, and why have to use the multiplication weight as a selection approach instead of others.
Review Point: - The proposed method is motivated by the conventional CL's robustness on uncurated datasets, as claimed by the authors. But the experimental analysis does not validate this well. The "wild" uncurated data is not only data-imbalance, it is also related to the quality and noise level of the data. As a result, suggest the authors either tone down the claim or provide more convincing justifications, either empirical or theoretical.
Review Point: - Although the authors demonstrate the effectiveness of the proposed method in a controlled experiment (Sec. 5.1), when compared to SOTAs (Sec. 5.2), the performance is not significant, just comparable/incremental to prior works.
Review Point: - It would be better if the authors could present an analysis of the add on computational complexity / number of parameters, by using the proposed CACR strategy.
Review Point: - The proposed method is also related to (though not exactly the same scenario) the hard-negative and hard-positive sampling used in video-audio representation learning (e.g. [*1, *2, *3]) and it would be better to discuss the relationships. [*1] "Cooperative learning of audio and video models from self-supervised synchronization", NeurIPS 2018 [*2] "Self-supervised contrastive video-speech representation learning for ultrasound", MICCAI 2020 [*3] "Spoken moments: Learning joint audio-visual representations from video descriptions", CVPR 2021.
==================================================

Focused review:

Weaknesses
W1. Clarity
The organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the “theoretical” results which are not rigorous and at times unsupported. Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing.
W2. Soundness
I have a lot of concerns and questions here as I read through Sect. 3. At a high-level, I don’t see a clear connection between “improved variance control of prediction y^ or the smoothness of loss landscape” and “zero-shot learning effectiveness.” Details below. This is in part due to poor clarity.
W3. Experiments
IMO, if the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2).
If baseline methods already adopt some of these tricks, it should be made clear and see if removing these tricks lead to inferior performance.
If baseline methods do not adopt some of these tricks, these tricks, especially class normalization, could be applied to show improved performance. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.)
This is done to some degree in the continual setting.
W4. Related work
As I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.
Detailed comments (mainly to clarify my points about weaknesses)
Statement 1
The main claim for this part is that this statement provides “a theoretical understanding of the trick” and “allows to speed up the search [of the optimal value fo \gamma].”
However, I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, “better stability” and “the training would not stale”) and the zero-shot learning accuracy for this to be the “why normalization + scaling works.” My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.
Moreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it?
Finally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?
Statement 2 and 3
Why wouldn’t the following statement in Sect. 3.3 invalidate Statement 1? “This may create an impression that it does not matter how we initialize the weights — normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit value”
It is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.
Similar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? In particular, the authors again claim that the main bottleneck in improving zero-shot learning is “variance control” (the end of Sect. 3.3).
I also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: “And these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).” H1: Would this statement still be true after we transform a_c with an MLP? H2: Why is it not “a sensible thing to do” if we just want zero mean and unit variance? H3: Why is “such an approach far from being scalable”? H4: What if these are things like word embeddings? H5: Fig. 12 and Fig. 13 are not explained. H6: Histograms in Fig. 13 look quite normal.
How useful is Statement 2? Why is the connection with Xavier initialization important?
Why is “preserving the variance between z and y~” in Statement 3 important for zero-shot learning?
Improved smoothness
The claim “improved smoothness” at the end of Sect. 3 and Appendix F is really hard to understand. F1: How do the authors define “irregular loss surface”? F2: “Santurkar et al. (2018) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model, which suggests that our class-wise standardization will provide the same impact.” This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear.
Similarly to my comments to Statement 1-3, how is improved smoothness related to zero-shot learning effectiveness?
Other more minor comments
Abstract: Are the authors the one to “generalize ZSL to a broader problem”? Please tone down the claim if not.
After Eq. (2): Why does attribute normalization look “inconsiderable” (possibly this is not the right word?) or why is it “surprising” that this is preferred in practice? Don’t most zero-shot learning methods use this (see for example Table 4 in [A])?
Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).
Under Table 1 “These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.”: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a “linear” or “multi-layer” model.
The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.
The second paragraph of Sect. 3: What exactly limits “the tools” for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].
What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected?
[A] Synthesized Classifiers for Zero-Shot Learning
[B] Zero-Shot Learning by Convex Combination of Semantic Embeddings
[C] Class-Balanced Loss Based on Effective Number of Samples

Review Point: 3. At a high-level, I don’t see a clear connection between “improved variance control of prediction y^ or the smoothness of loss landscape” and “zero-shot learning effectiveness.” Details below. This is in part due to poor clarity.
Review Point: 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization. The second paragraph of Sect.
Review Point: 3: What exactly limits “the tools” for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C]. What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected? [A] Synthesized Classifiers for Zero-Shot Learning [B] Zero-Shot Learning by Convex Combination of Semantic Embeddings [C] Class-Balanced Loss Based on Effective Number of Samples
==================================================

Focused review:

1) The projectivity of the Baxter permutation process (BPP) is *not* carried over to its corresponding floorplan partition (footnote 3 on page 6). As the projectivity of the block-breaking process (BBP) is built on BPP, BBP is also not projective. 2) Descriptions of some important steps are missing, such as 1) how to insert a new block in an existing floorplan partition (lack of explanations for Figure 5(right)); 2) how to expand the domain of a floorplan partition from an n*n 2-array to an (n+1)*(n+1) 2-array (lack of explanation for Figure 6(right)), and see more minor missing details in “Clarity” below. 3) The authors have provided a comprehensive collection of relevant works; however, the proposed BBP is only compared to three of them. I understand that the authors might think that they may only need to compare with axis-aligned partitioning methods, but if the proposed BBP could be compared with methods with sloped cuts to see which type of flexibility is more powerful in real-world relational modeling, it would be more interesting.

Review Point: 1) The projectivity of the Baxter permutation process (BPP) is *not* carried over to its corresponding floorplan partition (footnote 3 on page 6). As the projectivity of the block-breaking process (BBP) is built on BPP, BBP is also not projective.
Review Point: 2) Descriptions of some important steps are missing, such as 1) how to insert a new block in an existing floorplan partition (lack of explanations for Figure 5(right));
Review Point: 2) how to expand the domain of a floorplan partition from an n*n 2-array to an (n+1)*(n+1) 2-array (lack of explanation for Figure 6(right)), and see more minor missing details in “Clarity” below.
Review Point: 3) The authors have provided a comprehensive collection of relevant works; however, the proposed BBP is only compared to three of them. I understand that the authors might think that they may only need to compare with axis-aligned partitioning methods, but if the proposed BBP could be compared with methods with sloped cuts to see which type of flexibility is more powerful in real-world relational modeling, it would be more interesting.
==================================================

Focused review:

Consider demonstrating with a simple example what your results imply in terms of the number of problems you need to solve for some real data set. Also show via some simple experiment that standard approaches (such as 10-fold CV) can fail (as you claim they do) and that your method solves this problem.
Move section 4 to the appendix since ordinary lasso classification is not a standard or useful model.
State clearly what your contributions are in relation to previous work such as [1, 2, 3, 4] and cite their work where appropriate. In particular there seems to be overlap with [1] that isn't accounted for in the work.
Consider amending the literature section in the paper by including some or all of [1, 2, 3], and [4] regarding homotopy methods for the lasso.
Include at the very least [5] in your literature section on hyper-parameter tuning for the lasso.
It is not standard to call the Elastic Net (or elastic net) ElasticNet. Consider adding a space.
l. 58. There should be a period at the end of the sentence.
l. 77. w.h.p. is not a standard abbreviation. Consider spelling it out.
l. 208. There should be a period at the end of the equation.
l. 271. There should be a period after ϵ .
l. 386. There is an extra space after "values".
[2] P. Garrigues and L. Ghaoui, “An homotopy algorithm for the lasso with online observations,” in Advances in neural information processing systems 21, Vancouver, Canada, Dec. 2008, vol. 21, pp. 489–496. [Online]. Available: https://proceedings.neurips.cc/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf
[3] D. M. Malioutov, M. Cetin, and A. S. Willsky, “Homotopy continuation for sparse signal representation,” in Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005, Philadelphia, USA, Mar. 2005, vol. 5, pp. v733–v736. doi: 10.1109/ICASSP.2005.1416408.
[4] M. Osborne, B. Presnell, and B. Turlach, “A new approach to variable selection in least squares problems,” IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389–403, Jul. 2000, doi: 10.1093/imanum/20.3.389.
[5] Q. Bertrand, Q. Klopfenstein, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon, “Implicit differentiation of Lasso-type models for hyperparameter optimization,” in Proceedings of the 37th International Conference on Machine Learning, Nov. 2020, pp. 810–821. Accessed: Jun. 27, 2022. [Online]. Available: https://proceedings.mlr.press/v119/bertrand20a.html

Review Point: 58. There should be a period at the end of the sentence. l.
Review Point: 77. w.h.p. is not a standard abbreviation. Consider spelling it out. l. 208. There should be a period at the end of the equation. l. 271. There should be a period after ϵ . l. 386. There is an extra space after "values". [2] P. Garrigues and L. Ghaoui, “An homotopy algorithm for the lasso with online observations,” in Advances in neural information processing systems 21, Vancouver, Canada, Dec. 2008, vol. 21, pp. 489–496. [Online]. Available: https://proceedings.neurips.cc/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf [3] D. M. Malioutov, M. Cetin, and A. S. Willsky, “Homotopy continuation for sparse signal representation,” in Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005, Philadelphia, USA, Mar. 2005, vol. 5, pp. v733–v736. doi: 10.1109/ICASSP.2005.1416408. [4] M. Osborne, B. Presnell, and B. Turlach, “A new approach to variable selection in least squares problems,” IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389–403, Jul. 2000, doi: 10.1093/imanum/20.3.389. [5] Q. Bertrand, Q. Klopfenstein, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon, “Implicit differentiation of Lasso-type models for hyperparameter optimization,” in Proceedings of the 37th International Conference on Machine Learning, Nov. 2020, pp. 810–821. Accessed: Jun. 27, 2022. [Online]. Available: https://proceedings.mlr.press/v119/bertrand20a.html
==================================================

Focused review:

- Why J(\theta) has been deducted from immediate reward to compute for the Q-values (line 136)? A citation or explanation will be helpful. - To compute approximate gradient (Algorithm 1, line 7), it is assumed that agents have access to Q-values of neighbours, which is not clearly mentioned in the paper. - ALOHA protocol is used as baseline which seems too simplified to me. Comparision with other learning algorithm suited for average reward setting will make the results more convincing.

Review Point: - Why J(\theta) has been deducted from immediate reward to compute for the Q-values (line 136)? A citation or explanation will be helpful.
Review Point: - To compute approximate gradient (Algorithm 1, line 7), it is assumed that agents have access to Q-values of neighbours, which is not clearly mentioned in the paper.
Review Point: - ALOHA protocol is used as baseline which seems too simplified to me. Comparision with other learning algorithm suited for average reward setting will make the results more convincing.
==================================================

Focused review:

I have only a few remarks on this paper, even though they shouldn't be considered as weaknesses. They are listed below in no particular order. - in eq.1 | is used both as the absolute value operator and the cardinality one, which can lead to confusion - in eq.2, \tau and v have not been previously defined (unless I'm missing something) - I find it regrettable that no theoretical analysis of Mesa (e.g. convergence speed, generalization error, etc) is proposed aside from the complexity one, especially since it is built upon frameworks with strong theoretical properties - line 155 "is thus can be" typo - line 173 reference error "Haarnoja et al." - line 233 compares > compared - table 2, what does k correspond to? Is it the parameter of Algorithm 2? - a few more datasets would've been appreciated, especially concerning the cross-task transferability

Review Point: - a few more datasets would've been appreciated, especially concerning the cross-task transferability
==================================================

Focused review:

Weakness: 1. The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning. The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.” Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1. 2. The theory is a bit complicated and not easy to follow. 3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.

Review Point: 1. The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning. The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.” Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.
Review Point: 2. The theory is a bit complicated and not easy to follow.
Review Point: 3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.
==================================================

Focused review:

Weakness: despite the rigorous mathematical derivation, I am not sure how the presented PAC analysis result helps the community. One can directly apply the sample complexity analysis on the adversarial loss (say via Rademacher complexity). Does the presented result provide more insights or a better rate?
Weakness: the paper doesn't actually provide an executable algorithm, but only an existence results, i.e., the existence of measure mu, the existence of sample compression scheme. Whether the rate results pair with a computational feasible algorithm is unclear, and there is no simulation to demonstrate empirical performance/comparison. 4 Weakness: in conclusion, the paper neither provide a theoretical information limit result, nor an executable adversarial training algorithm.

Review Point: 4 Weakness: in conclusion, the paper neither provide a theoretical information limit result, nor an executable adversarial training algorithm.
==================================================

Focused review:

I have two major concerns about the paper: 1. The premise of the paper is that practitioners would like to optimize a predefined function of latent patient state; however, such a function cannot be predefined since the learned latent constructs do not, a priori, have meaning. More specifically, latent psychological constructs learned by a model such as factor analysis only take on meaning *after* practitioners interpret the learned factors in the context of the underlying items. Further, latent variables may be subject to many different identifiability issues that make defining the function g impossible without first examining the learned latent factors. For example, the proposed model cannot distinguish between Z and 1-Z which would have complementary interpretations. To see this, simply observe that \alpha + \sum_k \beta_k Z_k = \alpha' + \sum_k \beta'_k U_k, where U_k = 1 - Z_k, \alpha' = \alpha + \sum_k \beta_k, and \beta'_k = -\beta_k. 2. More broadly, I have serious concerns about the potential risks of optimizing treatment with respect to learned latent constructs without first extensively validating those constructs. As I'm sure the authors are aware, there is a substantial body of literature on validating psychological measure in order to ensure that they are measuring what is expected. In particular, in the broader impact statement, the authors state "No individual may be put at disadvantage from this research"; however, the authors absolutely cannot guarantee that latent factors learned using the proposed method do not have biases. Latent constructs such as IQ are well-known to have racial and socio-economic biases and to use such measures to optimize treatment before testing for such biases may lead to biased treatment.

Review Point: 1. The premise of the paper is that practitioners would like to optimize a predefined function of latent patient state; however, such a function cannot be predefined since the learned latent constructs do not, a priori, have meaning. More specifically, latent psychological constructs learned by a model such as factor analysis only take on meaning *after* practitioners interpret the learned factors in the context of the underlying items. Further, latent variables may be subject to many different identifiability issues that make defining the function g impossible without first examining the learned latent factors. For example, the proposed model cannot distinguish between Z and 1-Z which would have complementary interpretations. To see this, simply observe that \alpha + \sum_k \beta_k Z_k = \alpha' + \sum_k \beta'_k U_k, where U_k = 1 - Z_k, \alpha' = \alpha + \sum_k \beta_k, and \beta'_k = -\beta_k.
Review Point: 2. More broadly, I have serious concerns about the potential risks of optimizing treatment with respect to learned latent constructs without first extensively validating those constructs. As I'm sure the authors are aware, there is a substantial body of literature on validating psychological measure in order to ensure that they are measuring what is expected. In particular, in the broader impact statement, the authors state "No individual may be put at disadvantage from this research"; however, the authors absolutely cannot guarantee that latent factors learned using the proposed method do not have biases. Latent constructs such as IQ are well-known to have racial and socio-economic biases and to use such measures to optimize treatment before testing for such biases may lead to biased treatment.
==================================================

Focused review:

Weaknesses ---------- - a bit heavy on constraint-based causal discovery jargon, some explanation for non-experts would help - computational complexity not sufficiently discussed Quality ------- The algorithm and theory is technically sound. What I miss is a more thorough discussion of the increase in compational complexity since this may be a major impediment in adopting the algorithm.  The introduction could also mention other structure learning approaches that, I believe, don't have the inconsistency issue, e.g., SAT-solver based approaches [2]. One question to clarify for non-experts is whether the presented modification is relevant also for the graph class of DAGs instead of CPDAGs (no Markov ambiguity)? I.e., if only the skeleton phase is needed since all links are oriented due to time-order or other side knowledge. Also, since this is about finite sample consistency, the authors could discuss how the new algorithm relates to the uniform consistency proof of the PC algorithm [1]?  Clarity ------- Overall the article well-written, but a bit heavy on constraint-based causal discovery jargon. I have listed a number of points below that could be immproved upon. What would help is a figure and intuitive example of sepset inconsistency. I find a paper much more readable that illustrates a problem and solution on a simple example.  Originality ----------- As far as I know the work is new and related work is cited. Significance ------------ The new algorithm can be of significant impact since, as the authors also mention, it makes PC-based methods more robust and better interpretable. The question is just whether the increase in computational complexity makes this prohibitive. That's not clear to me from the paper. Minor further points -------------------- Abstract: - explain separating set for non-experts - "uncover spurious conditional independences" --> "uncover false negatives" ?  Introduction "sampling noise" --> finite sample sizes - could cite some more related work, what about SAT solver approaches to causal discovery [2]? - "invoked to remove" --> sounds odd - "achieve a better balance" --> "achieves a better balance" Section 2.2.1 first paragraph is unclear, could phrase "slicing the available data" better... Definition 1: Here I would like to see an example of a sepset inconsistency with the final that gives a non-expert an intuition Section 2.2.2 S, S', S_0, etc: all a bit confusing. Maybe a different notation would help. "(i.e. relaxed, majority or conservative rules)" --> difficult to understand for non-experts Proof of Theorem 4 "discard all" --> "discards all" "less stringent separating set consistency" --> what does this imply theoretically? The theorem 4 still holds, right? "the the edges" --> "the edges" Section 2.3 Here one could summarize the difference in complexity between PC-stable and the proposed algorithm Section 2.3.3 - "covariance ranges" --> Covariance between what? The SCM is run with the given coefficient ranges and gaussian unit variance errors, no? - Which specific alpha levels were chosen?  References ---------- [1] Kalisch, Markus. 2007. âEstimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm.â The Journal of Machine Learning Research 8: 613â36. [2] Hyttinen, Antti, Patrik O Hoyer, Frederick Eberhardt, and Matti JÃ¤rvisalo. 2013. âDiscovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure.â In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 301â10. ---------------------- Update after author feedback: The authors convincingly addressed my question regarding computational complexity and they have also found a better definition of orientation-consistency. I also acknowledge that the authors will explain the problem better with an example figure. With these, I would still stick to my previous score "a good submission" (7). I concur with the other reviewers evaluation that the work is more incremental, but in a very solid way. 

Review Point: ---------- - a bit heavy on constraint-based causal discovery jargon, some explanation for non-experts would help - computational complexity not sufficiently discussed Quality ------- The algorithm and theory is technically sound. What I miss is a more thorough discussion of the increase in compational complexity since this may be a major impediment in adopting the algorithm. The introduction could also mention other structure learning approaches that, I believe, don't have the inconsistency issue, e.g., SAT-solver based approaches [2]. One question to clarify for non-experts is whether the presented modification is relevant also for the graph class of DAGs instead of CPDAGs (no Markov ambiguity)? I.e., if only the skeleton phase is needed since all links are oriented due to time-order or other side knowledge. Also, since this is about finite sample consistency, the authors could discuss how the new algorithm relates to the uniform consistency proof of the PC algorithm [1]? Clarity ------- Overall the article well-written, but a bit heavy on constraint-based causal discovery jargon. I have listed a number of points below that could be immproved upon. What would help is a figure and intuitive example of sepset inconsistency. I find a paper much more readable that illustrates a problem and solution on a simple example. Originality ----------- As far as I know the work is new and related work is cited. Significance ------------ The new algorithm can be of significant impact since, as the authors also mention, it makes PC-based methods more robust and better interpretable. The question is just whether the increase in computational complexity makes this prohibitive. That's not clear to me from the paper. Minor further points -------------------- Abstract:
Review Point: - explain separating set for non-experts - "uncover spurious conditional independences" --> "uncover false negatives" ? Introduction "sampling noise" --> finite sample sizes - could cite some more related work, what about SAT solver approaches to causal discovery [2]?
Review Point: - Which specific alpha levels were chosen? References ---------- [1] Kalisch, Markus. 2007. âEstimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm.â The Journal of Machine Learning Research 8: 613â36. [2] Hyttinen, Antti, Patrik O Hoyer, Frederick Eberhardt, and Matti JÃ¤rvisalo. 2013. âDiscovering Cyclic Causal Models with Latent Variables: A General SAT-Based Procedure.â In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 301â10. ---------------------- Update after author feedback: The authors convincingly addressed my question regarding computational complexity and they have also found a better definition of orientation-consistency. I also acknowledge that the authors will explain the problem better with an example figure. With these, I would still stick to my previous score "a good submission" (7). I concur with the other reviewers evaluation that the work is more incremental, but in a very solid way.
==================================================

Focused review:

One big issue that I see is, it's not very meaningful to do model compression for unsupervised models before the current evolution of contrastive approaches plateau. - For example, you may distill from a big model to ResNet-50 to achieve a good performance today, then the next day a new contrastive paper comes out and achieves higher performance using only ResNet-50 without a teacher. Then why do we still need the distillation method proposed today, instead of directly using the new contrastive method? So this distillation method will quickly fade away. For instance, IIRC, [a] achieves 73.0% linear accuracy and transfer better to Pascal VOC and COCO, then the effectiveness of this paper will be largely discounted (by the way [a] might be discussed as well). - Distillation makes more sense in supervised setting because, you can always get performance boost for free, and that boost is non-trivial and can always be stacked to cross-entropy only models. But it's unclear how your method could improve upon better self-supervised methods, e.g., can you improve upon [a] using your method out of the box? [a] What makes for good views for contrastive learning I wonder if the proposed method could do self-distillation, i.e., you train a ResNet-50 by whatever self-supervised method, and then you trained a new ResNet-50 using your distillation method. For example, you can distill from ResNet-50 trained in [a] or MoCo v2 to see if you can improve upon it. - If this works, it would make your distillation method much more useful, as it could improve any new SoTA method that jump in in the future I did not buy the point of self-supervised AlexNet surpasses supervised one. - This is not surprising as you are distilling from a much bigger and better model. I can expect that once unsupervised ResNet-50 will surpasses supervised R-50 with distillation. - The self-supervised model is using heavier computation and longer training, making the comparison not head-to-head. Table 1 is a bit messy. Both supervised baseline, unsupervised baseline and different distilled models are put into the same section. Besides, whether the baseline methods have been carefully tuned is unclear. E.g., - Observation: while CC is much worse than CRD for AlexNet, it's much better for MobileNet-V2. Why? - Have you ever tuned different numbers of clusters for CC to make it optimal - Have you also tried to make every other components in CRD to be similar as your method here? E..g. non-linear projection head, as well as the number of negatives? For transferring to Pascal VOC, why not using ResNet-50, which is more interesting to see? Also, why not trying COCO? I might prefer COCO over PASCAL VOC.

Review Point: - For example, you may distill from a big model to ResNet-50 to achieve a good performance today, then the next day a new contrastive paper comes out and achieves higher performance using only ResNet-50 without a teacher. Then why do we still need the distillation method proposed today, instead of directly using the new contrastive method? So this distillation method will quickly fade away. For instance, IIRC, [a] achieves 73.0% linear accuracy and transfer better to Pascal VOC and COCO, then the effectiveness of this paper will be largely discounted (by the way [a] might be discussed as well).
Review Point: - Distillation makes more sense in supervised setting because, you can always get performance boost for free, and that boost is non-trivial and can always be stacked to cross-entropy only models. But it's unclear how your method could improve upon better self-supervised methods, e.g., can you improve upon [a] using your method out of the box? [a] What makes for good views for contrastive learning I wonder if the proposed method could do self-distillation, i.e., you train a ResNet-50 by whatever self-supervised method, and then you trained a new ResNet-50 using your distillation method. For example, you can distill from ResNet-50 trained in [a] or MoCo v2 to see if you can improve upon it.
Review Point: - If this works, it would make your distillation method much more useful, as it could improve any new SoTA method that jump in in the future I did not buy the point of self-supervised AlexNet surpasses supervised one.
Review Point: - This is not surprising as you are distilling from a much bigger and better model. I can expect that once unsupervised ResNet-50 will surpasses supervised R-50 with distillation.
Review Point: - The self-supervised model is using heavier computation and longer training, making the comparison not head-to-head. Table 1 is a bit messy. Both supervised baseline, unsupervised baseline and different distilled models are put into the same section. Besides, whether the baseline methods have been carefully tuned is unclear. E.g., - Observation: while CC is much worse than CRD for AlexNet, it's much better for MobileNet-V2. Why?
Review Point: - Have you ever tuned different numbers of clusters for CC to make it optimal - Have you also tried to make every other components in CRD to be similar as your method here? E..g. non-linear projection head, as well as the number of negatives? For transferring to Pascal VOC, why not using ResNet-50, which is more interesting to see? Also, why not trying COCO? I might prefer COCO over PASCAL VOC.
==================================================

Focused review:

Weakness
1 - The novelty of this work is limited.
2 - Miss important related works.
3 - Experiments should be improved.
Detailed Review
The novelty of proposed method is limited for me. The major contribution: data augmentation strategy (pseudo label) and negative sampling regularization, is simple and intuitively, depending on the threshold value. There are many works proposing better data augmentation strategies or pseudo label generation, such as:
Graph contrastive learning with augmentations, NeurIPS 2020
Big Self-Supervised Models are Strong Semi-Supervised Learners, NeurIPS 2020
More importantly, this paper lacks discussion and comparison of important related works. The authors seem to be not aware of graph few-shot learning works. There are many studies working on graph learning with few-shot labels, such as followings:
Graph meta learning via local subgraphs, NeurIPS 2020
Graph few-shot learning via knowledge transfer, AAAI 2020
Meta-gnn: On few-shot node classification in graph meta-learning, CIKM 2019
Node classification on graphs with few-shot novel labels via meta transformed network embedding, NeurIPS 2020

Review Point: 3 - Experiments should be improved. Detailed Review The novelty of proposed method is limited for me. The major contribution: data augmentation strategy (pseudo label) and negative sampling regularization, is simple and intuitively, depending on the threshold value. There are many works proposing better data augmentation strategies or pseudo label generation, such as: Graph contrastive learning with augmentations, NeurIPS 2020 Big Self-Supervised Models are Strong Semi-Supervised Learners, NeurIPS 2020 More importantly, this paper lacks discussion and comparison of important related works. The authors seem to be not aware of graph few-shot learning works. There are many studies working on graph learning with few-shot labels, such as followings: Graph meta learning via local subgraphs, NeurIPS 2020 Graph few-shot learning via knowledge transfer, AAAI 2020 Meta-gnn: On few-shot node classification in graph meta-learning, CIKM 2019 Node classification on graphs with few-shot novel labels via meta transformed network embedding, NeurIPS 2020
==================================================

Focused review:

- the training loss function of equation (5) contains several regularisation terms, but their impact of final 3D reconstruction quality is not evaluated qualitatively nor quantitatively. This makes the contribution of each single loss component impossible to quantify. - unlike deep implict fields, this representation is limited in resolution. No study of the impact of resolution on final reconstruction accuracy is provided by the authors. This makes it hard to compare the method to representation that are not limited in resolution, especially given that in several qualitative results (see supplementary section) OccNet seems to deliver higher quality reconstructions. - the presented applications do not highlight the true advantage of the proposed representation, i.e. its end-to-end differentiability, as several differentiable rendering techniques have been developed for deep implicit fields. Studying applications where end-to-end differentiability is necessary would have made the submission stronger, and would have better highlighted its advantages over deep implicit fields.

Review Point: - the training loss function of equation (5) contains several regularisation terms, but their impact of final 3D reconstruction quality is not evaluated qualitatively nor quantitatively. This makes the contribution of each single loss component impossible to quantify.
Review Point: - unlike deep implict fields, this representation is limited in resolution. No study of the impact of resolution on final reconstruction accuracy is provided by the authors. This makes it hard to compare the method to representation that are not limited in resolution, especially given that in several qualitative results (see supplementary section) OccNet seems to deliver higher quality reconstructions.
Review Point: - the presented applications do not highlight the true advantage of the proposed representation, i.e. its end-to-end differentiability, as several differentiable rendering techniques have been developed for deep implicit fields. Studying applications where end-to-end differentiability is necessary would have made the submission stronger, and would have better highlighted its advantages over deep implicit fields.
==================================================

Focused review:

1. Relationship to previous work. First and foremost, the paper lacks a discussion on previous related work on graph convolution network (GCN). As per my knowledge, the technologies used in 3.2 NAPL and 3.3 DAGG have already been proposed/discussed in [1, 2, 3]. These papers are very famous for improving GCN, but not mentioned in this paper. As a reader, I am not sure what contributions exactly this paper has made. I encourage the authors to present which papers the technologies that they use to improve GCN are from, and what this paper is adding to the literature or how this paper modifying these technologies to adapt traffic forecasting task. [1] Graph Attention Network, ICLR 2018. [2] Self-Attention Graph Pooling, ICML 2019. [3] Hierarchical Graph Representation Learning with Differentiable Pooling, NeurIPS 2018. 2. Technical Novelty: as explained in first item, due to lacks of a discussion on previous related work on graph convolution network, the proposed model in this paper seems replace the GCN part of previous traffic forecasting models, with a more advanced existing one. The technical novelty of the paper is limited. But I understand that technical novelty is not the aim of the paper, so I list this item in the bottom.

Review Point: 1. Relationship to previous work. First and foremost, the paper lacks a discussion on previous related work on graph convolution network (GCN). As per my knowledge, the technologies used in 3.2 NAPL and 3.3 DAGG have already been proposed/discussed in [1, 2, 3]. These papers are very famous for improving GCN, but not mentioned in this paper. As a reader, I am not sure what contributions exactly this paper has made. I encourage the authors to present which papers the technologies that they use to improve GCN are from, and what this paper is adding to the literature or how this paper modifying these technologies to adapt traffic forecasting task. [1] Graph Attention Network, ICLR 2018. [2] Self-Attention Graph Pooling, ICML 2019. [3] Hierarchical Graph Representation Learning with Differentiable Pooling, NeurIPS 2018.
Review Point: 2. Technical Novelty: as explained in first item, due to lacks of a discussion on previous related work on graph convolution network, the proposed model in this paper seems replace the GCN part of previous traffic forecasting models, with a more advanced existing one. The technical novelty of the paper is limited. But I understand that technical novelty is not the aim of the paper, so I list this item in the bottom.
==================================================

Focused review:

Weaknesses:
The technical term, "baseline value", is not defined until p. 4 in equation (2). To my knowledge, "baseline values" are not apart of the original theory of Shapley values, and so it would help to introduce a precise definition much earlier than p. 4. Note that "baseline values" are only needed when explaining deep networks with Shapley values if you insist on using a single neural network architecture that takes all features as input (contrast with approach cited in bullet 2 below). Only then do you need to find a "baseline value" that is equivalent to a scenario in which the neural network never "sees" that feature.
An alternative, possibly more direct translation of a "characteristic function" and Shapley values is explored in "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index" by Patel et al (https://aclanthology.org/2021.naacl-main.223.pdf). In that paper, they define the characteristic function, v(S), as the loss / output of a model trained on data which only retains features from S. The upside of this approach is that it avoids creating problematic "baseline values". The downside of this approach is that it requires training a new model in order to evaluate v(S), which is very expensive. I would suggest adding this paper to your related work.
The connection between Shapley values and the Harsanyi dividend isn't detailed until p. 7, equation (6). The paper hinges on this connection, so this should be made much earlier. In fact, I found equation (10) in Appx. B.2 to be much more helpful in clarifying this connection. I suggest stating equation (10) soon after introducing Shapley values.
How do the losses in equation (7) depend on the "baseline values", b, being learned? Are you learning Harsanyi dividends, I(S), via the characteristic function v(S) (eqn 4), via b (eqn 2)? Equations 5 an 7's dependence on the baseline values is not clear. The paper "aims to formulate the problem of estimating optimal baseline values" but baseline values do not appear explicitly in any of the loss functions, making it difficult to assess the exact approach.
In the experiments, a discussion of Figure 4 suggests that your approach is performing better than the others, but I fail to see this from the figures. All except for the zero baseline and baseline in SAGE seem to perform similarly. Any performance difference seems very marginal.
I see that your methods agreed somewhat with SHAP and SAGE, but I'm not sure how much I can extract from this experiment. Is that the only conclusion I am meant to draw?
In equation (7), if the absolute value was around the expectation in L m a r g i n a l
, would it be essentially the same as L s h a p l e y
? Is that the main difference between these two losses and why you say L m a r g i n a l
is more fine grained?
The penultimate step of proving the dummy property in Appx B.1 is used frequently throughout proofs. This deserves a citation or lemma. I believe it is true, but this is a combinatorial / binomial result that I cannot easily confirm by inspection.
Equation (5) gives an intuitive approximation of an absence state, but I see no rigorous argument for why V(S masked with baseline b) = V(S)?
Why is the number of salient patterns the correct metric? Why not some weighted sum? Why not some other function of the salient patterns? This seemed a bit arbitrary to me.
In table (6), does the left most column labeled "Functions" contain the Harsanyi dividends I ( S )
? I wasn't clear what "Functions" refers to.
Minor: In some places, you refer to "massive variables". I assumed you meant massive "numbers of" variables, i.e., many features. Maybe try to reword those parts so it's clear to the reader what you mean.

Review Point: 4 in equation (2). To my knowledge, "baseline values" are not apart of the original theory of Shapley values, and so it would help to introduce a precise definition much earlier than p.
Review Point: 4. Note that "baseline values" are only needed when explaining deep networks with Shapley values if you insist on using a single neural network architecture that takes all features as input (contrast with approach cited in bullet 2 below). Only then do you need to find a "baseline value" that is equivalent to a scenario in which the neural network never "sees" that feature. An alternative, possibly more direct translation of a "characteristic function" and Shapley values is explored in "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index" by Patel et al (https://aclanthology.org/2021.naacl-main.223.pdf). In that paper, they define the characteristic function, v(S), as the loss / output of a model trained on data which only retains features from S. The upside of this approach is that it avoids creating problematic "baseline values". The downside of this approach is that it requires training a new model in order to evaluate v(S), which is very expensive. I would suggest adding this paper to your related work. The connection between Shapley values and the Harsanyi dividend isn't detailed until p. 7, equation (6). The paper hinges on this connection, so this should be made much earlier. In fact, I found equation (10) in Appx. B.2 to be much more helpful in clarifying this connection. I suggest stating equation (10) soon after introducing Shapley values. How do the losses in equation (7) depend on the "baseline values", b, being learned? Are you learning Harsanyi dividends, I(S), via the characteristic function v(S) (eqn 4), via b (eqn 2)? Equations 5 an 7's dependence on the baseline values is not clear. The paper "aims to formulate the problem of estimating optimal baseline values" but baseline values do not appear explicitly in any of the loss functions, making it difficult to assess the exact approach. In the experiments, a discussion of Figure 4 suggests that your approach is performing better than the others, but I fail to see this from the figures. All except for the zero baseline and baseline in SAGE seem to perform similarly. Any performance difference seems very marginal. I see that your methods agreed somewhat with SHAP and SAGE, but I'm not sure how much I can extract from this experiment. Is that the only conclusion I am meant to draw? In equation (7), if the absolute value was around the expectation in L m a r g i n a l , would it be essentially the same as L s h a p l e y ? Is that the main difference between these two losses and why you say L m a r g i n a l is more fine grained? The penultimate step of proving the dummy property in Appx B.1 is used frequently throughout proofs. This deserves a citation or lemma. I believe it is true, but this is a combinatorial / binomial result that I cannot easily confirm by inspection. Equation (5) gives an intuitive approximation of an absence state, but I see no rigorous argument for why V(S masked with baseline b) = V(S)? Why is the number of salient patterns the correct metric? Why not some weighted sum? Why not some other function of the salient patterns? This seemed a bit arbitrary to me. In table (6), does the left most column labeled "Functions" contain the Harsanyi dividends I ( S ) ? I wasn't clear what "Functions" refers to. Minor: In some places, you refer to "massive variables". I assumed you meant massive "numbers of" variables, i.e., many features. Maybe try to reword those parts so it's clear to the reader what you mean.
==================================================

Focused review:

Weakness: 1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use “few-shot” and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps. 2. The definition of “domain” in this paper is unclear. For instance, why select multiple domains from the same single graph in ogbn-products? Should we consider the selected domains as “different domains”? 3. The application of adversarial learning in few-shot learning is confusing. Adversarial learning in domain adaptation aims to learn domain-invariant representations, but why do we need such kind of representation in few-shot learning?

Review Point: 1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use “few-shot” and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps.
Review Point: 2. The definition of “domain” in this paper is unclear. For instance, why select multiple domains from the same single graph in ogbn-products? Should we consider the selected domains as “different domains”?
Review Point: 3. The application of adversarial learning in few-shot learning is confusing. Adversarial learning in domain adaptation aims to learn domain-invariant representations, but why do we need such kind of representation in few-shot learning?
==================================================

Focused review:

- In experiments, comparisons with other existing algorithms (e.g., [Curi et al. (2019); Namkoong and Duch (2016)]) are not presented. - For readers who are unfamiliar with this are (like me), it is hard to intuitively understand the key reason why such bounds, which are independent of the training set size and the number of parameters, are possible.

Review Point: - In experiments, comparisons with other existing algorithms (e.g., [Curi et al. (2019); Namkoong and Duch (2016)]) are not presented.
Review Point: - For readers who are unfamiliar with this are (like me), it is hard to intuitively understand the key reason why such bounds, which are independent of the training set size and the number of parameters, are possible.
==================================================

Focused review:

1. The model consists of spectral normalized hidden layers to guarantee a bounded Lipchitz constant for top NN layers, and a random Fourier feature approximated Gaussian process as the last layer. The combination is new, but the overall method is not end-to-end. Thus it can be hard to balance these two components to let them work well with each other. In supplementary material, I saw the entire algorithm is an alternative minimization optimization. So I'm curious about how you choose the initial parameters to make it work well. 2. The main strength is the empirical performance, but the paper does not release code.

Review Point: 1. The model consists of spectral normalized hidden layers to guarantee a bounded Lipchitz constant for top NN layers, and a random Fourier feature approximated Gaussian process as the last layer. The combination is new, but the overall method is not end-to-end. Thus it can be hard to balance these two components to let them work well with each other. In supplementary material, I saw the entire algorithm is an alternative minimization optimization. So I'm curious about how you choose the initial parameters to make it work well.
Review Point: 2. The main strength is the empirical performance, but the paper does not release code.
==================================================

Focused review:

I’d like to see more discussion of limitations. There are many variants of the method and parameters to tune, so the complexity of this approach might make use in other domains difficult.
Compute needs: there is no mention of the substantial compute required in the paper, and the section in Appendix A could be elaborated on. 2 Billion frames per run * 3 seeds * 5 methods * 2 settings = 60 billion frames, not including any experimentation or hyperparameter tuning. In addition to the environmental impact, this level of computation is not accessible to most researchers. Limitations of compute needs and lack of sample efficiency should be addressed.
The checklist says N/A for societal impacts; even if the authors don’t see the need to include a section in the paper, I’d like to see a justification of why this is the case.

Review Point: 2 Billion frames per run * 3 seeds * 5 methods * 2 settings = 60 billion frames, not including any experimentation or hyperparameter tuning. In addition to the environmental impact, this level of computation is not accessible to most researchers. Limitations of compute needs and lack of sample efficiency should be addressed. The checklist says N/A for societal impacts; even if the authors don’t see the need to include a section in the paper, I’d like to see a justification of why this is the case.
==================================================

Focused review:

1. The result is not surprising at first glance, since PAC-Bayes results are essentially non-uniform learning bounds (the sample complexity depends on the identity of the best hypothesis) rather than uniform learning bounds (sample complexity independent of the identity of best hypothesis), where the hypothesis space can be seen as "gradated" based on the prior probability intensity. 2. Should explain earlier and more intuitively why having the adversarial distribution choice not depend on P is challenging and more interesting.

Review Point: 1. The result is not surprising at first glance, since PAC-Bayes results are essentially non-uniform learning bounds (the sample complexity depends on the identity of the best hypothesis) rather than uniform learning bounds (sample complexity independent of the identity of best hypothesis), where the hypothesis space can be seen as "gradated" based on the prior probability intensity.
Review Point: 2. Should explain earlier and more intuitively why having the adversarial distribution choice not depend on P is challenging and more interesting.
==================================================

Focused review:

weaknesses of the submission. * originality: This is a highly specialized contribution building up novel results on two main fronts: The derivation of the lower bound on the competitive ratio of any online algorithm and the introduction of two variants of an existing algorithm so as to meet this lower bound. Most of the proofs and techniques are natural and not surprising. In my view the main contribution is the introduction of the regularized version which brings a different, and arguably more modern interpretation, about the conditions under which these online algorithms perform well in these adversarial settings. * quality: The technical content of the paper is sound and rigorous * clarity: The paper is in general very well-written, and should be easy to follow for expert readers. * significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems. * minor details/comments: - p.1, line 6-7: I would rewrite the sentence to simply express that the lower bound is $\Omega(m^{-1/2})$ \- p.3, line 141: cost an algorithm => cost of an algorithm \- p.4, Algorithm 1, step 3: mention somewhere that this is the projection operator (not every reader will be familiar with this notation \- p.5, Theorem 2: remind the reader that the $\gamma$ in the statement is the parameter of OBD as defined in Algorithm 1 \- p.8, line 314: why surprisingly? 

Review Point: * quality: The technical content of the paper is sound and rigorous * clarity: The paper is in general very well-written, and should be easy to follow for expert readers.
Review Point: * significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems.
Review Point: * minor details/comments:- p.1, line 6-7: I would rewrite the sentence to simply express that the lower bound is $\Omega(m^{-1/2})$ \- p.3, line 141: cost an algorithm => cost of an algorithm \- p.4, Algorithm 1, step 3: mention somewhere that this is the projection operator (not every reader will be familiar with this notation \- p.5, Theorem 2: remind the reader that the $\gamma$ in the statement is the parameter of OBD as defined in Algorithm 1 \- p.8, line 314: why surprisingly?
==================================================

Focused review:

Weakness
The weakness of the paper results from the lack of clarity of the definition and configuration, as well as the worry about the overall significance of the results.
1. w.r.t. Definition 2: Selection diagram
Selection diagram is the core definition with respect to which major theoretical results in the paper are presented. While I agree with authors that we need some structural assumptions on domain generalization so that we can perform analyses, I am hesitant to agree with the way assumptions are introduced with "selection diagram". In my opinion, the definition is confusing. If the S
variable is an additional cause for the observed variable (e.g., Figure 1b - 1d), this is essentially changing the underlying causal semantics among the observed variables, which is not the setting described in the paper. In the medical study example in Section 2, what is the additional cause for W
(Figure 1b) in this context? Shouldn't it be W → S W
since it is a (general) kind of selection? Figure 1c - 1d share the same problem.
2. the modeling of domain generalization
As a follow-up to 1., a very relevant notion in the medical study context is selection bias (e.g., "A structural approach to selection bias" by Hernán et al., 2004). While I understand the fact that in this paper the domain generalization may not be limited to selection bias, the practical example presented (e.g., observational study vs. randomized trial) seems to be more aligned with the selection bias analysis. Further clarifications will be very helpful.
3. the limited significance of theoretical results
While it is nice to see a formalized connection between (certain types of) domain generalization to transportability theory, there is a worry about the significance of the theoretical results. For instance, Theorem 1 is directly applying Definition 3. Theorem 2 and Theorem 3 are, to a certain extent, direct outcomes of observational and interventional equivalences between different SCMs. The analysis on relative robustness is essentially just hierarchical relations between equivalence, observational equivalence, and interventional equivalence, which are established results in causal inference literature (e.g., "Elements of causal inference" by Peters et al., 2017).
Some additional minor questions/comments: Is Y
limited to binary?
In introduction, what is the reason behind the "informative purpose" served by 0 < l < u < 1 ?
Before Section 2, "share an unobserved confounder", maybe "at least one" is more rigorous (just a minor thing, though).
Typo: Section 2.1, "instead optimizing over" -> "instead of optimizing over"
Typo: Section 2.1, "a chosen uncertainty set" -> "a chosen uncertain set"

Review Point: 1. w.r.t. Definition 2: Selection diagram Selection diagram is the core definition with respect to which major theoretical results in the paper are presented. While I agree with authors that we need some structural assumptions on domain generalization so that we can perform analyses, I am hesitant to agree with the way assumptions are introduced with "selection diagram". In my opinion, the definition is confusing. If the S variable is an additional cause for the observed variable (e.g., Figure 1b - 1d), this is essentially changing the underlying causal semantics among the observed variables, which is not the setting described in the paper. In the medical study example in Section 2, what is the additional cause for W (Figure 1b) in this context? Shouldn't it be W → S W since it is a (general) kind of selection? Figure 1c - 1d share the same problem.
Review Point: 2. the modeling of domain generalization As a follow-up to 1., a very relevant notion in the medical study context is selection bias (e.g., "A structural approach to selection bias" by Hernán et al., 2004). While I understand the fact that in this paper the domain generalization may not be limited to selection bias, the practical example presented (e.g., observational study vs. randomized trial) seems to be more aligned with the selection bias analysis. Further clarifications will be very helpful.
==================================================

Focused review:

Weaknesses
While the backbone of the paper is strong, I think it could be improved in its head (motivation) and legs (experimental studies).
First, motivation. While the framing around rare words with the COVID-19 example is interesting, I think it has gaps. The introduction argues that since “COVID-19” is a rare word, in the course of training the model may lack the necessary signal to predict the masked word “lives.” But isn’t this fact exactly what should lead the model to improve its embedding of “COVID-19”? Because gradients flow into the embeddings both through the softmax layer and the input layer.
So while adding to the context may help the model get a foothold with more effective training signal for the masked token, it seems to me that the note could also “explain away” the rare word’s embedding in the input layer, reducing the learning signal on it. If that’s the case, then to the extent that TNF works, it would be by the tradeoff between improving the learning signal at the output layer for all words (and in contextualization) and degrading it at the input layer (for rare words).
As a broader example, see https://openreview.net/pdf?id=3Aoft6NWFej. That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency, whereas this paper argues essentially the opposite—that shortcuts must be added to hard cases in order to facilitate learning. It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful. Because it’s not clear where that line is, I think it’s not enough to motivate TNF from only one direction. It would be better to also have an explanation of why the note-taking approach does not also make things “too easy.” It’s not obvious to me how to best make this argument, though results from some of the ablations I will suggest below might help.
This brings me to my second point: Ablation experiments. If the motivation is to improve the representations of rare words in the input, then there are even simpler ways to do this. Experiments with simple baselines and ablations are important for figuring out why exactly TNF works.
First, if the note is such a useful addition to the word embedding, why not just use it to update the embeddings directly? At that rate, the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe. This suggests a critical ablation:
Initialize the word embeddings with word2vec, GloVe, or similar run over the wordpieces in the pretraining corpus. (Weirdly, I can’t find an example of this in the literature. It seems like an obvious thing to try. I may have just missed it.) Indeed, it seems to me that the framing in the paper could just as easily motivate this (much simpler) technique than TNF.
If TNF outperforms the critical ablation, that implies that its gains are coming from some of the other particulars of the technique, such as 1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or 2) the use of contextualized vectors for note embeddings (rather than the non-contextualized ones in the word embedding objectives).
To investigate these issues, I would suggest three more ancillary ablations on TNF:
Directly update the rare word’s embedding with a version of Eq. 5 rather than keeping a separate note dictionary.
Update the note embeddings via backprop instead of Eq. 5. This would amount to “partially tying” the input and output embeddings, giving more freedom to the input layer, which is partly what’s happening in TNF.
Pool over non-contextualized instead of contextualized representations in Eq. 4.
Finally, to address the “too easy” vs “too hard” distinction, two more ablations that might help would be:
Instead of using an exponential moving average for the note embedding update, just use the pooled context vectors from the last instance of the rare word (i.e., set γ
to 1 in Eq. 5).
instead of using an explicit note dictionary, augment the input context with retrieved text containing the rare word. See TEK-enriched representations (https://arxiv.org/pdf/2004.12006.pdf) for an example of this. For consistency, the exact last-seen context of the rare word could be used.
The first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF. This could then serve as a reference point for the second ablation, which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting “too easy.” (although context window sizes might also be a confound here, that could also be controlled carefully.)
All together I think these ablations would shed a lot of light on why TNF works, and make this work much more useful to researchers who wish to build on it in the future. However, I know I’ve suggested a lot of crazy experiments here. I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important. I am also sure the authors could come up with better ablations than these as well. But my sticking point is the first ablation — initializing with non-contextualized embeddings — which I think is critical. And I think it behooves the authors to address some of the lingering questions (including more written below), even if not all of them. Recommendation
Unfortunately, reject. The technique is simple and the results seem good, but the paper does not provide empirically-justified insight on why TNF works. I think ablations and investigation into the “why” aspect is the most important part of this kind of model engineering research.
More comments & questions
I am left with some more questions about how TNF works:
How does the quality of the representations of rare words specifically compare in your approach? Does it improve the representations of common words and contextualization at the expense of rare words? While it may be tricky to try to directly assess embedding or contextualization quality, breaking down the MLM perplexities by word frequency (or presence of rare words in the context) after removing the note dictionary might be informative. I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment. But any insight into this issue would be appreciated.
If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens, then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training? I am not aware of anyone showing such a thing to work, though I might have missed it. Just a thought.
While the pretraining corpus is huge, 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper. Questions:
What do the even rarer words look like? Are they just a source of noise? e.g., because they are components of names or don’t have clear and consistent semantic content?
What proportion of contexts contain words appearing less than 100 times? It seems that the 20% figure in the paper is meant to apply to your definition of rare words, which appear between 100 and 500 times.
What is the word vocabulary size? i.e., how many words appear more than 500 times, and less than 100?
Did you do any preliminary experiments with other thresholds? Would you expect this to work with more common words as well? Why or why not? (This may also relate to the “too easy” vs “too hard” issue.)
On pre-training efficiency results: I think Figs 3a and 3b need to be explicitly qualified a little better. AFAICT, having lower loss here doesn’t necessarily mean the model (modulo the note dictionary) is learning better, because it sees the notes in the input. So we’re looking at the loss in a different setting than we intend to fine-tune in. It’s still interesting to see, but I think it's best to include an explicit caveat.
What about training the models for more steps? Will the trend hold and performance improve overall, or will the gains eventually level off as the representations of rare words get better? Especially for pretrained models, since they are used as the starting point for many models, it is often worthwhile to train them longer (as in the RoBERTa paper), so it’s important to understand the usefulness of this method in that regime.
Typos etc.:
P.3: neglectable -> negligible
P.3: Representation -> Representations (in BERT acronym)
P.6 Sec. 4.1: after “MNLI” there is a space missing after the period.
P.6: “FULL-SENTENCES” would look better & be consistent with Liu et al if it were in small caps.
Please cite the individual dataset creators for the datasets in the GLUE benchmark.
Update: upped score from 4 to 5; see comment thread.
Update again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words.

Review Point: 5. This would amount to “partially tying” the input and output embeddings, giving more freedom to the input layer, which is partly what’s happening in TNF. Pool over non-contextualized instead of contextualized representations in Eq.
==================================================

Focused review:

Weakness
There are multiple overclaim in this paper. 1) In the abstract, there is no empirical nor theoretical evidence that ARPO finds optimal policy. It is highly skeptical that ARPO can find an optimal policy in practice as the added KL regularization will rarely converge to zero.
It might be better to discuss whether the policy will converge to the optimal policy, where KL term in eq 3 reaches zero at the end.
An important ablation study could be showing how does the KL regularization change during the training.
The three run averages of results present large variance, and it is not very convincing to me ARPO really achieves statistically better generalization results. Based on the results in Table 1, there is no significant improvement from ARPO compared to baselines.
There is no discussion nor ablations on whether cycle consistency loss finds the underlying structure of MDP. It would be better at least to provide examples of style-transferred images compared with original images.

Review Point: 1) In the abstract, there is no empirical nor theoretical evidence that ARPO finds optimal policy. It is highly skeptical that ARPO can find an optimal policy in practice as the added KL regularization will rarely converge to zero. It might be better to discuss whether the policy will converge to the optimal policy, where KL term in eq 3 reaches zero at the end. An important ablation study could be showing how does the KL regularization change during the training. The three run averages of results present large variance, and it is not very convincing to me ARPO really achieves statistically better generalization results. Based on the results in Table 1, there is no significant improvement from ARPO compared to baselines. There is no discussion nor ablations on whether cycle consistency loss finds the underlying structure of MDP. It would be better at least to provide examples of style-transferred images compared with original images.
==================================================

Focused review:

Weakness:
1 The generated collages are sometimes blurry. For example, generated human is blurry in Collages 3 and 4 in Figure 2.
2 Figure 3 is misleading in that it seems that the model uses more than one object discriminator. Please redraw the figure.
3 Although the paper claims the model enables control over the position and size of the objects. It is unclear how M&Ms handles the location and the size of the objects (For example, does the model preserve the ratio of the height and width). Please elaborate on it and show more examples of object generation in different locations and with different sizes as it would be good to see how the model controls them. So, please add some results of collage generation with the same object with different locations and sizes (from small scale to big scale or maybe with the change over the ratio of height and width).
4 In Table 1, M&Ms' object FID scores are better and scene FID scores are roughly the same. Why? Does it mean that although M&Ms can generate objects with better fidelity, it is not capable to put them together? As well as, isn't it expected as IC-GAN is conditioned on only global scene features? Please add results of an ablation model which is M&Ms conditioned on only global features like IC-GAN.
5 The model of M&Ms is designed by leveraging IC-GAN, so comparison on Table 1 is like comparing with the older version M&Ms. So please include other GANs to this table.

Review Point: 1 The generated collages are sometimes blurry. For example, generated human is blurry in Collages 3 and 4 in Figure 2.
Review Point: 2 Figure 3 is misleading in that it seems that the model uses more than one object discriminator. Please redraw the figure. 3 Although the paper claims the model enables control over the position and size of the objects. It is unclear how M&Ms handles the location and the size of the objects (For example, does the model preserve the ratio of the height and width). Please elaborate on it and show more examples of object generation in different locations and with different sizes as it would be good to see how the model controls them. So, please add some results of collage generation with the same object with different locations and sizes (from small scale to big scale or maybe with the change over the ratio of height and width).
Review Point: 4 In Table 1, M&Ms' object FID scores are better and scene FID scores are roughly the same. Why? Does it mean that although M&Ms can generate objects with better fidelity, it is not capable to put them together? As well as, isn't it expected as IC-GAN is conditioned on only global scene features? Please add results of an ablation model which is M&Ms conditioned on only global features like IC-GAN.
Review Point: 5 The model of M&Ms is designed by leveraging IC-GAN, so comparison on Table 1 is like comparing with the older version M&Ms. So please include other GANs to this table.
==================================================

Focused review:

I have several questions regarding to the method: 1. If I understand correctly, the combination of using a larger MLP projection head, and finetuning from the middle layer of the MLP head, is the same as just adding one more FC layer to the base network. Is this true? If so, I suggest the authors revise relevant parts of lines 103-130 because the current wording is both more confusing, and seems to imply a bigger contribution than simply adding a layer. Moreover, in this case, the contrastive learning part should be more properly described as using a combination of MoCo and SimCLR, rather than a variant of only the latter, which contributes MLP head and augmentations. 2. What exactly is named SimCLRv2? The contrastive learning part, or the entire method (including finetuning & distillation)? The paper refers to both with SimCLRv2 in various places. If it is the latter, I suggest using a different name because it is solving a different task.

Review Point: 1. If I understand correctly, the combination of using a larger MLP projection head, and finetuning from the middle layer of the MLP head, is the same as just adding one more FC layer to the base network. Is this true? If so, I suggest the authors revise relevant parts of lines 103-130 because the current wording is both more confusing, and seems to imply a bigger contribution than simply adding a layer. Moreover, in this case, the contrastive learning part should be more properly described as using a combination of MoCo and SimCLR, rather than a variant of only the latter, which contributes MLP head and augmentations.
Review Point: 2. What exactly is named SimCLRv2? The contrastive learning part, or the entire method (including finetuning & distillation)? The paper refers to both with SimCLRv2 in various places. If it is the latter, I suggest using a different name because it is solving a different task.
==================================================

Focused review:

Weakness 1. I did not follow what differences the authors were referring to, between “trivial random sparsity” and “structurally meaningful masks”. What’s the criteria? 2. Experiments are relatively limited. For example, only CIFAR-10 dataset is used as the testbed. Will the observations scale up to larger datasets? Also, could the authors find out whether some PaI methods can be obvious winners under their proposed metrics?

Review Point: 1. I did not follow what differences the authors were referring to, between “trivial random sparsity” and “structurally meaningful masks”. What’s the criteria?
Review Point: 2. Experiments are relatively limited. For example, only CIFAR-10 dataset is used as the testbed. Will the observations scale up to larger datasets? Also, could the authors find out whether some PaI methods can be obvious winners under their proposed metrics?
==================================================

Focused review:

Weakness:
My main concern is about nondeterminism, i.e., Sec. 5. I think the experimental and theoretical analysis seem weak to support the perspective. Specifically,
(Sec. 5.1) the authors claim that non-convexity can prevent forgetting, but the designed experiments are not good, where adding more samples when performing the k-means algorithm has little impact on forgetting samples. I think the reason should be that the employed algorithm does not change the features of samples, thus, always remember the samples, which is different from the scenario of training DNNs, i.e., updating makes features change. Thus, remembering comes from unchanged features rather than nondeterminism.
(Sec. 5.2) further experiments conducted to support the perspective are given in Sec. 5.2, which also seems not good enough. Specifically, the adversary holds the same data D, extra data D_p, and even the same batch order. This means that the adversary knows everything. I cannot understand why the adversary is interested in attacking the target because the adversary even knows exactly what the difference is from D_p.
(Sec. 5.3) the authors draw a conclusion using the introduced example of mean estimation: introducing two different samples causes the difference between models, but the nondeterminism can cause forgetting, i.e., hard to distinguish the difference. However, the difficulty in distinguishing actually results from more training iterations, k in Theorem 1, rather than the nondeterminism. I hope the authors could clarify the above questions.

Review Point: 5. I think the experimental and theoretical analysis seem weak to support the perspective. Specifically, (Sec. 5.1) the authors claim that non-convexity can prevent forgetting, but the designed experiments are not good, where adding more samples when performing the k-means algorithm has little impact on forgetting samples. I think the reason should be that the employed algorithm does not change the features of samples, thus, always remember the samples, which is different from the scenario of training DNNs, i.e., updating makes features change. Thus, remembering comes from unchanged features rather than nondeterminism. (Sec. 5.2) further experiments conducted to support the perspective are given in Sec. 5.2, which also seems not good enough. Specifically, the adversary holds the same data D, extra data D_p, and even the same batch order. This means that the adversary knows everything. I cannot understand why the adversary is interested in attacking the target because the adversary even knows exactly what the difference is from D_p. (Sec. 5.3) the authors draw a conclusion using the introduced example of mean estimation: introducing two different samples causes the difference between models, but the nondeterminism can cause forgetting, i.e., hard to distinguish the difference. However, the difficulty in distinguishing actually results from more training iterations, k in Theorem 1, rather than the nondeterminism. I hope the authors could clarify the above questions.
==================================================

Focused review:

1. There is no end-to-end approximation guarantee for the proposed method. There is still a large gap between the results to the required guarantee. Theorem 2 only guarantees the existence of function in the space with random features. Several components are missing. (1) The learning algorithm can find the right weight even in the correct function space; (2) One can find the correct proposal distribution \phi. C could easily have exponential dependence on the size of the graph size. (3) The training data generation involves NP-hard problem so the training data is at best approximate. How it effects the learned solution? 2. The problem setting is a little bit artificial. Usually the observation comes from the seeds of both positive and negative information and the outcome of the diffusion. Also, under this case, it would be interesting to compare the proposed method to first learning the influence function under the competitive model from normal observation data and then carry out optimization for prevention. 3. The description for the comparison to NN based method is little bit vague. How is the problem formulated? Do the authors directly train a network with |V| output nodes and use standard classification loss? Also, it would be better if the authors could provide run time experiments. In addition, experiment under different diffusion models would be interesting to see as well.

Review Point: 1. There is no end-to-end approximation guarantee for the proposed method. There is still a large gap between the results to the required guarantee. Theorem 2 only guarantees the existence of function in the space with random features. Several components are missing. (1) The learning algorithm can find the right weight even in the correct function space; (2) One can find the correct proposal distribution \phi. C could easily have exponential dependence on the size of the graph size. (3) The training data generation involves NP-hard problem so the training data is at best approximate. How it effects the learned solution?
Review Point: 2. The problem setting is a little bit artificial. Usually the observation comes from the seeds of both positive and negative information and the outcome of the diffusion. Also, under this case, it would be interesting to compare the proposed method to first learning the influence function under the competitive model from normal observation data and then carry out optimization for prevention.
Review Point: 3. The description for the comparison to NN based method is little bit vague. How is the problem formulated? Do the authors directly train a network with |V| output nodes and use standard classification loss? Also, it would be better if the authors could provide run time experiments. In addition, experiment under different diffusion models would be interesting to see as well.
==================================================

Focused review:

Even though the paper reads well in its current form, I found during the first reading that the presentation of the conceptual messages (which are the most important part in my opinion) were overshadowed by the algorithmic comparison between GOLEM and NOTEARS. The paper is dense and it is not necessarily clear what the take-away messages of the paper are. - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions. - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.

Review Point: - I would encourage the authors to elaborate more on the differences between the findings their findings that are related to algorithmic implementations and the findings that pertain to the asymptotic of loss functions.
Review Point: - I think it is very important to discuss in much more details (and maybe earlier in the paper) the relevance in your setting of transforming a constraint objective into a penalized objective. By duality there exists a penalized version of a loss that is equivalent to the constraint form of this loss. But your results seem to indicate that the penalty parameter asymptotically vanishes for the MLE when it may diverge for the square-loss. I also think that a paragraph concerning the limitations of the current study should be included in the conclusion of the paper.
==================================================

Focused review:

Weaknesses:
(1) Though the motivation and proposed method are reasonable, the comparison with similar long-tailed contrastive learning approaches is not complete. In addition to KCL and TSC, there are some other related solutions for long-tailed recognition problem, like PaCo [1] and BCL [2]. What is the advantage of sub-class clustering over these solutions?
(2) There are quite a few hyper-parameters, like temperature, β , δ and α
in Eq. 5. Though the authors provide some hyper-parameter study on CIFAR-100-LT, I still wonder how the hyper-parameters change across datasets? Do I need to re-adjust these hyper-parameters with lots of efforts for a new dataset to achieve good performance?
(3) In Table 1, it seems these contrastive methods are implemented on different code base. To verify the effectiveness of SBCL over the SCL baseline, it is better to use your own implementation on SCL.
(4) There are quite a few works trying to re-balance the supervised contrastive loss for long-tailed recognition. What if we instead use non-contrastive methods like BYOL? For non-contrastive methods, the subclass-balancing problem naturally disappears.
[1] Parametric contrastive learning, ICCV 2021.
[2] Balanced Contrastive Learning for Long-Tailed Visual Recognition, CVPR 2022.

Review Point: 5. Though the authors provide some hyper-parameter study on CIFAR-100-LT, I still wonder how the hyper-parameters change across datasets? Do I need to re-adjust these hyper-parameters with lots of efforts for a new dataset to achieve good performance? (3) In Table 1, it seems these contrastive methods are implemented on different code base. To verify the effectiveness of SBCL over the SCL baseline, it is better to use your own implementation on SCL. (4) There are quite a few works trying to re-balance the supervised contrastive loss for long-tailed recognition. What if we instead use non-contrastive methods like BYOL? For non-contrastive methods, the subclass-balancing problem naturally disappears. [1] Parametric contrastive learning, ICCV 2021. [2] Balanced Contrastive Learning for Long-Tailed Visual Recognition, CVPR 2022.
==================================================

Focused review:

Weaknesses: 1. The proposed improvement is targeted for a specific debiasing technique and is not relevant for others. 2. No mathematical proof for the convergence of iterative method is provided.
Questions: 1. How robust are WEAT/SEAT scores wrt the size of the lists? 2. Table 4 --> INLP is consistently better than ISR and iOSCAR. Why? 3. Why is there significant difference in the absolute effect size for BERT and RoBERTa? 4. How sensitive is the proposed method to the list of words (both number and specific words) used for training? Have you done an ablation study on the no of words used? 5. Why's the idea of vectorize sentences containing those words contained in a Wikipedia dump and average of sentences from Wikipedia containing the concept words correct representation of the concept? Might have concepts unrelated to the male vs. female gender and these might get lost. 6. In Table 7, SWEAT score increased for ISR relative to Orig (1.8705 vs 1.8677) for Achieve/Anx Gen(M/F). Why? 7. How's SWEAT score a measure of information preserved? 8. Can you prove that the debiasing technique proposed in the submission doesn't introduce new bias inadvertently?

Review Point: 1. The proposed improvement is targeted for a specific debiasing technique and is not relevant for others.
Review Point: 2. No mathematical proof for the convergence of iterative method is provided. Questions:
Review Point: 1. How robust are WEAT/SEAT scores wrt the size of the lists?
Review Point: 2. Table 4 --> INLP is consistently better than ISR and iOSCAR. Why?
Review Point: 3. Why is there significant difference in the absolute effect size for BERT and RoBERTa?
Review Point: 4. How sensitive is the proposed method to the list of words (both number and specific words) used for training? Have you done an ablation study on the no of words used?
Review Point: 5. Why's the idea of vectorize sentences containing those words contained in a Wikipedia dump and average of sentences from Wikipedia containing the concept words correct representation of the concept? Might have concepts unrelated to the male vs. female gender and these might get lost.
Review Point: 6. In Table 7, SWEAT score increased for ISR relative to Orig (1.8705 vs 1.8677) for Achieve/Anx Gen(M/F). Why?
Review Point: 8. Can you prove that the debiasing technique proposed in the submission doesn't introduce new bias inadvertently?
==================================================

Focused review:

This paper in its current state could definitely be useful in facilitating discussions on hallucinations in dialogue models and datasets. At the same time, I also think this paper could have more impact if it engaged in discussion on the relationship between hallucination and other metrics relevant for dialogue models, e.g., engagingness (specifically, is the takeaway that we should all try to remove hallucinations from datasets that our models are trained on? Would that remove generated hallucinations? Would this removal make conversational models less interesting or engaging? What are potential trade-offs?). I think starting this discussion is relevant and important especially for this "data quality audit" work, because this work sets a precedent/"standard" measurement for the types of hallucinations present in dialogue benchmarks, and thus the community should be aware of its intended use and potential considerations. 
- It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
- Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - "Limitations" section in the Appendix could be moved to the "Impact Statement & Ethics" section. 

Review Point: - It might be useful to include a brief discussion of why we need to fix hallucinations (i.e., why they are undesirable), in addition to pointing to existing work.
Review Point: - Might be interesting to expand on how the different VRM categories correlate with metrics like model engagingness - "Limitations" section in the Appendix could be moved to the "Impact Statement & Ethics" section.
==================================================

Focused review:

1. The paper is not very well-motivated. 2. The simulations are not convincing why the theoretical results would be important for practical applications.

Review Point: 2. The simulations are not convincing why the theoretical results would be important for practical applications.
==================================================

Focused review:

=============================== What is missing in this work? =============================== Here are the following things that I think are missing from this work and should be addressed: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture. 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach. 3. I have some reservation about the evaluations. Please see the next section for specific details.

Review Point: 1. Universal Texture Synthesis: The paper claims universal texture synthesis. However, it has been demonstrated to work regular texture patterns alone. There is a large variety of non-stationary texture (Zhou et al. [61]) that I think this work cannot address because of the fundamental regularity assumption or repetitive or stationary texture.
Review Point: 2. Competitive Baselines: I carefully looked through the outputs of Self-Tuning [2] and the results are equally impressive. The quantitative analysis and the human studies also seemingly suggest that. Impressively, [2] runs on a CPU with 8 core and the proposed formulation requires a Tesla V100 GPU. I would also point to the quality of results synthesized using Texture CNN. One may, however, complain about the amount of time it takes to synthesize a new texture using this approach.
Review Point: 3. I have some reservation about the evaluations. Please see the next section for specific details.
==================================================

Focused review:

weakness of the paper is a lack of theoretical results on the proposed methodology. Most of the benefits of the new model have been demonstrated by simulations. It would be very helpful if the authors could provide some theoretical insights on the relation between the model parameters and the tail dependence measures, and on the performance (consistency, efficiency etc) of the parameter estimators. Itemized comments: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired. 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2. 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general. 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful. 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.

Review Point: 1. The advantage of the new quantile function (3) compared to the existing function (2) seems unjustified. Compared with (2), (3) changes the multiplicative factors containing the up and down tail parameters into an additive term. While this makes the function less sensitive to the tail parameters when they are large, the paper does not present supporting data on why the reduced sensitivity is desired.
Review Point: 2. On Line 132, the authors concluded that v_{ij} determines mainly the down-tail dependence of y_i and y_j. For any 1 <= k < j, does v_{ik} also have similar interpretation as v_{ij}? For example, in Equation (4), by symmetry, v_{31} and v_{32} seems to have similar effect on the tail dependence between y_3 and y_2.
Review Point: 3. In Algorithm 1 on Page 5, \Psi (the set of \tau's in Equation (7)) should also be an input parameter of the algorithm. Moreover, since it determines which quantiles are estimated in the loss function, I'd expect it to have notable effect on the results. I think it would be helpful to discuss how \Psi was chosen in the experiments, and provide some guidance on its choice in general.
Review Point: 4. Equation (13) doesn't seem to have closed form solution in general. Some details about how it's solved in the experiments and on the computational complexity would be helpful.
Review Point: 5. In addition to the up and down tail dependences, how could we also model negative tail dependence, e.g., P(X < Q_X(t), Y > Q_Y(1 - t)) / t? This is the counterpart of negative correlations, and is also notably common in financial asset returns (e.g., when money flow from one asset class (e.g., stocks) another (e.g., bonds)). Minor comments:
Review Point: 1. In Figures 2 and 3, it may be clearer to see the fitting errors if we overlay the oracle and the fitted lines in the same plot. Update: Thanks to the authors for the feedback. I believe Items 2 and 5 above are well addressed. On the other hand, as pointed out by another reviewer as well, a lack of theoretical results still seems to be the main weakness of the paper, though I agree that due to the complexity of the learning procedure, an extensive theoretical analysis would be a luxury at this stage.
==================================================

Focused review:

1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements. This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines? 
2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509. 
3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning. 
4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required. 
1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}: 4 should be x1 x2 y1 y2, and this should be stated. Table 3: the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough. 
2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work. 
3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since "DVD" seems to distract readers from its real meaning. 

Review Point: 1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements. This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines?
Review Point: 2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509.
Review Point: 3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning.
Review Point: 4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required.
Review Point: 1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}:
Review Point: 4 should be x1 x2 y1 y2, and this should be stated. Table 3: the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough.
Review Point: 2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work.
Review Point: 3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since "DVD" seems to distract readers from its real meaning.
==================================================

Focused review:

- Although the proposed policy iteration is novel, it feels a bit like a straightforward extension of the previous work on Persistent Fitted Q-iteration [11]. It would be good to motivate why it is a non-trivial extension from the previous work and what is the new challenges. - The experimental results could be more comprehensive. For instance, it would be interesting to see how action persistence affects the performance by varying it. It would be also interesting to show some qualitative examples (e.g., traffic control) highlighting the limitation of the naive approaches in contrast to the proposed method.

Review Point: - Although the proposed policy iteration is novel, it feels a bit like a straightforward extension of the previous work on Persistent Fitted Q-iteration [11]. It would be good to motivate why it is a non-trivial extension from the previous work and what is the new challenges.
Review Point: - The experimental results could be more comprehensive. For instance, it would be interesting to see how action persistence affects the performance by varying it. It would be also interesting to show some qualitative examples (e.g., traffic control) highlighting the limitation of the naive approaches in contrast to the proposed method.
==================================================

Focused review:

Weaknesses: 1) The technical depth of the paper is shallow. It lacks deep theoretical underpinning. The key idea is mainly contained in Eq. (5), which is basically kernel approximation using truncated polynomials. The idea is simple and straightforward. It does not represent a major breakthrough. 2) The paper mainly conducts empirical studies by applying the proposed structure-aware convolution neural network to various data sets. But the performance gain is insignificant and the cost is high. 

Review Point: 1) The technical depth of the paper is shallow. It lacks deep theoretical underpinning. The key idea is mainly contained in Eq. (5), which is basically kernel approximation using truncated polynomials. The idea is simple and straightforward. It does not represent a major breakthrough.
Review Point: 2) The paper mainly conducts empirical studies by applying the proposed structure-aware convolution neural network to various data sets. But the performance gain is insignificant and the cost is high.
==================================================

Focused review:

Weaknesses: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.

Review Point: 1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance.
Review Point: 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations.
Review Point: 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods.
Review Point: 4. I want to know why the mode parameters in Table 1 and Figure 5 are different.
Review Point: 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.
==================================================

Focused review:

Weaknesses
The memory constrained results seem very contrived. 60 MB is a tiny amount of memory and even 9.0 GB of (presumably) CPU memory isn't that prohibitive.
Perhaps if wall-clock time was plotted in addition to samples, the smaller memory footprint of LeVER would mean the replay buffer can be stored on the GPU and training would be much faster since many expensive CPU -> GPU transfers would be eliminated?
The CNN is frozen all at once instead of frozen iteratively. Raghu 2017 and Figure 6c suggest that the early layers could be frozen much earlier, although this may increase the memory usage initially since CNNs typically increase the memory size of the feature map in lower layers.
T_f seems like yet another hyper-parameter to tune. In theory, SVCCA (or PWCCA from Morcos 2018) could be used to choose when to freeze (if the representation of the shallowest unfrozen layer didn't change in the last K steps, freeze it). There is a nontrivial cost to computing either of those so it
Suggestions for improvement
I very much like the idea of this paper, but I think the chosen application is making the idea look less convincing (i.e. freezing the CNN isn't really that impactful when the CNN and observations are tiny). I urge the authors to try this for visual navigation (i.e. PointGoal Navigation in Habitat/AI2 Thor/etc), where deeper CNNs, e.g. ResNet18, and higher resolution images, e.g. 256x256, are used.
One other potentially benefit of LeVER is the ability to increase the batch size during training (as in Smith 2017). This could perhaps increase its effectiveness further?
In figure 6a, there should also be a CURL + LeVER from Scratch line. Currently two variables are changing.
One paper that should be cited is Fang 2019. They do a very similar thing as LeVER out of necessity. Overall
While I like this paper and think the idea has a lot of potential, I don't think it is quite ready for publication yet. I urge the authors to try their idea in a setting with a larger CNN and higher resolutions and to see if there is a way to find T_f without it being "yet another hyper-parameter". References
Smith 2017: https://arxiv.org/abs/1711.00489
Morcos 2018: https://arxiv.org/abs/1806.05759
Fang 2019: https://arxiv.org/abs/1903.03878
Post Rebuttal
I thank the authors for their responses. The results with a larger CNN and increased batch size help show the benefit of the method further. I still believe the presentation of the method would be considerably stronger if results were presented in a setting with larger CNNs and higher resolution.

Review Point: 60 MB is a tiny amount of memory and even 9.0 GB of (presumably) CPU memory isn't that prohibitive. Perhaps if wall-clock time was plotted in addition to samples, the smaller memory footprint of LeVER would mean the replay buffer can be stored on the GPU and training would be much faster since many expensive CPU -> GPU transfers would be eliminated? The CNN is frozen all at once instead of frozen iteratively. Raghu 2017 and Figure 6c suggest that the early layers could be frozen much earlier, although this may increase the memory usage initially since CNNs typically increase the memory size of the feature map in lower layers. T_f seems like yet another hyper-parameter to tune. In theory, SVCCA (or PWCCA from Morcos 2018) could be used to choose when to freeze (if the representation of the shallowest unfrozen layer didn't change in the last K steps, freeze it). There is a nontrivial cost to computing either of those so it Suggestions for improvement I very much like the idea of this paper, but I think the chosen application is making the idea look less convincing (i.e. freezing the CNN isn't really that impactful when the CNN and observations are tiny). I urge the authors to try this for visual navigation (i.e. PointGoal Navigation in Habitat/AI2 Thor/etc), where deeper CNNs, e.g. ResNet18, and higher resolution images, e.g. 256x256, are used. One other potentially benefit of LeVER is the ability to increase the batch size during training (as in Smith 2017). This could perhaps increase its effectiveness further? In figure 6a, there should also be a CURL + LeVER from Scratch line. Currently two variables are changing. One paper that should be cited is Fang 2019. They do a very similar thing as LeVER out of necessity. Overall While I like this paper and think the idea has a lot of potential, I don't think it is quite ready for publication yet. I urge the authors to try their idea in a setting with a larger CNN and higher resolutions and to see if there is a way to find T_f without it being "yet another hyper-parameter". References Smith 2017: https://arxiv.org/abs/1711.00489 Morcos 2018: https://arxiv.org/abs/1806.05759 Fang 2019: https://arxiv.org/abs/1903.03878 Post Rebuttal I thank the authors for their responses. The results with a larger CNN and increased batch size help show the benefit of the method further. I still believe the presentation of the method would be considerably stronger if results were presented in a setting with larger CNNs and higher resolution.
==================================================

Focused review:

- Although the authors do a good job of providing results for the proposed methodology and compare against baseline to establish the functional effectiveness. The performance comparison seems to be missing. One of the primary claims is the with the reduction of gradient volume, the scaling efficiency would improve. However, there aren’t substantial results to back this, there than the limited detail in Appendix.F. Which has comparison for a family small minibactch size per work = 8, which is almost an order of magnitude smaller than typically used configurations. Also, with the lower batch sizes the comms bottlenecks get further exposed. So the question whether the results are skewed towards showing unrealistic benefits. - The amount reduction in communication volume is quite significant, however allreduce is a portion of the total iteration time. With upto 400X compression, it would be good to see the commensurate benefit in scaling. And where the benefit taper due to the Amdhal fraction (contribution from the other components). The results from Appendix seems to indicate a max of 2X benefits, in which the regime of demising returns could be better highlighted. - Furthermore, along similar lines it would be good if there were results which showcase the ability to scale further with this comms volume reductions This is a pretty significant gap, which reduces confidence in the method due to the question around practical applicability.

Review Point: - Although the authors do a good job of providing results for the proposed methodology and compare against baseline to establish the functional effectiveness. The performance comparison seems to be missing. One of the primary claims is the with the reduction of gradient volume, the scaling efficiency would improve. However, there aren’t substantial results to back this, there than the limited detail in Appendix.F. Which has comparison for a family small minibactch size per work = 8, which is almost an order of magnitude smaller than typically used configurations. Also, with the lower batch sizes the comms bottlenecks get further exposed. So the question whether the results are skewed towards showing unrealistic benefits.
Review Point: - The amount reduction in communication volume is quite significant, however allreduce is a portion of the total iteration time. With upto 400X compression, it would be good to see the commensurate benefit in scaling. And where the benefit taper due to the Amdhal fraction (contribution from the other components). The results from Appendix seems to indicate a max of 2X benefits, in which the regime of demising returns could be better highlighted.
Review Point: - Furthermore, along similar lines it would be good if there were results which showcase the ability to scale further with this comms volume reductions This is a pretty significant gap, which reduces confidence in the method due to the question around practical applicability.
==================================================

Focused review:

1. It seems that this method is only suitable for white box attacks. When the attacked GBDT tree based model is unknown (including the depth and number of trees), this method is obviously not applicable. 2. The paper does not give the experiments on more challenging data sets to verify the method performance on the tree with higher complexity. 3. The comparison methods are based on Python time statistics, while your method is based on C + +, which does not seem to have fair comparison. 4. Lack of ablation experiments and visual samples, which makes the manuscript difficult to understand.

Review Point: 1. It seems that this method is only suitable for white box attacks. When the attacked GBDT tree based model is unknown (including the depth and number of trees), this method is obviously not applicable.
Review Point: 2. The paper does not give the experiments on more challenging data sets to verify the method performance on the tree with higher complexity.
Review Point: 3. The comparison methods are based on Python time statistics, while your method is based on C + +, which does not seem to have fair comparison.
Review Point: 4. Lack of ablation experiments and visual samples, which makes the manuscript difficult to understand.
==================================================

Focused review:

Although this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant). But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. 
The following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. 

Review Point: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them.
==================================================

Focused review:

The work has a certain tendency to misrepresent the literature for the sake of highlighting the originality and impact of this paper. I do not particularly appreciate this type of approach to a scientific paper. More specifically, I strongly invite the Authors to reconsider the following aspects: - Multi-instance Vs Single-Instance: much of the introduction and motivation of the paper is devoted to stating that multi-instance interpretability is the only desired interpretation one would like to have. This is false, if only for a very practical aspect: general data protection regulations require the right of explanation on the “single” prediction. The one that pertains the single individual and the single decision that has been automatically taken by a computational model. Than is single instance explainability and it is very important to develop methodologies and methods for it. POST-REBUTTAL: no comments or discussions on this in the rebuttal. I hope, nonetheless, that it will be taken into consideration in future version of the paper. - GNNExplainer is only capable of single-instance explainability: this is FALSE. There is not much to say but reference Appendix A of the original paper. I can agree that it is not natively thought to be multi-instance, but the paper conveys all the necessary information to use it in multi-instance settings. So please avoid excessive statements referring to an incapability of GNNExplainer in dealing with multi-instance. POST-REBUTTAL: Again here the issue is associated to an unclear positioning of the paper and of the related models in literature. In the rebuttal the authors clearly state that the difference between the proposed model and GNNExplainer is in the parameterization (which I agree on). But in the paper it is written that a key difference in that he proposed method is capable of multi-instance explanation while GNNEplainer is not. And this is not true, in absolute terms. So Authors are invited to revise this discussion. Presentation quality and level of technical detail is not adequate to allow the reader to reconstruct all the details of the model. This is due to the presentation lacking sufficient details when it comes to key aspect of the models and inconsistencies in the formalization. To gain sufficient insight into the model I have read throughout the appendixes: there I have found two pseudo-codes which did not help in clarifying open points. First, for the most part they are highly redundant, so reporting a single setting would have been sufficient, provided that the space saved is used to supply more information on the inner workings of the method. In particular, a second pseudo-code could have been used to show how to build the model level explanation. Second, equations (10) and (11) are defined on triples and couples of node embeddings, respectively. However, in their use in Algorithm 1 and 2 they are used within a loop running on a single node i: what about nodes j, v in (10) and j in (11)? All in all, there is not enough clarity in the description of the model to allow a its straightforward replication, and the lack of associated source code worsens this aspect. The empirical analysis has some unclear aspects: 1) Why is AUC used in place of accuracy as in the original GNNExplainer paper? Since the experimental setup is the same (as stated by the Authors) also the performance assessment should be the same (unless there is a very good reason for this, which I couldn’t find in the paper). 2) Why only PGExplainer results have stds? If all models have been tested under the same conditions, this should be available and reported for all in Table 1.

Review Point: - Multi-instance Vs Single-Instance: much of the introduction and motivation of the paper is devoted to stating that multi-instance interpretability is the only desired interpretation one would like to have. This is false, if only for a very practical aspect: general data protection regulations require the right of explanation on the “single” prediction. The one that pertains the single individual and the single decision that has been automatically taken by a computational model. Than is single instance explainability and it is very important to develop methodologies and methods for it. POST-REBUTTAL: no comments or discussions on this in the rebuttal. I hope, nonetheless, that it will be taken into consideration in future version of the paper.
Review Point: - GNNExplainer is only capable of single-instance explainability: this is FALSE. There is not much to say but reference Appendix A of the original paper. I can agree that it is not natively thought to be multi-instance, but the paper conveys all the necessary information to use it in multi-instance settings. So please avoid excessive statements referring to an incapability of GNNExplainer in dealing with multi-instance. POST-REBUTTAL: Again here the issue is associated to an unclear positioning of the paper and of the related models in literature. In the rebuttal the authors clearly state that the difference between the proposed model and GNNExplainer is in the parameterization (which I agree on). But in the paper it is written that a key difference in that he proposed method is capable of multi-instance explanation while GNNEplainer is not. And this is not true, in absolute terms. So Authors are invited to revise this discussion. Presentation quality and level of technical detail is not adequate to allow the reader to reconstruct all the details of the model. This is due to the presentation lacking sufficient details when it comes to key aspect of the models and inconsistencies in the formalization. To gain sufficient insight into the model I have read throughout the appendixes: there I have found two pseudo-codes which did not help in clarifying open points. First, for the most part they are highly redundant, so reporting a single setting would have been sufficient, provided that the space saved is used to supply more information on the inner workings of the method. In particular, a second pseudo-code could have been used to show how to build the model level explanation. Second, equations (10) and (11) are defined on triples and couples of node embeddings, respectively. However, in their use in Algorithm 1 and 2 they are used within a loop running on a single node i: what about nodes j, v in (10) and j in (11)? All in all, there is not enough clarity in the description of the model to allow a its straightforward replication, and the lack of associated source code worsens this aspect. The empirical analysis has some unclear aspects:
Review Point: 1) Why is AUC used in place of accuracy as in the original GNNExplainer paper? Since the experimental setup is the same (as stated by the Authors) also the performance assessment should be the same (unless there is a very good reason for this, which I couldn’t find in the paper).
Review Point: 2) Why only PGExplainer results have stds? If all models have been tested under the same conditions, this should be available and reported for all in Table 1.
==================================================

Focused review:

1) the strategy is not clearly explained. The details on the implementation of the strategy, in Subsections 5.1 and 5.2, are extremely difficult to follow for a non-specialist of this literature, because no explanations on the quantities introduced are provided, and some formal definitions are missing. This looks almost like an informal description of the method. Consider for example the sentence "A fantasy GP model is the GP model obtained by conditioning the GP model on a fantasy observation simulated using the distribution implied by the GP model at the candidate point", it seems to me that a formal statement could make this sentence easier to understand. 2) it is difficult to assess the contribution of the authors, in the sense that the estimators of the gradient in the rho-KG strategy were studied in [42] and [43]...

Review Point: 1) the strategy is not clearly explained. The details on the implementation of the strategy, in Subsections 5.1 and 5.2, are extremely difficult to follow for a non-specialist of this literature, because no explanations on the quantities introduced are provided, and some formal definitions are missing. This looks almost like an informal description of the method. Consider for example the sentence "A fantasy GP model is the GP model obtained by conditioning the GP model on a fantasy observation simulated using the distribution implied by the GP model at the candidate point", it seems to me that a formal statement could make this sentence easier to understand.
Review Point: 2) it is difficult to assess the contribution of the authors, in the sense that the estimators of the gradient in the rho-KG strategy were studied in [42] and [43]...
==================================================

Focused review:

1. First of all, compared with other excellent papers, this paper is slightly less innovative. 
2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. 
3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. 
4. It lacks case study to show which document-level translation errors are improved by the proposed method. 
1. It is suggested to add a structure diagram to illustrate your proposed method. 
2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. 
3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. 

Review Point: 1. First of all, compared with other excellent papers, this paper is slightly less innovative.
Review Point: 2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited.
Review Point: 3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis.
Review Point: 4. It lacks case study to show which document-level translation errors are improved by the proposed method.
Review Point: 1. It is suggested to add a structure diagram to illustrate your proposed method.
Review Point: 2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution.
Review Point: 3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly.
==================================================

Focused review:

Weakness] * The same optimization problem has been independently studied using a similar approach. * The master algorithm cannot be seen as SGD. * No empirical comparison with the existing algorithms [Recommendation] I recommend this paper to be evaluated as "Marginally above the acceptance threshold". This is a theoretical paper and no empirical result is shown. Theoretical comparison with existing algorithms has been done comprehensively. One anxiety is practical usefulness of this algorithm. SGD needs many iteration until convergence, so communication cost between the master and workers may become larger than GD. [Detailed Comments] p.5 \Delta_i, \Delta_{med} and \nabla_i are not defined. 

Review Point: * The same optimization problem has been independently studied using a similar approach.
Review Point: * No empirical comparison with the existing algorithms [Recommendation] I recommend this paper to be evaluated as "Marginally above the acceptance threshold". This is a theoretical paper and no empirical result is shown. Theoretical comparison with existing algorithms has been done comprehensively. One anxiety is practical usefulness of this algorithm. SGD needs many iteration until convergence, so communication cost between the master and workers may become larger than GD. [Detailed Comments] p.5 \Delta_i, \Delta_{med} and \nabla_i are not defined.
==================================================

Focused review:

Weaknesses:
1. In section 3.1 they do not specify what certain notations mean , eg the difference between the two transaction tables on the right of figure 2. 
2. Jump from section 3.2 to 3.3 is big especially for people who are unfamiliar with algorithms they point to such as FP-growth Han et al. (2000) and apriori Agrawal et al. (1994). They use an example for section 3.1 but then they drop the example for subsequent sections in the algortihm . 
3. Other evaluation metrics employed by other papers eg, fidelity to the model and comprehensibility could have been explored . Human evaluations might make a more compelling case .
4. They don’t perform any study about which semantic features help and which harm the f1 score.
5. Visualizaition is an important part of explainable models which this paper lacks

Review Point: 1. In section 3.1 they do not specify what certain notations mean , eg the difference between the two transaction tables on the right of figure 2.
Review Point: 2. Jump from section 3.2 to 3.3 is big especially for people who are unfamiliar with algorithms they point to such as FP-growth Han et al. (2000) and apriori Agrawal et al. (1994). They use an example for section 3.1 but then they drop the example for subsequent sections in the algortihm .
Review Point: 3. Other evaluation metrics employed by other papers eg, fidelity to the model and comprehensibility could have been explored . Human evaluations might make a more compelling case .
Review Point: 4. They don’t perform any study about which semantic features help and which harm the f1 score.
Review Point: 5. Visualizaition is an important part of explainable models which this paper lacks
==================================================

Focused review:

Weaknesses: 1. Some of the results are confusing and need more illustration (see comments below). 2. Experiments are only conducted on synthetic datasets and more results on real-world datasets especially common GNN benchmarks are expected.
The major limitation lies in the experiments, though most of contents seem reasonable and sound. The experiment datasets only cover synthetic datasets which are simple and limited. I suggest adding more experiments on real-world datasets, especially some common GNN benchmarks for link prediction and comparing with some SOTA approaches (even though they are not designed for OOD regime). Though I understand that the main focus of this paper lies in the theoretical insights and perspecitve, more results on benchmarks and comparison with more strong models (currently used baselines seem too weak and simple) could make this work more complete and convincing, especially that one of the contributions of this work is a new approach for OOD link prediction.
Also, there are some recent works on out-of-distribution generalization on graphs, e.g., [1, 2, 3], which are missing in the related works and need to be discussed.
[1] Size-Invariant Graph Representations for Graph Classification Extrapolations, ICML 21.
[2] Handling Distribution Shifts on Graphs: An Invariance Perspective, ICLR 22.
[3] From Local Structures to Size Generalization in Graph Neural Networks, ICML 21.

Review Point: 1. Some of the results are confusing and need more illustration (see comments below).
Review Point: 2. Experiments are only conducted on synthetic datasets and more results on real-world datasets especially common GNN benchmarks are expected. The major limitation lies in the experiments, though most of contents seem reasonable and sound. The experiment datasets only cover synthetic datasets which are simple and limited. I suggest adding more experiments on real-world datasets, especially some common GNN benchmarks for link prediction and comparing with some SOTA approaches (even though they are not designed for OOD regime). Though I understand that the main focus of this paper lies in the theoretical insights and perspecitve, more results on benchmarks and comparison with more strong models (currently used baselines seem too weak and simple) could make this work more complete and convincing, especially that one of the contributions of this work is a new approach for OOD link prediction. Also, there are some recent works on out-of-distribution generalization on graphs, e.g., [1, 2, 3], which are missing in the related works and need to be discussed. [1] Size-Invariant Graph Representations for Graph Classification Extrapolations, ICML 21. [2] Handling Distribution Shifts on Graphs: An Invariance Perspective, ICLR 22. [3] From Local Structures to Size Generalization in Graph Neural Networks, ICML 21.
==================================================

Focused review:

Weakness:
The authors state that "Despite their fast speed in the early training phase, adaptive gradient methods, especially Adam, are found by studies (Wilson et al., 2017; Zhou et al., 2020) to be more likely to exhibit poorer generalization ability than SGD (M). This is discouraging because the ultimate goal of training in many machine learning tasks is to exhibit favorable performance during the testing phase." --- This statement seems incorrect to me. As far as I am aware, training deep neural networks using Adam is often much better than using SGD (M), including training Recurrent neural networks, transformers for NLP tasks, vision transformers for vision tasks, and generative adversarial networks.
The authors state that "Since the EMA of the gradient is a more accurate estimation of the appropriate direction to descent, we consider putting it in the second moment estimation term as well."--- What is the intuition of EMA of the gradient is a more accurate estimation? In Adam, the second moment is already a moving average of the square of the gradient. Replacing g_t^2 with m_t^2 in estimating the second moment v_t seems simular to using a smaller beta_2.
epsilon has already been added into Adam for numerical stability guarantee. If putting epsilon inside the square root affects the performance, I want the authors to do an ablation study. Also, the authors should contrast AdaM^3 with Adam using different values of epsilon.
The explanation in section 3.1 is unconvincing. 1) In area A, when g_t increases, both v_t and m_t increase, the the update \alpha*\hat{m}_t/\sqrt{\hat{v}_t} does not really decays significantly. If both step size and \hat{m}_t are very large, the stability of the optimizer will be a concern. Similarly, I do not agree with the other points in section 3.1. The argument on the discussion of the benefit of changing the location of \epsilon merely shows that \epsilon matters, and by using a larger \epsilon for Adam, Adam becomes similar to AdaM^3.
Assumption 3 is not true; clearly, the gradient direction is very different from the direction of m_t.
The discussion in section 3.2 does not imply AdaM^3 generalizes better than Adam. First, there are a lot of arguments about flat minima; whether this implies better generalization is not something for sure. Second, some assumptions in section 3.2 do not hold. 3. The authors should at least design a simple toy example that contains one flat and one non-flat minimum to show that 1) flat minima generalize better and 2) AdaM^3 converges to the flat one.
As far as I am aware, the theory in section 4 does not really better than the existing theory of Adam-style algorithms.
Tuning \beta_2 and \epsilon for Adam will make Adam have the same behavior as AdaM^3. I want the author to comment on this.
For the experiment in section 5.1: 1) How about testing the quadratic functions whose level sets are ellipses of different ratios between x- and y-axis? 2) How about plotting iteration vs. f(x^t)-f^* for both Adam and AdaM^3.
In section 5.2, the authors claim that AdaM^3 is tuning-friendly. I would like to point out that Adam and AdamW also very tuning-friendly. alpha=0.001, beta_1=0.9, and beta_2=0.999 are the default setting, and I usually simply use these settings without changing them at all.
Regarding the results in Table 2, I wonder why the authors use very large models for CIFAR10 classification. VGGNet-16, ResNet34, and DenseNet-121 are designed for ImageNet classification. In contrast, ResNet20, ResNet56, etc., are designed for classifying CIFAR10. See Table 6 of https://arxiv.org/pdf/1512.03385.pdf for details. I wonder why the test perplexity of LSTMs trained by Adam on Penn Treebank dataset in Table 4 is much worse than that reported in the paper https://arxiv.org/pdf/1708.02182.pdf
Overall, I doubt the proposed algorithm can replace the role of SGD and Adam in today’s deep learning practice. We can certify this by seeing what will happen in five years.

Review Point: 1) In area A, when g_t increases, both v_t and m_t increase, the the update \alpha*\hat{m}_t/\sqrt{\hat{v}_t} does not really decays significantly. If both step size and \hat{m}_t are very large, the stability of the optimizer will be a concern. Similarly, I do not agree with the other points in section 3.1. The argument on the discussion of the benefit of changing the location of \epsilon merely shows that \epsilon matters, and by using a larger \epsilon for Adam, Adam becomes similar to AdaM^3. Assumption 3 is not true; clearly, the gradient direction is very different from the direction of m_t. The discussion in section 3.2 does not imply AdaM^3 generalizes better than Adam. First, there are a lot of arguments about flat minima; whether this implies better generalization is not something for sure. Second, some assumptions in section 3.2 do not hold.
Review Point: 3. The authors should at least design a simple toy example that contains one flat and one non-flat minimum to show that 1) flat minima generalize better and 2) AdaM^3 converges to the flat one. As far as I am aware, the theory in section 4 does not really better than the existing theory of Adam-style algorithms. Tuning \beta_2 and \epsilon for Adam will make Adam have the same behavior as AdaM^3. I want the author to comment on this. For the experiment in section 5.1:
Review Point: 1) How about testing the quadratic functions whose level sets are ellipses of different ratios between x- and y-axis?
Review Point: 2) How about plotting iteration vs. f(x^t)-f^* for both Adam and AdaM^3. In section 5.2, the authors claim that AdaM^3 is tuning-friendly. I would like to point out that Adam and AdamW also very tuning-friendly. alpha=0.001, beta_1=0.9, and beta_2=0.999 are the default setting, and I usually simply use these settings without changing them at all. Regarding the results in Table 2, I wonder why the authors use very large models for CIFAR10 classification. VGGNet-16, ResNet34, and DenseNet-121 are designed for ImageNet classification. In contrast, ResNet20, ResNet56, etc., are designed for classifying CIFAR10. See Table 6 of https://arxiv.org/pdf/1512.03385.pdf for details. I wonder why the test perplexity of LSTMs trained by Adam on Penn Treebank dataset in Table 4 is much worse than that reported in the paper https://arxiv.org/pdf/1708.02182.pdf Overall, I doubt the proposed algorithm can replace the role of SGD and Adam in today’s deep learning practice. We can certify this by seeing what will happen in five years.
==================================================

Focused review:

weaknesses:
while this is a great step for the "deep thinking" models, I don't see a lot of comparison to/discussion of other (if any) architectures that could be applied to these tasks. E.g., why not compare to a universal transformer? In this sense the work is not well contextualized.
comments, including presentation comments and typos:
(general) this work uses 'recurrence', but in a different way from RNNs: RNNs recur alongside an input sequence, eating one token at a time and finishing at the end of the input, this architecture simply recurs on its own state (albeit with help of the input recall) for a given number of steps, independently of the input. Given the popularity of RNNs, for clarity, I suggest this distinction is clarified early in the paper, e.g. a sentence of the sort "this work uses recurrence in a manner different from that commonly referred to in RNN literature, the architecture behaviors should not be confused"
line 8 "because behavior degenerates" I don't know if 'behavior degenerates' so much as iterations continue past correct stop (losing solution), or representation degenerates (also losing solution), or some other issue. Rephrase (unless you prove that indeed "behavior degenerates" and is the "cause" of the failure to scale).
Fig 1 caption description of 59x59 as "center" is confusing, consider just labeling the subfigures
line 20 "lack the ability to solve complex reasoning tasks in a scalable, algorithmic, way" again strong concrete statement, not sure I agree, soften/rephrase.
Eq 2 (description of f_recall, r_recall) - bad notation: r_recall takes two inputs, one of which is its own previous output, and the other the original input to the net. By construction such a function cannot be applied recurrently (r^2_recall(phi,x) unrolls to r_recall(r_recall(phi,x)), which makes no sense because the x has to be put in again). Consider instead notation similar to that used in RNN descriptions, e.g.: phi_t+1=r_recall(phi_t,x), phi_1=r(x), f_recall(x,m)=h(phi_m) (something like that - fill it out right according to your architecture. Please also make sure to properly distinguish between the original input x and the processed input P(x), so we can tell which one is being used where!).
line 163: "re-initialize the network" - the entire network or just the recurrent part? clarify
line 164: clarify here that gradient is being discarded, easier to follow that way
line 169-170: (too) strong statements! 1) the network may arbitrarily implement counting its iterations, it is not "prevented" from doing so, and 2) the network might still learn iteration specific behaviors, whether by luck or even directly through the SGD (given that the standard loss is also still used!). The only claim I would make here is that encouraging random iterations to be correct a) encourages a stabilizing behavior (i.e. not losing the solution once obtained) and b) encourages 'faster' (shallower) solutions (i.e. having the solution ready in the earliest iteration possible) - basically, it pushes the network to have the correct solution available in the maximum number of iterations possible.
Fig 3: would also be nice to see the stability of DT-recall (i.e. no prog loss) after reaching the solution, e.g. plot accuracy up to 1000 iterations (given that prog loss is supposed to encourage stability, and here we only just see DT-recall reach the solution but not whether it stabilizes there)
line 254-255: [s/of sampling/of randomly sampling], [s/We modify/To show this, we modify], [s/slighty to always/slightly, to always]
line 259 s/random/randomly
line 265 "harder" - harder than what?
line 293-294: explicitly mention that the best models had progressive loss (currently missing from this sentence)

Review Point: 1) the network may arbitrarily implement counting its iterations, it is not "prevented" from doing so, and 2) the network might still learn iteration specific behaviors, whether by luck or even directly through the SGD (given that the standard loss is also still used!). The only claim I would make here is that encouraging random iterations to be correct a) encourages a stabilizing behavior (i.e. not losing the solution once obtained) and b) encourages 'faster' (shallower) solutions (i.e. having the solution ready in the earliest iteration possible) - basically, it pushes the network to have the correct solution available in the maximum number of iterations possible. Fig 3: would also be nice to see the stability of DT-recall (i.e. no prog loss) after reaching the solution, e.g. plot accuracy up to 1000 iterations (given that prog loss is supposed to encourage stability, and here we only just see DT-recall reach the solution but not whether it stabilizes there) line 254-255: [s/of sampling/of randomly sampling], [s/We modify/To show this, we modify], [s/slighty to always/slightly, to always] line 259 s/random/randomly line 265 "harder" - harder than what? line 293-294: explicitly mention that the best models had progressive loss (currently missing from this sentence)
==================================================

Focused review:

1. What is the computation cost compared to cross-entropy loss? 2. According to Table.4, it seems that cross-entropy loss still a bit better than Supervised contrastive. Also, the cross-entropy loss is simpler than Supervised contrastive because Supervised contrastive needs to learn a linear classifier on top of the feature. What circumstance do you think Supervised contrastive can replace cross-entropy loss? 3. Authors have provided classification results for transfer learning. However, as ImageNet pretrain is widely used in a lot of application and contrastvie loss have shown superior transfer performance on some task, I wonder how does Supervised Contrastive loss behave as a pre-training model for other vision tasks.(e.g. Object Detection) 4. Is cross-entropy loss and supervised contrastive loss learn similar pattern? If not, can we combine them to achieve better performance? 5. I understand that surpass cross-entropy loss is hard and the value of this paper. But my major concern is that supervised contrastive loss have very close performance to cross-entropy loss. It would be good if authors can provide more promising results. --------_After rebuttal----------- The authors address my concern. I would keep my rating as accept.

Review Point: 1. What is the computation cost compared to cross-entropy loss?
Review Point: 2. According to Table.4, it seems that cross-entropy loss still a bit better than Supervised contrastive. Also, the cross-entropy loss is simpler than Supervised contrastive because Supervised contrastive needs to learn a linear classifier on top of the feature. What circumstance do you think Supervised contrastive can replace cross-entropy loss?
Review Point: 3. Authors have provided classification results for transfer learning. However, as ImageNet pretrain is widely used in a lot of application and contrastvie loss have shown superior transfer performance on some task, I wonder how does Supervised Contrastive loss behave as a pre-training model for other vision tasks.(e.g. Object Detection) 4. Is cross-entropy loss and supervised contrastive loss learn similar pattern? If not, can we combine them to achieve better performance?
Review Point: 5. I understand that surpass cross-entropy loss is hard and the value of this paper. But my major concern is that supervised contrastive loss have very close performance to cross-entropy loss. It would be good if authors can provide more promising results. --------_After rebuttal----------- The authors address my concern. I would keep my rating as accept.
==================================================

